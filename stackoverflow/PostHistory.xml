<?xml version="1.0" encoding="utf-8"?>
<posthistory>
  <row Id="1" PostHistoryTypeId="2" PostId="1" RevisionGUID="ba07dfc5-d0e0-4374-a5b1-3ad6b2d28dc0" CreationDate="2017-05-16T18:01:06.383" UserId="43" Text="I'd like to learn which format it's commonly used for storing full human genome and why.&#xD;&#xA;&#xD;&#xA;I assume storing it in plain format would be very inefficient. I would think more about binary format like 2 bits per nucleotide.&#xD;&#xA;&#xD;&#xA;Therefore, which format would be the most common in terms of space efficiency?" />
  <row Id="2" PostHistoryTypeId="1" PostId="1" RevisionGUID="ba07dfc5-d0e0-4374-a5b1-3ad6b2d28dc0" CreationDate="2017-05-16T18:01:06.383" UserId="43" Text="What's the most efficient format to store DNA sequence?" />
  <row Id="3" PostHistoryTypeId="3" PostId="1" RevisionGUID="ba07dfc5-d0e0-4374-a5b1-3ad6b2d28dc0" CreationDate="2017-05-16T18:01:06.383" UserId="43" Text="&lt;human-genome&gt;&lt;storage&gt;&lt;formats&gt;" />
  <row Id="4" PostHistoryTypeId="5" PostId="1" RevisionGUID="fe74a65c-4dd1-4dba-9c28-1950c3e9ee28" CreationDate="2017-05-16T18:04:38.820" UserId="43" Comment="added 21 characters in body" Text="I'd like to learn which format it's commonly used for storing full sequence human genome (4 letters) and why.&#xD;&#xA;&#xD;&#xA;I assume storing it in plain format would be very inefficient. I would think more about binary format like 2 bits per nucleotide.&#xD;&#xA;&#xD;&#xA;Therefore, which format would be the most common in terms of space efficiency?" />
  <row Id="5" PostHistoryTypeId="2" PostId="2" RevisionGUID="3a90f523-9d63-4b0e-8fdf-d4e055bff1e7" CreationDate="2017-05-16T18:08:41.067" UserId="62" Text="In terms of raw storage capacity 2 bits per nucleotide would be the most efficient. However, you'd still have other storage considerations. Like what to do about non-standard bases: like if you want to indicate a gap or ambiguity. &#xD;&#xA;&#xD;&#xA;I'd also query if it is really necessary to store them as binary since it reduces the readability of the data. It is quite convenient having a whole bunch of unix and programming tools that can operate on the string level in text files." />
  <row Id="6" PostHistoryTypeId="2" PostId="3" RevisionGUID="b86fe02e-bae2-496a-9283-6e056caa0513" CreationDate="2017-05-16T18:09:58.197" UserId="9" Text="Genomes are commonly stored as either fasta files (.fa) or twoBit (.2bit) files. Fasta files store the entire sequence as text and are thus not particularly compressed. &#xD;&#xA;&#xD;&#xA;twoBit files store each nucleotide in two bits of information and contain additional metadata that indicates where there's regions containing `N` (unknown) bases.&#xD;&#xA;&#xD;&#xA;For more information, see the documentation on the twoBit format at the [UCSC genome browser](http://genome.ucsc.edu/FAQ/FAQformat.html#format7).&#xD;&#xA;&#xD;&#xA;You can convert between twoBit and fasta format using the [faToTwoBit and twoBitToFa utilities](https://genome.ucsc.edu/goldenpath/help/twoBit.html).&#xD;&#xA;&#xD;&#xA;For the human genome, you can download it in either fasta or twoBit format here: http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/&#xD;&#xA;&#xD;&#xA;" />
  <row Id="7" PostHistoryTypeId="2" PostId="4" RevisionGUID="f5b4f089-065b-4326-b794-d0b2cc4c55c3" CreationDate="2017-05-16T18:11:54.510" UserId="59" Text="The standard formats for storing sequence data are [fasta](https://en.wikipedia.org/wiki/FASTA_format) and [fastq](https://en.wikipedia.org/wiki/FASTQ_format).  Fasta is used if you only need the raw sequence data, fastq is used if you want to store the sequence data along with the quality information from base calling.  Each of these can be compressed using gzip or another standard compression algorithm.&#xD;&#xA;&#xD;&#xA;Typically we want to keep the quality information along with the raw sequence data, but the quality information accounts for half the storage space required.  Some people have developed [algorithms](https://web.stanford.edu/~mainakch/papers/QualComp_Final_v2.pdf) for lossy compression of the quality data that allow us to reduce the storage requirements.&#xD;&#xA;&#xD;&#xA;If you are interested in storing variant calling data, the standard format for that is [VCF](http://www.internationalgenome.org/wiki/Analysis/vcf4.0/).  VCF is useful if you want to store quality information of the variant calls, genomic positions, and any annotations you might have about the position.  VCFs can be compressed and indexed using [bgzip and tabix](http://www.htslib.org/doc/tabix.html).  Many tools require variant data to be compressed and indexed using these tools." />
  <row Id="8" PostHistoryTypeId="2" PostId="5" RevisionGUID="560c71e8-9961-44ea-bdf4-4ec7746b1acb" CreationDate="2017-05-16T18:14:20.903" UserId="43" Text="[The Human Genome Project][1] was the project of 'determining the sequence of nucleotide base pairs that make up human DNA, and of identifying and mapping all of the genes of the human genome'. It was completed in 2003.&#xD;&#xA;&#xD;&#xA;Are the datasets provided by HGP still accurate? Or given the technology in the past (such as using old techniques), or any other reason, the datasets could be not accurate?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Human_Genome_Project" />
  <row Id="9" PostHistoryTypeId="1" PostId="5" RevisionGUID="560c71e8-9961-44ea-bdf4-4ec7746b1acb" CreationDate="2017-05-16T18:14:20.903" UserId="43" Text="How accurate are human DNA sequenced by Human Genome Project?" />
  <row Id="10" PostHistoryTypeId="3" PostId="5" RevisionGUID="560c71e8-9961-44ea-bdf4-4ec7746b1acb" CreationDate="2017-05-16T18:14:20.903" UserId="43" Text="&lt;hgp&gt;&lt;dna&gt;" />
  <row Id="11" PostHistoryTypeId="2" PostId="6" RevisionGUID="cbbcab5b-32e5-4810-bdbf-9207981b3004" CreationDate="2017-05-16T18:16:13.057" UserId="-1" Text="" />
  <row Id="12" PostHistoryTypeId="1" PostId="6" RevisionGUID="cbbcab5b-32e5-4810-bdbf-9207981b3004" CreationDate="2017-05-16T18:16:13.057" UserId="-1" />
  <row Id="13" PostHistoryTypeId="2" PostId="7" RevisionGUID="bd1f7a38-534c-4760-916f-6c07030e74de" CreationDate="2017-05-16T18:16:13.057" UserId="-1" Text="" />
  <row Id="14" PostHistoryTypeId="1" PostId="7" RevisionGUID="bd1f7a38-534c-4760-916f-6c07030e74de" CreationDate="2017-05-16T18:16:13.057" UserId="-1" />
  <row Id="15" PostHistoryTypeId="5" PostId="5" RevisionGUID="93cd3057-0705-4a11-8ea1-fe62f1b4bd07" CreationDate="2017-05-16T18:19:35.360" UserId="43" Comment="added 25 characters in body; edited title" Text="[The Human Genome Project][1] was the project of 'determining the sequence of nucleotide base pairs that make up human DNA, and of identifying and mapping all of the genes of the human genome'. It was completed in 2003.&#xD;&#xA;&#xD;&#xA;Are the datasets provided by HGP still accurate? Or given the technology in the past (such as using old techniques), or any other reason (newer research studies), the datasets could be not accurate?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Human_Genome_Project" />
  <row Id="16" PostHistoryTypeId="4" PostId="5" RevisionGUID="93cd3057-0705-4a11-8ea1-fe62f1b4bd07" CreationDate="2017-05-16T18:19:35.360" UserId="43" Comment="added 25 characters in body; edited title" Text="How accurate are human DNA datasets sequenced by Human Genome Project?" />
  <row Id="17" PostHistoryTypeId="5" PostId="2" RevisionGUID="41de5071-4f2a-45de-b066-85c9d402c375" CreationDate="2017-05-16T18:19:38.200" UserId="62" Comment="added compression note" Text="In terms of raw storage capacity 2 bits per nucleotide, and then further compressed with standard compression techniques would be the most efficient. However, you'd still have other storage considerations. Like what to do about non-standard bases: like if you want to indicate a gap or ambiguity. &#xD;&#xA;&#xD;&#xA;I'd also query if it is really necessary to store them as binary since it reduces the readability of the data. It is quite convenient having a whole bunch of unix and programming tools that can operate on the string level in text files." />
  <row Id="18" PostHistoryTypeId="5" PostId="3" RevisionGUID="7175b1da-50c6-4470-ba67-85243cdbc7af" CreationDate="2017-05-16T18:21:25.940" UserId="9" Comment="deleted 15 characters in body" Text="Genomes are commonly stored as either fasta files (.fa) or twoBit (.2bit) files. Fasta files store the entire sequence as text and are thus not particularly compressed. &#xD;&#xA;&#xD;&#xA;twoBit files store each nucleotide in two bits and contain additional metadata that indicates where there's regions containing `N` (unknown) bases.&#xD;&#xA;&#xD;&#xA;For more information, see the documentation on the twoBit format at the [UCSC genome browser](http://genome.ucsc.edu/FAQ/FAQformat.html#format7).&#xD;&#xA;&#xD;&#xA;You can convert between twoBit and fasta format using the [faToTwoBit and twoBitToFa utilities](https://genome.ucsc.edu/goldenpath/help/twoBit.html).&#xD;&#xA;&#xD;&#xA;For the human genome, you can download it in either fasta or twoBit format here: http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/&#xD;&#xA;&#xD;&#xA;" />
  <row Id="22" PostHistoryTypeId="2" PostId="9" RevisionGUID="ffd7a354-33d2-419d-b5e6-d1932ce759bd" CreationDate="2017-05-16T18:28:49.047" UserId="59" Text="I'm interested working with the medication information provided by the [UK Biobank](http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20003).  In order to get these into a usable form I would like to map them to [ATC codes](https://en.wikipedia.org/wiki/Anatomical_Therapeutic_Chemical_Classification_System).  Since many of the drugs listed in the data showcase include dosage information, doing an exact string match between drug names is not very effective.  I've considered using something like [fuzzywuzzy](https://pypi.python.org/pypi/fuzzywuzzy) to do string matching between the medications in the data showcase and the ATC drug names but validating the matches could still be a laborious process.  Does anyone know of a tool that can match drug names to ATC codes or some other drug ontology?  If not, maybe there's a better way to do it that I haven't thought of." />
  <row Id="23" PostHistoryTypeId="1" PostId="9" RevisionGUID="ffd7a354-33d2-419d-b5e6-d1932ce759bd" CreationDate="2017-05-16T18:28:49.047" UserId="59" Text="Mapping drug names to ATC codes" />
  <row Id="24" PostHistoryTypeId="3" PostId="9" RevisionGUID="ffd7a354-33d2-419d-b5e6-d1932ce759bd" CreationDate="2017-05-16T18:28:49.047" UserId="59" Text="&lt;atc&gt;&lt;ukbiobank&gt;&lt;drugs&gt;&lt;ontology&gt;&lt;nlp&gt;" />
  <row Id="29" PostHistoryTypeId="2" PostId="11" RevisionGUID="bb07e65c-8e8c-4451-aab0-9c3a1f3347c4" CreationDate="2017-05-16T18:31:11.587" UserId="44" Text="There are several things to consider when asking for &quot;the most efficient&quot; way to store data, it all depends on your use case. Do you just need ACGT, or are there also IUPAC codings for combinations? Do you need additional data (like quality values)? What kind of application are you using the data for (does it need to load all at once or in in chunks? Once or multiple times? Sequential or random access? etc.pp)?&#xD;&#xA;&#xD;&#xA;E.g., most efficient for:&#xD;&#xA;&#xD;&#xA; 1. Lowest footprint on disk, without a lot of hassle: use either FASTA or 2bit, but run through standard compressor (gzip, bzip2, others). The literature you want to consult here is that of standard text compression I think. Also of interest [Large Text Compression Benchmark][1]&#xD;&#xA; 2. Keeping the file on disk, but ultra-fast loading small subsets into memory, being able to work in memory with character sized entities: a simple dump of the DNA as characters to disk, maybe combined with an index file to know which chromosome starts where. Then use [mmap][2]&#xD;&#xA; 3. Storing quality values: See papers like [Compression of FASTQ and SAM Format Sequencing Data][3] or [Sequence squeeze: an open contest for sequence compression][4]&#xD;&#xA; 4. Any combination of the above use cases + a lot more&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://mattmahoney.net/dc/text.html&#xD;&#xA;  [2]: http://pubs.opengroup.org/onlinepubs/009695399/functions/mmap.html&#xD;&#xA;  [3]: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0059190&#xD;&#xA;  [4]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3637481/" />
  <row Id="30" PostHistoryTypeId="2" PostId="12" RevisionGUID="7919babf-1ddf-4b03-8d34-857a74db89f2" CreationDate="2017-05-16T18:31:28.570" UserId="65" Text="I'm looking to dock a large ligand (~90kDa) to a receptor slightly larger receptor  (~125kDa) using Hex. If anyone is familiar with docking large structures, are there any recommended parameters for finding the best docking solution?&#xD;&#xA;&#xD;&#xA;Parameters in particular:&#xD;&#xA;Number of Solutions &#xD;&#xA;N order of correlation for initial and final searches&#xD;&#xA;Receptor Range&#xD;&#xA;Ligand Range&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Thanks!" />
  <row Id="31" PostHistoryTypeId="1" PostId="12" RevisionGUID="7919babf-1ddf-4b03-8d34-857a74db89f2" CreationDate="2017-05-16T18:31:28.570" UserId="65" Text="Anyone familiar with Hex docking software?" />
  <row Id="32" PostHistoryTypeId="3" PostId="12" RevisionGUID="7919babf-1ddf-4b03-8d34-857a74db89f2" CreationDate="2017-05-16T18:31:28.570" UserId="65" Text="&lt;proteins&gt;" />
  <row Id="33" PostHistoryTypeId="2" PostId="13" RevisionGUID="e2100677-48c7-482a-91a8-c8719974ca40" CreationDate="2017-05-16T18:37:07.157" UserId="8" Text="The HGP developed the first &quot;reference&quot; human genome - a genome that other genomes could be compared to, and was actually a composite of multiple human genome sequences. &#xD;&#xA;&#xD;&#xA;The standard human reference genome is actually continually updated with major and minor revisions, a bit like software. The [latest major version](https://www.ncbi.nlm.nih.gov/grc/human) is called GRCh38, was released in 2013, and has since had a number of minor updates. &#xD;&#xA;&#xD;&#xA;&gt;Are the datasets provided by HGP still accurate?&#xD;&#xA;&#xD;&#xA;Yes, in a sense, but we certainly have better information now. One way to measure the quality of the assembly is that the initial release from the HGP [had hundreds of thousands](https://www.nature.com/nmeth/journal/v7/n5/full/nmeth0510-331.html) of gaps - sequences that could not be resolved (this often occurs because of repetitive sequences). The newest reference genome has less than 500 gaps.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="34" PostHistoryTypeId="2" PostId="14" RevisionGUID="529a8bf7-073c-4db5-af20-d28be134d822" CreationDate="2017-05-16T18:37:27.773" UserId="43" Text="I'd like to learn the differences between 3 common formats such as [FASTA][1], [FASTQ][2] and [SAM][3]. How they are different? Are there any benefits of using one over another?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/FASTA_format&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/FASTQ_format&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/SAM_(file_format)" />
  <row Id="35" PostHistoryTypeId="1" PostId="14" RevisionGUID="529a8bf7-073c-4db5-af20-d28be134d822" CreationDate="2017-05-16T18:37:27.773" UserId="43" Text="What is the difference between FASTA, FASTQ, and SAM file formats?" />
  <row Id="36" PostHistoryTypeId="3" PostId="14" RevisionGUID="529a8bf7-073c-4db5-af20-d28be134d822" CreationDate="2017-05-16T18:37:27.773" UserId="43" Text="&lt;fasta&gt;&lt;fastq&gt;&lt;sam&gt;" />
  <row Id="37" PostHistoryTypeId="5" PostId="7" RevisionGUID="d63e5230-dbe5-48d6-bfa4-6f9fefe719a5" CreationDate="2017-05-16T18:41:13.577" UserId="43" Comment="added 114 characters in body" Text="The Human Genome Project was a scientific research project with the goal of determining the sequence of human DNA." />
  <row Id="38" PostHistoryTypeId="24" PostId="7" RevisionGUID="d63e5230-dbe5-48d6-bfa4-6f9fefe719a5" CreationDate="2017-05-16T18:41:13.577" Comment="Proposed by 43 approved by 55 edit id of 1" />
  <row Id="39" PostHistoryTypeId="2" PostId="15" RevisionGUID="0e438ab3-3352-4bc9-be0f-442a078ab5d7" CreationDate="2017-05-16T18:41:23.147" UserId="88" Text="Many of my colleagues recommend I use BWA mem instead of regular old BWA. The problem is I don't understand why and reading the BWA man page doesn't seem to help the matter.&#xD;&#xA;&#xD;&#xA;What is the difference between BWA and BWA  mem? And, in which instances would you employ one over the other?" />
  <row Id="40" PostHistoryTypeId="1" PostId="15" RevisionGUID="0e438ab3-3352-4bc9-be0f-442a078ab5d7" CreationDate="2017-05-16T18:41:23.147" UserId="88" Text="Difference between BWA and BWA men" />
  <row Id="41" PostHistoryTypeId="3" PostId="15" RevisionGUID="0e438ab3-3352-4bc9-be0f-442a078ab5d7" CreationDate="2017-05-16T18:41:23.147" UserId="88" Text="&lt;fastq&gt;" />
  <row Id="42" PostHistoryTypeId="2" PostId="16" RevisionGUID="286e36f5-ee46-43d6-b5e4-3731b1b85263" CreationDate="2017-05-16T18:46:51.883" UserId="47" Text="It is not yet standardized, but [graph format](https://www.sevenbridges.com/graph/better-reference/) has the potential for being the most space-efficient method for storing genomes.  The idea is this: rather than store a genome as a linear string of sequenced nucleotides, genomes are stored as overlapping graphs, where sequence variants branch off from the reference genome, and then rejoin when the alignment continues.  Basically, you start with a reference genome, and for every subsequent genome added to the graph, only the differences are stored.  This could allow for an enormous gain in space efficiency. " />
  <row Id="43" PostHistoryTypeId="2" PostId="17" RevisionGUID="8d20fc6d-9fe2-4bd3-85a7-72eaf998c992" CreationDate="2017-05-16T18:53:38.760" UserId="44" Text="Incidentally, the first part of your question is something you could have looked up yourself as the first hits on Google of &quot;NAME format&quot; point you to primers on Wikipedia, no less. In future, please do that before asking a question.&#xD;&#xA;&#xD;&#xA; 1. [FASTA][1]&#xD;&#xA; 2. [FASTQ][2]&#xD;&#xA; 3. [SAM][3]&#xD;&#xA;&#xD;&#xA;FASTA (officially) just stores the name of a sequence and the sequence, inofficially people also add comment fields after the name of the sequence. FASTQ was invented to store both sequence and associated quality values (e.g. from sequencing instruments). SAM was invented to store alignments of (small) sequences (e.g. generated from sequencing) with associated quality values and some further data onto a larger sequences, called reference sequences, the latter being anything from a tiny virus sequence to ultra-large plant sequences.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/FASTA_format&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/FASTQ_format&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/SAM_(file_format)" />
  <row Id="44" PostHistoryTypeId="2" PostId="18" RevisionGUID="7d17f622-23da-4b86-a87e-55eeac4a632c" CreationDate="2017-05-16T18:57:28.007" UserId="27" Text="In a nutshell, &#xD;&#xA;&#xD;&#xA;`FASTA` file format is a DNA sequence format for specifying or representing DNA sequences and was first described by Pearson `(Pearson,W.R. and Lipman,D.J. (1988) Improved tools for biological sequence comparison. Proc. Natl Acad. Sci. USA, 85, 2444–2448)` &#xD;&#xA;&#xD;&#xA;`FASTQ` is another DNA sequence file format that extends the FASTA format with the ability to store the sequence quality. The quality scores are often represented in  ASCII characters which correspond to a phred score)&#xD;&#xA;&#xD;&#xA;Both FASTA and FASTQ are common sequence representation formats and have emerged as key data interchange formats for molecular biology and bioinformatics. &#xD;&#xA;&#xD;&#xA;`SAM` is format for representing sequence alignment information from a read aligner. It represents sequence information in respect to a given reference sequence. The information is stored in a series of tab delimited ascii columns. The full SAM format specification is available at [http://samtools.sourceforge.net/SAM1.pdf][1] &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://samtools.sourceforge.net/SAM1.pdf" />
  <row Id="47" PostHistoryTypeId="2" PostId="19" RevisionGUID="06465ad8-deb7-4e41-b784-a2cb10aadcd0" CreationDate="2017-05-16T19:15:57.957" UserId="96" Text="A common bioinformatics task is to decompose a DNA sequence into its constituent k-mers and compute a hash value for each k-mer. [Rolling hash functions](https://en.wikipedia.org/wiki/Rolling_hash) are an appealing solution for this task, since they can be computed very quickly. A rolling hash does not compute each the hash value from scratch with each k-mer: rather it updates a running hash value using an update strategy and a sliding window over the data.&#xD;&#xA;&#xD;&#xA;It's also very useful for many applications to have a k-mer hash to the same value as its reverse complement. Unless the data were generated using a strand-specific sample prep, it's impossible to distinguish a k-mer from its reverse complement, and they should be treated as the same sequence.&#xD;&#xA;&#xD;&#xA;Are there any rolling hashes that will map reverse complements to the same value? If not, how would we develop such an algorithm?" />
  <row Id="48" PostHistoryTypeId="1" PostId="19" RevisionGUID="06465ad8-deb7-4e41-b784-a2cb10aadcd0" CreationDate="2017-05-16T19:15:57.957" UserId="96" Text="Are there any rolling hash functions that can hash a DNA sequence and its reverse complement to the same value?" />
  <row Id="49" PostHistoryTypeId="3" PostId="19" RevisionGUID="06465ad8-deb7-4e41-b784-a2cb10aadcd0" CreationDate="2017-05-16T19:15:57.957" UserId="96" Text="&lt;hashing&gt;&lt;k-mer&gt;&lt;algorithms&gt;" />
  <row Id="50" PostHistoryTypeId="2" PostId="20" RevisionGUID="e2380400-d479-43b9-a632-0dfd682f388c" CreationDate="2017-05-16T19:17:38.697" UserId="37" Text="The standard and the most common sequence format is FASTA for sure. You can compress it with a compressor. For the ~3GB human genome, gzip reduces the size to ~900MB, depending on the option in use.&#xD;&#xA;&#xD;&#xA;Another often used format is UCSC's 2-bit format. This format keeps each A/C/G/T with 2 bits. As I remember, it keeps non-A/C/G/T bases and lowercases in two separate lists. These lists basically tell you that bases between offset x and y are all &quot;N&quot;/lowercase. The 2-bit format loses IUB codes that GRCh37 has. UCSC's hg19 differs from GRCh37 at a few bases.&#xD;&#xA;&#xD;&#xA;BWA also produces its own 2-bit format with indexing. You can generate it separately with:&#xD;&#xA;&#xD;&#xA;    bwa fa2pac -f hg19.fa&#xD;&#xA;&#xD;&#xA;Unlike UCSC, BWA keeps all IUB codes but loses letter cases. BWA does not provide utilities to convert its 2-bit representation to FASTA, either.&#xD;&#xA;&#xD;&#xA;The 2-bit format typically reduces the file size down to 1/4 of its original size, unless there are too many scattered ambiguous bases. For human genome, you get a file ~784MB in size. You can compress it further with gzip, but that actually doesn't work well. A gzip'd 2-bit file is only ~5-10% smaller.&#xD;&#xA;&#xD;&#xA;If you want to achieve an even smaller file size, you can compress the BWT of 2-bit file. This gives you a ~633MB file:&#xD;&#xA;&#xD;&#xA;    bwa pac2bwtgen hg19.fa.pac tmp.bwt &amp;&amp; gzip tmp.bwt&#xD;&#xA;&#xD;&#xA;A bit-aware compression algorithm may achieve an even higher compression ratio. However, such BWT-based compression prevents you from extracting subsequences. It is probably of little use in practice." />
  <row Id="51" PostHistoryTypeId="2" PostId="21" RevisionGUID="e7db4a85-1d44-4c75-a180-1c1b4a7e8982" CreationDate="2017-05-16T19:24:16.303" UserId="82" Text="What are the actual differences between different annotation databases? &#xD;&#xA;&#xD;&#xA;My lab, for reasons still unknown to me, prefers Ensembl annotations (we're working with transcript/exon expression estimation), while some software ship with RefSeq annotations. Are there significant differences between them today, or are they, for all intents and purposes, interchangeable (e.g., are exon coordinates between RefSeq and Ensembl annotations interchangeable)?" />
  <row Id="52" PostHistoryTypeId="1" PostId="21" RevisionGUID="e7db4a85-1d44-4c75-a180-1c1b4a7e8982" CreationDate="2017-05-16T19:24:16.303" UserId="82" Text="Feature annotation: RefSeq vs Ensembl vs Gencode, what's the difference?" />
  <row Id="53" PostHistoryTypeId="3" PostId="21" RevisionGUID="e7db4a85-1d44-4c75-a180-1c1b4a7e8982" CreationDate="2017-05-16T19:24:16.303" UserId="82" Text="&lt;annotation&gt;&lt;ensembl&gt;&lt;refseq&gt;&lt;gencode&gt;" />
  <row Id="54" PostHistoryTypeId="4" PostId="12" RevisionGUID="1360f80a-ba07-472f-b0df-036af001ba4c" CreationDate="2017-05-16T19:29:09.583" UserId="109" Comment="Changed the title to be more specific. Add docking tag." Text="What are the optimal parameters for docking a large ligand using Hex?" />
  <row Id="55" PostHistoryTypeId="6" PostId="12" RevisionGUID="1360f80a-ba07-472f-b0df-036af001ba4c" CreationDate="2017-05-16T19:29:09.583" UserId="109" Comment="Changed the title to be more specific. Add docking tag." Text="&lt;proteins&gt;&lt;docking&gt;" />
  <row Id="56" PostHistoryTypeId="24" PostId="12" RevisionGUID="1360f80a-ba07-472f-b0df-036af001ba4c" CreationDate="2017-05-16T19:29:09.583" Comment="Proposed by 109 approved by 55 edit id of 2" />
  <row Id="57" PostHistoryTypeId="2" PostId="22" RevisionGUID="afdd21a5-874b-45c6-9269-1b0eca5e080f" CreationDate="2017-05-16T19:33:06.700" UserId="112" Text="The [CART][1] tool let's you upload a set of names and map them (optionally in a fuzzy way) to STITCH 4 identifiers, and then use those to map to ATC codes (using the chemicals sources [download file][2]). It's a bit indirect, and I'm not sure what CART will do with the dosage info you mention.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cart.embl.de/&#xD;&#xA;  [2]: http://stitch4.embl.de/cgi/show_download_page.pl" />
  <row Id="58" PostHistoryTypeId="2" PostId="23" RevisionGUID="1d3db215-b5e9-479c-bb66-deffbcb0501d" CreationDate="2017-05-16T19:36:42.037" UserId="109" Text="[DrugBank][1] seems to have a [tool to map ATC codes to drug names][2] and DrugBank IDs.&#xD;&#xA;&#xD;&#xA;A quick look in the XSD schema on the [release page][3] suggests the complete database includes ATC codes for drugs, you could then do a fuzzy match of the BioBank names against all of DrugBank's synonyms, or match on some other data (e.g. canonicalised SMILES) if available.&#xD;&#xA;&#xD;&#xA;The downside is that there may not be complete overlap between UK BioBank and DrugBank. Additionally, DrugBank is under licence for commercial use.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.drugbank.ca&#xD;&#xA;  [2]: https://www.drugbank.ca/atc&#xD;&#xA;  [3]: https://www.drugbank.ca/releases/latest" />
  <row Id="60" PostHistoryTypeId="2" PostId="24" RevisionGUID="a0866701-6a77-4f08-8e1a-0247c0ed8a49" CreationDate="2017-05-16T19:40:38.183" UserId="37" Text="While the quality of the reference human assembly keeps improving, there are still misassemblies in it. A common problem is recent segmental duplications are occasionally collapsed into one sequence in the reference. Another issue is that the centromeric sequences in the reference are computationally generated, which are probably different from real sequences. Issues like these often complicate data analyses.&#xD;&#xA;&#xD;&#xA;You should also beware that each human has a different genome. A large region having one copy in the reference genome may have two copies in a specific sample. While this is not really the problem with the reference, such copy-number changes will have the same effect as reference errors and mess up your pipelines.&#xD;&#xA;&#xD;&#xA;There is still room for improvement to the human reference genome. In some regions, the CHM1 and CHM13 PacBio assemblies are better than the current reference genome at the larger scale. Illumina population data can produce better consensus at the single base level. GRC is continuously releasing new patches to the latest assembly." />
  <row Id="62" PostHistoryTypeId="2" PostId="25" RevisionGUID="c3532593-259a-4118-9ff4-152abfbf906d" CreationDate="2017-05-16T19:50:20.683" UserId="94" Text="FASTA and FATSQ formats are both file formats that contain sequencing reads while SAM files are these reads aligned to a reference sequence. In other words, FASTA and FASTQ are the &quot;raw data&quot; of sequencing while SAM is the product of aligning the sequencing reads to a refseq. &#xD;&#xA;&#xD;&#xA;A FASTA file contains a read name followed by the sequence. An example of one of these reads for RNASeq might be: &#xD;&#xA;&#xD;&#xA;    &gt;@Flow cell number: lane number: chip coordinates etc.&#xD;&#xA;    ATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTA&#xD;&#xA;&#xD;&#xA;The FASTQ version of this read will have two more lines, one + as a space holder and then a line of quality scores for the base calls. The qualities are given as characters with '!' being the lowest and '~' being the highest, in increasing ASCII value. It would look something like this&#xD;&#xA;     &#xD;&#xA;    &gt;@Flow cell number: lane number: chip coordinates etc.&#xD;&#xA;    ATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTA&#xD;&#xA;    +&#xD;&#xA;    !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65&#xD;&#xA;&#xD;&#xA;A SAM file has many fields for each alignment, the header begins with the @ character. The alignment contains 11 mandatory fields and various optional ones. You can find the spec file here: https://samtools.github.io/hts-specs/SAMv1.pdf .&#xD;&#xA;&#xD;&#xA;Often you'll see BAM files which are just compressed binary versions of SAM files. You can view these alignment files using various tools, such as SAMtools, IGV or USCS Genome browser. &#xD;&#xA;&#xD;&#xA;As to the benefits, FASTA/FASTQ vs. SAM/BAM is comparing apples and oranges. I do a lot of RNASeq work so generally we take the FASTQ files and align them the a refseq using an aligner such as STAR which outputs SAM/BAM files. There's a lot you can do with just these alignment files, looking at expression, but usually I'll use a tool such as RSEM to &quot;count&quot; the reads from various genes to create an expression matrix, samples as columns and genes as rows. Whether you get FASTQ or FASTA files just depends on your sequencing platform. I've never heard of anybody really using the quality scores. &#xD;&#xA;" />
  <row Id="63" PostHistoryTypeId="2" PostId="26" RevisionGUID="e379f29d-69a5-4950-a7cf-9daa265e22e7" CreationDate="2017-05-16T19:51:29.843" UserId="37" Text="Not sure if I understand your question. It seems trivial to me. You take two integers. When you see a new base, you push it to the right of the first integer and its complement to the left of the second integer. The hash of the k-mer is the smaller of the two integers." />
  <row Id="65" PostHistoryTypeId="2" PostId="27" RevisionGUID="4795e2f1-cc73-4971-8ca8-1914396ea9d8" CreationDate="2017-05-16T20:00:43.050" UserId="110" Text="I have a set of BAM files that are aligned using the NCBI GRCh37 human genome reference (with the chromosome names as NC_000001.10) but I want to analyze it using a BED file that has the UCSC hg19 chromosome names (e.g. chr1). I want to use bedtools to pull out all the on-target and off-target reads.&#xD;&#xA;&#xD;&#xA; 1. Are NCBI and UCSC directly comparable? Or do I need to re-align the BAM/lift-over the BED to the UCSC reference?&#xD;&#xA; 1. Should I convert the BED file or the BAM file? Everyone here uses the UCSC chromosome names/positions so I'll need to convert the eventual files to UCSC anyway." />
  <row Id="66" PostHistoryTypeId="1" PostId="27" RevisionGUID="4795e2f1-cc73-4971-8ca8-1914396ea9d8" CreationDate="2017-05-16T20:00:43.050" UserId="110" Text="Convert a BAM file from one reference to another?" />
  <row Id="67" PostHistoryTypeId="3" PostId="27" RevisionGUID="4795e2f1-cc73-4971-8ca8-1914396ea9d8" CreationDate="2017-05-16T20:00:43.050" UserId="110" Text="&lt;formats&gt;&lt;bam&gt;&lt;bed&gt;&lt;reference&gt;" />
  <row Id="68" PostHistoryTypeId="2" PostId="28" RevisionGUID="2ee65312-724c-4a39-ab8c-2b1ef0b9b15f" CreationDate="2017-05-16T20:17:30.960" UserId="44" Text="If your goal is to minimise storage by just having one hash per kmer and its reverse complement, there is a simple solution for non-rolling hashes. For any sequence S, you compute and store the hash of the smaller of S and its reverse complement. A simple lexicographical comparison for &quot;smaller&quot; is enough. In programming terms:&#xD;&#xA;&#xD;&#xA;    hash=computeHash(min(S,rev(S));&#xD;&#xA;&#xD;&#xA;If your goal is to minimise computing time via rolling hashes AND store only one kmer value, this will be hard. Hashes are normally designed to minimise collisions and you are asking it not only to collide, but also to collide in very, very specific circumstances.&#xD;&#xA;&#xD;&#xA;Question is: is the computation of a hash a bottleneck for an application? Maybe, maybe not.&#xD;&#xA;&#xD;&#xA;[This][1] post on StackExchange seems to imply that, in 2012, MurmurHash was able to compute de-novo a hash for a UUID (similar to a standard kmer in size and complexity) in ~250ns, i.e., ~4 million hashes per second. That's already quite fast.&#xD;&#xA;&#xD;&#xA;My guess is that most applications will use a hash to look up things in memory or disk. And here, even with look-ups in memory, real life data will let you run into CPU cache miss speed penalties very quickly and I expect the impact of this to be higher than the speed of the hash computation itself.&#xD;&#xA;&#xD;&#xA;  [1]: https://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed/145633#145633" />
  <row Id="69" PostHistoryTypeId="5" PostId="15" RevisionGUID="a1392d51-9700-4bb2-aa58-745b4cb9e1cb" CreationDate="2017-05-16T20:20:17.683" UserId="123" Comment="mem -&gt; MEM, better tags" Text="Many of my colleagues recommend I use BWA MEM instead of regular old BWA. The problem is I don't understand why and reading the BWA man page doesn't seem to help the matter.&#xD;&#xA;&#xD;&#xA;What is the difference between BWA and BWA MEM? And, in which instances would you employ one over the other?" />
  <row Id="70" PostHistoryTypeId="6" PostId="15" RevisionGUID="a1392d51-9700-4bb2-aa58-745b4cb9e1cb" CreationDate="2017-05-16T20:20:17.683" UserId="123" Comment="mem -&gt; MEM, better tags" Text="&lt;alignment&gt;&lt;reads&gt;&lt;mapping&gt;" />
  <row Id="71" PostHistoryTypeId="24" PostId="15" RevisionGUID="a1392d51-9700-4bb2-aa58-745b4cb9e1cb" CreationDate="2017-05-16T20:20:17.683" Comment="Proposed by 123 approved by 55 edit id of 3" />
  <row Id="72" PostHistoryTypeId="2" PostId="29" RevisionGUID="8d263ebd-ae11-4964-9b84-945ee2520384" CreationDate="2017-05-16T20:22:01.540" UserId="94" Text="What's the most widely accepted tool for doing pseudo-temporal ordering from scRNAseq data? Also is there away to separate differential expression that occurs based on &quot;cell identity&quot; or maybe more accurately cell type fate from that which arises from cells being in different stages differentiation.&#xD;&#xA;&#xD;&#xA;To be more concrete, lets say there's a population of cells, some of which were born at time 1, time 2, and time 3. The progression along the temporal trajectory can be described via a set of genes that are fluctuating as the cell matures. So you might have the same cell type which was born at time 1 be transcriptionally distinct from a younger one born at time 3.  On the other hand, within this population there are subpopulations which will have different cell fates and are transcriptionally distinct. Is there away to reliably separate the temporal axis from the &quot;cell fate axes&quot;. If not is this something people are working on or is flawed logic to think this kind of thing is possible?" />
  <row Id="73" PostHistoryTypeId="1" PostId="29" RevisionGUID="8d263ebd-ae11-4964-9b84-945ee2520384" CreationDate="2017-05-16T20:22:01.540" UserId="94" Text="Pseudo-temporal ordering in heterogeneous populations" />
  <row Id="74" PostHistoryTypeId="3" PostId="29" RevisionGUID="8d263ebd-ae11-4964-9b84-945ee2520384" CreationDate="2017-05-16T20:22:01.540" UserId="94" Text="&lt;scrnaseq&gt;" />
  <row Id="75" PostHistoryTypeId="5" PostId="26" RevisionGUID="48237591-3bcb-47c7-a84d-50c572902e40" CreationDate="2017-05-16T20:44:37.010" UserId="37" Comment="Added more details." Text="Not sure if I understand your question. It seems trivial to me. You take two integers. When you see a new base, you push it to the right of the first integer and its complement to the left of the second integer. The hash of the k-mer is the smaller of the two integers.&#xD;&#xA;&#xD;&#xA;More precisely, let `f` and `r` be two integers. They always keep the k-mer on the forward and reverse strand, respectively. At a new base `c` in a proper 2-bit encoding, we update the two integers as follows:&#xD;&#xA;&#xD;&#xA;    f = (f&lt;&lt;2|c) &amp; ((1ULL&lt;&lt;2*k) - 1)&#xD;&#xA;    r = r&gt;&gt;2 | (3ULL-c)&lt;&lt;2*(k-1)&#xD;&#xA;&#xD;&#xA;With this updating rule, `f` keeps the forward strand k-mer ending at `c` and `r` is `f`'s reverse complement. The hash of the k-mer can be `min(f,r)`. If integer operations take constant time, computing the hashes of all k-mers of a sequence of length L takes O(L) time. This is the most typical way to count canonical k-mers from a collection of sequences." />
  <row Id="76" PostHistoryTypeId="5" PostId="19" RevisionGUID="1d1ee708-55c6-4640-8232-c696a9bf9dd5" CreationDate="2017-05-16T20:50:34.373" UserId="96" Comment="lossy, double storage" Text="A common bioinformatics task is to decompose a DNA sequence into its constituent k-mers and compute a hash value for each k-mer. [Rolling hash functions](https://en.wikipedia.org/wiki/Rolling_hash) are an appealing solution for this task, since they can be computed very quickly. A rolling hash does not compute each the hash value from scratch with each k-mer: rather it updates a running hash value using an update strategy and a sliding window over the data.&#xD;&#xA;&#xD;&#xA;It's also very useful for many applications to have a k-mer hash to the same value as its reverse complement. Unless the data were generated using a strand-specific sample prep, it's impossible to distinguish a k-mer from its reverse complement, and they should be treated as the same sequence.&#xD;&#xA;&#xD;&#xA;Are there any rolling hashes that will map reverse complements to the same value? If not, how would we develop such an algorithm?&#xD;&#xA;&#xD;&#xA;**UPDATE**: Ideally the hash function would be able to support k &gt; 32, which would be lossy unless using something larger than a 64-bit integer.&#xD;&#xA;&#xD;&#xA;**ANOTHER UPDATE**: I don't think it's necessary to *store* both the running k-mer and its reverse complement in a single value. If storing two k-mer strings and/or two hash values makes this easier, I'm totally cool with that." />
  <row Id="77" PostHistoryTypeId="2" PostId="30" RevisionGUID="ed269707-8924-4780-8886-5bd1cfb05b8d" CreationDate="2017-05-16T20:52:13.140" UserId="-1" Text="" />
  <row Id="78" PostHistoryTypeId="1" PostId="30" RevisionGUID="ed269707-8924-4780-8886-5bd1cfb05b8d" CreationDate="2017-05-16T20:52:13.140" UserId="-1" />
  <row Id="79" PostHistoryTypeId="2" PostId="31" RevisionGUID="61f5d466-e4c3-4b37-af5e-fd3dfd00013b" CreationDate="2017-05-16T20:52:13.140" UserId="-1" Text="" />
  <row Id="80" PostHistoryTypeId="1" PostId="31" RevisionGUID="61f5d466-e4c3-4b37-af5e-fd3dfd00013b" CreationDate="2017-05-16T20:52:13.140" UserId="-1" />
  <row Id="81" PostHistoryTypeId="2" PostId="32" RevisionGUID="80b04106-23a6-4522-8a07-1a888d630b01" CreationDate="2017-05-16T21:04:42.503" UserId="77" Text="You're the second person I have ever seen using NCBI &quot;chromosome names&quot; (they're more like supercontig IDs). Normally I would point you to [a resource providing mappings between chromosome names](https://github.com/dpryan79/ChromosomeMappings), but since no one has added NCBI names (yet, maybe I'll add them now) you're currently out of luck there.&#xD;&#xA;&#xD;&#xA;Anyway, the quickest way to do what you want is to `samtools view -H foo.bam &gt; header` to get the BAM header and then change each NCBI &quot;chromosome name&quot; to its corresponding UCSC chromosome name. DO NOT REORDER THE LINES! You can then use `samtools reheader` and be done.&#xD;&#xA;&#xD;&#xA;Why, you might ask, would this work? The answer is that chromosome/contig names in BAM files aren't stored in each alignment. Rather, the names are stored in a list in the header and each alignment just contains the integer index into that list (read group IDs are similar, for what it's worth). This also leads to the warning above against reordering entries, since that's a VERY convenient way to start swapping alignments between chromosomes.&#xD;&#xA;&#xD;&#xA;As an aside, you'd be well served switching to Gencode or Ensembl chromosome names, they're rather more coherent than the `something_random` mess that's present in hg19 from UCSC." />
  <row Id="82" PostHistoryTypeId="5" PostId="32" RevisionGUID="b5812641-2afc-44be-8a72-c150fbf9ae38" CreationDate="2017-05-16T21:36:39.010" UserId="77" Comment="Link to the NCBI -&gt; UCSC name conversion" Text="You're the second person I have ever seen using NCBI &quot;chromosome names&quot; (they're more like supercontig IDs). Normally I would point you to [a resource providing mappings between chromosome names](https://github.com/dpryan79/ChromosomeMappings), but since no one has added NCBI names (yet, maybe I'll add them now) you're currently out of luck there.&#xD;&#xA;&#xD;&#xA;Anyway, the quickest way to do what you want is to `samtools view -H foo.bam &gt; header` to get the BAM header and then change each NCBI &quot;chromosome name&quot; to its corresponding UCSC chromosome name. DO NOT REORDER THE LINES! You can then use `samtools reheader` and be done.&#xD;&#xA;&#xD;&#xA;Why, you might ask, would this work? The answer is that chromosome/contig names in BAM files aren't stored in each alignment. Rather, the names are stored in a list in the header and each alignment just contains the integer index into that list (read group IDs are similar, for what it's worth). This also leads to the warning above against reordering entries, since that's a VERY convenient way to start swapping alignments between chromosomes.&#xD;&#xA;&#xD;&#xA;As an aside, you'd be well served switching to Gencode or Ensembl chromosome names, they're rather more coherent than the `something_random` mess that's present in hg19 from UCSC.&#xD;&#xA;&#xD;&#xA;**Update**: Because I'm nice, [here](https://github.com/dpryan79/ChromosomeMappings/blob/master/GRCh37_NCBI2UCSC.txt) is the conversion between NCBI and UCSC. Note that not if you have any alignments to patches that there is simply no UCSC equivalent." />
  <row Id="86" PostHistoryTypeId="5" PostId="32" RevisionGUID="0e35d63b-549f-435b-bad7-bde1c94ae3a5" CreationDate="2017-05-16T21:46:53.327" UserId="77" Comment="spelling" Text="You're the second person I have ever seen using NCBI &quot;chromosome names&quot; (they're more like supercontig IDs). Normally I would point you to [a resource providing mappings between chromosome names](https://github.com/dpryan79/ChromosomeMappings), but since no one has added NCBI names (yet, maybe I'll add them now) you're currently out of luck there.&#xD;&#xA;&#xD;&#xA;Anyway, the quickest way to do what you want is to `samtools view -H foo.bam &gt; header` to get the BAM header and then change each NCBI &quot;chromosome name&quot; to its corresponding UCSC chromosome name. DO NOT REORDER THE LINES! You can then use `samtools reheader` and be done.&#xD;&#xA;&#xD;&#xA;Why, you might ask, would this work? The answer is that chromosome/contig names in BAM files aren't stored in each alignment. Rather, the names are stored in a list in the header and each alignment just contains the integer index into that list (read group IDs are similar, for what it's worth). This also leads to the warning above against reordering entries, since that's a VERY convenient way to start swapping alignments between chromosomes.&#xD;&#xA;&#xD;&#xA;As an aside, you'd be well served switching to Gencode or Ensembl chromosome names, they're rather more coherent than the `something_random` mess that's present in hg19 from UCSC.&#xD;&#xA;&#xD;&#xA;**Update**: Because I'm nice, [here](https://github.com/dpryan79/ChromosomeMappings/blob/master/GRCh37_NCBI2UCSC.txt) is the conversion between NCBI and UCSC. Note that if you have any alignments to patches that there is simply no UCSC equivalent. One of the many reasons not to use UCSC (avoid their annotations too)." />
  <row Id="89" PostHistoryTypeId="2" PostId="35" RevisionGUID="ebe8ca21-6c16-4529-98fc-b701d6426216" CreationDate="2017-05-16T22:49:39.000" UserId="149" Text="To quote the Introduction to `BWA` on [sourceforge](http://bio-bwa.sourceforge.net/):&#xD;&#xA;&#xD;&#xA;&gt; BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the rest two for longer sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads.&#xD;&#xA;&#xD;&#xA;In short, for anything where you have read lengths over 70bp BWA-MEM is faster, and more accurate." />
  <row Id="90" PostHistoryTypeId="5" PostId="26" RevisionGUID="2c79c13e-91d1-44ba-9ba2-06d3def3173b" CreationDate="2017-05-17T00:05:41.337" UserId="37" Comment="Responses to updates" Text="Not sure if I understand your question. It seems trivial to me. You take two integers. When you see a new base, you push it to the right of the first integer and its complement to the left of the second integer. The hash of the k-mer is the smaller of the two integers.&#xD;&#xA;&#xD;&#xA;More precisely, let `f` and `r` be two integers. They always keep the k-mer on the forward and reverse strand, respectively. At a new base `c` in a proper 2-bit encoding, we update the two integers as follows:&#xD;&#xA;&#xD;&#xA;    f = (f&lt;&lt;2|c) &amp; ((1ULL&lt;&lt;2*k) - 1)&#xD;&#xA;    r = r&gt;&gt;2 | (3ULL-c)&lt;&lt;2*(k-1)&#xD;&#xA;&#xD;&#xA;With this updating rule, `f` keeps the forward strand k-mer ending at `c` and `r` is `f`'s reverse complement. The hash of the k-mer can be `min(f,r)`. If integer operations take constant time, computing the hashes of all k-mers of a sequence of length L takes O(L) time. This is the most typical way to count canonical k-mers from a collection of sequences.&#xD;&#xA;&#xD;&#xA;RE correctness: this approach just moves along the sequence. It updates `f` and `r` for each coming base. It can &quot;compute all hashes for a genome sequence&quot;.&#xD;&#xA;&#xD;&#xA;RE Update to the original question on long k-mers: it is easy to extend the method to 64-mers with 4 integers, or 128-mers with 8 integers. Bit operations are very cheap anyway. In addition, supporting long k-mers for two strands is similar to one strand. If you can use a Rabin-Karp rolling hash for the forward strand, you can do the same to the reverse strand.&#xD;&#xA;&#xD;&#xA;RE Performance: 4 million hashes per second (or 12.5 minutes to hash the human genome) sound really slow to me. Traversing and hashing all k-mers in the human genome with a good enough randomization hash function should be done under one minute. Also note that for some k-mer operations, such as counting, the state of art is to sort the k-mers, not to put them into a hash table. A radix sort is perhaps as fast as k-mer hashing. If you choose a slow hashing algorithm, it may become the bottleneck." />
  <row Id="92" PostHistoryTypeId="2" PostId="36" RevisionGUID="121b690f-73b3-48a9-bcc2-ba67aed13423" CreationDate="2017-05-17T02:18:01.720" UserId="163" Text="I have run Oxford Nanopore Technologies' MinION sequencing on the same DNA sample using three flowcells, each aligned against the same reference genome (E.coli K12 MG1655) using both BWA MEM and GraphMap and stored as BAM files.&#xD;&#xA;&#xD;&#xA;How can I quantitatively and efficiently analyse the quality of alignment (percentage identity, insertion rate, deletion rate) of each of these files?" />
  <row Id="93" PostHistoryTypeId="1" PostId="36" RevisionGUID="121b690f-73b3-48a9-bcc2-ba67aed13423" CreationDate="2017-05-17T02:18:01.720" UserId="163" Text="Compare alignment quality of multiple sequencing runs aligned against the same reference genome?" />
  <row Id="94" PostHistoryTypeId="3" PostId="36" RevisionGUID="121b690f-73b3-48a9-bcc2-ba67aed13423" CreationDate="2017-05-17T02:18:01.720" UserId="163" Text="&lt;bam&gt;&lt;alignment&gt;&lt;nanopore&gt;" />
  <row Id="98" PostHistoryTypeId="4" PostId="36" RevisionGUID="7260ed4e-7a3a-4968-b64d-bb272f78228a" CreationDate="2017-05-17T02:23:22.180" UserId="163" Comment="no need for q mark in title" Text="Compare alignment quality of multiple sequencing runs aligned against the same reference genome" />
  <row Id="99" PostHistoryTypeId="2" PostId="38" RevisionGUID="a93858e5-837e-45f1-bd20-ab804eae3e59" CreationDate="2017-05-17T02:29:23.840" UserId="161" Text="## Ensembl vs Gencode&#xD;&#xA;&#xD;&#xA;https://www.gencodegenes.org/faq.html&#xD;&#xA;&#xD;&#xA;&gt; The GENCODE annotation is made by merging the Havana manual gene annotation and the Ensembl automated gene annotation. [...] In practical terms, the GENCODE annotation is identical to the Ensembl annotation.&#xD;&#xA;&#xD;&#xA;Further, for the GTF file differences:&#xD;&#xA;&#xD;&#xA;&gt; The only exception is that the genes which are common to the human chromosome X and Y PAR regions can be found twice in the GENCODE GTF, while they are shown only for chromosome X in the Ensembl file.&#xD;&#xA;&#xD;&#xA;## Gencode(Ensembl) vs RefSeq&#xD;&#xA;&#xD;&#xA;Gencode is in almost all cases [more comprehensive](https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-16-S8-S2). For example, this is NCBI RefSeq vs Ensembl (v24, release 83) for BRCA gene:&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;RefSeq and Gencode are not interchangeable in most cases, though RefSeq annotations will often be a subset of the Gencode ones.&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/MO5Gb.png&#xD;&#xA;" />
  <row Id="105" PostHistoryTypeId="2" PostId="42" RevisionGUID="e2280095-06cc-46ac-9e77-221678e01b37" CreationDate="2017-05-17T03:10:02.837" UserId="156" Text="Qualimap will do this for you. &#xD;&#xA;&#xD;&#xA;1. Go to qualimap.bioinfo.cipf.es&#xD;&#xA;2. Run qualimap (default params are fine) on each BAM file&#xD;&#xA;3. Open up the HTML output, and you can read off the %identity (they measure the opposite, i.e. mismatch rate, but 100% - mismatch rate is %identity of course), indel rate, etc.&#xD;&#xA;&#xD;&#xA;One thing to watch out for (you don't mention it in your question, but just in case) is that you cannot directly compare Q scores - these are a bit of a mess and calculated very differently in each piece of software. &#xD;&#xA;&#xD;&#xA;Unsolicited suggestion: you might also try LGM-LR for mapping MinION data. We've found it beats the others for our data (though we map to a distant reference).&#xD;&#xA;" />
  <row Id="106" PostHistoryTypeId="2" PostId="43" RevisionGUID="0bca3462-9343-4cd3-b8d5-484a5a763793" CreationDate="2017-05-17T03:20:00.923" UserId="113" Text="I think the question is a bit ambiguous so please excuse this answer that's a bit redundant from the rest of the ones provided.&#xD;&#xA;&#xD;&#xA;As others have mentioned, if you want to store a full genome, [`FASTA`](https://en.wikipedia.org/wiki/FASTA_format) and [`2bit`](https://genome.ucsc.edu/FAQ/FAQformat.html#format7) formats are appropriate.  For some context, [`hg19`](http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/) is about 900Mb compressed for the `FASTA` file and about 780Mb compressed for the `2bit` file .  `hg19` is a reference and is haploid so doesn't represent a &quot;full&quot; human genome that would normally have two alleles for the autosome (non-sex chromosomes).&#xD;&#xA;&#xD;&#xA;Common formats for representing a &quot;full&quot; human genome range from Variant Call Format ([`VCF`](https://vcftools.github.io/specs.html)) to General Feature Format ([`GFF`](https://en.wikipedia.org/wiki/General_feature_format)) among others which I'm sure I'm neglecting.  Both the `VCF` and `GFF` formats represent differences from a reference (`hg19`, say) that results in a &quot;whole genome&quot; file.  From personal experience, I've seen `VCF` and `GFF` files in the range of 100Mb, or more, compressed.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If you're considering just one &quot;whole genome&quot; in isolation, then the answer is pretty clear: `2bit` format is probably approaching the entropy limit of the human genome and you probably won't be able to do much better.&#xD;&#xA;The reason why your question is a bit ambiguous is that as soon as you start encoding more than one genome, a population of genomes, say, then you can start exploiting the redundancy of the genome as shared by the population.&#xD;&#xA;&#xD;&#xA;For example, say you want to store two &quot;whole genomes&quot;.  You could download the `hg19` reference and download two `GFF` files which would give around 1Gb worth of data (around 800Mb for the `2bit` file and around 200Mb for both of the `GFF` files).  Now you've been able to represent a &quot;whole genome&quot; in 500Mb instead of the 800Mb.  You can see a similar argument for downloading 3 `GFF` files and more.&#xD;&#xA;&#xD;&#xA;The minimum amount of information needed to represent a population of genomes is, as far as I know, unknown, but I would guess in the 2.5Mb-5Mb range.  For example, see [&quot;Human genomes as email attachments&quot; by Christley, Lu, Li and Xie](https://academic.oup.com/bioinformatics/article/25/2/274/218156/Human-genomes-as-email-attachments) which claims a 4Mb encoding of a genome.  &#xD;&#xA;&#xD;&#xA;Things get tricky because you have to ask what you're claiming as a &quot;whole genome&quot;.  `VCF` files are notoriously bad because older versions of the specification only store high quality differences from reference, throwing away high quality called sections.  If you want to store low quality information, the encoding is now going to depend on the sequencing technology in weird ways.&#xD;&#xA;&#xD;&#xA;Insertions, deletions, mobile insertion elements, copy number variants, other structural variants, etc. all complicate this matter further.  [Genome Graphs](http://biorxiv.org/content/early/2017/01/18/101378) are trying to tackle at least some of these problems but the focus is on variant calling rather than efficient individual whole genome representation, though perhaps can be adapted in the future.&#xD;&#xA;  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="107" PostHistoryTypeId="2" PostId="44" RevisionGUID="7e8c42e4-73b0-4d48-a57e-eb83690c30f2" CreationDate="2017-05-17T03:45:52.300" UserId="172" Text="While I have no experience with this *specific* question you have, according to Feinstein &amp; Brylinski ([2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4468813/)), a &quot;fully automated procedure&quot;, i.e. a pearl script, can be used to optimize the box size itself, and it can be found [here](http://brylinski.cct.lsu.edu/content/docking-box-size)." />
  <row Id="108" PostHistoryTypeId="5" PostId="44" RevisionGUID="9ce56eee-ccb1-421c-958e-634229aaf8b2" CreationDate="2017-05-17T04:01:09.563" UserId="172" Comment="Spelling" Text="While I have no experience with this *specific* question you have, according to Feinstein &amp; Brylinski ([2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4468813/)), a &quot;fully automated procedure&quot;, i.e. a Perl script, can be used to optimize the box size itself, and it can be found [here](http://brylinski.cct.lsu.edu/content/docking-box-size)." />
  <row Id="109" PostHistoryTypeId="2" PostId="45" RevisionGUID="bb6ca0d9-ccb6-4540-8fee-8678c9dfac5d" CreationDate="2017-05-17T04:38:52.420" UserId="163" Text="I have a ~10GB FASTA file generated from an Oxford Nanopore Technologies' MinION run. How can I quickly and efficiently calculate the distribution of read lengths?&#xD;&#xA;&#xD;&#xA;A naive approach would be to read the FASTA file in Biopython, check the length of each sequence, store the lengths in a numpy array and plot the results using matplotlib, but this seems like reinventing the wheel." />
  <row Id="110" PostHistoryTypeId="1" PostId="45" RevisionGUID="bb6ca0d9-ccb6-4540-8fee-8678c9dfac5d" CreationDate="2017-05-17T04:38:52.420" UserId="163" Text="Read length distribution from FASTA file" />
  <row Id="111" PostHistoryTypeId="3" PostId="45" RevisionGUID="bb6ca0d9-ccb6-4540-8fee-8678c9dfac5d" CreationDate="2017-05-17T04:38:52.420" UserId="163" Text="&lt;fasta&gt;&lt;nanopore&gt;" />
  <row Id="112" PostHistoryTypeId="2" PostId="46" RevisionGUID="508d396e-6d68-420e-8fb9-6b14460c9c28" CreationDate="2017-05-17T04:46:54.870" UserId="173" Text="I started working on bioinformatic (academic) project recently, which involves de-novo assembly. I came to know the terms `Transcriptome` and `Genome`.&#xD;&#xA;I know a transcriptome is the set of all messenger RNA molecules in a cell.&#xD;&#xA;&#xD;&#xA;But I cannot identify the difference between these two. I have a computer engineering background, not biology." />
  <row Id="113" PostHistoryTypeId="1" PostId="46" RevisionGUID="508d396e-6d68-420e-8fb9-6b14460c9c28" CreationDate="2017-05-17T04:46:54.870" UserId="173" Text="What is the difference between a transcriptome and a genome" />
  <row Id="114" PostHistoryTypeId="3" PostId="46" RevisionGUID="508d396e-6d68-420e-8fb9-6b14460c9c28" CreationDate="2017-05-17T04:46:54.870" UserId="173" Text="&lt;transcriptome&gt;&lt;genome&gt;&lt;assembly&gt;&lt;de-novo&gt;" />
  <row Id="115" PostHistoryTypeId="2" PostId="47" RevisionGUID="12bfbb36-c5d5-4e89-8456-78f30ce67753" CreationDate="2017-05-17T04:52:48.293" UserId="23" Text="In brief, the  `genome`  is the collection of all  DNA  present  in  the  nucleus  and  the  mitochondria of a  somatic  cell. The initial product of genome expression is the `transcriptome`, a collection of RNA molecules derived from those  protein-coding  genes." />
  <row Id="117" PostHistoryTypeId="2" PostId="49" RevisionGUID="88dcf759-b411-404b-9e5e-7309dde811e3" CreationDate="2017-05-17T05:09:23.507" UserId="150" Text="There are several potential approaches. For example:&#xD;&#xA;&#xD;&#xA; - [histogram of sequence lengths][1] in the Biopython tutorial&#xD;&#xA; - [plot_distribution][2] from the Ruby-based biopieces framework&#xD;&#xA; - various solutions to get sequence length [including bioawk][3] and [EMBOSS infoseq][4]&#xD;&#xA;&#xD;&#xA;As to which of these are &quot;quick and efficient&quot; using a 10 GB file...it's hard to say in advance. You may have to try and benchmark a few of them.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc295&#xD;&#xA;  [2]: https://github.com/maasha/biopieces/wiki/plot_distribution&#xD;&#xA;  [3]: https://gif.biotech.iastate.edu/calculate-sequence-lengths-fasta-file&#xD;&#xA;  [4]: https://www.biostars.org/p/118954/#119066" />
  <row Id="118" PostHistoryTypeId="2" PostId="50" RevisionGUID="a20c4fd6-415a-4963-8995-cd5f9c80c06f" CreationDate="2017-05-17T05:13:07.323" UserId="174" Text="They are two very different things. Your genome is a large section of about 3 billion DNA nucleotide bases. It has no concept of exon and introns.&#xD;&#xA;&#xD;&#xA;Transcriptome is a study of [transcriptions][1]. You have introns and exons. We can now talk about alternative splicing and gene expression.&#xD;&#xA;&#xD;&#xA;You can think your genome is like a cooking recipe. While it's good to have a good recipe, you can't do much if you don't use it for cooking. &#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Transcription_(biology)" />
  <row Id="119" PostHistoryTypeId="2" PostId="51" RevisionGUID="d98c1bfa-7e26-446b-aedd-dfa1459b682a" CreationDate="2017-05-17T05:19:48.690" UserId="174" Text="I want to focus on transcriptome analysis. It's possible to analyze based on alignment or k-mers.&#xD;&#xA;&#xD;&#xA;Possible alignment workflow:&#xD;&#xA;&#xD;&#xA; - Align sequence reads with [TopHat2][1]&#xD;&#xA; - Quantify the gene expression with [Cufflinks][2]&#xD;&#xA;&#xD;&#xA;Possible reference-free workflow:&#xD;&#xA;&#xD;&#xA; - Quantify sequence reads with [Kallisto][3] reference-free&#xD;&#xA;&#xD;&#xA;**Q:** What are pros and cons for each of the approach?&#xD;&#xA;&#xD;&#xA;  [1]: https://ccb.jhu.edu/software/tophat/index.shtml&#xD;&#xA;  [2]: http://cole-trapnell-lab.github.io/cufflinks/install/&#xD;&#xA;  [3]: https://pachterlab.github.io/kallisto/" />
  <row Id="120" PostHistoryTypeId="1" PostId="51" RevisionGUID="d98c1bfa-7e26-446b-aedd-dfa1459b682a" CreationDate="2017-05-17T05:19:48.690" UserId="174" Text="Alignment based vs reference-free (transcriptome analysis)?" />
  <row Id="121" PostHistoryTypeId="3" PostId="51" RevisionGUID="d98c1bfa-7e26-446b-aedd-dfa1459b682a" CreationDate="2017-05-17T05:19:48.690" UserId="174" Text="&lt;transcriptome&gt;&lt;rna-seq&gt;" />
  <row Id="123" PostHistoryTypeId="2" PostId="52" RevisionGUID="3f45f0da-3d53-4888-b02d-7c157e526003" CreationDate="2017-05-17T05:24:25.273" UserId="174" Text="[ERCC spike-in][1] is a set of synthetic controls developed for RNA-Seq. I'm interested in using it to normalize my RNA-Seq samples. In particular, I'd like to use the spike-ins to remove technical bias and anything variation that should not be part of my analysis.&#xD;&#xA;&#xD;&#xA;The site doesn't give any details on how I can do that.&#xD;&#xA;&#xD;&#xA;**Q:** What are the possible normalization strategies? Can you briefly describe them?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.thermofisher.com/order/catalog/product/4456740" />
  <row Id="124" PostHistoryTypeId="1" PostId="52" RevisionGUID="3f45f0da-3d53-4888-b02d-7c157e526003" CreationDate="2017-05-17T05:24:25.273" UserId="174" Text="Normalization methods with RNA-Seq ERCC spike in?" />
  <row Id="125" PostHistoryTypeId="3" PostId="52" RevisionGUID="3f45f0da-3d53-4888-b02d-7c157e526003" CreationDate="2017-05-17T05:24:25.273" UserId="174" Text="&lt;transcriptome&gt;&lt;rna-seq&gt;" />
  <row Id="126" PostHistoryTypeId="5" PostId="51" RevisionGUID="195fb1f9-b0dd-41da-b285-ce520047486b" CreationDate="2017-05-17T05:25:26.637" UserId="174" Comment="added 76 characters in body" Text="I want to focus on transcriptome analysis. We know it's possible to analyze RNA-Seq experiment based on alignment or k-mers.&#xD;&#xA;&#xD;&#xA;Possible alignment workflow:&#xD;&#xA;&#xD;&#xA; - Align sequence reads with [TopHat2][1]&#xD;&#xA; - Quantify the gene expression with [Cufflinks][2]&#xD;&#xA;&#xD;&#xA;Possible reference-free workflow:&#xD;&#xA;&#xD;&#xA; - Quantify sequence reads with [Kallisto][3] reference-free&#xD;&#xA;&#xD;&#xA;Both strategy generate gene expression table.&#xD;&#xA;&#xD;&#xA;**Q:** What are pros and cons for each of the approach?&#xD;&#xA;&#xD;&#xA;  [1]: https://ccb.jhu.edu/software/tophat/index.shtml&#xD;&#xA;  [2]: http://cole-trapnell-lab.github.io/cufflinks/install/&#xD;&#xA;  [3]: https://pachterlab.github.io/kallisto/" />
  <row Id="127" PostHistoryTypeId="5" PostId="52" RevisionGUID="5b9a1090-9725-47ab-b7ea-abdb2852fff7" CreationDate="2017-05-17T05:31:31.560" UserId="174" Comment="deleted 5 characters in body" Text="[ERCC spike-in][1] is a set of synthetic controls developed for RNA-Seq. I'm interested in using it to normalize my RNA-Seq samples. In particular, I'd like to use the spike-ins to remove technical bias and any variation that should not be part of my analysis.&#xD;&#xA;&#xD;&#xA;The site doesn't give any details on how I can do that.&#xD;&#xA;&#xD;&#xA;**Q:** What are the possible normalization strategies? Can you briefly describe them?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.thermofisher.com/order/catalog/product/4456740" />
  <row Id="128" PostHistoryTypeId="5" PostId="51" RevisionGUID="335dc0c6-ca9e-40aa-b5a2-a13d114b4f13" CreationDate="2017-05-17T05:31:49.723" UserId="174" Comment="added 6 characters in body" Text="I want to focus on transcriptome analysis. We know it's possible to analyze RNA-Seq experiment based on alignment or k-mers.&#xD;&#xA;&#xD;&#xA;Possible alignment workflow:&#xD;&#xA;&#xD;&#xA; - Align sequence reads with [TopHat2][1]&#xD;&#xA; - Quantify the gene expression with [Cufflinks][2]&#xD;&#xA;&#xD;&#xA;Possible reference-free workflow:&#xD;&#xA;&#xD;&#xA; - Quantify sequence reads with [Kallisto][3] reference-free index&#xD;&#xA;&#xD;&#xA;Both strategy generate gene expression table.&#xD;&#xA;&#xD;&#xA;**Q:** What are pros and cons for each of the approach?&#xD;&#xA;&#xD;&#xA;  [1]: https://ccb.jhu.edu/software/tophat/index.shtml&#xD;&#xA;  [2]: http://cole-trapnell-lab.github.io/cufflinks/install/&#xD;&#xA;  [3]: https://pachterlab.github.io/kallisto/" />
  <row Id="132" PostHistoryTypeId="5" PostId="51" RevisionGUID="58fa6a36-ae38-4a56-bbaa-4d6b9318d92f" CreationDate="2017-05-17T05:40:44.417" UserId="174" Comment="added 24 characters in body" Text="I want to focus on transcriptome analysis. We know it's possible to analyze RNA-Seq experiment based on alignment or k-mers.&#xD;&#xA;&#xD;&#xA;Possible alignment workflow:&#xD;&#xA;&#xD;&#xA; - Align sequence reads with [TopHat2][1]&#xD;&#xA; - Quantify the gene expression with [Cufflinks][2]&#xD;&#xA;&#xD;&#xA;Possible reference-free workflow:&#xD;&#xA;&#xD;&#xA; - Quantify sequence reads with [Kallisto][3] reference-free index&#xD;&#xA;&#xD;&#xA;Both strategy generate gene expression table.&#xD;&#xA;&#xD;&#xA;**Q:** What are pros and cons for each of the approach? Can you give guideline?&#xD;&#xA;&#xD;&#xA;  [1]: https://ccb.jhu.edu/software/tophat/index.shtml&#xD;&#xA;  [2]: http://cole-trapnell-lab.github.io/cufflinks/install/&#xD;&#xA;  [3]: https://pachterlab.github.io/kallisto/" />
  <row Id="133" PostHistoryTypeId="2" PostId="54" RevisionGUID="b7ac205d-0774-4ade-b1a1-043d1623ae24" CreationDate="2017-05-17T05:43:40.693" UserId="163" Text="You may consider using [RUVSeq][1]. Here is an excerpt from the [2013 Nature publication][2]:&#xD;&#xA;&#xD;&#xA;&gt; We evaluate the performance of the External RNA Control Consortium (ERCC) spike-in controls and investigate the possibility of using them directly for normalization. We show that the spike-ins are not reliable enough to be used in standard global-scaling or regression-based normalization procedures. We propose a normalization strategy, called remove unwanted variation (RUV), that adjusts for nuisance technical effects by performing factor analysis on suitable sets of control genes (e.g., ERCC spike-ins) or samples (e.g., replicate libraries).&#xD;&#xA;&#xD;&#xA;RUVSeq essentially fits a generalized linear model (GLM) to the expression data, where your expression matrix `Y` is a `m` by `n` matrix, where `m` is the number of samples and `n` the number of rows. The model boils down to&#xD;&#xA;&#xD;&#xA;    Y = X*beta + Z*gamma + W*alpha + epsilon&#xD;&#xA;&#xD;&#xA;where `X` describes the conditions of interest (e.g., treatment vs. control), `Z` describes observed covariates (e.g., gender) and `W` describes unobserved covariates (e.g., batch, temperature, lab). `beta`, `gamma` and `alpha` are parameter matrices which record the contribution of `X`, `Z` and `W`, and epsilon is random noise. For subset of carefully selected genes (e.g., ERCC spike-ins, housekeeping genes, or technical replicates) we can assume that `X` and `Z` are zero, and find `W` - the &quot;unwanted variation&quot; in your sample. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/html/RUVSeq.html&#xD;&#xA;  [2]: http://www.nature.com/nbt/journal/v32/n9/full/nbt.2931.html" />
  <row Id="134" PostHistoryTypeId="5" PostId="54" RevisionGUID="2f33f715-e1ae-4fb1-8b0c-f79bd11972b3" CreationDate="2017-05-17T05:53:43.373" UserId="163" Comment="fix rows/genes typo" Text="You may consider using [RUVSeq][1]. Here is an excerpt from the [2013 Nature publication][2]:&#xD;&#xA;&#xD;&#xA;&gt; We evaluate the performance of the External RNA Control Consortium (ERCC) spike-in controls and investigate the possibility of using them directly for normalization. We show that the spike-ins are not reliable enough to be used in standard global-scaling or regression-based normalization procedures. We propose a normalization strategy, called remove unwanted variation (RUV), that adjusts for nuisance technical effects by performing factor analysis on suitable sets of control genes (e.g., ERCC spike-ins) or samples (e.g., replicate libraries).&#xD;&#xA;&#xD;&#xA;RUVSeq essentially fits a generalized linear model (GLM) to the expression data, where your expression matrix `Y` is a `m` by `n` matrix, where `m` is the number of samples and `n` the number of genes. The model boils down to&#xD;&#xA;&#xD;&#xA;    Y = X*beta + Z*gamma + W*alpha + epsilon&#xD;&#xA;&#xD;&#xA;where `X` describes the conditions of interest (e.g., treatment vs. control), `Z` describes observed covariates (e.g., gender) and `W` describes unobserved covariates (e.g., batch, temperature, lab). `beta`, `gamma` and `alpha` are parameter matrices which record the contribution of `X`, `Z` and `W`, and epsilon is random noise. For subset of carefully selected genes (e.g., ERCC spike-ins, housekeeping genes, or technical replicates) we can assume that `X` and `Z` are zero, and find `W` - the &quot;unwanted variation&quot; in your sample. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/html/RUVSeq.html&#xD;&#xA;  [2]: http://www.nature.com/nbt/journal/v32/n9/full/nbt.2931.html" />
  <row Id="137" PostHistoryTypeId="2" PostId="55" RevisionGUID="dda15799-eb89-41b7-863c-d0d1be76cb14" CreationDate="2017-05-17T06:29:37.357" UserId="179" Text="I wouldn't day [Kallisto][1] (or [Salmon][2]) is reference-free. The use a transcriptome as reference but use a concept called *pseudo-alignment* which greatly speed up the process of assigning your reads to a transcript.&#xD;&#xA;&#xD;&#xA;That said, both approaches of (i) mapping against a reference genome (what you called *alignment workflow* ) and (ii) mapping against a reference transcriptome will serve different purposes&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Transcriptome mapping using pseudoalignent is becoming the method of choice for gene/transcript quantification and differential expression analysis. The drawback is that you only focus on known transcripts&#xD;&#xA;&#xD;&#xA;Two typical workflows are:&#xD;&#xA;&#xD;&#xA;- Kallisto followed by [sleuth][3]&#xD;&#xA;- Salmon, followed by tximport and DESeq2/EdgeR&#xD;&#xA;&#xD;&#xA;Genome mapping is useful for, per example discovery of new isoforms. You shouldn't use TopHat anymore as it has been discontinued by the author.&#xD;&#xA;&#xD;&#xA;A typical workflow would be:&#xD;&#xA;&#xD;&#xA;- Hisat2 (alignment)&#xD;&#xA;- StringTie (transcript assembly and abundance estimation)&#xD;&#xA;- Ballgown (differential expression)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://pachterlab.github.io/kallisto/&#xD;&#xA;  [2]: http://salmon.readthedocs.io/en/latest/&#xD;&#xA;  [3]: http://pachterlab.github.io/sleuth/" />
  <row Id="141" PostHistoryTypeId="2" PostId="56" RevisionGUID="945fec39-8e34-4232-9821-8e6b80ec948e" CreationDate="2017-05-17T06:38:49.653" UserId="179" Text="It is not exactly what you asked, but you can generate a histogram of read length distribution of your nanopore data directly from the HDF5 files using [poretools][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://poretools.readthedocs.io/en/latest/" />
  <row Id="144" PostHistoryTypeId="2" PostId="57" RevisionGUID="3f7ebf25-8c40-43f3-ab49-2fb29d981c17" CreationDate="2017-05-17T07:03:57.863" UserId="48" Text="A popular framework to analyze differences between groups, either experiments or diseases, in transcriptomics is using linear models ([limma][1] is a popular choice). &#xD;&#xA;&#xD;&#xA;For instance we have a disease D with three stages as defined by clinicians, A, B and C. 10 samples each stage and the healthy H to compare with is RNA-sequenced. A typical linear model would be to observe the three stages`~A+B+C` independently. &#xD;&#xA;&#xD;&#xA;My understanding is that such a model would not take into account that stage C appears only on 30% of patients in stage B. And that a healthy patient upon external factors can jump to stage B. &#xD;&#xA;&#xD;&#xA;If we want to find the role of a gene in the disease we should include somehow this information in the model. Which makes me think about mixing linear models and hidden Markov chains.&#xD;&#xA;&#xD;&#xA;How can such a disease be described in terms of linear models with such data and information?&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/limma" />
  <row Id="145" PostHistoryTypeId="1" PostId="57" RevisionGUID="3f7ebf25-8c40-43f3-ab49-2fb29d981c17" CreationDate="2017-05-17T07:03:57.863" UserId="48" Text="Linear models of complex diseases" />
  <row Id="146" PostHistoryTypeId="3" PostId="57" RevisionGUID="3f7ebf25-8c40-43f3-ab49-2fb29d981c17" CreationDate="2017-05-17T07:03:57.863" UserId="48" Text="&lt;transcriptome&gt;&lt;disease-model&gt;" />
  <row Id="148" PostHistoryTypeId="2" PostId="58" RevisionGUID="f42b75c2-fd3d-4787-bc3b-1fcfc2cf4012" CreationDate="2017-05-17T07:32:20.717" UserId="40" Text="Using Biopython and matplotlib would seem like the way to go, indeed.&#xD;&#xA;It really just boils down to three lines of code to get that graph:&#xD;&#xA;&#xD;&#xA;    import Bio, pandas&#xD;&#xA;    lengths = map(len, Bio.SeqIO.parse('/path/to/the/seqs.fasta', 'fasta'))&#xD;&#xA;    pandas.Series(lengths).hist(color='gray', bins=1000)&#xD;&#xA;&#xD;&#xA;Of course you might want to make a longer script that's callable from the command line, with a couple options. You are welcome to use mine:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python2&#xD;&#xA;&#xD;&#xA;    &quot;&quot;&quot;&#xD;&#xA;    A custom made script to plot the distribution of lengths&#xD;&#xA;    in a fasta file.&#xD;&#xA;&#xD;&#xA;    Written by Lucas Sinclair.&#xD;&#xA;    Kopimi.&#xD;&#xA;&#xD;&#xA;    You can use this script from the shell like this:&#xD;&#xA;    $ ./fastq_length_hist --input seqs.fasta --out seqs.pdf&#xD;&#xA;    &quot;&quot;&quot;&#xD;&#xA;&#xD;&#xA;    ###############################################################################&#xD;&#xA;    # Modules #&#xD;&#xA;    import argparse, sys, time, getpass, locale&#xD;&#xA;    from argparse import RawTextHelpFormatter&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    import pandas&#xD;&#xA;&#xD;&#xA;    # Matplotlib #&#xD;&#xA;    import matplotlib&#xD;&#xA;    matplotlib.use('Agg', warn=False)&#xD;&#xA;    from matplotlib import pyplot&#xD;&#xA;&#xD;&#xA;    ################################################################################&#xD;&#xA;    desc = &quot;fasta_length_hist v1.0&quot;&#xD;&#xA;    parser = argparse.ArgumentParser(description=desc, formatter_class=RawTextHelpFormatter)&#xD;&#xA;&#xD;&#xA;    # All the required arguments #&#xD;&#xA;    parser.add_argument(&quot;--input&quot;, help=&quot;The fasta file to process&quot;, type=str)&#xD;&#xA;    parser.add_argument(&quot;--out&quot;, type=str)&#xD;&#xA;&#xD;&#xA;    # All the optional arguments #&#xD;&#xA;    parser.add_argument(&quot;--x_log&quot;, default=True, type=bool)&#xD;&#xA;    parser.add_argument(&quot;--y_log&quot;, default=True, type=bool)&#xD;&#xA;&#xD;&#xA;    # Parse it #&#xD;&#xA;    args        = parser.parse_args()&#xD;&#xA;    input_path  = args.input&#xD;&#xA;    output_path = args.out&#xD;&#xA;    x_log       = bool(args.x_log)&#xD;&#xA;    y_log       = bool(args.y_log)&#xD;&#xA;&#xD;&#xA;    ################################################################################&#xD;&#xA;    # Read #&#xD;&#xA;    lengths = map(len, SeqIO.parse(input_path, 'fasta'))&#xD;&#xA;&#xD;&#xA;    # Report #&#xD;&#xA;    sys.stderr.write(&quot;Read all lengths (%i sequences)\n&quot; % len(lengths))&#xD;&#xA;    sys.stderr.write(&quot;Longest sequence: %i bp\n&quot; % max(lengths))&#xD;&#xA;    sys.stderr.write(&quot;Shortest sequence: %i bp\n&quot; % min(lengths))&#xD;&#xA;    sys.stderr.write(&quot;Making graph...\n&quot;)&#xD;&#xA;&#xD;&#xA;    # Data #&#xD;&#xA;    values = pandas.Series(lengths)&#xD;&#xA;&#xD;&#xA;    # Plot #&#xD;&#xA;    fig   = pyplot.figure()&#xD;&#xA;    axes  = values.hist(color='gray', bins=1000)&#xD;&#xA;    fig   = pyplot.gcf()&#xD;&#xA;    title = 'Distribution of sequence lengths'&#xD;&#xA;    axes.set_title(title)&#xD;&#xA;    axes.set_xlabel('Number of nucleotides in sequence')&#xD;&#xA;    axes.set_ylabel('Number of sequences with this length')&#xD;&#xA;    axes.xaxis.grid(False)&#xD;&#xA;&#xD;&#xA;    # Log #&#xD;&#xA;    if x_log: axes.set_yscale('symlog')&#xD;&#xA;    if y_log: axes.set_xscale('symlog')&#xD;&#xA;&#xD;&#xA;    # Adjust #&#xD;&#xA;    width=18.0; height=10.0; bottom=0.1; top=0.93; left=0.07; right=0.98&#xD;&#xA;    fig.set_figwidth(width)&#xD;&#xA;    fig.set_figheight(height)&#xD;&#xA;    fig.subplots_adjust(hspace=0.0, bottom=bottom, top=top, left=left, right=right)&#xD;&#xA;&#xD;&#xA;    # Data and source #&#xD;&#xA;    fig.text(0.99, 0.98, time.asctime(), horizontalalignment='right')&#xD;&#xA;    fig.text(0.01, 0.98, 'user: ' + getpass.getuser(), horizontalalignment='left')&#xD;&#xA;&#xD;&#xA;    # Nice digit grouping #&#xD;&#xA;    sep = ('x','y')&#xD;&#xA;    if 'x' in sep:&#xD;&#xA;        locale.setlocale(locale.LC_ALL, '')&#xD;&#xA;        seperate = lambda x,pos: locale.format(&quot;%d&quot;, x, grouping=True)&#xD;&#xA;        axes.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(seperate))&#xD;&#xA;    if 'y' in sep:&#xD;&#xA;        locale.setlocale(locale.LC_ALL, '')&#xD;&#xA;        seperate = lambda x,pos: locale.format(&quot;%d&quot;, x, grouping=True)&#xD;&#xA;        axes.yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(seperate))&#xD;&#xA;&#xD;&#xA;    # Save it #&#xD;&#xA;    fig.savefig(output_path, format='pdf')&#xD;&#xA;" />
  <row Id="149" PostHistoryTypeId="2" PostId="59" RevisionGUID="02f5d0a6-92be-4381-820d-896e92054903" CreationDate="2017-05-17T07:58:46.987" UserId="182" Text="First of all, I would agree and emphasize that &quot;alignment-free&quot; quantification tools like Salmon and Kallisto are *not* reference-free. The basic difference between them and more traditional aligners is that they do not report a specific position (either in a genome or transcriptome) to which a read maps. However, their overall purpose is still to quantify the expression levels (or differences) of a known set of transcripts; hence, they require a reference (which could be arbitrarily defined).&#xD;&#xA;&#xD;&#xA;The most important criterion for deciding which approach to use (and this is true of almost everything in genomics) is exactly what question you would like to answer. If you are primarily interested in quantifying and comparing expression of mature mRNA from known transcripts, then a transcriptome-based alignment may be fastest and best. However, you may miss potentially interesting features outside of those known transcripts, such as new isoforms, non-coding RNAs, or information about pre-mRNA levels, which can often be gleaned from intronic reads (see the [EISA](https://www.nature.com/nbt/journal/v33/n7/full/nbt.3269.html) method).&#xD;&#xA;&#xD;&#xA;[This paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4729156/) also has some good considerations about which tools may work best depending on the question you want to answer.&#xD;&#xA;&#xD;&#xA;Finally, another fast and flexible aligner (which can be used with or without a reference transcriptome) is [STAR](https://github.com/alexdobin/STAR)." />
  <row Id="150" PostHistoryTypeId="2" PostId="60" RevisionGUID="1577c56c-513b-47e4-b9f5-e444f742d7b7" CreationDate="2017-05-17T07:58:51.323" UserId="-1" Text="" />
  <row Id="151" PostHistoryTypeId="1" PostId="60" RevisionGUID="1577c56c-513b-47e4-b9f5-e444f742d7b7" CreationDate="2017-05-17T07:58:51.323" UserId="-1" />
  <row Id="152" PostHistoryTypeId="2" PostId="61" RevisionGUID="f208df43-fe89-4196-b208-4d1aca3d8084" CreationDate="2017-05-17T07:58:51.323" UserId="-1" Text="" />
  <row Id="153" PostHistoryTypeId="1" PostId="61" RevisionGUID="f208df43-fe89-4196-b208-4d1aca3d8084" CreationDate="2017-05-17T07:58:51.323" UserId="-1" />
  <row Id="154" PostHistoryTypeId="5" PostId="55" RevisionGUID="c328dd2a-7bea-4761-b0d5-e0ea3df8e754" CreationDate="2017-05-17T08:11:35.430" UserId="179" Comment="edited body" Text="I wouldn't say [Kallisto][1] (or [Salmon][2]) are reference-free. They use a transcriptome as reference anda concept called *pseudo-alignment* which greatly speed up the process of assigning your reads to a transcript.&#xD;&#xA;&#xD;&#xA;That said, both approaches of (i) mapping against a reference genome (what you called *alignment workflow* ) and (ii) mapping against a reference transcriptome will serve different purposes&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Transcriptome mapping using pseudoalignent is becoming the method of choice for gene/transcript quantification and differential expression analysis. The drawback is that you only focus on known transcripts&#xD;&#xA;&#xD;&#xA;Two typical workflows are:&#xD;&#xA;&#xD;&#xA;- Kallisto followed by [sleuth][3]&#xD;&#xA;- Salmon, followed by tximport and DESeq2/EdgeR&#xD;&#xA;&#xD;&#xA;Genome mapping is useful for, per example discovery of new isoforms. You shouldn't use TopHat anymore as it has been discontinued by the author.&#xD;&#xA;&#xD;&#xA;A typical workflow would be:&#xD;&#xA;&#xD;&#xA;- Hisat2 (alignment)&#xD;&#xA;- StringTie (transcript assembly and abundance estimation)&#xD;&#xA;- Ballgown (differential expression)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://pachterlab.github.io/kallisto/&#xD;&#xA;  [2]: http://salmon.readthedocs.io/en/latest/&#xD;&#xA;  [3]: http://pachterlab.github.io/sleuth/" />
  <row Id="157" PostHistoryTypeId="2" PostId="62" RevisionGUID="2115ec94-0656-4526-9249-4083157cd8e3" CreationDate="2017-05-17T09:15:38.363" UserId="191" Text="I would like to run model BUSTED in HYPHY with the same settings on many sequence alignments, in a batch mode. What is the proper way of doing that?" />
  <row Id="158" PostHistoryTypeId="1" PostId="62" RevisionGUID="2115ec94-0656-4526-9249-4083157cd8e3" CreationDate="2017-05-17T09:15:38.363" UserId="191" Text="How to run HYPHY on multiple files?" />
  <row Id="159" PostHistoryTypeId="3" PostId="62" RevisionGUID="2115ec94-0656-4526-9249-4083157cd8e3" CreationDate="2017-05-17T09:15:38.363" UserId="191" Text="&lt;codon-models&gt;&lt;positive-selection&gt;&lt;hyphy&gt;&lt;busted&gt;" />
  <row Id="160" PostHistoryTypeId="2" PostId="63" RevisionGUID="5fbf2b74-4781-403d-bc06-226da9cd18ef" CreationDate="2017-05-17T09:15:38.363" UserId="191" Text="**TL;DR**&#xD;&#xA;&#xD;&#xA;I made the following bash script to generate an input file for HYPHY.&#xD;&#xA;&#xD;&#xA;    #!/bin/bash&#xD;&#xA;    cat &lt;&lt; EOF&#xD;&#xA;    inputRedirect = {};&#xD;&#xA;    inputRedirect[&quot;01&quot;]=&quot;Universal&quot;; // genetic code&#xD;&#xA;    inputRedirect[&quot;02&quot;]=&quot;$(readlink -f $1)&quot;; // codon data&#xD;&#xA;    inputRedirect[&quot;03&quot;]=&quot;$(readlink -f $2)&quot;; // tree&#xD;&#xA;    inputRedirect[&quot;04&quot;]=&quot;${3:-All}&quot;; // Test for selection on a branch&#xD;&#xA;    inputRedirect[&quot;05&quot;]=&quot;&quot;; // complete selection&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    ExecuteAFile (HYPHY_LIB_DIRECTORY+&quot;TemplateBatchFiles/BUSTED.bf&quot;, inputRedirect);&#xD;&#xA;    EOF&#xD;&#xA;&#xD;&#xA;If you want to test all the branches jointly run:&#xD;&#xA;&#xD;&#xA;    ./gen_busted.sh alignment.phy tree.nwk &gt; script.bf&#xD;&#xA;&#xD;&#xA;Or for a particular branch:&#xD;&#xA;&#xD;&#xA;    ./gen_busted.sh alignment.phy tree.nwk branchLabel &gt; script.bf&#xD;&#xA;&#xD;&#xA;And then run:&#xD;&#xA;&#xD;&#xA;    HYPHYMP script.bf&#xD;&#xA;    &#xD;&#xA;**Longer version**&#xD;&#xA;&#xD;&#xA;I will describe how to make a similar script for any model, such as MEME, RELAX or PARRIS.&#xD;&#xA;&#xD;&#xA;First, find the `.bf` script which performs the analysis. On GNU/Linux it should be at `HYPHY_INSTALLATION_PATH/lib/hyphy/TemplateBatchFiles` (it might be different between different OS). In case of BUSTED your script has a name `BUSTED.bf`.&#xD;&#xA;&#xD;&#xA;Now run this model in the interactive mode in order to record all the inputs required by the model. Make sure to specify **full paths** to all the files.&#xD;&#xA;&#xD;&#xA;    HYPHYMP HYPHY_INSTALLATION_PATH/lib/hyphy/TemplateBatchFiles/BUSTED.bf&#xD;&#xA;&#xD;&#xA;In case of BUSTED the inputs are:&#xD;&#xA;&#xD;&#xA;1. Genetic code.&#xD;&#xA;2. Codon data (sequence alignment in phylip format).&#xD;&#xA;3. Tree (in newick format).&#xD;&#xA;4. Which branches to test.&#xD;&#xA;&#xD;&#xA;Now for every input alignment you need to generate a batch file which looks like this:&#xD;&#xA;&#xD;&#xA;    inputRedirect = {};&#xD;&#xA;    inputRedirect[&quot;01&quot;]=&quot;Universal&quot;; // genetic code&#xD;&#xA;    inputRedirect[&quot;02&quot;]=&quot;/path/to/alignment.phy&quot;; // codon data&#xD;&#xA;    inputRedirect[&quot;03&quot;]=&quot;/path/to/tree.nwk&quot;; // tree&#xD;&#xA;    inputRedirect[&quot;04&quot;]=&quot;All&quot;; // Test for selection on all branches&#xD;&#xA;    inputRedirect[&quot;05&quot;]=&quot;BRANCH1&quot;; // Test for selection on branch1 &#xD;&#xA;    inputRedirect[&quot;06&quot;]=&quot;BRANCH2&quot;; // Test for selection on branch2 &#xD;&#xA;    inputRedirect[&quot;07&quot;]=&quot;&quot;; // complete selection&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    ExecuteAFile (HYPHY_LIB_DIRECTORY+&quot;TemplateBatchFiles/BUSTED.bf&quot;, inputRedirect);&#xD;&#xA;&#xD;&#xA;Now you can create a script which generates the file. The most important thing, don't forget to use full paths in the `.bf` file." />
  <row Id="161" PostHistoryTypeId="5" PostId="63" RevisionGUID="538ec500-71fa-4db3-aabb-a83963ee7178" CreationDate="2017-05-17T09:21:31.653" UserId="191" Comment="added 124 characters in body" Text="**TL;DR**&#xD;&#xA;&#xD;&#xA;I made the following bash script to generate an input file for HYPHY.&#xD;&#xA;&#xD;&#xA;    #!/bin/bash&#xD;&#xA;    cat &lt;&lt; EOF&#xD;&#xA;    inputRedirect = {};&#xD;&#xA;    inputRedirect[&quot;01&quot;]=&quot;Universal&quot;; // genetic code&#xD;&#xA;    inputRedirect[&quot;02&quot;]=&quot;$(readlink -f $1)&quot;; // codon data&#xD;&#xA;    inputRedirect[&quot;03&quot;]=&quot;$(readlink -f $2)&quot;; // tree&#xD;&#xA;    inputRedirect[&quot;04&quot;]=&quot;${3:-All}&quot;; // Test for selection on a branch&#xD;&#xA;    inputRedirect[&quot;05&quot;]=&quot;&quot;; // complete selection&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    ExecuteAFile (HYPHY_LIB_DIRECTORY+&quot;TemplateBatchFiles/BUSTED.bf&quot;, inputRedirect);&#xD;&#xA;    EOF&#xD;&#xA;&#xD;&#xA;If you want to test all the branches jointly run:&#xD;&#xA;&#xD;&#xA;    ./gen_busted.sh alignment.phy tree.nwk &gt; script.bf&#xD;&#xA;&#xD;&#xA;Or for a particular branch:&#xD;&#xA;&#xD;&#xA;    ./gen_busted.sh alignment.phy tree.nwk branchLabel &gt; script.bf&#xD;&#xA;&#xD;&#xA;And then run:&#xD;&#xA;&#xD;&#xA;    HYPHYMP script.bf&#xD;&#xA;    &#xD;&#xA;Now you can use the script to generate a `.bf` file for every sequence alignment you have, and run HYPHY for every file.&#xD;&#xA;&#xD;&#xA;**Longer version**&#xD;&#xA;&#xD;&#xA;I will describe how to make a similar script for any model, such as MEME, RELAX or PARRIS.&#xD;&#xA;&#xD;&#xA;First, find the `.bf` script which performs the analysis. On GNU/Linux it should be at `HYPHY_INSTALLATION_PATH/lib/hyphy/TemplateBatchFiles` (it might be different between different OS). In case of BUSTED your script has a name `BUSTED.bf`.&#xD;&#xA;&#xD;&#xA;Now run this model in the interactive mode in order to record all the inputs required by the model. Make sure to specify **full paths** to all the files.&#xD;&#xA;&#xD;&#xA;    HYPHYMP HYPHY_INSTALLATION_PATH/lib/hyphy/TemplateBatchFiles/BUSTED.bf&#xD;&#xA;&#xD;&#xA;In case of BUSTED the inputs are:&#xD;&#xA;&#xD;&#xA;1. Genetic code.&#xD;&#xA;2. Codon data (sequence alignment in phylip format).&#xD;&#xA;3. Tree (in newick format).&#xD;&#xA;4. Which branches to test.&#xD;&#xA;&#xD;&#xA;Now for every input alignment you need to generate a batch file which looks like this:&#xD;&#xA;&#xD;&#xA;    inputRedirect = {};&#xD;&#xA;    inputRedirect[&quot;01&quot;]=&quot;Universal&quot;; // genetic code&#xD;&#xA;    inputRedirect[&quot;02&quot;]=&quot;/path/to/alignment.phy&quot;; // codon data&#xD;&#xA;    inputRedirect[&quot;03&quot;]=&quot;/path/to/tree.nwk&quot;; // tree&#xD;&#xA;    inputRedirect[&quot;04&quot;]=&quot;All&quot;; // Test for selection on all branches&#xD;&#xA;    inputRedirect[&quot;05&quot;]=&quot;BRANCH1&quot;; // Test for selection on branch1 &#xD;&#xA;    inputRedirect[&quot;06&quot;]=&quot;BRANCH2&quot;; // Test for selection on branch2 &#xD;&#xA;    inputRedirect[&quot;07&quot;]=&quot;&quot;; // complete selection&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    ExecuteAFile (HYPHY_LIB_DIRECTORY+&quot;TemplateBatchFiles/BUSTED.bf&quot;, inputRedirect);&#xD;&#xA;&#xD;&#xA;Now you can create a script which generates the file. The most important thing, don't forget to use full paths in the `.bf` file." />
  <row Id="162" PostHistoryTypeId="4" PostId="62" RevisionGUID="d3fb6944-bdd0-4609-8216-eeff6c9875a2" CreationDate="2017-05-17T09:22:29.507" UserId="191" Comment="clarify the question" Text="How to run HYPHY on multiple files non-interactively?" />
  <row Id="163" PostHistoryTypeId="2" PostId="64" RevisionGUID="8ef1867b-2fc8-4c6c-b4cf-a11b1c92499f" CreationDate="2017-05-17T09:34:08.350" UserId="203" Text="To add to [rightskewed answer](https://bioinformatics.stackexchange.com/a/38/203):&#xD;&#xA;While it is true that:&#xD;&#xA;&#xD;&#xA;Gencode is an additive set of annotation (the manual one done by Havana and an automated one done by Ensembl), &#xD;&#xA;&#xD;&#xA;the annotation (GTF) files are quite similar for a few exceptions involving the X chromosome and Y par and additional remarks in the Gencode file (see more at [FAQ - Gencode](https://www.gencodegenes.org/faq.html)).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt;What are the actual differences between different annotation databases?&#xD;&#xA;&#xD;&#xA;They are a few differences, but the main one  **for me (and it could be stupid)** is &#xD;&#xA;&#xD;&#xA;that **Refseq is developed by the American NCBI** and &#xD;&#xA;&#xD;&#xA;the **ENSEMBL is mainly developed by the European EMBL-EBI.**&#xD;&#xA;&#xD;&#xA;Often, labs or people will just start using what is the best known to them (because of a course or workshop) or because they start working with one of the databases with one specific tool and keep with it later.&#xD;&#xA;&#xD;&#xA;&gt;My lab, for reasons still unknown to me, prefers Ensembl annotations (we're working with transcript/exon expression estimation), while some software ship with RefSeq annotations. &#xD;&#xA;&#xD;&#xA;Your lab might be mostly European based people or they might also have read papers like the one from Frankish et al.  Comparison of GENCODE and RefSeq gene annotation and the impact of reference geneset on variant effect prediction. BMC Genomics 2015; 16(Suppl 8):S2 - DOI: 10.1186/1471-2164-16-S8-S2&#xD;&#xA;&#xD;&#xA;From the [Frankish et al. paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-16-S8-S2) paper:&#xD;&#xA;&#xD;&#xA;&gt; The GENCODE Comprehensive transcripts contain more exons, have greater genomic coverage and capture many more variants than RefSeq in both genome and exome datasets, while the GENCODE Basic set shows a higher degree of concordance with RefSeq and has fewer unique features.&#xD;&#xA;  &#xD;&#xA;As for: &#xD;&#xA;&#xD;&#xA;&gt;Are there significant differences between them today, or are they, for all intents and purposes, interchangeable (e.g., are exon coordinates between RefSeq and Ensembl annotations interchangeable)?&#xD;&#xA;&#xD;&#xA;No. I don't think they are great differences between them as that the global picture should stay the same (although you will see different results if you are interested in a small set of genes). However, **they are not directly interchangeable**. Particularly as there are many versions of Ensembl and Refseq based on different genome annotations (and those won't be interchangeable between themselves either in most cases).&#xD;&#xA;&#xD;&#xA;**However, you can easily translate** most[1] of your Refseq IDs to ENSEMBL IDs and vice-versa with tools as http://www.ensembl.org/biomart/martview for example (there are devoted libraries/API as well like [Biocondutor: biomaRt](https://bioconductor.org/packages/release/bioc/html/biomaRt.html)&#xD;&#xA;&#xD;&#xA;[1] Most as sometimes, they might be annotated in one of the database but haven't (yet) an equivalent in the other." />
  <row Id="166" PostHistoryTypeId="5" PostId="47" RevisionGUID="9abc9d67-cd86-4a23-aa01-d32bc657e83f" CreationDate="2017-05-17T10:15:21.560" UserId="29" Comment="fix formatting" Text="In brief, the  “genome”  is the collection of all  DNA  present  in  the  nucleus  and  the  mitochondria of a  somatic  cell. The initial product of genome expression is the “transcriptome”, a collection of RNA molecules derived from those  protein-coding  genes." />
  <row Id="167" PostHistoryTypeId="24" PostId="47" RevisionGUID="9abc9d67-cd86-4a23-aa01-d32bc657e83f" CreationDate="2017-05-17T10:15:21.560" Comment="Proposed by 29 approved by 23 edit id of 12" />
  <row Id="168" PostHistoryTypeId="5" PostId="64" RevisionGUID="3420b5cf-c845-43a3-b8da-a1505ec5e2c7" CreationDate="2017-05-17T10:16:19.113" UserId="203" Comment="Zhao paper added to emphase that the choice of one database can have great impacts depending the research context" Text="To add to [rightskewed answer](https://bioinformatics.stackexchange.com/a/38/203):&#xD;&#xA;While it is true that:&#xD;&#xA;&#xD;&#xA;Gencode is an additive set of annotation (the manual one done by Havana and an automated one done by Ensembl), &#xD;&#xA;&#xD;&#xA;the annotation (GTF) files are quite similar for a few exceptions involving the X chromosome and Y par and additional remarks in the Gencode file (see more at [FAQ - Gencode](https://www.gencodegenes.org/faq.html)).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt;What are the actual differences between different annotation databases?&#xD;&#xA;&#xD;&#xA;They are a few differences, but the main one  **for me (and it could be stupid)** is &#xD;&#xA;&#xD;&#xA;that **Refseq is developed by the American NCBI** and &#xD;&#xA;&#xD;&#xA;the **ENSEMBL is mainly developed by the European EMBL-EBI.**&#xD;&#xA;&#xD;&#xA;Often, labs or people will just start using what is the best known to them (because of a course or workshop) or because they start working with one of the databases with one specific tool and keep with it later.&#xD;&#xA;&#xD;&#xA;&gt;My lab, for reasons still unknown to me, prefers Ensembl annotations (we're working with transcript/exon expression estimation), while some software ship with RefSeq annotations. &#xD;&#xA;&#xD;&#xA;Your lab might be mostly European based people or they might also have read papers like the one from Frankish et al.  Comparison of GENCODE and RefSeq gene annotation and the impact of reference geneset on variant effect prediction. BMC Genomics 2015; 16(Suppl 8):S2 - DOI: 10.1186/1471-2164-16-S8-S2&#xD;&#xA;&#xD;&#xA;From the [Frankish et al. paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-16-S8-S2) paper:&#xD;&#xA;&#xD;&#xA;&gt; The GENCODE Comprehensive transcripts contain more exons, have greater genomic coverage and capture many more variants than RefSeq in both genome and exome datasets, while the GENCODE Basic set shows a higher degree of concordance with RefSeq and has fewer unique features.&#xD;&#xA;  &#xD;&#xA;As for: &#xD;&#xA;&#xD;&#xA;&gt;Are there significant differences between them today, or are they, for all intents and purposes, interchangeable (e.g., are exon coordinates between RefSeq and Ensembl annotations interchangeable)?&#xD;&#xA;&#xD;&#xA;No. I don't think they are great differences between them as that the global picture should stay the same (although you will see different results if you are interested in a small set of genes). However, **they are not directly interchangeable**. Particularly as there are many versions of Ensembl and Refseq based on different genome annotations (and those won't be interchangeable between themselves either in most cases).&#xD;&#xA;&#xD;&#xA;**However, you can easily translate** most[1] of your Refseq IDs to ENSEMBL IDs and vice-versa with tools as http://www.ensembl.org/biomart/martview for example (there are devoted libraries/API as well like [Biocondutor: biomaRt](https://bioconductor.org/packages/release/bioc/html/biomaRt.html)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[1] Most as sometimes, they might be annotated in one of the database but haven't (yet) an equivalent in the other.&#xD;&#xA;&#xD;&#xA;**EDIT**&#xD;&#xA;&#xD;&#xA;In fine, even if people tends to keep to what they are used to (and that the annotations are constantly expanded and corrected) depending on the research subject one might be interested in using one database over another: &#xD;&#xA;&#xD;&#xA;From [Zhao S, Zhang B. A comprehensive evaluation of ensembl, RefSeq, and UCSC annotations in the context of RNA-seq read mapping and gene quantification. BMC Genomics. 2015;16: 97.](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-1308-8) paper:&#xD;&#xA;&#xD;&#xA;&gt; When choosing an annotation database, researchers should keep in mind that no database is perfect and some gene annotations might be inaccurate or entirely wrong. [..] Wu et al. [27] suggested that when conducting research that emphasizes reproducible and robust gene expression estimates, a less complex genome annotation, such as RefGene, might be preferred. When conducting more exploratory research, a more complex genome annotation, such as Ensembl, should be chosen. &#xD;&#xA;&#xD;&#xA;&gt;[..]&#xD;&#xA;&#xD;&#xA;&gt;[27] Wu P-Y, Phan JH, Wang MD. Assessing the impact of human genome annotation choice on RNA-seq expression estimates. BMC Bioinformatics. 2013;14(Suppl 11):S8. doi: 10.1186/1471-2105-14-S11-S8.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="169" PostHistoryTypeId="2" PostId="65" RevisionGUID="cc67bd9c-bfab-4688-afec-700d66a90682" CreationDate="2017-05-17T10:18:12.290" UserId="29" Text="At its easiest, you just store the *forward* (*F*) and *reverse* (*R*) hash value.&#xD;&#xA;&#xD;&#xA;You update the forward hash value by conventional means, e.g. bit-shifting the base value into its lower bits:&#xD;&#xA;&#xD;&#xA;*F*&lt;sub&gt;*n*+1&lt;/sub&gt; = ((*F*&lt;sub&gt;*n*&lt;/sub&gt; ≪ 1) | x) &amp; *M*,&#xD;&#xA;&#xD;&#xA;where *M* is the word mask for a word of length *W* bits, and can be omitted if the width of the hash is the width of the machine word; and the reverse hash value by doing the mathematical inverse, e.g. bit-shifting the value of the  reverse complement of the base into the upper bits:&#xD;&#xA;&#xD;&#xA;*R*&lt;sub&gt;*n*+1&lt;/sub&gt; = ((*R*&lt;sub&gt;*n*&lt;/sub&gt; ≫ 1) | (compl(*x*) ≪ (*W*−1))) &amp; *M*.&#xD;&#xA;&#xD;&#xA;And then you compute a *combined hash* value by xoring the two hashes, *H* = *F* ⊕ *R*." />
  <row Id="170" PostHistoryTypeId="2" PostId="66" RevisionGUID="905aee59-7f57-49a5-af18-de060aa47915" CreationDate="2017-05-17T10:28:32.203" UserId="191" Text="I have the following data:&#xD;&#xA;&#xD;&#xA;    &gt; str(expression)&#xD;&#xA;    'data.frame':	42412 obs. of  16 variables:&#xD;&#xA;     $ sample1 : int  4555 49 122 351 53 27 1 0 0 2513 ...&#xD;&#xA;     $ sample2 : int  2991 51 55 94 49 10 55 0 0 978 ...&#xD;&#xA;     $ sample3 : int  3762 28 136 321 94 12 15 0 0 2181 ...&#xD;&#xA;     $ sample4 : int  4845 43 193 361 81 48 9 0 0 2883 ...&#xD;&#xA;     $ sample5 : int  2920 24 104 151 50 20 32 0 0 1743 ...&#xD;&#xA;     $ sample6 : int  4157 11 135 324 58 26 4 0 0 2364 ...&#xD;&#xA;     $ sample7 : int  3000 19 155 242 57 12 18 2 0 1946 ...&#xD;&#xA;     $ sample8 : int  5644 30 227 504 91 37 11 0 0 2988 ...&#xD;&#xA;     $ sample9 : int  2808 65 247 93 272 38 1108 1 0 1430 ...&#xD;&#xA;     $ sample10: int  2458 37 163 64 150 29 729 2 1 1049 ...&#xD;&#xA;     $ sample11: int  2064 30 123 51 142 23 637 0 0 1169 ...&#xD;&#xA;     $ sample12: int  1945 63 209 40 171 41 688 3 2 749 ...&#xD;&#xA;     $ sample13: int  2015 57 432 82 104 47 948 4 0 1171 ...&#xD;&#xA;     $ sample14: int  2550 54 177 59 201 36 730 0 0 1474 ...&#xD;&#xA;     $ sample15: int  2425 90 279 73 358 34 1052 3 3 1027 ...&#xD;&#xA;     $ sample16: int  2343 56 365 67 161 43 877 3 1 1333 ...&#xD;&#xA;&#xD;&#xA;How do I compute RPKM from it?" />
  <row Id="171" PostHistoryTypeId="1" PostId="66" RevisionGUID="905aee59-7f57-49a5-af18-de060aa47915" CreationDate="2017-05-17T10:28:32.203" UserId="191" Text="How to compute RPKM in R?" />
  <row Id="172" PostHistoryTypeId="3" PostId="66" RevisionGUID="905aee59-7f57-49a5-af18-de060aa47915" CreationDate="2017-05-17T10:28:32.203" UserId="191" Text="&lt;transcriptome&gt;&lt;r&gt;&lt;rpkm&gt;" />
  <row Id="173" PostHistoryTypeId="2" PostId="67" RevisionGUID="931feaed-4406-41a4-9c2f-aa6daf58bce5" CreationDate="2017-05-17T10:28:32.203" UserId="191" Text="RPKM is defined as:&#xD;&#xA;&#xD;&#xA;&gt;RPKM =   numberOfReads / ( geneLength/1000 * totalNumReads/1,000,000 )&#xD;&#xA;&#xD;&#xA;As you can see, you need to have gene lengths for every gene.&#xD;&#xA;&#xD;&#xA;Let's say `geneLength` is a vector which have the same number of rows as your `data.frame`, and every value of the vector corresponds to a gene (row) in `expression`.&#xD;&#xA;&#xD;&#xA;    # compute number of reads in each sample&#xD;&#xA;    totalNumReads &lt;- colSums(expression)&#xD;&#xA;    # compute RPKM&#xD;&#xA;    expression.rpkm &lt;- expression * 10^9 / geneLength / totalNumReads&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="174" PostHistoryTypeId="2" PostId="68" RevisionGUID="748fb4e8-c555-47af-a918-85a94ca45abb" CreationDate="2017-05-17T10:43:50.363" UserId="215" Text="If you want something quick and dirty you could rapidly index the FASTA with `samtools faidx` and then put the lengths column through R (other languages are available) on the command line.&#xD;&#xA;&#xD;&#xA;    samtools faidx $fasta&#xD;&#xA;    cut -f2 $fasta.fai | Rscript -e 'data &lt;- as.numeric (readLines (&quot;stdin&quot;)); summary(data); hist(data)'&#xD;&#xA;&#xD;&#xA;This outputs a statistical summary, and creates a PDF in the current directory called Rplots.pdf, containing a histogram." />
  <row Id="175" PostHistoryTypeId="2" PostId="69" RevisionGUID="ede4415e-07e9-4325-badc-34b7b3830a5b" CreationDate="2017-05-17T10:46:41.653" UserId="29" Text="First off,&#xD;&#xA;&#xD;&#xA;**Don’t use RPKMs**.&#xD;&#xA;&#xD;&#xA;[They are truly deprecated](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) because they’re confusing once it comes to paired-end reads. If anything, use *FPKM*s, which are mathematically the same but use a more correct name.&#xD;&#xA;&#xD;&#xA;Even better, [use TPMs, or an appropriate cross-library normalisation method](http://rpubs.com/klmr/rnaseq-norm).&#xD;&#xA;&#xD;&#xA;That said, FPKM an be calculated in R as follows. Note that most of the calculation happens in log transformed number space, to avoid overflows:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    fpkm = function (counts, effective_lengths) {&#xD;&#xA;        exp(log(counts) - log(effective_lengths) - log(sum(counts)) + log(1E9))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;And I mentioned that TPMs are superior, so here’s their function as well:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    tpm = function (counts, effective_lengths) {&#xD;&#xA;        rate = log(counts) - log(effective_lengths)&#xD;&#xA;        exp(rate - log(sum(exp(rate))) + log(1E6))&#xD;&#xA;    }&#xD;&#xA;" />
  <row Id="176" PostHistoryTypeId="5" PostId="62" RevisionGUID="d17ac554-be0c-4f11-ab5c-082e4ec5b3a2" CreationDate="2017-05-17T10:49:49.013" UserId="191" Comment="add some background" Text="I would like to test for positive selection in a large set of genes. More specifically I would like to run model BUSTED in HYPHY with the same settings on many sequence alignments, in a batch mode. What is the proper way of doing that?" />
  <row Id="177" PostHistoryTypeId="5" PostId="69" RevisionGUID="68f212ed-e9ef-4e38-ad57-b5cdecb6e233" CreationDate="2017-05-17T10:57:33.823" UserId="29" Comment="small clarification regarding the method names" Text="First off,&#xD;&#xA;&#xD;&#xA;**Don’t use RPKMs**.&#xD;&#xA;&#xD;&#xA;[They are truly deprecated](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) because they’re confusing once it comes to paired-end reads. If anything, use *FPKM*s, which are mathematically the same but use a more correct name (do we count paired reads separately? No, we count *fragments*).&#xD;&#xA;&#xD;&#xA;Even better, [use TPMs, or an appropriate cross-library normalisation method](http://rpubs.com/klmr/rnaseq-norm).&#xD;&#xA;&#xD;&#xA;That said, FPKM an be calculated in R as follows. Note that most of the calculation happens in log transformed number space, to avoid overflows:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    fpkm = function (counts, effective_lengths) {&#xD;&#xA;        exp(log(counts) - log(effective_lengths) - log(sum(counts)) + log(1E9))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;And I mentioned that TPMs are superior, so here’s their function as well:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    tpm = function (counts, effective_lengths) {&#xD;&#xA;        rate = log(counts) - log(effective_lengths)&#xD;&#xA;        exp(rate - log(sum(exp(rate))) + log(1E6))&#xD;&#xA;    }&#xD;&#xA;" />
  <row Id="178" PostHistoryTypeId="5" PostId="62" RevisionGUID="9189963e-b267-4659-a678-012af0f16d9c" CreationDate="2017-05-17T11:03:54.253" UserId="191" Comment="add even more intro to the question" Text="I would like to test for positive selection in a large set of genes.&#xD;&#xA;&#xD;&#xA;I want to have a yes/no answer to the question if gene was evolving under positive selection. Therefore I chose model [BUSTED][1] for my analysis, which is implemented in [HYPHY][2].&#xD;&#xA;&#xD;&#xA;When I launch HYPHY in the command-line, I get a series of question I  have to answer in the interactive form.&#xD;&#xA; &#xD;&#xA;Is there way to perform a batch analysis involving multiple datasets, i.e. many sequence alignments?&#xD;&#xA;&#xD;&#xA;  [1]: https://doi.org/10.1093/molbev/msv035&#xD;&#xA;  [2]: http://www.hyphy.org/" />
  <row Id="179" PostHistoryTypeId="5" PostId="69" RevisionGUID="11abbd8d-06d4-4ed7-b5c9-0d8a76bda094" CreationDate="2017-05-17T11:07:37.530" UserId="29" Comment="Explain effective length" Text="First off,&#xD;&#xA;&#xD;&#xA;**Don’t use RPKMs**.&#xD;&#xA;&#xD;&#xA;[They are truly deprecated](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) because they’re confusing once it comes to paired-end reads. If anything, use *FPKM*s, which are mathematically the same but use a more correct name (do we count paired reads separately? No, we count *fragments*).&#xD;&#xA;&#xD;&#xA;Even better, [use TPMs, or an appropriate cross-library normalisation method](http://rpubs.com/klmr/rnaseq-norm).&#xD;&#xA;&#xD;&#xA;That said, FPKM an be calculated in R as follows. Note that most of the calculation happens in log transformed number space, to avoid overflows:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    fpkm = function (counts, effective_lengths) {&#xD;&#xA;        exp(log(counts) - log(effective_lengths) - log(sum(counts)) + log(1E9))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Here, the *effective length* is the transcript length minus the mean fragment length plus 1; that is, all the possible positions of an average fragment inside the transcript, which equals the number of all distinct fragments that can be sampled from a transcript.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;And I mentioned that TPMs are superior, so here’s their function as well:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    tpm = function (counts, effective_lengths) {&#xD;&#xA;        rate = log(counts) - log(effective_lengths)&#xD;&#xA;        exp(rate - log(sum(exp(rate))) + log(1E6))&#xD;&#xA;    }&#xD;&#xA;" />
  <row Id="180" PostHistoryTypeId="5" PostId="69" RevisionGUID="7527c238-d27e-4a2a-9854-8dcbae011916" CreationDate="2017-05-17T11:17:22.983" UserId="29" Comment="added 153 characters in body" Text="First off,&#xD;&#xA;&#xD;&#xA;**Don’t use RPKMs**.&#xD;&#xA;&#xD;&#xA;[They are truly deprecated](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) because they’re confusing once it comes to paired-end reads. If anything, use *FPKM*s, which are mathematically the same but use a more correct name (do we count paired reads separately? No, we count *fragments*).&#xD;&#xA;&#xD;&#xA;Even better, [use TPMs, or an appropriate cross-library normalisation method](http://rpubs.com/klmr/rnaseq-norm).&#xD;&#xA;&#xD;&#xA;That said, FPKM an be calculated in R as follows. Note that most of the calculation happens in log transformed number space, to avoid overflows:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    fpkm = function (counts, effective_lengths) {&#xD;&#xA;        exp(log(counts) - log(effective_lengths) - log(sum(counts)) + log(1E9))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Here, the *effective length* is the transcript length minus the mean fragment length plus 1; that is, all the possible positions of an average fragment inside the transcript, which equals the number of all distinct fragments that can be sampled from a transcript.&#xD;&#xA;&#xD;&#xA;This equation fails if all your counts are zero; instead of zeros you will get a vector of NaNs. You might want to account for that.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;And I mentioned that TPMs are superior, so here’s their function as well:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    tpm = function (counts, effective_lengths) {&#xD;&#xA;        rate = log(counts) - log(effective_lengths)&#xD;&#xA;        exp(rate - log(sum(exp(rate))) + log(1E6))&#xD;&#xA;    }&#xD;&#xA;" />
  <row Id="182" PostHistoryTypeId="10" PostId="46" RevisionGUID="b7369a0b-9cad-45c0-af84-2374efb70347" CreationDate="2017-05-17T11:33:29.333" UserId="-1" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;},{&quot;Id&quot;:104,&quot;DisplayName&quot;:&quot;Chris_Rands&quot;},{&quot;Id&quot;:203,&quot;DisplayName&quot;:&quot;Mitra&quot;},{&quot;Id&quot;:109,&quot;DisplayName&quot;:&quot;Harry&quot;},{&quot;Id&quot;:215,&quot;DisplayName&quot;:&quot;SamStudio8&quot;}]}" />
  <row Id="183" PostHistoryTypeId="5" PostId="1" RevisionGUID="4508ab1c-34c4-4414-a0ab-06d1a1aea95d" CreationDate="2017-05-17T11:35:45.053" UserId="48" Comment="Adding information from comments" Text="I'd like to learn which format it's commonly used for storing full sequence human genome (4 letters without quality score) and why.&#xD;&#xA;&#xD;&#xA;I assume storing it in plain format would be very inefficient. I would think more about binary format like 2 bits per nucleotide.&#xD;&#xA;&#xD;&#xA;Therefore, which format would be the most common in terms of space efficiency?" />
  <row Id="184" PostHistoryTypeId="24" PostId="1" RevisionGUID="4508ab1c-34c4-4414-a0ab-06d1a1aea95d" CreationDate="2017-05-17T11:35:45.053" Comment="Proposed by 48 approved by 43 edit id of 9" />
  <row Id="185" PostHistoryTypeId="2" PostId="70" RevisionGUID="8ca67da7-9c10-4f9e-9736-199bf58c8c08" CreationDate="2017-05-17T11:51:45.250" UserId="196" Text="I have two annotations of the same genome generated with different annotation pipelines. I want to identify overlapping gene models. &#xD;&#xA;&#xD;&#xA;*An important feature of this genome is that there are many 'genes within genes', i.e. a genemodel in the intron of another genemodel*. Therefore, I only want to count two genemodels as overlapping when their coding sequence exon annotations overlap.&#xD;&#xA;&#xD;&#xA;Using something like ```bedtools intersect``` it is straightforward to calculate overlap between the gene-level annotations. &#xD;&#xA;&#xD;&#xA;However: I am not sure how to select genes as overlapping when only their coding sequence exons (CDS features) overlap." />
  <row Id="186" PostHistoryTypeId="1" PostId="70" RevisionGUID="8ca67da7-9c10-4f9e-9736-199bf58c8c08" CreationDate="2017-05-17T11:51:45.250" UserId="196" Text="How to calculate overlapping genes between two genome annotation versions" />
  <row Id="187" PostHistoryTypeId="3" PostId="70" RevisionGUID="8ca67da7-9c10-4f9e-9736-199bf58c8c08" CreationDate="2017-05-17T11:51:45.250" UserId="196" Text="&lt;annotation&gt;" />
  <row Id="188" PostHistoryTypeId="5" PostId="66" RevisionGUID="73254858-a287-42fc-9ed5-7f3e1a51fcfa" CreationDate="2017-05-17T11:56:24.147" UserId="141" Comment="Clarified what data the question answer (probably) have" Text="I have the following data of fragment counts for each gene in 16 samples:&#xD;&#xA;&#xD;&#xA;    &gt; str(expression)&#xD;&#xA;    'data.frame':	42412 obs. of  16 variables:&#xD;&#xA;     $ sample1 : int  4555 49 122 351 53 27 1 0 0 2513 ...&#xD;&#xA;     $ sample2 : int  2991 51 55 94 49 10 55 0 0 978 ...&#xD;&#xA;     $ sample3 : int  3762 28 136 321 94 12 15 0 0 2181 ...&#xD;&#xA;     $ sample4 : int  4845 43 193 361 81 48 9 0 0 2883 ...&#xD;&#xA;     $ sample5 : int  2920 24 104 151 50 20 32 0 0 1743 ...&#xD;&#xA;     $ sample6 : int  4157 11 135 324 58 26 4 0 0 2364 ...&#xD;&#xA;     $ sample7 : int  3000 19 155 242 57 12 18 2 0 1946 ...&#xD;&#xA;     $ sample8 : int  5644 30 227 504 91 37 11 0 0 2988 ...&#xD;&#xA;     $ sample9 : int  2808 65 247 93 272 38 1108 1 0 1430 ...&#xD;&#xA;     $ sample10: int  2458 37 163 64 150 29 729 2 1 1049 ...&#xD;&#xA;     $ sample11: int  2064 30 123 51 142 23 637 0 0 1169 ...&#xD;&#xA;     $ sample12: int  1945 63 209 40 171 41 688 3 2 749 ...&#xD;&#xA;     $ sample13: int  2015 57 432 82 104 47 948 4 0 1171 ...&#xD;&#xA;     $ sample14: int  2550 54 177 59 201 36 730 0 0 1474 ...&#xD;&#xA;     $ sample15: int  2425 90 279 73 358 34 1052 3 3 1027 ...&#xD;&#xA;     $ sample16: int  2343 56 365 67 161 43 877 3 1 1333 ...&#xD;&#xA;&#xD;&#xA;How do I compute RPKM values from these?" />
  <row Id="189" PostHistoryTypeId="24" PostId="66" RevisionGUID="73254858-a287-42fc-9ed5-7f3e1a51fcfa" CreationDate="2017-05-17T11:56:24.147" Comment="Proposed by 141 approved by 191 edit id of 14" />
  <row Id="190" PostHistoryTypeId="2" PostId="71" RevisionGUID="28e55b80-dd1c-4ead-a644-02bc5c20ace0" CreationDate="2017-05-17T11:57:13.740" UserId="-1" Text="" />
  <row Id="191" PostHistoryTypeId="1" PostId="71" RevisionGUID="28e55b80-dd1c-4ead-a644-02bc5c20ace0" CreationDate="2017-05-17T11:57:13.740" UserId="-1" />
  <row Id="192" PostHistoryTypeId="2" PostId="72" RevisionGUID="a8ea6366-ccea-464d-8e0c-6e0b6378b338" CreationDate="2017-05-17T11:57:13.740" UserId="-1" Text="" />
  <row Id="193" PostHistoryTypeId="1" PostId="72" RevisionGUID="a8ea6366-ccea-464d-8e0c-6e0b6378b338" CreationDate="2017-05-17T11:57:13.740" UserId="-1" />
  <row Id="195" PostHistoryTypeId="2" PostId="73" RevisionGUID="df020ad9-2438-4134-8d81-129d6d508660" CreationDate="2017-05-17T13:02:17.453" UserId="133" Text="I am trying to understand the benefits of joint genotyping and would be grateful if someone could provide an argument (ideally mathematically) that would clearly demonstrate the benefit of joint vs. single-sample genotyping.&#xD;&#xA;&#xD;&#xA;This is what I've gathered from other resources (Biostars, GATK forums, etc.)&#xD;&#xA;&#xD;&#xA;- Joint-genotyping helps control FDR because errors from individually genotyped samples are added up, and amplified when merging call-sets (by Heng Li on https://www.biostars.org/p/10926/)&#xD;&#xA;&#xD;&#xA;If someone understands this, can you please clarify what is the difference on the overall FDR rate between the two scenarios (again, with an example ideally)&#xD;&#xA;&#xD;&#xA;- Greater sensitivity for low-frequency variants - By sharing information across all samples, joint calling makes it possible to “rescue” genotype calls at sites where a carrier has low coverage but other samples within the call set have a confident variant at that location. (from https://software.broadinstitute.org/gatk/documentation/article.php?id=4150)&#xD;&#xA;&#xD;&#xA;I don't understand how the presence of a confidently called variant at the same locus in another individual can affect the genotyping of an individual with low coverage. Is there some valid argument that allows one to consider reads from another person as evidence of a particular variant in a third person? What are the assumptions for such an argument? What if that person is from a different population with entirely different allele frequencies for that variant?&#xD;&#xA;&#xD;&#xA;Having read several of the papers (or method descriptions) that describe the latest haplotype-aware SNP calling methods (HaplotypeCaller, freebayes, Platypus) the overall framework seems to be:&#xD;&#xA;&#xD;&#xA;- 1. Establish a prior on the allele frequency distribution at a site of interest using one (or combination) of: non-informative prior, population genetics model-based prior like Wright Fisher, prior based on established variation patterns like dbSNP, ExAC, or gnomAD.&#xD;&#xA;- 2. Build a list of plausible haplotypes in a region around the locus of interest using local assembly.&#xD;&#xA;- 3. Select haplotype with highest likelihood based on prior and reads data and infer the locus genotype accordingly.&#xD;&#xA;&#xD;&#xA;At which point(s) in the above procedure can information between samples be shared or pooled? Should one not trust the AFS from a large-scale resource like gnomAD much more than the distribution obtained from other samples that are nominally party of the same &quot;cohort&quot; but may have little to do with each other because of different ancestry, for example?&#xD;&#xA;&#xD;&#xA;I really want to understand the justifications and benefits offered by multi-sample genotyping and would appreciate your insights.    " />
  <row Id="196" PostHistoryTypeId="1" PostId="73" RevisionGUID="df020ad9-2438-4134-8d81-129d6d508660" CreationDate="2017-05-17T13:02:17.453" UserId="133" Text="Single-sample vs. joint genotyping" />
  <row Id="197" PostHistoryTypeId="3" PostId="73" RevisionGUID="df020ad9-2438-4134-8d81-129d6d508660" CreationDate="2017-05-17T13:02:17.453" UserId="133" Text="&lt;genotyping&gt;&lt;gatk&gt;&lt;freebayes&gt;&lt;platypus&gt;&lt;multi-sample&gt;" />
  <row Id="198" PostHistoryTypeId="5" PostId="57" RevisionGUID="6a32f956-6143-406b-a43c-a98ed39f2f2c" CreationDate="2017-05-17T13:04:24.530" UserId="48" Comment="added 90 characters in body" Text="A popular framework to analyze differences between groups, either experiments or diseases, in transcriptomics is using linear models ([limma][1] is a popular choice). &#xD;&#xA;&#xD;&#xA;For instance we have a disease D with three stages as defined by clinicians, A, B and C. 10 samples each stage and the healthy H to compare with is RNA-sequenced. A typical linear model would be to observe the three stages`~A+B+C` independently. The data of each stage is not from the same person. (but for the question assume it isn't)&#xD;&#xA;&#xD;&#xA;My understanding is that such a model would not take into account that stage C appears only on 30% of patients in stage B. And that a healthy patient upon external factors can jump to stage B. &#xD;&#xA;&#xD;&#xA;If we want to find the role of a gene in the disease we should include somehow this information in the model. Which makes me think about mixing linear models and hidden Markov chains.&#xD;&#xA;&#xD;&#xA;How can such a disease be described in terms of linear models with such data and information?&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/limma" />
  <row Id="199" PostHistoryTypeId="2" PostId="74" RevisionGUID="cf22ac5e-9515-4b0b-957a-5c5c2a942949" CreationDate="2017-05-17T13:13:18.863" UserId="77" Text="The benefit to additional samples is seen in your point 1. The likelihood of making a variant call is a function of (1) the depth of coverage supporting a given variant (ignoring mapping/base quality considerations) and (2) the likelihood of that variant existing given background knowledge. With low depth and no background knowledge, poorly covered variants will be assumed to be sequencing errors. Adding more samples can just serve then to increase the background knowledge on a position." />
  <row Id="201" PostHistoryTypeId="5" PostId="67" RevisionGUID="cb44a92c-37f6-4a0c-8208-aa3f75ad2cfc" CreationDate="2017-05-17T13:21:24.593" UserId="191" Comment="updating the code" Text="RPKM is defined as:&#xD;&#xA;&#xD;&#xA;&gt;RPKM =   numberOfReads / ( geneLength/1000 * totalNumReads/1,000,000 )&#xD;&#xA;&#xD;&#xA;As you can see, you need to have gene lengths for every gene.&#xD;&#xA;&#xD;&#xA;Let's say `geneLength` is a vector which have the same number of rows as your `data.frame`, and every value of the vector corresponds to a gene (row) in `expression`.&#xD;&#xA;&#xD;&#xA;    # compute number of reads in each sample&#xD;&#xA;    totalNumReads &lt;- colSums(expression)&#xD;&#xA;    # compute RPKM&#xD;&#xA;    expression.rpkm &lt;- 10^9 * sweep(expression, 2, totalNumReads, `/`) / geneLength&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="202" PostHistoryTypeId="2" PostId="75" RevisionGUID="1a7bdc5c-0e82-4e57-9e23-5dae21e425d5" CreationDate="2017-05-17T13:34:35.107" UserId="235" Text="To add practical advice to what others have said:&#xD;&#xA;&#xD;&#xA;In a practical sense, I think the biggest difference between RefSeq and Ensembl/GENCODE is in the sensitivity/specificity trade off. &#xD;&#xA;&#xD;&#xA;Ensembl aims more towards the inclusive end, including a far larger number of transcript variants, many of which are only weakly supported.&#xD;&#xA;&#xD;&#xA;RefSeq trades some of this sensitivity for specificity - you can be more confident that a RefSeq transcript exists, but less confident that the ReqSeq annotation includes all of the real transcripts for a gene. " />
  <row Id="203" PostHistoryTypeId="2" PostId="76" RevisionGUID="1cfb6be5-d3cc-448c-8418-2e604f13819c" CreationDate="2017-05-17T14:08:21.663" UserId="37" Text="Say you are sequencing to 2X coverage. Suppose at a site, sample S has one reference base and one alternate base. It is hard to tell if this is a sequencing error or a heterozygote. Now suppose you have 1000 other samples, all at 2X read depth. One of them has two ALT bases; 10 of them have one REF and one ALT. It is usually improbable that all these samples have the same sequencing error. Then you can assert sample S has a het. Multi-sample calling helps to increase the sensitivity of not so rare SNPs. Note that what matters here is the assumption of error independency. Ancestry only has a tiny indirect effect.&#xD;&#xA;&#xD;&#xA;Multi-sample calling penalizes very rare SNPs, in particular singletons. When you care about variants only, this is for good. Naively combining single-sample calls yields a higher error rate. Multi-sample calling also helps variant filtering at a later stage. For example, for a sample sequenced to 30X coverage, you would not know if a site at 45X depth is caused by a potential CNV/mismapping or by statistical fluctuation. When you see 1000 30X samples at 45X depth, you can easily know you are looking at a CNV/systematic mismapping. Multiple samples enhance most statistical signals.&#xD;&#xA;&#xD;&#xA;Older methods pool all BAMs when calling variants. This is necessary because a single low-coverage sample does not have enough data to recover hidden INDELs. However, this strategy is not that easy to massively parallelized; adding a new sample triggers re-calling, which is very expensive as well. As we are mostly doing high-coverage sequencing these days, the old problem with INDEL calling does not matter now. GATK has this new single-sample calling pipeline where you combine per-sample gVCFs at a later stage. Such sample combining strategy is perhaps the only sensible solution when you are dealing with 100k samples.&#xD;&#xA;&#xD;&#xA;The so-called haplotype based variant calling is a separate question. This type of approach helps to call INDELs, but is not of much relevance to multi-sample calling. Also, of the three variant callers in your question, only GATK (and Scalpel which you have not mentioned) use assembly at large. Freebayes does not. Platypus does but only to a limited extent and does not work well in practice.&#xD;&#xA;&#xD;&#xA;I guess what you really want to talk about is imputation based calling. This approach further improves sensitivity with LD. With enough samples, you can measure the LD between two positions. Suppose at position 1000, you see one REF read and no ALT reads; at position 1500, you see one REF read and two ALT reads. You would not call any SNPs at position 1000 even given multiple samples. However, when you know the two positions are strongly linked and the dominant haplotypes are REF-REF and ALT-ALT, you know the sample under investigation is likely to have a missing ALT allele. LD transfers signals across sites and enhances the power to make correct genotyping calls. Nonetheless, as we are mostly doing high-coverage sequencing nowadays, imputation based methods only have a minor effect and are rarely applied." />
  <row Id="204" PostHistoryTypeId="6" PostId="66" RevisionGUID="89ac318b-d67b-44d5-80ab-658d0e9713d3" CreationDate="2017-05-17T14:17:04.643" UserId="191" Comment="adding tag" Text="&lt;transcriptome&gt;&lt;rna-seq&gt;&lt;r&gt;&lt;rpkm&gt;" />
  <row Id="205" PostHistoryTypeId="6" PostId="46" RevisionGUID="37079c37-8516-4eec-8325-8cab48a98872" CreationDate="2017-05-17T14:18:26.200" UserId="23" Comment="these tags are not necessary for the question asked" Text="&lt;transcriptome&gt;&lt;genome&gt;" />
  <row Id="206" PostHistoryTypeId="24" PostId="46" RevisionGUID="37079c37-8516-4eec-8325-8cab48a98872" CreationDate="2017-05-17T14:18:26.200" Comment="Proposed by 23 approved by 173 edit id of 7" />
  <row Id="208" PostHistoryTypeId="5" PostId="67" RevisionGUID="31bde091-6a9b-4ed6-b941-7d8241a8e933" CreationDate="2017-05-17T14:27:47.577" UserId="191" Comment="more R-style code" Text="RPKM is defined as:&#xD;&#xA;&#xD;&#xA;&gt;RPKM =   numberOfReads / ( geneLength/1000 * totalNumReads/1,000,000 )&#xD;&#xA;&#xD;&#xA;As you can see, you need to have gene lengths for every gene.&#xD;&#xA;&#xD;&#xA;Let's say `geneLength` is a vector which have the same number of rows as your `data.frame`, and every value of the vector corresponds to a gene (row) in `expression`.&#xD;&#xA;&#xD;&#xA;    # compute number of reads in each sample&#xD;&#xA;    totalNumReads &lt;- colSums(expression)&#xD;&#xA;    # compute RPKM&#xD;&#xA;    expression.rpkm &lt;- sapply(expression, function(column) 10^9 * column / geneLength / sum(column))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="209" PostHistoryTypeId="2" PostId="77" RevisionGUID="c702b59c-7fb9-4022-a79d-f5012382c380" CreationDate="2017-05-17T14:56:36.030" UserId="243" Text="**Short Answer:**&#xD;&#xA;In my opinion, my approach would be to pull out the CDS exons and run bedtools on those.  &#xD;&#xA;&#xD;&#xA;**A Few More Details:**&#xD;&#xA;When you pull out the exons, make sure that you assign them all IDs if the don't already have them assigned and record which IDs &quot;belong&quot; to which genes. Now when you get exons that overlap, you know that they are coding and you can tie them back to which genes they originate from." />
  <row Id="210" PostHistoryTypeId="2" PostId="78" RevisionGUID="6701055b-9baf-4d4e-b2f8-fef0314b085c" CreationDate="2017-05-17T15:16:10.923" UserId="35" Text="If you are looking for a more visual solution (in addition to the other answers), [NCI Genomic Data Commons (TCGA repository)][1] offers a nice formula:&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;&gt; Where:&#xD;&#xA;&gt;&#xD;&#xA;&gt; RCg: Number of reads mapped to the gene&#xD;&#xA;&gt; &#xD;&#xA;&gt; RCpc: Number of reads mapped to all protein-coding genes&#xD;&#xA;&gt; &#xD;&#xA;&gt; RCg75: The 75th percentile read count value for genes in the sample&#xD;&#xA;&gt; &#xD;&#xA;&gt; L: Length of the gene in base pairs&#xD;&#xA;&#xD;&#xA;As others have pointed out, FPKMs have some problems. GDC also calculates FPKM-UQ values that are upper quartile normalized. Those are recommended for cross-sample comparison and differential expression analysis.&#xD;&#xA;&#xD;&#xA;  [1]: https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/&#xD;&#xA;  [2]: https://i.stack.imgur.com/RY53Q.png" />
  <row Id="212" PostHistoryTypeId="2" PostId="79" RevisionGUID="515f8816-f764-40e6-85b1-e106d4ebeed9" CreationDate="2017-05-17T15:33:34.013" UserId="206" Text="If you are planning to do a differential expression analysis, you will probably don't need the RPKM calculation.&#xD;&#xA;&#xD;&#xA;RPK= No.of Mapped reads/ length of transcript in kb (transcript length/1000)&#xD;&#xA;&#xD;&#xA;RPKM = RPK/total no.of reads in million (total no of reads/ 1000000)&#xD;&#xA;&#xD;&#xA;**The whole formula together:**&#xD;&#xA;&#xD;&#xA;RPKM = (10^9 * C)/(N * L)&#xD;&#xA;Where,&#xD;&#xA;&#xD;&#xA;C = Number of reads mapped to a gene&#xD;&#xA;&#xD;&#xA;N = Total mapped reads in the experiment&#xD;&#xA;&#xD;&#xA;L = exon length in base-pairs for a gene&#xD;&#xA;" />
  <row Id="213" PostHistoryTypeId="2" PostId="80" RevisionGUID="024dafd2-83da-4bb5-8f03-b371d72f21b9" CreationDate="2017-05-17T15:55:59.503" UserId="191" Text="There are two potential sources of bias in this design.&#xD;&#xA;&#xD;&#xA;1. We cannot distinguish correlation from causation. &#xD;&#xA;&#xD;&#xA;Imagine two cases. In the first, the disease progression is inducing immune response. Later stages will be associated with the higher gene expression levels. In the second scenario, the disease is caused by overexpression of a gene. Later stages will be also associated with the higher expression.&#xD;&#xA;&#xD;&#xA;This is typical for observational studies. But I just want to mention that a special care should be taken during interpretation of the results.&#xD;&#xA;&#xD;&#xA;2. If we are not following our individuals, we cannot distinguish correlation and avoidance.&#xD;&#xA;&#xD;&#xA;Let's say the disease is lethal in certain cases; the survival is negatively correlated with the disease stage. Now imagine there is a gene, which causes severe symptoms when highly expressed. On the later stages you will only observe those patients in which the gene was not highly expressed. From that you would conclude that gene expression is decreasing with the disease progression. In reality this gene is very important and causal, you just do not have patients which are alive to see this.&#xD;&#xA;&#xD;&#xA;This is a similar to [Wald's studies of aircrafts][1].&#xD;&#xA;&#xD;&#xA;&gt; Researchers from the Center for Naval Analyses had conducted a study&#xD;&#xA;&gt; of the damage done to aircraft that had returned from missions, and&#xD;&#xA;&gt; had recommended that armor be added to the areas that showed the most&#xD;&#xA;&gt; damage.&#xD;&#xA;&gt; Wald proposed that the Navy instead reinforce the areas where the&#xD;&#xA;&gt; returning aircraft were unscathed, since those were the areas that, if&#xD;&#xA;&gt; hit, would cause the plane to be lost.&#xD;&#xA;&#xD;&#xA;I think the second point is crucial, and can and will lead to false conclusions. &#xD;&#xA;&#xD;&#xA;I suggest following individuals instead.&#xD;&#xA;&#xD;&#xA;There are different approaches you can use later. For example you can have two-stage procedure:&#xD;&#xA;&#xD;&#xA;1. Identify genes which are differentially expressed between healthy (H) and sick (A, B or C).&#xD;&#xA;2. Build a linear model of disease stage stage ~ gene1 + gene2 + ..., using genes identified at step 1.&#xD;&#xA;3. Similarly build a linear model of survival as a function gene expression.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Abraham_Wald&#xD;&#xA;&#xD;&#xA;" />
  <row Id="214" PostHistoryTypeId="5" PostId="69" RevisionGUID="2007d97f-09c2-48b1-9178-5a3a42f4ad7c" CreationDate="2017-05-17T16:02:42.330" UserId="29" Comment="expand to multiple libraries" Text="First off,&#xD;&#xA;&#xD;&#xA;**Don’t use RPKMs**.&#xD;&#xA;&#xD;&#xA;[They are truly deprecated](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) because they’re confusing once it comes to paired-end reads. If anything, use *FPKM*s, which are mathematically the same but use a more correct name (do we count paired reads separately? No, we count *fragments*).&#xD;&#xA;&#xD;&#xA;Even better, [use TPMs, or an appropriate cross-library normalisation method](http://rpubs.com/klmr/rnaseq-norm).&#xD;&#xA;&#xD;&#xA;That said, FPKM an be calculated in R as follows. Note that most of the calculation happens in log transformed number space, to avoid overflows:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    fpkm = function (counts, effective_lengths) {&#xD;&#xA;        exp(log(counts) - log(effective_lengths) - log(sum(counts)) + log(1E9))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Here, the *effective length* is the transcript length minus the mean fragment length plus 1; that is, all the possible positions of an average fragment inside the transcript, which equals the number of all distinct fragments that can be sampled from a transcript.&#xD;&#xA;&#xD;&#xA;This function handles *one* library at a time. I ([and others](http://varianceexplained.org/r/tidy-genomics-biobroom/)) argue that this is the way functions should be written. If you want to apply the code to multiple libraries, nothing is easier using [‹dplyr›](http://dplyr.tidyverse.org/):&#xD;&#xA;&#xD;&#xA;    tidy_expression = tidy_expression %&gt;%&#xD;&#xA;        group_by(Sample) %&gt;%&#xD;&#xA;        mutate(FPKM = fpkm(Count, col_data$Lengths))&#xD;&#xA;&#xD;&#xA;However, the data in the question isn’t in tidy data format, so we first need to transform it accordingly using [‹tidyr›](http://tidyr.tidyverse.org/):&#xD;&#xA;&#xD;&#xA;    tidy_expression = expression %&gt;%&#xD;&#xA;        gather(Sample, Count)&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;This equation fails if all your counts are zero; instead of zeros you will get a vector of NaNs. You might want to account for that.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;And I mentioned that TPMs are superior, so here’s their function as well:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    tpm = function (counts, effective_lengths) {&#xD;&#xA;        rate = log(counts) - log(effective_lengths)&#xD;&#xA;        exp(rate - log(sum(exp(rate))) + log(1E6))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="215" PostHistoryTypeId="5" PostId="80" RevisionGUID="e326562a-34b5-47bf-a2ad-48cb2344527f" CreationDate="2017-05-17T16:13:40.247" UserId="191" Comment="suggestion" Text="There are two potential sources of bias in this design.&#xD;&#xA;&#xD;&#xA;1. We cannot distinguish correlation from causation. &#xD;&#xA;&#xD;&#xA;Imagine two cases. In the first, the disease progression is inducing immune response. Later stages will be associated with the higher gene expression levels. In the second scenario, the disease is caused by overexpression of a gene. Later stages will be also associated with the higher expression.&#xD;&#xA;&#xD;&#xA;This is typical for observational studies. But I just want to mention that a special care should be taken during interpretation of the results.&#xD;&#xA;&#xD;&#xA;2. If we are not following our individuals, we cannot distinguish correlation and avoidance.&#xD;&#xA;&#xD;&#xA;Let's say the disease is lethal in certain cases; the survival is negatively correlated with the disease stage. Now imagine there is a gene, which causes severe symptoms when highly expressed. On the later stages you will only observe those patients in which the gene was not highly expressed. From that you would conclude that gene expression is decreasing with the disease progression. In reality this gene is very important and causal, you just do not have patients which are alive to see this.&#xD;&#xA;&#xD;&#xA;This is a similar to [Wald's studies of aircrafts][1].&#xD;&#xA;&#xD;&#xA;&gt; Researchers from the Center for Naval Analyses had conducted a study&#xD;&#xA;&gt; of the damage done to aircraft that had returned from missions, and&#xD;&#xA;&gt; had recommended that armor be added to the areas that showed the most&#xD;&#xA;&gt; damage.&#xD;&#xA;&gt; Wald proposed that the Navy instead reinforce the areas where the&#xD;&#xA;&gt; returning aircraft were unscathed, since those were the areas that, if&#xD;&#xA;&gt; hit, would cause the plane to be lost.&#xD;&#xA;&#xD;&#xA;I think the second point is crucial, and can and will lead to false conclusions. &#xD;&#xA;&#xD;&#xA;I suggest that the same individuals are followed for a long time.&#xD;&#xA;&#xD;&#xA;There are different approaches you can use later. For example you can have two-stage procedure:&#xD;&#xA;&#xD;&#xA;1. Identify genes which are differentially expressed between healthy (H) and sick (A, B or C).&#xD;&#xA;2. Build a linear model of disease stage stage ~ gene1 + gene2 + ..., using genes identified at step 1.&#xD;&#xA;3. Similarly build a linear model of survival as a function gene expression.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Abraham_Wald&#xD;&#xA;&#xD;&#xA;" />
  <row Id="216" PostHistoryTypeId="2" PostId="81" RevisionGUID="755bc07a-3c93-49c4-950e-296eb5ebfa1b" CreationDate="2017-05-17T16:26:16.783" UserId="174" Text="I have a FASTA file with 100+ sequences like this:&#xD;&#xA;&#xD;&#xA;    &gt;Sequence1&#xD;&#xA;    GTGCCTATTGCTACTAAAA ...&#xD;&#xA;    &gt;Sequence2&#xD;&#xA;    GCAATGCAAGGAAGTGATGGCGGAAATAGCGTTA&#xD;&#xA;    ......&#xD;&#xA;&#xD;&#xA;I also have a text file like this:&#xD;&#xA;&#xD;&#xA;    Sequence1 40&#xD;&#xA;    Sequence2 30&#xD;&#xA;    ......&#xD;&#xA;&#xD;&#xA;I would like to simulate next-generation paired-end reads for all the sequences in my FASTA file. For `Sequence1`, I would like to simulate at 40x coverage. For `Sequence2`, I would like to simulate at 30x coverage. In other words, I want to control my sequence coverage for each sequence in my simulation.&#xD;&#xA;&#xD;&#xA;**Q:** What is the simplest way to do that? Any software I should use? Bioconductor?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="217" PostHistoryTypeId="1" PostId="81" RevisionGUID="755bc07a-3c93-49c4-950e-296eb5ebfa1b" CreationDate="2017-05-17T16:26:16.783" UserId="174" Text="How to simulate NGS reads, controlling sequence coverage?" />
  <row Id="218" PostHistoryTypeId="3" PostId="81" RevisionGUID="755bc07a-3c93-49c4-950e-296eb5ebfa1b" CreationDate="2017-05-17T16:26:16.783" UserId="174" Text="&lt;genome&gt;" />
  <row Id="219" PostHistoryTypeId="2" PostId="82" RevisionGUID="201bcadc-2ab7-40ef-b810-070f240232e2" CreationDate="2017-05-17T16:36:36.997" UserId="31" Text="I am not aware of a software to do this directly but I would split the fasta file into one sequence per file, loop over them in BASH and invoke ART the sequence simulator (or another) on each sequence." />
  <row Id="220" PostHistoryTypeId="5" PostId="80" RevisionGUID="5b7a761d-2413-464b-90da-92498f0413d2" CreationDate="2017-05-17T16:45:14.847" UserId="191" Comment="Remove &quot;a'" Text="There are two potential sources of bias in this design.&#xD;&#xA;&#xD;&#xA;1. We cannot distinguish correlation from causation. &#xD;&#xA;&#xD;&#xA;Imagine two cases. In the first, the disease progression is inducing immune response. Later stages will be associated with the higher gene expression levels. In the second scenario, the disease is caused by overexpression of a gene. Later stages will be also associated with the higher expression.&#xD;&#xA;&#xD;&#xA;This is typical for observational studies. But I just want to mention that a special care should be taken during interpretation of the results.&#xD;&#xA;&#xD;&#xA;2. If we are not following our individuals, we cannot distinguish correlation and avoidance.&#xD;&#xA;&#xD;&#xA;Let's say the disease is lethal in certain cases; the survival is negatively correlated with the disease stage. Now imagine there is a gene, which causes severe symptoms when highly expressed. On the later stages you will only observe those patients in which the gene was not highly expressed. From that you would conclude that gene expression is decreasing with the disease progression. In reality this gene is very important and causal, you just do not have patients which are alive to see this.&#xD;&#xA;&#xD;&#xA;This is similar to [Wald's studies of aircrafts][1].&#xD;&#xA;&#xD;&#xA;&gt; Researchers from the Center for Naval Analyses had conducted a study&#xD;&#xA;&gt; of the damage done to aircraft that had returned from missions, and&#xD;&#xA;&gt; had recommended that armor be added to the areas that showed the most&#xD;&#xA;&gt; damage.&#xD;&#xA;&gt; Wald proposed that the Navy instead reinforce the areas where the&#xD;&#xA;&gt; returning aircraft were unscathed, since those were the areas that, if&#xD;&#xA;&gt; hit, would cause the plane to be lost.&#xD;&#xA;&#xD;&#xA;I think the second point is crucial, and can and will lead to false conclusions. &#xD;&#xA;&#xD;&#xA;I suggest that the same individuals are followed for a long time.&#xD;&#xA;&#xD;&#xA;There are different approaches you can use later. For example you can have two-stage procedure:&#xD;&#xA;&#xD;&#xA;1. Identify genes which are differentially expressed between healthy (H) and sick (A, B or C).&#xD;&#xA;2. Build a linear model of disease stage stage ~ gene1 + gene2 + ..., using genes identified at step 1.&#xD;&#xA;3. Similarly build a linear model of survival as a function gene expression.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Abraham_Wald&#xD;&#xA;&#xD;&#xA;" />
  <row Id="221" PostHistoryTypeId="2" PostId="83" RevisionGUID="80e10533-66b9-4a19-a607-cc3a7028c221" CreationDate="2017-05-17T16:52:03.917" UserId="235" Text="This question pertains to iCLIP, but it could just as easily be ChIP-seq or ATAC-seq or mutation frequencies. &#xD;&#xA;&#xD;&#xA;I have iCLIP read counts across the transcriptome and I wish to know if the signals are correlated - that is, where one of them is high, is the other likely to be high.&#xD;&#xA;&#xD;&#xA;Often when dealing with such data (e.g. iCLIP data) we know that the data is generally sparse - that is at most positions both signals are zero and this is correct, and also zero-inflated - that is many bases that &quot;should&quot; have a signal are missing that data. So just calculating the Spearman's correlation is likely to give an artificially low value.&#xD;&#xA;&#xD;&#xA;**What might be a way to asses the association?**&#xD;&#xA;&#xD;&#xA;Things I have thought of:&#xD;&#xA;&#xD;&#xA;* Apply some sort of smoothing to the data (eg a rolling mean). Remove any bases with 0 in both samples. Compute the spearmans. &#xD;&#xA;* Calculate the average pairwise distance between every read in sample one and every read in sample two. Compare this to data where the reads have been randomised within genes. &#xD;&#xA;&#xD;&#xA;In the first case removing all bases with 0 in both samples seems wrong. But if 99.99% of all bases have zero in both samples, then this seems like its necessary for Spearman. &#xD;&#xA;&#xD;&#xA;In the second case, the result seems like it would be non-intuitive to interpret. And also calculating this would be massively computationally intensive. " />
  <row Id="222" PostHistoryTypeId="1" PostId="83" RevisionGUID="80e10533-66b9-4a19-a607-cc3a7028c221" CreationDate="2017-05-17T16:52:03.917" UserId="235" Text="How to correlate two zero inflated bedgraph-like signals?" />
  <row Id="223" PostHistoryTypeId="3" PostId="83" RevisionGUID="80e10533-66b9-4a19-a607-cc3a7028c221" CreationDate="2017-05-17T16:52:03.917" UserId="235" Text="&lt;iclip&gt;&lt;statistics&gt;" />
  <row Id="224" PostHistoryTypeId="2" PostId="84" RevisionGUID="328ede18-7d52-44bb-a2c4-f18e5898bbaa" CreationDate="2017-05-17T17:33:19.747" UserId="73" Text="Statistics for nanopore reads are tricky because of the huge range of read lengths that can be present in a single run. I have found that the best way to display lengths is by using a log scale on both the x axis (length) and the y axis (sequenced bases, or counts, depending on preference).&#xD;&#xA;&#xD;&#xA;I have written my own scripts for doing this: one for generating the read lengths, and another for plotting the length distribution in various ways. The script that generates read lengths also spits out basic length summary statistics to standard error:&#xD;&#xA;&#xD;&#xA;    $ ~/scripts/fastx-length.pl &gt; lengths_mtDNA_called.txt&#xD;&#xA;    Total sequences: 2110&#xD;&#xA;    Total length: 5.106649 Mb&#xD;&#xA;    Longest sequence: 107.414 kb&#xD;&#xA;    Shortest sequence: 219 b&#xD;&#xA;    Mean Length: 2.42 kb&#xD;&#xA;    Median Length: 1.504 kb&#xD;&#xA;    N50: 336 sequences; L50: 3.644 kb&#xD;&#xA;    N90: 1359 sequences; L90: 1.103 kb&#xD;&#xA;&#xD;&#xA;    $ ~/scripts/length_plot.r lengths_mtDNA_called.txt&#xD;&#xA;    lengths_mtDNA_called.txt ... done&#xD;&#xA;    Number of sequences: 2110 &#xD;&#xA;    Length quantiles:&#xD;&#xA;          0%      10%      20%      30%      40%      50%      60%      70% &#xD;&#xA;       219.0    506.9    724.4    953.0   1196.2   1503.0   1859.2   2347.3 &#xD;&#xA;         80%      90%     100% &#xD;&#xA;      3128.2   4804.7 107414.0 &#xD;&#xA;&#xD;&#xA;Here are a couple of of the produced graphs:&#xD;&#xA;&#xD;&#xA;[![Digital electrophoresis plot][1]][1]&#xD;&#xA;&#xD;&#xA;[![Read length distribution plot][2]][2]&#xD;&#xA;&#xD;&#xA;The scripts to generate these can be found here:&#xD;&#xA;&#xD;&#xA; - https://github.com/gringer/bioinfscripts/blob/master/fastx-length.pl&#xD;&#xA; - https://github.com/gringer/bioinfscripts/blob/master/length_plot.r&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/CnECX.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/el6d3.png" />
  <row Id="227" PostHistoryTypeId="2" PostId="85" RevisionGUID="c7e57fe9-251a-409c-8cf3-b08372033f58" CreationDate="2017-05-17T17:55:24.600" UserId="59" Text="If you're ok with some randomness you can generate reads from your sequence file using a Poisson random variable.  You'll need to do some math to figure out what value of lambda to use in order for the expected coverage at each base pair in your read to match what you set in your text file.&#xD;&#xA;&#xD;&#xA;For example you have a sequence S of length 1,000, a read length of 50, and an insert size of 100.  For each base b in S generate a Poisson random variable p.  You will then generate p reads from base b to b+50.  Then, generate the paired read starting at b+50+100.  &#xD;&#xA;&#xD;&#xA;Again, you would have to play with it to figure out what lambda to use but this would give you basically what you want, as long as you're ok with not having exactly the coverage you're targeting for each read." />
  <row Id="228" PostHistoryTypeId="2" PostId="86" RevisionGUID="9d845004-c8b0-4683-b8f3-3ac46f220511" CreationDate="2017-05-17T18:04:16.737" UserId="77" Text="Honestly I'd just use `multiBigwigSummary` and then `plotCorrelation` from deepTools for this, but I'm a bit biased. There, the idea would be to consider each gene as a unit (you could instead use bins, but I don't think that would as nicely do what you want), namely by giving the tools a BED or GTF file input. It would then calculate the average signal in each gene/transcript and you could do your spearman's correlation. Features with 0 in all samples could optionally be removed (`plotCorrelation --skipZeros`).&#xD;&#xA;&#xD;&#xA;While you certainly could go the whole 9 yards and use per-base comparisons, that seems a bit overkill and I suspect that it won't really yield appreciably more information (especially when one considers the additional time overhead)." />
  <row Id="229" PostHistoryTypeId="2" PostId="87" RevisionGUID="9c0bfabd-9e16-4aa9-8c6d-facc07908a56" CreationDate="2017-05-17T18:08:38.830" UserId="161" Text="Rather than working on the base level, you could probably work on say gene level counts. [Kendall's tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient), an ordinal association metric, can then be used as an appropriate correlation measure.&#xD;&#xA;&#xD;&#xA;If $X$ and $Y$ are your iCLIP replicates, $i$ represents gene index and $(x_i, y_i)$ represents the number of RBP binding sites in $X$ and $Y$ respectively for the $i^{th}$ gene, Kendall's tau is defined as :&#xD;&#xA;&#xD;&#xA;$$&#xD;&#xA;\frac{\text{\#(concordant pairs)} - \text{\# (discordant pairs)}}{n(n-1)/2}&#xD;&#xA;$$&#xD;&#xA;&#xD;&#xA;Where any two pairs $(x_i, y_i)$ and $(x_j, y_j)$ are concordant if:&#xD;&#xA;&#xD;&#xA;- $x_i &gt; x_j$ AND $y_i &gt; y_j$&#xD;&#xA;&#xD;&#xA;OR&#xD;&#xA;&#xD;&#xA;- $x_i &lt; x_j$ AND $y_i &lt; y_j$&#xD;&#xA;&#xD;&#xA;Correspondingly they are discordant if:&#xD;&#xA;&#xD;&#xA;- $x_i &lt; x_j$ AND $y_i &gt; y_j$&#xD;&#xA;&#xD;&#xA;OR&#xD;&#xA;&#xD;&#xA;- $x_i &gt; x_j$ AND $y_i &lt; y_j$&#xD;&#xA;" />
  <row Id="230" PostHistoryTypeId="5" PostId="58" RevisionGUID="d914883b-91c2-4cf6-92ba-18f24df64f7d" CreationDate="2017-05-17T18:09:52.143" UserId="40" Comment="added image output" Text="Using Biopython and matplotlib would seem like the way to go, indeed.&#xD;&#xA;It really just boils down to three lines of code to get that graph:&#xD;&#xA;&#xD;&#xA;    import Bio, pandas&#xD;&#xA;    lengths = map(len, Bio.SeqIO.parse('/path/to/the/seqs.fasta', 'fasta'))&#xD;&#xA;    pandas.Series(lengths).hist(color='gray', bins=1000)&#xD;&#xA;&#xD;&#xA;Of course you might want to make a longer script that's callable from the command line, with a couple options. You are welcome to use mine:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python2&#xD;&#xA;&#xD;&#xA;    &quot;&quot;&quot;&#xD;&#xA;    A custom made script to plot the distribution of lengths&#xD;&#xA;    in a fasta file.&#xD;&#xA;&#xD;&#xA;    Written by Lucas Sinclair.&#xD;&#xA;    Kopimi.&#xD;&#xA;&#xD;&#xA;    You can use this script from the shell like this:&#xD;&#xA;    $ ./fastq_length_hist --input seqs.fasta --out seqs.pdf&#xD;&#xA;    &quot;&quot;&quot;&#xD;&#xA;&#xD;&#xA;    ###############################################################################&#xD;&#xA;    # Modules #&#xD;&#xA;    import argparse, sys, time, getpass, locale&#xD;&#xA;    from argparse import RawTextHelpFormatter&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    import pandas&#xD;&#xA;&#xD;&#xA;    # Matplotlib #&#xD;&#xA;    import matplotlib&#xD;&#xA;    matplotlib.use('Agg', warn=False)&#xD;&#xA;    from matplotlib import pyplot&#xD;&#xA;&#xD;&#xA;    ################################################################################&#xD;&#xA;    desc = &quot;fasta_length_hist v1.0&quot;&#xD;&#xA;    parser = argparse.ArgumentParser(description=desc, formatter_class=RawTextHelpFormatter)&#xD;&#xA;&#xD;&#xA;    # All the required arguments #&#xD;&#xA;    parser.add_argument(&quot;--input&quot;, help=&quot;The fasta file to process&quot;, type=str)&#xD;&#xA;    parser.add_argument(&quot;--out&quot;, type=str)&#xD;&#xA;&#xD;&#xA;    # All the optional arguments #&#xD;&#xA;    parser.add_argument(&quot;--x_log&quot;, default=True, type=bool)&#xD;&#xA;    parser.add_argument(&quot;--y_log&quot;, default=True, type=bool)&#xD;&#xA;&#xD;&#xA;    # Parse it #&#xD;&#xA;    args        = parser.parse_args()&#xD;&#xA;    input_path  = args.input&#xD;&#xA;    output_path = args.out&#xD;&#xA;    x_log       = bool(args.x_log)&#xD;&#xA;    y_log       = bool(args.y_log)&#xD;&#xA;&#xD;&#xA;    ################################################################################&#xD;&#xA;    # Read #&#xD;&#xA;    lengths = map(len, SeqIO.parse(input_path, 'fasta'))&#xD;&#xA;&#xD;&#xA;    # Report #&#xD;&#xA;    sys.stderr.write(&quot;Read all lengths (%i sequences)\n&quot; % len(lengths))&#xD;&#xA;    sys.stderr.write(&quot;Longest sequence: %i bp\n&quot; % max(lengths))&#xD;&#xA;    sys.stderr.write(&quot;Shortest sequence: %i bp\n&quot; % min(lengths))&#xD;&#xA;    sys.stderr.write(&quot;Making graph...\n&quot;)&#xD;&#xA;&#xD;&#xA;    # Data #&#xD;&#xA;    values = pandas.Series(lengths)&#xD;&#xA;&#xD;&#xA;    # Plot #&#xD;&#xA;    fig   = pyplot.figure()&#xD;&#xA;    axes  = values.hist(color='gray', bins=1000)&#xD;&#xA;    fig   = pyplot.gcf()&#xD;&#xA;    title = 'Distribution of sequence lengths'&#xD;&#xA;    axes.set_title(title)&#xD;&#xA;    axes.set_xlabel('Number of nucleotides in sequence')&#xD;&#xA;    axes.set_ylabel('Number of sequences with this length')&#xD;&#xA;    axes.xaxis.grid(False)&#xD;&#xA;&#xD;&#xA;    # Log #&#xD;&#xA;    if x_log: axes.set_yscale('symlog')&#xD;&#xA;    if y_log: axes.set_xscale('symlog')&#xD;&#xA;&#xD;&#xA;    # Adjust #&#xD;&#xA;    width=18.0; height=10.0; bottom=0.1; top=0.93; left=0.07; right=0.98&#xD;&#xA;    fig.set_figwidth(width)&#xD;&#xA;    fig.set_figheight(height)&#xD;&#xA;    fig.subplots_adjust(hspace=0.0, bottom=bottom, top=top, left=left, right=right)&#xD;&#xA;&#xD;&#xA;    # Data and source #&#xD;&#xA;    fig.text(0.99, 0.98, time.asctime(), horizontalalignment='right')&#xD;&#xA;    fig.text(0.01, 0.98, 'user: ' + getpass.getuser(), horizontalalignment='left')&#xD;&#xA;&#xD;&#xA;    # Nice digit grouping #&#xD;&#xA;    sep = ('x','y')&#xD;&#xA;    if 'x' in sep:&#xD;&#xA;        locale.setlocale(locale.LC_ALL, '')&#xD;&#xA;        seperate = lambda x,pos: locale.format(&quot;%d&quot;, x, grouping=True)&#xD;&#xA;        axes.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(seperate))&#xD;&#xA;    if 'y' in sep:&#xD;&#xA;        locale.setlocale(locale.LC_ALL, '')&#xD;&#xA;        seperate = lambda x,pos: locale.format(&quot;%d&quot;, x, grouping=True)&#xD;&#xA;        axes.yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(seperate))&#xD;&#xA;&#xD;&#xA;    # Save it #&#xD;&#xA;    fig.savefig(output_path, format='pdf')&#xD;&#xA;&#xD;&#xA;EDIT - an example output:&#xD;&#xA;&#xD;&#xA;[![Sequence length distribution][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/5kmaX.png" />
  <row Id="231" PostHistoryTypeId="2" PostId="88" RevisionGUID="12d35b7b-09b2-42ad-9c50-20f4344cda7b" CreationDate="2017-05-17T18:56:13.670" UserId="74" Text="The &quot;right&quot; solution would be realignment, but that's expensive and most of us would not go that route. My preferred solution would be to convert the bed file, as opposed to the bam. Here's why:&#xD;&#xA;&#xD;&#xA;1) Reheadering the bam means that you may have reads aligned to contigs without a corresponding entry in UCSC (see [Devon's list for the mappings](https://github.com/dpryan79/ChromosomeMappings/blob/master/GRCh37_NCBI2UCSC.txt)). This is a problem because:&#xD;&#xA;&#xD;&#xA; - Some of those reads would likely have been mapped elsewhere if a reference without those contigs was used. &#xD;&#xA; - I'm not even sure what happens to those reads after reheadering - I guess they would need to be marked as unmapped? Lots of potential for screwiness there.&#xD;&#xA;&#xD;&#xA;2) It seems cleaner to convert the bed file from UCSC-&gt;NCBI, where you are guaranteed that every entry has a &quot;home&quot;. Then, after you pull your info from the bam, you can always convert chromosome names back if you need to." />
  <row Id="232" PostHistoryTypeId="2" PostId="89" RevisionGUID="8a67b5a6-d3cf-4fb1-bd4f-c862e41ff30c" CreationDate="2017-05-17T19:20:51.663" UserId="83" Text="I have SNP data from several cultivars of rice which I have used to produce alignments, but I don't think that the usual models and algorithms used for generating phylogenetic trees are appropriate, because these cultivars are not the result of speciation events and have been interbred in their histories. How can I best calculate and visualize their degree of relatedness?" />
  <row Id="233" PostHistoryTypeId="1" PostId="89" RevisionGUID="8a67b5a6-d3cf-4fb1-bd4f-c862e41ff30c" CreationDate="2017-05-17T19:20:51.663" UserId="83" Text="How does one construct a cladogram of intraspecies relationships?" />
  <row Id="234" PostHistoryTypeId="3" PostId="89" RevisionGUID="8a67b5a6-d3cf-4fb1-bd4f-c862e41ff30c" CreationDate="2017-05-17T19:20:51.663" UserId="83" Text="&lt;phylogeny&gt;&lt;cladistics&gt;" />
  <row Id="235" PostHistoryTypeId="2" PostId="90" RevisionGUID="3222737f-1e64-4e5e-b8e3-3e9cafa2be04" CreationDate="2017-05-17T20:18:09.070" UserId="177" Text="It depends whether you want to treat the peak intensities as binary (comparing presence/absence of peaks in the sets) or continuous (comparing the relative magnitudes of the peaks).&#xD;&#xA;&#xD;&#xA;**Binary**&#xD;&#xA;&#xD;&#xA;For starting out, a simple binary comparison may be appropriate. You can use a [peak caller](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0096303) of your choosing to identify peaks in each sample according to your desired criteria. Then you can use a similarity metric such as the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) to quantify the level of agreement among the peaks in the two samples.&#xD;&#xA;&#xD;&#xA;One potential obstacle is that defining the boundaries of your peaks won't be totally straightforward. For example, a peak in one sample might have 2 overlapping peaks in the other sample, one on each end. A rough solution for this is to divide the genome into bins (maybe around 100-1000 bp, depending on your desired resolution). You can treat a peak as present in a bin if more than half of the peak lies in the bin. That way, bins in one sample can be directly compared to the corresponding bins in the other sample. Obviously, this isn't the only way to do this; other appropriate methods exist too.&#xD;&#xA;&#xD;&#xA;**Continuous**&#xD;&#xA;&#xD;&#xA;If you want to treat the peak intensities as continuous, you could apply a similar binning method, taking the &quot;score&quot; of a bin to be the average peak intensity at positions within that bin. You could then throw away all bins with no peaks or only low-intensity peaks throughout the genome. Then you could compute the Spearman's correlation for the remaining bins. I'm guessing it will be harder to find a strong correlation for continuous intensities, because of the amount of experimental variability that is inherently present.&#xD;&#xA;&#xD;&#xA;If, after following these steps, the Spearman's correlation is still &quot;artificially low&quot; as you suggested, then this is likely a problem with the underlying data, not the overall analysis; maybe your two datasets actually don't agree that well." />
  <row Id="236" PostHistoryTypeId="5" PostId="65" RevisionGUID="e8ab857f-afd8-4f91-a4d8-254d8128941f" CreationDate="2017-05-17T20:38:25.827" UserId="29" Comment="Fix bit shifting width" Text="At its easiest, you just store the *forward* (*F*) and *reverse* (*R*) hash value.&#xD;&#xA;&#xD;&#xA;You update the forward hash value by conventional means, e.g. bit-shifting the base value into its lower bits:&#xD;&#xA;&#xD;&#xA;*F*&lt;sub&gt;*n*+1&lt;/sub&gt; = ((*F*&lt;sub&gt;*n*&lt;/sub&gt; ≪ *B*) | *x*) &amp; *M*,&#xD;&#xA;&#xD;&#xA;*B* is the bit size of the base encoding, *M* is the word mask for a word of length *W* bits, and can be omitted if the width of the hash is the width of the machine word; and the reverse hash value by doing the mathematical inverse, e.g. bit-shifting the value of the  reverse complement of the base into the upper bits:&#xD;&#xA;&#xD;&#xA;*R*&lt;sub&gt;*n*+1&lt;/sub&gt; = ((*R*&lt;sub&gt;*n*&lt;/sub&gt; ≫ *B*) | (compl(*x*) ≪ (*W*−*B*))) &amp; *M*.&#xD;&#xA;&#xD;&#xA;And then you compute a *combined hash* value by xoring the two hashes, *H* = *F* ⊕ *R*." />
  <row Id="238" PostHistoryTypeId="6" PostId="89" RevisionGUID="802b3f2c-d449-49d0-8756-c1be4ae03bac" CreationDate="2017-05-18T00:00:52.860" UserId="131" Comment="added snp tag" Text="&lt;phylogeny&gt;&lt;cladistics&gt;&lt;snp&gt;" />
  <row Id="239" PostHistoryTypeId="24" PostId="89" RevisionGUID="802b3f2c-d449-49d0-8756-c1be4ae03bac" CreationDate="2017-05-18T00:00:52.860" Comment="Proposed by 131 approved by 55 edit id of 20" />
  <row Id="240" PostHistoryTypeId="5" PostId="59" RevisionGUID="92457555-ad62-40c3-840f-e1c6f04acb7a" CreationDate="2017-05-18T00:01:14.437" UserId="141" Comment="Removed reference to a comment to make the answer self contained." Text="First of all, I would emphasize that &quot;alignment-free&quot; quantification tools like Salmon and Kallisto are *not* reference-free. The basic difference between them and more traditional aligners is that they do not report a specific position (either in a genome or transcriptome) to which a read maps. However, their overall purpose is still to quantify the expression levels (or differences) of a known set of transcripts; hence, they require a reference (which could be arbitrarily defined).&#xD;&#xA;&#xD;&#xA;The most important criterion for deciding which approach to use (and this is true of almost everything in genomics) is exactly what question you would like to answer. If you are primarily interested in quantifying and comparing expression of mature mRNA from known transcripts, then a transcriptome-based alignment may be fastest and best. However, you may miss potentially interesting features outside of those known transcripts, such as new isoforms, non-coding RNAs, or information about pre-mRNA levels, which can often be gleaned from intronic reads (see the [EISA](https://www.nature.com/nbt/journal/v33/n7/full/nbt.3269.html) method).&#xD;&#xA;&#xD;&#xA;[This paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4729156/) also has some good considerations about which tools may work best depending on the question you want to answer.&#xD;&#xA;&#xD;&#xA;Finally, another fast and flexible aligner (which can be used with or without a reference transcriptome) is [STAR](https://github.com/alexdobin/STAR)." />
  <row Id="241" PostHistoryTypeId="24" PostId="59" RevisionGUID="92457555-ad62-40c3-840f-e1c6f04acb7a" CreationDate="2017-05-18T00:01:14.437" Comment="Proposed by 141 approved by 55 edit id of 17" />
  <row Id="242" PostHistoryTypeId="5" PostId="42" RevisionGUID="afd93008-dcdc-4eab-92f9-d6f5fcff5a12" CreationDate="2017-05-18T00:01:18.567" UserId="163" Comment="fix typo on name of NGM-LR software (previously LGM-LR) and add hyperlinks." Text="Qualimap will do this for you. &#xD;&#xA;&#xD;&#xA;1. Go to [qualimap.bioinfo.cipf.es][1]&#xD;&#xA;2. Run qualimap (default params are fine) on each BAM file&#xD;&#xA;3. Open up the HTML output, and you can read off the %identity (they measure the opposite, i.e. mismatch rate, but 100% - mismatch rate is %identity of course), indel rate, etc.&#xD;&#xA;&#xD;&#xA;One thing to watch out for (you don't mention it in your question, but just in case) is that you cannot directly compare Q scores - these are a bit of a mess and calculated very differently in each piece of software. &#xD;&#xA;&#xD;&#xA;Unsolicited suggestion: you might also try [NGM-LR][2] for mapping MinION data. We've found it beats the others for our data (though we map to a distant reference).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://qualimap.bioinfo.cipf.es&#xD;&#xA;  [2]: https://github.com/philres/ngmlr" />
  <row Id="243" PostHistoryTypeId="24" PostId="42" RevisionGUID="afd93008-dcdc-4eab-92f9-d6f5fcff5a12" CreationDate="2017-05-18T00:01:18.567" Comment="Proposed by 163 approved by 55 edit id of 8" />
  <row Id="244" PostHistoryTypeId="5" PostId="12" RevisionGUID="63a89d41-f2ff-4f82-9589-99bb10c5a05d" CreationDate="2017-05-18T00:01:22.427" UserId="48" Comment="Improved formatting of the list, removing greetings" Text="I'm looking to dock a large ligand (~90kDa) to a receptor slightly larger receptor  (~125kDa) using Hex. If anyone is familiar with docking large structures, are there any recommended parameters for finding the best docking solution?&#xD;&#xA;&#xD;&#xA;Parameters in particular:&#xD;&#xA;&#xD;&#xA; - Number of Solutions  &#xD;&#xA; - N order of correlation for initial and final&#xD;&#xA;   searches &#xD;&#xA; - Receptor Range &#xD;&#xA; - Ligand Range" />
  <row Id="245" PostHistoryTypeId="24" PostId="12" RevisionGUID="63a89d41-f2ff-4f82-9589-99bb10c5a05d" CreationDate="2017-05-18T00:01:22.427" Comment="Proposed by 48 approved by 55 edit id of 10" />
  <row Id="246" PostHistoryTypeId="6" PostId="81" RevisionGUID="440a853c-0124-44b4-a3c2-921edc2b6272" CreationDate="2017-05-18T00:01:25.537" UserId="59" Comment="Added tags" Text="&lt;genome&gt;&lt;ngs&gt;&lt;simulation&gt;" />
  <row Id="247" PostHistoryTypeId="24" PostId="81" RevisionGUID="440a853c-0124-44b4-a3c2-921edc2b6272" CreationDate="2017-05-18T00:01:25.537" Comment="Proposed by 59 approved by 55 edit id of 19" />
  <row Id="248" PostHistoryTypeId="4" PostId="15" RevisionGUID="099178e0-344a-4dbe-aba9-d34405c1816a" CreationDate="2017-05-18T00:01:30.063" UserId="134" Comment="Fix title typo" Text="Difference between BWA-backtrack and BWA-MEM" />
  <row Id="249" PostHistoryTypeId="24" PostId="15" RevisionGUID="099178e0-344a-4dbe-aba9-d34405c1816a" CreationDate="2017-05-18T00:01:30.063" Comment="Proposed by 134 approved by 55 edit id of 6" />
  <row Id="250" PostHistoryTypeId="5" PostId="47" RevisionGUID="68e9d19c-d8dd-45fd-9527-62add753c9a7" CreationDate="2017-05-18T00:01:35.137" UserId="141" Comment="Removed requirement of genes to be protein coding." Text="In brief, the  “genome”  is the collection of all  DNA  present  in  the  nucleus  and  the  mitochondria of a  somatic  cell. The initial product of genome expression is the “transcriptome”, a collection of RNA molecules derived from those genes." />
  <row Id="251" PostHistoryTypeId="24" PostId="47" RevisionGUID="68e9d19c-d8dd-45fd-9527-62add753c9a7" CreationDate="2017-05-18T00:01:35.137" Comment="Proposed by 141 approved by 55 edit id of 13" />
  <row Id="252" PostHistoryTypeId="5" PostId="72" RevisionGUID="289073e2-cfd8-4463-936b-c1c7457cc003" CreationDate="2017-05-18T00:02:05.573" UserId="131" Comment="added 339 characters in body" Text="R is a free, open-source programming language and software environment for statistical computing, bioinformatics, and graphics. Please supplement your question with a minimal reproducible example. Use dput() for data and specify all non-base packages with library calls. For statistical questions please use http://stats.stackexchange.com." />
  <row Id="253" PostHistoryTypeId="24" PostId="72" RevisionGUID="289073e2-cfd8-4463-936b-c1c7457cc003" CreationDate="2017-05-18T00:02:05.573" Comment="Proposed by 131 approved by 55 edit id of 16" />
  <row Id="254" PostHistoryTypeId="5" PostId="30" RevisionGUID="26323d33-ab4c-49e6-8840-29b21ad6a36e" CreationDate="2017-05-18T00:02:17.593" UserId="8" Comment="added 298 characters in body" Text="The [FASTA format][1] has a single-line definition (defline), followed by one or more lines of DNA, RNA or amino acid sequences. The definition line is specified by the greater than (`&gt;`) character.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=BlastHelp" />
  <row Id="255" PostHistoryTypeId="24" PostId="30" RevisionGUID="26323d33-ab4c-49e6-8840-29b21ad6a36e" CreationDate="2017-05-18T00:02:17.593" Comment="Proposed by 8 approved by 55 edit id of 4" />
  <row Id="256" PostHistoryTypeId="5" PostId="80" RevisionGUID="c6c4914b-6400-497c-91ea-f7e6ca205e70" CreationDate="2017-05-18T00:02:25.597" UserId="131" Comment="added link to survival bias" Text="There are two potential sources of bias in this design.&#xD;&#xA;&#xD;&#xA;1. We cannot distinguish correlation from causation. &#xD;&#xA;&#xD;&#xA;Imagine two cases. In the first, the disease progression is inducing immune response. Later stages will be associated with the higher gene expression levels. In the second scenario, the disease is caused by overexpression of a gene. Later stages will be also associated with the higher expression.&#xD;&#xA;&#xD;&#xA;This is typical for observational studies. But I just want to mention that a special care should be taken during interpretation of the results.&#xD;&#xA;&#xD;&#xA;2. If we are not following our individuals, we cannot distinguish correlation and avoidance.&#xD;&#xA;&#xD;&#xA;Let's say the disease is lethal in certain cases; the survival is negatively correlated with the disease stage. Now imagine there is a gene, which causes severe symptoms when highly expressed. On the later stages you will only observe those patients in which the gene was not highly expressed. From that you would conclude that gene expression is decreasing with the disease progression. In reality this gene is very important and causal, you just do not have patients which are alive to see this.&#xD;&#xA;&#xD;&#xA;This is similar to [Wald's studies of aircrafts][1] ([Survival bias](https://en.wikipedia.org/wiki/Survivorship_bias)).&#xD;&#xA;&#xD;&#xA;&gt; Researchers from the Center for Naval Analyses had conducted a study&#xD;&#xA;&gt; of the damage done to aircraft that had returned from missions, and&#xD;&#xA;&gt; had recommended that armor be added to the areas that showed the most&#xD;&#xA;&gt; damage.&#xD;&#xA;&gt; Wald proposed that the Navy instead reinforce the areas where the&#xD;&#xA;&gt; returning aircraft were unscathed, since those were the areas that, if&#xD;&#xA;&gt; hit, would cause the plane to be lost.&#xD;&#xA;&#xD;&#xA;I think the second point is crucial, and can and will lead to false conclusions. &#xD;&#xA;&#xD;&#xA;I suggest that the same individuals are followed for a long time.&#xD;&#xA;&#xD;&#xA;There are different approaches you can use later. For example you can have two-stage procedure:&#xD;&#xA;&#xD;&#xA;1. Identify genes which are differentially expressed between healthy (H) and sick (A, B or C).&#xD;&#xA;2. Build a linear model of disease stage stage ~ gene1 + gene2 + ..., using genes identified at step 1.&#xD;&#xA;3. Similarly build a linear model of survival as a function gene expression.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Abraham_Wald&#xD;&#xA;&#xD;&#xA;" />
  <row Id="257" PostHistoryTypeId="24" PostId="80" RevisionGUID="c6c4914b-6400-497c-91ea-f7e6ca205e70" CreationDate="2017-05-18T00:02:25.597" Comment="Proposed by 131 approved by 55 edit id of 21" />
  <row Id="258" PostHistoryTypeId="5" PostId="71" RevisionGUID="f95dcf3b-7c9f-4874-9f22-5c4318b09cba" CreationDate="2017-05-18T00:04:34.707" UserId="131" Comment="added 1199 characters in body" Text="# R Programming Language&#xD;&#xA;&#xD;&#xA;[R][1] is a free, open-source programming language and software environment for [statistical computing][2], [bioinformatics][3], and [graphics][4]. It is a multi-paradigm language and dynamically typed. R is an implementation of the [S programming language][5] combined with lexical scoping semantics inspired by [Scheme][6]. R was created by [Ross Ihaka][7] and [Robert Gentleman][8] and is now developed by the [R Development Core Team][9]. The R environment is easily extended through a packaging system on [CRAN][10], the Comprehensive R Archive Network. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.r-project.org &quot;R project homepage&quot;&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Computational_statistics&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/Bioinformatics&#xD;&#xA;  [4]: https://en.wikipedia.org/wiki/Graphics&#xD;&#xA;  [5]: http://lib.stat.cmu.edu/S/Spoetry/Tutor/slanguage.html&#xD;&#xA;  [6]: http://stackoverflow.com/tags/scheme/info &quot;Scheme on StackOverflow&quot;&#xD;&#xA;  [7]: http://www.stat.auckland.ac.nz/~ihaka/&#xD;&#xA;  [8]: https://en.wikipedia.org/wiki/Robert_Gentleman_%28statistician%29&#xD;&#xA;  [9]: http://www.r-project.org/contributors.html&#xD;&#xA;  [10]: http://cran.r-project.org &quot;CRAN The Comprehensive R Archive Network&quot;&#xD;&#xA;" />
  <row Id="259" PostHistoryTypeId="24" PostId="71" RevisionGUID="f95dcf3b-7c9f-4874-9f22-5c4318b09cba" CreationDate="2017-05-18T00:04:34.707" Comment="Proposed by 131 approved by 55 edit id of 15" />
  <row Id="260" PostHistoryTypeId="2" PostId="91" RevisionGUID="a86dc1be-c84f-4a42-83de-2922fd76f75f" CreationDate="2017-05-18T00:14:58.347" UserId="174" Text="I have a FASTA file:&#xD;&#xA;&#xD;&#xA;    &gt; Sequence_1&#xD;&#xA;    GCAATGCAAGGAAGTGATGGCGGAAATAGCGTTAGATGTATGTGTAGCGGTCCC...&#xD;&#xA;    &gt; Sequence_2&#xD;&#xA;    GCAATGCAAGGAAGTGATGGCGGAAATAGCGTTAGATGTATGTGTAGCGGTCCC....&#xD;&#xA;    ....&#xD;&#xA;&#xD;&#xA;I want to generate a BED file for each sequence like:&#xD;&#xA;    &#xD;&#xA;    Sequence_1 0 1500&#xD;&#xA;    Sequence_2 0 1700&#xD;&#xA;&#xD;&#xA;The BED regions will simply be the size of the sequences.&#xD;&#xA;&#xD;&#xA;**Q:** I did that before with a one-line command. I don't remember what that is, it was on Biostars. I can't find the post now. What's the simplest way to do the conversion?" />
  <row Id="261" PostHistoryTypeId="1" PostId="91" RevisionGUID="a86dc1be-c84f-4a42-83de-2922fd76f75f" CreationDate="2017-05-18T00:14:58.347" UserId="174" Text="How to convert FASTA to BED" />
  <row Id="262" PostHistoryTypeId="3" PostId="91" RevisionGUID="a86dc1be-c84f-4a42-83de-2922fd76f75f" CreationDate="2017-05-18T00:14:58.347" UserId="174" Text="&lt;bed&gt;" />
  <row Id="263" PostHistoryTypeId="2" PostId="92" RevisionGUID="b614396d-93fd-4da5-9c94-8a648e7dee6c" CreationDate="2017-05-18T00:28:24.290" UserId="163" Text="I am working with a set of (bulk) RNA-Seq data collected across multiple runs, run at different times of the year. I have normalized my data using library size / quantile / RUV normalization, and would like to check (quantitatively and/or qualitatively) whether or not normalization has succeeded in removing the batch effects. &#xD;&#xA;&#xD;&#xA;It is important to note that by &quot;normalization has succeeded&quot;, I simply mean that unwanted variation has been removed - further analysis is required to ensure that biological variation has not been removed. What are some plots / statistical tests / software packages which provide a first-step QC for normalization?" />
  <row Id="264" PostHistoryTypeId="1" PostId="92" RevisionGUID="b614396d-93fd-4da5-9c94-8a648e7dee6c" CreationDate="2017-05-18T00:28:24.290" UserId="163" Text="Confirm success or failure RNA-Seq normalization" />
  <row Id="265" PostHistoryTypeId="3" PostId="92" RevisionGUID="b614396d-93fd-4da5-9c94-8a648e7dee6c" CreationDate="2017-05-18T00:28:24.290" UserId="163" Text="&lt;rna-seq&gt;&lt;normalization&gt;" />
  <row Id="266" PostHistoryTypeId="2" PostId="93" RevisionGUID="aa3d5815-6228-4e1b-b857-d2c8280ab0f7" CreationDate="2017-05-18T00:49:44.233" UserId="174" Text="You should use box plots and PCA plot. Let's take a look at the RUV paper:&#xD;&#xA;&#xD;&#xA;&gt; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4404308/&#xD;&#xA;&#xD;&#xA;**Before normalization and after UQ normalization**:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&gt; Libraries do not cluster as expected according to treatment. ... for UQ-normalized counts. UQ normalization does not lead to better clustering of the samples...&#xD;&#xA;&#xD;&#xA;Before normalization, the medians in the box-plot obviously look very different among replicates.&#xD;&#xA;&#xD;&#xA;After UQ normalization, the medians look closer but Trt.11 look like an outlier. Furthermore, the treatments aren't clustered on the PCA plot. Since they are replicates, you'd like them be close on the plot.&#xD;&#xA;&#xD;&#xA;**After RUV normalization**&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;&gt; ... RUVg shrinks the expression measures for Library 11 towards the median across libraries, suggesting robustness against outliers. ... Libraries cluster as expected by treatment. ...&#xD;&#xA;&#xD;&#xA;The RUV has made the distribution more robust and the samples closer on the PCA plot. However, it's still not perfect as one of the treatments is not close to the other two on the first PC.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/aZBua.jpg&#xD;&#xA;  [2]: https://i.stack.imgur.com/m4tYK.png" />
  <row Id="267" PostHistoryTypeId="5" PostId="93" RevisionGUID="115c3944-742b-48c5-9c25-4bea5674a401" CreationDate="2017-05-18T00:55:59.730" UserId="174" Comment="added 97 characters in body" Text="You should use box plots and PCA plot. Let's take a look at the RUV paper:&#xD;&#xA;&#xD;&#xA;&gt; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4404308/&#xD;&#xA;&#xD;&#xA;**Before normalization and after UQ normalization**:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&gt; Libraries do not cluster as expected according to treatment. ... for UQ-normalized counts. UQ normalization does not lead to better clustering of the samples...&#xD;&#xA;&#xD;&#xA;Before normalization, the medians in the box-plot obviously look very different among replicates.&#xD;&#xA;&#xD;&#xA;After UQ normalization, the medians look closer but Trt.11 look like an outlier. Furthermore, the treatments aren't clustered on the PCA plot. Since they are replicates, you'd like them be close on the plot.&#xD;&#xA;&#xD;&#xA;**After RUV normalization**&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;&gt; ... RUVg shrinks the expression measures for Library 11 towards the median across libraries, suggesting robustness against outliers. ... Libraries cluster as expected by treatment. ...&#xD;&#xA;&#xD;&#xA;The RUV has made the distribution more robust and the samples closer on the PCA plot. However, it's still not perfect as one of the treatments is not close to the other two on the first PC.&#xD;&#xA;&#xD;&#xA;The vignettes for Bioconductor `RUVSeq` describes the two functions: `plotRLE` and `plotPCA`.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/aZBua.jpg&#xD;&#xA;  [2]: https://i.stack.imgur.com/m4tYK.png" />
  <row Id="268" PostHistoryTypeId="2" PostId="94" RevisionGUID="6b001a16-9c91-4300-84f0-069ed53a8b4c" CreationDate="2017-05-18T01:01:14.320" UserId="150" Text="You could adapt this [awk one-liner][1]. Note that it assumes that sequence IDs are not longer than 100 characters and that there is no description following the sequence ID on the header line.&#xD;&#xA;&#xD;&#xA;    cat myseqs.fasta | awk '$0 ~ &quot;&gt;&quot; {print c; c=0;printf substr($0,2,100) &quot;\t0\t&quot;; } $0 !~ &quot;&gt;&quot; {c+=length($0);} END { print c; }'&#xD;&#xA;&#xD;&#xA;Otherwise, any Bio* library (Perl, Python, Ruby) provides FASTA format parsers which will extract sequence IDs and lengths.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.danielecook.com/generate-fasta-sequence-lengths/" />
  <row Id="269" PostHistoryTypeId="2" PostId="95" RevisionGUID="2e34cd9d-1cad-440e-8250-326774bbaec0" CreationDate="2017-05-18T01:04:48.900" UserId="73" Text="If the FASTA sequences are all on a single line, then the following perl one-liner should work:&#xD;&#xA;&#xD;&#xA;    cat myseqs.fasta | perl -ne 'if(/^&gt;([^ ]+)/){print $1} else {print &quot; 0 &quot;,length,&quot;\n&quot;}'&#xD;&#xA;&#xD;&#xA;Explanation:&#xD;&#xA;&#xD;&#xA; - If the line begins with a '&gt;', then print everything up to the first space (but don't put a line break at the end)&#xD;&#xA; - Otherwise, print &quot; 0 &quot;, followed by the length of the line, followed by a line break" />
  <row Id="270" PostHistoryTypeId="5" PostId="94" RevisionGUID="3f7a8b54-2489-436c-996c-c8113544f391" CreationDate="2017-05-18T01:07:42.050" UserId="150" Comment="added 156 characters in body" Text="You could adapt this [awk one-liner][1]. Note that it assumes that sequence IDs are not longer than 100 characters and that there is no description following the sequence ID on the header line.&#xD;&#xA;&#xD;&#xA;    cat myseqs.fasta | awk '$0 ~ &quot;&gt;&quot; {print c; c=0;printf substr($0,2,100) &quot;\t0\t&quot;; } $0 !~ &quot;&gt;&quot; {c+=length($0);} END { print c; }'&#xD;&#xA;&#xD;&#xA;Otherwise, any Bio* library (Perl, Python, Ruby) provides FASTA format parsers which will extract sequence IDs and lengths.&#xD;&#xA;&#xD;&#xA;I'd point out that whilst this resembles BED it is not, strictly-speaking, since BED maps to coordinates on a chromosome or some longer sequence object.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.danielecook.com/generate-fasta-sequence-lengths/" />
  <row Id="271" PostHistoryTypeId="2" PostId="96" RevisionGUID="f592b0b6-21bb-4846-aacb-36b0b0b41d22" CreationDate="2017-05-18T01:08:32.233" UserId="163" Text="Inspired by [this answer][1] to a related question on read length distributions, you could do this with Biopython:&#xD;&#xA;&#xD;&#xA;    import Bio.SeqIO&#xD;&#xA;    output = map(lambda x: '\t'.join([x.id, '0', str(len(x))]) + '\n', Bio.SeqIO.parse('/path/to/seqs.fasta', 'fasta'))&#xD;&#xA;    with open('sequence_lengths.bed', 'w') as f:&#xD;&#xA;        f.writelines(output)&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/45/read-length-distribution-from-fasta-file/84#84" />
  <row Id="273" PostHistoryTypeId="5" PostId="96" RevisionGUID="38167c67-6955-4258-8bc8-5a21c45158b3" CreationDate="2017-05-18T02:45:36.133" UserId="163" Comment="clean up code" Text="Inspired by [this answer][1] to a related question on read length distributions, you could do this with Biopython:&#xD;&#xA;&#xD;&#xA;    import Bio.SeqIO&#xD;&#xA;    fasta = Bio.SeqIO.parse('/path/to/seqs.fasta', 'fasta')&#xD;&#xA;    output = map(lambda x: '{id}\t0\t{length}\n'.format(id=x.id, length=len(x)), fasta)&#xD;&#xA;    with open('sequence_lengths.bed', 'w') as f:&#xD;&#xA;        f.writelines(output)&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/45/read-length-distribution-from-fasta-file/84#84" />
  <row Id="274" PostHistoryTypeId="2" PostId="97" RevisionGUID="c3400515-58be-405f-8a4d-425370370188" CreationDate="2017-05-18T03:11:37.330" UserId="163" Text="I have a DNA sample which I know doesn't *quite* match my reference genome. From visual inspection with IGV, a significant number of both SNPs and SVs appear to be present, but an assembly built entirely from my own sequencing data is not high enough quality for my purposes. &#xD;&#xA;&#xD;&#xA;How can I polish this draft genome with new sequencing data (preferably with Oxford Nanopore Technologies long reads, but interested in short read methods too), taking advantage of my knowledge that the draft genome is *mostly* very good, without accessing the reads which were originally used to construct the draft genome?" />
  <row Id="275" PostHistoryTypeId="1" PostId="97" RevisionGUID="c3400515-58be-405f-8a4d-425370370188" CreationDate="2017-05-18T03:11:37.330" UserId="163" Text="Polish a reference genome with sequencing data" />
  <row Id="276" PostHistoryTypeId="3" PostId="97" RevisionGUID="c3400515-58be-405f-8a4d-425370370188" CreationDate="2017-05-18T03:11:37.330" UserId="163" Text="&lt;genome&gt;&lt;dna&gt;&lt;assembly&gt;" />
  <row Id="277" PostHistoryTypeId="2" PostId="98" RevisionGUID="9e6c5532-f784-43b6-87be-0d994fa6408f" CreationDate="2017-05-18T03:14:29.460" UserId="274" Text="After DNA sequencing, I generated a sam file through alignment of a fastq file. &#xD;&#xA;Before using well known variant calling programs (eg. Annovar etc.), I want to pick some reads and know what kinds of mutations are there.&#xD;&#xA;&#xD;&#xA;Is there any quick way of doing it?" />
  <row Id="278" PostHistoryTypeId="1" PostId="98" RevisionGUID="9e6c5532-f784-43b6-87be-0d994fa6408f" CreationDate="2017-05-18T03:14:29.460" UserId="274" Text="How to quickly determine mutations in a read of a sam file?" />
  <row Id="279" PostHistoryTypeId="3" PostId="98" RevisionGUID="9e6c5532-f784-43b6-87be-0d994fa6408f" CreationDate="2017-05-18T03:14:29.460" UserId="274" Text="&lt;variant-calling&gt;" />
  <row Id="280" PostHistoryTypeId="2" PostId="99" RevisionGUID="4161a705-a9dc-460e-aed1-f137f0a93628" CreationDate="2017-05-18T03:24:19.127" UserId="163" Text="For qualitative analysis, you're probably better off using something less granular like [IGV][1] or [IGB][2]. However, if you really want to look at a couple of reads:&#xD;&#xA;&#xD;&#xA;If you're willing to ignore sequencing errors, you can inspect the CIGAR string or the MD tag, both of which give information on the alignment of a single read.&#xD;&#xA;&#xD;&#xA;The CIGAR string gives details on insertions, deletions, and clipping (and sometimes mismatches.) From [Genome Analysis Wiki][3],&#xD;&#xA;&#xD;&#xA;&gt; The sequence being aligned to a reference may have additional bases that are not in the reference or may be missing bases that are in the reference. The CIGAR string is a sequence of of base lengths and the associated operation. They are used to indicate things like which bases align (either a match/mismatch) with the reference, are deleted from the reference, and are insertions that are not in the reference. For example:&#xD;&#xA;&#xD;&#xA;    RefPos:     1  2  3  4  5  6  7     8  9 10 11 12 13 14 15 16 17 18 19&#xD;&#xA;    Reference:  C  C  A  T  A  C  T     G  A  A  C  T  G  A  C  T  A  A  C&#xD;&#xA;    Read:                   A  C  T  A  G  A  A     T  G  G  C  T&#xD;&#xA;    With the alignment above, you get:&#xD;&#xA;    POS: 5&#xD;&#xA;    CIGAR: 3M1I3M1D5M&#xD;&#xA;&#xD;&#xA;Most common usage of the CIGAR string uses M (match/mismatch), I (insertion), D (deletion), S (soft clipping), and H (hard clipping).&#xD;&#xA;&#xD;&#xA;The MD tag gives specific details on mismatches and deletions. From the [SAMtools tags specification][4],&#xD;&#xA;&#xD;&#xA;&gt; The MD field aims to achieve SNP/indel calling without looking at the reference. For example, a string ‘10A5^AC6’ means from the leftmost reference base in the alignment, there are 10 matches followed by an A on the reference which is different from the aligned read base; the next 5 reference bases are matches followed by a 2bp deletion from the reference; the deleted sequence is AC; the last 6 bases are matches. The MD field ought to match the CIGAR string.&#xD;&#xA;&#xD;&#xA;Note that neither of these will give you any idea of structural variants in short reads, and neither will be particularly readable (or helpful, due to the higher error rate) in long reads.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://software.broadinstitute.org/software/igv/&#xD;&#xA;  [2]: http://bioviz.org/igb/overview.html&#xD;&#xA;  [3]: http://genome.sph.umich.edu/wiki/SAM&#xD;&#xA;  [4]: https://samtools.github.io/hts-specs/SAMtags.pdf" />
  <row Id="281" PostHistoryTypeId="5" PostId="99" RevisionGUID="9394d325-7277-47a5-a9c0-7f0950c13981" CreationDate="2017-05-18T03:33:12.387" UserId="5" Comment="CIGAR string records matches and mismatches always, not sometimes" Text="For qualitative analysis, you're probably better off using something less granular like [IGV][1] or [IGB][2]. However, if you really want to look at a couple of reads:&#xD;&#xA;&#xD;&#xA;If you're willing to ignore sequencing errors, you can inspect the CIGAR string or the MD tag, both of which give information on the alignment of a single read.&#xD;&#xA;&#xD;&#xA;The CIGAR string gives details on insertions, deletions, clippings, matches and mismatches. From [Genome Analysis Wiki][3],&#xD;&#xA;&#xD;&#xA;&gt; The sequence being aligned to a reference may have additional bases that are not in the reference or may be missing bases that are in the reference. The CIGAR string is a sequence of of base lengths and the associated operation. They are used to indicate things like which bases align (either a match/mismatch) with the reference, are deleted from the reference, and are insertions that are not in the reference. For example:&#xD;&#xA;&#xD;&#xA;    RefPos:     1  2  3  4  5  6  7     8  9 10 11 12 13 14 15 16 17 18 19&#xD;&#xA;    Reference:  C  C  A  T  A  C  T     G  A  A  C  T  G  A  C  T  A  A  C&#xD;&#xA;    Read:                   A  C  T  A  G  A  A     T  G  G  C  T&#xD;&#xA;    With the alignment above, you get:&#xD;&#xA;    POS: 5&#xD;&#xA;    CIGAR: 3M1I3M1D5M&#xD;&#xA;&#xD;&#xA;Most common usage of the CIGAR string uses M (match/mismatch), I (insertion), D (deletion), S (soft clipping), and H (hard clipping).&#xD;&#xA;&#xD;&#xA;The MD tag gives specific details on mismatches and deletions. From the [SAMtools tags specification][4],&#xD;&#xA;&#xD;&#xA;&gt; The MD field aims to achieve SNP/indel calling without looking at the reference. For example, a string ‘10A5^AC6’ means from the leftmost reference base in the alignment, there are 10 matches followed by an A on the reference which is different from the aligned read base; the next 5 reference bases are matches followed by a 2bp deletion from the reference; the deleted sequence is AC; the last 6 bases are matches. The MD field ought to match the CIGAR string.&#xD;&#xA;&#xD;&#xA;Note that neither of these will give you any idea of structural variants in short reads, and neither will be particularly readable (or helpful, due to the higher error rate) in long reads.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://software.broadinstitute.org/software/igv/&#xD;&#xA;  [2]: http://bioviz.org/igb/overview.html&#xD;&#xA;  [3]: http://genome.sph.umich.edu/wiki/SAM&#xD;&#xA;  [4]: https://samtools.github.io/hts-specs/SAMtags.pdf" />
  <row Id="282" PostHistoryTypeId="24" PostId="99" RevisionGUID="9394d325-7277-47a5-a9c0-7f0950c13981" CreationDate="2017-05-18T03:33:12.387" Comment="Proposed by 5 approved by -1 edit id of 22" />
  <row Id="283" PostHistoryTypeId="5" PostId="99" RevisionGUID="dbd5bc8a-de0e-49a8-87a5-547f4fc737b5" CreationDate="2017-05-18T03:33:12.387" UserId="163" Comment="CIGAR string records matches and mismatches always, not sometimes (except not really)" Text="For qualitative analysis, you're probably better off using something less granular like [IGV][1] or [IGB][2]. However, if you really want to look at a couple of reads:&#xD;&#xA;&#xD;&#xA;If you're willing to ignore sequencing errors, you can inspect the CIGAR string or the MD tag, both of which give information on the alignment of a single read.&#xD;&#xA;&#xD;&#xA;The CIGAR string gives details on insertions, deletions, clippings, matches and mismatches. From [Genome Analysis Wiki][3],&#xD;&#xA;&#xD;&#xA;&gt; The sequence being aligned to a reference may have additional bases that are not in the reference or may be missing bases that are in the reference. The CIGAR string is a sequence of of base lengths and the associated operation. They are used to indicate things like which bases align (either a match/mismatch) with the reference, are deleted from the reference, and are insertions that are not in the reference. For example:&#xD;&#xA;&#xD;&#xA;    RefPos:     1  2  3  4  5  6  7     8  9 10 11 12 13 14 15 16 17 18 19&#xD;&#xA;    Reference:  C  C  A  T  A  C  T     G  A  A  C  T  G  A  C  T  A  A  C&#xD;&#xA;    Read:                   A  C  T  A  G  A  A     T  G  G  C  T&#xD;&#xA;    With the alignment above, you get:&#xD;&#xA;    POS: 5&#xD;&#xA;    CIGAR: 3M1I3M1D5M&#xD;&#xA;&#xD;&#xA;Most common usage of the CIGAR string uses M (match/mismatch), I (insertion), D (deletion), S (soft clipping), and H (hard clipping). Note that = (match) and X (mismatch) are available as alternatives to the less informative M, but they are less widely used.&#xD;&#xA;&#xD;&#xA;The MD tag gives specific details on mismatches and deletions. From the [SAMtools tags specification][4],&#xD;&#xA;&#xD;&#xA;&gt; The MD field aims to achieve SNP/indel calling without looking at the reference. For example, a string ‘10A5^AC6’ means from the leftmost reference base in the alignment, there are 10 matches followed by an A on the reference which is different from the aligned read base; the next 5 reference bases are matches followed by a 2bp deletion from the reference; the deleted sequence is AC; the last 6 bases are matches. The MD field ought to match the CIGAR string.&#xD;&#xA;&#xD;&#xA;Note that neither of these will give you any idea of structural variants in short reads, and neither will be particularly readable (or helpful, due to the higher error rate) in long reads.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://software.broadinstitute.org/software/igv/&#xD;&#xA;  [2]: http://bioviz.org/igb/overview.html&#xD;&#xA;  [3]: http://genome.sph.umich.edu/wiki/SAM&#xD;&#xA;  [4]: https://samtools.github.io/hts-specs/SAMtags.pdf" />
  <row Id="284" PostHistoryTypeId="5" PostId="97" RevisionGUID="cbee119e-9f5e-4e60-a337-cd60477f07b8" CreationDate="2017-05-18T03:49:35.583" UserId="163" Comment="clarify sequencing data availability" Text="I have a DNA sample which I know doesn't *quite* match my reference genome. From visual inspection with IGV, a significant number of both SNPs and SVs appear to be present, but an assembly built entirely from my own sequencing data is not high enough quality for my purposes. &#xD;&#xA;&#xD;&#xA;How can I polish this draft genome with new sequencing data (preferably with Oxford Nanopore Technologies long reads, but I can also use these to scaffold short reads if necessary), taking advantage of my knowledge that the draft genome is *mostly* very good, without accessing the reads which were originally used to construct the draft genome?" />
  <row Id="285" PostHistoryTypeId="2" PostId="100" RevisionGUID="2d0b3587-7fa0-454e-a84d-bada685dc0e7" CreationDate="2017-05-18T04:30:57.133" UserId="280" Text="Even with inbreeding and other genetic phenomena that might mask actual evolution of these cultivars, any phylogenetic methodology would be capable of determining relationships accurately. &#xD;&#xA;&#xD;&#xA;Try creating a Neighbour Joining tree with [MEGA][1], which is one of the simplest methods available. This should give you enough to check the relationships of the cultivars.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.megasoftware.net/download_form" />
  <row Id="286" PostHistoryTypeId="2" PostId="101" RevisionGUID="cb8231b7-5b54-47b2-ad69-9c7d80fc3f61" CreationDate="2017-05-18T05:29:39.427" UserId="282" Text="well u can use nanopolish using the illumina reads check this link https://github.com/jts/nanopolish and pilon also another option " />
  <row Id="287" PostHistoryTypeId="2" PostId="102" RevisionGUID="3ceb4457-68a0-45eb-8c03-cff981426262" CreationDate="2017-05-18T05:38:32.473" UserId="223" Text="If it is a short-read draft assembly and you have long-reads (ONT or Pacbio) run [links][1] to scaffold the genome and then run [Pilon][2] iteratively to try to polish and fill gaps using the short-reads. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/warrenlr/LINKS&#xD;&#xA;  [2]: https://github.com/broadinstitute/pilon" />
  <row Id="288" PostHistoryTypeId="5" PostId="97" RevisionGUID="64f17ea6-eb84-4eb6-bf86-aed0a5028cee" CreationDate="2017-05-18T06:31:08.643" UserId="163" Comment="removed confusing 'polish' term" Text="I have a DNA sample which I know doesn't *quite* match my reference genome - my culture comes from a subpopulation which has undergone significant mutation since the reference was created.&#xD;&#xA;&#xD;&#xA;From visual inspection with IGV, a significant number of both SNPs and SVs appear to be present, but an assembly built entirely from my own sequencing data is not high enough quality for my purposes. &#xD;&#xA;&#xD;&#xA;How can I modify this reference genome to match my sample with new sequencing data (preferably with Oxford Nanopore Technologies long reads, but I can also use these to scaffold short reads if necessary), taking advantage of my knowledge that the existing reference is *mostly* very good, without having to access the reads which were originally used to construct the reference genome?" />
  <row Id="289" PostHistoryTypeId="4" PostId="97" RevisionGUID="64f17ea6-eb84-4eb6-bf86-aed0a5028cee" CreationDate="2017-05-18T06:31:08.643" UserId="163" Comment="removed confusing 'polish' term" Text="Improve a reference genome with sequencing data" />
  <row Id="290" PostHistoryTypeId="10" PostId="14" RevisionGUID="3ace8e16-00b1-459b-b0e1-e1c604c5cf7a" CreationDate="2017-05-18T07:05:09.920" UserId="-1" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:104,&quot;DisplayName&quot;:&quot;Chris_Rands&quot;},{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:219,&quot;DisplayName&quot;:&quot;anonymous2&quot;},{&quot;Id&quot;:176,&quot;DisplayName&quot;:&quot;JonMark Perry&quot;},{&quot;Id&quot;:161,&quot;DisplayName&quot;:&quot;rightskewed&quot;}]}" />
  <row Id="291" PostHistoryTypeId="2" PostId="103" RevisionGUID="c91ff1e5-4342-4b68-9f96-a06f407248e6" CreationDate="2017-05-18T07:17:15.830" UserId="286" Text="*Bootstrap analysis*, *bootstrapping* etc are quite common jargons of bioinformatics and phylogenetics. &#xD;&#xA;&#xD;&#xA;However, it is not very much clear to us, what exactly being meant by *&quot;a boot's straps&quot;.* &#xD;&#xA;&#xD;&#xA;Does it means a *&quot;comparison&quot;*? (Such as we hold the two straps of a boot (shoe) in &quot;two hands&quot;; may be an analogy to comparing 2 things (say left-'hand' side and right-'hand' side of an algebraic proof-task)) or it is something about *sequence alignment* (since we 'tie' a knot in boot (shoe) so that the 2 ends of the strap remain attached to each-other). &#xD;&#xA;&#xD;&#xA;Some of my classmates even assumed it may be a process during computer booting (but I could not agree with that). &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I've searched internet, but could not found anything very helpful. This [wikipedia][1] page ([permalink][2]) about bootstrapping, tells, &#xD;&#xA;&#xD;&#xA;&gt; &lt;sup&gt; Tall boots may have a tab, loop or handle at the top known as a bootstrap, allowing one to use fingers or a boot hook tool to help pulling the boots on. The saying &quot;to pull oneself up by one's bootstraps&quot; was already in use during the 19th century as an example of an impossible task. The idiom dates at least to 1834, when it appeared in the Workingman's Advocate: &quot;It is conjectured that Mr. Murphee will now be enabled to hand himself over the Cumberland river or a barn yard fence by the straps of his boots.&quot; In 1860 it appeared in a comment on philosophy of mind: ***&quot;The attempt of the mind to analyze itself an effort analogous to one who would lift himself by his own bootstraps&quot;***.  **Bootstrap as a metaphor, meaning to better oneself by one's own unaided efforts**, was in use in 1922. This metaphor spawned additional metaphors for a series of self-sustaining processes that proceed without external help.&lt;/sup&gt;  &#xD;&#xA;&gt;&#xD;&#xA;&gt; &lt;sup&gt;The term is sometimes attributed to a story in Rudolf Erich Raspe's The Surprising Adventures of Baron Munchausen, but in that story Baron Munchausen pulls himself (and his horse) out of a swamp by his hair (specifically, his pigtail), not by his bootstraps – and no explicit reference to bootstraps has been found elsewhere in the various versions of the Munchausen tales.&lt;/sup&gt;&#xD;&#xA;&#xD;&#xA;(In the quotation, highlighted portion seemed to tried to tell a meaning), but it wasn't also very much helpful. Could not correlate with bioinformatics. (Btw the same wikipedia article mentions computer booting). &#xD;&#xA;&#xD;&#xA;Wikipedia article about [Bootstrapping (statistics)][3] ([permalink][4]) tells: &#xD;&#xA;&#xD;&#xA;&gt; &lt;sup&gt;The bootstrap was published by Bradley Efron in &quot;Bootstrap methods: another look at the jackknife&quot; (1979), inspired by earlier work on the jackknife.Improved estimates of the variance were developed later. A Bayesian extension was developed in 1981. The bias-corrected and accelerated (BCa) bootstrap was developed by Efron in 1987, and the ABC procedure in 1992.&lt;/sup&gt; &#xD;&#xA;&#xD;&#xA;Still that is not very helpful. &#xD;&#xA;&#xD;&#xA;However, the textbook, *Introduction to Bioinformatics/ Arthur M. Lesk/ 3rd Edition*/ Oxford / (Low Price edition) ; in its *chapter 5* (Alignments and phylogenetic trees), section The problem of varying rates of evolution -&gt; *computational consideration* (page 296); a clearer definition has been given: &#xD;&#xA;&#xD;&#xA;&gt; 3. Formal statistical tests, involving rerunning the calculation on subsets of the original data, are known as **jackknifing** and **bootstrapping**.     &#xD;&#xA;     - **jackknifing** is calculation with data sets sampled randomly from the original data. For phylogeny calculations from from multiple sequence alignments, select different subsets of the position in the alignment, and rerun the calculation. Finding that each subset gives the same phylogenetic tree lends it credibility. If each subset gives a different tree, none of them are trustworthy. &#xD;&#xA;     - **Bootstrapping** is  similar to jackknifing except that the position chosen at random may include multiple copies of the same position, to form data sets of the same size as original, to preserve statistical properties of data sampling. &#xD;&#xA;&#xD;&#xA;More clear definition of &quot;Boot-strapping&quot;; but it does not explain, where the relation exists with the boot (shoe) and its straps. &#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Bootstrapping#Etymology&#xD;&#xA;  [2]: https://en.wikipedia.org/w/index.php?title=Bootstrapping&amp;oldid=776992221#Etymology&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#History&#xD;&#xA;  [4]: https://en.wikipedia.org/w/index.php?title=Bootstrapping_(statistics)&amp;oldid=780324952#History" />
  <row Id="292" PostHistoryTypeId="1" PostId="103" RevisionGUID="c91ff1e5-4342-4b68-9f96-a06f407248e6" CreationDate="2017-05-18T07:17:15.830" UserId="286" Text="Bootstrap analysis - why it is called bootstrap?" />
  <row Id="293" PostHistoryTypeId="3" PostId="103" RevisionGUID="c91ff1e5-4342-4b68-9f96-a06f407248e6" CreationDate="2017-05-18T07:17:15.830" UserId="286" Text="&lt;terminology&gt;&lt;bootstrap-analysis&gt;" />
  <row Id="294" PostHistoryTypeId="2" PostId="104" RevisionGUID="eaa0ea44-a4f5-4d30-a8df-1390b0aefdce" CreationDate="2017-05-18T07:30:52.260" UserId="77" Text="I wouldn't be surprised if this is marked as off-topic, but I honestly don't know that you'll get a different reaction even on cross validated. So...&#xD;&#xA;&#xD;&#xA;The method itself has nothing really to do with boots or straps, just as the jack-knife method also has nothing to do with knives. In the case of bootstrapping, the goal is to determine the accuracy of an estimate from random subsets of a population. Normally estimating something like the variance of the mean requires multiple independent samples, but bootstrapping allows you to perform estimates from a single population. This is essentially allowing the estimate to &quot;[pull itself up by its bootstraps](https://en.wiktionary.org/wiki/pull_oneself_up_by_one%27s_bootstraps)&quot;, which is the only reasonable source for the term. See also the more general wikipedia article on [bootstrapping outside of statistics](https://en.wikipedia.org/wiki/Bootstrapping)." />
  <row Id="295" PostHistoryTypeId="2" PostId="105" RevisionGUID="b053c91f-25d9-4a05-9c8b-d7a885f0fdfc" CreationDate="2017-05-18T07:43:12.813" UserId="204" Text="In general, branch support values indicate how much of the total data support that branch in a proposed tree. But what do you do if you used all of the available data to infer the tree in the first place? &#xD;&#xA;&#xD;&#xA;With Bootstrap supports, you *generate* new alignments (almost) *ex nihilo*. You calculate a trees support values, using its own data. &#xD;&#xA;&#xD;&#xA;This is the connection to &quot;pulling yourself up by your bootstraps&quot;: you don't &quot;pull&quot; on additionaly collected data (a rope or something in the metaphor), but rather on the very thing that your observation is based on (your boots)." />
  <row Id="296" PostHistoryTypeId="2" PostId="106" RevisionGUID="2d0cc636-e375-41b9-b8fc-d1b94642296a" CreationDate="2017-05-18T08:04:25.233" UserId="104" Text="Here is an approach with [`BioPython`][1]. The `with` statement ensures both the input and output file handles are closed and a [*lazy*][2] approach is taken so that only a single fasta record is held in memory at a time, rather than reading the whole file into memory, which is a bad idea for large input files:&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    &#xD;&#xA;    with open('sequences.fasta') as in_f, open('sequences.bed','w') as out_f:&#xD;&#xA;        for record in SeqIO.parse(in_f, 'fasta'):&#xD;&#xA;            out_f.write('{}\t0\t{}\n'.format(record.id, len(record)))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biopython.org/&#xD;&#xA;  [2]: http://stackoverflow.com/questions/20535342/lazy-evaluation-python" />
  <row Id="297" PostHistoryTypeId="2" PostId="107" RevisionGUID="ca5f811d-cc36-44d6-a699-1af6b3561122" CreationDate="2017-05-18T08:05:10.337" UserId="182" Text="What are the best tools to design gRNAs in a high-throughput way for a CRISPR screen, e.g. targeting all protein-coding genes in a genome? I would like to take into account possible off-target effects, as well as to allow for flexibility in the PAM sequence, to have the possibility of using non-canonical PAMs.&#xD;&#xA;&#xD;&#xA;I'm aware of [CRISPRseek](https://www.bioconductor.org/packages/release/bioc/html/CRISPRseek.html), but I'm afraid it might be quite slow to run for a genome-scale search.&#xD;&#xA;" />
  <row Id="298" PostHistoryTypeId="1" PostId="107" RevisionGUID="ca5f811d-cc36-44d6-a699-1af6b3561122" CreationDate="2017-05-18T08:05:10.337" UserId="182" Text="Large-scale gRNA design for a CRISPR screen" />
  <row Id="299" PostHistoryTypeId="3" PostId="107" RevisionGUID="ca5f811d-cc36-44d6-a699-1af6b3561122" CreationDate="2017-05-18T08:05:10.337" UserId="182" Text="&lt;crispr&gt;" />
  <row Id="300" PostHistoryTypeId="2" PostId="108" RevisionGUID="fe6f70b1-8183-4197-9917-19b9cf9fb776" CreationDate="2017-05-18T08:40:08.150" UserId="215" Text="It's good practice to have your `FASTA` indexed, so you can leverage the `.fai` you are likely to already have. If not, you can just generate the index with `samtools` and use some `awk` to make your `BED`:&#xD;&#xA;&#xD;&#xA;    samtools faidx $fasta&#xD;&#xA;    awk 'BEGIN {FS=&quot;\t&quot;}; {print $1 FS &quot;0&quot; FS $3}' $fasta.fai &gt; $fasta.bed&#xD;&#xA;&#xD;&#xA;This will maintain tab separation but you can drop the `BEGIN` statement to use spaces. The [BED spec](https://genome.ucsc.edu/FAQ/FAQformat.html#format1) only requires &quot;whitespace&quot; for the simple `BED` format." />
  <row Id="301" PostHistoryTypeId="5" PostId="83" RevisionGUID="3ffbc029-ce8a-4908-82f7-41c21f861a19" CreationDate="2017-05-18T08:41:44.793" UserId="235" Comment="Clarified intension of question. " Text="This question pertains to iCLIP, but it could just as easily be ChIP-seq or ATAC-seq or mutation frequencies. &#xD;&#xA;&#xD;&#xA;I have iCLIP read counts across the transcriptome and I wish to know if the signals are correlated - that is, where one of them is high, is the other likely to be high.&#xD;&#xA;&#xD;&#xA;Often when dealing with such data (e.g. iCLIP data) we know that the data is generally sparse - that is at most positions both signals are zero and this is correct, and also zero-inflated - that is many bases that &quot;should&quot; have a signal are missing that data. So just calculating the Spearman's correlation is likely to give an artificially low value.&#xD;&#xA;&#xD;&#xA;**What might be a way to asses the association? I should add that the aim is the assess association of binding patterns within genes, rather than (or as well as) between genes.**&#xD;&#xA;&#xD;&#xA;Things I have thought of:&#xD;&#xA;&#xD;&#xA;* Apply some sort of smoothing to the data (eg a rolling mean). Remove any bases with 0 in both samples. Compute the spearmans. &#xD;&#xA;* Calculate the average pairwise distance between every read in sample one and every read in sample two. Compare this to data where the reads have been randomised within genes. &#xD;&#xA;&#xD;&#xA;In the first case removing all bases with 0 in both samples seems wrong. But if 99.99% of all bases have zero in both samples, then this seems like its necessary for Spearman. &#xD;&#xA;&#xD;&#xA;In the second case, the result seems like it would be non-intuitive to interpret. And also calculating this would be massively computationally intensive. " />
  <row Id="302" PostHistoryTypeId="5" PostId="106" RevisionGUID="b483b760-ac92-4d8e-8992-448ab80cb65a" CreationDate="2017-05-18T08:56:58.783" UserId="104" Comment="added 125 characters in body" Text="Here is an approach with [`BioPython`][1]. The `with` statement ensures both the input and output file handles are closed and a [*lazy*][2] approach is taken so that only a single fasta record is held in memory at a time, rather than reading the whole file into memory, which is a bad idea for large input files. The solution makes no assumptions about the sequence ID lengths or the number of lines that the sequences are spread across:&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    &#xD;&#xA;    with open('sequences.fasta') as in_f, open('sequences.bed','w') as out_f:&#xD;&#xA;        for record in SeqIO.parse(in_f, 'fasta'):&#xD;&#xA;            out_f.write('{}\t0\t{}\n'.format(record.id, len(record)))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biopython.org/&#xD;&#xA;  [2]: http://stackoverflow.com/questions/20535342/lazy-evaluation-python" />
  <row Id="303" PostHistoryTypeId="2" PostId="109" RevisionGUID="c415db3f-9b1b-407f-b26e-6c9bcf8e830b" CreationDate="2017-05-18T08:57:06.240" UserId="235" Text="The [polyester][1] bioconductor package can do this. It says it simulates RNA-seq reads, but I don't know if that's really any different from other NGS reads. &#xD;&#xA;&#xD;&#xA;It can use a range of error and bias models, or learn them from a dataset.&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/html/polyester.html" />
  <row Id="304" PostHistoryTypeId="5" PostId="108" RevisionGUID="18da99ab-dffc-4a46-b6fc-62e4ffd43ed3" CreationDate="2017-05-18T09:06:45.380" UserId="215" Comment="Fixed field misnumbering" Text="It's good practice to have your `FASTA` indexed, so you can leverage the `.fai` you are likely to already have. If not, you can just generate the index with `samtools` and use some `awk` to make your `BED`:&#xD;&#xA;&#xD;&#xA;    samtools faidx $fasta&#xD;&#xA;    awk 'BEGIN {FS=&quot;\t&quot;}; {print $1 FS &quot;0&quot; FS $2}' $fasta.fai &gt; $fasta.bed&#xD;&#xA;&#xD;&#xA;This will maintain tab separation but you can drop the `BEGIN` statement to use spaces. The [BED spec](https://genome.ucsc.edu/FAQ/FAQformat.html#format1) only requires &quot;whitespace&quot; for the simple `BED` format." />
  <row Id="305" PostHistoryTypeId="2" PostId="110" RevisionGUID="79d0a9b5-5108-48f2-8420-0efdc69e000d" CreationDate="2017-05-18T09:11:09.517" UserId="215" Text="I've executed the `BLAT` aligner as a means to find alignments of translated DNA reads against a small protein database. I selected `BLAT` as it provided the easiest means to access information on the actual alignment blocks (with the smallest amount of parsing). That is, the PSL has a comma delimited field that enumerates each of the starting positions of a query to a target.&#xD;&#xA;&#xD;&#xA;However, `BLAT` outputs a significantly greater number of alignments than alternative tools. This is likely due to its identity percentage threshold defaulting to 25% when emulating a `blastx` style query.&#xD;&#xA;I can cope with this (apologies to my disks for the I/O), but for filtering purposes, the output `PSL` format appears to leave me nothing but raw match counts to work with.&#xD;&#xA;&#xD;&#xA;I know that the [BLAT FAQ](http://genome.ucsc.edu/FAQ/FAQblat.html#blat4) describes a Perl script for adding &quot;web-based identity and score calculations&quot;. I can actually get identity percentages and scores from this script, but I cannot tell what the score actually represents? Is it comparable to bit-score?&#xD;&#xA;&#xD;&#xA;There's probably another question in here for someone to discuss the pros and cons of match percentages, e-values and bitscores, but just so I can pretend that I am doing something more &quot;complicated&quot; than setting a match threshold, how can I recover something like an e-value, from a `PSL` output file?" />
  <row Id="306" PostHistoryTypeId="1" PostId="110" RevisionGUID="79d0a9b5-5108-48f2-8420-0efdc69e000d" CreationDate="2017-05-18T09:11:09.517" UserId="215" Text="Is it possible to calculate e-values from a BLAT PSL output file?" />
  <row Id="307" PostHistoryTypeId="3" PostId="110" RevisionGUID="79d0a9b5-5108-48f2-8420-0efdc69e000d" CreationDate="2017-05-18T09:11:09.517" UserId="215" Text="&lt;alignment&gt;" />
  <row Id="308" PostHistoryTypeId="2" PostId="111" RevisionGUID="91c5039f-3cce-4b61-b0ff-e2d45c02eae4" CreationDate="2017-05-18T09:23:17.877" UserId="292" Text="[bioawk](https://github.com/lh3/bioawk) could be reasonably efficient for this kind of task.&#xD;&#xA;&#xD;&#xA;    $ bioawk -c fastx '{histo[length($seq)]++} END {for (l in histo) print l,histo[l]}' \&#xD;&#xA;        | sort -n&#xD;&#xA;    0	33270&#xD;&#xA;    1	1542&#xD;&#xA;    2	1132&#xD;&#xA;    3	3397&#xD;&#xA;    4	8776&#xD;&#xA;    5	11884&#xD;&#xA;    6	12474&#xD;&#xA;    7	14341&#xD;&#xA;    8	13165&#xD;&#xA;    9	15467&#xD;&#xA;    10	21089&#xD;&#xA;    11	30469&#xD;&#xA;    12	45204&#xD;&#xA;    13	62311&#xD;&#xA;    14	88744&#xD;&#xA;    15	115767&#xD;&#xA;    16	140770&#xD;&#xA;    17	191810&#xD;&#xA;    18	313088&#xD;&#xA;    19	518111&#xD;&#xA;    20	1097867&#xD;&#xA;    21	4729715&#xD;&#xA;    22	6575557&#xD;&#xA;    23	2734062&#xD;&#xA;    24	1015476&#xD;&#xA;    25	493323&#xD;&#xA;    26	323827&#xD;&#xA;    27	164419&#xD;&#xA;    28	107120&#xD;&#xA;    29	72487&#xD;&#xA;    30	40120&#xD;&#xA;    31	24538&#xD;&#xA;    32	22295&#xD;&#xA;    33	13121&#xD;&#xA;    34	9382&#xD;&#xA;    35	4847&#xD;&#xA;    36	3858&#xD;&#xA;    37	3161&#xD;&#xA;    38	2852&#xD;&#xA;    39	2388&#xD;&#xA;    40	1639&#xD;&#xA;    41	961&#xD;&#xA;    42	686&#xD;&#xA;    43	377&#xD;&#xA;    44	199&#xD;&#xA;    45	114&#xD;&#xA;    46	78&#xD;&#xA;    47	59&#xD;&#xA;    48	50&#xD;&#xA;    49	52&#xD;&#xA;    50	48&#xD;&#xA;    51	42&#xD;&#xA;    52	39&#xD;&#xA;    53	28&#xD;&#xA;    54	49&#xD;&#xA;    55	59&#xD;&#xA;    56	55&#xD;&#xA;    57	51&#xD;&#xA;    58	55&#xD;&#xA;    59	43&#xD;&#xA;    60	52&#xD;&#xA;    61	56&#xD;&#xA;    62	48&#xD;&#xA;    63	67&#xD;&#xA;    64	95&#xD;&#xA;    65	488&#xD;&#xA;&#xD;&#xA;The `-c fastx` tells the program to parse the data as fastq or fasta. This gives access to the different parts of the records as `$name`, `$seq` (and `$qual` in the case of fastq format) in the awk code (bioawk is based on awk, so you can use whatever language features you want from [awk](http://www.grymoire.com/Unix/Awk.html)).&#xD;&#xA;&#xD;&#xA;Between the single quotes come a series of `&lt;condition&gt; {&lt;action&gt;}` blocks.&#xD;&#xA;&#xD;&#xA;The first one has no `&lt;condition&gt;` part, which mean it is executed for each record. Here, it updates the lengths counts in a table which I named &quot;histo&quot;. `length` is a predefined function in awk.&#xD;&#xA;&#xD;&#xA;In the second block, the `END` condition means we want it to be executed after all the input has been processed. The action part consists in looping over the recorded length values and print them together with the associated count.&#xD;&#xA;&#xD;&#xA;The output is piped to `sort -n` in order to sort the results numerically.&#xD;&#xA;&#xD;&#xA;On my workstation, the above code took 20 seconds to execute for a 1.2G fasta file." />
  <row Id="309" PostHistoryTypeId="2" PostId="112" RevisionGUID="de85ff19-0f8d-49ae-a06b-4bee6dd46cb0" CreationDate="2017-05-18T09:27:51.110" UserId="191" Text="I am currently looking for a system which will allow me to version both the code and the data in my research.&#xD;&#xA;&#xD;&#xA;I think my way of analyzing data is not uncommon, and this will be useful for many people doing bioinformatics and aiming for the reproducibility.&#xD;&#xA;&#xD;&#xA;Here are the requrements:&#xD;&#xA;&#xD;&#xA; - Analysis is performed on multiple machines (local, cluster, server).&#xD;&#xA; - Transparent synchronization between the machines.&#xD;&#xA; - Source code versioning.&#xD;&#xA; - Generated data versioning.&#xD;&#xA; - Support for large files (&gt;1Gb). At some point old generated files can  permanently deleted.&#xD;&#xA; - Support for large number of generated files (&gt;10k). These also could be deleted.&#xD;&#xA;&#xD;&#xA;So far I am using **git** + rsync/scp. But there are several downsides to it.&#xD;&#xA;&#xD;&#xA; - Synchronization between multiple machines is a bit tedious, i.e. you have to git pull before you start working and git push after each update. I can live with that.&#xD;&#xA; - You are not supposed to store large generated data files or large number of files inside your repository.&#xD;&#xA; - Therefore I have to synchronize data files manually using rsync, which is error prone.&#xD;&#xA;&#xD;&#xA;There is something called [git annex][1]. It seems really close to what I need. But:&#xD;&#xA;&#xD;&#xA;- A bit more work than git, but that's ok.&#xD;&#xA;- Unfortunately it seems it does not work well with the large number of files. Often I have more that 10k small files in my analysis. There are some tricks to [improve indexing][2], but it doesn't solve the issue. What I need is one symlink representing the full contents of directory.&#xD;&#xA;&#xD;&#xA;One potential solution is to use Dropbox or something similar (like [syncthing][3]) in combination with git. But the downside is there will be no connection between the source code version and the data version.&#xD;&#xA;&#xD;&#xA;Is there any versioning system for the code and the data meeting the requirements you can recommend?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://git-annex.branchable.com/&#xD;&#xA;  [2]: http://git-annex.branchable.com/tips/Repositories_with_large_number_of_files/&#xD;&#xA;  [3]: https://syncthing.net/" />
  <row Id="310" PostHistoryTypeId="1" PostId="112" RevisionGUID="de85ff19-0f8d-49ae-a06b-4bee6dd46cb0" CreationDate="2017-05-18T09:27:51.110" UserId="191" Text="How to version the code and the data during the analysis?" />
  <row Id="311" PostHistoryTypeId="3" PostId="112" RevisionGUID="de85ff19-0f8d-49ae-a06b-4bee6dd46cb0" CreationDate="2017-05-18T09:27:51.110" UserId="191" Text="&lt;versioning&gt;&lt;reproducibility&gt;&lt;data-management&gt;&lt;git&gt;" />
  <row Id="312" PostHistoryTypeId="2" PostId="113" RevisionGUID="bb9ee9f1-dd1e-423a-a6fc-61feb6ec03c2" CreationDate="2017-05-18T09:51:51.780" UserId="215" Text="Your question is somewhat open, but I think it could prove an interesting discussion. I don't believe in many cases it is worth storing the data you have created in `git`. As you've noted, it isn't designed for large files (although we have `git-lfs`) and it's definitely not designed for binary formats such as `BAM`.&#xD;&#xA;&#xD;&#xA;I'm of the opinion that `how` a file was created and `what` has been done to it since is key. Large files that took much effort to create should be mirrored somewhere (but not necessarily in a version control system). Other less-important (or less difficult to create) files that have been lost, clobbered or otherwise tainted can be regenerated as long as you know how they came to be.&#xD;&#xA;&#xD;&#xA;For what it's worth, I've been working on [a piece of software called `chitin`](https://github.com/SamStudio8/chitin/) (self described as a shell for disorganised bioinformaticians). I also wrote [a long blog post](https://samnicholls.net/2016/11/16/disorganised-disaster/) on why I thought this was a necessary project for me, but the main reason was that despite my attempts to organise my filesystem and make good archives of my experiments, over time I forget what my shorthand directory names meant, or exactly what program generated which data.&#xD;&#xA;&#xD;&#xA;`chitin`'s goal is to automatically capture changes made to the file system during the execution of a command. It knows what commands to run to re-create a particular file, what commands have used that file and can tell you when and why that file was changed (and by who ;) too).&#xD;&#xA;&#xD;&#xA;It's not finished (nothing ever is), but I feel that you might be going down the wrong road by wanting to store all your data and its versions when really, I think most people just want to know the commands that instigated changes. If the data history is important (and your code is well versioned), then you can simply check out any commit and execute your analysis to re-generate data." />
  <row Id="313" PostHistoryTypeId="2" PostId="114" RevisionGUID="ddcbb589-617e-4ea1-8362-da54be1407bb" CreationDate="2017-05-18T10:05:47.703" UserId="196" Text="I would suggest using blastx and parsing the output XML. As an example, have a look at this: http://www.perlmonks.org/?node_id=1006367.&#xD;&#xA;&#xD;&#xA;The relevant part of the hit would be the following:&#xD;&#xA;&#xD;&#xA;    &lt;Hit&gt;&#xD;&#xA;      &lt;Hit_num&gt;1&lt;/Hit_num&gt;&#xD;&#xA;      &lt;Hit_id&gt;gnl|BL_ORD_ID|1515029&lt;/Hit_id&gt;&#xD;&#xA;      &lt;Hit_def&gt;43989.cce_0262 (Cyanothece ATCC 51142)&lt;/Hit_def&gt;&#xD;&#xA;      &lt;Hit_accession&gt;1515029&lt;/Hit_accession&gt;&#xD;&#xA;      &lt;Hit_len&gt;65&lt;/Hit_len&gt;&#xD;&#xA;      &lt;Hit_hsps&gt;&#xD;&#xA;        &lt;Hsp&gt;&#xD;&#xA;          &lt;Hsp_num&gt;1&lt;/Hsp_num&gt;&#xD;&#xA;          &lt;Hsp_bit-score&gt;40.0466&lt;/Hsp_bit-score&gt;&#xD;&#xA;          &lt;Hsp_score&gt;92&lt;/Hsp_score&gt;&#xD;&#xA;          &lt;Hsp_evalue&gt;0.00664016&lt;/Hsp_evalue&gt;&#xD;&#xA;          &lt;Hsp_query-from&gt;155&lt;/Hsp_query-from&gt;&#xD;&#xA;          &lt;Hsp_query-to&gt;253&lt;/Hsp_query-to&gt;&#xD;&#xA;          &lt;Hsp_hit-from&gt;12&lt;/Hsp_hit-from&gt;&#xD;&#xA;          &lt;Hsp_hit-to&gt;44&lt;/Hsp_hit-to&gt;&#xD;&#xA;          &lt;Hsp_query-frame&gt;-1&lt;/Hsp_query-frame&gt;&#xD;&#xA;          &lt;Hsp_hit-frame&gt;0&lt;/Hsp_hit-frame&gt;&#xD;&#xA;          &lt;Hsp_identity&gt;17&lt;/Hsp_identity&gt;&#xD;&#xA;          &lt;Hsp_positive&gt;27&lt;/Hsp_positive&gt;&#xD;&#xA;          &lt;Hsp_gaps&gt;0&lt;/Hsp_gaps&gt;&#xD;&#xA;          &lt;Hsp_align-len&gt;33&lt;/Hsp_align-len&gt;&#xD;&#xA;          &lt;Hsp_qseq&gt;LRGAICSMEHIEEALGKLKDWARKLIELLLGPR&lt;/Hsp_qseq&gt;&#xD;&#xA;          &lt;Hsp_hseq&gt;ITGAVCLMDYLEKVLEKLRELAQKLIETLLGPQ&lt;/Hsp_hseq&gt;&#xD;&#xA;          &lt;Hsp_midline&gt;+ GA+C M+++E+ L KL++ A+KLIE LLGP+&lt;/Hsp_midline&gt;&#xD;&#xA;        &lt;/Hsp&gt;&#xD;&#xA;      &lt;/Hit_hsps&gt;&#xD;&#xA;    &lt;/Hit&gt;&#xD;&#xA;&#xD;&#xA;You should be able to get the position information from the following tags: `&lt;Hsp_query-from&gt;`,`&lt;Hsp_query-to&gt;`,`&lt;Hsp_hit-from&gt;`,`&lt;Hsp_hit-to&gt;`.&#xD;&#xA;&#xD;&#xA;To translate back to you original DNA sequence I can imagine you need the `&lt;Hsp_query-frame&gt;` tag.&#xD;&#xA;&#xD;&#xA;And then of course you get the evalue for free: `&lt;Hsp_evalue&gt;`&#xD;&#xA;" />
  <row Id="315" PostHistoryTypeId="2" PostId="115" RevisionGUID="ef5caa7e-8c38-47d5-8f8b-ef1054dbd31e" CreationDate="2017-05-18T11:07:13.690" UserId="156" Text="One approach to this is to use whatever data you have to iteratively update the reference genome. You can keep chain files along the way so you can convert coordinates (e.g. in gff files) from the original reference to your new pseudoreference.&#xD;&#xA;&#xD;&#xA;A simple approach might be:&#xD;&#xA;&#xD;&#xA;1. Align new data to existing reference&#xD;&#xA;2. Call variants (e.g. samtools mpileup, GATK, or whatever is best for you)&#xD;&#xA;3. Create new reference incorporating variants from 2&#xD;&#xA;4. Rinse and repeat (i.e. go to 1)&#xD;&#xA;&#xD;&#xA;You can track some simple stats as you do this - e.g. the number of new variants should decrease, the number of reads mapped should increase, and the mismatch rate should decrease, with every iteration of the above loop. Once the pseudoreference stabilises, you know you can't do much more. " />
  <row Id="316" PostHistoryTypeId="2" PostId="116" RevisionGUID="1a514518-9bae-47f5-bdd7-ac7222b7e004" CreationDate="2017-05-18T11:33:23.620" UserId="292" Text="You can do this easily with [bioawk](https://github.com/lh3/bioawk), which is a version of awk with added features facilitating bioinformatics:&#xD;&#xA;&#xD;&#xA;    bioawk -c fastx '{print $name&quot;\t0\t&quot;length($seq)}' test.fa&#xD;&#xA;&#xD;&#xA;`-c fastx` tells the program that the data should be parsed as fasta or fastq format. This makes the `$name` and `$seq` variables available in the awk commands." />
  <row Id="317" PostHistoryTypeId="6" PostId="91" RevisionGUID="f90b0867-b2dd-4fe5-83cc-c986c12843ee" CreationDate="2017-05-18T11:50:55.307" UserId="292" Comment="Added &quot;formats&quot; and &quot;fasta&quot; tags" Text="&lt;fasta&gt;&lt;bed&gt;&lt;formats&gt;" />
  <row Id="318" PostHistoryTypeId="24" PostId="91" RevisionGUID="f90b0867-b2dd-4fe5-83cc-c986c12843ee" CreationDate="2017-05-18T11:50:55.307" Comment="Proposed by 292 approved by 174 edit id of 25" />
  <row Id="319" PostHistoryTypeId="5" PostId="110" RevisionGUID="7292369f-f99f-4696-a9e8-c73998727681" CreationDate="2017-05-18T12:04:56.640" UserId="215" Comment="added 2764 characters in body; edited title" Text="I've executed the `BLAT` aligner as a means to find alignments of translated DNA reads against a small protein database. I selected `BLAT` as it provided the easiest means to access information on the actual alignment blocks (with the smallest amount of parsing). That is, the PSL has a comma delimited field that enumerates each of the starting positions of a query to a target.&#xD;&#xA;&#xD;&#xA;However, `BLAT` outputs a significantly greater number of alignments than alternative tools. This is likely due to its identity percentage threshold defaulting to 25% when emulating a `blastx` style query.&#xD;&#xA;I can cope with this (apologies to my disks for the I/O), but for filtering purposes, the output `PSL` format appears to leave me nothing but raw match counts to work with.&#xD;&#xA;&#xD;&#xA;I know that the [BLAT FAQ](http://genome.ucsc.edu/FAQ/FAQblat.html#blat4) describes a Perl script for adding &quot;web-based identity and score calculations&quot;. I can actually get identity percentages and scores from this script, but I cannot tell what the score actually represents? Is it comparable to bit-score?&#xD;&#xA;&#xD;&#xA;There's probably another question in here for someone to discuss the pros and cons of match percentages, e-values and bitscores, but just so I can pretend that I am doing something more &quot;complicated&quot; than setting a match threshold, how can I recover something like an e-value, from a `PSL` output file?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**To clarify** I'm attempting to reconstruct the read fragments that form a hit to a target, excluding/skipping gaps. BLA**S**T's `outfmt 6`, and alternatives such as Diamond's `M8` format only give the number of gap opens on the alignment. One can't just use the `start` and `end` positions of the alignment as indices to slice the read DNA sequence, as this will include the nucleotides that were gapped by the alignment.&#xD;&#xA;&#xD;&#xA;`PSL` appeared to be the only format I could find that gave the actual start positions (and lengths) of each of the pieces that formed a hit (that is, each gap forces a new block).&#xD;&#xA;&#xD;&#xA;The regular BLA**S**T output is human readable and is a nightmare to parse. The `XML` format mentioned has a structure for each HSP, but an HSP can still include gaps:&#xD;&#xA;&#xD;&#xA;      [...]&#xD;&#xA;      &lt;Hit_hsps&gt;&#xD;&#xA;        &lt;Hsp&gt;&#xD;&#xA;          &lt;Hsp_num&gt;1&lt;/Hsp_num&gt;&#xD;&#xA;          &lt;Hsp_bit-score&gt;51.6&lt;/Hsp_bit-score&gt;&#xD;&#xA;          &lt;Hsp_score&gt;122&lt;/Hsp_score&gt;&#xD;&#xA;          &lt;Hsp_evalue&gt;9.8e-06&lt;/Hsp_evalue&gt;&#xD;&#xA;          &lt;Hsp_query-from&gt;2&lt;/Hsp_query-from&gt;&#xD;&#xA;          &lt;Hsp_query-to&gt;98&lt;/Hsp_query-to&gt;&#xD;&#xA;          &lt;Hsp_hit-from&gt;35&lt;/Hsp_hit-from&gt;&#xD;&#xA;          &lt;Hsp_hit-to&gt;65&lt;/Hsp_hit-to&gt;&#xD;&#xA;          &lt;Hsp_query-frame&gt;2&lt;/Hsp_query-frame&gt;&#xD;&#xA;          &lt;Hsp_hit-frame&gt;0&lt;/Hsp_hit-frame&gt;&#xD;&#xA;          &lt;Hsp_identity&gt;24&lt;/Hsp_identity&gt;&#xD;&#xA;          &lt;Hsp_positive&gt;27&lt;/Hsp_positive&gt;&#xD;&#xA;          &lt;Hsp_gaps&gt;1&lt;/Hsp_gaps&gt;&#xD;&#xA;          &lt;Hsp_align-len&gt;32&lt;/Hsp_align-len&gt;&#xD;&#xA;             &lt;Hsp_qseq&gt;ITAIGAGLQGPAGCEVIDAGGLLVMPGGIDTH&lt;/Hsp_qseq&gt;&#xD;&#xA;             &lt;Hsp_hseq&gt;IAAVGTGLE-PAGAEIIDAGGLLVMPGGIDVH&lt;/Hsp_hseq&gt;&#xD;&#xA;          &lt;Hsp_midline&gt;I A+G GL+ PAG E+IDAGGLLVMPGGID H&lt;/Hsp_midline&gt;&#xD;&#xA;        &lt;/Hsp&gt;&#xD;&#xA;      &lt;/Hit_hsps&gt;&#xD;&#xA;    &lt;/Hit&gt;&#xD;&#xA;&#xD;&#xA;I'm trying to be lazy, I don't want to have to parse the `Hsp_qseq` and `Hsp_hseq` to determine exactly where my gaps are. I just want co-ordinates of where my read hits a protein. The `PSL` format (below) tells me this (`tStarts`):&#xD;&#xA;&#xD;&#xA;    match   mis-    rep.    N's     Q gap   Q gap   T gap   T gap   strand  Q               Q       Q       Q       T               T       T       T       block   blockSizes      qStarts  tStarts&#xD;&#xA;            match   match           count   bases   count   bases           name            size    start   end     name            size    start   end     count&#xD;&#xA;    ---------------------------------------------------------------------------------------------------------------------------------------------------------------&#xD;&#xA;    [...]&#xD;&#xA;    21      4       0       0       1       8       1       24      ++      &lt;PROTEIN&gt;   662     321     354     &lt;READ&gt;      101     2       101     2       8,17,   321,337,        2,50,&#xD;&#xA;&#xD;&#xA;The `PSL` gives me what I want, except now I don't have information on its significance - which is what I was asking about." />
  <row Id="320" PostHistoryTypeId="4" PostId="110" RevisionGUID="7292369f-f99f-4696-a9e8-c73998727681" CreationDate="2017-05-18T12:04:56.640" UserId="215" Comment="added 2764 characters in body; edited title" Text="Retrieve exact co-ordinates of where a read matches a sequence, from its alignment" />
  <row Id="322" PostHistoryTypeId="2" PostId="117" RevisionGUID="326cddf6-3d45-4ba9-92ea-fd05aae1d990" CreationDate="2017-05-18T12:46:12.660" UserId="179" Text="The Open Science Framework uses versioning for all files and is free to use: https://osf.io&#xD;&#xA;&#xD;&#xA;You can integrate data or code from various sources such as github, dropbox, google drive, figshare or amazon cloud&#xD;&#xA;&#xD;&#xA;You can also store files on their server using OSF data storage, but I do not know exactly what the file size limit is.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="323" PostHistoryTypeId="2" PostId="118" RevisionGUID="926f046e-422a-4a5e-8336-77fc09e5fb5a" CreationDate="2017-05-18T12:52:23.073" UserId="23" Text="I am interested in identifying mappings between different types of Affymetrix arrays. I am aware that mappings between gene and probeset can be extracted using Ensembl's Biomart database. Is there any way to extract mappings of probeset ids between two arrays(eg: HG-U133_Plus_2, HuGene-1_0-st-v1)?" />
  <row Id="324" PostHistoryTypeId="1" PostId="118" RevisionGUID="926f046e-422a-4a5e-8336-77fc09e5fb5a" CreationDate="2017-05-18T12:52:23.073" UserId="23" Text="probeset to probeset mappings between Affymetrix arrays" />
  <row Id="325" PostHistoryTypeId="3" PostId="118" RevisionGUID="926f046e-422a-4a5e-8336-77fc09e5fb5a" CreationDate="2017-05-18T12:52:23.073" UserId="23" Text="&lt;ensembl&gt;&lt;mapping&gt;&lt;biomart&gt;" />
  <row Id="326" PostHistoryTypeId="6" PostId="15" RevisionGUID="d3cbf5dd-0dcd-4402-8bc7-25f8a885c4ba" CreationDate="2017-05-18T13:36:10.287" UserId="131" Comment="maybe deserves a new tag? http://bio-bwa.sourceforge.net/" Text="&lt;alignment&gt;&lt;reads&gt;&lt;mapping&gt;&lt;bwa&gt;" />
  <row Id="327" PostHistoryTypeId="24" PostId="15" RevisionGUID="d3cbf5dd-0dcd-4402-8bc7-25f8a885c4ba" CreationDate="2017-05-18T13:36:10.287" Comment="Proposed by 131 approved by 88 edit id of 23" />
  <row Id="328" PostHistoryTypeId="2" PostId="119" RevisionGUID="3347b912-8221-434b-8ee7-fcef90e221cf" CreationDate="2017-05-18T13:44:31.117" UserId="47" Text="Using Git for version-controlling code is a good practice, but it does not lend itself well to versioning large data files.  Manually syncing data across multiple nodes is asking for trouble, you want this syncing to either be handled automatically in a managed environment, or just keep the files on a single network-attached storage device.&#xD;&#xA;&#xD;&#xA;One tool you might want to look into is [Arvados](https://arvados.org/), which is designed for syncing bioinformatics data and workflows across multiple machines.  From the project website:&#xD;&#xA;&#xD;&#xA;&gt;Arvados is a platform for storing, organizing, processing, and sharing genomic and other big data. The platform is designed to make it easier for data scientists to develop analyses, developers to create genomic web applications and IT administers to manage large-scale compute and storage genomic resources. The platform is designed to run in the cloud or on your own hardware." />
  <row Id="330" PostHistoryTypeId="5" PostId="112" RevisionGUID="e337637d-3fee-4f5e-8a35-47dc831d66ef" CreationDate="2017-05-18T13:50:39.270" UserId="191" Comment="clarify that small number of files also a problem" Text="I am currently looking for a system which will allow me to version both the code and the data in my research.&#xD;&#xA;&#xD;&#xA;I think my way of analyzing data is not uncommon, and this will be useful for many people doing bioinformatics and aiming for the reproducibility.&#xD;&#xA;&#xD;&#xA;Here are the requrements:&#xD;&#xA;&#xD;&#xA; - Analysis is performed on multiple machines (local, cluster, server).&#xD;&#xA; - All the code is transparently synchronized between the machines.&#xD;&#xA; - Source code versioning.&#xD;&#xA; - Generated data versioning.&#xD;&#xA; - Support for large number of small generated files (&gt;10k). These also could be deleted.&#xD;&#xA; - Support for large files (&gt;1Gb). At some point old generated files can  permanently deleted. It would be insane to have *transparent* synchronization of those, but being able to synchronize them on demand would be nice.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So far I am using **git** + rsync/scp. But there are several downsides to it.&#xD;&#xA;&#xD;&#xA; - Synchronization between multiple machines is a bit tedious, i.e. you have to git pull before you start working and git push after each update. I can live with that.&#xD;&#xA; - You are not supposed to store large generated data files or large number of files inside your repository.&#xD;&#xA; - Therefore I have to synchronize data files manually using rsync, which is error prone.&#xD;&#xA;&#xD;&#xA;There is something called [git annex][1]. It seems really close to what I need. But:&#xD;&#xA;&#xD;&#xA;- A bit more work than git, but that's ok.&#xD;&#xA;- Unfortunately it seems it does not work well with the large number of files. Often I have more that 10k small files in my analysis. There are some tricks to [improve indexing][2], but it doesn't solve the issue. What I need is one symlink representing the full contents of directory.&#xD;&#xA;&#xD;&#xA;One potential solution is to use Dropbox or something similar (like [syncthing][3]) in combination with git. But the downside is there will be no connection between the source code version and the data version.&#xD;&#xA;&#xD;&#xA;Is there any versioning system for the code and the data meeting the requirements you can recommend?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://git-annex.branchable.com/&#xD;&#xA;  [2]: http://git-annex.branchable.com/tips/Repositories_with_large_number_of_files/&#xD;&#xA;  [3]: https://syncthing.net/" />
  <row Id="332" PostHistoryTypeId="5" PostId="96" RevisionGUID="dd3998eb-7985-480f-be63-983ec6afce87" CreationDate="2017-05-18T13:55:20.790" UserId="132" Comment="Upped the pythonicity of the code snippet" Text="Inspired by [this answer][1] to a related question on read length distributions, you could do this with Biopython:&#xD;&#xA;&#xD;&#xA;    from Bio.SeqIO import parse&#xD;&#xA;    with open(&quot;regions.bed&quot;, &quot;w&quot;) as bed:&#xD;&#xA;        for record in parse(&quot;regions.fasta&quot;, &quot;fasta&quot;):&#xD;&#xA;            print(record.id, 0, len(record.seq), sep=&quot;\t&quot;, file=bed)&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/45/read-length-distribution-from-fasta-file/84#84" />
  <row Id="333" PostHistoryTypeId="24" PostId="96" RevisionGUID="dd3998eb-7985-480f-be63-983ec6afce87" CreationDate="2017-05-18T13:55:20.790" Comment="Proposed by 132 approved by 163 edit id of 24" />
  <row Id="334" PostHistoryTypeId="5" PostId="101" RevisionGUID="68056bd5-3bc4-4e03-855b-ff5664a11f1a" CreationDate="2017-05-18T13:58:52.057" UserId="77" Comment="Make links inline and correct to actual English" Text="You can use [nanopolish](https://github.com/jts/nanopolish) using the illumina reads. Also have a look at [pilon](https://github.com/broadinstitute/pilon/wiki)." />
  <row Id="335" PostHistoryTypeId="24" PostId="101" RevisionGUID="68056bd5-3bc4-4e03-855b-ff5664a11f1a" CreationDate="2017-05-18T13:58:52.057" Comment="Proposed by 77 approved by 55 edit id of 26" />
  <row Id="336" PostHistoryTypeId="5" PostId="118" RevisionGUID="ba2833f9-b12f-450c-92e1-13db7f2e9fed" CreationDate="2017-05-18T14:07:52.753" UserId="23" Comment="added an example" Text="I am interested in identifying mappings between different types of Affymetrix arrays. I am aware that mappings between gene and probeset can be extracted using Ensembl's Biomart database. &#xD;&#xA;&#xD;&#xA;    Ensembe gene id ENSG00000181019 maps to &#xD;&#xA;    1. AFFY HG-U133_Plus_2's 210519_s_at&#xD;&#xA;    2. AFFY HuGene-1_0-st-v1's 8002303&#xD;&#xA;&#xD;&#xA;Is there any way to extract mappings of probeset ids between two arrays(eg: HG-U133_Plus_2, HuGene-1_0-st-v1)?&#xD;&#xA;&#xD;&#xA;    eg: 210519_s_at and 8002303" />
  <row Id="338" PostHistoryTypeId="2" PostId="120" RevisionGUID="f86ef27f-3c65-4751-a362-23c5ce1cd0a1" CreationDate="2017-05-18T14:28:47.653" UserId="57" Text="I have a reference genome and now I would like to call structural variants from Illumina pair-end resequencing data (insert size 700bp). &#xD;&#xA;&#xD;&#xA;There are couple of tools like [Lumpy][1], [breakdancer][2], [Manta][3] or [Delly][4] for SV calls. There is also a tool for merging SV calls from multiple methods / samples - [SURVIVOR][5]. What is the ideal combination of methods for SV detection?&#xD;&#xA;&#xD;&#xA;I know about [this benchmarking paper][6], but it elaborates only on individual methods, not on their combination.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/arq5x/lumpy-sv&#xD;&#xA;  [2]: http://breakdancer.sourceforge.net/&#xD;&#xA;  [3]: https://github.com/Illumina/manta&#xD;&#xA;  [4]: https://github.com/dellytools/delly&#xD;&#xA;  [5]: https://github.com/fritzsedlazeck/SURVIVOR&#xD;&#xA;  [6]: http://www.ijpmbs.com/uploadfile/2016/1017/20161017025004545.pdf" />
  <row Id="339" PostHistoryTypeId="1" PostId="120" RevisionGUID="f86ef27f-3c65-4751-a362-23c5ce1cd0a1" CreationDate="2017-05-18T14:28:47.653" UserId="57" Text="How to call structural variants (SVs) from pair-end short read resequencing data" />
  <row Id="340" PostHistoryTypeId="3" PostId="120" RevisionGUID="f86ef27f-3c65-4751-a362-23c5ce1cd0a1" CreationDate="2017-05-18T14:28:47.653" UserId="57" Text="&lt;variant-calling&gt;&lt;genomics&gt;" />
  <row Id="341" PostHistoryTypeId="5" PostId="120" RevisionGUID="c2ce6703-e3c3-4e4f-a695-30a0af398093" CreationDate="2017-05-18T14:34:56.930" UserId="57" Comment="described bit more what is in the paper" Text="I have a reference genome and now I would like to call structural variants from Illumina pair-end resequencing data (insert size 700bp). &#xD;&#xA;&#xD;&#xA;There are couple of tools like [Lumpy][1], [breakdancer][2], [Manta][3] or [Delly][4] for SV calls. There is also a tool for merging SV calls from multiple methods / samples - [SURVIVOR][5]. What is the ideal combination of methods for SV detection?&#xD;&#xA;&#xD;&#xA;There is a [benchmarking paper][6], evaluating sensitivity and specificity of SV calls of individual methods using simulated pair-end reads. However, there is no elaboration on the combination of methods.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/arq5x/lumpy-sv&#xD;&#xA;  [2]: http://breakdancer.sourceforge.net/&#xD;&#xA;  [3]: https://github.com/Illumina/manta&#xD;&#xA;  [4]: https://github.com/dellytools/delly&#xD;&#xA;  [5]: https://github.com/fritzsedlazeck/SURVIVOR&#xD;&#xA;  [6]: http://www.ijpmbs.com/uploadfile/2016/1017/20161017025004545.pdf" />
  <row Id="342" PostHistoryTypeId="6" PostId="110" RevisionGUID="8d695a82-3a79-42ab-8b76-a71a8a023d50" CreationDate="2017-05-18T14:38:40.857" UserId="131" Comment="new tags added/proposed" Text="&lt;alignment&gt;&lt;blat&gt;&lt;blast&gt;" />
  <row Id="343" PostHistoryTypeId="24" PostId="110" RevisionGUID="8d695a82-3a79-42ab-8b76-a71a8a023d50" CreationDate="2017-05-18T14:38:40.857" Comment="Proposed by 131 approved by 215 edit id of 27" />
  <row Id="344" PostHistoryTypeId="5" PostId="120" RevisionGUID="36f2cf30-4384-4b2d-89c8-6ed0783a7b7a" CreationDate="2017-05-18T14:42:44.090" UserId="57" Comment="described bit more what is in the paper" Text="I have a reference genome and now I would like to call structural variants from Illumina pair-end resequencing data (insert size 700bp). &#xD;&#xA;&#xD;&#xA;There are couple of tools like [Lumpy][1], [breakdancer][2], [Manta][3] or [Delly][4] for SV calls. There is also a tool for merging SV calls from multiple methods / samples - [SURVIVOR][5]. Is there a combination of methods for SV detection with optimal balance between sensitivity and specificity?&#xD;&#xA;&#xD;&#xA;There is a [benchmarking paper][6], evaluating sensitivity and specificity of SV calls of individual methods using simulated pair-end reads. However, there is no elaboration on the combination of methods.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/arq5x/lumpy-sv&#xD;&#xA;  [2]: http://breakdancer.sourceforge.net/&#xD;&#xA;  [3]: https://github.com/Illumina/manta&#xD;&#xA;  [4]: https://github.com/dellytools/delly&#xD;&#xA;  [5]: https://github.com/fritzsedlazeck/SURVIVOR&#xD;&#xA;  [6]: http://www.ijpmbs.com/uploadfile/2016/1017/20161017025004545.pdf" />
  <row Id="345" PostHistoryTypeId="5" PostId="69" RevisionGUID="ed425d10-4051-49e2-aecd-1df7875c7c1d" CreationDate="2017-05-18T15:28:27.693" UserId="29" Comment="added 65 characters in body" Text="First off,&#xD;&#xA;&#xD;&#xA;**Don’t use RPKMs**.&#xD;&#xA;&#xD;&#xA;[They are truly deprecated](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) because they’re confusing once it comes to paired-end reads. If anything, use *FPKM*s, which are mathematically the same but use a more correct name (do we count paired reads separately? No, we count *fragments*).&#xD;&#xA;&#xD;&#xA;Even better, [use TPMs, or an appropriate cross-library normalisation method](http://rpubs.com/klmr/rnaseq-norm).&#xD;&#xA;&#xD;&#xA;That said, FPKM an be calculated in R as follows. Note that most of the calculation happens in log transformed number space, to avoid [numerical instability](https://en.wikipedia.org/wiki/Numerical_stability):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    fpkm = function (counts, effective_lengths) {&#xD;&#xA;        exp(log(counts) - log(effective_lengths) - log(sum(counts)) + log(1E9))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Here, the *effective length* is the transcript length minus the mean fragment length plus 1; that is, all the possible positions of an average fragment inside the transcript, which equals the number of all distinct fragments that can be sampled from a transcript.&#xD;&#xA;&#xD;&#xA;This function handles *one* library at a time. I ([and others](http://varianceexplained.org/r/tidy-genomics-biobroom/)) argue that this is the way functions should be written. If you want to apply the code to multiple libraries, nothing is easier using [‹dplyr›](http://dplyr.tidyverse.org/):&#xD;&#xA;&#xD;&#xA;    tidy_expression = tidy_expression %&gt;%&#xD;&#xA;        group_by(Sample) %&gt;%&#xD;&#xA;        mutate(FPKM = fpkm(Count, col_data$Lengths))&#xD;&#xA;&#xD;&#xA;However, the data in the question isn’t in tidy data format, so we first need to transform it accordingly using [‹tidyr›](http://tidyr.tidyverse.org/):&#xD;&#xA;&#xD;&#xA;    tidy_expression = expression %&gt;%&#xD;&#xA;        gather(Sample, Count)&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;This equation fails if all your counts are zero; instead of zeros you will get a vector of NaNs. You might want to account for that.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;And I mentioned that TPMs are superior, so here’s their function as well:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    tpm = function (counts, effective_lengths) {&#xD;&#xA;        rate = log(counts) - log(effective_lengths)&#xD;&#xA;        exp(rate - log(sum(exp(rate))) + log(1E6))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="346" PostHistoryTypeId="2" PostId="121" RevisionGUID="fec5badb-cc50-4da6-bf1e-2e07eea5b005" CreationDate="2017-05-18T15:44:11.847" UserId="215" Text="It would appear that `BLAT` and `exonerate` (as suggested by [@terdon](https://bioinformatics.stackexchange.com/users/298/terdon)) are the only pieces of software that provide enough information for &quot;HSPFragments&quot; to be parsed from their outputs, at least according to [BioPython's documentation](http://biopython.org/DIST/docs/api/Bio.SearchIO._model-module.html), anyway.&#xD;&#xA;&#xD;&#xA;&gt; Most search programs only have HSPs with one HSPFragment in them,&#xD;&#xA;&gt; making these two objects inseparable. However, there are programs&#xD;&#xA;&gt; (e.g. BLAT and Exonerate) which may have more than one HSPFragment&#xD;&#xA;&gt; objects in any given HSP. If you are not using these programs, you can&#xD;&#xA;&gt; safely consider HSP and HSPFragment as a single union.&#xD;&#xA;&#xD;&#xA;It would seem the options are sticking with `BLAT` and generating scores afterwards with the [script described in their FAQ](http://genome.ucsc.edu/FAQ/FAQblat.html#blat4), or using `exonerate` which provides a raw score as part of its default output. What these scores actually mean I will have to investigate, but for now I think the case is closed!&#xD;&#xA;" />
  <row Id="347" PostHistoryTypeId="5" PostId="121" RevisionGUID="30bbcd9c-8895-4965-9866-84c60193b15a" CreationDate="2017-05-18T15:50:52.953" UserId="215" Comment="Link to exonerate" Text="It would appear that `BLAT` and [`exonerate`](http://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate) (as suggested by [@terdon](https://bioinformatics.stackexchange.com/users/298/terdon)) are the only pieces of software that provide enough information for &quot;HSPFragments&quot; to be parsed from their outputs, at least according to [BioPython's documentation](http://biopython.org/DIST/docs/api/Bio.SearchIO._model-module.html), anyway:&#xD;&#xA;&#xD;&#xA;&gt; Most search programs only have HSPs with one HSPFragment in them,&#xD;&#xA;&gt; making these two objects inseparable. However, there are programs&#xD;&#xA;&gt; (e.g. BLAT and Exonerate) which may have more than one HSPFragment&#xD;&#xA;&gt; objects in any given HSP. If you are not using these programs, you can&#xD;&#xA;&gt; safely consider HSP and HSPFragment as a single union.&#xD;&#xA;&#xD;&#xA;It would seem the options are sticking with `BLAT` and generating scores afterwards with the [script described in their FAQ](http://genome.ucsc.edu/FAQ/FAQblat.html#blat4), or using `exonerate` which provides a raw score as part of its default output. What these scores actually mean I will have to investigate, but for now I think the case is closed!&#xD;&#xA;" />
  <row Id="348" PostHistoryTypeId="5" PostId="67" RevisionGUID="06561b60-2b85-43d4-8b87-1ec83b8954aa" CreationDate="2017-05-18T15:52:18.433" UserId="191" Comment="correct range" Text="RPKM is defined as:&#xD;&#xA;&#xD;&#xA;&gt;RPKM =   numberOfReads / ( geneLength/1000 * totalNumReads/1,000,000 )&#xD;&#xA;&#xD;&#xA;As you can see, you need to have gene lengths for every gene.&#xD;&#xA;&#xD;&#xA;Let's say `geneLength` is a vector which have the same number of rows as your `data.frame`, and every value of the vector corresponds to a gene (row) in `expression`.&#xD;&#xA;&#xD;&#xA;    # compute number of reads in each sample&#xD;&#xA;    totalNumReads &lt;- colSums(expression)&#xD;&#xA;    # compute RPKM&#xD;&#xA;    expression.rpkm &lt;- sapply(expression, function(column) 10^9 * column / geneLength / sum(column))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Regarding numerical stability**&#xD;&#xA;&#xD;&#xA;It is suggested in [one of the answers][1], that all the computations should be performed in a log-transformed scale. In my opinion there is absolutely no reason for doing that. IEEE [binary64][2] stores a number as binary number 1.b_{51}b{50}...b_0 times 2^{e-1023}. The *relative* precision doesn't depend on the exponent value given that a number is in the range [~10^-308; 10^308].&#xD;&#xA;&#xD;&#xA;In case of RPKM we can only get out of the range if total number of reads is around 10^300, which is not realistic at all.&#xD;&#xA;&#xD;&#xA;On the bright site there is not much harm in doing computations in the log-scale either. Worst case you loose a bit of precision.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/69/191&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format" />
  <row Id="349" PostHistoryTypeId="2" PostId="122" RevisionGUID="f42fdd96-2a15-4ff9-9b71-d20843742d21" CreationDate="2017-05-18T16:23:09.657" UserId="73" Text="`samtools mpileup` can do this quickly:&#xD;&#xA;&#xD;&#xA;    samtools mpileup -f reference.fasta -uv input.sam &gt; variants.vcf&#xD;&#xA;&#xD;&#xA;This will produce a [VCF-formatted][1] file containing information about what variants have been seen in the SAM file, aggregated for all the mapped reads.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://vcftools.sourceforge.net/VCF-poster.pdf" />
  <row Id="350" PostHistoryTypeId="4" PostId="120" RevisionGUID="c7bc9275-0db5-43e0-911f-7eb77f010679" CreationDate="2017-05-18T16:38:23.767" UserId="298" Comment="&quot;How to&quot; is a declaration, not a question. " Text="How can I call structural variants (SVs) from pair-end short read resequencing data?" />
  <row Id="351" PostHistoryTypeId="24" PostId="120" RevisionGUID="c7bc9275-0db5-43e0-911f-7eb77f010679" CreationDate="2017-05-18T16:38:23.767" Comment="Proposed by 298 approved by 57 edit id of 28" />
  <row Id="352" PostHistoryTypeId="5" PostId="120" RevisionGUID="5fb7ef5b-ab32-4505-be07-468a5d7a7875" CreationDate="2017-05-18T16:39:22.127" UserId="57" Comment="added 13 characters in body" Text="I have a reference genome and now I would like to call structural variants from Illumina pair-end whole genome resequencing data (insert size 700bp). &#xD;&#xA;&#xD;&#xA;There are couple of tools like [Lumpy][1], [breakdancer][2], [Manta][3] or [Delly][4] for SV calls. There is also a tool for merging SV calls from multiple methods / samples - [SURVIVOR][5]. Is there a combination of methods for SV detection with optimal balance between sensitivity and specificity?&#xD;&#xA;&#xD;&#xA;There is a [benchmarking paper][6], evaluating sensitivity and specificity of SV calls of individual methods using simulated pair-end reads. However, there is no elaboration on the combination of methods.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/arq5x/lumpy-sv&#xD;&#xA;  [2]: http://breakdancer.sourceforge.net/&#xD;&#xA;  [3]: https://github.com/Illumina/manta&#xD;&#xA;  [4]: https://github.com/dellytools/delly&#xD;&#xA;  [5]: https://github.com/fritzsedlazeck/SURVIVOR&#xD;&#xA;  [6]: http://www.ijpmbs.com/uploadfile/2016/1017/20161017025004545.pdf" />
  <row Id="353" PostHistoryTypeId="5" PostId="26" RevisionGUID="24102859-4abd-4a8d-a5cd-043988e245dd" CreationDate="2017-05-18T16:54:03.623" UserId="37" Comment="Added evaluation" Text="Let `f` and `r` be two integers. They always keep the k-mer on the forward and reverse strand, respectively. At a new base `c` in a proper 2-bit encoding, we update the two integers as follows:&#xD;&#xA;&#xD;&#xA;    f = (f&lt;&lt;2|c) &amp; ((1ULL&lt;&lt;2*k) - 1)&#xD;&#xA;    r = r&gt;&gt;2 | (3ULL-c)&lt;&lt;2*(k-1)&#xD;&#xA;&#xD;&#xA;With this updating rule, `f` keeps the forward strand k-mer ending at `c` and `r` is `f`'s reverse complement. The hash of the k-mer can be `min(f,r)`. If integer operations take constant time, computing the hashes of all k-mers of a sequence of length L takes O(L) time.&#xD;&#xA;&#xD;&#xA;The above only works if k-mer fits a word, which means `k&lt;=32` on x86. For long k-mers, you can overload bit operators in C++. In case of `32&lt;k&lt;=64`, a simpler solution is to split into lower and higher bits:&#xD;&#xA;&#xD;&#xA;    c1 = c&amp;1, c2 = c&gt;&gt;1&#xD;&#xA;    f1 = (f1&lt;&lt;1|c1) &amp; ((1ULL&lt;&lt;k) - 1)&#xD;&#xA;    f2 = (f2&lt;&lt;1|c2) &amp; ((1ULL&lt;&lt;k) - 1)&#xD;&#xA;    r1 = r1&gt;&gt;1 | (1ULL-c1)&lt;&lt;(k-1)&#xD;&#xA;    r2 = r2&gt;&gt;1 | (1ULL-c2)&lt;&lt;(k-1)&#xD;&#xA;&#xD;&#xA;This is twice as slow as the `k&lt;=32` version. It is possible to implement these lines with SSE2 intrinsics, but that is overkilling.&#xD;&#xA;&#xD;&#xA;When you want to hash a long k-mer into fewer bits, there are a few options:&#xD;&#xA;&#xD;&#xA;    xor:      f^r&#xD;&#xA;    min:      min(f,r)&#xD;&#xA;    min+hash: hash(min(f,r))&#xD;&#xA;&#xD;&#xA;where `hash()` is a generic randomization hash function, which could be Murmur3, Wang's integer hash function, etc.&#xD;&#xA;&#xD;&#xA;The following is a microbenchmark. Here, we squeeze all k-mers into a 32-bit hash table. `collision_rate = 1 - #distinct_hash/#distinct_kmer`. This is not the best metric, but it somehow measures the randomness, which is better than nothing.&#xD;&#xA;&#xD;&#xA;    ====================================================&#xD;&#xA;     Algorithm    data  max-k  k   %collision  CPU time&#xD;&#xA;    ----------------------------------------------------&#xD;&#xA;     xor          chr11  32    31     8.5%       0.9s&#xD;&#xA;     xor+Wang     chr11  32    31     9.6%       1.3s&#xD;&#xA;     min+Wang     chr11  32    31     1.4%       1.3s&#xD;&#xA;     min+Wang     chr11  64    31     1.4%       1.9s&#xD;&#xA;     min+murmur3  chr11  inf   31     1.4%       5.7s&#xD;&#xA;     min+Wang     chr11  64    51     1.5%       2.1s&#xD;&#xA;     min+murmur3  chr11  inf   51     1.5%       6.8s&#xD;&#xA;     min+Wang     chr11  64    63     1.5%       2.1s&#xD;&#xA;     min+murmur3  chr11  inf   63     1.5%       7.5s&#xD;&#xA;     radix sort   chr11  -     -      -        5.6-7.3s&#xD;&#xA;    ----------------------------------------------------&#xD;&#xA;     min+Wang     hg38   64    63    26.2%        37s&#xD;&#xA;     min+murmur3  hg38   inf   63    26.2%       192s&#xD;&#xA;     radix sort   hg38   -     -      -         ~138s&#xD;&#xA;    ====================================================&#xD;&#xA;&#xD;&#xA;Observations:&#xD;&#xA;&#xD;&#xA;* Don't use XOR. In fact, XOR hashes all palindromes to `0xffff...`. This is already worrying enough.&#xD;&#xA;&#xD;&#xA;* Unless you want to work with very long k-mers, don't use generic string hash functions like FNV and Murmur. Murmur is even slower than radix sorting all hashes.&#xD;&#xA;&#xD;&#xA;* For `k&lt;=64`, min+Wang is the best here. It is fast and simple to compute and has randomness nearly as good as murmur." />
  <row Id="354" PostHistoryTypeId="2" PostId="123" RevisionGUID="15355bfd-c896-46f6-a6d2-86603a995282" CreationDate="2017-05-18T16:57:57.520" UserId="69" Text="There is a couple of points to consider here, which I outline below. The goal here should be to find a workflow that is minimally intrusive on top of already using `git`.&#xD;&#xA;&#xD;&#xA;As of yet, there is no ideal workflow that covers all use cases, but what I outline below is the closest I could come to it.&#xD;&#xA;&#xD;&#xA;### Reproducibility is not just keeping all your data&#xD;&#xA;&#xD;&#xA;You have got your raw data that you start your project with.&#xD;&#xA;&#xD;&#xA;All other data in your project directory should never just &quot;be there&quot;, but have some record of where it comes from. Data processing scripts are great for this, because they *already document how* you went from your raw to your analytical data, and then the files needed for your analyses.&#xD;&#xA;&#xD;&#xA;And those scripts can be versioned, with an appropriate single entry point of processing (e.g. a `Makefile` that describes how to run your scripts).&#xD;&#xA;&#xD;&#xA;This way, the state of all your project files is defined by the raw data, and the version of your processing scripts (and versions of external software, but that's a whole different kind of problem).&#xD;&#xA;&#xD;&#xA;### What data/code should and should not be versioned&#xD;&#xA;&#xD;&#xA;Just as you would not version generated code files, you should not want to version 10k intermediary data files that you produced when performing your analyses. The data that *should be* versioned is your *raw data* (at the start of your pipeline), not automatically generated files.&#xD;&#xA;&#xD;&#xA;You might want to take snapshots of your project directory, but not keep every version of every file ever produced. This already cuts down your problem by a fair margin.&#xD;&#xA;&#xD;&#xA;### Approach 1: Actual versioning of data&#xD;&#xA;&#xD;&#xA;For your **raw or analytical data**, [Git LFS](https://git-lfs.github.com/) (and alternatively [Git Annex](https://git-annex.branchable.com/), that you already mention) is designed to solve exactly this problem: add tracking information of files in your Git tree, but do not store the content of those files in the repository (because otherwise it would add the size of a non-diffable file with every change you make).&#xD;&#xA;&#xD;&#xA;For your **intermediate files**, you do the same as you would do with intermediate code files: add them to your `.gitignore` and do not version them.&#xD;&#xA;&#xD;&#xA;This begs a couple of considerations:&#xD;&#xA;&#xD;&#xA;* Git LFS is a paid service from Github (the free tier is limited to 1 GB of storage/bandwidth per month, which is very little), and it is more expensive than other comparable cloud storage solutions. You could consider paying for the storage at Github or running your own LFS server (there is a reference implementation, but I assume this would still be a substantial effort)&#xD;&#xA;* Git Annex is free, but it replaces files by links and hence changes time stamps, which is a problem for e.g. GNU Make based workflows (major drawback for me). Also, fetching of files needs to be done manually or via a commit hook&#xD;&#xA;&#xD;&#xA;### Approach 2: Versioning code only, syncing data&#xD;&#xA;&#xD;&#xA;If your analytical data stays the same for most of your analyses, so the actual need to version it (as opposed to back up and document data provenance, which is essential) may be limited.&#xD;&#xA;&#xD;&#xA;The key to get this this working is to put all **data files** in your `.gitignore` and ignore all your **code files** in `rsync`, with a script in your project root (extensions and directories are an example only):&#xD;&#xA;&#xD;&#xA;    #!/bin/bash&#xD;&#xA;    cd $(dirname $0)&#xD;&#xA;    rsync -auvr \&#xD;&#xA;        --exclude &quot;*.r&quot; \&#xD;&#xA;        --include &quot;*.RData&quot; \&#xD;&#xA;        --exclude &quot;dir with huge files that you don't need locally&quot; \&#xD;&#xA;        yourhost:/your/project/path/* .&#xD;&#xA;&#xD;&#xA;The advantage here is that you don't need to remember the `rsync` command you are running. The script itself goes into version control.&#xD;&#xA;&#xD;&#xA;This is especially useful if you do your heavy processing on a computing cluster but want to make plots from your result files on your local machine. I argue that you *generally don't need* bidirectional sync." />
  <row Id="355" PostHistoryTypeId="2" PostId="124" RevisionGUID="cc673089-fa44-4959-b884-b655b3e79ac2" CreationDate="2017-05-18T17:32:11.053" UserId="57" Text="This python should do the job if `bar.tsv` and `foo.fasta` will be your files:&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    &#xD;&#xA;    repeat = {}&#xD;&#xA;    for line in open(&quot;bar.tsv&quot;):&#xD;&#xA;        seq_id, coverage = line.split()&#xD;&#xA;        repeat[seq_id] = int(coverage)&#xD;&#xA;    &#xD;&#xA;    for seq_record in SeqIO.parse(foo.fasta, &quot;fasta&quot;):&#xD;&#xA;        for i in range(repeat.get(seq_record.name, 0)):&#xD;&#xA;            print(&quot;&gt;&quot;,seq_record.name,&quot;_&quot;,i,sep='')&#xD;&#xA;            print(seq_record.seq)&#xD;&#xA;" />
  <row Id="357" PostHistoryTypeId="5" PostId="124" RevisionGUID="5d4b04b3-c90b-49d9-8d6f-4892372a65a9" CreationDate="2017-05-18T17:35:46.877" UserId="191" Comment="syntax highlight python" Text="This python should do the job if `bar.tsv` and `foo.fasta` will be your files:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    &#xD;&#xA;    repeat = {}&#xD;&#xA;    for line in open(&quot;bar.tsv&quot;):&#xD;&#xA;        seq_id, coverage = line.split()&#xD;&#xA;        repeat[seq_id] = int(coverage)&#xD;&#xA;    &#xD;&#xA;    for seq_record in SeqIO.parse(foo.fasta, &quot;fasta&quot;):&#xD;&#xA;        for i in range(repeat.get(seq_record.name, 0)):&#xD;&#xA;            print(&quot;&gt;&quot;,seq_record.name,&quot;_&quot;,i,sep='')&#xD;&#xA;            print(seq_record.seq)&#xD;&#xA;" />
  <row Id="358" PostHistoryTypeId="24" PostId="124" RevisionGUID="5d4b04b3-c90b-49d9-8d6f-4892372a65a9" CreationDate="2017-05-18T17:35:46.877" Comment="Proposed by 191 approved by 57 edit id of 29" />
  <row Id="359" PostHistoryTypeId="6" PostId="120" RevisionGUID="b9462aaf-331e-4816-85dd-069c4bf9b5af" CreationDate="2017-05-18T17:38:42.653" UserId="57" Comment="added ngs tag" Text="&lt;variant-calling&gt;&lt;genomics&gt;&lt;ngs&gt;" />
  <row Id="360" PostHistoryTypeId="5" PostId="63" RevisionGUID="c53de844-44ac-4cc4-b940-bb57cce33e5f" CreationDate="2017-05-18T17:39:37.100" UserId="191" Comment="adding syntax highilghting for bash" Text="**TL;DR**&#xD;&#xA;&#xD;&#xA;I made the following bash script to generate an input file for HYPHY.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    #!/bin/bash&#xD;&#xA;    cat &lt;&lt; EOF&#xD;&#xA;    inputRedirect = {};&#xD;&#xA;    inputRedirect[&quot;01&quot;]=&quot;Universal&quot;; // genetic code&#xD;&#xA;    inputRedirect[&quot;02&quot;]=&quot;$(readlink -f $1)&quot;; // codon data&#xD;&#xA;    inputRedirect[&quot;03&quot;]=&quot;$(readlink -f $2)&quot;; // tree&#xD;&#xA;    inputRedirect[&quot;04&quot;]=&quot;${3:-All}&quot;; // Test for selection on a branch&#xD;&#xA;    inputRedirect[&quot;05&quot;]=&quot;&quot;; // complete selection&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    ExecuteAFile (HYPHY_LIB_DIRECTORY+&quot;TemplateBatchFiles/BUSTED.bf&quot;, inputRedirect);&#xD;&#xA;    EOF&#xD;&#xA;&#xD;&#xA;If you want to test all the branches jointly run:&#xD;&#xA;&#xD;&#xA;    ./gen_busted.sh alignment.phy tree.nwk &gt; script.bf&#xD;&#xA;&#xD;&#xA;Or for a particular branch:&#xD;&#xA;&#xD;&#xA;    ./gen_busted.sh alignment.phy tree.nwk branchLabel &gt; script.bf&#xD;&#xA;&#xD;&#xA;And then run:&#xD;&#xA;&#xD;&#xA;    HYPHYMP script.bf&#xD;&#xA;    &#xD;&#xA;Now you can use the script to generate a `.bf` file for every sequence alignment you have, and run HYPHY for every file.&#xD;&#xA;&#xD;&#xA;**Longer version**&#xD;&#xA;&#xD;&#xA;I will describe how to make a similar script for any model, such as MEME, RELAX or PARRIS.&#xD;&#xA;&#xD;&#xA;First, find the `.bf` script which performs the analysis. On GNU/Linux it should be at `HYPHY_INSTALLATION_PATH/lib/hyphy/TemplateBatchFiles` (it might be different between different OS). In case of BUSTED your script has a name `BUSTED.bf`.&#xD;&#xA;&#xD;&#xA;Now run this model in the interactive mode in order to record all the inputs required by the model. Make sure to specify **full paths** to all the files.&#xD;&#xA;&#xD;&#xA;    HYPHYMP HYPHY_INSTALLATION_PATH/lib/hyphy/TemplateBatchFiles/BUSTED.bf&#xD;&#xA;&#xD;&#xA;In case of BUSTED the inputs are:&#xD;&#xA;&#xD;&#xA;1. Genetic code.&#xD;&#xA;2. Codon data (sequence alignment in phylip format).&#xD;&#xA;3. Tree (in newick format).&#xD;&#xA;4. Which branches to test.&#xD;&#xA;&#xD;&#xA;Now for every input alignment you need to generate a batch file which looks like this:&#xD;&#xA;&#xD;&#xA;    inputRedirect = {};&#xD;&#xA;    inputRedirect[&quot;01&quot;]=&quot;Universal&quot;; // genetic code&#xD;&#xA;    inputRedirect[&quot;02&quot;]=&quot;/path/to/alignment.phy&quot;; // codon data&#xD;&#xA;    inputRedirect[&quot;03&quot;]=&quot;/path/to/tree.nwk&quot;; // tree&#xD;&#xA;    inputRedirect[&quot;04&quot;]=&quot;All&quot;; // Test for selection on all branches&#xD;&#xA;    inputRedirect[&quot;05&quot;]=&quot;BRANCH1&quot;; // Test for selection on branch1 &#xD;&#xA;    inputRedirect[&quot;06&quot;]=&quot;BRANCH2&quot;; // Test for selection on branch2 &#xD;&#xA;    inputRedirect[&quot;07&quot;]=&quot;&quot;; // complete selection&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    ExecuteAFile (HYPHY_LIB_DIRECTORY+&quot;TemplateBatchFiles/BUSTED.bf&quot;, inputRedirect);&#xD;&#xA;&#xD;&#xA;Now you can create a script which generates the file. The most important thing, don't forget to use full paths in the `.bf` file." />
  <row Id="361" PostHistoryTypeId="2" PostId="125" RevisionGUID="561888c4-fe1b-435b-a85f-e9ef31a6c9ee" CreationDate="2017-05-18T18:52:27.690" UserId="191" Text="Let's say I want to construct a phylogenetic tree based on orthologous nucleotide sequences; I do not want to use protein sequences to have a better resolution. These species have different GC-content.&#xD;&#xA;&#xD;&#xA;If we use a straightforward approach like maximum likelihood with JC69 or any other classical nucleotide model, conserved protein coding sequences of distant species with similar GC-content will artificially cluster together. This will happen because GC-content will mainly affect wobbling codon positions, and they will look similar on the nucleotide level.&#xD;&#xA;&#xD;&#xA;What are possible ways to overcome this? I considered the following options so far:&#xD;&#xA;&#xD;&#xA;1. Using protein sequence. This is possible of course, but we loose a lot of information on the short distance. Not applicable to non-coding sequences. &#xD;&#xA;&#xD;&#xA;2. [Recoding][1]. In this approach C and T can be combined into a single pyrimidine state Y (G and A could be also combined in some implementation). This sounds interesting, but, first, we also loose some information here. Mathematical properties of the resulting process are not clear. As a result, this approach is not widely used.&#xD;&#xA;&#xD;&#xA;3. Excluding third codon position from the analysis. Loosing some short-distance information again. Also, not all synonymous substitution are specific to the third codon positions, so we still expect to have some bias. Not applicable to non-coding sequence.&#xD;&#xA;&#xD;&#xA;It should be possible in theory to have a model which allows shifts in GC-content. This will be a non time-reversible Markov process. As far as I understand there are some computational difficulties estimating likelihood for such models.&#xD;&#xA;&#xD;&#xA;  [1]: https://doi.org/10.1186/1471-2105-15-S2-S8" />
  <row Id="362" PostHistoryTypeId="1" PostId="125" RevisionGUID="561888c4-fe1b-435b-a85f-e9ef31a6c9ee" CreationDate="2017-05-18T18:52:27.690" UserId="191" Text="What is the best way to account for GC-content switch while constructing phylogenetic tree?" />
  <row Id="363" PostHistoryTypeId="3" PostId="125" RevisionGUID="561888c4-fe1b-435b-a85f-e9ef31a6c9ee" CreationDate="2017-05-18T18:52:27.690" UserId="191" Text="&lt;phylogeny&gt;&lt;nucleotide-models&gt;&lt;gc-content&gt;" />
  <row Id="366" PostHistoryTypeId="4" PostId="125" RevisionGUID="fc88a575-f984-4e4f-a13f-d68271236e98" CreationDate="2017-05-18T18:58:22.873" UserId="191" Comment="title" Text="What is the best way to account for GC-content shift in a nucleotide-based phylogenetic tree?" />
  <row Id="367" PostHistoryTypeId="5" PostId="125" RevisionGUID="52cd1000-3c84-480b-8f28-f5111d82b84d" CreationDate="2017-05-18T18:59:12.183" UserId="280" Comment="Loose is different of lose, lose one O" Text="Let's say I want to construct a phylogenetic tree based on orthologous nucleotide sequences; I do not want to use protein sequences to have a better resolution. These species have different GC-content.&#xD;&#xA;&#xD;&#xA;If we use a straightforward approach like maximum likelihood with JC69 or any other classical nucleotide model, conserved protein coding sequences of distant species with similar GC-content will artificially cluster together. This will happen because GC-content will mainly affect wobbling codon positions, and they will look similar on the nucleotide level.&#xD;&#xA;&#xD;&#xA;What are possible ways to overcome this? I considered the following options so far:&#xD;&#xA;&#xD;&#xA;1. Using protein sequence. This is possible of course, but we lose a lot of information on the short distance. Not applicable to non-coding sequences. &#xD;&#xA;&#xD;&#xA;2. [Recoding][1]. In this approach C and T can be combined into a single pyrimidine state Y (G and A could be also combined in some implementation). This sounds interesting, but, first, we also lose some information here. Mathematical properties of the resulting process are not clear. As a result, this approach is not widely used.&#xD;&#xA;&#xD;&#xA;3. Excluding third codon position from the analysis. Losing some short-distance information again. Also, not all synonymous substitution are specific to the third codon positions, so we still expect to have some bias. Not applicable to non-coding sequence.&#xD;&#xA;&#xD;&#xA;It should be possible in theory to have a model which allows shifts in GC-content. This will be a non time-reversible Markov process. As far as I understand there are some computational difficulties estimating likelihood for such models.&#xD;&#xA;&#xD;&#xA;  [1]: https://doi.org/10.1186/1471-2105-15-S2-S8" />
  <row Id="368" PostHistoryTypeId="4" PostId="125" RevisionGUID="52cd1000-3c84-480b-8f28-f5111d82b84d" CreationDate="2017-05-18T18:59:12.183" UserId="280" Comment="Loose is different of lose, lose one O" Text="What is the best way to account for GC-content switch while constructing phylogenetic tree?" />
  <row Id="369" PostHistoryTypeId="24" PostId="125" RevisionGUID="52cd1000-3c84-480b-8f28-f5111d82b84d" CreationDate="2017-05-18T18:59:12.183" Comment="Proposed by 280 approved by 191 edit id of 33" />
  <row Id="370" PostHistoryTypeId="4" PostId="125" RevisionGUID="2ce845f0-bbb1-401b-9d35-6a50cdaedd0a" CreationDate="2017-05-18T18:59:38.917" UserId="191" Comment="switch-&gt;shift" Text="What is the best way to account for GC-content shift while constructing phylogenetic tree?" />
  <row Id="371" PostHistoryTypeId="4" PostId="125" RevisionGUID="e4a27ad8-8069-405b-ace5-bb3df8fb939e" CreationDate="2017-05-18T19:06:51.733" UserId="191" Comment="title: nucleotide based" Text="What is the best way to account for GC-content shift while constructing nucleotide-based phylogenetic tree?" />
  <row Id="374" PostHistoryTypeId="5" PostId="125" RevisionGUID="d2e71074-f724-4c58-8aa7-ed5749845e6c" CreationDate="2017-05-18T19:37:39.770" UserId="191" Comment="typo" Text="Let's say I want to construct a phylogenetic tree based on orthologous nucleotide sequences; I do not want to use protein sequences to have a better resolution. These species have different GC-content.&#xD;&#xA;&#xD;&#xA;If we use a straightforward approach like maximum likelihood with JC69 or any other classical nucleotide model, conserved protein coding sequences of distant species with similar GC-content will artificially cluster together. This will happen because GC-content will mainly affect wobbling codon positions, and they will look similar on the nucleotide level.&#xD;&#xA;&#xD;&#xA;What are possible ways to overcome this? I considered the following options so far:&#xD;&#xA;&#xD;&#xA;1. Using protein sequence. This is possible of course, but we lose a lot of information on the short distance. Not applicable to non-coding sequences. &#xD;&#xA;&#xD;&#xA;2. [Recoding][1]. In this approach C and T can be combined into a single pyrimidine state Y (G and A could be also combined in some implementations). This sounds interesting, but, first, we also lose some information here. Mathematical properties of the resulting process are not clear. As a result, this approach is not widely used.&#xD;&#xA;&#xD;&#xA;3. Excluding third codon position from the analysis. Losing some short-distance information again. Also, not all synonymous substitution are specific to the third codon positions, so we still expect to have some bias. Not applicable to non-coding sequence.&#xD;&#xA;&#xD;&#xA;It should be possible in theory to have a model which allows shifts in GC-content. This will be a non time-reversible Markov process. As far as I understand there are some computational difficulties estimating likelihood for such models.&#xD;&#xA;&#xD;&#xA;  [1]: https://doi.org/10.1186/1471-2105-15-S2-S8" />
  <row Id="375" PostHistoryTypeId="5" PostId="104" RevisionGUID="896b1186-5fc1-4cea-a968-1c32b6d9d241" CreationDate="2017-05-18T19:59:53.980" UserId="43" Comment="No need to suggest that it's off-topic." Text="The method itself has nothing really to do with boots or straps, just as the jack-knife method also has nothing to do with knives. In the case of bootstrapping, the goal is to determine the accuracy of an estimate from random subsets of a population. Normally estimating something like the variance of the mean requires multiple independent samples, but bootstrapping allows you to perform estimates from a single population. This is essentially allowing the estimate to &quot;[pull itself up by its bootstraps](https://en.wiktionary.org/wiki/pull_oneself_up_by_one%27s_bootstraps)&quot;, which is the only reasonable source for the term. See also the more general wikipedia article on [bootstrapping outside of statistics](https://en.wikipedia.org/wiki/Bootstrapping)." />
  <row Id="376" PostHistoryTypeId="24" PostId="104" RevisionGUID="896b1186-5fc1-4cea-a968-1c32b6d9d241" CreationDate="2017-05-18T19:59:53.980" Comment="Proposed by 43 approved by 77 edit id of 30" />
  <row Id="381" PostHistoryTypeId="5" PostId="43" RevisionGUID="fd023205-fee1-4c63-b241-4567d5a0fa83" CreationDate="2017-05-18T21:40:17.467" UserId="113" Comment="Edited to reflect general nature of GFF" Text="I think the question is a bit ambiguous so please excuse this answer that's a bit redundant from the rest of the ones provided.&#xD;&#xA;&#xD;&#xA;As others have mentioned, if you want to store a full genome, [`FASTA`](https://en.wikipedia.org/wiki/FASTA_format) and [`2bit`](https://genome.ucsc.edu/FAQ/FAQformat.html#format7) formats are appropriate.  For some context, [`hg19`](http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/) is about 900Mb compressed for the `FASTA` file and about 780Mb compressed for the `2bit` file .  `hg19` is a reference and is haploid so doesn't represent a &quot;full&quot; human genome that would normally have two alleles for the autosome (non-sex chromosomes).&#xD;&#xA;&#xD;&#xA;Common formats for representing a &quot;full&quot; human genome range from Variant Call Format ([`VCF`](https://vcftools.github.io/specs.html)) to General Feature Format ([`GFF`](https://en.wikipedia.org/wiki/General_feature_format)) among others which I'm sure I'm neglecting.  The `VCF` format represent differences from a reference (`hg19`, say) that results in a &quot;whole genome&quot; file.  A `GFF` file can be made to store similar differences.  From personal experience, I've seen `VCF` and `GFF` files in the range of 100Mb, or more, compressed.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If you're considering just one &quot;whole genome&quot; in isolation, then the answer is pretty clear: `2bit` format is probably approaching the entropy limit of the human genome and you probably won't be able to do much better.&#xD;&#xA;The reason why your question is a bit ambiguous is that as soon as you start encoding more than one genome, a population of genomes, say, then you can start exploiting the redundancy of the genome as shared by the population.&#xD;&#xA;&#xD;&#xA;For example, say you want to store two &quot;whole genomes&quot;.  You could download the `hg19` reference and download two `GFF` files which would give around 1Gb worth of data (around 800Mb for the `2bit` file and around 200Mb for both of the `GFF` files).  Now you've been able to represent a &quot;whole genome&quot; in 500Mb instead of the 800Mb.  You can see a similar argument for downloading 3 `GFF` files and more.&#xD;&#xA;&#xD;&#xA;The minimum amount of information needed to represent a population of genomes is, as far as I know, unknown, but I would guess in the 2.5Mb-5Mb range.  For example, see [&quot;Human genomes as email attachments&quot; by Christley, Lu, Li and Xie](https://academic.oup.com/bioinformatics/article/25/2/274/218156/Human-genomes-as-email-attachments) which claims a 4Mb encoding of a genome.  &#xD;&#xA;&#xD;&#xA;Things get tricky because you have to ask what you're claiming as a &quot;whole genome&quot;.  `VCF` files are notoriously bad because older versions of the specification only store high quality differences from reference, throwing away high quality called sections.  If you want to store low quality information, the encoding is now going to depend on the sequencing technology in weird ways.&#xD;&#xA;&#xD;&#xA;Insertions, deletions, mobile insertion elements, copy number variants, other structural variants, etc. all complicate this matter further.  [Genome Graphs](http://biorxiv.org/content/early/2017/01/18/101378) are trying to tackle at least some of these problems but the focus is on variant calling rather than efficient individual whole genome representation, though perhaps can be adapted in the future.&#xD;&#xA;  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="382" PostHistoryTypeId="2" PostId="127" RevisionGUID="a2595501-d901-4bdb-8ff7-5d0da99e3483" CreationDate="2017-05-18T22:38:52.100" UserId="45" Text="There are models that take into account compositional heterogeneity both under the [maximum likelihood][1] and [Bayesian][2] frameworks. Although the substitution process is not time-reversible, the computations are [simplified by assuming][3] that the instantaneous rate matrix can be decomposed into an &quot;equilibrium frequency vector&quot; (non-homogeneous) and a symmetric, constant exchange rate matrix.&#xD;&#xA;&#xD;&#xA;I guess all your suggestions are also valid, and I remember recoding being used successfully to reduce the GC-content bias (examples in the references above and [here][4]).  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://%20https://doi.org/10.1080/10635150600975218&#xD;&#xA;  [2]: https://doi.org/10.1093/molbev/msl091&#xD;&#xA;  [3]: https://doi.org/10.1080/10635150490445779&#xD;&#xA;  [4]: https://doi.org/10.1093/molbev/msh137" />
  <row Id="386" PostHistoryTypeId="2" PostId="129" RevisionGUID="eb864949-1ea1-4cca-bfde-e9e83d6b6672" CreationDate="2017-05-19T01:18:14.417" UserId="150" Text="If your question is: can probeset IDs from different platforms be mapped to one another in a similar way as mapping probesets to genes, then the answer is: Yes. BioMart allows you to map almost anything that has an ID to anything else that has an ID.&#xD;&#xA;&#xD;&#xA;You can use BioMart either via the web interface or programatically. A brief guide to using the web interface, for mapping HG U133 Plus 2 to HuGene 1.0:&#xD;&#xA;&#xD;&#xA;1. Go to [the start page][1] &#xD;&#xA;1. Select H. sapiens Ensembl genes for your database; Human genes (GRCh38.p10) for your dataset&#xD;&#xA;1. Click Filters in the left-hand column&#xD;&#xA;1. Expand &quot;REGION&quot;, scroll down to GENE and select &quot;Input microarray probes/probesets ID list [Max 500 advised]&quot; &#xD;&#xA;1. Select AFFY HG U133 PLUS 2 probe ID(s) and either copy/paste or upload a list, one per line&#xD;&#xA;1. Click Attributes in the left-hand column&#xD;&#xA;1. Scroll through, uncheck what you don't want to see and choose what you do, for example Gene Stable ID, AFFY HuGene 1 0 st v1 probe and AFFY HG U133 Plus 2 probe&#xD;&#xA;1. Finally click &quot;Results&quot; in the menu at the top of the left-hand column&#xD;&#xA;&#xD;&#xA;And you should see [a result like this][2].&#xD;&#xA;&#xD;&#xA;You should not expect that there be a one-to-one mapping, for two reasons:&#xD;&#xA;&#xD;&#xA;- Genes have multiple transcripts and probesets are mapped to each transcript&#xD;&#xA;- Some transcripts have more than one probeset: in this case, the HuGene IDs 8002301 and 8002303 map to transcript for this gene&#xD;&#xA;&#xD;&#xA;Finally: here's a programmatic example using [R/BioMart][3]:&#xD;&#xA;&#xD;&#xA;    library(biomaRt)&#xD;&#xA;    mart.hs &lt;- useMart(&quot;ensembl&quot;, &quot;hsapiens_gene_ensembl&quot;)&#xD;&#xA;    results &lt;- getBM(attributes = c(&quot;ensembl_gene_id&quot;, &quot;affy_hugene_1_0_st_v1&quot;, &quot;affy_hg_u133_plus_2&quot;),&#xD;&#xA;                     filters = &quot;affy_hg_u133_plus_2&quot;,&#xD;&#xA;				     values = c(&quot;210519_s_at&quot;),&#xD;&#xA;				     mart = mart.hs)&#xD;&#xA;&#xD;&#xA;    results&#xD;&#xA;      ensembl_gene_id affy_hugene_1_0_st_v1 affy_hg_u133_plus_2&#xD;&#xA;    1 ENSG00000181019               8002301         210519_s_at&#xD;&#xA;    2 ENSG00000181019               8002303         210519_s_at&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ensembl.org/biomart/martview&#xD;&#xA;  [2]: http://www.ensembl.org/biomart/martview/5e02aa3df275d697dcd2c806c6e5c1a0&#xD;&#xA;  [3]: https://bioconductor.org/packages/release/bioc/html/biomaRt.html" />
  <row Id="387" PostHistoryTypeId="5" PostId="129" RevisionGUID="f98ff20d-6300-43f5-87f4-05b0ef186680" CreationDate="2017-05-19T01:28:17.620" UserId="150" Comment="added 44 characters in body" Text="If your question is: can probeset IDs from different platforms be mapped to one another in a similar way as mapping probesets to genes, then the answer is: Yes. BioMart allows you to map almost anything that has an ID to anything else that has an ID.&#xD;&#xA;&#xD;&#xA;You can use BioMart either via the web interface or programatically. A brief guide to using the web interface, for mapping HG U133 Plus 2 to HuGene 1.0:&#xD;&#xA;&#xD;&#xA;1. Go to [the start page][1] &#xD;&#xA;1. Select H. sapiens Ensembl genes for your database; Human genes (GRCh38.p10) for your dataset&#xD;&#xA;1. Click Filters in the left-hand column&#xD;&#xA;1. Expand &quot;REGION&quot;, scroll down to GENE and select &quot;Input microarray probes/probesets ID list [Max 500 advised]&quot; &#xD;&#xA;1. Select AFFY HG U133 PLUS 2 probe ID(s) and either copy/paste or upload a list, one per line&#xD;&#xA;1. Click Attributes in the left-hand column&#xD;&#xA;1. Scroll through, uncheck what you don't want to see and choose what you do, for example Gene Stable ID, AFFY HuGene 1 0 st v1 probe and AFFY HG U133 Plus 2 probe&#xD;&#xA;1. Finally click &quot;Results&quot; in the menu at the top of the left-hand column&#xD;&#xA;&#xD;&#xA;And you should see [a result like this][2] (you'll need to click the &quot;Results&quot; button).&#xD;&#xA;&#xD;&#xA;You should not expect that there be a one-to-one mapping, for two reasons:&#xD;&#xA;&#xD;&#xA;- Genes have multiple transcripts and probesets are mapped to each transcript&#xD;&#xA;- Some transcripts have more than one probeset: in this case, the HuGene IDs 8002301 and 8002303 map to transcript for this gene&#xD;&#xA;&#xD;&#xA;Finally: here's a programmatic example using [R/BioMart][3]:&#xD;&#xA;&#xD;&#xA;    library(biomaRt)&#xD;&#xA;    mart.hs &lt;- useMart(&quot;ensembl&quot;, &quot;hsapiens_gene_ensembl&quot;)&#xD;&#xA;    results &lt;- getBM(attributes = c(&quot;ensembl_gene_id&quot;, &quot;affy_hugene_1_0_st_v1&quot;, &quot;affy_hg_u133_plus_2&quot;),&#xD;&#xA;                     filters = &quot;affy_hg_u133_plus_2&quot;,&#xD;&#xA;				     values = c(&quot;210519_s_at&quot;),&#xD;&#xA;				     mart = mart.hs)&#xD;&#xA;&#xD;&#xA;    results&#xD;&#xA;      ensembl_gene_id affy_hugene_1_0_st_v1 affy_hg_u133_plus_2&#xD;&#xA;    1 ENSG00000181019               8002301         210519_s_at&#xD;&#xA;    2 ENSG00000181019               8002303         210519_s_at&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ensembl.org/biomart/martview&#xD;&#xA;  [2]: http://www.ensembl.org/biomart/martview/5e02aa3df275d697dcd2c806c6e5c1a0&#xD;&#xA;  [3]: https://bioconductor.org/packages/release/bioc/html/biomaRt.html" />
  <row Id="388" PostHistoryTypeId="5" PostId="129" RevisionGUID="d131d916-5977-43e6-b4dd-75f2422bdc6a" CreationDate="2017-05-19T01:39:33.997" UserId="150" Comment="added 1 character in body" Text="If your question is: can probeset IDs from different platforms be mapped to one another in a similar way as mapping probesets to genes, then the answer is: Yes. BioMart allows you to map almost anything that has an ID to anything else that has an ID.&#xD;&#xA;&#xD;&#xA;You can use BioMart either via the web interface or programatically. A brief guide to using the web interface, for mapping HG U133 Plus 2 to HuGene 1.0:&#xD;&#xA;&#xD;&#xA;1. Go to [the start page][1] &#xD;&#xA;1. Select H. sapiens Ensembl genes for your database; Human genes (GRCh38.p10) for your dataset&#xD;&#xA;1. Click Filters in the left-hand column&#xD;&#xA;1. Expand &quot;REGION&quot;, scroll down to GENE and select &quot;Input microarray probes/probesets ID list [Max 500 advised]&quot; &#xD;&#xA;1. Select AFFY HG U133 PLUS 2 probe ID(s) and either copy/paste or upload a list, one per line&#xD;&#xA;1. Click Attributes in the left-hand column&#xD;&#xA;1. Scroll through, uncheck what you don't want to see and choose what you do, for example Gene Stable ID, AFFY HuGene 1 0 st v1 probe and AFFY HG U133 Plus 2 probe&#xD;&#xA;1. Finally click &quot;Results&quot; in the menu at the top of the left-hand column&#xD;&#xA;&#xD;&#xA;And you should see [a result like this][2] (you'll need to click the &quot;Results&quot; button).&#xD;&#xA;&#xD;&#xA;You should not expect that there be a one-to-one mapping, for two reasons:&#xD;&#xA;&#xD;&#xA;- Genes have multiple transcripts and probesets are mapped to each transcript&#xD;&#xA;- Some transcripts have more than one probeset: in this case, the HuGene IDs 8002301 and 8002303 map to transcripts for this gene&#xD;&#xA;&#xD;&#xA;Finally: here's a programmatic example using [R/BioMart][3]:&#xD;&#xA;&#xD;&#xA;    library(biomaRt)&#xD;&#xA;    mart.hs &lt;- useMart(&quot;ensembl&quot;, &quot;hsapiens_gene_ensembl&quot;)&#xD;&#xA;    results &lt;- getBM(attributes = c(&quot;ensembl_gene_id&quot;, &quot;affy_hugene_1_0_st_v1&quot;, &quot;affy_hg_u133_plus_2&quot;),&#xD;&#xA;                     filters = &quot;affy_hg_u133_plus_2&quot;,&#xD;&#xA;				     values = c(&quot;210519_s_at&quot;),&#xD;&#xA;				     mart = mart.hs)&#xD;&#xA;&#xD;&#xA;    results&#xD;&#xA;      ensembl_gene_id affy_hugene_1_0_st_v1 affy_hg_u133_plus_2&#xD;&#xA;    1 ENSG00000181019               8002301         210519_s_at&#xD;&#xA;    2 ENSG00000181019               8002303         210519_s_at&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ensembl.org/biomart/martview&#xD;&#xA;  [2]: http://www.ensembl.org/biomart/martview/5e02aa3df275d697dcd2c806c6e5c1a0&#xD;&#xA;  [3]: https://bioconductor.org/packages/release/bioc/html/biomaRt.html" />
  <row Id="389" PostHistoryTypeId="2" PostId="130" RevisionGUID="eeadf255-66e0-4d49-a938-5a4388c40ebc" CreationDate="2017-05-19T01:53:56.150" UserId="51" Text="Instead of biomaRt, it is also possible to use the mapping databases built into Bioconductor itself, and map from probe to gene, and then from gene to probe in the second. Some R code to convert between hgu133 and hgu95 using the same probe ID provided in another:&#xD;&#xA;&#xD;&#xA;    library(hgu133plus2.db)&#xD;&#xA;    library(hgu95av2.db)&#xD;&#xA;&#xD;&#xA;    query_probe &lt;- &quot;210519_s_at&quot;&#xD;&#xA;&#xD;&#xA;    hgu133_ensembl &lt;- select(hgu133plus2.db, keys = query_probe, columns = &quot;ENSEMBL&quot;)&#xD;&#xA;&#xD;&#xA;    ensembl_hgu95 &lt;- select(hgu95av2.db, keys = hgu133_ensembl$ENSEMBL, keytype = &quot;ENSEMBL&quot;, columns = &quot;PROBEID&quot;)&#xD;&#xA;&#xD;&#xA;    dplyr::inner_join(hgu133_ensembl, ensembl_hgu95, by = &quot;ENSEMBL&quot;, suffix = c(&quot;.133&quot;, &quot;.95&quot;))&#xD;&#xA;" />
  <row Id="390" PostHistoryTypeId="10" PostId="103" RevisionGUID="90bd4dab-6b18-4668-a9ad-ab9b0771e765" CreationDate="2017-05-19T01:54:22.530" UserId="-1" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:163,&quot;DisplayName&quot;:&quot;Scott Gigante&quot;},{&quot;Id&quot;:3,&quot;DisplayName&quot;:&quot;Nicholas Mancuso&quot;},{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:35,&quot;DisplayName&quot;:&quot;burger&quot;},{&quot;Id&quot;:174,&quot;DisplayName&quot;:&quot;SmallChess&quot;}]}" />
  <row Id="391" PostHistoryTypeId="2" PostId="131" RevisionGUID="2a0e2aa2-7602-4082-8505-b4d76735cdc6" CreationDate="2017-05-19T01:57:32.150" UserId="51" Text="The [AbsID database](http://bioinformatics.louisville.edu/abid/) does conversion based on mapping the probe sequences to a genome build, and then determines mappings based on overlapping genome alignment coordinates. This is really useful if you want to be sure that two probes are *actually likely* measuring the same transcript.&#xD;&#xA;&#xD;&#xA;It is dependent on both probes aligning to the genome, however.&#xD;&#xA;&#xD;&#xA;Disclaimer: I worked in the group that provides the AbsID mapping tool." />
  <row Id="392" PostHistoryTypeId="5" PostId="131" RevisionGUID="2e8f3424-a323-48d7-87fa-28b5910fd67a" CreationDate="2017-05-19T02:16:24.147" UserId="51" Comment="note that other answers provide reasoning for not getting one to one" Text="Other answers explain why there might not be one to one mapping between the probes.&#xD;&#xA;&#xD;&#xA;The [AbsID database](http://bioinformatics.louisville.edu/abid/) does conversion based on mapping the probe sequences to a genome build, and then determines mappings based on overlapping genome alignment coordinates. This is really useful if you want to be sure that two probes are *actually likely* measuring the same transcript.&#xD;&#xA;&#xD;&#xA;It is dependent on both probes aligning to the genome, however.&#xD;&#xA;&#xD;&#xA;Disclaimer: I worked in the group that provides the AbsID mapping tool." />
  <row Id="393" PostHistoryTypeId="5" PostId="54" RevisionGUID="13f79bb6-3e1e-4e20-a957-bd27c99a8a75" CreationDate="2017-05-19T03:41:58.313" UserId="69" Comment="this is nature biotech, not nature" Text="You may consider using [RUVSeq][1]. Here is an excerpt from the [2013 Nature Biotechnology publication][2]:&#xD;&#xA;&#xD;&#xA;&gt; We evaluate the performance of the External RNA Control Consortium (ERCC) spike-in controls and investigate the possibility of using them directly for normalization. We show that the spike-ins are not reliable enough to be used in standard global-scaling or regression-based normalization procedures. We propose a normalization strategy, called remove unwanted variation (RUV), that adjusts for nuisance technical effects by performing factor analysis on suitable sets of control genes (e.g., ERCC spike-ins) or samples (e.g., replicate libraries).&#xD;&#xA;&#xD;&#xA;RUVSeq essentially fits a generalized linear model (GLM) to the expression data, where your expression matrix `Y` is a `m` by `n` matrix, where `m` is the number of samples and `n` the number of genes. The model boils down to&#xD;&#xA;&#xD;&#xA;    Y = X*beta + Z*gamma + W*alpha + epsilon&#xD;&#xA;&#xD;&#xA;where `X` describes the conditions of interest (e.g., treatment vs. control), `Z` describes observed covariates (e.g., gender) and `W` describes unobserved covariates (e.g., batch, temperature, lab). `beta`, `gamma` and `alpha` are parameter matrices which record the contribution of `X`, `Z` and `W`, and epsilon is random noise. For subset of carefully selected genes (e.g., ERCC spike-ins, housekeeping genes, or technical replicates) we can assume that `X` and `Z` are zero, and find `W` - the &quot;unwanted variation&quot; in your sample. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/html/RUVSeq.html&#xD;&#xA;  [2]: http://www.nature.com/nbt/journal/v32/n9/full/nbt.2931.html" />
  <row Id="394" PostHistoryTypeId="24" PostId="54" RevisionGUID="13f79bb6-3e1e-4e20-a957-bd27c99a8a75" CreationDate="2017-05-19T03:41:58.313" Comment="Proposed by 69 approved by 163 edit id of 31" />
  <row Id="398" PostHistoryTypeId="2" PostId="133" RevisionGUID="f6b8a502-7daa-4c31-9516-8bd652a48e91" CreationDate="2017-05-19T05:14:21.100" UserId="174" Text="We have many excellent answers! This will be an excellent reference for future users.&#xD;&#xA;&#xD;&#xA;I found what exactly what I was asking in my question:&#xD;&#xA;&#xD;&#xA;&gt; https://www.biostars.org/p/191052/&#xD;&#xA;&#xD;&#xA;    $ pip install pyfaidx  &#xD;&#xA;    $ faidx --transform bed test.fasta &gt; test.bed&#xD;&#xA;&#xD;&#xA;This is the one-line command I was asking. The other answers also work, but I want to accept my own answer." />
  <row Id="399" PostHistoryTypeId="2" PostId="134" RevisionGUID="ded28448-75d3-44e6-bec3-9db2e337939c" CreationDate="2017-05-19T05:49:46.937" UserId="174" Text="I have a BAM file:&#xD;&#xA;&#xD;&#xA;    @SQ	SN:chr1	LN:248956422&#xD;&#xA;    @SQ	SN:chrx	LN:248956423&#xD;&#xA;    ST-E00110:348:HGVKKALXX:1:1201:5822:48670	323	chr1	9999	0	67H66M16H	chrx	1000	0	GATAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	JJJJJJJJJJJJJJJJAJJJJJJJJJJJJFJJJJJJFJFJJJJJJFJJJJJJJJJJJA77FJFJJJ	NM:i:0	MD:Z:66	AS:i:66	XS:i:65	SA:Z:chr5,18606834,-,73S76M,34,0;	RG:Z:g1&#xD;&#xA;&#xD;&#xA;There is a read aligned to `chr1`, and it's mate aligned to `chrx`. &#xD;&#xA;&#xD;&#xA;I have a BED file:&#xD;&#xA;&#xD;&#xA;    chr1	0	100000	TestOnly&#xD;&#xA;&#xD;&#xA;I would like to filter out everything that falls outside my BED region, that includes cross-alignments. In my example, although my read aligned to `chr1` but it's mate is not. I don't want this read.&#xD;&#xA;&#xD;&#xA;When I did:&#xD;&#xA;&#xD;&#xA;&gt; samtools view -L test.bed test.bam&#xD;&#xA;&#xD;&#xA;the command gives me the read because it doesn't check cross-alignments.&#xD;&#xA;&#xD;&#xA;My solution:&#xD;&#xA;&#xD;&#xA;&gt; samtools view -L test.bed test.bam | grep -v chrx&#xD;&#xA;&#xD;&#xA;but this is very slow and clumsy. In my production pipeline I would have to do something like:&#xD;&#xA;&#xD;&#xA;&gt; samtools view -L test.bed test.bam | grep -v chrx | grep -v ... | grep -v ... | grep -v ... | grep -v ...&#xD;&#xA;&#xD;&#xA;**Q:** Is there a better solution?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="400" PostHistoryTypeId="1" PostId="134" RevisionGUID="ded28448-75d3-44e6-bec3-9db2e337939c" CreationDate="2017-05-19T05:49:46.937" UserId="174" Text="How to filter out cross alignments from a BED file?" />
  <row Id="401" PostHistoryTypeId="3" PostId="134" RevisionGUID="ded28448-75d3-44e6-bec3-9db2e337939c" CreationDate="2017-05-19T05:49:46.937" UserId="174" Text="&lt;genomics&gt;" />
  <row Id="402" PostHistoryTypeId="2" PostId="135" RevisionGUID="2c8ce9e2-aff1-47cb-b4a3-434f260e8f6b" CreationDate="2017-05-19T05:50:24.497" UserId="163" Text="I am working with over a million (long) reads, and aligning them to a large genome. I am considering running my alignment jobs in parallel, distributing horizontally across hundreds of nodes rather than trying to run a single job with dozens of cores.&#xD;&#xA;&#xD;&#xA;I would like to merge the sorted BAM files together for further downstream analysis. What is the most efficient way to do so while maintaining a valid file header and taking advantage of the fact that the input bam files are already sorted?" />
  <row Id="403" PostHistoryTypeId="1" PostId="135" RevisionGUID="2c8ce9e2-aff1-47cb-b4a3-434f260e8f6b" CreationDate="2017-05-19T05:50:24.497" UserId="163" Text="Merge hundreds of small BAM files into a single BAM file" />
  <row Id="404" PostHistoryTypeId="3" PostId="135" RevisionGUID="2c8ce9e2-aff1-47cb-b4a3-434f260e8f6b" CreationDate="2017-05-19T05:50:24.497" UserId="163" Text="&lt;alignment&gt;&lt;bam&gt;" />
  <row Id="406" PostHistoryTypeId="2" PostId="137" RevisionGUID="b0e72c0c-72b6-4fc2-b6db-a661a16a3a81" CreationDate="2017-05-19T07:06:35.520" UserId="77" Text="`samtools merge merged.bam *.bam` is efficient enough since the input files are sorted. You can get a bit faster with sambamba and/or biobambam, but they're not typically already installed and IO quickly becomes a bottleneck anyway." />
  <row Id="407" PostHistoryTypeId="2" PostId="138" RevisionGUID="725f5862-f858-4a7a-8a17-5ed76d5fca80" CreationDate="2017-05-19T07:08:03.053" UserId="-1" Text="" />
  <row Id="408" PostHistoryTypeId="1" PostId="138" RevisionGUID="725f5862-f858-4a7a-8a17-5ed76d5fca80" CreationDate="2017-05-19T07:08:03.053" UserId="-1" />
  <row Id="409" PostHistoryTypeId="2" PostId="139" RevisionGUID="94e49c58-3be1-4478-b3f1-2767227ce48b" CreationDate="2017-05-19T07:08:03.053" UserId="-1" Text="" />
  <row Id="410" PostHistoryTypeId="1" PostId="139" RevisionGUID="94e49c58-3be1-4478-b3f1-2767227ce48b" CreationDate="2017-05-19T07:08:03.053" UserId="-1" />
  <row Id="412" PostHistoryTypeId="2" PostId="140" RevisionGUID="5ef76f72-d8b9-4fae-b0c2-5e18ef155f78" CreationDate="2017-05-19T07:18:49.330" UserId="134" Text="Merging sorted files is a linear operation, so any well-implemented tools that do it will do it with approximately the same efficiency.  So `samtools merge` (use the most up-to-date version, as there have been improvements in merge header handling in the 1.3.x and 1.4.x versions), `picard MergeSamFiles`, etc.&#xD;&#xA;&#xD;&#xA;These tools need to hold all the input BAM files open simultaneously, so depending on how many hundred of input files you have you may run into the system limit on open file descriptors.  Use `ulimit` to maximise this first; if there are still too many, you may need to merge the first 500 files, then merge the next 500 into that, etc.  Samtools does not do this internally; I'm not sure whether any of the other merge implementations do." />
  <row Id="413" PostHistoryTypeId="6" PostId="134" RevisionGUID="cafbd636-de17-4a68-800c-7c0023301ec5" CreationDate="2017-05-19T09:13:27.873" UserId="131" Comment="added tags bam and bed" Text="&lt;bam&gt;&lt;bed&gt;&lt;genomics&gt;" />
  <row Id="414" PostHistoryTypeId="24" PostId="134" RevisionGUID="cafbd636-de17-4a68-800c-7c0023301ec5" CreationDate="2017-05-19T09:13:27.873" Comment="Proposed by 131 approved by 174 edit id of 34" />
  <row Id="415" PostHistoryTypeId="5" PostId="134" RevisionGUID="be7a2c07-5797-4521-9f8f-f258c2420525" CreationDate="2017-05-19T09:14:23.940" UserId="174" Comment="deleted 1 character in body" Text="I have a BAM file:&#xD;&#xA;&#xD;&#xA;    @SQ	SN:chr1	LN:248956422&#xD;&#xA;    @SQ	SN:chrx	LN:248956423&#xD;&#xA;    ST-E00110:348:HGVKKALXX:1:1201:5822:48670	323	chr1	9999	0	67H66M16H	chrx	1000	0	GATAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC	JJJJJJJJJJJJJJJJAJJJJJJJJJJJJFJJJJJJFJFJJJJJJFJJJJJJJJJJJA77FJFJJJ	NM:i:0	MD:Z:66	AS:i:66	XS:i:65	SA:Z:chr5,18606834,-,73S76M,34,0;	RG:Z:g1&#xD;&#xA;&#xD;&#xA;There is a read aligned to `chr1`, and it's mate aligned to `chrx`. &#xD;&#xA;&#xD;&#xA;I have a BED file:&#xD;&#xA;&#xD;&#xA;    chr1	0	100000	TestOnly&#xD;&#xA;&#xD;&#xA;I would like to filter out everything that falls outside my BED region, that includes cross-alignments. In my example, although my read aligned to `chr1` but it's mate is not. I don't want this read.&#xD;&#xA;&#xD;&#xA;When I do:&#xD;&#xA;&#xD;&#xA;&gt; samtools view -L test.bed test.bam&#xD;&#xA;&#xD;&#xA;the command gives me the read because it doesn't check cross-alignments.&#xD;&#xA;&#xD;&#xA;My solution:&#xD;&#xA;&#xD;&#xA;&gt; samtools view -L test.bed test.bam | grep -v chrx&#xD;&#xA;&#xD;&#xA;but this is very slow and clumsy. In my production pipeline I would have to do something like:&#xD;&#xA;&#xD;&#xA;&gt; samtools view -L test.bed test.bam | grep -v chrx | grep -v ... | grep -v ... | grep -v ... | grep -v ...&#xD;&#xA;&#xD;&#xA;**Q:** Is there a better solution?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="416" PostHistoryTypeId="2" PostId="141" RevisionGUID="a664b051-fc95-4ecd-ac0f-0c1e3b16d029" CreationDate="2017-05-19T09:18:45.637" UserId="292" Text="The following 2004 paper describes a way to model compositional changes across the tree, in a Bayesian framework: &lt;https://doi.org/10.1080/10635150490445779&gt;&#xD;&#xA;&#xD;&#xA;A python package implementing this (&quot;p4&quot;), and improvements added along the years, is available here: &lt;https://github.com/pgfoster/p4-phylogenetics&gt;&#xD;&#xA;&#xD;&#xA;To get started, you may find useful examples here: &lt;http://p4.nhm.ac.uk/scripts.html&gt;&#xD;&#xA;&#xD;&#xA;This has been used in a few large-scale phylogenetic analyses." />
  <row Id="417" PostHistoryTypeId="5" PostId="127" RevisionGUID="87712622-fae0-4b49-8a0f-410f7eedef8f" CreationDate="2017-05-19T09:24:00.620" UserId="191" Comment="correct url" Text="There are models that take into account compositional heterogeneity both under the [maximum likelihood][1] and [Bayesian][2] frameworks. Although the substitution process is not time-reversible, the computations are [simplified by assuming][3] that the instantaneous rate matrix can be decomposed into an &quot;equilibrium frequency vector&quot; (non-homogeneous) and a symmetric, constant exchange rate matrix.&#xD;&#xA;&#xD;&#xA;I guess all your suggestions are also valid, and I remember recoding being used successfully to reduce the GC-content bias (examples in the references above and [here][4]).  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://doi.org/10.1080/10635150600975218&#xD;&#xA;  [2]: https://doi.org/10.1093/molbev/msl091&#xD;&#xA;  [3]: https://doi.org/10.1080/10635150490445779&#xD;&#xA;  [4]: https://doi.org/10.1093/molbev/msh137" />
  <row Id="418" PostHistoryTypeId="24" PostId="127" RevisionGUID="87712622-fae0-4b49-8a0f-410f7eedef8f" CreationDate="2017-05-19T09:24:00.620" Comment="Proposed by 191 approved by 45 edit id of 37" />
  <row Id="419" PostHistoryTypeId="2" PostId="142" RevisionGUID="2f30bba0-2cfd-41da-aae8-25b382092da4" CreationDate="2017-05-19T09:33:58.507" UserId="235" Text="The way we deal with this is:&#xD;&#xA;&#xD;&#xA;* All work is done in a single filesystem mounted on the cluster&#xD;&#xA;* This file system is mounted on local machines via sshfs/samba (depending on the location of the current &quot;local&quot; machine on the network).&#xD;&#xA;* Code is versioned with git hub&#xD;&#xA;* All computation is carried out via light-weight automated pipelines. We use ruffus in combination with an in-house utility layer. The system doesn't really matter as long is it no more work to add another step to the pipeline than it would be to execute it manually. &#xD;&#xA;* All questionably design decisions are encoded in configuration files. These configuration files, along with a very detailed log output by the pipeline (what was run, what was the git commit of the code run, what was the time stamp of the files it was run on, etc) and the initial input files are version controlled.&#xD;&#xA;* The benefit of this is that code + configuration + time = output. It is not expected that the whole pipeline will be rerun everytime anything is changed, but the pipeline will tell you if something is out of date (it can use timestamps or file hashes), and it can all be run in one go before publication.&#xD;&#xA;* Any other analysis is carried out in juptyer notebooks. These are version controlled.&#xD;&#xA;&#xD;&#xA;To summarise, we don't synchronise because we only ever work from one disk location even if we use multiple CPU locations. We version control:&#xD;&#xA;&#xD;&#xA;* Code&#xD;&#xA;* Inputs, configuration, logs&#xD;&#xA;* Juptyer notebooks&#xD;&#xA;&#xD;&#xA;Log records the git commits used to produce the current outputs. &#xD;&#xA;" />
  <row Id="420" PostHistoryTypeId="2" PostId="143" RevisionGUID="8763bf3c-c7cd-4bb5-9286-2a0b85111fd9" CreationDate="2017-05-19T10:09:11.327" UserId="328" Text="I currently have ~180 whole germlines and around 10M snps/indels. I would like to build a predictive model using Machine Learning (ML) techniques to predict cancer risk according to these germline variants. The thing is, most of these 10M variants are not relevant to cancer, so first I have to annotate them and remove non-relevant snps/indels.&#xD;&#xA;&#xD;&#xA;Which tools/measurements do you think are important to use in order to filter out irrelevant variants and keep only, let's say, around 10,000 snps/indels?&#xD;&#xA;&#xD;&#xA;I guess the most obvious is CADD score, so I could keep those snps/indels with PHRED &gt;= 10.&#xD;&#xA;Which one would you suggest taking into account that downstream analysis would be ML algorithms that take as features variants that REALLY contribute to cancer?&#xD;&#xA;&#xD;&#xA;Many thanks,&#xD;&#xA;Eze" />
  <row Id="421" PostHistoryTypeId="1" PostId="143" RevisionGUID="8763bf3c-c7cd-4bb5-9286-2a0b85111fd9" CreationDate="2017-05-19T10:09:11.327" UserId="328" Text="what are the best annotations to get relevant variants associated with cancer risk?" />
  <row Id="422" PostHistoryTypeId="3" PostId="143" RevisionGUID="8763bf3c-c7cd-4bb5-9286-2a0b85111fd9" CreationDate="2017-05-19T10:09:11.327" UserId="328" Text="&lt;annotation&gt;&lt;machine&gt;&lt;learning&gt;" />
  <row Id="424" PostHistoryTypeId="2" PostId="144" RevisionGUID="fe1713aa-6b71-48e9-bb87-00d6678bbc9e" CreationDate="2017-05-19T10:18:40.500" UserId="156" Text="I think the best method or combination of methods will depend on aspects of the data that might vary from one dataset to another. E.g. the type, size, and frequency of structural variants, the number SNVs, the quality of the reference, contaminants or other issues (e.g. read quality, sequencing errors) etc.&#xD;&#xA;&#xD;&#xA;For that reason, I'd take two approaches:&#xD;&#xA;&#xD;&#xA;1. Try a lot of methods, and look at their overlap&#xD;&#xA;2. Validate a subset of calls from different methods - in the end this is the only real way of knowing the accuracy for a particular case." />
  <row Id="425" PostHistoryTypeId="2" PostId="145" RevisionGUID="29f609ec-a158-4e55-8ae0-30aff4664ac2" CreationDate="2017-05-19T10:24:51.223" UserId="222" Text="While your question is specific to cancerous germline mutations, I'd suggest you look at the [COSMIC][1] database of somatic mutations to include in your analysis. &#xD;&#xA;&#xD;&#xA;There are other factors to include in this kind of analysis you're suggesting, such as predictive deleterious effects (PolyPhen for example can perform such predictions).&#xD;&#xA;&#xD;&#xA;If you have 10M variants/indels, for the same form of cancer, then look for common variants, or maybe the frequency of variants identified across exons of a gene.&#xD;&#xA;&#xD;&#xA;What you've asked is highly broad and we'd need more specific details about your cohort to suggest more specific approaches. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cancer.sanger.ac.uk/cosmic" />
  <row Id="426" PostHistoryTypeId="2" PostId="146" RevisionGUID="38118246-2c03-4686-a1ea-f6c7898dd95a" CreationDate="2017-05-19T10:32:45.047" UserId="45" Text="In Computer Science a [De Bruijn graph][1] has (1) all possible `m^n` sequences of length `n`  (vertices) over `m` symbols, and (2) directed edges connecting nodes that differ by a shift of `n-1` elements (the successor having the new element at the right).&#xD;&#xA;&#xD;&#xA;However in Bioinformatics while condition (2) is preserved, what is called a De Bruijn graph [doesn't seem to respect condition][2] (1). In some cases the graph doesn't look anything like a de Bruijn graph at all (e.g. http://genome.cshlp.org/content/18/5/821.full).     &#xD;&#xA;&#xD;&#xA;So my question is, if I want to make it explicit that I am using the Bioinformatics interpretation of a de Bruijn graph, is there a term for it? Something like &quot;simplified de Bruijn graph&quot;, &quot;projection of a de Bruijn graph&quot;, or &quot;graph of neighbouring k-mers&quot;? Are there any papers making this distinction, or did I get it all wrong?&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/De_Bruijn_graph&#xD;&#xA;  [2]: http://www.homolog.us/Tutorials/index.php?p=2.1&amp;s=1" />
  <row Id="427" PostHistoryTypeId="1" PostId="146" RevisionGUID="38118246-2c03-4686-a1ea-f6c7898dd95a" CreationDate="2017-05-19T10:32:45.047" UserId="45" Text="How to make a distinction between the &quot;classical&quot; de Bruijn graph and the one described in NGS papers?" />
  <row Id="428" PostHistoryTypeId="3" PostId="146" RevisionGUID="38118246-2c03-4686-a1ea-f6c7898dd95a" CreationDate="2017-05-19T10:32:45.047" UserId="45" Text="&lt;ngs&gt;&lt;assembly&gt;&lt;k-mer&gt;&lt;de-bruijn&gt;" />
  <row Id="429" PostHistoryTypeId="5" PostId="43" RevisionGUID="85610d98-07cb-481c-a2f7-081cc3bb5ada" CreationDate="2017-05-19T10:36:42.200" UserId="113" Comment="removed reference to GFF.  Changed confusing &quot;full genome format/encoding&quot; of VCF to the more common &quot;variant format&quot;.  Mention the reference file and size explicitly in the same paragraph as VCF is introduced" Text="I think the question is a bit ambiguous so please excuse this answer that's a bit redundant from the rest of the ones provided.&#xD;&#xA;&#xD;&#xA;As others have mentioned, if you want to store a full genome, [`FASTA`](https://en.wikipedia.org/wiki/FASTA_format) and [`2bit`](https://genome.ucsc.edu/FAQ/FAQformat.html#format7) formats are appropriate.  For some context, [`hg19`](http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/) is about 900Mb compressed for the `FASTA` file and about 780Mb compressed for the `2bit` file .  `hg19` is a reference and is haploid so doesn't represent a &quot;full&quot; human genome that would normally have two alleles for the autosome (non-sex chromosomes).&#xD;&#xA;&#xD;&#xA;A common format for representing variant information is Variant Call Format ([`VCF`](https://vcftools.github.io/specs.html)). The `VCF` format represent differences from a reference (`hg19`, say) that can be used to recover the original full sequence by using the reference and the differences encoded in the `VCF` file.  I've seen `VCF` files in the range of 100Mb, but a reference file is still needed to recover the full genome sequence which is the range of 800Mb+, as mentioned above.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If you're considering just one &quot;whole genome&quot; in isolation, then the answer is pretty clear: `2bit` format is probably approaching the entropy limit of the human genome and you probably won't be able to do much better.&#xD;&#xA;The reason why your question is a bit ambiguous is that as soon as you start encoding more than one genome, a population of genomes, say, then you can start exploiting the redundancy of the genome as shared by the population.&#xD;&#xA;&#xD;&#xA;For example, say you want to store two &quot;whole genomes&quot;.  You could download the `hg19` reference and download two `VCF` files which would give around 1Gb worth of data (around 800Mb for the `2bit` file and around 200Mb for both of the `VCF` files).  Now you've been able to represent a &quot;whole genome&quot; in 500Mb instead of the 800Mb.  You can see a similar argument for downloading 3 `VCF` files and more.&#xD;&#xA;&#xD;&#xA;The minimum amount of information needed to represent a population of genomes is, as far as I know, unknown, but I would guess in the 2.5Mb-5Mb range.  For example, see [&quot;Human genomes as email attachments&quot; by Christley, Lu, Li and Xie](https://academic.oup.com/bioinformatics/article/25/2/274/218156/Human-genomes-as-email-attachments) which claims a 4Mb encoding of a genome.  &#xD;&#xA;&#xD;&#xA;Things get tricky because you have to ask what you're claiming as a &quot;whole genome&quot;.  `VCF` files are notoriously bad because older versions of the specification only store high quality differences from reference, throwing away high quality called sections.  If you want to store low quality information, the encoding is now going to depend on the sequencing technology in weird ways.&#xD;&#xA;&#xD;&#xA;Insertions, deletions, mobile insertion elements, copy number variants, other structural variants, etc. all complicate this matter further.  [Genome Graphs](http://biorxiv.org/content/early/2017/01/18/101378) are trying to tackle at least some of these problems but the focus is on variant calling rather than efficient individual whole genome representation, though perhaps can be adapted in the future.&#xD;&#xA;  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="430" PostHistoryTypeId="2" PostId="147" RevisionGUID="110e4f47-eb16-43c2-be64-becc10df3a16" CreationDate="2017-05-19T11:07:00.467" UserId="196" Text="In addition to the regular De Bruijn graph as depicted on the wikipedia, some implementations in bioinformatics feature additional processing. I guess the main reason figure 1 in the paper you linked (concerning the Velvet genome assembler) is slightly different is that a node represents *a series of overlapping k-mers*. In order to visualize this as a more classic De Bruin graph you would have to connect the k-mers depicted *above* the nodes. The caption next to figure one describes the processing quite clearly.&#xD;&#xA;&#xD;&#xA;As per your last question: I don't think there is a 'Bioinformatic interpretation of a De Bruijn graph'. There are different implementations, which all have there specifics. Thus it would be best to refer to the actual implementation.&#xD;&#xA;&#xD;&#xA;As an example: [this](https://www.ncbi.nlm.nih.gov/pubmed/27587666) is a nice paper on how to construct a pan-genome De Bruijn graph of multiple genomes simultaneously.  " />
  <row Id="431" PostHistoryTypeId="5" PostId="80" RevisionGUID="05951d75-230c-40d7-8949-f923954c72ea" CreationDate="2017-05-19T11:58:27.690" UserId="191" Comment="the" Text="There are two potential sources of bias in this design.&#xD;&#xA;&#xD;&#xA;1. We cannot distinguish correlation from causation. &#xD;&#xA;&#xD;&#xA;Imagine two cases. In the first, the disease progression is inducing immune response. Later stages will be associated with the higher gene expression levels. In the second scenario, the disease is caused by overexpression of a gene. Later stages will be also associated with the higher expression.&#xD;&#xA;&#xD;&#xA;This is typical for observational studies. But I just want to mention that a special care should be taken during interpretation of the results.&#xD;&#xA;&#xD;&#xA;2. If we are not following our individuals, we cannot distinguish correlation and avoidance.&#xD;&#xA;&#xD;&#xA;Let's say the disease is lethal in certain cases; the survival is negatively correlated with the disease stage. Now imagine there is a gene, which causes severe symptoms when highly expressed. On the later stages you will only observe those patients in which the gene was not highly expressed. From that you would conclude that gene expression is decreasing with the disease progression. In reality this gene is very important and causal, you just do not have patients which are alive to see this.&#xD;&#xA;&#xD;&#xA;This is similar to [Wald's studies of aircrafts][1] ([Survival bias](https://en.wikipedia.org/wiki/Survivorship_bias)).&#xD;&#xA;&#xD;&#xA;&gt; Researchers from the Center for Naval Analyses had conducted a study&#xD;&#xA;&gt; of the damage done to aircraft that had returned from missions, and&#xD;&#xA;&gt; had recommended that armor be added to the areas that showed the most&#xD;&#xA;&gt; damage.&#xD;&#xA;&gt; Wald proposed that the Navy instead reinforce the areas where the&#xD;&#xA;&gt; returning aircraft were unscathed, since those were the areas that, if&#xD;&#xA;&gt; hit, would cause the plane to be lost.&#xD;&#xA;&#xD;&#xA;I think the second point is crucial, and can and will lead to false conclusions. I do not see immediately how to avoid this bias if you study different individuals.&#xD;&#xA;&#xD;&#xA;Therefore I suggest that the same individuals are followed for a long time.&#xD;&#xA;&#xD;&#xA;There are different approaches you can use later. For example you can have two-stage procedure:&#xD;&#xA;&#xD;&#xA;1. Identify genes which are differentially expressed between healthy (H) and sick (A, B or C).&#xD;&#xA;2. Build a linear model of disease stage stage ~ gene1 + gene2 + ..., using genes identified at step 1.&#xD;&#xA;3. Similarly build a linear model of survival as a function gene expression.&#xD;&#xA;4. It is possible you can use logistic regression to infer probability of transition from stage B to stage C.&#xD;&#xA;&#xD;&#xA;This is not the only possible approach of course. As you suggested, Markov model is also applicable.&#xD;&#xA;&#xD;&#xA;While following the same individuals it is possible to use continuous-time Markov model for such a study. In this case discrete or continuous measurements are recorded at certain time points, and the model parameters are inferred using maximum likelihood.&#xD;&#xA;&#xD;&#xA;I'm not an expert in this field, but I think [this paper][2] describing the [msm][3] package for R will be useful. It also supports hidden Markov models.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Abraham_Wald&#xD;&#xA;  [2]: https://dx.doi.org/10.18637/jss.v038.i08&#xD;&#xA;  [3]: https://cran.r-project.org/web/packages/msm/index.html" />
  <row Id="436" PostHistoryTypeId="2" PostId="149" RevisionGUID="bbd359f7-abad-4d82-b5ff-8644c87cea6a" CreationDate="2017-05-19T12:32:54.203" UserId="48" Text="Several gene set enrichment methods are available, the most famous/popular is the [Broad Institute tool][1]. Many other tools are available (See for example the [biocView of GSE][2] which list 82 different packages). There are several parameters in consideration :&#xD;&#xA;&#xD;&#xA; - the statistic used to order the genes, &#xD;&#xA; - if it competitive or self-contained,&#xD;&#xA; - if it is supervised or not,&#xD;&#xA; - and how is the enrichment score calculated.&#xD;&#xA;&#xD;&#xA;I am using the [fgsea][3] package to calculate the enrichment scores and someone told me that the numbers are different from the ones on the Broad Institute despite all the other parameters being equivalent.&#xD;&#xA;&#xD;&#xA;Are these two methods (fgsea and Broad Institute GSEA) equivalent?&#xD;&#xA;&#xD;&#xA;I looked to the algorithms of both papers, and they seem fairly similar, but I don't know if in real datasets they are equivalent or not.&#xD;&#xA;&#xD;&#xA;&lt;sub&gt;Is there any article reviewing and comparing how does the enrichment score method affect to the result?&lt;/sub&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://software.broadinstitute.org/gsea/index.jsp&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/BiocViews.html#___GeneSetEnrichment&#xD;&#xA;  [3]: http://bioconductor.org/packages/fgsea" />
  <row Id="437" PostHistoryTypeId="1" PostId="149" RevisionGUID="bbd359f7-abad-4d82-b5ff-8644c87cea6a" CreationDate="2017-05-19T12:32:54.203" UserId="48" Text="Are fgsea and Broad Institute GSEA equivalent?" />
  <row Id="438" PostHistoryTypeId="3" PostId="149" RevisionGUID="bbd359f7-abad-4d82-b5ff-8644c87cea6a" CreationDate="2017-05-19T12:32:54.203" UserId="48" Text="&lt;gse&gt;" />
  <row Id="443" PostHistoryTypeId="2" PostId="150" RevisionGUID="7fb0523d-21bf-475c-9d2b-48afa0c2d61c" CreationDate="2017-05-19T13:57:30.553" UserId="200" Text="Can someone describe what tools and steps one needs to take to run a certain job in parallel if functions are not designed to be run in parallel out of the box?&#xD;&#xA;&#xD;&#xA;For example, imagine having [this script](http://datadryad.org/bitstream/handle/10255/dryad.122664/obitools_script.txt?sequence=1), how would you run it in parallel to cut down on time?&#xD;&#xA;&#xD;&#xA;    illuminapairedend -r rawdata_scandinavia_R2.fastq rawdata_scandinavia_R1.fastq | tee rawdata_scandinavia.fastq | obiannotate -S goodAli:'&quot;Alignement&quot; if score&gt;40.00 else &quot;Bad&quot;' | obisplit -t goodAli -p rawdata_scandinavia.&#xD;&#xA;    touch rawdata_scandinavia.Bad.fastq&#xD;&#xA;    touch rawdata_scandinavia.Alignement.fastq&#xD;&#xA;    touch rawdata_scandinavia.fastq&#xD;&#xA;    ngsfilter -t rawdata_scandinavia.ngsfilter -u rawdata_scandinavia.unidentified.fastq rawdata_scandinavia.Alignement.fastq &gt; rawdata_scandinavia.filtered.fastq&#xD;&#xA;    obisplit -p MICROSAT.PCR_ -t experiment rawdata_scandinavia.filtered.fastq" />
  <row Id="444" PostHistoryTypeId="1" PostId="150" RevisionGUID="7fb0523d-21bf-475c-9d2b-48afa0c2d61c" CreationDate="2017-05-19T13:57:30.553" UserId="200" Text="parallel processing of scripts" />
  <row Id="445" PostHistoryTypeId="3" PostId="150" RevisionGUID="7fb0523d-21bf-475c-9d2b-48afa0c2d61c" CreationDate="2017-05-19T13:57:30.553" UserId="200" Text="&lt;ngs&gt;&lt;obitools&gt;&lt;linux&gt;&lt;parallel&gt;" />
  <row Id="446" PostHistoryTypeId="2" PostId="151" RevisionGUID="c05d89c0-15e4-4dea-ab2d-a55db1ec7381" CreationDate="2017-05-19T14:03:08.770" UserId="77" Text="Your only option here is to first split the fastq files into small chunks, process each and then merge them back together. Alternatively, find something else that runs faster." />
  <row Id="447" PostHistoryTypeId="2" PostId="152" RevisionGUID="8cdbd7aa-e02a-4714-b0dd-85e0bd015d01" CreationDate="2017-05-19T14:14:34.317" UserId="37" Text="Let's first assume DNA only has one strand. An assembly de Bruijn graph is a subgraph of a complete de Bruijn graph. It contains a vertex u if u is a k-mer in reads; it contains an edge u-&gt;v, if u and v are adjacent k-mers on a read. Alternatively, we note that an edge u-&gt;v is represented by a (k+1)-mer. An assembly de Bruijn graph can be considered a subgraph edge induced from all (k+1)-mers in reads – in fact, some assemblers take the list of (k+1)-mer as a succinct representation of de Bruijn graphs.&#xD;&#xA;&#xD;&#xA;DNA has two strands. We just need to induce an assembly de Bruijn graph from all (k+1)-mers and their reverse complement. It is still a subgraph of a complete de Bruijn graph.&#xD;&#xA;&#xD;&#xA;Because an assembly de Bruijn graph is just a subgraph. It is not necessary to give it a new name.&#xD;&#xA;&#xD;&#xA;PS: I deleted my old answer as that was not what you are asking for based on your comments. I was confused by your mentioning velvet. Velvet uses an equivalent but uncommon representation of de Bruijn graphs, which complicates your question." />
  <row Id="448" PostHistoryTypeId="2" PostId="153" RevisionGUID="61a21f4c-a73c-4a6f-8585-e0874be691de" CreationDate="2017-05-19T14:40:13.787" UserId="292" Text="I have a set of high-troughput experiments with 2 genotypes (&quot;WT&quot; and &quot;prg1&quot;) and 3 treatments (&quot;RT&quot;, &quot;HS30&quot; and &quot;HS30RT120&quot;), and there are 2 replicates for each of the genotype x treatment combinations.&#xD;&#xA;&#xD;&#xA;The read counts for the genes are summarized in a file that I load as follows in R:&#xD;&#xA;&#xD;&#xA;    &gt; counts_data &lt;- read.table(&quot;path/to/my/file&quot;, header=TRUE, row.names=&quot;gene&quot;)&#xD;&#xA;    &gt; colnames(counts_data)&#xD;&#xA;     [1] &quot;WT_RT_1&quot;          &quot;WT_HS30_1&quot;        &quot;WT_HS30RT120_1&quot;   &quot;prg1_RT_1&quot;       &#xD;&#xA;     [5] &quot;prg1_HS30_1&quot;      &quot;prg1_HS30RT120_1&quot; &quot;WT_RT_2&quot;          &quot;WT_HS30_2&quot;       &#xD;&#xA;     [9] &quot;WT_HS30RT120_2&quot;   &quot;prg1_RT_2&quot;        &quot;prg1_HS30_2&quot;      &quot;prg1_HS30RT120_2&quot;&#xD;&#xA;&#xD;&#xA;I describe the experiments as follows:&#xD;&#xA;&#xD;&#xA;    &gt; col_data &lt;- DataFrame(&#xD;&#xA;        geno = c(rep(&quot;WT&quot;, times=3), rep(&quot;prg1&quot;, times=3), rep(&quot;WT&quot;, times=3), rep(&quot;prg1&quot;, times=3)),&#xD;&#xA;        treat = rep(c(&quot;RT&quot;, &quot;HS30&quot;, &quot;HS30RT120&quot;), times=4),&#xD;&#xA;        rep = c(rep(&quot;1&quot;, times=6), rep(&quot;2&quot;, times=6)),&#xD;&#xA;        row.names = colnames(counts_data))&#xD;&#xA;    &gt; col_data&#xD;&#xA;    DataFrame with 12 rows and 3 columns&#xD;&#xA;                            geno       treat         rep&#xD;&#xA;                     &lt;character&gt; &lt;character&gt; &lt;character&gt;&#xD;&#xA;    WT_RT_1                   WT          RT           1&#xD;&#xA;    WT_HS30_1                 WT        HS30           1&#xD;&#xA;    WT_HS30RT120_1            WT   HS30RT120           1&#xD;&#xA;    prg1_RT_1               prg1          RT           1&#xD;&#xA;    prg1_HS30_1             prg1        HS30           1&#xD;&#xA;    ...                      ...         ...         ...&#xD;&#xA;    WT_HS30_2                 WT        HS30           2&#xD;&#xA;    WT_HS30RT120_2            WT   HS30RT120           2&#xD;&#xA;    prg1_RT_2               prg1          RT           2&#xD;&#xA;    prg1_HS30_2             prg1        HS30           2&#xD;&#xA;    prg1_HS30RT120_2        prg1   HS30RT120           2&#xD;&#xA;&#xD;&#xA;I want to build a DESeq2 object that I could use to find differentially expressed genes when either the treatment varies for a given fixed genotype, or the genotype varies for a given fixed treatment.&#xD;&#xA;&#xD;&#xA;In the [bioconductor help forum](https://support.bioconductor.org/p/74101/#74163) I think I've found a somewhat similar situation, and I read the following:&#xD;&#xA;&#xD;&#xA;&gt; Try a design of ~ genotype + genotype:condition&#xD;&#xA;&#xD;&#xA;&gt; Then you will have a condition effect for each level of genotype, including the reference level.&#xD;&#xA;&#xD;&#xA;&gt; You can constrast pairs of them using the list style of the 'contrast' argument.&#xD;&#xA;&#xD;&#xA;However, this doesn't explain how to do apply this &quot;list style&quot; to the &quot;contrast&quot; argument. And the above situation seems to be asymmetrical. By that I mean that genotype and condition do not seem to have an interchangeable role.&#xD;&#xA;&#xD;&#xA;So I tried the following more symmetric formula:&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeqDataSetFromMatrix(&#xD;&#xA;                                    countData = counts_data,&#xD;&#xA;                                    colData = col_data,&#xD;&#xA;                                    design = ~ geno + treat + geno:treat)&#xD;&#xA;    &gt; dds &lt;- DESeq(dds)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Now, can I for instance get the differential expression results when comparing treatment &quot;HS30&quot; against &quot;RT&quot; as a reference, in genotype &quot;prg1&quot;?&#xD;&#xA;&#xD;&#xA;And how?&#xD;&#xA;&#xD;&#xA;If I understand correctly, the above-mentioned &quot;list style&quot; uses names given by the `resultsNames` function. In my case, I have the following:&#xD;&#xA;&#xD;&#xA;    &gt; resultsNames(dds)&#xD;&#xA;    [1] &quot;Intercept&quot;               &quot;geno_WT_vs_prg1&quot;        &#xD;&#xA;    [3] &quot;treat_HS30RT120_vs_HS30&quot; &quot;treat_RT_vs_HS30&quot;       &#xD;&#xA;    [5] &quot;genoWT.treatHS30RT120&quot;   &quot;genoWT.treatRT&quot;&#xD;&#xA;&#xD;&#xA;I guess I would need a contrast between &quot;genoprg1.treatRT&quot; and a &quot;genoprg1.treatHS30&quot;, but these are not in the above results names.&#xD;&#xA;&#xD;&#xA;I'm lost." />
  <row Id="449" PostHistoryTypeId="1" PostId="153" RevisionGUID="61a21f4c-a73c-4a6f-8585-e0874be691de" CreationDate="2017-05-19T14:40:13.787" UserId="292" Text="Understanding DESeq2 design, contrast and results" />
  <row Id="450" PostHistoryTypeId="3" PostId="153" RevisionGUID="61a21f4c-a73c-4a6f-8585-e0874be691de" CreationDate="2017-05-19T14:40:13.787" UserId="292" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;deseq2&gt;" />
  <row Id="452" PostHistoryTypeId="2" PostId="154" RevisionGUID="f164a274-50a5-49ee-8625-095b041ffa38" CreationDate="2017-05-19T14:51:01.017" UserId="77" Text="The simplest manner is to not use a wald test, but rather an LRT with a reduced model lacking the factor of interest:&#xD;&#xA;&#xD;&#xA;    dds = DESeq(dds, test=&quot;LRT&quot; reduced=~geno+geno:Treatment)&#xD;&#xA;&#xD;&#xA;The above would give you results for Treatment regardless of level while still accounting for a possible interaction (i.e., a &quot;main effect of treatment, regardless of the type of treatment&quot;).&#xD;&#xA;&#xD;&#xA;As an aside, this is probably a case where the edgeR-preferred way of creating groups of genotype-treatment combinations and then using a model of `~0+group` might make your life a bit easier. You'll get the same results (more or less) regardless, but it'll probably be easier for you to think in those terms rather than remembering that the base level will be `treatment HS30` and `geno prg1`." />
  <row Id="453" PostHistoryTypeId="5" PostId="143" RevisionGUID="ad46851b-aa56-46df-afee-299327ef213b" CreationDate="2017-05-19T14:53:32.100" UserId="191" Comment="improve formatting &amp; remove &quot;thanks&quot; at the end" Text="I currently have ~180 whole germlines and around 10M SNPs/indels. I would like to build a predictive model using Machine Learning (ML) techniques to predict cancer risk according to these germline variants. The thing is, most of these 10M variants are not relevant to cancer, so first I have to annotate them and remove non-relevant SNPs/indels.&#xD;&#xA;&#xD;&#xA;Which tools/measurements do you think are important to use in order to filter out irrelevant variants and keep only, let's say, around 10,000 SNPs/indels?&#xD;&#xA;&#xD;&#xA;I guess the most obvious is CADD score, so I could keep those SNPs/indels with PHRED &gt;= 10.&#xD;&#xA;Which one would you suggest taking into account that downstream analysis would be ML algorithms that take as features variants that **really** contribute to cancer?" />
  <row Id="454" PostHistoryTypeId="4" PostId="143" RevisionGUID="ad46851b-aa56-46df-afee-299327ef213b" CreationDate="2017-05-19T14:53:32.100" UserId="191" Comment="improve formatting &amp; remove &quot;thanks&quot; at the end" Text="What are the best annotations to get relevant variants associated with cancer risk?" />
  <row Id="455" PostHistoryTypeId="6" PostId="143" RevisionGUID="ad46851b-aa56-46df-afee-299327ef213b" CreationDate="2017-05-19T14:53:32.100" UserId="191" Comment="improve formatting &amp; remove &quot;thanks&quot; at the end" Text="&lt;annotation&gt;&lt;machine-learning&gt;&lt;cancer&gt;" />
  <row Id="456" PostHistoryTypeId="24" PostId="143" RevisionGUID="ad46851b-aa56-46df-afee-299327ef213b" CreationDate="2017-05-19T14:53:32.100" Comment="Proposed by 191 approved by 328 edit id of 38" />
  <row Id="457" PostHistoryTypeId="5" PostId="146" RevisionGUID="69f371a1-dd10-41fd-b9d6-c19b345c79c6" CreationDate="2017-05-19T14:56:34.173" UserId="57" Comment="reformulating condition 1 (before it sounded like there should be `m` verteces in the graph)" Text="In Computer Science a [De Bruijn graph][1] has (1) `m^n` vertices representing all possible  sequences of length `n` over `m` symbols, and (2) directed edges connecting nodes that differ by a shift of `n-1` elements (the successor having the new element at the right).&#xD;&#xA;&#xD;&#xA;However in Bioinformatics while condition (2) is preserved, what is called a De Bruijn graph [doesn't seem to respect condition][2] (1). In some cases the graph doesn't look anything like a de Bruijn graph at all (e.g. http://genome.cshlp.org/content/18/5/821.full).     &#xD;&#xA;&#xD;&#xA;So my question is, if I want to make it explicit that I am using the Bioinformatics interpretation of a de Bruijn graph, is there a term for it? Something like &quot;simplified de Bruijn graph&quot;, &quot;projection of a de Bruijn graph&quot;, or &quot;graph of neighbouring k-mers&quot;? Are there any papers making this distinction, or did I get it all wrong?&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/De_Bruijn_graph&#xD;&#xA;  [2]: http://www.homolog.us/Tutorials/index.php?p=2.1&amp;s=1" />
  <row Id="458" PostHistoryTypeId="24" PostId="146" RevisionGUID="69f371a1-dd10-41fd-b9d6-c19b345c79c6" CreationDate="2017-05-19T14:56:34.173" Comment="Proposed by 57 approved by 45 edit id of 39" />
  <row Id="459" PostHistoryTypeId="4" PostId="143" RevisionGUID="18a753e1-2763-4f9e-8b5b-74f16e4460d0" CreationDate="2017-05-19T14:58:12.770" UserId="328" Comment="edited title" Text="How can I use annotations to remove variants not relevant to cancer risk?" />
  <row Id="460" PostHistoryTypeId="5" PostId="153" RevisionGUID="ed2daedc-fb48-4c50-8abe-8f56243671c5" CreationDate="2017-05-19T16:14:02.053" UserId="292" Comment="Tried to highlight the practical questions" Text="I have a set of high-troughput experiments with 2 genotypes (&quot;WT&quot; and &quot;prg1&quot;) and 3 treatments (&quot;RT&quot;, &quot;HS30&quot; and &quot;HS30RT120&quot;), and there are 2 replicates for each of the genotype x treatment combinations.&#xD;&#xA;&#xD;&#xA;The read counts for the genes are summarized in a file that I load as follows in R:&#xD;&#xA;&#xD;&#xA;    &gt; counts_data &lt;- read.table(&quot;path/to/my/file&quot;, header=TRUE, row.names=&quot;gene&quot;)&#xD;&#xA;    &gt; colnames(counts_data)&#xD;&#xA;     [1] &quot;WT_RT_1&quot;          &quot;WT_HS30_1&quot;        &quot;WT_HS30RT120_1&quot;   &quot;prg1_RT_1&quot;       &#xD;&#xA;     [5] &quot;prg1_HS30_1&quot;      &quot;prg1_HS30RT120_1&quot; &quot;WT_RT_2&quot;          &quot;WT_HS30_2&quot;       &#xD;&#xA;     [9] &quot;WT_HS30RT120_2&quot;   &quot;prg1_RT_2&quot;        &quot;prg1_HS30_2&quot;      &quot;prg1_HS30RT120_2&quot;&#xD;&#xA;&#xD;&#xA;I describe the experiments as follows:&#xD;&#xA;&#xD;&#xA;    &gt; col_data &lt;- DataFrame(&#xD;&#xA;        geno = c(rep(&quot;WT&quot;, times=3), rep(&quot;prg1&quot;, times=3), rep(&quot;WT&quot;, times=3), rep(&quot;prg1&quot;, times=3)),&#xD;&#xA;        treat = rep(c(&quot;RT&quot;, &quot;HS30&quot;, &quot;HS30RT120&quot;), times=4),&#xD;&#xA;        rep = c(rep(&quot;1&quot;, times=6), rep(&quot;2&quot;, times=6)),&#xD;&#xA;        row.names = colnames(counts_data))&#xD;&#xA;    &gt; col_data&#xD;&#xA;    DataFrame with 12 rows and 3 columns&#xD;&#xA;                            geno       treat         rep&#xD;&#xA;                     &lt;character&gt; &lt;character&gt; &lt;character&gt;&#xD;&#xA;    WT_RT_1                   WT          RT           1&#xD;&#xA;    WT_HS30_1                 WT        HS30           1&#xD;&#xA;    WT_HS30RT120_1            WT   HS30RT120           1&#xD;&#xA;    prg1_RT_1               prg1          RT           1&#xD;&#xA;    prg1_HS30_1             prg1        HS30           1&#xD;&#xA;    ...                      ...         ...         ...&#xD;&#xA;    WT_HS30_2                 WT        HS30           2&#xD;&#xA;    WT_HS30RT120_2            WT   HS30RT120           2&#xD;&#xA;    prg1_RT_2               prg1          RT           2&#xD;&#xA;    prg1_HS30_2             prg1        HS30           2&#xD;&#xA;    prg1_HS30RT120_2        prg1   HS30RT120           2&#xD;&#xA;&#xD;&#xA;I want to build a DESeq2 object that I could use to either:&#xD;&#xA;&#xD;&#xA;- **find differentially expressed genes when the treatment varies for a given fixed genotype**&#xD;&#xA;&#xD;&#xA;or:&#xD;&#xA;&#xD;&#xA;- **find differentially expressed genes when the genotype varies for a given fixed treatment**&#xD;&#xA;&#xD;&#xA;In the [bioconductor help forum](https://support.bioconductor.org/p/74101/#74163) I think I've found a somewhat similar situation, and I read the following:&#xD;&#xA;&#xD;&#xA;&gt; Try a design of ~ genotype + genotype:condition&#xD;&#xA;&#xD;&#xA;&gt; Then you will have a condition effect for each level of genotype, including the reference level.&#xD;&#xA;&#xD;&#xA;&gt; You can constrast pairs of them using the list style of the 'contrast' argument.&#xD;&#xA;&#xD;&#xA;However, this doesn't explain how to do apply this &quot;list style&quot; to the &quot;contrast&quot; argument. And the above situation seems to be asymmetrical. By that I mean that genotype and condition do not seem to have an interchangeable role.&#xD;&#xA;&#xD;&#xA;So I tried the following more symmetric formula:&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeqDataSetFromMatrix(&#xD;&#xA;                                    countData = counts_data,&#xD;&#xA;                                    colData = col_data,&#xD;&#xA;                                    design = ~ geno + treat + geno:treat)&#xD;&#xA;    &gt; dds &lt;- DESeq(dds)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Now, can I for instance **get the differential expression results when comparing treatment &quot;HS30&quot; against &quot;RT&quot; as a reference, in genotype &quot;prg1&quot;**?&#xD;&#xA;&#xD;&#xA;And how?&#xD;&#xA;&#xD;&#xA;If I understand correctly, the above-mentioned &quot;list style&quot; uses names given by the `resultsNames` function. In my case, I have the following:&#xD;&#xA;&#xD;&#xA;    &gt; resultsNames(dds)&#xD;&#xA;    [1] &quot;Intercept&quot;               &quot;geno_WT_vs_prg1&quot;        &#xD;&#xA;    [3] &quot;treat_HS30RT120_vs_HS30&quot; &quot;treat_RT_vs_HS30&quot;       &#xD;&#xA;    [5] &quot;genoWT.treatHS30RT120&quot;   &quot;genoWT.treatRT&quot;&#xD;&#xA;&#xD;&#xA;I guess I would need a contrast between &quot;genoprg1.treatRT&quot; and a &quot;genoprg1.treatHS30&quot;, but these are not in the above results names.&#xD;&#xA;&#xD;&#xA;I'm lost." />
  <row Id="461" PostHistoryTypeId="2" PostId="155" RevisionGUID="c95c364e-56d5-437a-b848-4226531e5ee0" CreationDate="2017-05-19T17:44:29.193" UserId="298" Text="According to the [SAM specificaction][1], the 3rd field of a SAM line (`RNAME`) is:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt; RNAME: Reference sequence NAME of the alignment. If @SQ header lines&#xD;&#xA;&gt; are present, RNAME (if not ‘\*’) must be present in one of the SQ-SN&#xD;&#xA;&gt; tag. An unmapped segment without coordinate has a ‘\*’ at this field.&#xD;&#xA;&gt; However, an unmapped segment may also have an ordinary coordinate such&#xD;&#xA;&gt; that it can be placed at a desired position after sorting. If RNAME is&#xD;&#xA;&gt; ‘\*’, no assumptions can be made about POS and CIGAR.&#xD;&#xA;&#xD;&#xA;And the 7th field is (emphasis mine, missing &quot;to&quot; theirs):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt;  RNEXT: Reference sequence name of the primary alignment of the NEXT&#xD;&#xA;&gt; read in the template. For the last read, the next read is the first&#xD;&#xA;&gt; read in the template. If @SQ header lines are present, RNEXT (if not&#xD;&#xA;&gt; ‘\*’ or ‘=’) must be present in one of the SQ-SN tag. This field is set&#xD;&#xA;&gt; as ‘\*’ when the information is unavailable, **and set as ‘=’ if RNEXT is&#xD;&#xA;&gt; identical RNAME**. If not ‘=’ and the next read in the template has one&#xD;&#xA;&gt; primary mapping (see also bit 0x100 in FLAG), this field is identical&#xD;&#xA;&gt; to RNAME at the primary line of the next read. If RNEXT is ‘\*’, no&#xD;&#xA;&gt; assumptions can be made on PNEXT and bit 0x20&#xD;&#xA;&#xD;&#xA;So, you want to remove those lines whose 7th field isn't `=` and, just in case, those lines whose 7th field isn't `=` *and* isn't the same as the 3rd field. You can therefore use something like this:&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | awk '$7!=&quot;=&quot; &amp;&amp; $3!=$7&#xD;&#xA;    &#xD;&#xA;And, to save as a bam file again:&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | awk '$7!=&quot;=&quot; &amp;&amp; $3!=$7 | &#xD;&#xA;        samtolls view -b &gt; fixed.bam&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;On a separate note, there's very rarely a need to chain multiple grep commands like that. You can just use `\|` (or `|` with the `-E` or `P` options) to separate them. Something like:&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | grep -v 'chrx\|chr2\|chr10\|chrN'&#xD;&#xA;&#xD;&#xA;Or&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | grep -Ev 'chrx|chr2|chr10|chrN'&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://samtools.github.io/hts-specs/SAMv1.pdf&#xD;&#xA;" />
  <row Id="462" PostHistoryTypeId="10" PostId="150" RevisionGUID="52be0d97-c4bc-443d-9607-6c8e12be3271" CreationDate="2017-05-19T17:57:56.623" UserId="-1" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:104,&quot;DisplayName&quot;:&quot;Chris_Rands&quot;},{&quot;Id&quot;:176,&quot;DisplayName&quot;:&quot;JonMark Perry&quot;},{&quot;Id&quot;:109,&quot;DisplayName&quot;:&quot;Harry&quot;},{&quot;Id&quot;:174,&quot;DisplayName&quot;:&quot;SmallChess&quot;},{&quot;Id&quot;:191,&quot;DisplayName&quot;:&quot;Iakov Davydov&quot;}]}" />
  <row Id="463" PostHistoryTypeId="12" PostId="155" RevisionGUID="50ba3f64-0e8f-40c4-8688-8f7cfa3b011e" CreationDate="2017-05-19T18:06:01.083" UserId="298" Comment="via Vote" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;}]}" />
  <row Id="464" PostHistoryTypeId="5" PostId="155" RevisionGUID="ca7728b4-aeb8-41fe-8103-e2d248ea19d4" CreationDate="2017-05-19T18:08:30.050" UserId="298" Comment="Whoops!" Text="According to the [SAM specificaction][1], the 3rd field of a SAM line (`RNAME`) is:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt; RNAME: Reference sequence NAME of the alignment. If @SQ header lines&#xD;&#xA;&gt; are present, RNAME (if not ‘\*’) must be present in one of the SQ-SN&#xD;&#xA;&gt; tag. An unmapped segment without coordinate has a ‘\*’ at this field.&#xD;&#xA;&gt; However, an unmapped segment may also have an ordinary coordinate such&#xD;&#xA;&gt; that it can be placed at a desired position after sorting. If RNAME is&#xD;&#xA;&gt; ‘\*’, no assumptions can be made about POS and CIGAR.&#xD;&#xA;&#xD;&#xA;And the 7th field is (emphasis mine, missing &quot;to&quot; theirs):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt;  RNEXT: Reference sequence name of the primary alignment of the NEXT&#xD;&#xA;&gt; read in the template. For the last read, the next read is the first&#xD;&#xA;&gt; read in the template. If @SQ header lines are present, RNEXT (if not&#xD;&#xA;&gt; ‘\*’ or ‘=’) must be present in one of the SQ-SN tag. This field is set&#xD;&#xA;&gt; as ‘\*’ when the information is unavailable, **and set as ‘=’ if RNEXT is&#xD;&#xA;&gt; identical RNAME**. If not ‘=’ and the next read in the template has one&#xD;&#xA;&gt; primary mapping (see also bit 0x100 in FLAG), this field is identical&#xD;&#xA;&gt; to RNAME at the primary line of the next read. If RNEXT is ‘\*’, no&#xD;&#xA;&gt; assumptions can be made on PNEXT and bit 0x20&#xD;&#xA;&#xD;&#xA;So, you want to remove those lines whose 7th field isn't `=` and, just in case, those lines whose 7th field isn't `=` *and* isn't the same as the 3rd field. You can therefore use something like this:&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | awk '$7==&quot;=&quot; || $3==$7&#xD;&#xA;    &#xD;&#xA;And, to save as a bam file again:&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | awk '$7==&quot;=&quot; &amp;&amp; $3==$7 | &#xD;&#xA;        samtolls view -b &gt; fixed.bam&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;On a separate note, there's very rarely a need to chain multiple grep commands like that. You can just use `\|` (or `|` with the `-E` or `P` options) to separate them. Something like:&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | grep -v 'chrx\|chr2\|chr10\|chrN'&#xD;&#xA;&#xD;&#xA;Or&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | grep -Ev 'chrx|chr2|chr10|chrN'&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://samtools.github.io/hts-specs/SAMv1.pdf&#xD;&#xA;" />
  <row Id="465" PostHistoryTypeId="13" PostId="155" RevisionGUID="94adbc82-6543-41e8-ab99-9f319f5163a9" CreationDate="2017-05-19T18:08:33.633" UserId="298" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;}]}" />
  <row Id="466" PostHistoryTypeId="2" PostId="156" RevisionGUID="e60734fc-bd23-4e1a-baea-8049cb411a9b" CreationDate="2017-05-19T18:34:20.767" UserId="57" Text="Why some assemblers like [SOAPdenovo2][1] or [Velvet][2] require an odd kmer for construction of De Brujin graph, while some other assemblers like [ABySS][3] are fine with even kmers?&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/aquaskyline/SOAPdenovo2&#xD;&#xA;  [2]: https://github.com/dzerbino/velvet&#xD;&#xA;  [3]: https://github.com/bcgsc/abyss" />
  <row Id="467" PostHistoryTypeId="1" PostId="156" RevisionGUID="e60734fc-bd23-4e1a-baea-8049cb411a9b" CreationDate="2017-05-19T18:34:20.767" UserId="57" Text="Why some assemblers require odd kmer for construction of De Brujin graphs?" />
  <row Id="468" PostHistoryTypeId="3" PostId="156" RevisionGUID="e60734fc-bd23-4e1a-baea-8049cb411a9b" CreationDate="2017-05-19T18:34:20.767" UserId="57" Text="&lt;ngs&gt;&lt;k-mer&gt;&lt;assembly&gt;&lt;de-bruijn&gt;" />
  <row Id="469" PostHistoryTypeId="2" PostId="157" RevisionGUID="526a6df4-4bab-487c-9197-57f817370e37" CreationDate="2017-05-19T18:50:05.307" UserId="272" Text="If there are soft clipped base pairs specified in the CIGAR string for a read in a SAM/BAM file, will these be used for variant calling in a [samtools][1] + [bcftools][2] workflow? &#xD;&#xA;&#xD;&#xA;The [GATK HaplotypeCaller][3], for example, has an explicit option `--dontUseSoftClippedBases` for whether to use soft clipped bases. The samtools documentation does not mention clipped bases. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.htslib.org/doc/samtools.html&#xD;&#xA;  [2]: http://www.htslib.org/doc/bcftools.html&#xD;&#xA;  [3]: https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php" />
  <row Id="470" PostHistoryTypeId="1" PostId="157" RevisionGUID="526a6df4-4bab-487c-9197-57f817370e37" CreationDate="2017-05-19T18:50:05.307" UserId="272" Text="Are soft-clipped bases used for variant calling in samtools + bcftools?" />
  <row Id="471" PostHistoryTypeId="3" PostId="157" RevisionGUID="526a6df4-4bab-487c-9197-57f817370e37" CreationDate="2017-05-19T18:50:05.307" UserId="272" Text="&lt;bam&gt;&lt;sam&gt;&lt;samtools&gt;" />
  <row Id="472" PostHistoryTypeId="2" PostId="158" RevisionGUID="eadd786d-7a4d-41f6-a090-7fcbd180d599" CreationDate="2017-05-19T18:52:34.737" UserId="57" Text="From the [manual of Velvet][1]:&#xD;&#xA;&#xD;&#xA;&gt; it must be an odd number, to avoid palindromes. If you put in an even&#xD;&#xA;&gt; number, Velvet will just decrement it and proceed.&#xD;&#xA;&#xD;&#xA;An explanation of why palindromes are problem I found in this [review][3] they wrote&#xD;&#xA;&#xD;&#xA;&gt; Palindromes induce paths that fold back on themselves. At least one&#xD;&#xA;&gt; assembler avoids these elegantly; Velvet requires K, the length of a&#xD;&#xA;&gt; K-mer, to be odd. An odd-size K-mer cannot match its reverse&#xD;&#xA;&gt; complement.&#xD;&#xA;&#xD;&#xA;It is possible to construct graph with palindromes, but then the interpretation will be harder. Allowing only graphs of odd kmers is just an elegant way how to avoid writing a code for interpretation of a more complicated graph.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ebi.ac.uk/~zerbino/velvet/Manual.pdf&#xD;&#xA;  [2]: http://homolog.us/blogs/blog/2011/09/27/k-mer-sizes-for-genome-assembly/&#xD;&#xA;  [3]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2874646/" />
  <row Id="473" PostHistoryTypeId="2" PostId="159" RevisionGUID="0fa5a3f1-4571-459a-bcee-288c7aebd5aa" CreationDate="2017-05-19T18:59:48.617" UserId="65" Text="Does anyone have experience generating pdb structures with Phyre and ITasser online tools. The results generated from each given the same amino acid sequence input are very different and I am wondering whether or not this is a usual experience. I know ITasser was the number 1 rated in the CASP trails, but should the results really be this disparate?" />
  <row Id="474" PostHistoryTypeId="1" PostId="159" RevisionGUID="0fa5a3f1-4571-459a-bcee-288c7aebd5aa" CreationDate="2017-05-19T18:59:48.617" UserId="65" Text="Phyre2 vs ITasser, completely different models generated" />
  <row Id="475" PostHistoryTypeId="3" PostId="159" RevisionGUID="0fa5a3f1-4571-459a-bcee-288c7aebd5aa" CreationDate="2017-05-19T18:59:48.617" UserId="65" Text="&lt;structural&gt;&lt;homology-modelling&gt;" />
  <row Id="476" PostHistoryTypeId="2" PostId="160" RevisionGUID="fafc36f0-acb3-48bf-9fac-ad7ec89d7fd8" CreationDate="2017-05-19T19:19:13.093" UserId="77" Text="No, samtools (and therefore bcftools) does not use soft-clipped bases. You can quickly confirm this by using either `samtools depth` or `samtools mpileup` to look at a region with a soft-clipped alignment. You'll note that the soft-clipped region isn't used in the depth/pileup (both tools use the same underlying code, so it doesn't matter which you use). If you're curious, samtools ignores soft-clipped bases because it's based on making a per-base stack of alignments covering each position. In the BAM format, alignments are sorted and assigned to bins according to their start/end positions, which won't include soft-clipping. Consequently, when samtools is making the pileup it won't even see the alignments that would overlap a given base if soft-clipped bases were included.&#xD;&#xA;&#xD;&#xA;This then sort of begs the question of what GATK's HaplotypeCaller is doing differently. There, regions in the genome are essentially assembled in a small de Bruijn graph, which allows for soft-clipped bases around indels to then be resolved, given that the graph would start/end a little-way on past each side of indels. This is also why you don't need to do indel realignment with the HaplotypeCaller (this was needed in the old UnifiedGenotyper)." />
  <row Id="477" PostHistoryTypeId="5" PostId="158" RevisionGUID="6b7ee38d-6405-44e7-9517-b06a7cf9454f" CreationDate="2017-05-19T19:35:34.697" UserId="57" Comment="added link for wiki article about sequence palindromes" Text="From the [manual of Velvet][1]:&#xD;&#xA;&#xD;&#xA;&gt; it must be an odd number, to avoid palindromes. If you put in an even&#xD;&#xA;&gt; number, Velvet will just decrement it and proceed.&#xD;&#xA;&#xD;&#xA;the [palindromes][2] in biology are defined as reverse complementary sequences. The problem of palindromes is explained in this [review][3]:&#xD;&#xA;&#xD;&#xA;&gt; Palindromes induce paths that fold back on themselves. At least one&#xD;&#xA;&gt; assembler avoids these elegantly; Velvet requires K, the length of a&#xD;&#xA;&gt; K-mer, to be odd. An odd-size K-mer cannot match its reverse&#xD;&#xA;&gt; complement.&#xD;&#xA;&#xD;&#xA;It is possible to construct graph with palindromes, but then the interpretation will be harder. Allowing only graphs of odd kmers is just an elegant way how to avoid writing a code for interpretation of a more complicated graph.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ebi.ac.uk/~zerbino/velvet/Manual.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Palindromic_sequence&#xD;&#xA;  [3]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2874646/" />
  <row Id="478" PostHistoryTypeId="5" PostId="160" RevisionGUID="5305ed59-d77b-4668-9fa9-77764953672f" CreationDate="2017-05-19T20:15:11.303" UserId="77" Comment="added 227 characters in body" Text="No, samtools (and therefore bcftools) does not use soft-clipped bases. You can quickly confirm this by using either `samtools depth` or `samtools mpileup` to look at a region with a soft-clipped alignment. You'll note that the soft-clipped region isn't used in the depth/pileup (both tools use the same underlying code, so it doesn't matter which you use). If you're curious, samtools ignores soft-clipped bases because it's based on making a per-base stack of alignments covering each position. In the BAM format, alignments are sorted and assigned to bins according to their start/end positions, which won't include soft-clipping. Consequently, when samtools is making the pileup it won't even see the alignments that would overlap a given base if soft-clipped bases were included.&#xD;&#xA;&#xD;&#xA;This then sort of begs the question of what GATK's HaplotypeCaller is doing differently. There, regions in the genome are essentially assembled in a small de Bruijn graph, which allows for soft-clipped bases around indels to then be resolved, given that the graph would start/end a little-way on past each side of indels. This is also why you don't need to do indel realignment with the HaplotypeCaller (this was needed in the old UnifiedGenotyper).&#xD;&#xA;&#xD;&#xA;**Edit**: For more details regarding the HaplotypeCaller, see [this nice page](https://software.broadinstitute.org/gatk/documentation/article.php?id=4146) on GATK's website, which goes into much more detail than I did here." />
  <row Id="479" PostHistoryTypeId="2" PostId="161" RevisionGUID="6a1bac50-6c84-4744-8205-ba385797f33a" CreationDate="2017-05-20T00:21:08.950" UserId="235" Text="The peak calling tool MACS2 can call peaks in either narrow peak mode (for focused signals like transcription factor ChIPseq) or broad peak mode (for more defuse signals, like certain histone modifications). &#xD;&#xA;&#xD;&#xA;The algorithm for narrow peak calling is well described in the MACS publication. But I don't find much documentation for how peak calling is different in broad peak mode. The manual only contains the following:&#xD;&#xA;&#xD;&#xA;&gt; --broad&#xD;&#xA;&gt; &#xD;&#xA;&gt; When this flag is on, MACS will try to composite broad regions in&#xD;&#xA;&gt; BED12 ( a gene-model-like format ) by putting nearby highly enriched&#xD;&#xA;&gt; regions into a broad region with loose cutoff. The broad region is&#xD;&#xA;&gt; controlled by another cutoff through --broad-cutoff. The maximum&#xD;&#xA;&gt; length of broad region length is 4 times of d from MACS&#xD;&#xA;&#xD;&#xA;But this doesn't really describe exactly how this is performed. &#xD;&#xA;&#xD;&#xA;So what is the algorithm MACS uses for calling broad peaks?" />
  <row Id="480" PostHistoryTypeId="1" PostId="161" RevisionGUID="6a1bac50-6c84-4744-8205-ba385797f33a" CreationDate="2017-05-20T00:21:08.950" UserId="235" Text="How are MACS2's narrow peak and broad peak algorithms different?" />
  <row Id="481" PostHistoryTypeId="3" PostId="161" RevisionGUID="6a1bac50-6c84-4744-8205-ba385797f33a" CreationDate="2017-05-20T00:21:08.950" UserId="235" Text="&lt;algorithms&gt;&lt;chip-seq&gt;&lt;macs2&gt;" />
  <row Id="488" PostHistoryTypeId="2" PostId="163" RevisionGUID="8001b06f-09e2-41bb-8268-76411fb287e1" CreationDate="2017-05-20T04:11:29.220" UserId="138" Text="I'm less familiar with Phyre, but I-TASSER is a really sophisticated system that takes the results of a search using multiple threaders and plugs them into an ab initio simulation which tries to minimize the energy of the models by sampling many possible 3D conformations, which I don't think Phyre does.&#xD;&#xA;&#xD;&#xA;https://en.wikipedia.org/wiki/I-TASSER#/media/File:I-TASSER-pipeline.jpg&#xD;&#xA;&#xD;&#xA;Compare with a similar workflow schematic for Phyre:&#xD;&#xA;&#xD;&#xA;http://www.nature.com/nprot/journal/v10/n6/images/nprot.2015.053-F1.jpg&#xD;&#xA;&#xD;&#xA;Structure prediction still has a long way to go, and you'll always get better results if there are close homologues available in the PDB, but given the consistent high performance of I-TASSER in CASP I would treat those results as more significant. That said, it can't hurt to consider multiple answers." />
  <row Id="490" PostHistoryTypeId="2" PostId="164" RevisionGUID="e78cf987-4e59-445d-8b89-9b695e9316bf" CreationDate="2017-05-20T08:33:58.813" UserId="349" Text="In case if you are really dedicated to obtain perfect results, you can use strategy described [there][1], in 1000GP 3rd Phase SV detection paper - use these tools, validate your calls with IRS test, merge calls into one callset. &#xD;&#xA;&#xD;&#xA;If you do not wanna spend thousands human-hours as was spent during this paper preparation, from my experience, it is better to use 1 paired-end insert distance method and one read-depth based method. Each of them cover &quot;different&quot; regions in the genome. (even thou they have huge overlap, paired-end detection requires both SV breakpoints to be located within the regions with good mappability which is not always the case, but resolution of read-depth methods is lower in general, paired-ends works well for deletions/tandem duplications/inversions, but have troubles with non-tandem duplications).&#xD;&#xA;&#xD;&#xA;Hope it helps.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.nature.com/nature/journal/v526/n7571/full/nature15394.html" />
  <row Id="491" PostHistoryTypeId="2" PostId="165" RevisionGUID="7406ef9c-11d6-48b9-9b53-8db70fe125ea" CreationDate="2017-05-20T11:31:59.123" UserId="351" Text="Whether a module is complete can easily be checked by evaluating the `Definition` entry associated with the module; e.g. in module [M00010](http://www.genome.jp/kegg-bin/show_module?M00010), it is given by&#xD;&#xA;&#xD;&#xA;    Definition 	K01647 (K01681,K01682) (K00031,K00030)&#xD;&#xA;&#xD;&#xA;which can be translated to&#xD;&#xA;&#xD;&#xA;    K01647 AND (K01681 OR K01682) AND (K00031 OR K00030)&#xD;&#xA;&#xD;&#xA;If this expression evaluates to `TRUE`, the module is complete.&#xD;&#xA;&#xD;&#xA;I am wondering whether analogue information exists for a single reaction. So e.g. for [R00352](http://www.genome.jp/dbget-bin/www_bget?rn:R00352), one finds the following information about Orthology:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;But how do I now know in which logical relation the KOs are?&#xD;&#xA;&#xD;&#xA;So it could be  	&#xD;&#xA;&#xD;&#xA;    K01648 AND K15230 AND K15231 &#xD;&#xA;&#xD;&#xA;or&#xD;&#xA;&#xD;&#xA;    K01648 OR K15230 OR K15231&#xD;&#xA;&#xD;&#xA;or&#xD;&#xA;&#xD;&#xA;    K01648 OR (K15230 AND K15231)&#xD;&#xA;&#xD;&#xA;and so on.&#xD;&#xA;&#xD;&#xA;In the above example, the correct expression would be:&#xD;&#xA;&#xD;&#xA;    K01648 OR (K15230 AND K15231)&#xD;&#xA;&#xD;&#xA;One either needs `K01648` or the other two subunits together.&#xD;&#xA;&#xD;&#xA;Can this information be retrieved from KEGG for each reaction and if so, how?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/9jRBq.png" />
  <row Id="492" PostHistoryTypeId="1" PostId="165" RevisionGUID="7406ef9c-11d6-48b9-9b53-8db70fe125ea" CreationDate="2017-05-20T11:31:59.123" UserId="351" Text="How to retrieve logical expression (KO based) for reactions from KEGG?" />
  <row Id="493" PostHistoryTypeId="3" PostId="165" RevisionGUID="7406ef9c-11d6-48b9-9b53-8db70fe125ea" CreationDate="2017-05-20T11:31:59.123" UserId="351" Text="&lt;kegg&gt;&lt;orthology&gt;&lt;database&gt;&lt;data-retrieval&gt;" />
  <row Id="496" PostHistoryTypeId="5" PostId="120" RevisionGUID="09321ebb-a63b-4e26-adf0-7247dfc8e6cc" CreationDate="2017-05-20T15:06:34.820" UserId="57" Comment="added tools for calling structural variants" Text="I have a reference genome and now I would like to call structural variants from Illumina pair-end whole genome resequencing data (insert size 700bp). &#xD;&#xA;&#xD;&#xA;There are many tools for SV calls (I made an incomplete list of tools bellow). There is also a tool for merging SV calls from multiple methods / samples - [SURVIVOR][1]. Is there a combination of methods for SV detection with optimal balance between sensitivity and specificity?&#xD;&#xA;&#xD;&#xA;There is a [benchmarking paper][2], evaluating sensitivity and specificity of SV calls of individual methods using simulated pair-end reads. However, there is no elaboration on the combination of methods.&#xD;&#xA;&#xD;&#xA;List of tools for calling structural variants:&#xD;&#xA;&#xD;&#xA;- [Lumpy][3]&#xD;&#xA;- [BreakDancer][4]&#xD;&#xA;- [Manta][5]&#xD;&#xA;- [Delly][6]&#xD;&#xA;- [GRIDSS][7]&#xD;&#xA;- [Meerkat][8]&#xD;&#xA;- [Pindel][9]&#xD;&#xA;- [Softsv][10]&#xD;&#xA;- [Prism][11]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/fritzsedlazeck/SURVIVOR&#xD;&#xA;  [2]: http://www.ijpmbs.com/uploadfile/2016/1017/20161017025004545.pdf&#xD;&#xA;  [3]: https://github.com/arq5x/lumpy-sv&#xD;&#xA;  [4]: http://breakdancer.sourceforge.net/&#xD;&#xA;  [5]: https://github.com/Illumina/manta&#xD;&#xA;  [6]: https://github.com/dellytools/delly&#xD;&#xA;  [7]: https://github.com/PapenfussLab/gridss&#xD;&#xA;  [8]: http://compbio.med.harvard.edu/Meerkat/&#xD;&#xA;  [9]: https://github.com/genome/pindel&#xD;&#xA;  [10]: https://sourceforge.net/projects/softsv/&#xD;&#xA;  [11]: http://compbio.cs.toronto.edu/prism/" />
  <row Id="497" PostHistoryTypeId="2" PostId="166" RevisionGUID="098223e3-3645-474e-bae5-04307879312b" CreationDate="2017-05-20T20:24:45.047" UserId="35" Text="According to the [FGSEA preprint][1]:&#xD;&#xA;&#xD;&#xA;&gt; We ran reference GSEA with default parameters. The permutation number&#xD;&#xA;&gt; was set to 1000, which means that for each input gene set 1000&#xD;&#xA;&gt; independent samples were generated. The run took 100 seconds and&#xD;&#xA;&gt; resulted in 79 gene sets with GSEA-adjusted FDR q-value of less than&#xD;&#xA;&gt; 10−2. All significant gene sets were in a positive mode. First, to get&#xD;&#xA;&gt; a similar nominal p-values accuracy we ran FGSEA algorithm on 1000&#xD;&#xA;&gt; permutations. This took 2 seconds, but resulted in no significant hits&#xD;&#xA;&gt; due after multiple testing correction (with FRD ≤ 1%).&#xD;&#xA;&#xD;&#xA;Thus, FGSEA and GSEA are not identical.&#xD;&#xA;&#xD;&#xA;And again in the conclusion:&#xD;&#xA;&#xD;&#xA;&gt; Consequently, gene sets can be ranked more precisely in the results&#xD;&#xA;&gt; and, which is even more important, standard multiple testing&#xD;&#xA;&gt; correction methods can be applied instead of approximate ones as in&#xD;&#xA;&gt; [GSEA].&#xD;&#xA;&#xD;&#xA;The author argues that FGSEA is more accurate, so it can't be equivalent.&#xD;&#xA;&#xD;&#xA;  [1]: http://biorxiv.org/content/early/2016/06/20/060012" />
  <row Id="498" PostHistoryTypeId="2" PostId="167" RevisionGUID="bc6a8dfe-af8f-4a19-aa0a-ba2e8f6ba28f" CreationDate="2017-05-20T21:38:54.743" UserId="13" Text="The key function is here:&#xD;&#xA;https://github.com/taoliu/MACS/blob/master/MACS2/IO/CallPeakUnit.pyx#L1443&#xD;&#xA;&#xD;&#xA;The description attached to the function says:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    &quot;&quot;&quot;This function try to find enriched regions within which,&#xD;&#xA;            scores are continuously higher than a given cutoff for level&#xD;&#xA;            1, and link them using the gap above level 2 cutoff with a&#xD;&#xA;            maximum length of lvl2_max_gap.&#xD;&#xA;&#xD;&#xA;            scoring_function_s: symbols of functions to calculate score. 'p' for pscore, 'q' for qscore, 'f' for fold change, 's' for subtraction. for example: ['p', 'q']&#xD;&#xA;&#xD;&#xA;            lvl1_cutoff_s:  list of cutoffs at highly enriched regions, corresponding to scoring functions.&#xD;&#xA;            lvl2_cutoff_s:  list of cutoffs at less enriched regions, corresponding to scoring functions.&#xD;&#xA;            min_length :    minimum peak length, default 200.&#xD;&#xA;            lvl1_max_gap   :  maximum gap to merge nearby enriched peaks, default 50.&#xD;&#xA;            lvl2_max_gap   :  maximum length of linkage regions, default 400.        &#xD;&#xA;            Return both general PeakIO object for highly enriched regions&#xD;&#xA;            and gapped broad regions in BroadPeakIO. &#xD;&#xA;    &quot;&quot;&quot;&#xD;&#xA;&#xD;&#xA;To give some basic explanation, the algorithm (briefly) appears to be as follows:&#xD;&#xA;&#xD;&#xA;1) Two separate levels of peaks are called, level 1 (a higher pval) and level 2 (a lower pval). Level 1 is controlled by `-p` and level 2 is controlled by `--broad-cutoff`. When each peakset is called, they are immediately linked by the max gap parameter for each set.&#xD;&#xA;&#xD;&#xA;2) Then, assuming that all level 1 peaks should be inside level 2 peaks (this is an explicit assumption by MACS2), the algorithm groups level 1 peaks inside level 2 peaks to output a broad peak.&#xD;&#xA;&#xD;&#xA;...&#xD;&#xA;&#xD;&#xA;This has a few implications:&#xD;&#xA;&#xD;&#xA;1) The broad peak calls really come from the level 2 peaks alone (+ linking). The level 1 peak calls allow you to distinguish sub peaks (so that you can have gapped peaks).&#xD;&#xA;&#xD;&#xA;2) Aside from the linking, the broad peak calls would be the same as narrow peak calls, if you called both with the same pval threshold (for example, if you set `--broad-cutoff 0.1` in broad peak mode, and the `-p 0.1` for narrow peak mode)&#xD;&#xA;" />
  <row Id="499" PostHistoryTypeId="2" PostId="168" RevisionGUID="f547027f-9c17-43ab-9651-2852d04bfa7c" CreationDate="2017-05-20T21:49:51.210" UserId="146" Text="I'm looking for tools to check the quality of a VCF I have of a human genome. I would like to check the VCF against publicly known variants across other human genomes, e.g. how many SNPs are already in public databases, whether insertions/deletions are at known positions, insertion/deletion length distribution, other SNVs/SVs, etc.? I suspect that there are resources from previous projects to check for known SNPs and InDels by human subpopulations.&#xD;&#xA;&#xD;&#xA;What resources exist for this, and how do I do it? " />
  <row Id="500" PostHistoryTypeId="1" PostId="168" RevisionGUID="f547027f-9c17-43ab-9651-2852d04bfa7c" CreationDate="2017-05-20T21:49:51.210" UserId="146" Text="Given a VCF of a human genome, how do I assess the quality against known SNVs?" />
  <row Id="501" PostHistoryTypeId="3" PostId="168" RevisionGUID="f547027f-9c17-43ab-9651-2852d04bfa7c" CreationDate="2017-05-20T21:49:51.210" UserId="146" Text="&lt;vcf&gt;&lt;snv&gt;&lt;public-databases&gt;&lt;variants&gt;&lt;structural-variation&gt;" />
  <row Id="502" PostHistoryTypeId="6" PostId="143" RevisionGUID="ff12d219-0562-4d60-bd9a-201bd655c080" CreationDate="2017-05-20T21:54:37.907" UserId="298" Comment="Added relevant tag" Text="&lt;ngs&gt;&lt;annotation&gt;&lt;machine-learning&gt;&lt;cancer&gt;" />
  <row Id="503" PostHistoryTypeId="24" PostId="143" RevisionGUID="ff12d219-0562-4d60-bd9a-201bd655c080" CreationDate="2017-05-20T21:54:37.907" Comment="Proposed by 298 approved by 77, 328 edit id of 40" />
  <row Id="504" PostHistoryTypeId="2" PostId="169" RevisionGUID="352c69e0-371b-4e5d-b508-55a355117125" CreationDate="2017-05-21T01:12:08.377" UserId="57" Text="The greatest **protein** coding variant catalogue is definitely [ExAC][1] (&gt;65k individuals). They also published a [blogpost][2] where they describe how to reproduce figures in the paper (it is a good start how to get familiar with the dataset).&#xD;&#xA;&#xD;&#xA;For the **whole-genome** variants I would look at the data created by [1000 genomes][3] project (the latest release should have more than 2.5k individuals). They provide a [guide][4] how to create the catalogue of SNV and catalogue of SVs can be found [here][5].&#xD;&#xA;&#xD;&#xA;In [this][6] paper (also 1000 genomes project) they speak about non-precise placement of SVs by SV callers. I would keep this in mind for the comparison of your genome to the known variants.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.nature.com/nature/journal/v536/n7616/full/nature19057.html&#xD;&#xA;  [2]: https://macarthurlab.org/2016/03/17/reproduce-all-the-figures-a-users-guide-to-exac-part-2/&#xD;&#xA;  [3]: http://www.internationalgenome.org/&#xD;&#xA;  [4]: http://www.internationalgenome.org/faq/are-there-any-fasta-files-containing-1000-genomes-variants-or-haplotypes/&#xD;&#xA;  [5]: http://www.internationalgenome.org/phase-3-structural-variant-dataset&#xD;&#xA;  [6]: http://www.nature.com/nature/journal/v526/n7571/full/nature15394.html" />
  <row Id="506" PostHistoryTypeId="2" PostId="170" RevisionGUID="eb2ffc76-d276-48e5-a416-14b28adbca8c" CreationDate="2017-05-21T05:18:15.573" UserId="280" Text="Your best bet is to use programs that provide you an complete annotation of variants present in your VCF. Two examples are [snpEff][1] and [Annovar][2]. These programs work on known variants deem different sources and provide you with information on each item in your file, that you can filter after to try to understand the effects of each variant.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://snpeff.sourceforge.net&#xD;&#xA;  [2]: http://annovar.openbioinformatics.org/en/latest/" />
  <row Id="507" PostHistoryTypeId="2" PostId="171" RevisionGUID="c79782c7-1d11-44ac-98b3-89761439b8b9" CreationDate="2017-05-21T07:27:58.337" UserId="96" Text="First of all, kudos to you for taking versioning seriously. The fact that you're mindful of this issue is a good sign that you want to do responsible research!&#xD;&#xA;&#xD;&#xA;For many bioinformatics projects, data files are so large that versioning the data directly with a tool like git is impractical. But your question is really getting at a couple of different issues.&#xD;&#xA;&#xD;&#xA;- *How do I do my research reproducibly, and show full provenance for each data point and result I produce?*&#xD;&#xA;- *How do I manage and synchronize my research work across multiple machines?*&#xD;&#xA;&#xD;&#xA;**The short answer**:&#xD;&#xA;&#xD;&#xA;- Archive the primary data.&#xD;&#xA;- Place your workflow under version control.&#xD;&#xA;- Version checksums of large data files.&#xD;&#xA;- Use GitHub to synchronize your workflow between machines.&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;**The long answer**:&#xD;&#xA;&#xD;&#xA;## Archive the primary data&#xD;&#xA;&#xD;&#xA;As far as reproducibility is concerned, what matters most is the primary data: the raw, unprocessed data you collect from the instrument. If you are analyzing data that has been published by others, then write a script that will automate the task of downloading the data from its primary official source, and place that script under version control.&#xD;&#xA;&#xD;&#xA;If you or a lab mate or a colleague produced the data and it is not yet published, then you should already have plans for submitting it to an archive. Indeed, most journals and funding agencies now require this prior to publication. I'd even go so far as to say the data should be submitted as soon as it's collected. Scientists worry a lot about having their data stolen and their ideas scooped, but statistically speaking, getting scooped is much less likely than nobody ever touching your data or reading your paper. But if you or an advisor insists, most data archives allow you to keep data private for an extended period of time until a supporting manuscript is published.&#xD;&#xA;&#xD;&#xA;Putting (for example) Fastq files in a git repository is a bad idea for a lot of reasons. No hosting service will support files that big, git will be very slow with files that big, but most importantly git/GitHub is not archival! Use a proper data archive!&#xD;&#xA;&#xD;&#xA;## Place your workflow under version control&#xD;&#xA;&#xD;&#xA;Treat your raw data as read-only. Only process the raw data using scripts, and keep these scripts under version control. Vince Buffalo describes this well in his book [Bioinformatics Data Skills](http://shop.oreilly.com/product/0636920030157.do). Check it out!&#xD;&#xA;&#xD;&#xA;## Version checksums of large data files&#xD;&#xA;&#xD;&#xA;If there are any data files that you want to track but are too big to place under version control, compute checksums and place *these* under version control. Checksums are very small alphanumeric strings that are, for all practical purposes, unique for each data file. So instead of putting that 5GB trimmed Fastq file or the 7GB BAM file under version control, compute their checksums and put the *checksums* under version control. The checksums won't tell you the contents of your files, but they can tell you when the file contents change.&#xD;&#xA;&#xD;&#xA;This should give full disclosure and complete provenance for every data point in your analysis. The workflow has a scripts/command for downloading the primary data, scripts/commands for processing the data, and checksums that serve as a signature to validate intermediate and final output files. With this, anyone should be able to reproduce you analysis!&#xD;&#xA;&#xD;&#xA;## Use GitHub to synchronize your workflow between machines&#xD;&#xA;&#xD;&#xA;If your workflow is already under version control with git, it's trivial to push this to a hosting service like GitHub, GitLab, or BitBucket. Then it's just a matter of using `git push` and `git pull` to keep your code up-to-date on your various machines." />
  <row Id="508" PostHistoryTypeId="2" PostId="172" RevisionGUID="6d7169bd-c132-4043-8113-81764ab8bb69" CreationDate="2017-05-21T07:43:43.940" UserId="96" Text="Depending on the coverage of your data and the complexity of the genome, you could either reassemble the genome *de novo* or run a reference guided (or reference assisted) assembly. It sounds like you're leaning more towards the latter.&#xD;&#xA;&#xD;&#xA;I only have experience with a single reference-guided assembly tool: [AlignGraph](https://doi.org/10.1093/bioinformatics/btu291). This may or may not be appropriate depending on the organism of interest and your data types. For example, it's very unlikely to work well with Oxford Nanopore reads that have not first been error corrected." />
  <row Id="509" PostHistoryTypeId="2" PostId="173" RevisionGUID="f0e651c8-bc82-46ae-9299-c63432572aa0" CreationDate="2017-05-21T10:41:33.910" UserId="48" Text="I found that the author responded that in the [discussion][1] in the preprint, &#xD;&#xA;&gt;I wonder is the enrichment score calculated the same way as in broad gsea?&#xD;&#xA;&#xD;&#xA;&gt; Values of enrichment scores and normalized enrichment scores are the same for both broad version and fgsea.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://disq.us/p/1f7wolq" />
  <row Id="510" PostHistoryTypeId="5" PostId="169" RevisionGUID="a9eaf539-f86d-4dfc-a085-119cc8f177db" CreationDate="2017-05-21T11:13:48.910" UserId="57" Comment="downloading the integrated variant calls though portal instead of bash piped to perl script" Text="The greatest **protein** coding variant catalogue is definitely [ExAC][1] (&gt;65k individuals). They also published a [blogpost][2] where they describe how to reproduce figures in the paper (it is a good start how to get familiar with the dataset).&#xD;&#xA;&#xD;&#xA;For the **whole-genome** variants I would look at the data created by [1000 genomes][3] project (the latest release has more than 3k individuals). The integrated variant call sets can be downloaded though the [portal][4] and catalogue of SVs can be found [here][5].&#xD;&#xA;&#xD;&#xA;In [this][6] paper (also 1000 genomes project) they speak about non-precise placement of SVs by SV callers. I would keep this in mind for the comparison of your genome to the known variants.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.nature.com/nature/journal/v536/n7616/full/nature19057.html&#xD;&#xA;  [2]: https://macarthurlab.org/2016/03/17/reproduce-all-the-figures-a-users-guide-to-exac-part-2/&#xD;&#xA;  [3]: http://www.internationalgenome.org/&#xD;&#xA;  [4]: http://www.internationalgenome.org/data-portal/data-collection/phase-3&#xD;&#xA;  [5]: http://www.internationalgenome.org/phase-3-structural-variant-dataset&#xD;&#xA;  [6]: http://www.nature.com/nature/journal/v526/n7571/full/nature15394.html" />
  <row Id="511" PostHistoryTypeId="2" PostId="174" RevisionGUID="a6f61b52-adf6-4a13-b7eb-4e9be65ca0a6" CreationDate="2017-05-21T11:49:58.693" UserId="57" Text="All the long-read sequencing platforms are based on single-molecule sequencing which cases higher per-base error rates. For this reason a polishing step was added to genome assembly pipelines - mapping raw reads back to assembly and correcting details of the assembly.&#xD;&#xA;&#xD;&#xA;I have decent PacBio RSII dataset of single individual genome of heavily heterozygous non-model species. Assembly went well, but when I tried to polish the assembly using [quiver](https://github.com/PacificBiosciences/GenomicConsensus) it could not converge over couple of iterations and I bet it is because of two huge divergence of haplotypes.&#xD;&#xA;&#xD;&#xA;Is there any other way polish the genome with such properties?&#xD;&#xA;For instance if there would be a way how to separate long reads to haplotypes, so I could polish using one haplotype only.&#xD;&#xA;" />
  <row Id="512" PostHistoryTypeId="1" PostId="174" RevisionGUID="a6f61b52-adf6-4a13-b7eb-4e9be65ca0a6" CreationDate="2017-05-21T11:49:58.693" UserId="57" Text="How to deal with heterozygocity during polishing of genome assembly based on long reads?" />
  <row Id="513" PostHistoryTypeId="3" PostId="174" RevisionGUID="a6f61b52-adf6-4a13-b7eb-4e9be65ca0a6" CreationDate="2017-05-21T11:49:58.693" UserId="57" Text="&lt;ngs&gt;&lt;assembly&gt;&lt;long-reads&gt;" />
  <row Id="514" PostHistoryTypeId="2" PostId="175" RevisionGUID="75135e4d-1afc-4706-a374-e63bdec83785" CreationDate="2017-05-21T14:48:39.303" UserId="84" Text="To achieve (at least some of) your goals, I would recommend the [Variant Effect Predictor (VEP)](http://grch37.ensembl.org/Homo_sapiens/Tools/VEP). It is a flexible tool that provides several types of annotations on an input .vcf file.  I agree that ExAC is the *de facto* gold standard catalog for human genetic variation in coding regions.  To see the frequency distribution of variants by global subpopulation make sure &quot;ExAC allele frequencies&quot; is checked in addition to the 1000 genomes. [![VEP ExAC][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Output in the web-browser:&#xD;&#xA;[![VEP_ExAC_res][2]][2]&#xD;&#xA;&#xD;&#xA;If you download the annotated .vcf, frequencies will be in the `INFO` field:&#xD;&#xA;&#xD;&#xA;    ##INFO=&lt;ID=CSQ,Number=.,Type=String,Description=&quot;Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID|TSL|SIFT|PolyPhen|AF|AFR_AF|AMR_AF|EAS_AF|EUR_AF|SAS_AF|AA_AF|EA_AF|ExAC_AF|ExAC_Adj_AF|ExAC_AFR_AF|ExAC_AMR_AF|ExAC_EAS_AF|ExAC_FIN_AF|ExAC_NFE_AF|ExAC_OTH_AF|ExAC_SAS_AF|CLIN_SIG|SOMATIC|PHENO|MOTIF_NAME|MOTIF_POS|HIGH_INF_POS|MOTIF_SCORE_CHANGE&#xD;&#xA;&#xD;&#xA;The previously mentioned Annovar can also [annotate with ExAC allele frequencies](http://doc-openbio.readthedocs.io/projects/annovar/en/latest/user-guide/filter/#exac-annotations).  Finally, should mention the newest whole-genome resource, [gnomAD](http://gnomad.broadinstitute.org/).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/zDUTT.jpg&#xD;&#xA;  [2]: https://i.stack.imgur.com/0OCCY.jpg&#xD;&#xA;" />
  <row Id="515" PostHistoryTypeId="5" PostId="174" RevisionGUID="868e6c7e-80b1-40cb-9c21-fe9ddc9ebdfd" CreationDate="2017-05-21T15:42:58.207" UserId="298" Comment="Minor corrections" Text="All the long-read sequencing platforms are based on single-molecule sequencing which cases higher per-base error rates. For this reason a polishing step was added to genome assembly pipelines - mapping raw reads back to assembly and correcting details of the assembly.&#xD;&#xA;&#xD;&#xA;I have decent PacBio RSII dataset of single individual genome of heavily heterozygous non-model species. Assembly went well, but when I tried to polish the assembly using [quiver](https://github.com/PacificBiosciences/GenomicConsensus) it could not converge over a couple of iterations and I bet it is because of too great divergence of haplotypes.&#xD;&#xA;&#xD;&#xA;Is there any other way to polish a genome with such properties?&#xD;&#xA;For instance, is there a way to separate long reads by haplotype, so I could polish using one haplotype only?&#xD;&#xA;" />
  <row Id="516" PostHistoryTypeId="24" PostId="174" RevisionGUID="868e6c7e-80b1-40cb-9c21-fe9ddc9ebdfd" CreationDate="2017-05-21T15:42:58.207" Comment="Proposed by 298 approved by 57 edit id of 44" />
  <row Id="517" PostHistoryTypeId="2" PostId="176" RevisionGUID="c4a21845-3d81-4d6e-8bfb-9a69f4e0a3e3" CreationDate="2017-05-21T17:47:47.147" UserId="339" Text="I wrote a program, [ASCIIGenome][1], that I find handy in cases where you want to have a quick look at genomic data. It's a genome browser for the command line. &#xD;&#xA;&#xD;&#xA;To view only reads containing mismatches you can use the internal function `awk`. To filter for reads where the NM tag (number of mismatches) is &gt;0:&#xD;&#xA;&#xD;&#xA;    ASCIIGenome -fa genome.fa aln.bam&#xD;&#xA;    ...&#xD;&#xA;    &#xD;&#xA;    [h] for help: awk 'getSamTag(&quot;NM&quot;) &gt; 0'&#xD;&#xA;&#xD;&#xA;The view on terminal screen may look something like this:[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;Similarly, to get only reads containing indels you can use `awk '$6 ~ &quot;D|I&quot;'`&#xD;&#xA;&#xD;&#xA;Hope this helps and feel free to report bugs &amp; issues.&#xD;&#xA;&#xD;&#xA;  [1]: http://asciigenome.readthedocs.io/en/latest/description.html&#xD;&#xA;  [2]: https://i.stack.imgur.com/qXKkp.png" />
  <row Id="518" PostHistoryTypeId="5" PostId="149" RevisionGUID="76f8578d-3eaf-4acb-9b93-cca4c07d3835" CreationDate="2017-05-21T19:51:43.143" UserId="48" Comment="added 34 characters in body" Text="Several gene set enrichment methods are available, the most famous/popular is the [Broad Institute tool][1]. Many other tools are available (See for example the [biocView of GSE][2] which list 82 different packages). There are several parameters in consideration :&#xD;&#xA;&#xD;&#xA; - the statistic used to order the genes, &#xD;&#xA; - if it competitive or self-contained,&#xD;&#xA; - if it is supervised or not,&#xD;&#xA; - and how is the enrichment score calculated.&#xD;&#xA;&#xD;&#xA;I am using the [fgsea][3] package to calculate the enrichment scores and someone told me that the numbers are different from the ones on the Broad Institute despite all the other parameters being equivalent.&#xD;&#xA;&#xD;&#xA;Are these two methods (fgsea and Broad Institute GSEA) equivalent to calculate the enrichment score?&#xD;&#xA;&#xD;&#xA;I looked to the algorithms of both papers, and they seem fairly similar, but I don't know if in real datasets they are equivalent or not.&#xD;&#xA;&#xD;&#xA;&lt;sub&gt;Is there any article reviewing and comparing how does the enrichment score method affect to the result?&lt;/sub&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://software.broadinstitute.org/gsea/index.jsp&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/BiocViews.html#___GeneSetEnrichment&#xD;&#xA;  [3]: http://bioconductor.org/packages/fgsea" />
  <row Id="519" PostHistoryTypeId="5" PostId="166" RevisionGUID="3709a277-843c-4847-a980-8d201c8095f4" CreationDate="2017-05-21T19:52:04.773" UserId="35" Comment="added 314 characters in body" Text="According to the [FGSEA preprint][1]:&#xD;&#xA;&#xD;&#xA;&gt; We ran reference GSEA with default parameters. The permutation number&#xD;&#xA;&gt; was set to 1000, which means that for each input gene set 1000&#xD;&#xA;&gt; independent samples were generated. The run took 100 seconds and&#xD;&#xA;&gt; resulted in 79 gene sets with GSEA-adjusted FDR q-value of less than&#xD;&#xA;&gt; 10−2. All significant gene sets were in a positive mode. First, to get&#xD;&#xA;&gt; a similar nominal p-values accuracy we ran FGSEA algorithm on 1000&#xD;&#xA;&gt; permutations. This took 2 seconds, but resulted in no significant hits&#xD;&#xA;&gt; due after multiple testing correction (with FRD ≤ 1%).&#xD;&#xA;&#xD;&#xA;Thus, FGSEA and GSEA are not identical.&#xD;&#xA;&#xD;&#xA;And again in the conclusion:&#xD;&#xA;&#xD;&#xA;&gt; Consequently, gene sets can be ranked more precisely in the results&#xD;&#xA;&gt; and, which is even more important, standard multiple testing&#xD;&#xA;&gt; correction methods can be applied instead of approximate ones as in&#xD;&#xA;&gt; [GSEA].&#xD;&#xA;&#xD;&#xA;The author argues that FGSEA is more accurate, so it can't be equivalent.&#xD;&#xA;&#xD;&#xA;If you are interested specifically in the enrichment score, that was [addressed by the author in the preprint comments][2]:&#xD;&#xA;&#xD;&#xA;&gt; Values of enrichment scores and normalized enrichment scores are the&#xD;&#xA;&gt; same for both broad version and fgsea.&#xD;&#xA;&#xD;&#xA;So that part seems to be the same.&#xD;&#xA;&#xD;&#xA;  [1]: http://biorxiv.org/content/early/2016/06/20/060012&#xD;&#xA;  [2]: http://disq.us/p/1f683mc" />
  <row Id="520" PostHistoryTypeId="5" PostId="172" RevisionGUID="ffd5c14f-c39e-4dd6-ab82-5504646b38ac" CreationDate="2017-05-21T21:36:50.493" UserId="163" Comment="add ragout, nanopolish and canu" Text="Depending on the coverage of your data and the complexity of the genome, you could either reassemble the genome *de novo* or run a reference guided (or reference assisted) assembly. It sounds like you're leaning more towards the latter.&#xD;&#xA;&#xD;&#xA;There are a couple of reference-guided assembly tools available: [AlignGraph](https://doi.org/10.1093/bioinformatics/btu291) and [Ragout][1]. These may or may not be appropriate depending on the organism of interest and your data types. For example, these tools are very unlikely to work well on Oxford Nanopore reads which have not been error corrected using [Nanopolish][2] or [Canu -correct][3]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/pubmed/24931998&#xD;&#xA;  [2]: https://github.com/jts/nanopolish&#xD;&#xA;  [3]: http://canu.readthedocs.io/en/latest/index.html" />
  <row Id="521" PostHistoryTypeId="24" PostId="172" RevisionGUID="ffd5c14f-c39e-4dd6-ab82-5504646b38ac" CreationDate="2017-05-21T21:36:50.493" Comment="Proposed by 163 approved by 96 edit id of 42" />
  <row Id="523" PostHistoryTypeId="5" PostId="174" RevisionGUID="6654c3f4-83b3-42f8-af9b-e0ab8d69fbfc" CreationDate="2017-05-21T22:15:12.523" UserId="57" Comment="Fixed typo" Text="All the long-read sequencing platforms are based on single-molecule sequencing which causes higher per-base error rates. For this reason a polishing step was added to genome assembly pipelines - mapping raw reads back to assembly and correcting details of the assembly.&#xD;&#xA;&#xD;&#xA;I have decent PacBio RSII dataset of single individual genome of heavily heterozygous non-model species. Assembly went well, but when I tried to polish the assembly using [quiver](https://github.com/PacificBiosciences/GenomicConsensus) it could not converge over a couple of iterations and I bet it is because of too great divergence of haplotypes.&#xD;&#xA;&#xD;&#xA;Is there any other way to polish a genome with such properties?&#xD;&#xA;For instance, is there a way to separate long reads by haplotype, so I could polish using one haplotype only?&#xD;&#xA;" />
  <row Id="524" PostHistoryTypeId="5" PostId="155" RevisionGUID="2fde8f55-d7b0-4915-a926-80c555876528" CreationDate="2017-05-21T23:21:14.657" UserId="298" Comment="deleted 1 character in body" Text="According to the [SAM specification][1], the 3rd field of a SAM line (`RNAME`) is:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt; RNAME: Reference sequence NAME of the alignment. If @SQ header lines&#xD;&#xA;&gt; are present, RNAME (if not ‘\*’) must be present in one of the SQ-SN&#xD;&#xA;&gt; tag. An unmapped segment without coordinate has a ‘\*’ at this field.&#xD;&#xA;&gt; However, an unmapped segment may also have an ordinary coordinate such&#xD;&#xA;&gt; that it can be placed at a desired position after sorting. If RNAME is&#xD;&#xA;&gt; ‘\*’, no assumptions can be made about POS and CIGAR.&#xD;&#xA;&#xD;&#xA;And the 7th field is (emphasis mine, missing &quot;to&quot; theirs):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt;  RNEXT: Reference sequence name of the primary alignment of the NEXT&#xD;&#xA;&gt; read in the template. For the last read, the next read is the first&#xD;&#xA;&gt; read in the template. If @SQ header lines are present, RNEXT (if not&#xD;&#xA;&gt; ‘\*’ or ‘=’) must be present in one of the SQ-SN tag. This field is set&#xD;&#xA;&gt; as ‘\*’ when the information is unavailable, **and set as ‘=’ if RNEXT is&#xD;&#xA;&gt; identical RNAME**. If not ‘=’ and the next read in the template has one&#xD;&#xA;&gt; primary mapping (see also bit 0x100 in FLAG), this field is identical&#xD;&#xA;&gt; to RNAME at the primary line of the next read. If RNEXT is ‘\*’, no&#xD;&#xA;&gt; assumptions can be made on PNEXT and bit 0x20&#xD;&#xA;&#xD;&#xA;So, you want to remove those lines whose 7th field isn't `=` and, just in case, those lines whose 7th field isn't `=` *and* isn't the same as the 3rd field. You can therefore use something like this:&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | awk '$7==&quot;=&quot; || $3==$7&#xD;&#xA;    &#xD;&#xA;And, to save as a bam file again:&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | awk '$7==&quot;=&quot; &amp;&amp; $3==$7 | &#xD;&#xA;        samtolls view -b &gt; fixed.bam&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;On a separate note, there's very rarely a need to chain multiple grep commands like that. You can just use `\|` (or `|` with the `-E` or `-P` options) to separate them. Something like:&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | grep -v 'chrx\|chr2\|chr10\|chrN'&#xD;&#xA;&#xD;&#xA;Or&#xD;&#xA;&#xD;&#xA;    samtools view -L test.bed test.bam | grep -Ev 'chrx|chr2|chr10|chrN'&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://samtools.github.io/hts-specs/SAMv1.pdf&#xD;&#xA;" />
  <row Id="525" PostHistoryTypeId="5" PostId="167" RevisionGUID="1a373386-77e0-4199-9e1e-6b9c9e783025" CreationDate="2017-05-22T03:07:46.403" UserId="57" Comment="improved formating" Text="The key function is here:&#xD;&#xA;https://github.com/taoliu/MACS/blob/master/MACS2/IO/CallPeakUnit.pyx#L1443&#xD;&#xA;&#xD;&#xA;The description attached to the function says:&#xD;&#xA;&#xD;&#xA;    This function try to find enriched regions within which,&#xD;&#xA;    scores are continuously higher than a given cutoff for level&#xD;&#xA;    1, and link them using the gap above level 2 cutoff with a&#xD;&#xA;    maximum length of lvl2_max_gap.&#xD;&#xA;    scoring_function_s: symbols of functions to calculate score. 'p' for pscore, 'q' for qscore, 'f' for fold change, 's' for subtraction. for example: ['p', 'q']&#xD;&#xA;    lvl1_cutoff_s:  list of cutoffs at highly enriched regions, corresponding to scoring functions.&#xD;&#xA;    lvl2_cutoff_s:  list of cutoffs at less enriched regions, corresponding to scoring functions.&#xD;&#xA;    min_length :minimum peak length, default 200.&#xD;&#xA;    lvl1_max_gap   :  maximum gap to merge nearby enriched peaks, default 50.&#xD;&#xA;    lvl2_max_gap   :  maximum length of linkage regions, default 400.&#xD;&#xA;    Return both general PeakIO object for highly enriched regions&#xD;&#xA;    and gapped broad regions in BroadPeakIO.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;To give some basic explanation, the algorithm (briefly) appears to be as follows:&#xD;&#xA;&#xD;&#xA;1) Two separate levels of peaks are called, level 1 (a higher pval) and level 2 (a lower pval). Level 1 is controlled by `-p` and level 2 is controlled by `--broad-cutoff`. When each peakset is called, they are immediately linked by the max gap parameter for each set.&#xD;&#xA;&#xD;&#xA;2) Then, assuming that all level 1 peaks should be inside level 2 peaks (this is an explicit assumption by MACS2), the algorithm groups level 1 peaks inside level 2 peaks to output a broad peak.&#xD;&#xA;&#xD;&#xA;...&#xD;&#xA;&#xD;&#xA;This has a few implications:&#xD;&#xA;&#xD;&#xA;1) The broad peak calls really come from the level 2 peaks alone (+ linking). The level 1 peak calls allow you to distinguish sub peaks (so that you can have gapped peaks).&#xD;&#xA;&#xD;&#xA;2) Aside from the linking, the broad peak calls would be the same as narrow peak calls, if you called both with the same pval threshold (for example, if you set `--broad-cutoff 0.1` in broad peak mode, and the `-p 0.1` for narrow peak mode)&#xD;&#xA;" />
  <row Id="526" PostHistoryTypeId="24" PostId="167" RevisionGUID="1a373386-77e0-4199-9e1e-6b9c9e783025" CreationDate="2017-05-22T03:07:46.403" Comment="Proposed by 57 approved by 13, 77 edit id of 45" />
  <row Id="527" PostHistoryTypeId="5" PostId="167" RevisionGUID="cde2dbaa-4dd2-4bc5-b151-6f2dde0a1ba2" CreationDate="2017-05-22T03:08:06.977" UserId="13" Comment="added 21 characters in body" Text="The key function is here:&#xD;&#xA;https://github.com/taoliu/MACS/blob/master/MACS2/IO/CallPeakUnit.pyx#L1443&#xD;&#xA;&#xD;&#xA;The description attached to the function says:&#xD;&#xA;&#xD;&#xA;    This function try to find enriched regions within which,&#xD;&#xA;    scores are continuously higher than a given cutoff for level&#xD;&#xA;    1, and link them using the gap above level 2 cutoff with a&#xD;&#xA;    maximum length of lvl2_max_gap.&#xD;&#xA;    scoring_function_s: symbols of functions to calculate score. 'p' for pscore, 'q' for qscore, 'f' for fold change, 's' for subtraction. for example: ['p', 'q']&#xD;&#xA;    lvl1_cutoff_s:  list of cutoffs at highly enriched regions, corresponding to scoring functions.&#xD;&#xA;    lvl2_cutoff_s:  list of cutoffs at less enriched regions, corresponding to scoring functions.&#xD;&#xA;    min_length :minimum peak length, default 200.&#xD;&#xA;    lvl1_max_gap   :  maximum gap to merge nearby enriched peaks, default 50.&#xD;&#xA;    lvl2_max_gap   :  maximum length of linkage regions, default 400.&#xD;&#xA;    Return both general PeakIO object for highly enriched regions&#xD;&#xA;    and gapped broad regions in BroadPeakIO.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;To give some basic explanation, the algorithm (briefly) appears to be as follows:&#xD;&#xA;&#xD;&#xA;1) Two separate levels of peaks are called, level 1 (a higher pval, ie more significant) and level 2 (a lower pval). Level 1 is controlled by `-p` and level 2 is controlled by `--broad-cutoff`. When each peakset is called, they are immediately linked by the max gap parameter for each set.&#xD;&#xA;&#xD;&#xA;2) Then, assuming that all level 1 peaks should be inside level 2 peaks (this is an explicit assumption by MACS2), the algorithm groups level 1 peaks inside level 2 peaks to output a broad peak.&#xD;&#xA;&#xD;&#xA;...&#xD;&#xA;&#xD;&#xA;This has a few implications:&#xD;&#xA;&#xD;&#xA;1) The broad peak calls really come from the level 2 peaks alone (+ linking). The level 1 peak calls allow you to distinguish sub peaks (so that you can have gapped peaks).&#xD;&#xA;&#xD;&#xA;2) Aside from the linking, the broad peak calls would be the same as narrow peak calls, if you called both with the same pval threshold (for example, if you set `--broad-cutoff 0.1` in broad peak mode, and the `-p 0.1` for narrow peak mode)&#xD;&#xA;" />
  <row Id="528" PostHistoryTypeId="2" PostId="177" RevisionGUID="00d3dbea-f1a2-4f12-96c0-f0ddb22f34b0" CreationDate="2017-05-22T03:36:11.637" UserId="156" Text="A few possibilities:&#xD;&#xA;&#xD;&#xA;**Falcon**&#xD;&#xA;&#xD;&#xA;Try falcon and falcon-unzip. These are designed exactly for your problem and your data: https://github.com/PacificBiosciences/FALCON&#xD;&#xA;&#xD;&#xA;**Not Falcon**&#xD;&#xA;&#xD;&#xA;If you think you have assembled haplotypes (which seems reasonable to expect given enough coverage), you should be able to see the two haplotypes by just doing all pairwise alignments of your contigs. Haplotypes should show up as pairs of contigs that are MUCH more similar (even with a lot of between-haplotype divergence) than other pairs. Once you have all such pairs, you can simply select one of each pair to polish.&#xD;&#xA;" />
  <row Id="529" PostHistoryTypeId="5" PostId="144" RevisionGUID="a0346b3e-04b5-4c6c-9c98-a1f324b26594" CreationDate="2017-05-22T03:36:59.117" UserId="57" Comment="specifing what is meant by verification (frim comment)" Text="I think the best method or combination of methods will depend on aspects of the data that might vary from one dataset to another. E.g. the type, size, and frequency of structural variants, the number SNVs, the quality of the reference, contaminants or other issues (e.g. read quality, sequencing errors) etc.&#xD;&#xA;&#xD;&#xA;For that reason, I'd take two approaches:&#xD;&#xA;&#xD;&#xA;1. Try a lot of methods, and look at their overlap&#xD;&#xA;2. Validate a subset of calls from different methods by wet lab experiments - in the end this is the only real way of knowing the accuracy for a particular case." />
  <row Id="530" PostHistoryTypeId="24" PostId="144" RevisionGUID="a0346b3e-04b5-4c6c-9c98-a1f324b26594" CreationDate="2017-05-22T03:36:59.117" Comment="Proposed by 57 approved by 77, 156 edit id of 43" />
  <row Id="531" PostHistoryTypeId="2" PostId="178" RevisionGUID="e77f9e7a-49fc-48f6-a722-a31ae214ecc4" CreationDate="2017-05-22T06:32:44.180" UserId="-1" Text="" />
  <row Id="532" PostHistoryTypeId="1" PostId="178" RevisionGUID="e77f9e7a-49fc-48f6-a722-a31ae214ecc4" CreationDate="2017-05-22T06:32:44.180" UserId="-1" />
  <row Id="533" PostHistoryTypeId="2" PostId="179" RevisionGUID="f4ad936a-1a0a-4753-9296-4f88eacac9c8" CreationDate="2017-05-22T06:32:44.180" UserId="-1" Text="" />
  <row Id="534" PostHistoryTypeId="1" PostId="179" RevisionGUID="f4ad936a-1a0a-4753-9296-4f88eacac9c8" CreationDate="2017-05-22T06:32:44.180" UserId="-1" />
  <row Id="535" PostHistoryTypeId="2" PostId="180" RevisionGUID="efb4cf8b-ec19-447c-b59d-f99c1eb9846c" CreationDate="2017-05-22T06:33:31.493" UserId="-1" Text="" />
  <row Id="536" PostHistoryTypeId="1" PostId="180" RevisionGUID="efb4cf8b-ec19-447c-b59d-f99c1eb9846c" CreationDate="2017-05-22T06:33:31.493" UserId="-1" />
  <row Id="537" PostHistoryTypeId="2" PostId="181" RevisionGUID="13629c61-4cab-4065-a175-d9f8dd29aa52" CreationDate="2017-05-22T06:33:31.493" UserId="-1" Text="" />
  <row Id="538" PostHistoryTypeId="1" PostId="181" RevisionGUID="13629c61-4cab-4065-a175-d9f8dd29aa52" CreationDate="2017-05-22T06:33:31.493" UserId="-1" />
  <row Id="539" PostHistoryTypeId="2" PostId="182" RevisionGUID="9b40a504-eefe-46ff-b5f5-afb23d6c2fea" CreationDate="2017-05-22T06:38:11.750" UserId="-1" Text="" />
  <row Id="540" PostHistoryTypeId="1" PostId="182" RevisionGUID="9b40a504-eefe-46ff-b5f5-afb23d6c2fea" CreationDate="2017-05-22T06:38:11.750" UserId="-1" />
  <row Id="541" PostHistoryTypeId="2" PostId="183" RevisionGUID="60ce6ec5-f51b-4770-b87b-8e394048e3a2" CreationDate="2017-05-22T06:38:11.750" UserId="-1" Text="" />
  <row Id="542" PostHistoryTypeId="1" PostId="183" RevisionGUID="60ce6ec5-f51b-4770-b87b-8e394048e3a2" CreationDate="2017-05-22T06:38:11.750" UserId="-1" />
  <row Id="543" PostHistoryTypeId="2" PostId="184" RevisionGUID="35188fb1-c99c-44c5-9bc1-bad3c89c04fa" CreationDate="2017-05-22T06:42:42.650" UserId="-1" Text="" />
  <row Id="544" PostHistoryTypeId="1" PostId="184" RevisionGUID="35188fb1-c99c-44c5-9bc1-bad3c89c04fa" CreationDate="2017-05-22T06:42:42.650" UserId="-1" />
  <row Id="545" PostHistoryTypeId="2" PostId="185" RevisionGUID="d46c5303-ec15-4711-ade4-5345381b4002" CreationDate="2017-05-22T06:42:42.650" UserId="-1" Text="" />
  <row Id="546" PostHistoryTypeId="1" PostId="185" RevisionGUID="d46c5303-ec15-4711-ade4-5345381b4002" CreationDate="2017-05-22T06:42:42.650" UserId="-1" />
  <row Id="547" PostHistoryTypeId="2" PostId="186" RevisionGUID="a004538d-137d-4fdb-8961-c0de558a9552" CreationDate="2017-05-22T06:43:49.197" UserId="-1" Text="" />
  <row Id="548" PostHistoryTypeId="1" PostId="186" RevisionGUID="a004538d-137d-4fdb-8961-c0de558a9552" CreationDate="2017-05-22T06:43:49.197" UserId="-1" />
  <row Id="549" PostHistoryTypeId="2" PostId="187" RevisionGUID="5e3ce59d-968a-4b78-b8c7-c68fb1af79a5" CreationDate="2017-05-22T06:43:49.197" UserId="-1" Text="" />
  <row Id="550" PostHistoryTypeId="1" PostId="187" RevisionGUID="5e3ce59d-968a-4b78-b8c7-c68fb1af79a5" CreationDate="2017-05-22T06:43:49.197" UserId="-1" />
  <row Id="552" PostHistoryTypeId="2" PostId="188" RevisionGUID="5abd4846-7d56-4d4b-8d84-a02c872f98f6" CreationDate="2017-05-22T06:46:31.737" UserId="-1" Text="" />
  <row Id="553" PostHistoryTypeId="1" PostId="188" RevisionGUID="5abd4846-7d56-4d4b-8d84-a02c872f98f6" CreationDate="2017-05-22T06:46:31.737" UserId="-1" />
  <row Id="554" PostHistoryTypeId="2" PostId="189" RevisionGUID="41435586-6448-45dd-b32c-89c05ece4ac6" CreationDate="2017-05-22T06:46:31.737" UserId="-1" Text="" />
  <row Id="555" PostHistoryTypeId="1" PostId="189" RevisionGUID="41435586-6448-45dd-b32c-89c05ece4ac6" CreationDate="2017-05-22T06:46:31.737" UserId="-1" />
  <row Id="556" PostHistoryTypeId="2" PostId="190" RevisionGUID="b7ec5009-ef49-4038-8136-6575dac16dd3" CreationDate="2017-05-22T08:12:38.290" UserId="73" Text="You could also have a go at [Canu][1]. It's designed for long-read assembly (both PacBio and Nanopore), although not specifically for complex population sequencing. It tries to strip a genome down into its unique components, and generates paths from those components that are well-supported from the reads.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/marbl/canu/releases" />
  <row Id="558" PostHistoryTypeId="4" PostId="174" RevisionGUID="65b8bc57-fb9c-4626-a82e-a7af80506c40" CreationDate="2017-05-22T09:28:42.417" UserId="73" Comment="title spelling" Text="How to deal with heterozygosity during polishing of genome assembly based on long reads?" />
  <row Id="559" PostHistoryTypeId="24" PostId="174" RevisionGUID="65b8bc57-fb9c-4626-a82e-a7af80506c40" CreationDate="2017-05-22T09:28:42.417" Comment="Proposed by 73 approved by 57, 77 edit id of 57" />
  <row Id="560" PostHistoryTypeId="5" PostId="156" RevisionGUID="389ef7d0-38f6-4b26-9c11-ab5a8a507e7b" CreationDate="2017-05-22T09:29:36.113" UserId="73" Comment="grammar in title / post" Text="Why do some assemblers like [SOAPdenovo2][1] or [Velvet][2] require an odd-length kmer for the construction of De Brujin graphs, while some other assemblers like [ABySS][3] are fine with even-length kmers?&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/aquaskyline/SOAPdenovo2&#xD;&#xA;  [2]: https://github.com/dzerbino/velvet&#xD;&#xA;  [3]: https://github.com/bcgsc/abyss" />
  <row Id="561" PostHistoryTypeId="4" PostId="156" RevisionGUID="389ef7d0-38f6-4b26-9c11-ab5a8a507e7b" CreationDate="2017-05-22T09:29:36.113" UserId="73" Comment="grammar in title / post" Text="Why do some assemblers require an odd-length kmer for the construction of De Brujin graphs?" />
  <row Id="562" PostHistoryTypeId="24" PostId="156" RevisionGUID="389ef7d0-38f6-4b26-9c11-ab5a8a507e7b" CreationDate="2017-05-22T09:29:36.113" Comment="Proposed by 73 approved by -1, 77 edit id of 58" />
  <row Id="563" PostHistoryTypeId="5" PostId="156" RevisionGUID="50011275-fbb5-4433-a212-a2eca89ba947" CreationDate="2017-05-22T09:29:36.113" UserId="57" Comment="grammar in title / post" Text="Why do some assemblers like [SOAPdenovo2][1] or [Velvet][2] require an odd-length kmer for the construction of De Brujin graph, while some other assemblers like [ABySS][3] are fine with even-length kmers?&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/aquaskyline/SOAPdenovo2&#xD;&#xA;  [2]: https://github.com/dzerbino/velvet&#xD;&#xA;  [3]: https://github.com/bcgsc/abyss" />
  <row Id="564" PostHistoryTypeId="5" PostId="149" RevisionGUID="4b46bf8d-64d0-404f-ab7d-062e84c4adcf" CreationDate="2017-05-22T09:32:02.147" UserId="131" Comment="added tags" Text="Several gene set enrichment methods are available, the most famous/popular is the [Broad Institute tool][1]. Many other tools are available (See for example the [biocView of GSE][2] which list 82 different packages). There are several parameters in consideration :&#xD;&#xA;&#xD;&#xA; - the statistic used to order the genes, &#xD;&#xA; - if it competitive or self-contained,&#xD;&#xA; - if it is supervised or not,&#xD;&#xA; - and how is the enrichment score calculated.&#xD;&#xA;&#xD;&#xA;I am using the [fgsea - Fast Gene Set Enrichment Analysis][3] package to calculate the enrichment scores and someone told me that the numbers are different from the ones on the Broad Institute despite all the other parameters being equivalent.&#xD;&#xA;&#xD;&#xA;Are these two methods (fgsea and Broad Institute GSEA) equivalent to calculate the enrichment score?&#xD;&#xA;&#xD;&#xA;I looked to the algorithms of both papers, and they seem fairly similar, but I don't know if in real datasets they are equivalent or not.&#xD;&#xA;&#xD;&#xA;&lt;sub&gt;Is there any article reviewing and comparing how does the enrichment score method affect to the result?&lt;/sub&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://software.broadinstitute.org/gsea/index.jsp&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/BiocViews.html#___GeneSetEnrichment&#xD;&#xA;  [3]: http://bioconductor.org/packages/fgsea" />
  <row Id="565" PostHistoryTypeId="6" PostId="149" RevisionGUID="4b46bf8d-64d0-404f-ab7d-062e84c4adcf" CreationDate="2017-05-22T09:32:02.147" UserId="131" Comment="added tags" Text="&lt;r&gt;&lt;gse&gt;&lt;bioconductor&gt;" />
  <row Id="566" PostHistoryTypeId="24" PostId="149" RevisionGUID="4b46bf8d-64d0-404f-ab7d-062e84c4adcf" CreationDate="2017-05-22T09:32:02.147" Comment="Proposed by 131 approved by 48, 77 edit id of 46" />
  <row Id="567" PostHistoryTypeId="2" PostId="191" RevisionGUID="71fe04b6-7076-460d-adc7-6d460cbf4cdd" CreationDate="2017-05-22T10:47:44.397" UserId="349" Text="I have a huge amount of ~20x human WGS samples, aligned, and all SNVs that were called with GATK under standard germline parameters set.&#xD;&#xA;&#xD;&#xA;What I need to do is to model SNVs Allele Frequency (AF) for different underlying Copy Numbers. I'd better provide a toy example. For particular genomic region X:&#xD;&#xA;&#xD;&#xA;*If X is presented by **2** copies for the particular samples, we expect AF to be super-close to 1 or to 0.5.*&#xD;&#xA;&#xD;&#xA;*If X is presented by **4** copies, I expect any particular AF to be close to 0.25, 0.5, 0.75 or 1.*&#xD;&#xA;&#xD;&#xA;Of course, I can use *Binomial Distribution* for these purposes. However, as we know, the distribution is not exactly Binomial due to alignment/sequencing biases and the median AF for all heterozygous SNVs is more close to 0.48 but not to 0.5 as we would expect. Another thing: for high copy numbers we expect higher coverages. And GATK use several filters so I suppose that we will not see SNVs with AF like 0.125 (in case if the segment has ploidy 8) - despite the super high coverage there GATK may reject this &quot;weird&quot; AF.&#xD;&#xA;&#xD;&#xA;I have read several papers that model SNVs AFs (and I agree that Beta Binomial Distribution may be quite accurate), however, I was not convinced enough that I should use the particular modelling. From your experience (in case if you do SNVs calling), which probabilistic distribution should I use? How should I estimate parameters for each of them (should I expect for CN4 AF=0.5 more frequent than AF=0.75 or vice versa)?" />
  <row Id="568" PostHistoryTypeId="1" PostId="191" RevisionGUID="71fe04b6-7076-460d-adc7-6d460cbf4cdd" CreationDate="2017-05-22T10:47:44.397" UserId="349" Text="SNVs modelling in real NGS data" />
  <row Id="569" PostHistoryTypeId="3" PostId="191" RevisionGUID="71fe04b6-7076-460d-adc7-6d460cbf4cdd" CreationDate="2017-05-22T10:47:44.397" UserId="349" Text="&lt;snv&gt;&lt;modelling&gt;" />
  <row Id="570" PostHistoryTypeId="5" PostId="191" RevisionGUID="f9f81070-e7c6-49f8-8c15-348dc2d0ed2f" CreationDate="2017-05-22T10:55:41.207" UserId="349" Comment="added 32 characters in body" Text="I have a huge amount of ~20x human WGS samples, aligned, and all SNVs that were called with GATK under standard germline parameters set.&#xD;&#xA;&#xD;&#xA;What I need to do is to model SNVs Allele Frequency (AF) for different underlying Copy Numbers. I'd better provide a toy example. For particular genomic region X:&#xD;&#xA;&#xD;&#xA;*If X is presented by **2** copies for the particular samples, we expect AF to be super-close to 1 or to 0.5.*&#xD;&#xA;&#xD;&#xA;*If X is presented by **4** copies, I expect any particular AF to be close to 0.25, 0.5, 0.75 or 1.*&#xD;&#xA;&#xD;&#xA;Of course, I can use *Binomial Distribution* for these purposes. However, as we know, the distribution is not exactly Binomial due to alignment/sequencing biases and the median AF for all heterozygous SNVs is more close to 0.48 but not to 0.5 as we would expect. Another thing: for high copy numbers we expect higher coverages. And GATK use several filters so I suppose that we will not see SNVs with AF like 0.125 (in case if the segment has ploidy 8) - despite the super high coverage there GATK may reject this &quot;weird&quot; AF.&#xD;&#xA;&#xD;&#xA;I have read several papers that model SNVs AFs (and I agree that Beta Binomial Distribution may be quite accurate), however, I was not convinced enough that I should use the particular modelling. From your experience (in case if you do SNVs calling), which probabilistic distribution should I use? How should I estimate parameters for each of them (should I expect for CN4 AF=0.5 more frequent than AF=0.75 or vice versa, how to estimate this from data)?" />
  <row Id="571" PostHistoryTypeId="6" PostId="191" RevisionGUID="f9f81070-e7c6-49f8-8c15-348dc2d0ed2f" CreationDate="2017-05-22T10:55:41.207" UserId="349" Comment="added 32 characters in body" Text="&lt;snv&gt;&lt;modelling&gt;&lt;statistics&gt;" />
  <row Id="573" PostHistoryTypeId="33" PostId="107" RevisionGUID="554b9984-97b6-4889-80e2-b8982e2b040f" CreationDate="2017-05-22T11:40:16.670" UserId="182" Comment="1" />
  <row Id="574" PostHistoryTypeId="5" PostId="191" RevisionGUID="d61f1896-cd9b-41da-b2c2-0a4970368672" CreationDate="2017-05-22T11:58:08.050" UserId="349" Comment="added 317 characters in body" Text="I have a huge amount of ~20x human WGS samples, aligned, and all SNVs that were called with GATK under standard germline parameters set.&#xD;&#xA;&#xD;&#xA;What I need to do is to model SNVs Allele Frequency (AF) for different underlying Copy Numbers. I'd better provide a toy example. For particular genomic region X:&#xD;&#xA;&#xD;&#xA;*If X is presented by **2** copies for the particular samples, we expect AF to be super-close to 1 or to 0.5.*&#xD;&#xA;&#xD;&#xA;*If X is presented by **4** copies, I expect any particular AF to be close to 0.25, 0.5, 0.75 or 1.*&#xD;&#xA;&#xD;&#xA;Of course, I can use *Binomial Distribution* for these purposes. However, as we know, the distribution is not exactly Binomial due to alignment/sequencing biases and the median AF for all heterozygous SNVs is more close to 0.48 but not to 0.5 as we would expect. Another thing: for high copy numbers we expect higher coverages. And GATK use several filters so I suppose that we will not see SNVs with AF like 0.125 (in case if the segment has ploidy 8) - despite the super high coverage there GATK may reject this &quot;weird&quot; AF.&#xD;&#xA;&#xD;&#xA;I have read several papers that model SNVs AFs (and I agree that Beta Binomial Distribution may be quite accurate), however, I was not convinced enough that I should use the particular modelling. From your experience (in case if you do SNVs calling), which probabilistic distribution should I use? How should I estimate parameters for each of them (should I expect for CN4 AF=0.5 more frequent than AF=0.75 or vice versa, how to estimate this from data)?&#xD;&#xA;&#xD;&#xA;**UPD:** For simplicity we can say that we have a lot of previously identified regions with ploidy different from CN2, and I can take these coordinates from [here][1]. So I can use more or less &quot;supervised&quot; learning for parameters' estimation.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.nature.com/ng/journal/v47/n3/full/ng.3200.html" />
  <row Id="575" PostHistoryTypeId="2" PostId="192" RevisionGUID="462310e0-a551-4315-b713-d1a3980a1880" CreationDate="2017-05-22T12:05:18.343" UserId="292" Text="It seems that the &quot;combining factors&quot; trick described in part 3.3 of DESeq2 current &quot;vignette&quot; (as of may 2017) under the title &quot;Interaction&quot; is a way to access to the desired contrasts.&#xD;&#xA;&#xD;&#xA;It seems possible do do it directly when building the `colData` and when calling `DESeqDataSetFromMatrix`:&#xD;&#xA;&#xD;&#xA;Let's add a combined &quot;geno&quot; and &quot;treat&quot; factors to the future `colData` parameter:&#xD;&#xA;&#xD;&#xA;    &gt; col_data$geno_treat &lt;- as.factor(paste(col_data$geno, col_data$treat, sep=&quot;_&quot;))&#xD;&#xA;    &gt; col_data&#xD;&#xA;    DataFrame with 12 rows and 4 columns&#xD;&#xA;                            geno       treat         rep     geno_treat&#xD;&#xA;                     &lt;character&gt; &lt;character&gt; &lt;character&gt;       &lt;factor&gt;&#xD;&#xA;    WT_RT_1                   WT          RT           1          WT_RT&#xD;&#xA;    WT_HS30_1                 WT        HS30           1        WT_HS30&#xD;&#xA;    WT_HS30RT120_1            WT   HS30RT120           1   WT_HS30RT120&#xD;&#xA;    prg1_RT_1               prg1          RT           1        prg1_RT&#xD;&#xA;    prg1_HS30_1             prg1        HS30           1      prg1_HS30&#xD;&#xA;    ...                      ...         ...         ...            ...&#xD;&#xA;    WT_HS30_2                 WT        HS30           2        WT_HS30&#xD;&#xA;    WT_HS30RT120_2            WT   HS30RT120           2   WT_HS30RT120&#xD;&#xA;    prg1_RT_2               prg1          RT           2        prg1_RT&#xD;&#xA;    prg1_HS30_2             prg1        HS30           2      prg1_HS30&#xD;&#xA;    prg1_HS30RT120_2        prg1   HS30RT120           2 prg1_HS30RT120&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;We can now use a design where differential expression will be explained by these combined factors:&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeqDataSetFromMatrix(&#xD;&#xA;        countData = counts_data,&#xD;&#xA;        colData = col_data,&#xD;&#xA;        design = ~ geno_treat)&#xD;&#xA;&#xD;&#xA;We run the analysis:&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Then we can query results for a particular contrast between such factor combinations. For instance, to have the results for the effect of treatment &quot;HS30&quot; against the reference state &quot;RT&quot; in genotype &quot;prg1&quot;:&#xD;&#xA;&#xD;&#xA;    res &lt;- results(dds, contrast=c(&quot;geno_treat&quot;, &quot;prg1_HS30&quot;, &quot;prg1_RT&quot;))&#xD;&#xA;&#xD;&#xA;" />
  <row Id="578" PostHistoryTypeId="2" PostId="193" RevisionGUID="0826e17d-ae87-41ff-ac87-c55db69cb7d0" CreationDate="2017-05-22T14:51:27.550" UserId="292" Text="The results obtained by running the `results` command from DESeq2 contain a &quot;baseMean&quot; column, which I assume is the mean across samples of the normalized counts for a given gene.&#xD;&#xA;&#xD;&#xA;**How can I access the normalized counts proper?**&#xD;&#xA;&#xD;&#xA;I tried the following (continuing with the example used [here](https://bioinformatics.stackexchange.com/a/192/292)):&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeqDataSetFromMatrix(countData = counts_data, colData = col_data, design = ~ geno_treat)&#xD;&#xA;    &gt; dds &lt;- DESeq(dds)&#xD;&#xA;    estimating size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    final dispersion estimates&#xD;&#xA;    fitting model and testing&#xD;&#xA;    &gt; res &lt;- results(dds, contrast=c(&quot;geno_treat&quot;, &quot;prg1_HS30&quot;, &quot;prg1_RT&quot;))&#xD;&#xA;&#xD;&#xA;Here is what I have for the first gene:&#xD;&#xA;&#xD;&#xA;    &gt; res[&quot;WBGene00000001&quot;,]$baseMean&#xD;&#xA;    [1] 181.7862&#xD;&#xA;    &gt; mean(assays(dds)$mu[&quot;WBGene00000001&quot;,])&#xD;&#xA;    [1] 231.4634&#xD;&#xA;    &gt; mean(assays(dds)$counts[&quot;WBGene00000001&quot;,])&#xD;&#xA;    [1] 232.0833&#xD;&#xA;&#xD;&#xA;`assays(dds)$counts` corresponds to the raw counts. `assays(dds)$mu` seems to be a transformation of these counts approximately preserving their mean, but this mean is very different from the &quot;baseMean&quot; value, so these are likely not the normalized values." />
  <row Id="579" PostHistoryTypeId="1" PostId="193" RevisionGUID="0826e17d-ae87-41ff-ac87-c55db69cb7d0" CreationDate="2017-05-22T14:51:27.550" UserId="292" Text="How can I extract normalized read count values from DESeq2 results?" />
  <row Id="580" PostHistoryTypeId="3" PostId="193" RevisionGUID="0826e17d-ae87-41ff-ac87-c55db69cb7d0" CreationDate="2017-05-22T14:51:27.550" UserId="292" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;deseq2&gt;&lt;normalization&gt;" />
  <row Id="581" PostHistoryTypeId="2" PostId="194" RevisionGUID="4f88335f-a5f0-4963-b6ec-de4befd5aba7" CreationDate="2017-05-22T15:02:28.473" UserId="48" Text="In a weighted gene co-expression network analysis (using [WGCNA][1]), the soft-threshold power is recommended as a noise filtering. It consists on raising the correlation to a certain number. To decide this power the scale-free topology is estimated for some powers. The function to estimate this prints:&#xD;&#xA;&#xD;&#xA;       Power SFT.R.sq  slope truncated.R.sq mean.k. median.k. max.k.&#xD;&#xA;    1      1   0.9300  3.110          0.996  2960.0    3060.0   3970&#xD;&#xA;    2      2   0.7510  1.010          0.964  1750.0    1780.0   2900&#xD;&#xA;    3      3   0.1730  0.258          0.806  1170.0    1150.0   2280&#xD;&#xA;    4      4   0.0942 -0.183          0.713   833.0     782.0   1870&#xD;&#xA;    5      5   0.3800 -0.463          0.777   623.0     559.0   1580&#xD;&#xA;    6      6   0.5350 -0.656          0.834   481.0     412.0   1360&#xD;&#xA;    7      7   0.6270 -0.797          0.872   381.0     312.0   1190&#xD;&#xA;    8      8   0.6870 -0.910          0.900   307.0     241.0   1050&#xD;&#xA;    9      9   0.7270 -1.000          0.918   252.0     189.0    936&#xD;&#xA;    10    10   0.7490 -1.080          0.928   210.0     150.0    841&#xD;&#xA;    11    12   0.7850 -1.190          0.948   150.0      98.0    693&#xD;&#xA;    12    14   0.8090 -1.280          0.958   111.0      65.9    582&#xD;&#xA;    13    16   0.8290 -1.360          0.968    84.0      45.6    497&#xD;&#xA;    14    18   0.8410 -1.410          0.973    65.2      32.1    429&#xD;&#xA;    15    20   0.8490 -1.450          0.977    51.6      23.0    375&#xD;&#xA;&#xD;&#xA;The recommendations of the [FAQ][2] indicate that a SFT.R.sq value should be above&#xD;&#xA;&#xD;&#xA;&gt;  0.8 for reasonable powers (less than 15 for unsigned or signed hybrid networks, and less than 30 for signed networks) &#xD;&#xA;&#xD;&#xA;and the mean connectivity below the hundreds.&#xD;&#xA;&#xD;&#xA;Others have used the power just as noise filtering without caring much about the scale-free topology fit. I would pick the first power even if the mean connectivity is in the order of thousands because the scale-free topology fit is pretty high, however the slope is puzzling me.&#xD;&#xA;&#xD;&#xA;How should the soft-threshold power be selected?&#xD;&#xA;&#xD;&#xA;&lt;sup&gt;Based on an example [question](https://area51.stackexchange.com/proposals/109245/bioinformatics/109254#109254)&lt;/sup&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://cran.r-project.org/package=WGCNA&#xD;&#xA;  [2]: https://labs.genetics.ucla.edu/horvath/CoexpressionNetwork/Rpackages/WGCNA/faq.html" />
  <row Id="582" PostHistoryTypeId="1" PostId="194" RevisionGUID="4f88335f-a5f0-4963-b6ec-de4befd5aba7" CreationDate="2017-05-22T15:02:28.473" UserId="48" Text="How to select a power for a scale-free topology network" />
  <row Id="583" PostHistoryTypeId="3" PostId="194" RevisionGUID="4f88335f-a5f0-4963-b6ec-de4befd5aba7" CreationDate="2017-05-22T15:02:28.473" UserId="48" Text="&lt;statistics&gt;&lt;networks&gt;&lt;threshold&gt;" />
  <row Id="584" PostHistoryTypeId="2" PostId="195" RevisionGUID="accf0453-4f6f-4273-9405-e2f34e833c53" CreationDate="2017-05-22T16:14:17.213" UserId="377" Text="**Background:** We're increasingly needing some way of storing lots of variant data associated with lots of subjects: think clinical trials and hospital patients, looking for disease-causing or relevant genes. A thousand subjects is where we'd start, there's talk of millions on the horizon. With various genomic medicine initiatives, this is likely a wider need.&#xD;&#xA;&#xD;&#xA;**The problem:** While there's plenty of platforms out there, it's a rapidly evolving field. It's difficult to get a feel for how (and if) they perform and how they line up against each other:&#xD;&#xA;&#xD;&#xA; - What's scalable and can handle a lot of data? What sort of limits?&#xD;&#xA; - What's robust and not a teetering pile of hacked-together components?&#xD;&#xA; - What has a large community behind it and is actually used widely?&#xD;&#xA; - What makes for easy access and search from another service? (Commandline, REST or software APIs)&#xD;&#xA; - What sort of variants they handle?&#xD;&#xA; - What sort of parameters can be used in searching? &#xD;&#xA;&#xD;&#xA;**Solutions I've seen so far:**&#xD;&#xA;&#xD;&#xA; - BigQ: used with i2b2, wider use unclear&#xD;&#xA; - OpenCGA: looks the most developed, heard complaints about the size of data it spits out&#xD;&#xA; - Using BigQuery over a Google Genomics db: not really a general solution?&#xD;&#xA; - Gemini: is it scalable and accessible from other services?&#xD;&#xA; - SciDb: commercial general db&#xD;&#xA; - Quince&#xD;&#xA; - LOVD&#xD;&#xA; - Adam&#xD;&#xA; - Whatever platform DIVAS &amp; RVD run on&#xD;&#xA; - Several graphical / graph genome solutions&#xD;&#xA; - Roll your own: is this really a plausible solution?&#xD;&#xA;&#xD;&#xA;Anyone with experience give a review or high-level guide to this platform space?&#xD;&#xA;" />
  <row Id="585" PostHistoryTypeId="1" PostId="195" RevisionGUID="accf0453-4f6f-4273-9405-e2f34e833c53" CreationDate="2017-05-22T16:14:17.213" UserId="377" Text="The state, limitations and comparisons of large variant stores" />
  <row Id="586" PostHistoryTypeId="3" PostId="195" RevisionGUID="accf0453-4f6f-4273-9405-e2f34e833c53" CreationDate="2017-05-22T16:14:17.213" UserId="377" Text="&lt;genomics&gt;&lt;variants&gt;&lt;human-genome&gt;&lt;data-management&gt;" />
  <row Id="587" PostHistoryTypeId="2" PostId="196" RevisionGUID="bbbfaf82-4353-4db8-bbd7-b2e36f2a1afe" CreationDate="2017-05-22T16:32:40.663" UserId="243" Text="I am the resident Bioinfo Geek in a hospital academic lab that routinely employs NGS as well as CyTOF and other large volume data producing technologies. I am sick of our current &quot;protocol&quot; for metadata collection and association with the final products (miriad excel sheets and a couple poorly designed RedCap DBs).&#xD;&#xA;&#xD;&#xA;I want to implement a central structured, controlled datastore that will take care of this. I know that the interface to the technicians how will be inputing the data is crucial to its adoption, but this is not the focus of THIS particular question: **Does there exist a schema or schema guidelines for this type of database?**&#xD;&#xA;&#xD;&#xA;I would rather use a model that has been developed by people who know how to do this well. I know of BioSQL but it seems more geared towards full protein/nucleotide records like those found in uniprot or genbank. That is not what we have here. What I want is something similar to the system touched on in this preprint: [http://biorxiv.org/content/early/2017/05/10/136358][1]&#xD;&#xA;&#xD;&#xA;Alternatively, can anyone provide links to where I might find relevant guidelines or supply personal advice?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biorxiv.org/content/early/2017/05/10/136358 &quot;Managing The Analysis Of High-Throughput Sequencing Data&quot;" />
  <row Id="588" PostHistoryTypeId="1" PostId="196" RevisionGUID="bbbfaf82-4353-4db8-bbd7-b2e36f2a1afe" CreationDate="2017-05-22T16:32:40.663" UserId="243" Text="Designing a lab NGS results database schema" />
  <row Id="589" PostHistoryTypeId="3" PostId="196" RevisionGUID="bbbfaf82-4353-4db8-bbd7-b2e36f2a1afe" CreationDate="2017-05-22T16:32:40.663" UserId="243" Text="&lt;ngs&gt;&lt;database&gt;&lt;schema&gt;&lt;sample-database&gt;" />
  <row Id="590" PostHistoryTypeId="5" PostId="195" RevisionGUID="404de3c8-ea10-4818-9dd4-111cc685f68c" CreationDate="2017-05-22T17:00:16.357" UserId="377" Comment="deleted 519 characters in body" Text="**Background:** We're increasingly needing some way of storing lots of variant data associated with lots of subjects: think clinical trials and hospital patients, looking for disease-causing or relevant genes. A thousand subjects is where we'd start, there's talk of millions on the horizon. With various genomic medicine initiatives, this is likely a wider need.&#xD;&#xA;&#xD;&#xA;**The problem:** While there's plenty of platforms out there, it's a rapidly evolving field. It's difficult to get a feel for how (and if) they perform and how they line up against each other:&#xD;&#xA;&#xD;&#xA; - What's scalable and can handle a lot of data? What sort of limits?&#xD;&#xA; - What's robust and not a teetering pile of hacked-together components?&#xD;&#xA; - What has a large community behind it and is actually used widely?&#xD;&#xA; - What makes for easy access and search from another service? (Commandline, REST or software APIs)&#xD;&#xA; - What sort of variants they handle?&#xD;&#xA; - What sort of parameters can be used in searching? &#xD;&#xA;&#xD;&#xA;Anyone with experience give a review or high-level guide to this platform space?&#xD;&#xA;" />
  <row Id="591" PostHistoryTypeId="2" PostId="197" RevisionGUID="30b1494e-f657-4dd0-9280-962f35ffb137" CreationDate="2017-05-22T17:01:00.507" UserId="47" Text="The [Global Alliance for Genomics and Health](http://ga4gh.org/) has been working on the issue of representing sequencing data and metadata for storage and sharing for quite some time, though with mixed results.  They do offer a model and API for storing NGS data in their [GitHub repository](https://github.com/ga4gh/ga4gh-schemas), but it can be a bit of a pain to get a high-level view.  I am not sure if any better representation of this exists elsewhere.&#xD;&#xA;&#xD;&#xA;I can say from personal experience (having built over a dozen genomic databases), there is no ideal data model and storage best practices.  Genomic data comes in many shapes and sizes, and your needs are going to vary from every other organization, so what works for one bioinformatics group won't necessarily work for you.  The best thing to do is design and implement a model that will cover all of the data types in your workflow." />
  <row Id="592" PostHistoryTypeId="4" PostId="196" RevisionGUID="db8024c0-7ae1-4fbd-aa2f-1807cd4c5ffb" CreationDate="2017-05-22T17:01:44.230" UserId="243" Comment="edited title" Text="Designing a lab NGS file database schema" />
  <row Id="593" PostHistoryTypeId="5" PostId="195" RevisionGUID="59564b56-7f6d-4951-adf9-50197968a45a" CreationDate="2017-05-22T17:06:53.827" UserId="377" Comment="Removed question to add to comment, changed mind about best place." Text="**Background:** We're increasingly needing some way of storing lots of variant data associated with lots of subjects: think clinical trials and hospital patients, looking for disease-causing or relevant genes. A thousand subjects is where we'd start, there's talk of millions on the horizon. With various genomic medicine initiatives, this is likely a wider need.&#xD;&#xA;&#xD;&#xA;**The problem:** While there's plenty of platforms out there, it's a rapidly evolving field. It's difficult to get a feel for how (and if) they perform and how they line up against each other:&#xD;&#xA;&#xD;&#xA; - What's scalable and can handle a lot of data? What sort of limits?&#xD;&#xA; - What's robust and not a teetering pile of hacked-together components?&#xD;&#xA; - What has a large community behind it and is actually used widely?&#xD;&#xA; - What makes for easy access and search from another service? (Commandline, REST or software APIs)&#xD;&#xA; - What sort of variants they handle?&#xD;&#xA; - What sort of parameters can be used in searching? &#xD;&#xA;&#xD;&#xA;**Solutions I've seen so far:**&#xD;&#xA;&#xD;&#xA; - BigQ: used with i2b2, but its wider use is unclear&#xD;&#xA; - OpenCGA: looks the most developed, but I've heard complaints about the size of data it spits out&#xD;&#xA; - Using BigQuery over a Google Genomics db: doesn't seem to be a general solution&#xD;&#xA; - Gemini: recommended but is it really scalable and accessible from other services?&#xD;&#xA; - SciDb: a commercial general db&#xD;&#xA; - Quince&#xD;&#xA; - LOVD&#xD;&#xA; - Adam&#xD;&#xA; - Whatever platform DIVAS &amp; RVD run on: which may not be freely available&#xD;&#xA; - Several graphical / graph genome solutions: We (and most other people) are probably not dealing with graph genome data at the moment, but is this a possible solution?&#xD;&#xA; - Roll your own: Frequently recommended but I'm sceptical this is a plausible solution for a large dataset.&#xD;&#xA;&#xD;&#xA;Anyone with experience give a review or high-level guide to this platform space?&#xD;&#xA;" />
  <row Id="595" PostHistoryTypeId="2" PostId="199" RevisionGUID="1168688e-d163-433b-9fca-f9bab0560b93" CreationDate="2017-05-22T18:04:31.807" UserId="96" Text="I agree that there is no ideal data model that is going to be stable for very long in a quick-moving field like genome informatics. Perhaps a schema-less (NoSQL or some other document-based system, such as [MongoDB](https://www.mongodb.com/)) database approach would work better? This gives you ultimate flexibility to attach whatever information is relevant to database entries you're adding to your database now, without the need to rebuild the database later if you want to attach more/different information to subsequent database entries." />
  <row Id="596" PostHistoryTypeId="5" PostId="197" RevisionGUID="7f9eec13-aee7-4c63-9245-75aac44668e6" CreationDate="2017-05-22T18:08:04.757" UserId="47" Comment="added 64 characters in body" Text="The [Global Alliance for Genomics and Health](http://ga4gh.org/) has been working on the issue of representing sequencing data and metadata for storage and sharing for quite some time, though with mixed results.  They do offer a model and API for storing NGS data in their [GitHub repository](https://github.com/ga4gh/ga4gh-schemas), but it can be a bit of a pain to get a high-level view.  I am not sure if any better representation of this exists elsewhere.&#xD;&#xA;&#xD;&#xA;I can say from personal experience (having built over a dozen genomic databases), there is no ideal data model and storage best practices.  Genomic data comes in many shapes and sizes, and your needs are going to vary from every other organization, so what works for one bioinformatics group won't necessarily work for you.  The best thing to do is design and implement a model that will cover all of the data types in your workflow and downstream analyses you might do with the data and metadata." />
  <row Id="597" PostHistoryTypeId="2" PostId="200" RevisionGUID="4b9e1de5-9733-49a2-ba48-6b4841cca49f" CreationDate="2017-05-22T18:11:10.693" UserId="96" Text="In all seriousness, the most efficient way to store DNA sequence data is...you guessed it...in DNA. [(Church, Gao, and Kasuri, 2012)](http://dx.doi.org/10.1126/science.1226355) and others have used DNA synthesis and sequencing as an information writing/reading mechanism.&#xD;&#xA;&#xD;&#xA;Practical? Not yet.&#xD;&#xA;&#xD;&#xA;Storage efficiency? Unparalleled!" />
  <row Id="598" PostHistoryTypeId="2" PostId="201" RevisionGUID="dc421ac7-3958-4dd4-82cd-3d8f31d5f1a1" CreationDate="2017-05-22T18:17:38.167" UserId="77" Text="The normalized counts themselves can be accessed with `counts(dds, normalized=T)`.&#xD;&#xA;&#xD;&#xA;Now as to what the baseMean actually means, that will depend upon whether an &quot;expanded model matrix&quot; is in use or not. Given your previous question, we can see that `geno_treat` has a bunch of levels, which means that expanded models are not in use. In such cases, the baseMean should be the mean of the base factor in `geno_treat`." />
  <row Id="599" PostHistoryTypeId="2" PostId="202" RevisionGUID="88b11427-259a-4e35-9639-fa6ed5934d90" CreationDate="2017-05-22T18:19:53.483" UserId="96" Text="Are there any free open source software tools available for simulating Oxford Nanopore reads?" />
  <row Id="600" PostHistoryTypeId="1" PostId="202" RevisionGUID="88b11427-259a-4e35-9639-fa6ed5934d90" CreationDate="2017-05-22T18:19:53.483" UserId="96" Text="Tools for simulating Oxford Nanopore reads" />
  <row Id="601" PostHistoryTypeId="3" PostId="202" RevisionGUID="88b11427-259a-4e35-9639-fa6ed5934d90" CreationDate="2017-05-22T18:19:53.483" UserId="96" Text="&lt;genome-sequencing&gt;&lt;oxford-nanopore&gt;&lt;3rd-gen-sequencing&gt;" />
  <row Id="602" PostHistoryTypeId="2" PostId="203" RevisionGUID="dad92866-6d30-451d-b80d-8fdd49d82cf9" CreationDate="2017-05-22T18:22:02.093" UserId="-1" Text="" />
  <row Id="603" PostHistoryTypeId="1" PostId="203" RevisionGUID="dad92866-6d30-451d-b80d-8fdd49d82cf9" CreationDate="2017-05-22T18:22:02.093" UserId="-1" />
  <row Id="604" PostHistoryTypeId="2" PostId="204" RevisionGUID="99b1c4da-bb3f-4493-9e45-be7366e482f2" CreationDate="2017-05-22T18:22:02.093" UserId="-1" Text="" />
  <row Id="605" PostHistoryTypeId="1" PostId="204" RevisionGUID="99b1c4da-bb3f-4493-9e45-be7366e482f2" CreationDate="2017-05-22T18:22:02.093" UserId="-1" />
  <row Id="606" PostHistoryTypeId="2" PostId="205" RevisionGUID="eec10d2c-c1cc-4e54-acef-8f0278c0dff6" CreationDate="2017-05-22T18:24:39.177" UserId="-1" Text="" />
  <row Id="607" PostHistoryTypeId="1" PostId="205" RevisionGUID="eec10d2c-c1cc-4e54-acef-8f0278c0dff6" CreationDate="2017-05-22T18:24:39.177" UserId="-1" />
  <row Id="608" PostHistoryTypeId="2" PostId="206" RevisionGUID="bbae13a7-6ec2-4dd3-8a67-d2d3271a4511" CreationDate="2017-05-22T18:24:39.177" UserId="-1" Text="" />
  <row Id="609" PostHistoryTypeId="1" PostId="206" RevisionGUID="bbae13a7-6ec2-4dd3-8a67-d2d3271a4511" CreationDate="2017-05-22T18:24:39.177" UserId="-1" />
  <row Id="610" PostHistoryTypeId="2" PostId="207" RevisionGUID="d2db3df3-aff0-47bc-b145-c65815180ce0" CreationDate="2017-05-22T18:29:07.460" UserId="-1" Text="" />
  <row Id="611" PostHistoryTypeId="1" PostId="207" RevisionGUID="d2db3df3-aff0-47bc-b145-c65815180ce0" CreationDate="2017-05-22T18:29:07.460" UserId="-1" />
  <row Id="612" PostHistoryTypeId="2" PostId="208" RevisionGUID="08542c4f-ffc4-4419-9d81-8c64763d7a45" CreationDate="2017-05-22T18:29:07.460" UserId="-1" Text="" />
  <row Id="613" PostHistoryTypeId="1" PostId="208" RevisionGUID="08542c4f-ffc4-4419-9d81-8c64763d7a45" CreationDate="2017-05-22T18:29:07.460" UserId="-1" />
  <row Id="614" PostHistoryTypeId="2" PostId="209" RevisionGUID="87ccf322-9020-40d2-a208-75719a8c4b9c" CreationDate="2017-05-22T18:34:08.353" UserId="-1" Text="" />
  <row Id="615" PostHistoryTypeId="1" PostId="209" RevisionGUID="87ccf322-9020-40d2-a208-75719a8c4b9c" CreationDate="2017-05-22T18:34:08.353" UserId="-1" />
  <row Id="616" PostHistoryTypeId="2" PostId="210" RevisionGUID="5fa14303-413b-4b5e-a7ab-654e1cc663a4" CreationDate="2017-05-22T18:34:08.353" UserId="-1" Text="" />
  <row Id="617" PostHistoryTypeId="1" PostId="210" RevisionGUID="5fa14303-413b-4b5e-a7ab-654e1cc663a4" CreationDate="2017-05-22T18:34:08.353" UserId="-1" />
  <row Id="618" PostHistoryTypeId="2" PostId="211" RevisionGUID="295b08e9-ee1c-4056-99da-245a4a929ffc" CreationDate="2017-05-22T18:36:31.463" UserId="-1" Text="" />
  <row Id="619" PostHistoryTypeId="1" PostId="211" RevisionGUID="295b08e9-ee1c-4056-99da-245a4a929ffc" CreationDate="2017-05-22T18:36:31.463" UserId="-1" />
  <row Id="620" PostHistoryTypeId="2" PostId="212" RevisionGUID="a8b3b8e1-8f7e-4898-9b0d-d65977e658d2" CreationDate="2017-05-22T18:36:31.463" UserId="-1" Text="" />
  <row Id="621" PostHistoryTypeId="1" PostId="212" RevisionGUID="a8b3b8e1-8f7e-4898-9b0d-d65977e658d2" CreationDate="2017-05-22T18:36:31.463" UserId="-1" />
  <row Id="622" PostHistoryTypeId="2" PostId="213" RevisionGUID="d7ab63bf-a231-46e9-a541-ca8469fb1dd8" CreationDate="2017-05-22T18:53:31.967" UserId="-1" Text="" />
  <row Id="623" PostHistoryTypeId="1" PostId="213" RevisionGUID="d7ab63bf-a231-46e9-a541-ca8469fb1dd8" CreationDate="2017-05-22T18:53:31.967" UserId="-1" />
  <row Id="624" PostHistoryTypeId="2" PostId="214" RevisionGUID="2045c13c-ad12-4916-89b2-d25bfc5875c5" CreationDate="2017-05-22T18:53:31.967" UserId="-1" Text="" />
  <row Id="625" PostHistoryTypeId="1" PostId="214" RevisionGUID="2045c13c-ad12-4916-89b2-d25bfc5875c5" CreationDate="2017-05-22T18:53:31.967" UserId="-1" />
  <row Id="627" PostHistoryTypeId="2" PostId="215" RevisionGUID="efd813d8-34e2-4eb1-aa3b-ea3bbde13d03" CreationDate="2017-05-22T19:07:50.327" UserId="96" Text="Visual inspection with histograms, boxplots, or some other distribution visualization is the way to go. Prior to normalization, your abundances may look something like this.&#xD;&#xA;[![Pre-norm][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Post-normalization, they should look something like this.&#xD;&#xA;[![Post-norm][2]][2]&#xD;&#xA;&#xD;&#xA;See [this blog post](https://biowize.wordpress.com/2013/12/12/normalization-for-differential-expression-analysis/) for example code.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/VFVcC.jpg&#xD;&#xA;  [2]: https://i.stack.imgur.com/XrgU2.jpg" />
  <row Id="628" PostHistoryTypeId="2" PostId="216" RevisionGUID="154995d6-4b37-4e50-9495-17ac52247e9c" CreationDate="2017-05-22T19:13:49.987" UserId="96" Text="The [wgsim package](https://github.com/lh3/wgsim) by Heng Li (of BWA and samtools fame) is my go-to tool for simulating Illumina reads. It doesn't provide any convenient way to simulate differential coverage across different sequences, but it shouldn't be to hard to run wgsim multiple times, generating the desired level of coverage for each sequence of interest.&#xD;&#xA;&#xD;&#xA;I would implement a Python script to slurp up your test file, and call wgsim (using the `subprocess` module) for each sequence. This will probably require you to have each sequence in a separate file. :-(&#xD;&#xA;" />
  <row Id="629" PostHistoryTypeId="2" PostId="217" RevisionGUID="76f6ffa6-a419-48a5-9781-04b7eec906c7" CreationDate="2017-05-22T19:31:41.353" UserId="37" Text="For *metadata*, I would use a SQL schema something like the following:&#xD;&#xA;&#xD;&#xA;    CREATE TABLE Project (&#xD;&#xA;        ac TEXT, -- project/Study accession&#xD;&#xA;        PRIMARY KEY (ac)&#xD;&#xA;    );&#xD;&#xA;    CREATE TABLE Sample ( -- biological sample/biopsy&#xD;&#xA;    	ac TEXT,&#xD;&#xA;    	PRIMARY KEY (ac)&#xD;&#xA;    );&#xD;&#xA;    CREATE TABLE AnalysisSample (&#xD;&#xA;        prj_ac TEXT, -- project acccession (Project.ac)&#xD;&#xA;        symbol TEXT, -- a short name unique in the project&#xD;&#xA;    	sample_ac TEXT, -- sample accession (Sample.ac)&#xD;&#xA;        PRIMARY KEY (prj_ac, symbol)&#xD;&#xA;    );&#xD;&#xA;    CREATE TABLE Collection ( -- a BAM file&#xD;&#xA;        ac TEXT, -- collection/alignment file accession&#xD;&#xA;        prj_ac TEXT, -- project accession (Project.ac)&#xD;&#xA;        PRIMARY KEY (ac)&#xD;&#xA;    );&#xD;&#xA;    CREATE TABLE ReadGroup (&#xD;&#xA;        cl_ac TEXT, -- collection accession (Collection.ac)&#xD;&#xA;        rg_id TEXT, -- @RG-ID&#xD;&#xA;        sample_sym TEXT, -- @RG-SM; matching AnalysisSample.symbol&#xD;&#xA;        PRIMARY KEY (cl_ac, rg_id)&#xD;&#xA;    );&#xD;&#xA;    CREATE TABLE VariantSet ( -- a VCF file&#xD;&#xA;    	ac TEXT, -- VCF file accession&#xD;&#xA;    	prj_ac TEXT, -- project accession (Project.ac)&#xD;&#xA;    	PRIMARY KEY (ac)&#xD;&#xA;    );&#xD;&#xA;    CREATE TABLE VariantSample (&#xD;&#xA;    	vs_ac TEXT, -- VCF file accession (VariantSet.ac)&#xD;&#xA;    	sample_sym TEXT, -- sample symbol in the VCF file; matching AnalysisSample.symbol&#xD;&#xA;    	PRIMARY KEY (vs_ac, sample_sym)&#xD;&#xA;    );&#xD;&#xA;&#xD;&#xA;In the schema, you have `Project` and biological `Sample` tables, which are independent of each other at the high level. An `AnalysisSample` describes a sample used in BAM or VCF and connects `Project` and biological `Sample`. Importantly, each `AnalysisSample` has a symbol unique in a project (see the primary index). This is the symbol on a BAM read group line or on a VCF sample line. A `Collection` is in effect a BAM/CRAM file. In theory, a BAM file may contain more than one samples (though rare in practice), which is addressed by a separate `ReadGroup` table. Finally, a `VariantSet` is a VCF file. `VariantSample` tells you which samples are included in each VCF file.&#xD;&#xA;&#xD;&#xA;This is the skeleton of a full schema. You can add extra fields to appropriate tables (e.g. file path and hg19/hg38/etc to `Collection`, read length to `ReadGroup` and family ID to `Sample`). You also need indices for efficient table joining and perhaps more tables for complex structures (e.g. pedigree).&#xD;&#xA;&#xD;&#xA;For the projects I have participated, this schema should work most of time. It is inspired by GA4GH's JSON schema, but my version is in SQL, is simpler and also has a slightly different structure which I think is better." />
  <row Id="631" PostHistoryTypeId="2" PostId="218" RevisionGUID="5d5ccecc-c9f3-42a2-94a7-93435a3f66de" CreationDate="2017-05-22T20:33:56.417" UserId="45" Text="Several papers have made this distinction, and a few indeed use different terms to distinguish between them. For example, [Kazaux et al. (2016)][1] acknowledge that:&#xD;&#xA;&gt; These constraints favour the use of a version of the de Bruijn Graph (dBG) dedicated to genome assembly – a version which differs from the combinatorial structure invented by N.G. de Bruijn.&#xD;&#xA;&#xD;&#xA;[Kingsford et al. (2010)][2] also recognise the distinction:&#xD;&#xA;&gt; Note that this definition of a de Bruijn graph differs from the traditional definition described in the mathematical literature in the 1940s that requires the graph to contain all length-k strings that can be formed from an alphabet (rather than just those strings present in the genome). &#xD;&#xA;&#xD;&#xA;The oldest reference I found for a specific term to refer to the assembly-related structure is [Skiena and Sundaram (1995)][3], where they call it a **subgraph of the de Bruijn digraph**. Later, in 2002, [Błażewicz et al.][4] will refer to it as a **de Bruijn induced subgraph**.  The term **de Bruijn subgraph** is also formally defined in [Quitzau’s thesis (2009)][5]. There, and also in the article ([Quitzau and Stoye, 2008)][6] the authors describe the **sequence graph** as a modification of the sparse de Bruijn subgraph (commonly used in assembly problems), where non-branching paths are replaced by a single vertex. The term **sparse de Bruijn graph** is also used by [Chauve et al. (2013)][7].&#xD;&#xA;&#xD;&#xA;Another term that I found was **word graph**, described by both [Malde et al. (2005)][8] and by [Heath and Pati (2007)][9] as a *subgraph* or as a *generalization* of a de Bruijn graph. [Rødland (2013)][10] summarises some of the terms used for this data structure:&#xD;&#xA;&gt; The data structure is best understood in terms of the de Bruijn subgraph representation of S[k]. (...) Some authors may refer to this as a word graph, or even just a de Bruijn graph.&#xD;&#xA;&#xD;&#xA;Although we can recognise that the distinction is not very relevant, the question is asking specifically for the situation where one wants to make such a distinction. &#xD;&#xA;&#xD;&#xA;[1]: http://www.sciencedirect.com/science/article/pii/S0022000016300502&#xD;&#xA;[2]: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-21&#xD;&#xA;[3]: http://online.liebertpub.com/doi/abs/10.1089/cmb.1995.2.333&#xD;&#xA;[4]: http://www.sciencedirect.com/science/article/pii/S0012365X01001339&#xD;&#xA;[5]: https://pub.uni-bielefeld.de/publication/2302499&#xD;&#xA;[6]: http://link.springer.com/chapter/10.1007/978-3-540-87361-7_29&#xD;&#xA;[7]: https://arxiv.org/abs/1306.4353&#xD;&#xA;[8]: https://academic.oup.com/bioinformatics/article/21/8/1371/249821/A-graph-based-algorithm-for-generating-EST&#xD;&#xA;[9]: http://link.springer.com/chapter/10.1007/978-3-540-72031-7_29&#xD;&#xA;[10]: http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-313&#xD;&#xA;&#xD;&#xA;" />
  <row Id="632" PostHistoryTypeId="2" PostId="219" RevisionGUID="92f35fe4-657f-41e0-84f5-3187531b480d" CreationDate="2017-05-22T20:43:59.303" UserId="45" Text="By chance, just today I've heard of a nanopore read simulator, [NanoSim][1]. It is  [released under a GPL license][2]. I have never used it, though...&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biorxiv.org/content/early/2016/03/18/044545.1&#xD;&#xA;  [2]: https://github.com/bcgsc/NanoSim&#xD;&#xA;" />
  <row Id="633" PostHistoryTypeId="2" PostId="220" RevisionGUID="ca5baf4e-e4b3-4ce6-83e3-0fd330fb79de" CreationDate="2017-05-22T22:13:52.937" UserId="37" Text="An epic question. Unfortunately, the short answer is: no, there are no widely used solutions.&#xD;&#xA;&#xD;&#xA;For several thousand samples, BCF2, the binary representation of VCF, should work well. I don't see the need of new tools at this scale. For a larger sample size, ExAC people are using spark-based hail. It keeps all per-sample annotations (like GL, GQ and DP) in addition to genotypes. Hail is at least something heavily used in practice, although mostly by a few groups so far.&#xD;&#xA;&#xD;&#xA;A simpler problem is to store genotypes only. This is sufficient to the majority of end users. There are better approaches to store and query genotypes. GQT, developed by the Gemini team, enables fast query of samples. It allows you to quickly pull samples under certain genotype configurations. As I remember, GQT is orders of magnitude faster than google genomics API to do PCA. Another tool is BGT. It produces a much smaller file and provides fast and convenient queries over sites. Its paper talks about ~32k whole-genome samples. I am in the camp who believe specialized binary formats like GQT and BGT are faster than solutions built on top of generic databases. I would encourage you to have a look if you only want to query genotypes.&#xD;&#xA;&#xD;&#xA;Intel's GenomicDB approaches the problem in a different angle. It does not actually keep a &quot;squared&quot; multi-sample VCF internally. It instead keeps per-sample genotypes/annotations and generates merged VCF on the fly (this is my understanding, which could be wrong). I don't have first-hand experience with GenomicDB, but I think something in this line should be the ultimate solution in the era of 1M samples. I know GATK4 is using it at some step.&#xD;&#xA;&#xD;&#xA;As to others in your list, Gemini might not scale that well, I guess. It is partly the reason why they work on GQT. Last time I checked, BigQuery did not query individual genotypes. It only queries over site statistics. Google genomics APIs access individual genotypes, but I doubt it can be performant. Adam is worth trying. I have not tried, though." />
  <row Id="634" PostHistoryTypeId="2" PostId="221" RevisionGUID="a07e85ee-29c4-4232-8a6c-cf54dfcda60d" CreationDate="2017-05-22T23:05:06.307" UserId="35" Text="In addition to the already mentioned [NanoSim][1], there is also [SiLiCO][2] and [ReadSim][3] (although it hasn't been updated in over 2 years, so I am not sure how relevant it is at this point considering how fast the technology is progressing).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/bcgsc/NanoSim&#xD;&#xA;  [2]: https://github.com/ethanagbaker/SiLiCO&#xD;&#xA;  [3]: https://sourceforge.net/p/readsim/wiki/Home/" />
  <row Id="635" PostHistoryTypeId="2" PostId="222" RevisionGUID="c1bc754b-56b0-4f38-9935-5cc76a78f47a" CreationDate="2017-05-23T00:10:55.690" UserId="64" Text="There's a brand new tool that appeared today [SNaReSim](http://biorxiv.org/content/early/2017/05/22/133652).&#xD;&#xA;&#xD;&#xA;&gt; Nanopores represent the first commercial technology in decades to&#xD;&#xA;&gt; present a significantly different technique for DNA sequencing, and&#xD;&#xA;&gt; one of the first technologies to propose direct RNA sequencing.&#xD;&#xA;&gt; Despite significant differences with previous sequencing technologies,&#xD;&#xA;&gt; read simulators to date make similar assumptions with respect to error&#xD;&#xA;&gt; profiles and their analysis. This is a great disservice to both&#xD;&#xA;&gt; nanopore sequencing and to algorithm developers who seek to optimize&#xD;&#xA;&gt; their tools to the platform. Previous works have discussed the&#xD;&#xA;&gt; occurrence of some k-mer bias, but this discussion has been focused on&#xD;&#xA;&gt; homopolymers, leaving unanswered the question of whether k-mer bias&#xD;&#xA;&gt; exists over general k-mers, how it occurs, and what can be done to&#xD;&#xA;&gt; reduce the effects. In this work, we demonstrate that current read&#xD;&#xA;&gt; simulators fail to accurately represent k-mer error distributions, We&#xD;&#xA;&gt; explore the sources of k-mer bias in nanopore basecalls, and we&#xD;&#xA;&gt; present a model for predicting k-mers that are difficult to identify.&#xD;&#xA;&gt; We also propose a new SNaReSim, a new state-of-the-art simulator, and&#xD;&#xA;&gt; demonstrate that it provides higher accuracy with respect to 6-mer&#xD;&#xA;&gt; accuracy biases." />
  <row Id="636" PostHistoryTypeId="5" PostId="106" RevisionGUID="219ab416-bc45-4870-af39-396cc4e9dbba" CreationDate="2017-05-23T12:41:55.037" UserId="-1" Comment="replaced http://stackoverflow.com/ with https://stackoverflow.com/" Text="Here is an approach with [`BioPython`][1]. The `with` statement ensures both the input and output file handles are closed and a [*lazy*][2] approach is taken so that only a single fasta record is held in memory at a time, rather than reading the whole file into memory, which is a bad idea for large input files. The solution makes no assumptions about the sequence ID lengths or the number of lines that the sequences are spread across:&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    &#xD;&#xA;    with open('sequences.fasta') as in_f, open('sequences.bed','w') as out_f:&#xD;&#xA;        for record in SeqIO.parse(in_f, 'fasta'):&#xD;&#xA;            out_f.write('{}\t0\t{}\n'.format(record.id, len(record)))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biopython.org/&#xD;&#xA;  [2]: https://stackoverflow.com/questions/20535342/lazy-evaluation-python" />
  <row Id="637" PostHistoryTypeId="5" PostId="190" RevisionGUID="963a2b3c-f4b3-43cd-bcca-567f85b2a4b9" CreationDate="2017-05-23T14:18:02.627" UserId="73" Comment="added polishing explanation" Text="You could also have a go at [Canu][1]. It's designed for long-read assembly (both PacBio and Nanopore), although not specifically for complex population sequencing. It tries to strip a genome down into its unique components, and generates paths from those components that are well-supported from the reads.&#xD;&#xA;&#xD;&#xA;With regards to polishing, it seems to be the case that polishing doesn't converge, and there will be a lot of variants that just oscillate between two possibilities. For me and at least one other person at London Calling this year, there was basically no gain in accuracy for polishing past the third iteration. I used my own error correction algorithm, but they used the more &quot;standard&quot; polishing with Pilon. For what it's worth, the nanopore WGS consortium used Racon for polishing their Canu assemblies.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/marbl/canu/releases" />
  <row Id="638" PostHistoryTypeId="2" PostId="223" RevisionGUID="76d1e3e9-835a-416e-a49d-89d7b8e55591" CreationDate="2017-05-23T18:43:01.447" UserId="33" Text="As a newcomer, what are some influential research papers published in the past few years that I should read to catch up in the field?" />
  <row Id="639" PostHistoryTypeId="1" PostId="223" RevisionGUID="76d1e3e9-835a-416e-a49d-89d7b8e55591" CreationDate="2017-05-23T18:43:01.447" UserId="33" Text="Infulential Papers in Bioinformatics" />
  <row Id="640" PostHistoryTypeId="3" PostId="223" RevisionGUID="76d1e3e9-835a-416e-a49d-89d7b8e55591" CreationDate="2017-05-23T18:43:01.447" UserId="33" Text="&lt;reference&gt;" />
  <row Id="641" PostHistoryTypeId="2" PostId="224" RevisionGUID="6fd74262-457b-45ed-92ce-745f76cafe12" CreationDate="2017-05-23T19:15:08.387" UserId="77" Text="Over on biostars there's a thread like this every year or so. I'll link to [the 2016 edition](https://www.biostars.org/p/229807/) and the (much shorter) [2015 edition](https://www.biostars.org/p/171860/).&#xD;&#xA;&#xD;&#xA;My personal picks from those would be:&#xD;&#xA;&#xD;&#xA; * [ExAC](http://www.nature.com/nature/journal/v536/n7616/full/nature19057.html)&#xD;&#xA; * [salmon](http://biorxiv.org/content/early/2015/06/27/021592), which is now published&#xD;&#xA; * [kallisto](http://arxiv.org/abs/1505.02710), which is also now published" />
  <row Id="642" PostHistoryTypeId="4" PostId="223" RevisionGUID="21db7d28-4043-46a8-a28c-4a3add8c1b3b" CreationDate="2017-05-23T20:31:06.803" UserId="77" Comment="Typo in the title" Text="Influential papers in bioinformatics" />
  <row Id="643" PostHistoryTypeId="10" PostId="223" RevisionGUID="fa8120ad-e4db-4730-b0d3-ea509bbde532" CreationDate="2017-05-23T22:06:43.123" UserId="-1" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:104,&quot;DisplayName&quot;:&quot;Chris_Rands&quot;},{&quot;Id&quot;:35,&quot;DisplayName&quot;:&quot;burger&quot;},{&quot;Id&quot;:349,&quot;DisplayName&quot;:&quot;German Demidov&quot;},{&quot;Id&quot;:57,&quot;DisplayName&quot;:&quot;Kamil S Jaron&quot;},{&quot;Id&quot;:150,&quot;DisplayName&quot;:&quot;neilfws&quot;}]}" />
  <row Id="644" PostHistoryTypeId="2" PostId="225" RevisionGUID="97b13f0e-3b89-4c6f-af82-17a972bb8310" CreationDate="2017-05-24T03:26:50.163" UserId="163" Text="I am using a reference genome for mm10 mouse downloaded from [NCBI][1], and would like to understand in greater detail the difference between lowercase and uppercase letters, which make up roughly equal parts of the genome. I understand that N is used for 'hard masking' (areas in the genome that could not be assembled) and lowercase letters for 'soft masking' in unresolved repeat regions.&#xD;&#xA;&#xD;&#xA;1. What does this soft masking actually mean? &#xD;&#xA;2. How confident can I be about the sequence in these regions?&#xD;&#xA;3. What, then, does a lowercase n represent? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/genome?term=mus%20musculus" />
  <row Id="645" PostHistoryTypeId="1" PostId="225" RevisionGUID="97b13f0e-3b89-4c6f-af82-17a972bb8310" CreationDate="2017-05-24T03:26:50.163" UserId="163" Text="Uppercase vs lowercase letters in reference genome" />
  <row Id="646" PostHistoryTypeId="3" PostId="225" RevisionGUID="97b13f0e-3b89-4c6f-af82-17a972bb8310" CreationDate="2017-05-24T03:26:50.163" UserId="163" Text="&lt;fasta&gt;&lt;genome&gt;" />
  <row Id="647" PostHistoryTypeId="5" PostId="225" RevisionGUID="586dad3d-89f6-449b-91bf-f6011e948612" CreationDate="2017-05-24T03:32:17.953" UserId="163" Comment="no need for sass" Text="I am using a reference genome for mm10 mouse downloaded from [NCBI][1], and would like to understand in greater detail the difference between lowercase and uppercase letters, which make up roughly equal parts of the genome. I understand that N is used for 'hard masking' (areas in the genome that could not be assembled) and lowercase letters for 'soft masking' in unresolved repeat regions.&#xD;&#xA;&#xD;&#xA;1. What does this soft masking actually mean? &#xD;&#xA;2. How confident can I be about the sequence in these regions?&#xD;&#xA;3. What does a lowercase n represent? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/genome?term=mus%20musculus" />
  <row Id="648" PostHistoryTypeId="2" PostId="226" RevisionGUID="f1ffdd96-25b9-44c5-a0bc-b318aaabe259" CreationDate="2017-05-24T03:36:03.760" UserId="73" Text="The best nanopore read simulators would be associated with the best base-callers. For a base-caller to effectively model the DNA strand, it needs to take into account the expected underlying electrical model together with the associated signal noise (both in the time dimension as well as the amplitude dimension). In theory, it should be possible to reverse the algorithm and generate an electrical signal given an underlying sequence.&#xD;&#xA;&#xD;&#xA;Unfortunately, I'm not aware of any tools that attempt to simulate nanopore reads at the electrical level. Any &quot;nanopore read simulator&quot; that concentrates only on base sequence would need to encompass all the possible base-calling software models that exist, which is an impossible task (particularly given how quickly ONT updates their own base callers)." />
  <row Id="651" PostHistoryTypeId="2" PostId="227" RevisionGUID="d3fddf5d-738d-47c5-8351-adb87d9f1683" CreationDate="2017-05-24T06:01:32.763" UserId="161" Text="&gt; What does this soft masking actually mean?&#xD;&#xA;&#xD;&#xA;A lot of the sequence in genomes are repetitive. Human genome, for example, has (at least) two-third repetitive elements.[1]. &#xD;&#xA;&#xD;&#xA;These repetitive elements are soft-masked by converting the upper case letters to lower case. An important use-case of these soft-masked bases will be in homology searches: An `atatatatatat` will tend to appear both in human and mouse genomes but is likely non-homologous.&#xD;&#xA;&#xD;&#xA;&gt; How confident can I be about the sequence in these regions?&#xD;&#xA;&#xD;&#xA;As you can be about in non soft-masked based positions. Soft-masking is done after determining portions in the genome that are likely repetitive. There is no uncertainty whether a particular base is 'A' or 'G', just that it is part of a repeat and hence should be represented as an 'a'.&#xD;&#xA;&#xD;&#xA;&gt;What does a lowercase n represent?&#xD;&#xA;&#xD;&#xA;UCSC uses [Tandom Repeat Finder](https://tandem.bu.edu/trf/trfdesc.html) and [RepeatMasker](http://www.repeatmasker.org/) for soft-masking potential repeats. NCBI most likely uses [TANTAN](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3045581/). 'N's represents no sequence information is available for that base. It being replaced by 'n' is likely an artifact of the repeat-masking software where it soft-masks an 'N' by an 'n' to indicate that portion of the genome is likely a repeat too.&#xD;&#xA;&#xD;&#xA;[1] http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1002384" />
  <row Id="652" PostHistoryTypeId="2" PostId="228" RevisionGUID="005e111e-c066-4af6-b076-6c31970b5cc6" CreationDate="2017-05-24T06:41:10.320" UserId="88" Text="I have sequenced numerous multiplexed pools of BS amplicon-seq libraries derived from human samples on a MiSeq over the past few weeks. I have been utilising trim-galore and Bismark for alignment and am finding the mapping efficiency to be really low for two pools (55% &amp; 30% respectively) both of which had a cytosine per base sequence of around 10-20% throughout the entire read in their fastqc files.&#xD;&#xA;&#xD;&#xA;The high C content visible in the fastqc files makes me think the poor mapping efficiency is due to poor bisulfite conversion, as this would be expected to be close to zero is bisulfite conversion had actually took place.&#xD;&#xA;&#xD;&#xA;I am new to the field so any help would be greatly appreciated.&#xD;&#xA;" />
  <row Id="653" PostHistoryTypeId="1" PostId="228" RevisionGUID="005e111e-c066-4af6-b076-6c31970b5cc6" CreationDate="2017-05-24T06:41:10.320" UserId="88" Text="The effects of incomplete bisulfite conversion upon mapping efficiency" />
  <row Id="654" PostHistoryTypeId="3" PostId="228" RevisionGUID="005e111e-c066-4af6-b076-6c31970b5cc6" CreationDate="2017-05-24T06:41:10.320" UserId="88" Text="&lt;alignment&gt;&lt;fastq&gt;" />
  <row Id="655" PostHistoryTypeId="2" PostId="229" RevisionGUID="6dcecb04-cc92-4147-aa39-f36b40c8acd8" CreationDate="2017-05-24T06:46:36.547" UserId="77" Text="Bisulfite conversion efficiency has no effect on the mapping rate in bismark and similar tools. The reason is that the reads are fully bisulfite converted *in silico* before alignment to minimize mapping bias.&#xD;&#xA;&#xD;&#xA;I would suggest that you play around with the settings handed to bowtie2, such as using local alignment and modifying the `--score-min` option to allow more mismatches. Alternatively, you might try a different aligner like [bwameth](https://github.com/brentp/bwa-meth), which will always do local alignment." />
  <row Id="656" PostHistoryTypeId="2" PostId="230" RevisionGUID="97ada7fa-f417-497c-a5da-848071653585" CreationDate="2017-05-24T07:35:31.097" UserId="48" Text="As enrichment analysis a usual step is to infer the pathways enriched in a list of genes. However I can't find a discussion about which database is better. Two of the most popular (in my particular environment) are Reactome and KEGG (Maybe because there are tools using them in Bioconductor).&#xD;&#xA;KEGG requires a subscription for ftp access, and for my research I would need to download huge amounts of KGML files I am now leaning towards Reactome&#xD;&#xA;&#xD;&#xA;Which is the one with more genes associated to pathways ? &#xD;&#xA;Which is more completely annotated ?&#xD;&#xA;Is there any paper comparing them ? " />
  <row Id="657" PostHistoryTypeId="1" PostId="230" RevisionGUID="97ada7fa-f417-497c-a5da-848071653585" CreationDate="2017-05-24T07:35:31.097" UserId="48" Text="Which pathway database choose KEGG or Reactome?" />
  <row Id="658" PostHistoryTypeId="3" PostId="230" RevisionGUID="97ada7fa-f417-497c-a5da-848071653585" CreationDate="2017-05-24T07:35:31.097" UserId="48" Text="&lt;database&gt;" />
  <row Id="661" PostHistoryTypeId="2" PostId="231" RevisionGUID="644f79fe-0542-41a3-bdfa-717721546d3e" CreationDate="2017-05-24T08:16:19.220" UserId="104" Text="The use of lower/upper case letters and `N`/`n` letters in genomes sequences is not completely standardised and you should always check the specification of the resource you are using.&#xD;&#xA;&#xD;&#xA;Lower case letters are most commonly used to represent “soft-masked sequences”, a convention popularised by [RepeatMasker][1], where interspersed repeats (which covers transposons, retrotransposons and processed pseudogenes) and low complexity sequences are marked with lower case letters. Note that larger repeats, such as sizable tandem repeats, segmental duplications, and whole gene duplications are not generally masked.&#xD;&#xA;&#xD;&#xA;However, there are other uses for lower/upper case letters, for example, [Ensembl have used][2] upper/lower case letters to represent exonic and intronic sequences respectively.&#xD;&#xA;&#xD;&#xA;`N` and `n` nucleotides may represent “hard masked sequences”, where interspersed repeats and low complexity sequences are replaced by `N`s. But `N`/`n`s may alternatively represent ambiguous nucleotides, indeed this is the [IUPAC][3] specification.&#xD;&#xA;&#xD;&#xA;Also note occasionally (although fortunately rarely) `X`/`x` is used to represent ambiguous nucleotides or “hard-masked sequences” too.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.repeatmasker.org/&#xD;&#xA;  [2]: http://www.ensembl.org/Help/View?id=155&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/Nucleic_acid_notation#IUPAC_notation" />
  <row Id="662" PostHistoryTypeId="2" PostId="232" RevisionGUID="b229bd84-53be-470e-9b32-73841aaf5dbc" CreationDate="2017-05-24T09:32:26.523" UserId="29" Text="I’d normally use `collapseReplicates` (or do the collapsing upstream) to handle technical replicates.&#xD;&#xA;&#xD;&#xA;However, in my current RNA-seq experimental design, samples were sequenced twice using different library preparation protocols, leading to marked differences in the resulting count estimates (in fact, the choice of different protocol explains most of the variance in the bulk data according to PCA). I would therefore like to model these differences by adding them as a covariate to the DESeq2 model.&#xD;&#xA;&#xD;&#xA;I *thought* I could just add another factor to the design formula, equivalent to [the “pasilla” example in the DESeq2 vignette](https://www.bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#multi-factor-designs), which adds the factor `type` for the library type (single-end vs paired-end). However, the [pasilla vignette](http://bioconductor.org/packages/release/data/experiment/vignettes/pasilla/inst/doc/create_objects.html) states that these different libraries are actually independent *biological* replicates, not technical replicates as I had previously assumed.&#xD;&#xA;&#xD;&#xA;I’m now concerned that having technical replicates in the design might artificially inflate the power. Can the design be salvaged?" />
  <row Id="663" PostHistoryTypeId="1" PostId="232" RevisionGUID="b229bd84-53be-470e-9b32-73841aaf5dbc" CreationDate="2017-05-24T09:32:26.523" UserId="29" Text="Can I model technical replicates in DESeq2?" />
  <row Id="664" PostHistoryTypeId="3" PostId="232" RevisionGUID="b229bd84-53be-470e-9b32-73841aaf5dbc" CreationDate="2017-05-24T09:32:26.523" UserId="29" Text="&lt;rna-seq&gt;&lt;deseq2&gt;&lt;technical-replicates&gt;" />
  <row Id="665" PostHistoryTypeId="5" PostId="167" RevisionGUID="a57f5438-903b-4d40-8ca7-fac02afb7c80" CreationDate="2017-05-24T10:01:42.090" UserId="29" Comment="formatting improvements" Text="The key function is [`call_broadpeaks`](https://github.com/taoliu/MACS/blob/master/MACS2/IO/CallPeakUnit.pyx#L1443):&#xD;&#xA;&#xD;&#xA;The description attached to the function says:&#xD;&#xA;&#xD;&#xA;&gt; This function try to find enriched regions within which, scores are&#xD;&#xA;&gt; continuously higher than a given cutoff for level 1, and link them&#xD;&#xA;&gt; using the gap above level 2 cutoff with a maximum length of&#xD;&#xA;&gt; lvl2_max_gap.&#xD;&#xA;&#xD;&#xA;&gt; scoring_function_s: symbols of functions to calculate&#xD;&#xA;&gt; score. 'p' for pscore, 'q' for qscore, 'f' for fold change, 's' for&#xD;&#xA;&gt; subtraction. for example: ['p', 'q']&#xD;&#xA;&#xD;&#xA;&gt; lvl1_cutoff_s:  list of cutoffs&#xD;&#xA;&gt; at highly enriched regions, corresponding to scoring functions.&#xD;&#xA;&#xD;&#xA;&gt; lvl2_cutoff_s:  list of cutoffs at less enriched regions,&#xD;&#xA;&gt; corresponding to scoring functions.&#xD;&#xA;&#xD;&#xA;&gt; min_length :minimum peak length,&#xD;&#xA;&gt; default 200.&#xD;&#xA;&#xD;&#xA;&gt; lvl1_max_gap   :  maximum gap to merge nearby enriched&#xD;&#xA;&gt; peaks, default 50.&#xD;&#xA;&#xD;&#xA;&gt; lvl2_max_gap   :  maximum length of linkage&#xD;&#xA;&gt; regions, default 400.&#xD;&#xA;&#xD;&#xA;&gt; Return both general PeakIO object for highly&#xD;&#xA;&gt; enriched regions and gapped broad regions in BroadPeakIO.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;To give some basic explanation, the algorithm (briefly) appears to be as follows:&#xD;&#xA;&#xD;&#xA;1. Two separate levels of peaks are called, level 1 (a higher pval, ie more significant) and level 2 (a lower pval). Level 1 is controlled by `-p` and level 2 is controlled by `--broad-cutoff`. When each peakset is called, they are immediately linked by the max gap parameter for each set.&#xD;&#xA;&#xD;&#xA;2. Then, assuming that all level 1 peaks should be inside level 2 peaks (this is an explicit assumption by MACS2), the algorithm groups level 1 peaks inside level 2 peaks to output a broad peak.&#xD;&#xA;&#xD;&#xA;...&#xD;&#xA;&#xD;&#xA;This has a few implications:&#xD;&#xA;&#xD;&#xA;1. The broad peak calls really come from the level 2 peaks alone (+ linking). The level 1 peak calls allow you to distinguish sub peaks (so that you can have gapped peaks).&#xD;&#xA;&#xD;&#xA;2. Aside from the linking, the broad peak calls would be the same as narrow peak calls, if you called both with the same pval threshold (for example, if you set `--broad-cutoff 0.1` in broad peak mode, and the `-p 0.1` for narrow peak mode)&#xD;&#xA;" />
  <row Id="666" PostHistoryTypeId="24" PostId="167" RevisionGUID="a57f5438-903b-4d40-8ca7-fac02afb7c80" CreationDate="2017-05-24T10:01:42.090" Comment="Proposed by 29 approved by 57, 77 edit id of 69" />
  <row Id="667" PostHistoryTypeId="5" PostId="232" RevisionGUID="da7582d1-6925-4505-b6ac-2f9d6c737d10" CreationDate="2017-05-24T10:02:03.190" UserId="29" Comment="added 170 characters in body" Text="I’d normally use `collapseReplicates` (or do the collapsing upstream) to handle technical replicates.&#xD;&#xA;&#xD;&#xA;However, in my current RNA-seq experimental design, samples were sequenced twice using different library preparation protocols, leading to marked differences in the resulting count estimates (in fact, the choice of different protocol explains most of the variance in the bulk data according to PCA). I would therefore like to model these differences by adding them as a covariate to the DESeq2 model.&#xD;&#xA;&#xD;&#xA;I *thought* I could just add another factor to the design formula, equivalent to [the “pasilla” example in the DESeq2 vignette](https://www.bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#multi-factor-designs), which adds the factor `type` for the library type (single-end vs paired-end). However, the [pasilla vignette](http://bioconductor.org/packages/release/data/experiment/vignettes/pasilla/inst/doc/create_objects.html) states that these different libraries are actually independent *biological* replicates, not technical replicates as I had previously assumed.&#xD;&#xA;&#xD;&#xA;I’m now concerned that having technical replicates in the design might artificially inflate the power. Can the design be salvaged?&#xD;&#xA;&#xD;&#xA;It’s worth noting that we *also* have biological replicates, and a full rank design; i.e. every combination of treatment and library prep, with biological replicates." />
  <row Id="668" PostHistoryTypeId="4" PostId="230" RevisionGUID="4052b583-53b8-4bc3-9a39-d94744f23762" CreationDate="2017-05-24T10:16:12.237" UserId="48" Comment="edited title" Text="What are the advantages and disadvantages between using KEGG or Reactome?" />
  <row Id="669" PostHistoryTypeId="2" PostId="233" RevisionGUID="cd0ba22b-f63b-4fb2-8695-b59678c788a0" CreationDate="2017-05-24T10:17:43.520" UserId="29" Text="It depends what you mean by “normalised”. As Devon said, the `normalized = TRUE` argument to the `count` function gives you normalised counts. However, these are “only” library-size normalised (i.e. divided by the `sizeFactors(dds)`).&#xD;&#xA;&#xD;&#xA;However, [as the vignette explains](https://www.bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#data-transformations-and-visualization), downstream processing generally requires more advanced normalisation, to account for the [heteroscedasticity](https://en.wikipedia.org/wiki/Heteroscedasticity) of the counts. This is often done by simply `log`ging the counts but this has obvious drawbacks (most trivially, what do we do with 0 counts? A workaround is to add a pseudocount but that’s problematic too).&#xD;&#xA;&#xD;&#xA;DESeq2 offers two different methods to perform a more rigorous analysis:&#xD;&#xA;&#xD;&#xA;* `rlog` — a *regularised log*, and&#xD;&#xA;* `vst` — a *variance stabilising transformation*.&#xD;&#xA;&#xD;&#xA;You’d generally use either of these for downstream analysis, not `count(dds, normalized = TRUE)`." />
  <row Id="670" PostHistoryTypeId="2" PostId="234" RevisionGUID="5245fa2a-2678-4c90-bb28-095700f66793" CreationDate="2017-05-24T10:20:25.597" UserId="77" Text="Practically speaking, there's no way to include the technical replicates in that design (in DESeq2 at least). Your concern regarding inflating the power is exactly correct and the only way to combat that would be to add a pairing factor like one might do with case-control or tumor-normal studies. That is, something like:&#xD;&#xA;&#xD;&#xA;      group libraryPrep sample&#xD;&#xA;    1    WT           A      1&#xD;&#xA;    2    WT           B      1&#xD;&#xA;    3   MUT           A      2&#xD;&#xA;    4   MUT           B      2&#xD;&#xA;    5    WT           A      3&#xD;&#xA;    6    WT           B      3&#xD;&#xA;    7   MUT           A      4&#xD;&#xA;    8   MUT           B      4&#xD;&#xA;&#xD;&#xA;Here `sample` pairs the technical replicates together. However this ends up either being rank deficient or in the end not more informative." />
  <row Id="671" PostHistoryTypeId="2" PostId="235" RevisionGUID="e49e0afe-4ec9-4b11-8e24-c1f009ad7746" CreationDate="2017-05-24T11:09:19.483" UserId="222" Text="If they're truly technical replicates, then there's no way to model them using DESeq2, as you've alluded to with the `collapseReplicates` function. DESeq2/ Mike Love's general recommendation with `collapseReplicates` is to just add the reads together for technical replicates. &#xD;&#xA;&#xD;&#xA;If you want to model them instead of collapsing them down, you can `voom` transform your data and follow an example similar to section 11.3 in the [Limma users guide][1].&#xD;&#xA;&#xD;&#xA;**(Code example from 11.3):**&#xD;&#xA;&#xD;&#xA;*Design*&#xD;&#xA;&#xD;&#xA;    | FileName  | Cy3   | Cy5  |&#xD;&#xA;    | --------- |:-----:| ----:|&#xD;&#xA;    | File1     | wt1   | mu1  |&#xD;&#xA;    | File2     | wt1   | mu1  |&#xD;&#xA;    | File1     | wt2   | mu2  |&#xD;&#xA;    | File2     | wt2   | mu2  |&#xD;&#xA;&#xD;&#xA;*Example*&#xD;&#xA;&#xD;&#xA;    &gt; biolrep &lt;- c(1, 1, 2, 2)&#xD;&#xA;    &gt; corfit &lt;- duplicateCorrelation(MA, ndups = 1, block = biolrep)&#xD;&#xA;    &gt; fit &lt;- lmFit(MA, block = biolrep, cor = corfit$consensus)&#xD;&#xA;    &gt; fit &lt;- eBayes(fit)&#xD;&#xA;    &gt; topTable(fit, adjust = &quot;BH&quot;)&#xD;&#xA;&#xD;&#xA;  [1]: https://www.bioconductor.org/packages/devel/bioc/vignettes/limma/inst/doc/usersguide.pdf" />
  <row Id="672" PostHistoryTypeId="5" PostId="235" RevisionGUID="71d1c212-d8fc-4107-b611-2d2a99a3d6dd" CreationDate="2017-05-24T11:16:35.620" UserId="222" Comment="Filename mistake" Text="If they're truly technical replicates, then there's no way to model them using DESeq2, as you've alluded to with the `collapseReplicates` function. DESeq2/ Mike Love's general recommendation with `collapseReplicates` is to just add the reads together for technical replicates. &#xD;&#xA;&#xD;&#xA;If you want to model them instead of collapsing them down, you can `voom` transform your data and follow an example similar to section 11.3 in the [Limma users guide][1].&#xD;&#xA;&#xD;&#xA;**(Code example from 11.3):**&#xD;&#xA;&#xD;&#xA;*Design*&#xD;&#xA;&#xD;&#xA;    | FileName  | Cy3   | Cy5  |&#xD;&#xA;    | --------- |:-----:| ----:|&#xD;&#xA;    | File1     | wt1   | mu1  |&#xD;&#xA;    | File2     | wt1   | mu1  |&#xD;&#xA;    | File3     | wt2   | mu2  |&#xD;&#xA;    | File4     | wt2   | mu2  |&#xD;&#xA;&#xD;&#xA;*Example*&#xD;&#xA;&#xD;&#xA;    &gt; biolrep &lt;- c(1, 1, 2, 2)&#xD;&#xA;    &gt; corfit &lt;- duplicateCorrelation(MA, ndups = 1, block = biolrep)&#xD;&#xA;    &gt; fit &lt;- lmFit(MA, block = biolrep, cor = corfit$consensus)&#xD;&#xA;    &gt; fit &lt;- eBayes(fit)&#xD;&#xA;    &gt; topTable(fit, adjust = &quot;BH&quot;)&#xD;&#xA;&#xD;&#xA;  [1]: https://www.bioconductor.org/packages/devel/bioc/vignettes/limma/inst/doc/usersguide.pdf" />
  <row Id="674" PostHistoryTypeId="5" PostId="225" RevisionGUID="e269b6c8-41b7-4b5a-a9f1-4cf5b750e9fe" CreationDate="2017-05-24T11:56:09.067" UserId="163" Comment="repeat regions aren't necessarily unresolved" Text="I am using a reference genome for mm10 mouse downloaded from [NCBI][1], and would like to understand in greater detail the difference between lowercase and uppercase letters, which make up roughly equal parts of the genome. I understand that N is used for 'hard masking' (areas in the genome that could not be assembled) and lowercase letters for 'soft masking' in repeat regions.&#xD;&#xA;&#xD;&#xA;1. What does this soft masking actually mean? &#xD;&#xA;2. How confident can I be about the sequence in these regions?&#xD;&#xA;3. What does a lowercase n represent? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/genome?term=mus%20musculus" />
  <row Id="676" PostHistoryTypeId="6" PostId="159" RevisionGUID="976ad714-b0f1-475a-8a54-930024563da6" CreationDate="2017-05-24T12:13:12.043" UserId="57" Comment="changed tag &quot;structural&quot; to &quot;protein-structure&quot; and added &quot;proteins&quot; tag" Text="&lt;proteins&gt;&lt;homology-modelling&gt;&lt;protein-structure&gt;" />
  <row Id="677" PostHistoryTypeId="24" PostId="159" RevisionGUID="976ad714-b0f1-475a-8a54-930024563da6" CreationDate="2017-05-24T12:13:12.043" Comment="Proposed by 57 approved by 77, 163 edit id of 41" />
  <row Id="680" PostHistoryTypeId="2" PostId="236" RevisionGUID="842f17f9-58e6-4db3-9313-5a486a22c6fc" CreationDate="2017-05-24T12:29:31.210" UserId="57" Text="Lower case nucleotides commonly denotes a **soft** masked sequences. How exactly the genome was masked you can find in FAQ of [NCBI][1]:&#xD;&#xA;&#xD;&#xA;&gt; **Are repetitive sequences in eukaryotic genomes masked?**&#xD;&#xA;&gt; &#xD;&#xA;&gt; Repetitive sequences in eukaryotic genome assembly sequence files, as&#xD;&#xA;&gt; identified by [WindowMasker][2], have been masked to lower-case.&#xD;&#xA;&gt; &#xD;&#xA;&gt; The location and identity of repeats found by [RepeatMasker][3] are also&#xD;&#xA;&gt; provided in a separate file. These spans could be used to mask the&#xD;&#xA;&gt; genomic sequences if desired. Be aware, however, that many less&#xD;&#xA;&gt; studied organisms do not have good repeat libraries available for&#xD;&#xA;&gt; RepeatMasker to use.&#xD;&#xA;&#xD;&#xA;**An example of usage of the soft mask**&#xD;&#xA;&#xD;&#xA;Mapping of sequence to reference usually starts with perfect matches of seeds (substrings) of the mapped reads and the reference sequence. Soft masked (low complexity) regions are not used for matches of seeds, but they are used only for the extension of the alignment if there was a seed in a neighbouring region. This application of softmasking applied to problem of long read assembly is described on this [blog][4].&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/genome/doc/ftpfaq/#masking&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/pubmed/16287941&#xD;&#xA;  [3]: http://www.repeatmasker.org/&#xD;&#xA;  [4]: https://dazzlerblog.wordpress.com/2016/04/01/detecting-and-soft-masking-repeats/" />
  <row Id="681" PostHistoryTypeId="5" PostId="236" RevisionGUID="eeab6ccb-0ff5-421f-b324-b4f431890466" CreationDate="2017-05-24T14:24:15.477" UserId="57" Comment="added an opinionated answer to question 2; and no answer to 3" Text="1. Lower case nucleotides commonly denotes a **soft** masked sequences. How exactly the genome was masked you can find in FAQ of [NCBI][1]:&#xD;&#xA;&#xD;&#xA;&gt; **Are repetitive sequences in eukaryotic genomes masked?**&#xD;&#xA;&gt; &#xD;&#xA;&gt; Repetitive sequences in eukaryotic genome assembly sequence files, as&#xD;&#xA;&gt; identified by [WindowMasker][2], have been masked to lower-case.&#xD;&#xA;&gt; &#xD;&#xA;&gt; The location and identity of repeats found by [RepeatMasker][3] are also&#xD;&#xA;&gt; provided in a separate file. These spans could be used to mask the&#xD;&#xA;&gt; genomic sequences if desired. Be aware, however, that many less&#xD;&#xA;&gt; studied organisms do not have good repeat libraries available for&#xD;&#xA;&gt; RepeatMasker to use.&#xD;&#xA;&#xD;&#xA;2. IMHO, low complexity regions are always more likely to be missassembled than high complexity sequences. However, this will be problem for non-model organisms. I would guess that the reliability of the softmasked regions of mouse genome will very high.&#xD;&#xA;&#xD;&#xA;3. No idea, looks like an artefact.&#xD;&#xA;&#xD;&#xA;**An example of usage of the soft mask**&#xD;&#xA;&#xD;&#xA;Mapping of sequence to reference usually starts with perfect matches of seeds (substrings) of the mapped reads and the reference sequence. Soft masked (low complexity) regions are not used for matches of seeds, but they are used only for the extension of the alignment if there was a seed in a neighbouring region. This application of softmasking applied to problem of long read assembly is described on this [blog][4].&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/genome/doc/ftpfaq/#masking&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/pubmed/16287941&#xD;&#xA;  [3]: http://www.repeatmasker.org/&#xD;&#xA;  [4]: https://dazzlerblog.wordpress.com/2016/04/01/detecting-and-soft-masking-repeats/" />
  <row Id="682" PostHistoryTypeId="2" PostId="237" RevisionGUID="a04ab726-985f-408a-93af-c5f035799c9d" CreationDate="2017-05-24T15:37:17.157" UserId="180" Text="I have a bam file of alignments to a small synthetic set of sequences, and I am heavily filtering reads that are not paired and of a certain orientation. When I apply the filtering, samtools mpileup returns empty:&#xD;&#xA;&#xD;&#xA;    samtools view -h myfile.bam | \&#xD;&#xA;    samtools view -h -F4 - | \&#xD;&#xA;    samtools view -h -F8 - | \&#xD;&#xA;    samtools view -h -F256 - | \&#xD;&#xA;    samtools view -h -F512 - | \&#xD;&#xA;    samtools view -h -F1024 - | \&#xD;&#xA;    samtools view -h -F2048 - | \&#xD;&#xA;    samtools view -h -f16 - | \&#xD;&#xA;    samtools view -h -f32 -  | \&#xD;&#xA;    samtools mpileup --excl-flags 0 -Q0 -B -d 999999 - | wc -l&#xD;&#xA;&#xD;&#xA;Returns 0.&#xD;&#xA;&#xD;&#xA;I tried different combinations of filtering, and when I do both `-f 16` and `-f 32` returns empty, but if I do either of those, then it works:&#xD;&#xA;&#xD;&#xA;    samtools view -h myfile.bam | \&#xD;&#xA;    samtools view -h -F4 - | \&#xD;&#xA;    samtools view -h -F8 - | \&#xD;&#xA;    samtools view -h -F256 - | \&#xD;&#xA;    samtools view -h -F512 - | \&#xD;&#xA;    samtools view -h -F1024 - | \&#xD;&#xA;    samtools view -h -F2048 - | \&#xD;&#xA;    samtools view -h -f16 - | \&#xD;&#xA;    samtools mpileup --excl-flags 0 -Q0 -B -d 999999 - | wc -l&#xD;&#xA;&#xD;&#xA;Returns 1056&#xD;&#xA;&#xD;&#xA;Any ideas why? My thinking was that it would work with `--excl-flags 0`.&#xD;&#xA;&#xD;&#xA;Thanks in advance&#xD;&#xA;" />
  <row Id="683" PostHistoryTypeId="1" PostId="237" RevisionGUID="a04ab726-985f-408a-93af-c5f035799c9d" CreationDate="2017-05-24T15:37:17.157" UserId="180" Text="samtools mpileup empty when filtering out flags" />
  <row Id="684" PostHistoryTypeId="3" PostId="237" RevisionGUID="a04ab726-985f-408a-93af-c5f035799c9d" CreationDate="2017-05-24T15:37:17.157" UserId="180" Text="&lt;bam&gt;&lt;samtools&gt;&lt;mpileup&gt;" />
  <row Id="685" PostHistoryTypeId="2" PostId="238" RevisionGUID="2cfd6682-bcbe-4f5f-b991-bc67cba9201d" CreationDate="2017-05-24T15:41:28.400" UserId="64" Text="I have a few sets of marker genes that I can classify RNA-seq samples using semi-supervised clustering.  I would like to automate the process, however, I am struggling to find the ideal algorithm that could generate some kind of score for marker gene set from a given sample.  &#xD;&#xA;&#xD;&#xA;I presume that this is a standard analyses in many groups but I am not sure which method(s) are yielding good results in practice." />
  <row Id="686" PostHistoryTypeId="1" PostId="238" RevisionGUID="2cfd6682-bcbe-4f5f-b991-bc67cba9201d" CreationDate="2017-05-24T15:41:28.400" UserId="64" Text="Classifying samples based on marker gene expression" />
  <row Id="687" PostHistoryTypeId="3" PostId="238" RevisionGUID="2cfd6682-bcbe-4f5f-b991-bc67cba9201d" CreationDate="2017-05-24T15:41:28.400" UserId="64" Text="&lt;rna-seq&gt;&lt;classification&gt;" />
  <row Id="688" PostHistoryTypeId="5" PostId="237" RevisionGUID="f7f62965-598e-4131-a253-5d3c265817db" CreationDate="2017-05-24T16:58:14.030" UserId="180" Comment="added explanation on how it was aligned" Text="I produced a bam file by aligning reads to a small set of synthetic sequences using bwa-mem.&#xD;&#xA;&#xD;&#xA;I am heavily filtering reads that are not paired and of a certain orientation.&#xD;&#xA;Applying the filtering, I get a few thousands of reads:&#xD;&#xA;&#xD;&#xA;    samtools view -h $myfilebam | \&#xD;&#xA;    samtools view -h -F4 - | \&#xD;&#xA;    samtools view -h -F8 - | \&#xD;&#xA;    samtools view -h -F256 - | \&#xD;&#xA;    samtools view -h -F512 - | \&#xD;&#xA;    samtools view -h -F1024 - | \&#xD;&#xA;    samtools view -h -F2048 - | \&#xD;&#xA;    samtools view -h -f16 - | \&#xD;&#xA;    samtools view -h -f32 -  | wc -l&#xD;&#xA;&#xD;&#xA;Gives me `89502` reads.&#xD;&#xA;&#xD;&#xA;If I then pipe this into `samtools mpileup`, I get no results:&#xD;&#xA;&#xD;&#xA;    samtools view -h $myfilebam | \&#xD;&#xA;    samtools view -h -F4 - | \&#xD;&#xA;    samtools view -h -F8 - | \&#xD;&#xA;    samtools view -h -F256 - | \&#xD;&#xA;    samtools view -h -F512 - | \&#xD;&#xA;    samtools view -h -F1024 - | \&#xD;&#xA;    samtools view -h -F2048 - | \&#xD;&#xA;    samtools view -h -f16 - | \&#xD;&#xA;    samtools view -h -f32 -  | \&#xD;&#xA;    samtools mpileup --excl-flags 0 -Q0 -B -d 999999 - | wc -l&#xD;&#xA;&#xD;&#xA;Returns 0.&#xD;&#xA;&#xD;&#xA;I tried different combinations of filtering, and when I do both `-f 16` and `-f 32` returns empty, but if I do either of those, then it works:&#xD;&#xA;&#xD;&#xA;    samtools view -h $myfilebam | \&#xD;&#xA;    samtools view -h -F4 - | \&#xD;&#xA;    samtools view -h -F8 - | \&#xD;&#xA;    samtools view -h -F256 - | \&#xD;&#xA;    samtools view -h -F512 - | \&#xD;&#xA;    samtools view -h -F1024 - | \&#xD;&#xA;    samtools view -h -F2048 - | \&#xD;&#xA;    samtools view -h -f16 - | \&#xD;&#xA;    samtools mpileup --excl-flags 0 -Q0 -B -d 999999 - | wc -l&#xD;&#xA;&#xD;&#xA;Returns `1056`.&#xD;&#xA;&#xD;&#xA;Any ideas why? My thinking was that it would work with `--excl-flags 0`.&#xD;&#xA;&#xD;&#xA;Thanks in advance&#xD;&#xA;" />
  <row Id="690" PostHistoryTypeId="5" PostId="163" RevisionGUID="aed894cc-362d-4385-bb4c-347e2e159d93" CreationDate="2017-05-24T18:17:03.157" UserId="138" Comment="blmoore correctly points out that I was only linking first half of Phyre protocol schematic, so I included his link in my answer also." Text="I'm less familiar with Phyre, but I-TASSER is a really sophisticated system that takes the results of a search using multiple threaders and plugs them into an ab initio simulation which tries to minimize the energy of the models by sampling many possible 3D conformations, which I don't think Phyre does.&#xD;&#xA;&#xD;&#xA;https://en.wikipedia.org/wiki/I-TASSER#/media/File:I-TASSER-pipeline.jpg&#xD;&#xA;&#xD;&#xA;Compare with a similar workflow schematic for Phyre:&#xD;&#xA;&#xD;&#xA;http://www.nature.com/nprot/journal/v10/n6/images/nprot.2015.053-F1.jpg&#xD;&#xA;http://www.nature.com/nprot/journal/v10/n6/fig_tab/nprot.2015.053_F2.html&#xD;&#xA;&#xD;&#xA;Structure prediction still has a long way to go, and you'll always get better results if there are close homologues available in the PDB, but given the consistent high performance of I-TASSER in CASP I would treat those results as more significant. That said, it can't hurt to consider multiple answers.&#xD;&#xA;&#xD;&#xA;Edit: included blmoore's link to second half of Phyre protocol schematic" />
  <row Id="692" PostHistoryTypeId="6" PostId="202" RevisionGUID="1ed209f9-cc6c-4616-81db-a9aff652ee63" CreationDate="2017-05-24T18:21:51.993" UserId="93" Comment="add Software Recommendation tag" Text="&lt;genome-sequencing&gt;&lt;oxford-nanopore&gt;&lt;3rd-gen-sequencing&gt;&lt;software-recommendation&gt;" />
  <row Id="693" PostHistoryTypeId="24" PostId="202" RevisionGUID="1ed209f9-cc6c-4616-81db-a9aff652ee63" CreationDate="2017-05-24T18:21:51.993" Comment="Proposed by 93 approved by 57, 77 edit id of 70" />
  <row Id="694" PostHistoryTypeId="5" PostId="237" RevisionGUID="83426c3d-692c-477c-af68-382b64b454ad" CreationDate="2017-05-24T20:53:49.517" UserId="180" Comment="added 111 characters in body" Text="I produced a bam file by aligning reads to a small set of synthetic sequences using bwa-mem.&#xD;&#xA;&#xD;&#xA;I am heavily filtering reads that are not paired and of a certain orientation.&#xD;&#xA;Applying the filtering, I get a few thousands of reads:&#xD;&#xA;&#xD;&#xA;    samtools view -h $myfilebam | \&#xD;&#xA;    samtools view -h -F4 - | \&#xD;&#xA;    samtools view -h -F8 - | \&#xD;&#xA;    samtools view -h -F256 - | \&#xD;&#xA;    samtools view -h -F512 - | \&#xD;&#xA;    samtools view -h -F1024 - | \&#xD;&#xA;    samtools view -h -F2048 - | \&#xD;&#xA;    samtools view -h -f16 - | \&#xD;&#xA;    samtools view -h -f32 -  | wc -l&#xD;&#xA;&#xD;&#xA;Gives me `89502` reads.&#xD;&#xA;&#xD;&#xA;If I then pipe this into `samtools mpileup`, I get no results:&#xD;&#xA;&#xD;&#xA;    samtools view -h $myfilebam | \&#xD;&#xA;    samtools view -h -F4 - | \&#xD;&#xA;    samtools view -h -F8 - | \&#xD;&#xA;    samtools view -h -F256 - | \&#xD;&#xA;    samtools view -h -F512 - | \&#xD;&#xA;    samtools view -h -F1024 - | \&#xD;&#xA;    samtools view -h -F2048 - | \&#xD;&#xA;    samtools view -h -f16 - | \&#xD;&#xA;    samtools view -h -f32 -  | \&#xD;&#xA;    samtools mpileup --excl-flags 0 -Q0 -B -d 999999 - | wc -l&#xD;&#xA;&#xD;&#xA;Returns 0.&#xD;&#xA;&#xD;&#xA;I tried different combinations of filtering, and when I do both `-f 16` and `-f 32` returns empty, but if I do either of those, then it works:&#xD;&#xA;&#xD;&#xA;    samtools view -h $myfilebam | \&#xD;&#xA;    samtools view -h -F4 - | \&#xD;&#xA;    samtools view -h -F8 - | \&#xD;&#xA;    samtools view -h -F256 - | \&#xD;&#xA;    samtools view -h -F512 - | \&#xD;&#xA;    samtools view -h -F1024 - | \&#xD;&#xA;    samtools view -h -F2048 - | \&#xD;&#xA;    samtools view -h -f16 - | \&#xD;&#xA;    samtools mpileup --excl-flags 0 -Q0 -B -d 999999 - | wc -l&#xD;&#xA;&#xD;&#xA;Returns `1056`.&#xD;&#xA;&#xD;&#xA;Any ideas why? My thinking was that it would work with `--excl-flags 0`.&#xD;&#xA;&#xD;&#xA;EDIT: substituting `mpileup` for `depth` does work, and prints out each position and the depth as expected.&#xD;&#xA;&#xD;&#xA;EDIT2: adding `-q 0` to `mpileup` gives the same empty result.&#xD;&#xA;&#xD;&#xA;Thanks in advance&#xD;&#xA;" />
  <row Id="695" PostHistoryTypeId="2" PostId="239" RevisionGUID="b2ebc4ff-0c45-408d-b1bf-709a0a83de76" CreationDate="2017-05-24T22:35:24.987" UserId="73" Text="By using ```-h``` in the ```samtools view``` command, you're including all the header lines in your word count. If you happen to have about 89500 reference sequences, then the lengths of those would all appear in the header and inflate the ```-h``` word count, but not the mpileup count. Try piping it through an additional ```samtools view``` (i.e. without ```-h```) and see if the counts change:&#xD;&#xA;&#xD;&#xA;    ...&#xD;&#xA;    samtools view -h -f32 -  | \&#xD;&#xA;    samtools view | wc -l&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Also, ```samtools mpileup``` by default only considers high-quality bases and concordant reads. Try adding a ```-A``` to your mpileup line (which stops anomalous read pairs from being discarded):&#xD;&#xA;&#xD;&#xA;    ...&#xD;&#xA;    samtools mpileup -A -Q0 -B -d 999999 - | wc -l" />
  <row Id="696" PostHistoryTypeId="2" PostId="240" RevisionGUID="d3c0e4cb-2ceb-4b53-a084-7b10b419a0e5" CreationDate="2017-05-24T22:59:54.537" UserId="57" Text="One big downside of KEGG is the licensing issue. One big advantage of Reacome are various crosslinks to other databases and data.&#xD;&#xA;&#xD;&#xA;**ad 1,** This depends on which pathway, they are both primary databases. Sometimes other databases that for instance combine data of primary databases have better annotation of pathways (there is an example in the review paper bellow)&#xD;&#xA;&#xD;&#xA;**ad 3,** There is very extensive relatively new (2015) review on this topic focused on human pathways: [Comparison of human cell signaling pathway databases—evolution, drawbacks and challenges][1]. However I could not find there which one is more complete ...&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://academic.oup.com/database/article/2433126/Comparison-of-human-cell-signaling-pathway" />
  <row Id="697" PostHistoryTypeId="2" PostId="241" RevisionGUID="6a54359a-3d6e-4447-b493-eb9d116d6961" CreationDate="2017-05-24T23:32:41.013" UserId="154" Text="I would consider using gene expression signatures to classify samples (especially cancer subtypes but the same principles apply to other problems of this type) one of the classic problems of bioinformatics. Quite a lot of work has been done on methods to derive gene sets that provide good classification performance. This is slightly different from your problem since you already have a gene signature but it may still prove useful.&#xD;&#xA;&#xD;&#xA;These methods will typically fit a model that selects a (small) number of genes from genome-wide expression data that distinguish between the cell types/conditions in question, i.e. they derive a gene signature. The resulting model then allows for the classification of new samples. I've had success using [GeneRave][1] for this purpose (but note that this was designed for microarray data, I haven't used it with RNA-seq data and don't know how well it holds up there). A more recent paper relating to this issue can be found [here][2].&#xD;&#xA;&#xD;&#xA;So how does that help you? One option would be to fit one of these classifiers to gene expression data for the genes you already know to obtain a model that can then be applied to new samples automatically.&#xD;&#xA;&#xD;&#xA;  [1]: http://bioinformatics.csiro.au/generave/&#xD;&#xA;  [2]: https://www.nature.com/articles/srep32976." />
  <row Id="699" PostHistoryTypeId="5" PostId="235" RevisionGUID="c9371cb1-8234-4964-86f5-1d3409c9cb82" CreationDate="2017-05-25T10:04:02.870" UserId="222" Comment="Response to Devon's answer" Text="If they're truly technical replicates, then &lt;strike&gt;there's no way to model them using DESeq2&lt;/strike&gt;*, as you've alluded to with the `collapseReplicates` function. DESeq2/ Mike Love's general recommendation with `collapseReplicates` is to just add the reads together for technical replicates. &#xD;&#xA;&#xD;&#xA;If you want to model them instead of collapsing them down, you can `voom` transform your data and follow an example similar to section 11.3 in the [Limma users guide][1].&#xD;&#xA;&#xD;&#xA;**(Code example from 11.3):**&#xD;&#xA;&#xD;&#xA;*Design*&#xD;&#xA;&#xD;&#xA;    | FileName  | Cy3   | Cy5  |&#xD;&#xA;    | --------- |:-----:| ----:|&#xD;&#xA;    | File1     | wt1   | mu1  |&#xD;&#xA;    | File2     | wt1   | mu1  |&#xD;&#xA;    | File3     | wt2   | mu2  |&#xD;&#xA;    | File4     | wt2   | mu2  |&#xD;&#xA;&#xD;&#xA;*Example*&#xD;&#xA;&#xD;&#xA;    &gt; biolrep &lt;- c(1, 1, 2, 2)&#xD;&#xA;    &gt; corfit &lt;- duplicateCorrelation(MA, ndups = 1, block = biolrep)&#xD;&#xA;    &gt; fit &lt;- lmFit(MA, block = biolrep, cor = corfit$consensus)&#xD;&#xA;    &gt; fit &lt;- eBayes(fit)&#xD;&#xA;    &gt; topTable(fit, adjust = &quot;BH&quot;)&#xD;&#xA;&#xD;&#xA;  [1]: https://www.bioconductor.org/packages/devel/bioc/vignettes/limma/inst/doc/usersguide.pdf&#xD;&#xA;&#xD;&#xA;*Edit: As @Devon Ryan mentions, you can design a paired model in `DESeq2`, but beware of making this full rank. " />
  <row Id="700" PostHistoryTypeId="2" PostId="242" RevisionGUID="d7036924-991a-468d-92d7-4f4cef19b960" CreationDate="2017-05-25T11:46:36.237" UserId="73" Text="I'm currently trying to assembly a genome from a rodent parasite, *Nippostrongylus brasiliensis*. This genome does have an existing reference genome, but it is highly fragmented. Here are some continuity statistics for the scaffolds of the current Nippo reference genome (assembled from Illumina reads):&#xD;&#xA;&#xD;&#xA;    Total sequences: 29375&#xD;&#xA;    Total length: 294.400206 Mb&#xD;&#xA;    Longest sequence: 394.171 kb&#xD;&#xA;    Shortest sequence: 500 b&#xD;&#xA;    Mean Length: 10.022 kb&#xD;&#xA;    Median Length: 2.682 kb&#xD;&#xA;    N50: 2024 sequences; L50: 33.527 kb&#xD;&#xA;    N90: 11638 sequences; L90: 4.263 kb&#xD;&#xA;&#xD;&#xA;This genome is most likely difficult to assemble because of the highly repetitive nature of the genomic sequences. These repetitive sequences come in three classes:&#xD;&#xA;&#xD;&#xA; 1. Tandem repeats with a *repeat unit* length greater than the read length of Illumina sequencers (e.g. 171bp)&#xD;&#xA; 1. Tandem repeats with a *cumulative* length greater than the fragment length of Illumina sequencers, or the template length for linked reads (e.g. 20kb)&#xD;&#xA; 1. Complex (i.e. non-repetitive) sequence that appears at multiple places throughout the genome&#xD;&#xA;&#xD;&#xA;Canu seems to deal quite well with the first two types of repeats, despite the abundance of repetitive structure in the genome. Here's the unitigging summary produced by Canu on one of the assemblies I've attempted. Notice that about 30% of the reads either span or contain a long repeat:&#xD;&#xA;&#xD;&#xA;    category            reads     %          read length        feature size or coverage  analysis&#xD;&#xA;    ----------------  -------  -------  ----------------------  ------------------------  --------------------&#xD;&#xA;    middle-missing        694    0.07     7470.92 +- 5552.00        953.06 +- 1339.13    (bad trimming)&#xD;&#xA;    middle-hump           549    0.05     3770.05 +- 3346.10         74.23 +- 209.86     (bad trimming)&#xD;&#xA;    no-5-prime           3422    0.33     6711.32 +- 5411.26         70.92 +- 272.99     (bad trimming)&#xD;&#xA;    no-3-prime           3161    0.30     6701.35 +- 5739.86         87.41 +- 329.42     (bad trimming)&#xD;&#xA;    &#xD;&#xA;    low-coverage        27158    2.59     3222.51 +- 1936.79          4.99 +- 1.79       (easy to assemble, potential for lower quality consensus)&#xD;&#xA;    unique             636875   60.76     6240.20 +- 3908.44         25.22 +- 8.49       (easy to assemble, perfect, yay)&#xD;&#xA;    repeat-cont         48398    4.62     4099.55 +- 3002.72        335.54 +- 451.43     (potential for consensus errors, no impact on assembly)&#xD;&#xA;    repeat-dove           135    0.01    16996.33 +- 6860.08        397.37 +- 319.52     (hard to assemble, likely won't assemble correctly or even at all)&#xD;&#xA;    &#xD;&#xA;    span-repeat        137927   13.16     9329.94 +- 6906.27       2630.06 +- 3539.53    (read spans a large repeat, usually easy to assemble)&#xD;&#xA;    uniq-repeat-cont   155725   14.86     6529.83 +- 3463.16                             (should be uniquely placed, low potential for consensus errors, no impact on assembly)&#xD;&#xA;    uniq-repeat-dove    28248    2.70    12499.99 +- 8446.95                             (will end contigs, potential to misassemble)&#xD;&#xA;    uniq-anchor          5721    0.55     8379.86 +- 4575.71       3166.22 +- 3858.35    (repeat read, with unique section, probable bad read)&#xD;&#xA;&#xD;&#xA;However, the third type of repeat is giving me a bit of grief. Using the above assembly, here are the continuity parameters from the assembled contigs:&#xD;&#xA;&#xD;&#xA;    Total sequences: 3505&#xD;&#xA;    Total length: 322.867456 Mb&#xD;&#xA;    Longest sequence: 1.762243 Mb&#xD;&#xA;    Shortest sequence: 2.606 kb&#xD;&#xA;    Mean Length: 92.116 kb&#xD;&#xA;    Median Length: 42.667 kb&#xD;&#xA;    N50: 417 sequences; L50: 194.126 kb&#xD;&#xA;    N90: 1996 sequences; L90: 35.634 kb&#xD;&#xA;&#xD;&#xA;It's not a *bad* assembly, particularly given the complexity of the genome, but I feel like it could be improved by tackling the complex genomic repeats in some fashion. About 60Mb of the contigs in this assembly are linked with each other in a huge web (based on the GFA output from Canu):&#xD;&#xA;&#xD;&#xA;[![60Mb linked structure from Canu GFA][1]][1]&#xD;&#xA;&#xD;&#xA;The repetitive regions are typically over 500bp in length, average about 3kb, and I've seen at least one case which seems to be a 20kb sequence duplicated in multiple regions.&#xD;&#xA;&#xD;&#xA;The Canu defaults seem to give the best assembly results for the few parameters that I've tried, with one exception: trimming. I've tried playing around a little bit with the trimming parameters, and curiously a trimming coverage of 5X (with overlap of 500bp) seems to give a more contiguous assembly than with a trimming coverage of 2X (with the same overlap).&#xD;&#xA;&#xD;&#xA;If anyone is interested in having a look at these data themselves, called FASTQ files from Nippo sequencing runs can be found [here][2]. I'm still in the process of uploading the raw nanopore signal files, but they will be available in the next couple of weeks associated with ENA project PRJEB20824. There's also a Zenodo archive [here][3] that contains the GFA and assembly contigs.&#xD;&#xA;&#xD;&#xA;Does anyone have any other suggestions on how I could resolve these complex repeats?&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/Tw8SQ.jpg&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov//sra/?term=SRP092357&#xD;&#xA;  [3]: https://doi.org/10.5281/zenodo.569899" />
  <row Id="701" PostHistoryTypeId="1" PostId="242" RevisionGUID="d7036924-991a-468d-92d7-4f4cef19b960" CreationDate="2017-05-25T11:46:36.237" UserId="73" Text="How can I improve a long-read assembly with a repetitive genome?" />
  <row Id="702" PostHistoryTypeId="3" PostId="242" RevisionGUID="d7036924-991a-468d-92d7-4f4cef19b960" CreationDate="2017-05-25T11:46:36.237" UserId="73" Text="&lt;assembly&gt;&lt;nanopore&gt;&lt;long-reads&gt;&lt;canu&gt;&lt;repeat&gt;" />
  <row Id="703" PostHistoryTypeId="5" PostId="242" RevisionGUID="b750eadd-1843-453a-9fa3-fd770abee2da" CreationDate="2017-05-25T11:51:41.337" UserId="73" Comment="added 11 characters in body" Text="I'm currently trying to assembly a genome from a rodent parasite, *Nippostrongylus brasiliensis*. This genome does have an existing reference genome, but it is highly fragmented. Here are some continuity statistics for the scaffolds of the current Nippo reference genome (assembled from Illumina reads):&#xD;&#xA;&#xD;&#xA;    Total sequences: 29375&#xD;&#xA;    Total length: 294.400206 Mb&#xD;&#xA;    Longest sequence: 394.171 kb&#xD;&#xA;    Shortest sequence: 500 b&#xD;&#xA;    Mean Length: 10.022 kb&#xD;&#xA;    Median Length: 2.682 kb&#xD;&#xA;    N50: 2024 sequences; L50: 33.527 kb&#xD;&#xA;    N90: 11638 sequences; L90: 4.263 kb&#xD;&#xA;&#xD;&#xA;This genome is most likely difficult to assemble because of the highly repetitive nature of the genomic sequences. These repetitive sequences come in (at least) three classes:&#xD;&#xA;&#xD;&#xA; 1. Tandem repeats with a *repeat unit* length greater than the read length of Illumina sequencers (e.g. 171bp)&#xD;&#xA; 1. Tandem repeats with a *cumulative* length greater than the fragment length of Illumina sequencers, or the template length for linked reads (e.g. 20kb)&#xD;&#xA; 1. Complex (i.e. non-repetitive) sequence that appears at multiple places throughout the genome&#xD;&#xA;&#xD;&#xA;Canu seems to deal quite well with the first two types of repeats, despite the abundance of repetitive structure in the genome. Here's the unitigging summary produced by Canu on one of the assemblies I've attempted. Notice that about 30% of the reads either span or contain a long repeat:&#xD;&#xA;&#xD;&#xA;    category            reads     %          read length        feature size or coverage  analysis&#xD;&#xA;    ----------------  -------  -------  ----------------------  ------------------------  --------------------&#xD;&#xA;    middle-missing        694    0.07     7470.92 +- 5552.00        953.06 +- 1339.13    (bad trimming)&#xD;&#xA;    middle-hump           549    0.05     3770.05 +- 3346.10         74.23 +- 209.86     (bad trimming)&#xD;&#xA;    no-5-prime           3422    0.33     6711.32 +- 5411.26         70.92 +- 272.99     (bad trimming)&#xD;&#xA;    no-3-prime           3161    0.30     6701.35 +- 5739.86         87.41 +- 329.42     (bad trimming)&#xD;&#xA;    &#xD;&#xA;    low-coverage        27158    2.59     3222.51 +- 1936.79          4.99 +- 1.79       (easy to assemble, potential for lower quality consensus)&#xD;&#xA;    unique             636875   60.76     6240.20 +- 3908.44         25.22 +- 8.49       (easy to assemble, perfect, yay)&#xD;&#xA;    repeat-cont         48398    4.62     4099.55 +- 3002.72        335.54 +- 451.43     (potential for consensus errors, no impact on assembly)&#xD;&#xA;    repeat-dove           135    0.01    16996.33 +- 6860.08        397.37 +- 319.52     (hard to assemble, likely won't assemble correctly or even at all)&#xD;&#xA;    &#xD;&#xA;    span-repeat        137927   13.16     9329.94 +- 6906.27       2630.06 +- 3539.53    (read spans a large repeat, usually easy to assemble)&#xD;&#xA;    uniq-repeat-cont   155725   14.86     6529.83 +- 3463.16                             (should be uniquely placed, low potential for consensus errors, no impact on assembly)&#xD;&#xA;    uniq-repeat-dove    28248    2.70    12499.99 +- 8446.95                             (will end contigs, potential to misassemble)&#xD;&#xA;    uniq-anchor          5721    0.55     8379.86 +- 4575.71       3166.22 +- 3858.35    (repeat read, with unique section, probable bad read)&#xD;&#xA;&#xD;&#xA;However, the third type of repeat is giving me a bit of grief. Using the above assembly, here are the continuity parameters from the assembled contigs:&#xD;&#xA;&#xD;&#xA;    Total sequences: 3505&#xD;&#xA;    Total length: 322.867456 Mb&#xD;&#xA;    Longest sequence: 1.762243 Mb&#xD;&#xA;    Shortest sequence: 2.606 kb&#xD;&#xA;    Mean Length: 92.116 kb&#xD;&#xA;    Median Length: 42.667 kb&#xD;&#xA;    N50: 417 sequences; L50: 194.126 kb&#xD;&#xA;    N90: 1996 sequences; L90: 35.634 kb&#xD;&#xA;&#xD;&#xA;It's not a *bad* assembly, particularly given the complexity of the genome, but I feel like it could be improved by tackling the complex genomic repeats in some fashion. About 60Mb of the contigs in this assembly are linked with each other in a huge web (based on the GFA output from Canu):&#xD;&#xA;&#xD;&#xA;[![60Mb linked structure from Canu GFA][1]][1]&#xD;&#xA;&#xD;&#xA;The repetitive regions are typically over 500bp in length, average about 3kb, and I've seen at least one case which seems to be a 20kb sequence duplicated in multiple regions.&#xD;&#xA;&#xD;&#xA;The Canu defaults seem to give the best assembly results for the few parameters that I've tried, with one exception: trimming. I've tried playing around a little bit with the trimming parameters, and curiously a trimming coverage of 5X (with overlap of 500bp) seems to give a more contiguous assembly than with a trimming coverage of 2X (with the same overlap).&#xD;&#xA;&#xD;&#xA;If anyone is interested in having a look at these data themselves, called FASTQ files from Nippo sequencing runs can be found [here][2]. I'm still in the process of uploading the raw nanopore signal files, but they will be available in the next couple of weeks associated with ENA project PRJEB20824. There's also a Zenodo archive [here][3] that contains the GFA and assembly contigs.&#xD;&#xA;&#xD;&#xA;Does anyone have any other suggestions on how I could resolve these complex repeats?&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/Tw8SQ.jpg&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov//sra/?term=SRP092357&#xD;&#xA;  [3]: https://doi.org/10.5281/zenodo.569899" />
  <row Id="704" PostHistoryTypeId="5" PostId="181" RevisionGUID="3ff73cc5-b3b8-420b-b1d3-d367a6de1317" CreationDate="2017-05-25T14:11:27.163" UserId="131" Comment="added 113 characters in body" Text="Bioconductor provides tools for the analysis and comprehension of high-throughput genomic data in the R language." />
  <row Id="705" PostHistoryTypeId="24" PostId="181" RevisionGUID="3ff73cc5-b3b8-420b-b1d3-d367a6de1317" CreationDate="2017-05-25T14:11:27.163" Comment="Proposed by 131 approved by 55, 77 edit id of 49" />
  <row Id="706" PostHistoryTypeId="5" PostId="185" RevisionGUID="f9ab4f7c-6e43-41e6-be8e-64f702133f26" CreationDate="2017-05-25T14:11:32.513" UserId="131" Comment="added 23 characters in body" Text="Browser Extensible Data" />
  <row Id="707" PostHistoryTypeId="24" PostId="185" RevisionGUID="f9ab4f7c-6e43-41e6-be8e-64f702133f26" CreationDate="2017-05-25T14:11:32.513" Comment="Proposed by 131 approved by 55 edit id of 53" />
  <row Id="708" PostHistoryTypeId="5" PostId="209" RevisionGUID="9cf31409-ae25-408c-95ed-9baf9029c3f6" CreationDate="2017-05-25T14:11:44.250" UserId="96" Comment="added 247 characters in body" Text="Hash functions map data of arbitrary size to a value (usually an integer value) of fixed size. In bioinformatics, hash functions are often used to convert DNA sequences (especially *k*-mers) into values that can be stored and analyzed efficiently." />
  <row Id="709" PostHistoryTypeId="24" PostId="209" RevisionGUID="9cf31409-ae25-408c-95ed-9baf9029c3f6" CreationDate="2017-05-25T14:11:44.250" Comment="Proposed by 96 approved by 55, 77 edit id of 63" />
  <row Id="710" PostHistoryTypeId="5" PostId="213" RevisionGUID="02e69b05-7a1e-4a4b-b5a7-81c22dca804b" CreationDate="2017-05-25T14:12:34.707" UserId="96" Comment="added 1658 characters in body" Text="Reproducibility refers to the ability to repeat a scientific result that was reported previously. Several terms are frequently used synonymously with reproducibility, such as *replicability* or *repeatability*. These various terms come with subtle differences in connotation, although there is no universally accepted interpretation of each term.&#xD;&#xA;&#xD;&#xA;In the context of computation in biology, these terms could refer to any of the following, which constitute a continuum of reproducibility.&#xD;&#xA;&#xD;&#xA;- Given a scientist's exact code and data, the ability to produce the same exact result as the original scientist.&#xD;&#xA;- Given a scientist's code and a description of the data, the ability to produce a similar and consistent result with similar data.&#xD;&#xA;- Given a scientist's data and a description of the code, the ability to produce a similar and consistent result with a similar independent implementation of the code.&#xD;&#xA;- Given a description of the code and the data, the ability to produce a similar result or reach a similar conclusion based on independent data collection and analysis.&#xD;&#xA;&#xD;&#xA;There is some debate as to what type/level of reproducibility can and should be expected of scientists. The strongest scientific claims come from completely reproducing a result from independent data collection and software implementation, but resources and career incentives make it difficult to reproduce studies/results in this way. On the other hand, reproducing a precise result can sometimes be difficult even with the exact same code and data as the original author, due to differences in computing platforms and the lack of computational training for many biologists." />
  <row Id="711" PostHistoryTypeId="24" PostId="213" RevisionGUID="02e69b05-7a1e-4a4b-b5a7-81c22dca804b" CreationDate="2017-05-25T14:12:34.707" Comment="Proposed by 96 approved by 55, 77 edit id of 66" />
  <row Id="712" PostHistoryTypeId="5" PostId="184" RevisionGUID="11b7bfc3-ce19-4bdc-b2a1-5eb72e5ada79" CreationDate="2017-05-25T14:18:33.007" UserId="131" Comment="added 144 characters in body" Text="- [UCSC description](https://genome.ucsc.edu/FAQ/FAQformat.html)&#xD;&#xA;&#xD;&#xA;- [Ensembl description](http://www.ensembl.org/info/website/upload/bed.html)" />
  <row Id="713" PostHistoryTypeId="24" PostId="184" RevisionGUID="11b7bfc3-ce19-4bdc-b2a1-5eb72e5ada79" CreationDate="2017-05-25T14:18:33.007" Comment="Proposed by 131 approved by 55 edit id of 52" />
  <row Id="714" PostHistoryTypeId="5" PostId="207" RevisionGUID="493df2c3-257c-4a2e-8672-ff1fdab751bc" CreationDate="2017-05-25T14:18:50.583" UserId="96" Comment="added 270 characters in body" Text="Many bioinformatics analyses involve decomposing long nucleotide (or, more rarely, peptide) sequences can into their constituent *k*-mers, overlapping subsequences of length *k*. For sequence `D = GATTACA` and `k = 4`, the *k*-mers in `D` are `{GATT, ATTA, TTAC, TACA}`." />
  <row Id="715" PostHistoryTypeId="24" PostId="207" RevisionGUID="493df2c3-257c-4a2e-8672-ff1fdab751bc" CreationDate="2017-05-25T14:18:50.583" Comment="Proposed by 96 approved by 55, 77 edit id of 61" />
  <row Id="716" PostHistoryTypeId="5" PostId="223" RevisionGUID="5bc3c160-ad97-4d59-9a13-675b2b6b72da" CreationDate="2017-05-25T14:21:11.510" UserId="33" Comment="Explained Reasoning behind question. Take it or leave it." Text="As a newcomer, what are some influential research papers published in the past few years that I should read to catch up in the field?&#xD;&#xA;&#xD;&#xA;Edit: @Devon Ryan answered it perfectly. I tried a google search but it turned up a bunch of opinionated articles that varied. The biostar discussions are perfect and was the link I was searching for. I hope someone else new to the field finds this useful!" />
  <row Id="718" PostHistoryTypeId="2" PostId="243" RevisionGUID="c44a4895-02d5-4fbe-96e2-6ac442de5fe2" CreationDate="2017-05-25T17:51:23.720" UserId="-1" Text="" />
  <row Id="719" PostHistoryTypeId="1" PostId="243" RevisionGUID="c44a4895-02d5-4fbe-96e2-6ac442de5fe2" CreationDate="2017-05-25T17:51:23.720" UserId="-1" />
  <row Id="720" PostHistoryTypeId="2" PostId="244" RevisionGUID="3a109305-50c6-472d-94f5-401038150920" CreationDate="2017-05-25T17:51:23.720" UserId="-1" Text="" />
  <row Id="721" PostHistoryTypeId="1" PostId="244" RevisionGUID="3a109305-50c6-472d-94f5-401038150920" CreationDate="2017-05-25T17:51:23.720" UserId="-1" />
  <row Id="722" PostHistoryTypeId="2" PostId="245" RevisionGUID="ce78cd50-71e3-479f-b426-f9ff5e89e306" CreationDate="2017-05-25T17:59:21.297" UserId="-1" Text="" />
  <row Id="723" PostHistoryTypeId="1" PostId="245" RevisionGUID="ce78cd50-71e3-479f-b426-f9ff5e89e306" CreationDate="2017-05-25T17:59:21.297" UserId="-1" />
  <row Id="724" PostHistoryTypeId="2" PostId="246" RevisionGUID="ab65cf1e-04bf-4183-a585-daa49195c2d2" CreationDate="2017-05-25T17:59:21.297" UserId="-1" Text="" />
  <row Id="725" PostHistoryTypeId="1" PostId="246" RevisionGUID="ab65cf1e-04bf-4183-a585-daa49195c2d2" CreationDate="2017-05-25T17:59:21.297" UserId="-1" />
  <row Id="726" PostHistoryTypeId="2" PostId="247" RevisionGUID="fcc104be-d3c5-44ba-acb8-a0d4f11a40de" CreationDate="2017-05-25T18:05:03.600" UserId="-1" Text="" />
  <row Id="727" PostHistoryTypeId="1" PostId="247" RevisionGUID="fcc104be-d3c5-44ba-acb8-a0d4f11a40de" CreationDate="2017-05-25T18:05:03.600" UserId="-1" />
  <row Id="728" PostHistoryTypeId="2" PostId="248" RevisionGUID="47245216-1a31-44a0-a070-7528aa91f07c" CreationDate="2017-05-25T18:05:03.600" UserId="-1" Text="" />
  <row Id="729" PostHistoryTypeId="1" PostId="248" RevisionGUID="47245216-1a31-44a0-a070-7528aa91f07c" CreationDate="2017-05-25T18:05:03.600" UserId="-1" />
  <row Id="730" PostHistoryTypeId="2" PostId="249" RevisionGUID="af041a1a-de5f-45fa-9397-23b5c32e8fd6" CreationDate="2017-05-25T18:21:49.743" UserId="9" Text="[The ENCODE Experiment Matrix at UCSC](https://genome.ucsc.edu/ENCODE/dataMatrix/encodeDataMatrixHuman.html) lists the different available cell types under the categories &quot;Tier 1&quot;, &quot;Tier 2&quot; and &quot;Tier 3&quot;. What is the difference between these classifications?&#xD;&#xA;&#xD;&#xA;What, for example, makes GM12878 a Tier 1 cell type and A549 a Tier 2 cell type?&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/Ejhqx.png" />
  <row Id="731" PostHistoryTypeId="1" PostId="249" RevisionGUID="af041a1a-de5f-45fa-9397-23b5c32e8fd6" CreationDate="2017-05-25T18:21:49.743" UserId="9" Text="What is the difference between Tier 1, 2 and 3 cell types?" />
  <row Id="732" PostHistoryTypeId="3" PostId="249" RevisionGUID="af041a1a-de5f-45fa-9397-23b5c32e8fd6" CreationDate="2017-05-25T18:21:49.743" UserId="9" Text="&lt;encode&gt;" />
  <row Id="733" PostHistoryTypeId="2" PostId="250" RevisionGUID="2076adf6-9ed8-4e90-9901-49876308dd9a" CreationDate="2017-05-25T18:44:04.180" UserId="77" Text="The tiers denote the original priority of sequencing and processing the samples. This is understandable given the number of marks and experiment types that ENCODE tried to sequence. Further details are available on [ENCODE's website](http://genome.ucsc.edu/ENCODE/cellTypes.html). &#xD;&#xA;&#xD;&#xA;**Edit**: Just to expand a bit, tier one cells were supposed to get processed first. Then tier 2 came second. Of course tier 2 had a LOT more cells, so it got split into 2 and 2.5, since there were already apparently tier 3 cells." />
  <row Id="734" PostHistoryTypeId="2" PostId="251" RevisionGUID="6d52a564-47e3-4edb-aeae-720c60151e04" CreationDate="2017-05-25T22:47:46.183" UserId="193" Text="Does GISTIC 2.0 estimates the background model:&#xD;&#xA;&#xD;&#xA;`G = -log(Probability | Background)`&#xD;&#xA;&#xD;&#xA;by permuting within the sample or across all samples in the set?&#xD;&#xA;&#xD;&#xA;Basically, does it matter if the set is constituted by, for example, 10 samples of the same tissue of origin or ~1000 from multiple tissues?&#xD;&#xA;&#xD;&#xA;Thanks," />
  <row Id="735" PostHistoryTypeId="1" PostId="251" RevisionGUID="6d52a564-47e3-4edb-aeae-720c60151e04" CreationDate="2017-05-25T22:47:46.183" UserId="193" Text="Does GISTIC 2.0 estimate the probability of SCNA on a single sample basis?" />
  <row Id="736" PostHistoryTypeId="3" PostId="251" RevisionGUID="6d52a564-47e3-4edb-aeae-720c60151e04" CreationDate="2017-05-25T22:47:46.183" UserId="193" Text="&lt;gistic&gt;&lt;cnv&gt;&lt;snp6&gt;" />
  <row Id="737" PostHistoryTypeId="4" PostId="249" RevisionGUID="8045e923-4abe-45e9-b470-837d63a7a151" CreationDate="2017-05-26T09:05:34.530" UserId="61" Comment="these designations are specific to the encode project" Text="What is the difference between ENCODE Tier 1, 2 and 3 cell types?" />
  <row Id="738" PostHistoryTypeId="24" PostId="249" RevisionGUID="8045e923-4abe-45e9-b470-837d63a7a151" CreationDate="2017-05-26T09:05:34.530" Comment="Proposed by 61 approved by 57, 77 edit id of 76" />
  <row Id="739" PostHistoryTypeId="2" PostId="252" RevisionGUID="9deaf73b-e55d-4fd4-9012-81fb53f03f7a" CreationDate="2017-05-26T09:41:03.727" UserId="377" Text="For you the main point would be whether an enrichment analysis is going to give you an *informative* answer. That's what will make a particular database better. And there's all sorts of subjective decisions made in their construction as to what what a pathway is, what to include, where to draw boundaries, etc. so different databases will give different answers and it will not be clear which is more correct.&#xD;&#xA;&#xD;&#xA;So browse over the references / pointers people have given, select a service and use it and no others. Don't jump around databases until you get an answer you like, that's just fishing. " />
  <row Id="740" PostHistoryTypeId="2" PostId="253" RevisionGUID="dcad0078-b3a4-4b82-8867-9232e1d0905b" CreationDate="2017-05-26T13:23:00.853" UserId="180" Text="I am running samtools mpileup (v1.4) on a bam file with very choppy coverage (ChIP-seq style data). I want to get a first-pass list of positions with SNVs and their frequency as reported by the read counts, but no matter what I do, I keep getting all SNVs filtered out as not passing QC.&#xD;&#xA;&#xD;&#xA;What's the magic parameter set for an initial list of SNVs and frequencies?&#xD;&#xA;&#xD;&#xA;EDIT: this is a question I posted on &quot;the other&quot; website, but didn't get a reply there.&#xD;&#xA;" />
  <row Id="741" PostHistoryTypeId="1" PostId="253" RevisionGUID="dcad0078-b3a4-4b82-8867-9232e1d0905b" CreationDate="2017-05-26T13:23:00.853" UserId="180" Text="variant calling on ChIP-seq style data: samtools mpileup with minimal filters" />
  <row Id="742" PostHistoryTypeId="3" PostId="253" RevisionGUID="dcad0078-b3a4-4b82-8867-9232e1d0905b" CreationDate="2017-05-26T13:23:00.853" UserId="180" Text="&lt;variant-calling&gt;&lt;samtools&gt;&lt;chip-seq&gt;&lt;mpileup&gt;" />
  <row Id="743" PostHistoryTypeId="2" PostId="254" RevisionGUID="253bcc4d-93ad-4f48-a936-9629f2688353" CreationDate="2017-05-26T14:06:59.617" UserId="48" Text="I have perform an enrichment analysis to a cluster of genes. The output is a list of pathways and their p-value (the pathways are selected because p-value &lt; 0.05). The list is still quite long, so I want to reduce it. For that purpose I have a calculated the Dice coefficient of the pathways in a matrix $p$x$p$ where $p$ is the number of pathways in the list. I want both the ones that are more different (they overlap less, their Dice coefficient is lower) and the pathways more representative of the most similar pathways (So if a there is a group of 5 pathways that overlap over 0.8 take just one).&#xD;&#xA;&#xD;&#xA;How can I select the most representatives pathways? &#xD;&#xA;&#xD;&#xA;There is a similar [tool][1] for GO but it relays on discarding not significant GO, while here all the initial pathways are already significant. &#xD;&#xA; &#xD;&#xA;If I do a clustering of the genes using the Dice coefficient matrix I don't know where (or how) to cut.&#xD;&#xA; [![circular dendrogara][2]][2]&#xD;&#xA;&#xD;&#xA;I tried using the height to select the pathways. But I am unsure of the interpretation of height. &#xD;&#xA;&#xD;&#xA;Some other tools I have seen use a multidimensional scaling plot, but I am not sure if performing it and cutting at certain point of the first dimension would help.&#xD;&#xA;[![MDS plot][3]][3]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://revigo.irb.hr/&#xD;&#xA;  [2]: https://i.stack.imgur.com/mMdEH.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/qJaMm.png" />
  <row Id="744" PostHistoryTypeId="1" PostId="254" RevisionGUID="253bcc4d-93ad-4f48-a936-9629f2688353" CreationDate="2017-05-26T14:06:59.617" UserId="48" Text="How to select pathways?" />
  <row Id="745" PostHistoryTypeId="3" PostId="254" RevisionGUID="253bcc4d-93ad-4f48-a936-9629f2688353" CreationDate="2017-05-26T14:06:59.617" UserId="48" Text="&lt;report&gt;&lt;pathway&gt;" />
  <row Id="746" PostHistoryTypeId="5" PostId="254" RevisionGUID="f5778ce0-8d7f-4e45-ba52-48e622802db5" CreationDate="2017-05-26T14:25:36.437" UserId="48" Comment="added 2 characters in body" Text="I have perform an enrichment analysis to a cluster of genes. The output is a list of pathways and their p-value (the pathways are selected because p-value &lt; 0.05). The list is still quite long, so I want to reduce it. For that purpose I have a calculated the Dice coefficient of the pathways in a matrix $p$x$p$ where $p$ is the number of pathways in the list. I want both the ones that are more different (they overlap less, their Dice coefficient is lower) and the pathways more representative of the most similar pathways (So if a there is a group of 5 pathways that overlap over 0.8 take just one).&#xD;&#xA;&#xD;&#xA;How can I select the most representatives pathways? &#xD;&#xA;&#xD;&#xA;There is a similar [tool][1] for GO but it relays on discarding not significant GO, while here all the initial pathways are already significant. &#xD;&#xA; &#xD;&#xA;If I do a clustering of the genes using the Dice coefficient matrix I don't know where (or how) to cut.&#xD;&#xA; [![circular dendrogara][2]][2]&#xD;&#xA;&#xD;&#xA;I tried using the height to select the pathways. But I am unsure of the interpretation of height. &#xD;&#xA;&#xD;&#xA;Some other tools I have seen use a multidimensional scaling plot, but I am not sure if performing it and cutting at certain point of the first dimension would help.&#xD;&#xA;[![MDS plot][3]][3]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://revigo.irb.hr/&#xD;&#xA;  [2]: https://i.stack.imgur.com/mMdEH.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/CeetJ.png&#xD;&#xA;" />
  <row Id="747" PostHistoryTypeId="2" PostId="255" RevisionGUID="97b91643-e642-42ac-831a-36db1553fc94" CreationDate="2017-05-26T20:27:47.253" UserId="99" Text="From [Encode's website](https://www.genome.gov/26524238/encode-project-common-cell-types/):&#xD;&#xA;&#xD;&#xA;&gt; ... To aid in the integration and&#xD;&#xA;&gt; comparison of data produced using different technologies and&#xD;&#xA;&gt; platforms, the ENCODE Consortium has designated cell types that will&#xD;&#xA;&gt; be used by all investigators. These common cell types include both&#xD;&#xA;&gt; cell lines and primary cell types, and plans are being made to explore&#xD;&#xA;&gt; the use of primary tissues and embryonic stem (ES) cells.&#xD;&#xA;&gt; &#xD;&#xA;&gt; Cell types were selected largely for practical reasons, including&#xD;&#xA;&gt; their wide availability, the ability to grow them easily, and their&#xD;&#xA;&gt; capacity to produce sufficient numbers of cells for use in all&#xD;&#xA;&gt; technologies being used by ENCODE investigators. Secondary&#xD;&#xA;&gt; considerations were the diversity in tissue source of the cells, germ&#xD;&#xA;&gt; layer lineage representation, the availability of existing data&#xD;&#xA;&gt; generated using the cell type, and coordination with other ongoing&#xD;&#xA;&gt; projects. Effort was also made to select at least some cell types that&#xD;&#xA;&gt; have a relatively normal karyotype..." />
  <row Id="752" PostHistoryTypeId="2" PostId="257" RevisionGUID="2abec62f-1de8-45d1-b8fc-0576efca2315" CreationDate="2017-05-27T01:16:15.507" UserId="35" Text="I used this in the past for ChIP-seq data and it generated SNVs:&#xD;&#xA;&#xD;&#xA;    samtools mpileup \&#xD;&#xA;    --uncompressed --max-depth 10000 --min-MQ 20 --ignore-RG --skip-indels \&#xD;&#xA;    --fasta-ref ref.fa file.bam \&#xD;&#xA;    | bcftools call --consensus-caller \&#xD;&#xA;    &gt; out.vcf&#xD;&#xA;&#xD;&#xA;This was samtools 1.3 in case that makes a difference." />
  <row Id="763" PostHistoryTypeId="2" PostId="259" RevisionGUID="9b97eb8d-706b-4852-b4f8-716bf05ec8de" CreationDate="2017-05-27T10:43:34.673" UserId="163" Text="If you're happy with a ranking of the most representative gene sets, rather than necessarily cutting down the list, you might try EGSEA. It uses an ensemble approach to give a ranking of the most relevant gene sets, and also produces an interactive HTML output with statistics, heatmaps, pathway maps, summary plots and GO graphs which allows you to examine the output at varying levels of granularity. &#xD;&#xA;&#xD;&#xA;You can read the paper on [bioRxiv][1] or download the package from [Bioconductor][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biorxiv.org/content/early/2016/07/08/042580&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/bioc/html/EGSEA.html" />
  <row Id="764" PostHistoryTypeId="4" PostId="254" RevisionGUID="193d6061-9d96-4d70-ad4d-882a7a2056e8" CreationDate="2017-05-27T10:48:49.137" UserId="48" Comment="Improve title to the actual question" Text="How to select the most representatives pathways?" />
  <row Id="766" PostHistoryTypeId="2" PostId="261" RevisionGUID="bb4c2591-039f-48e9-a3c7-ba5bcb76a011" CreationDate="2017-05-27T16:05:14.763" UserId="33" Text="Perhaps [GenomeCRISPR](http://genomecrispr.dkfz.de/#!/), I've personally never used it, but it has a well-documented API and appears you could automate it to go through the whole genome." />
  <row Id="767" PostHistoryTypeId="5" PostId="251" RevisionGUID="bca2b88f-a227-46ea-b0c2-276338dc2f03" CreationDate="2017-05-27T21:48:01.237" UserId="193" Comment="Expanded" Text="Does [GISTIC 2.0][1] estimates the background model:&#xD;&#xA;&#xD;&#xA;`G = -log(Probability | Background)`&#xD;&#xA;&#xD;&#xA;by permuting within the sample or across all samples in the set?&#xD;&#xA;&#xD;&#xA;The paper describes the probabilistic scoring method based on permutations, but I could not understand if this permutation is performed within the sample only. The [documentation page][2] seems to suggest across samples, but this would mean that different sized sets might lead to different outcomes on the sample sample.&#xD;&#xA;&#xD;&#xA;Basically, does it matter if the set is constituted by, for example, 10 samples of the same tissue of origin or ~1000 from multiple tissues?&#xD;&#xA;&#xD;&#xA;Thanks,&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3218867/&#xD;&#xA;  [2]: ftp://ftp.broadinstitute.org/pub/GISTIC2.0/GISTICDocumentation_standalone.htm" />
  <row Id="768" PostHistoryTypeId="4" PostId="251" RevisionGUID="bca2b88f-a227-46ea-b0c2-276338dc2f03" CreationDate="2017-05-27T21:48:01.237" UserId="193" Comment="Expanded" Text="Does GISTIC (v 2.0) estimates the amplified/deleted probabilities on a single sample basis?" />
  <row Id="769" PostHistoryTypeId="5" PostId="259" RevisionGUID="d36629e3-fe23-4fa9-801a-7ce83c58763d" CreationDate="2017-05-27T23:58:34.373" UserId="163" Comment="clarify that ranking is already done" Text="If you're happy with a more confident ranking of the most representative gene sets, rather than necessarily cutting down the list, you might try EGSEA. It uses an ensemble approach to give a ranking of the most relevant gene sets, and also produces an interactive HTML output with statistics, heatmaps, pathway maps, summary plots and GO graphs which allows you to examine the output at varying levels of granularity. &#xD;&#xA;&#xD;&#xA;You can read the paper on [bioRxiv][1] or download the package from [Bioconductor][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biorxiv.org/content/early/2016/07/08/042580&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/bioc/html/EGSEA.html" />
  <row Id="771" PostHistoryTypeId="2" PostId="262" RevisionGUID="67befbd4-ef57-4b9e-a007-b1f6873659b8" CreationDate="2017-05-28T10:23:33.793" UserId="73" Text="This sounds like something that might be amenable to a clustered heatmap plot, or a correlation matrix plot, or something similar. Have you looked at a correlation matrix of the dice coefficient matrix (or maybe just a heatmap plot of that matrix without the correlation matrix)?&#xD;&#xA;&#xD;&#xA;The ```corrplot``` package looks like it might be useful, in particular the ```hclust``` / drawing rectangles presentation:&#xD;&#xA;&#xD;&#xA;https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html&#xD;&#xA;&#xD;&#xA;I can't vouch for this package though; it's just something I found by a search for &quot;R plot correlation matrix&quot;." />
  <row Id="772" PostHistoryTypeId="4" PostId="254" RevisionGUID="8ac84b00-5918-4c56-9930-17dac3a3fa72" CreationDate="2017-05-28T10:50:21.933" UserId="73" Comment="fixed grammar, clarified question" Text="How to select the most representative pathways from a gene enrichment analysis?" />
  <row Id="773" PostHistoryTypeId="24" PostId="254" RevisionGUID="8ac84b00-5918-4c56-9930-17dac3a3fa72" CreationDate="2017-05-28T10:50:21.933" Comment="Proposed by 73 approved by 48 edit id of 77" />
  <row Id="774" PostHistoryTypeId="2" PostId="263" RevisionGUID="05134226-c8f5-4d17-acaf-ce283d4083e6" CreationDate="2017-05-28T11:40:16.593" UserId="48" Text="There doesn't seem to bu much differences between GISTIC 1.0 and 2.0 as it says:&#xD;&#xA;&#xD;&#xA;&gt; As with GISTIC 1.0, we obtain P-values for each marker by comparing the score at each locus to a background score distribution generated by random permutation of the marker locations in each sample&#xD;&#xA;&#xD;&#xA;But on the [supplementary material][1] of the GISTIC 1.0 there is a more detailed explanation of the method. See the &quot;Stage 2&quot; section:&#xD;&#xA;&#xD;&#xA;&gt; Second, we compare these G-scores to the distribution of scores expected if only random aberrations were observed. This distribution can be determined by rescoring the genome after permuting marker locations within each sample; we instead derive a semiexact estimate.&#xD;&#xA;&#xD;&#xA;Furthermore in another section (&quot;Stage 2: Aggregation of Data from Different Tumors to Differentiate Between Driver and Passenger Aberrations&quot;) it says:&#xD;&#xA;&#xD;&#xA;&gt;To determine which of the aberrations identified in Stage 1 are likely to represent driver events, we aggregate the data from all tumors used in the analysis to generate summary scores for amplifications, deletions, and LOH. The statistical significance of each score is determined by comparison to the distribution of scores obtained by all permutations of the data (using a semiexact approximation), with correction for multiple hypothesis testing. &#xD;&#xA;&#xD;&#xA;However, the relevant section (of the supplementary materials) seems to be &quot;Null Hypothesis Generation: An Analytic Derivation of the Null Distribution&quot;, where it describes the semiexact approximation used.&#xD;&#xA;&#xD;&#xA;In a [supplementary file][2] it describes it as &quot;Generate all permutations of SNP labels within each sample to simulate datasets with random aberrations&quot;&#xD;&#xA;&#xD;&#xA;**Conclusion**:&#xD;&#xA;&#xD;&#xA;It seems that the background probabilities are calculated within sample.&#xD;&#xA;&#xD;&#xA;I can't say if it matters how the sets are constituted but I would say that the broader the sets are, the better estimation of the structural variations it will perform but the same estimation of the background will be done. &#xD;&#xA;&#xD;&#xA;Ultimately you can check the code of the program, or test with 10 samples and replace one of them to see if the results are changing accordingly. &#xD;&#xA; &#xD;&#xA;  [1]: http://www.pnas.org.sare.upf.edu/content/suppl/2007/11/20/0710052104.DC1#ST&#xD;&#xA;  [2]: http://www.pnas.org.sare.upf.edu/content/suppl/2007/11/20/0710052104.DC1/10052Fig9.pdf" />
  <row Id="778" PostHistoryTypeId="2" PostId="264" RevisionGUID="0914ff99-d11a-4b25-89f3-5bc100c0a0b3" CreationDate="2017-05-28T21:40:13.940" UserId="57" Text="I got a bunch of vcf files (v4.1) with structural variations of bunch of non-model organisms (i.e. there are no known variants). I found there are quite a some tools to manipulate vcf files like [VCFtools][1], R package [vcfR][2] or python library [PyVCF][3]. However none of them seems to provide a quick summary, something like (preferably categorised by size as well):&#xD;&#xA;&#xD;&#xA;    type    count&#xD;&#xA;    DEL     x&#xD;&#xA;    INS     y&#xD;&#xA;    INV     z&#xD;&#xA;    ....&#xD;&#xA;&#xD;&#xA;Is there any tool or a function I overlooked that produces summaries of this style?&#xD;&#xA;&#xD;&#xA;I know that vcf file is just a plain text file and if I will dissect `REF` and `ALT` columns I should be able to write a script that will do the job, but I hoped that I could avoid to write my own parser.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://vcftools.github.io/&#xD;&#xA;  [2]: https://cran.r-project.org/web/packages/vcfR/index.html&#xD;&#xA;  [3]: https://pypi.python.org/pypi/PyVCF" />
  <row Id="779" PostHistoryTypeId="1" PostId="264" RevisionGUID="0914ff99-d11a-4b25-89f3-5bc100c0a0b3" CreationDate="2017-05-28T21:40:13.940" UserId="57" Text="Is there an easy way how to creat a summary of vcf file?" />
  <row Id="780" PostHistoryTypeId="3" PostId="264" RevisionGUID="0914ff99-d11a-4b25-89f3-5bc100c0a0b3" CreationDate="2017-05-28T21:40:13.940" UserId="57" Text="&lt;vcf&gt;&lt;structural-variation&gt;" />
  <row Id="781" PostHistoryTypeId="4" PostId="264" RevisionGUID="51219e0b-1573-46f2-b762-937697f725dd" CreationDate="2017-05-28T21:54:51.787" UserId="215" Comment="Tidy title" Text="Is there an easy way to create a summary of a VCF file?" />
  <row Id="782" PostHistoryTypeId="24" PostId="264" RevisionGUID="51219e0b-1573-46f2-b762-937697f725dd" CreationDate="2017-05-28T21:54:51.787" Comment="Proposed by 215 approved by 57 edit id of 78" />
  <row Id="784" PostHistoryTypeId="2" PostId="266" RevisionGUID="7a293378-bd50-4644-bc50-c1cfaec60db3" CreationDate="2017-05-29T00:01:03.100" UserId="126" Text="I'm just getting started with the Mauve aligner and I'm finding the documentation a bit lacking. I'm using the `progressiveMauve` tool from the command line and would like to output an identity matrix file. The set of output files it generates by default for `progressiveMauve --output=outfile` are `outfile`, `outfile.backbone` and `output.bbcols`.&#xD;&#xA;&#xD;&#xA;The [documentation](http://darlinglab.org/mauve/user-guide/progressivemauve.html) for the tool lists an `--input-id-matrix` option but this doesn't seem to have any effect. The [documentation page for output files](http://darlinglab.org/mauve/user-guide/files.html) does describe an identity matrix file but doesn't say anything about how to generate it." />
  <row Id="785" PostHistoryTypeId="1" PostId="266" RevisionGUID="7a293378-bd50-4644-bc50-c1cfaec60db3" CreationDate="2017-05-29T00:01:03.100" UserId="126" Text="How can I output an identity matrix in progressiveMauve?" />
  <row Id="786" PostHistoryTypeId="3" PostId="266" RevisionGUID="7a293378-bd50-4644-bc50-c1cfaec60db3" CreationDate="2017-05-29T00:01:03.100" UserId="126" Text="&lt;alignment&gt;&lt;mauve&gt;" />
  <row Id="792" PostHistoryTypeId="2" PostId="268" RevisionGUID="511df202-84f2-424c-b0f4-3f50ca564046" CreationDate="2017-05-29T05:16:36.277" UserId="73" Text="According to the ```bcftools``` man page, it is able to produce statistics using the command ```bcftools stats```. Running this myself, the statistics look like what you're asking for:&#xD;&#xA;&#xD;&#xA;    # This file was produced by bcftools stats (1.2-187-g1a55e45+htslib-1.2.1-256-ga356746) and can be plotted using plot-vcfstats.&#xD;&#xA;    # The command line was:	bcftools stats  OVLNormalised_STARout_KCCG_called.vcf.gz&#xD;&#xA;    #&#xD;&#xA;    # Definition of sets:&#xD;&#xA;    # ID	[2]id	[3]tab-separated file names&#xD;&#xA;    ID	0	OVLNormalised_STARout_KCCG_called.vcf.gz&#xD;&#xA;    # SN, Summary numbers:&#xD;&#xA;    # SN	[2]id	[3]key	[4]value&#xD;&#xA;    SN	0	number of samples:	108&#xD;&#xA;    SN	0	number of records:	333&#xD;&#xA;    SN	0	number of no-ALTs:	0&#xD;&#xA;    SN	0	number of SNPs:	313&#xD;&#xA;    SN	0	number of MNPs:	0&#xD;&#xA;    SN	0	number of indels:	20&#xD;&#xA;    SN	0	number of others:	0&#xD;&#xA;    SN	0	number of multiallelic sites:	0&#xD;&#xA;    SN	0	number of multiallelic SNP sites:	0&#xD;&#xA;    # TSTV, transitions/transversions:&#xD;&#xA;    # TSTV	[2]id	[3]ts	[4]tv	[5]ts/tv	[6]ts (1st ALT)	[7]tv (1st ALT)	[8]ts/tv (1st ALT)&#xD;&#xA;    TSTV	0	302	11	27.45	302	11	27.45&#xD;&#xA;    # SiS, Singleton stats:&#xD;&#xA;    ...&#xD;&#xA;    # IDD, InDel distribution:&#xD;&#xA;    # IDD	[2]id	[3]length (deletions negative)	[4]count&#xD;&#xA;    IDD	0	-9	1&#xD;&#xA;    IDD	0	-2	4&#xD;&#xA;    IDD	0	-1	6&#xD;&#xA;    IDD	0	1	4&#xD;&#xA;    IDD	0	2	1&#xD;&#xA;    IDD	0	3	3&#xD;&#xA;    IDD	0	4	1&#xD;&#xA;    # ST, Substitution types:&#xD;&#xA;    # ST	[2]id	[3]type	[4]count&#xD;&#xA;    ST	0	A&gt;C	2&#xD;&#xA;    ST	0	A&gt;G	78&#xD;&#xA;    ST	0	A&gt;T	2&#xD;&#xA;    ST	0	C&gt;A	5&#xD;&#xA;    ST	0	C&gt;G	0&#xD;&#xA;    ST	0	C&gt;T	66&#xD;&#xA;    ST	0	G&gt;A	67&#xD;&#xA;    ST	0	G&gt;C	0&#xD;&#xA;    ST	0	G&gt;T	1&#xD;&#xA;    ST	0	T&gt;A	1&#xD;&#xA;    ST	0	T&gt;C	91&#xD;&#xA;    ST	0	T&gt;G	0&#xD;&#xA;    # DP, Depth distribution&#xD;&#xA;    # DP	[2]id	[3]bin	[4]number of genotypes	[5]fraction of genotypes (%)	[6]number of sites	[7]fraction of sites (%)&#xD;&#xA;    DP	0	&gt;500	0	0.000000	333	100.000000&#xD;&#xA;&#xD;&#xA;" />
  <row Id="793" PostHistoryTypeId="2" PostId="269" RevisionGUID="c6658448-3367-4a9a-999a-85eadcee0af6" CreationDate="2017-05-29T05:23:32.083" UserId="27" Text="You might want to try [Bio-VCF][1]. From the authors description &#xD;&#xA; &#xD;&#xA;&gt; Bio-vcf is a new generation VCF parser, filter and converter. Bio-vcf&#xD;&#xA;&gt; is not only very fast for genome-wide (WGS) data, it also comes with a&#xD;&#xA;&gt; really nice filtering, evaluation and rewrite language and it can&#xD;&#xA;&gt; output any type of textual data, including VCF header and contents in&#xD;&#xA;&gt; RDF and JSON.&#xD;&#xA;&#xD;&#xA;In addition it is a very fast parser. The rewrite DSL might help you customise your filtering and needs. &#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/pjotrp/bioruby-vcf" />
  <row Id="795" PostHistoryTypeId="2" PostId="270" RevisionGUID="fd76e86c-8e66-4d82-b4af-721e23321d76" CreationDate="2017-05-29T06:22:59.417" UserId="57" Text="The &quot;my own parser&quot; solution:&#xD;&#xA;&#xD;&#xA;The information I was searching for in part of column `INFO`, namely in variables `SVLEN` and `SVTYPE`.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    SV_colnames &lt;- c('CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO', 'FORMAT', 'SAMPLE1')&#xD;&#xA;&#xD;&#xA;    ssplit &lt;- function(s, split = '='){&#xD;&#xA;        unlist(strsplit(s, split = split))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;    # note, capital letters just respect original naming conventions of the VCF file&#xD;&#xA;    getSVTYPE &lt;- function(info){&#xD;&#xA;        ssplit(grep(&quot;SVTYPE&quot;, info, value = T))[2]&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    getSVLEN &lt;- function(info){&#xD;&#xA;        SVLEN &lt;- ssplit(grep(&quot;SVLEN&quot;, info, value = T))&#xD;&#xA;        ifelse(length(SVLEN) == 0, NA, as.numeric(SVLEN[2]))&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    load_sv &lt;- function(file){&#xD;&#xA;        vcf_sv_table &lt;- read.table(file, stringsAsFactors = F)&#xD;&#xA;        colnames(vcf_sv_table) &lt;- SV_colnames&#xD;&#xA;        # possible filtering&#xD;&#xA;        # vcf_sv_table &lt;- vcf_sv_table[vcf_sv_table$FILTER == 'PASS',]&#xD;&#xA;        vcf_sv_table_info &lt;- strsplit(vcf_sv_table$INFO, ';')&#xD;&#xA;        vcf_sv_table$SVTYPE &lt;- unlist(lapply(vcf_sv_table_info, getSVTYPE))&#xD;&#xA;        vcf_sv_table$SVLEN &lt;- unlist(lapply(vcf_sv_table_info, getSVLEN))&#xD;&#xA;        return(vcf_sv_table)&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;    sv_df &lt;- load_sv('my_sv_calls.vcf')&#xD;&#xA;    table(sv_df$SVTYPE)" />
  <row Id="796" PostHistoryTypeId="2" PostId="271" RevisionGUID="4344de2e-e08d-4076-b67f-546a4fc67dbd" CreationDate="2017-05-29T06:25:30.990" UserId="174" Text="&gt; http://www.internationalgenome.org/wiki/Analysis/Variant%20Call%20Format/VCF%20%28Variant%20Call%20Format%29%20version%204.0/encoding-structural-variants&#xD;&#xA;&#xD;&#xA;has a sample for encoding structural variants in the VCF 4.0 format.&#xD;&#xA;&#xD;&#xA;An example from the site:&#xD;&#xA;&#xD;&#xA;    #CHROM  POS   ID  REF ALT   QUAL  FILTER  INFO  FORMAT  NA00001&#xD;&#xA;    1 2827693   . CCGTGGATGCGGGGACCCGCATCCCCTCTCCCTTCACAGCTGAGTGACCCACATCCCCTCTCCCCTCGCA  C . PASS  SVTYPE=DEL;END=2827680;BKPTID=Pindel_LCS_D1099159;HOMLEN=1;HOMSEQ=C;SVLEN=-66 GT:GQ 1/1:13.9&#xD;&#xA;&#xD;&#xA;How to read it? From what I can see:&#xD;&#xA;&#xD;&#xA; - This is a deletion&#xD;&#xA; - The reference starts from `2827693 ` to `2827680` (13 bases on the reverse strand)&#xD;&#xA; - The difference between reference and alternative is `66` bases&#xD;&#xA;&#xD;&#xA;This doesn't sound right to me. For instance, I don't see where exactly the deletion starts. The `SVLEN` field says `66` bases deleted, but where? `2827693` to `2827680` only has 13 bases between.&#xD;&#xA;&#xD;&#xA;**Q:** How to read the deletion correctly from this structural VCF record?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="797" PostHistoryTypeId="1" PostId="271" RevisionGUID="4344de2e-e08d-4076-b67f-546a4fc67dbd" CreationDate="2017-05-29T06:25:30.990" UserId="174" Text="How to read structural variant VCF?" />
  <row Id="798" PostHistoryTypeId="3" PostId="271" RevisionGUID="4344de2e-e08d-4076-b67f-546a4fc67dbd" CreationDate="2017-05-29T06:25:30.990" UserId="174" Text="&lt;vcf&gt;&lt;structural&gt;" />
  <row Id="799" PostHistoryTypeId="5" PostId="251" RevisionGUID="51f8cde2-cc8e-4626-be29-cf14d3ac4343" CreationDate="2017-05-29T06:36:18.757" UserId="215" Comment="Tidy title" Text="Does [GISTIC 2.0][1] estimate the background model:&#xD;&#xA;&#xD;&#xA;`G = -log(Probability | Background)`&#xD;&#xA;&#xD;&#xA;by permuting within the sample or across all samples in the set?&#xD;&#xA;&#xD;&#xA;The paper describes the probabilistic scoring method based on permutations, but I could not understand if this permutation is performed within the sample only. The [documentation page][2] seems to suggest across samples, but this would mean that different sized sets might lead to different outcomes on the sample sample.&#xD;&#xA;&#xD;&#xA;Basically, does it matter if the set is constituted by, for example, 10 samples of the same tissue of origin or ~1000 from multiple tissues?&#xD;&#xA;&#xD;&#xA;Thanks,&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3218867/&#xD;&#xA;  [2]: ftp://ftp.broadinstitute.org/pub/GISTIC2.0/GISTICDocumentation_standalone.htm" />
  <row Id="800" PostHistoryTypeId="4" PostId="251" RevisionGUID="51f8cde2-cc8e-4626-be29-cf14d3ac4343" CreationDate="2017-05-29T06:36:18.757" UserId="215" Comment="Tidy title" Text="Does GISTIC (v 2.0) estimate amplified/deleted probabilities on a single sample basis?" />
  <row Id="801" PostHistoryTypeId="24" PostId="251" RevisionGUID="51f8cde2-cc8e-4626-be29-cf14d3ac4343" CreationDate="2017-05-29T06:36:18.757" Comment="Proposed by 215 approved by 57, 77 edit id of 79" />
  <row Id="802" PostHistoryTypeId="6" PostId="271" RevisionGUID="52c5cd1d-0072-4a85-8bf8-5b4ef0657c02" CreationDate="2017-05-29T06:38:07.580" UserId="174" Comment="edited tags" Text="&lt;vcf&gt;&lt;variants&gt;&lt;structural&gt;" />
  <row Id="803" PostHistoryTypeId="5" PostId="271" RevisionGUID="52c5cd1d-0072-4a85-8bf8-5b4ef0657c02" CreationDate="2017-05-29T06:38:07.580" UserId="174" Comment="edited tags" Text="&gt; http://www.internationalgenome.org/wiki/Analysis/Variant%20Call%20Format/VCF%20%28Variant%20Call%20Format%29%20version%204.0/encoding-structural-variants&#xD;&#xA;&#xD;&#xA;has a sample for encoding structural variants in the VCF 4.0 format.&#xD;&#xA;&#xD;&#xA;An example from the site (the first record):&#xD;&#xA;&#xD;&#xA;    #CHROM  POS   ID  REF ALT   QUAL  FILTER  INFO  FORMAT  NA00001&#xD;&#xA;    1 2827693   . CCGTGGATGCGGGGACCCGCATCCCCTCTCCCTTCACAGCTGAGTGACCCACATCCCCTCTCCCCTCGCA  C . PASS  SVTYPE=DEL;END=2827680;BKPTID=Pindel_LCS_D1099159;HOMLEN=1;HOMSEQ=C;SVLEN=-66 GT:GQ 1/1:13.9&#xD;&#xA;&#xD;&#xA;How to read it? From what I can see:&#xD;&#xA;&#xD;&#xA; - This is a deletion (`SVTYPE=DEL`)&#xD;&#xA; - The reference starts from `2827693 ` to `2827680` (13 bases on the reverse strand)&#xD;&#xA; - The difference between reference and alternative is `66` bases (`SVLEN=-66`)&#xD;&#xA;&#xD;&#xA;This doesn't sound right to me. For instance, I don't see where exactly the deletion starts. The `SVLEN` field says `66` bases deleted, but where? `2827693` to `2827680` only has 13 bases between.&#xD;&#xA;&#xD;&#xA;**Q:** How to read the deletion correctly from this structural VCF record?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="804" PostHistoryTypeId="5" PostId="271" RevisionGUID="7e829197-a2d5-4fdf-8cfd-89475aa522ae" CreationDate="2017-05-29T06:43:51.747" UserId="174" Comment="added 71 characters in body" Text="&gt; http://www.internationalgenome.org/wiki/Analysis/Variant%20Call%20Format/VCF%20%28Variant%20Call%20Format%29%20version%204.0/encoding-structural-variants&#xD;&#xA;&#xD;&#xA;has a sample for encoding structural variants in the VCF 4.0 format.&#xD;&#xA;&#xD;&#xA;An example from the site (the first record):&#xD;&#xA;&#xD;&#xA;    #CHROM  POS   ID  REF ALT   QUAL  FILTER  INFO  FORMAT  NA00001&#xD;&#xA;    1 2827693   . CCGTGGATGCGGGGACCCGCATCCCCTCTCCCTTCACAGCTGAGTGACCCACATCCCCTCTCCCCTCGCA  C . PASS  SVTYPE=DEL;END=2827680;BKPTID=Pindel_LCS_D1099159;HOMLEN=1;HOMSEQ=C;SVLEN=-66 GT:GQ 1/1:13.9&#xD;&#xA;&#xD;&#xA;How to read it? From what I can see:&#xD;&#xA;&#xD;&#xA; - This is a deletion (`SVTYPE=DEL`)&#xD;&#xA; - The end position of the variant comes before the starting position (reverse strand?)&#xD;&#xA; - The reference starts from `2827693 ` to `2827680` (13 bases on the reverse strand)&#xD;&#xA; - The difference between reference and alternative is `66` bases (`SVLEN=-66`)&#xD;&#xA;&#xD;&#xA;This doesn't sound right to me. For instance, I don't see where exactly the deletion starts. The `SVLEN` field says `66` bases deleted, but where? `2827693` to `2827680` only has 13 bases between.&#xD;&#xA;&#xD;&#xA;**Q:** How to read the deletion correctly from this structural VCF record? Where is the missing 66-13=53 bases?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="808" PostHistoryTypeId="34" PostId="107" RevisionGUID="c1b191a4-d352-4cae-a699-017375abfdfe" CreationDate="2017-05-29T11:28:57.753" UserId="182" Comment="1" />
  <row Id="811" PostHistoryTypeId="2" PostId="272" RevisionGUID="12d46200-fa95-42e5-9c4f-9b981a8b897d" CreationDate="2017-05-29T14:47:26.867" UserId="37" Text="Another approach is [htsbox](https://github.com/lh3/htsbox). You can get a candidate list with:&#xD;&#xA;&#xD;&#xA;    htsbox pileup -Cvcf ref.fa -q20 -Q20 -s5 file.bam &gt; out.vcf&#xD;&#xA;&#xD;&#xA;Here, `-q` sets min mapping quality, `-Q` sets min base quality, `-v` outputs variants only `-c` outputs VCF, `-C` gives you base counts on both strands and finally `-s5` requires at least 5 high-quality bases to call out an allele. It is useful when your data are failing the assumptions made by typical variant callers.&#xD;&#xA;&#xD;&#xA;Why not samtools+bcftools or varscan? Transparency and speed. This command line simply counts based on the parameters you use. It does not apply additional operations. And because of this, it is over an order of magnitude faster than samtools mpileup or varscan. It is worth noting that samtools uses BAQ by default, which reduces FPs occasionally. However, BAQ is not quite necessary for longer Illumina reads and it hurts sensitivity at the same time." />
  <row Id="812" PostHistoryTypeId="5" PostId="67" RevisionGUID="dcbda075-e9e8-4610-b4e0-a7011ca75f74" CreationDate="2017-05-29T14:50:26.827" UserId="191" Comment="convert back to the data.frame" Text="RPKM is defined as:&#xD;&#xA;&#xD;&#xA;&gt;RPKM =   numberOfReads / ( geneLength/1000 * totalNumReads/1,000,000 )&#xD;&#xA;&#xD;&#xA;As you can see, you need to have gene lengths for every gene.&#xD;&#xA;&#xD;&#xA;Let's say `geneLength` is a vector which have the same number of rows as your `data.frame`, and every value of the vector corresponds to a gene (row) in `expression`.&#xD;&#xA;&#xD;&#xA;    # compute number of reads in each sample&#xD;&#xA;    totalNumReads &lt;- colSums(expression)&#xD;&#xA;    # compute RPKM&#xD;&#xA;    expression.rpkm &lt;- data.frame(sapply(expression, function(column) 10^9 * column / geneLength / sum(column)))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Regarding numerical stability**&#xD;&#xA;&#xD;&#xA;It is suggested in [one of the answers][1], that all the computations should be performed in a log-transformed scale. In my opinion there is absolutely no reason for doing that. IEEE [binary64][2] stores a number as binary number 1.b_{51}b{50}...b_0 times 2^{e-1023}. The *relative* precision doesn't depend on the exponent value given that a number is in the range [~10^-308; 10^308].&#xD;&#xA;&#xD;&#xA;In case of RPKM we can only get out of the range if total number of reads is around 10^300, which is not realistic at all.&#xD;&#xA;&#xD;&#xA;On the bright site there is not much harm in doing computations in the log-scale either. Worst case you loose a bit of precision.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/69/191&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Double-precision_floating-point_format" />
  <row Id="814" PostHistoryTypeId="2" PostId="273" RevisionGUID="71238fce-7254-4fe8-aa6e-371ce77a82f1" CreationDate="2017-05-29T19:09:21.650" UserId="425" Text="**Simulators designed specifically for Oxford Nanopore:**&#xD;&#xA;&#xD;&#xA; - [NanoSim](https://github.com/bcgsc/NanoSim)&#xD;&#xA; - [NanoSim-H](https://github.com/karel-brinda/nanosim-h)&#xD;&#xA; - [SiLiCO](https://github.com/ethanagbaker/SiLiCO)&#xD;&#xA; - [ReadSim](https://sourceforge.net/p/readsim/wiki/Home/)&#xD;&#xA;&#xD;&#xA;**General long read simulators:**&#xD;&#xA;&#xD;&#xA; - [Loresim](https://github.com/gt1/loresim)&#xD;&#xA; - [Loresim 2](https://github.com/gt1/loresim2)&#xD;&#xA; - [FASTQsim](https://sourceforge.net/projects/fastqsim/)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;For an exhaustive list of existing read simulators, see page 15 of this [thesis](http://brinda.cz/publications/brinda_phd.pdf)." />
  <row Id="815" PostHistoryTypeId="5" PostId="273" RevisionGUID="3d326ad6-f949-41a6-bcc3-d472e891f4a0" CreationDate="2017-05-29T19:14:45.463" UserId="425" Comment="Add LongISLND" Text="**Simulators designed specifically for Oxford Nanopore:**&#xD;&#xA;&#xD;&#xA; - [NanoSim](https://github.com/bcgsc/NanoSim)&#xD;&#xA; - [NanoSim-H](https://github.com/karel-brinda/nanosim-h)&#xD;&#xA; - [SiLiCO](https://github.com/ethanagbaker/SiLiCO)&#xD;&#xA; - [ReadSim](https://sourceforge.net/p/readsim/wiki/Home/)&#xD;&#xA;&#xD;&#xA;**General long read simulators:**&#xD;&#xA;&#xD;&#xA; - [Loresim](https://github.com/gt1/loresim)&#xD;&#xA; - [Loresim 2](https://github.com/gt1/loresim2)&#xD;&#xA; - [FASTQsim](https://sourceforge.net/projects/fastqsim/)&#xD;&#xA; - [LongISLND](https://github.com/bioinform/longislnd)&#xD;&#xA;&#xD;&#xA;For an exhaustive list of existing read simulators, see page 15 of this [thesis](http://brinda.cz/publications/brinda_phd.pdf)." />
  <row Id="817" PostHistoryTypeId="2" PostId="274" RevisionGUID="9f898431-904c-4d19-a92d-e657746ee39e" CreationDate="2017-05-29T19:46:25.187" UserId="425" Text="You can split your FASTA file sequence-by-sequence using&#xD;&#xA;&#xD;&#xA;`split -a 9 -p '^&gt;' your_file.fa`&#xD;&#xA;&#xD;&#xA;and then use any existing read simulator supporting coverage (ART, DWGsim, etc.). If you want to have all the reads mixed (not ordered by the original sequence), you can use RNFtools." />
  <row Id="818" PostHistoryTypeId="5" PostId="274" RevisionGUID="f82807e9-db59-4594-b1a9-1abb44632505" CreationDate="2017-05-29T19:56:33.593" UserId="425" Comment="better names for the output files" Text="You can split your FASTA file sequence-by-sequence using&#xD;&#xA;&#xD;&#xA;`split -a 6 -p '^&gt;' your_file.fa seq_`&#xD;&#xA;&#xD;&#xA;and then use any existing read simulator supporting coverage (ART, DWGsim, etc.). If you want to have all the reads mixed (not ordered by the original sequence), you can use RNFtools." />
  <row Id="819" PostHistoryTypeId="5" PostId="271" RevisionGUID="bad5a19f-947b-4ff9-a0d2-6ee2796f56c1" CreationDate="2017-05-29T21:07:13.593" UserId="125" Comment="Prettified the link" Text="The IGSR has a [sample][1] for encoding structural variants in the VCF 4.0 format.&#xD;&#xA;&#xD;&#xA;An example from the site (the first record):&#xD;&#xA;&#xD;&#xA;    #CHROM  POS   ID  REF ALT   QUAL  FILTER  INFO  FORMAT  NA00001&#xD;&#xA;    1 2827693   . CCGTGGATGCGGGGACCCGCATCCCCTCTCCCTTCACAGCTGAGTGACCCACATCCCCTCTCCCCTCGCA  C . PASS  SVTYPE=DEL;END=2827680;BKPTID=Pindel_LCS_D1099159;HOMLEN=1;HOMSEQ=C;SVLEN=-66 GT:GQ 1/1:13.9&#xD;&#xA;&#xD;&#xA;How to read it? From what I can see:&#xD;&#xA;&#xD;&#xA; - This is a deletion (`SVTYPE=DEL`)&#xD;&#xA; - The end position of the variant comes before the starting position (reverse strand?)&#xD;&#xA; - The reference starts from `2827693 ` to `2827680` (13 bases on the reverse strand)&#xD;&#xA; - The difference between reference and alternative is `66` bases (`SVLEN=-66`)&#xD;&#xA;&#xD;&#xA;This doesn't sound right to me. For instance, I don't see where exactly the deletion starts. The `SVLEN` field says `66` bases deleted, but where? `2827693` to `2827680` only has 13 bases between.&#xD;&#xA;&#xD;&#xA;**Q:** How to read the deletion correctly from this structural VCF record? Where is the missing 66-13=53 bases?&#xD;&#xA;&#xD;&#xA;[1]:http://www.internationalgenome.org/wiki/Analysis/Variant%20Call%20Format/VCF%20%28Variant%20Call%20Format%29%20version%204.0/encoding-structural-variants&#xD;&#xA;" />
  <row Id="820" PostHistoryTypeId="24" PostId="271" RevisionGUID="bad5a19f-947b-4ff9-a0d2-6ee2796f56c1" CreationDate="2017-05-29T21:07:13.593" Comment="Proposed by 125 approved by 37, 77 edit id of 80" />
  <row Id="821" PostHistoryTypeId="5" PostId="263" RevisionGUID="60c2617e-5e57-4ac2-bd24-65fd035a0a9a" CreationDate="2017-05-29T21:34:18.543" UserId="48" Comment="edited body" Text="There doesn't seem to be much differences between GISTIC 1.0 and 2.0 as it says:&#xD;&#xA;&#xD;&#xA;&gt; As with GISTIC 1.0, we obtain P-values for each marker by comparing the score at each locus to a background score distribution generated by random permutation of the marker locations in each sample&#xD;&#xA;&#xD;&#xA;But on the [supplementary material][1] of the GISTIC 1.0 there is a more detailed explanation of the method. See the &quot;Stage 2&quot; section:&#xD;&#xA;&#xD;&#xA;&gt; Second, we compare these G-scores to the distribution of scores expected if only random aberrations were observed. This distribution can be determined by rescoring the genome after permuting marker locations within each sample; we instead derive a semiexact estimate.&#xD;&#xA;&#xD;&#xA;Furthermore in another section (&quot;Stage 2: Aggregation of Data from Different Tumors to Differentiate Between Driver and Passenger Aberrations&quot;) it says:&#xD;&#xA;&#xD;&#xA;&gt;To determine which of the aberrations identified in Stage 1 are likely to represent driver events, we aggregate the data from all tumors used in the analysis to generate summary scores for amplifications, deletions, and LOH. The statistical significance of each score is determined by comparison to the distribution of scores obtained by all permutations of the data (using a semiexact approximation), with correction for multiple hypothesis testing. &#xD;&#xA;&#xD;&#xA;However, the relevant section (of the supplementary materials) seems to be &quot;Null Hypothesis Generation: An Analytic Derivation of the Null Distribution&quot;, where it describes the semiexact approximation used.&#xD;&#xA;&#xD;&#xA;In a [supplementary file][2] it describes it as &quot;Generate all permutations of SNP labels within each sample to simulate datasets with random aberrations&quot;&#xD;&#xA;&#xD;&#xA;**Conclusion**:&#xD;&#xA;&#xD;&#xA;It seems that the background probabilities are calculated within sample.&#xD;&#xA;&#xD;&#xA;I can't say if it matters how the sets are constituted but I would say that the broader the sets are, the better estimation of the structural variations it will perform but the same estimation of the background will be done. &#xD;&#xA;&#xD;&#xA;Ultimately you can check the code of the program, or test with 10 samples and replace one of them to see if the results are changing accordingly. &#xD;&#xA; &#xD;&#xA;  [1]: http://www.pnas.org.sare.upf.edu/content/suppl/2007/11/20/0710052104.DC1#ST&#xD;&#xA;  [2]: http://www.pnas.org.sare.upf.edu/content/suppl/2007/11/20/0710052104.DC1/10052Fig9.pdf" />
  <row Id="822" PostHistoryTypeId="2" PostId="275" RevisionGUID="f65391f3-edb6-424f-9a3f-0010932c25b2" CreationDate="2017-05-29T22:34:37.427" UserId="298" Text="So, first off, as others have pointed out, I'm pretty sure that example is just wrong. At least, the numbers don't match as you've pointed out. &#xD;&#xA;&#xD;&#xA;That said, it is impossible to be sure without showing us the header of the VCF file as well. The INFO field (the 5th field of a VCF file) is very, very variable and depends entirely on the header lines. Each program (or human) implementing a VCF is free to choose to have whatever they feel like in the INFO field. However, each `IDENTIFIER=` needs to have an associated INFO line at the beginning of the file. &#xD;&#xA;&#xD;&#xA;So, the `SVTYPE`, `SVLEN`, `HOMLEN` etc will have commented (start with a `#`) lines at the beginning of the file explaining what these values are. So check those, even though they're relatively standard, you never know, the obvious reading you used might be wrong despite its seeming so reasonable. &#xD;&#xA;&#xD;&#xA;Here's a newer example of a VCF line for an SV taken from the current [VCF specification][1]:&#xD;&#xA;&#xD;&#xA;    ##fileformat=VCFv4.1&#xD;&#xA;    ##fileDate=20100501&#xD;&#xA;    ##reference=1000GenomesPilot-NCBI36&#xD;&#xA;    ##assembly=ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/sv/breakpoint_assemblies.fasta&#xD;&#xA;    ##INFO=&lt;ID=BKPTID,Number=.,Type=String,Description=&quot;ID of the assembled alternate allele in the assembly file&quot;&gt;&#xD;&#xA;    ##INFO=&lt;ID=CIEND,Number=2,Type=Integer,Description=&quot;Confidence interval around END for imprecise variants&quot;&gt;&#xD;&#xA;    ##INFO=&lt;ID=CIPOS,Number=2,Type=Integer,Description=&quot;Confidence interval around POS for imprecise variants&quot;&gt;&#xD;&#xA;    ##INFO=&lt;ID=END,Number=1,Type=Integer,Description=&quot;End position of the variant described in this record&quot;&gt;&#xD;&#xA;    ##INFO=&lt;ID=HOMLEN,Number=.,Type=Integer,Description=&quot;Length of base pair identical micro-homology at event breakpoints&quot;&gt;&#xD;&#xA;    ##INFO=&lt;ID=HOMSEQ,Number=.,Type=String,Description=&quot;Sequence of base pair identical micro-homology at event breakpoints&quot;&gt;&#xD;&#xA;    ##INFO=&lt;ID=SVLEN,Number=.,Type=Integer,Description=&quot;Difference in length between REF and ALT alleles&quot;&gt;&#xD;&#xA;    ##INFO=&lt;ID=SVTYPE,Number=1,Type=String,Description=&quot;Type of structural variant&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=DEL,Description=&quot;Deletion&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=DEL:ME:ALU,Description=&quot;Deletion of ALU element&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=DEL:ME:L1,Description=&quot;Deletion of L1 element&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=DUP,Description=&quot;Duplication&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=DUP:TANDEM,Description=&quot;Tandem Duplication&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=INS,Description=&quot;Insertion of novel sequence&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=INS:ME:ALU,Description=&quot;Insertion of ALU element&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=INS:ME:L1,Description=&quot;Insertion of L1 element&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=INV,Description=&quot;Inversion&quot;&gt;&#xD;&#xA;    ##ALT=&lt;ID=CNV,Description=&quot;Copy number variable region&quot;&gt;&#xD;&#xA;    ##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=&quot;Genotype&quot;&gt;&#xD;&#xA;    ##FORMAT=&lt;ID=GQ,Number=1,Type=Float,Description=&quot;Genotype quality&quot;&gt;&#xD;&#xA;    ##FORMAT=&lt;ID=CN,Number=1,Type=Integer,Description=&quot;Copy number genotype for imprecise events&quot;&gt;&#xD;&#xA;    ##FORMAT=&lt;ID=CNQ,Number=1,Type=Float,Description=&quot;Copy number genotype quality for imprecise events&quot;&gt;&#xD;&#xA;    #CHROM POS     ID        REF             ALT QUAL FILTER INFO                                               FORMAT NA00001&#xD;&#xA;    1      2827694 rs2376870 CGTGGATGCGGGGAC C   .    PASS   SVTYPE=DEL;END=2827708;HOMLEN=1;HOMSEQ=G;SVLEN=-14 GT:GQ  1/1:13.9&#xD;&#xA;    &#xD;&#xA;Note how the numbers do match and also note how each of the subfields in the INFO field is explained with an `##INFO` line. &#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/samtools/hts-specs/blob/master/VCFv4.3.pdf" />
  <row Id="823" PostHistoryTypeId="2" PostId="276" RevisionGUID="8dc96187-4b21-4b2e-a340-5a5293aafe9d" CreationDate="2017-05-29T23:41:52.653" UserId="37" Text="You can't resolve 20kb near identical repeats/segdups with 10kb reads. All you can do is to bet your luck on a few excessively long reads spanning some units by chance. For divergent copies, it is worth looking at [this paper](http://biorxiv.org/content/early/2016/05/14/053256). It uses Illumina reads to identify k-mers in unique regions and ignores non-unique k-mers at the overlapping stage. The paper said that this strategy is better than using standard overlappers, which I buy, but probably it can't resolve a 20kb segdup with a handful of mismatches, either.&#xD;&#xA;&#xD;&#xA;Such mismatch-based approaches always have limitations and may not work for recent segdups/repeats. The ultimate solution is to get long reads, longer than your repeat/segdup units. The ~100kb reads in the [recent preprint](http://biorxiv.org/content/early/2017/04/20/128835) will be a game changer for you. If your ~20kb repeats are not tandem, 10X's ~100kb linked reads may help, too." />
  <row Id="825" PostHistoryTypeId="5" PostId="274" RevisionGUID="7285e8c1-881e-42cf-82ea-cc1f385ee9d7" CreationDate="2017-05-30T03:34:10.090" UserId="425" Comment="Make splitting work on Linux" Text="You can split your FASTA file sequence-by-sequence using&#xD;&#xA;&#xD;&#xA;`split -a 6 -p '^&gt;' your_file.fa seq_`&#xD;&#xA;&#xD;&#xA;and then use any existing read simulator supporting coverage (ART, DWGsim, etc.). If you want to have all the reads mixed (not ordered by the original sequence), you can use RNFtools.&#xD;&#xA;&#xD;&#xA;**Edit 1:**&#xD;&#xA;&#xD;&#xA;As @terdon pointed out, the previous command works on OS X only. An analogical one liner for Linux (but with a slightly different naming scheme using numbers rather than letters) can be&#xD;&#xA;&#xD;&#xA;`csplit -f seq_ -n 6 your_file.fa '/^&gt;/' {*}`&#xD;&#xA;&#xD;&#xA;To make this command work also on OS X, one needs to install coreutils (e.g., using brew) and then use `gcsplit` instead of `csplit`." />
  <row Id="826" PostHistoryTypeId="4" PostId="264" RevisionGUID="899d24bd-47a1-4841-b537-d9c0b37b2b81" CreationDate="2017-05-30T06:22:02.757" UserId="57" Comment="made mroe explicit about version of VCF file in the title." Text="Is there an easy way to create a summary of a VCF file (v4.1) with structural variations?" />
  <row Id="827" PostHistoryTypeId="5" PostId="82" RevisionGUID="1ccb865b-79e9-4bc8-875e-d8a25013099c" CreationDate="2017-05-30T06:23:51.133" UserId="298" Comment="&quot;Software&quot; isn't countable in English (no &quot;a software&quot;), so we need to use clunky workarounds like this one. " Text="I am not aware of any software that can do this directly, but I would split the fasta file into one sequence per file, loop over them in BASH and invoke ART the sequence simulator (or another) on each sequence." />
  <row Id="828" PostHistoryTypeId="24" PostId="82" RevisionGUID="1ccb865b-79e9-4bc8-875e-d8a25013099c" CreationDate="2017-05-30T06:23:51.133" Comment="Proposed by 298 approved by 37, 57 edit id of 81" />
  <row Id="829" PostHistoryTypeId="2" PostId="277" RevisionGUID="aea63b81-f435-44a2-865a-2487dccb9743" CreationDate="2017-05-30T07:06:02.543" UserId="179" Text="I am working on a Illumina sequencing simulator for metagenomics: [InSilicoSeq][1]&#xD;&#xA;&#xD;&#xA;It is still in alpha release and very experimental, but given a multi-fasta and an abundance file, it will generate reads from your input genomes with different coverages.&#xD;&#xA;&#xD;&#xA;From the documentation:&#xD;&#xA;&#xD;&#xA;    iss generate --genomes genomes.fasta --abundance abundance_file.txt \&#xD;&#xA;        --model_file HiSeq2500 --output HiSeq_reads&#xD;&#xA;&#xD;&#xA;Where:&#xD;&#xA;&#xD;&#xA;    # multi-fasta file&#xD;&#xA;    &gt;genome_A&#xD;&#xA;    ATGC...&#xD;&#xA;    &gt;genome_B&#xD;&#xA;    CCGT...&#xD;&#xA;    ...&#xD;&#xA;    &#xD;&#xA;    # abundance file (total abundance must be 1!)&#xD;&#xA;    genome_A    0.2&#xD;&#xA;    genome_B    0.4&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;I didn't design it to work with coverage but rather abundance of the genome in a metagenome, so you might have to do a tiny bit of math ;)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/HadrienG/InSilicoSeq" />
  <row Id="832" PostHistoryTypeId="2" PostId="278" RevisionGUID="03088c43-168d-4c2c-9ef8-a0ae83a88d97" CreationDate="2017-05-30T07:30:54.223" UserId="48" Text="Using a [laser-capture microdissection](https://en.wikipedia.org/wiki/Laser_capture_microdissection) of cells a group of cells stained with the marker of interest was sequenced. In another cohort of patients (this is all  human liver tissue) the whole tissue was sequenced (RNA-seq in both cases)&#xD;&#xA;&#xD;&#xA;Can I estimate the contribution of the cells marked in the whole liver (&quot;weight of these cells&quot; in the liver in words of my PI)?&#xD;&#xA;&#xD;&#xA;My gut feeling is that it can't be done this way, it would require both single cell sequencing and whole tissue sequencing to estimate the contribution of each cell line. But perhaps there is a tool that given the cell lines or the main expression of other cells lines it can be compared to using [GSVA][1] or some similar tool. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.bioconductor.org/packages/GSVA/" />
  <row Id="833" PostHistoryTypeId="1" PostId="278" RevisionGUID="03088c43-168d-4c2c-9ef8-a0ae83a88d97" CreationDate="2017-05-30T07:30:54.223" UserId="48" Text="How to estimate cell line contribution?" />
  <row Id="834" PostHistoryTypeId="3" PostId="278" RevisionGUID="03088c43-168d-4c2c-9ef8-a0ae83a88d97" CreationDate="2017-05-30T07:30:54.223" UserId="48" Text="&lt;rna-seq&gt;&lt;estimation&gt;&lt;cell-line&gt;" />
  <row Id="836" PostHistoryTypeId="2" PostId="280" RevisionGUID="aa45741b-ed5b-46b1-8fe1-d0b5f9640ac2" CreationDate="2017-05-30T09:05:00.953" UserId="161" Text="I have been using [STAR](https://github.com/alexdobin/STAR) for our RNA-Seq samples. The `final.out` log file reports percentage of uniquely mapped reads along with percentage of reads that map to `multiple loci` (less than or equal to 10) and percentage of reads mapping to `too many loci` (greater than 10). However, I want to break down the `multiple loci` part to individual counts: Reads mapping to 2 locations, 3 locations, 4 locations .. 10 locations.&#xD;&#xA;&#xD;&#xA;The `NH` tag seems to be used by `STAR`. However a naive read counting approach results in it reporting more number of reads than total reads.&#xD;&#xA;&#xD;&#xA;For example, my `final.out` looks like this:&#xD;&#xA;&#xD;&#xA;       Mapping speed, Million of reads per hour |       1403.36&#xD;&#xA;&#xD;&#xA;                          Number of input reads |       53015978&#xD;&#xA;                      Average input read length |       26&#xD;&#xA;                                    UNIQUE READS:&#xD;&#xA;                   Uniquely mapped reads number |       368916&#xD;&#xA;                        Uniquely mapped reads % |       0.70%&#xD;&#xA;                          Average mapped length |       26.45&#xD;&#xA;                       Number of splices: Total |       1057&#xD;&#xA;            Number of splices: Annotated (sjdb) |       0&#xD;&#xA;                       Number of splices: GT/AG |       802&#xD;&#xA;                       Number of splices: GC/AG |       1&#xD;&#xA;                       Number of splices: AT/AC |       0&#xD;&#xA;               Number of splices: Non-canonical |       254&#xD;&#xA;                      Mismatch rate per base, % |       0.31%&#xD;&#xA;                         Deletion rate per base |       0.00%&#xD;&#xA;                        Deletion average length |       1.45&#xD;&#xA;                        Insertion rate per base |       0.00%&#xD;&#xA;                       Insertion average length |       1.00&#xD;&#xA;                             MULTI-MAPPING READS:&#xD;&#xA;        Number of reads mapped to multiple loci |       45766732&#xD;&#xA;             % of reads mapped to multiple loci |       86.33%&#xD;&#xA;        Number of reads mapped to too many loci |       3757890&#xD;&#xA;             % of reads mapped to too many loci |       7.09%&#xD;&#xA;                                  UNMAPPED READS:&#xD;&#xA;       % of reads unmapped: too many mismatches |       0.00%&#xD;&#xA;                 % of reads unmapped: too short |       5.89%&#xD;&#xA;                     % of reads unmapped: other |       0.00%&#xD;&#xA; &#xD;&#xA;Counting histogram of number of positions a read maps to using `pysam`:&#xD;&#xA;&#xD;&#xA;    def get_reads_hist(bam): &#xD;&#xA;        bam = pysam.AlignmentFile(bam, 'rb')&#xD;&#xA;        counts = Counter()&#xD;&#xA;        for query in bam.fetch():&#xD;&#xA;            nh_count = Counter(dict(query.get_tags())['NH'])&#xD;&#xA;            counts += nh_count            &#xD;&#xA;        return counts&#xD;&#xA;&#xD;&#xA;results in &#xD;&#xA;&#xD;&#xA;    Counter({1: 330606,&#xD;&#xA;         2: 86772164,&#xD;&#xA;         3: 329,&#xD;&#xA;         4: 38083,&#xD;&#xA;         5: 31,&#xD;&#xA;         6: 1094,&#xD;&#xA;         7: 129,&#xD;&#xA;         8: 50,&#xD;&#xA;         10: 50})&#xD;&#xA;&#xD;&#xA;The count `1` reads are fine even though they do not match the counts in `final.out` file since I am counting a certain category of reads (say those mapping to `tRNA` only), but the reads mapping to 2 locations are highly overestimated." />
  <row Id="837" PostHistoryTypeId="1" PostId="280" RevisionGUID="aa45741b-ed5b-46b1-8fe1-d0b5f9640ac2" CreationDate="2017-05-30T09:05:00.953" UserId="161" Text="Extracting reads mapping to multiple loci" />
  <row Id="838" PostHistoryTypeId="3" PostId="280" RevisionGUID="aa45741b-ed5b-46b1-8fe1-d0b5f9640ac2" CreationDate="2017-05-30T09:05:00.953" UserId="161" Text="&lt;rna-seq&gt;&lt;bam&gt;&lt;samtools&gt;&lt;star&gt;&lt;pysam&gt;" />
  <row Id="839" PostHistoryTypeId="2" PostId="281" RevisionGUID="ab6c8248-ec7d-48a8-99cb-c2426237a533" CreationDate="2017-05-30T09:08:11.583" UserId="77" Text="You almost had the correct python code already, you just need to filter out secondary alignments:&#xD;&#xA;&#xD;&#xA;    def get_reads_hist(bam): &#xD;&#xA;        bam = pysam.AlignmentFile(bam, 'rb')&#xD;&#xA;        counts = Counter()&#xD;&#xA;        for query in bam.fetch():&#xD;&#xA;            if query.is_secondary:&#xD;&#xA;                continue&#xD;&#xA;            nh_count = Counter(dict(query.get_tags())['NH'])&#xD;&#xA;            counts += nh_count            &#xD;&#xA;        return counts&#xD;&#xA;" />
  <row Id="840" PostHistoryTypeId="5" PostId="280" RevisionGUID="6b8d5d3e-eb46-4930-ba61-e1f8952b6176" CreationDate="2017-05-30T09:33:43.417" UserId="77" Comment="Make the question explicit and specify that it's the quantification rather than extraction that's desired." Text="I have been using [STAR](https://github.com/alexdobin/STAR) for our RNA-Seq samples. The `final.out` log file reports percentage of uniquely mapped reads along with percentage of reads that map to `multiple loci` (less than or equal to 10) and percentage of reads mapping to `too many loci` (greater than 10). However, I want to break down the `multiple loci` part to individual counts: Reads mapping to 2 locations, 3 locations, 4 locations .. 10 locations.&#xD;&#xA;&#xD;&#xA;The `NH` tag seems to be used by `STAR`. However a naive read counting approach results in it reporting more number of reads than total reads.&#xD;&#xA;&#xD;&#xA;For example, my `final.out` looks like this:&#xD;&#xA;&#xD;&#xA;       Mapping speed, Million of reads per hour |       1403.36&#xD;&#xA;&#xD;&#xA;                          Number of input reads |       53015978&#xD;&#xA;                      Average input read length |       26&#xD;&#xA;                                    UNIQUE READS:&#xD;&#xA;                   Uniquely mapped reads number |       368916&#xD;&#xA;                        Uniquely mapped reads % |       0.70%&#xD;&#xA;                          Average mapped length |       26.45&#xD;&#xA;                       Number of splices: Total |       1057&#xD;&#xA;            Number of splices: Annotated (sjdb) |       0&#xD;&#xA;                       Number of splices: GT/AG |       802&#xD;&#xA;                       Number of splices: GC/AG |       1&#xD;&#xA;                       Number of splices: AT/AC |       0&#xD;&#xA;               Number of splices: Non-canonical |       254&#xD;&#xA;                      Mismatch rate per base, % |       0.31%&#xD;&#xA;                         Deletion rate per base |       0.00%&#xD;&#xA;                        Deletion average length |       1.45&#xD;&#xA;                        Insertion rate per base |       0.00%&#xD;&#xA;                       Insertion average length |       1.00&#xD;&#xA;                             MULTI-MAPPING READS:&#xD;&#xA;        Number of reads mapped to multiple loci |       45766732&#xD;&#xA;             % of reads mapped to multiple loci |       86.33%&#xD;&#xA;        Number of reads mapped to too many loci |       3757890&#xD;&#xA;             % of reads mapped to too many loci |       7.09%&#xD;&#xA;                                  UNMAPPED READS:&#xD;&#xA;       % of reads unmapped: too many mismatches |       0.00%&#xD;&#xA;                 % of reads unmapped: too short |       5.89%&#xD;&#xA;                     % of reads unmapped: other |       0.00%&#xD;&#xA; &#xD;&#xA;Counting histogram of number of positions a read maps to using `pysam`:&#xD;&#xA;&#xD;&#xA;    def get_reads_hist(bam): &#xD;&#xA;        bam = pysam.AlignmentFile(bam, 'rb')&#xD;&#xA;        counts = Counter()&#xD;&#xA;        for query in bam.fetch():&#xD;&#xA;            nh_count = Counter(dict(query.get_tags())['NH'])&#xD;&#xA;            counts += nh_count            &#xD;&#xA;        return counts&#xD;&#xA;&#xD;&#xA;results in &#xD;&#xA;&#xD;&#xA;    Counter({1: 330606,&#xD;&#xA;         2: 86772164,&#xD;&#xA;         3: 329,&#xD;&#xA;         4: 38083,&#xD;&#xA;         5: 31,&#xD;&#xA;         6: 1094,&#xD;&#xA;         7: 129,&#xD;&#xA;         8: 50,&#xD;&#xA;         10: 50})&#xD;&#xA;&#xD;&#xA;The count `1` reads are fine even though they do not match the counts in `final.out` file since I am counting a certain category of reads (say those mapping to `tRNA` only), but the reads mapping to 2 locations are highly overestimated. Why is that?" />
  <row Id="841" PostHistoryTypeId="4" PostId="280" RevisionGUID="6b8d5d3e-eb46-4930-ba61-e1f8952b6176" CreationDate="2017-05-30T09:33:43.417" UserId="77" Comment="Make the question explicit and specify that it's the quantification rather than extraction that's desired." Text="Quantifying reads mapping to multiple loci" />
  <row Id="842" PostHistoryTypeId="2" PostId="282" RevisionGUID="14c4d79a-11b6-4b73-81cc-467384ae2185" CreationDate="2017-05-30T12:16:20.780" UserId="416" Text="Is there a way to use R for manipulating STRING-DB using bioconductor package STRINGdb to extract a network of about 20.000 proteins with max interactions ? without having to cite the proteins of interest ? &#xD;&#xA;Thank you" />
  <row Id="843" PostHistoryTypeId="1" PostId="282" RevisionGUID="14c4d79a-11b6-4b73-81cc-467384ae2185" CreationDate="2017-05-30T12:16:20.780" UserId="416" Text="Manipulating String database to retreive a large network using R" />
  <row Id="844" PostHistoryTypeId="3" PostId="282" RevisionGUID="14c4d79a-11b6-4b73-81cc-467384ae2185" CreationDate="2017-05-30T12:16:20.780" UserId="416" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;database&gt;&lt;networks&gt;" />
  <row Id="845" PostHistoryTypeId="2" PostId="283" RevisionGUID="e593ee7a-c1a6-47f8-a0d0-f1f64225eba1" CreationDate="2017-05-30T12:28:59.600" UserId="274" Text="I am searching for a multi-omics dataset which includes parallel genomics, transcriptomics and proteomics (eg. [snyderome][1]).&#xD;&#xA;The multi-omics dataset need not be of human samples. Dataset for *E. coli* or yeast would also work.&#xD;&#xA;&#xD;&#xA;Is there any database/repository for multi-omics datasets?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://snyderome.stanford.edu/" />
  <row Id="846" PostHistoryTypeId="1" PostId="283" RevisionGUID="e593ee7a-c1a6-47f8-a0d0-f1f64225eba1" CreationDate="2017-05-30T12:28:59.600" UserId="274" Text="Where can I get multi-omics datasets?" />
  <row Id="847" PostHistoryTypeId="3" PostId="283" RevisionGUID="e593ee7a-c1a6-47f8-a0d0-f1f64225eba1" CreationDate="2017-05-30T12:28:59.600" UserId="274" Text="&lt;multi-omics&gt;" />
  <row Id="849" PostHistoryTypeId="2" PostId="284" RevisionGUID="ee98c016-68b9-4df8-b239-310156e75f1b" CreationDate="2017-05-30T12:53:39.583" UserId="274" Text="I just found answer to my own question.&#xD;&#xA;&#xD;&#xA;There is a database called OmicsDI.&#xD;&#xA;One can search for multi-omics datasets by this link:&#xD;&#xA;http://www.omicsdi.org/search?q=omics_type:%22Multi-Omics%22&#xD;&#xA;&#xD;&#xA;Here's a link of the associated publication for more details: http://biorxiv.org/content/early/2016/04/18/049205" />
  <row Id="850" PostHistoryTypeId="2" PostId="285" RevisionGUID="0cfdf8a1-92de-4369-b79d-0b89f9ccf2b3" CreationDate="2017-05-30T14:35:07.713" UserId="148" Text="I'm interested in obtaining coding sequences of my favourite gene in all individuals from the 1000Genomes (and similar projects). I use GATK to get the right subset of variants, vcf-consensus to map these variants onto the reference genome and finally samtools to extract the individual exons. This works fine if the variants are SNPs but if there are any indels, this changes the coordinates of exons and I end up getting the wrong region. Is there any generic way of remapping genomic coordinates to account for the changes created by indels?" />
  <row Id="851" PostHistoryTypeId="1" PostId="285" RevisionGUID="0cfdf8a1-92de-4369-b79d-0b89f9ccf2b3" CreationDate="2017-05-30T14:35:07.713" UserId="148" Text="Remapping genomic coordinates to account for indels" />
  <row Id="852" PostHistoryTypeId="3" PostId="285" RevisionGUID="0cfdf8a1-92de-4369-b79d-0b89f9ccf2b3" CreationDate="2017-05-30T14:35:07.713" UserId="148" Text="&lt;variants&gt;&lt;structural-variation&gt;" />
  <row Id="853" PostHistoryTypeId="5" PostId="274" RevisionGUID="e936ddc1-2fd4-47f8-8b36-7bfbbd580c1b" CreationDate="2017-05-30T14:51:36.553" UserId="425" Comment="Formatting" Text="You can split your FASTA file sequence-by-sequence using&#xD;&#xA;&#xD;&#xA;`split -a 6 -p '^&gt;' your_file.fa seq_`&#xD;&#xA;&#xD;&#xA;and then use any existing read simulator supporting coverage (ART, DWGsim, etc.). If you want to have all the reads mixed (not ordered by the original sequence), you can use RNFtools.&#xD;&#xA;&#xD;&#xA;**Edit 1:**&#xD;&#xA;&#xD;&#xA;As @terdon pointed out, the previous command works on OS X only. An analogical one liner for Linux (but with a slightly different naming scheme using numbers rather than letters) can be&#xD;&#xA;&#xD;&#xA;`csplit -f seq_ -n 6 your_file.fa '/^&gt;/' {*}`&#xD;&#xA;&#xD;&#xA;To make this command work also on OS X, one needs to install coreutils (e.g., using brew) and then use `gcsplit` instead of `csplit`.&#xD;&#xA;&#xD;&#xA;**Edit 2:**&#xD;&#xA;&#xD;&#xA;Once FASTA is split by sequences, the simulation becomes straightforward and many different approaches can be used. My favorite one is using GNU Parallel. Imagine that you have your coverages in a text file called `covs.txt` on separate lines and in the same order as the sequences in `your_file.fa`, e.g.,&#xD;&#xA;&#xD;&#xA;`40&#xD;&#xA;30&#xD;&#xA;...&#xD;&#xA;`&#xD;&#xA;&#xD;&#xA;Then you can simulate reads from the original sequences using DWGsim by&#xD;&#xA;&#xD;&#xA;`ls -1 seq_* | paste covs.txt - |  parallel -v --colsep '\t' dwgsim -1 100 -2 100 -C {1} {2} sim_{2}&#xD;&#xA;`&#xD;&#xA;&#xD;&#xA;and merge the obtained FASTQ files using:&#xD;&#xA;&#xD;&#xA;`cat sim_seq_*.bwa.read1.fastq &gt; reads1.fq&#xD;&#xA;cat sim_seq_*.bwa.read2.fastq &gt; reads2.fq&#xD;&#xA;`&#xD;&#xA;&#xD;&#xA;A possible dangerous of this approach is that we have assumed that the number of seq_* files is the same as the number of lines in `covs.txt`, which might not be true. We should check this prior to the simulation step, e.g., by:&#xD;&#xA;&#xD;&#xA;`[[ &quot;$(ls -1 seq_* | wc -l)&quot; == &quot;$(cat covs.txt | wc -l)&quot; ]] || echo &quot;Incorrent number of lines in covs.txt&quot;&#xD;&#xA;`&#xD;&#xA;&#xD;&#xA;Another caveat is that the simulated reads are not in a random order (they are grouped by source sequences)." />
  <row Id="854" PostHistoryTypeId="5" PostId="274" RevisionGUID="569ca809-249a-4822-b134-ecec1c595416" CreationDate="2017-05-30T15:00:00.753" UserId="425" Comment="Typo" Text="You can split your FASTA file sequence-by-sequence using&#xD;&#xA;&#xD;&#xA;    split -a 6 -p '^&gt;' your_file.fa seq_&#xD;&#xA;&#xD;&#xA;and then use any existing read simulator supporting coverage (ART, DWGsim, etc.). If you want to have all the reads mixed (not ordered by the original sequence), you can use RNFtools.&#xD;&#xA;&#xD;&#xA;**Edit 1:**&#xD;&#xA;&#xD;&#xA;As @terdon pointed out, the previous command works on OS X only. An analogical one liner for Linux (but with a slightly different naming scheme using numbers rather than letters) can be&#xD;&#xA;&#xD;&#xA;    csplit -f seq_ -n 6 your_file.fa '/^&gt;/' {*}&#xD;&#xA;&#xD;&#xA;To make this command work also on OS X, one needs to install coreutils (e.g., using brew) and then use `gcsplit` instead of `csplit`.&#xD;&#xA;&#xD;&#xA;**Edit 2:**&#xD;&#xA;&#xD;&#xA;Once FASTA is split by sequences, the simulation becomes straightforward and many different approaches can be used. My favorite one is using GNU Parallel. Imagine that you have your coverages in a text file called `covs.txt` on separate lines and in the same order as the sequences in `your_file.fa`, e.g.,&#xD;&#xA;&#xD;&#xA;    40&#xD;&#xA;    30&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;Then you can simulate reads from the original sequences using DWGsim by&#xD;&#xA;&#xD;&#xA;    ls -1 seq_* | paste covs.txt - \&#xD;&#xA;        | parallel -v --colsep '\t' dwgsim -1 100 -2 100 -C {1} {2} sim_{2}&#xD;&#xA;&#xD;&#xA;and merge the obtained FASTQ files using:&#xD;&#xA;&#xD;&#xA;    cat sim_seq_*.bwa.read1.fastq &gt; reads1.fq&#xD;&#xA;    cat sim_seq_*.bwa.read2.fastq &gt; reads2.fq&#xD;&#xA;&#xD;&#xA;One possible danger of this approach is that we have assumed that the number of seq_* files is the same as the number of lines in `covs.txt`, which might not be true. We should check this prior to the simulation step, e.g., by:&#xD;&#xA;&#xD;&#xA;    [[ &quot;$(ls -1 seq_* | wc -l)&quot; == &quot;$(cat covs.txt | wc -l)&quot; ]] \&#xD;&#xA;        || echo &quot;Incorrent number of lines in covs.txt&quot;&#xD;&#xA;&#xD;&#xA;Another caveat is that the simulated reads are not in a random order (they are grouped by source sequences).&#xD;&#xA;" />
  <row Id="855" PostHistoryTypeId="2" PostId="286" RevisionGUID="971bca24-8cc4-4563-93da-18357316c6a9" CreationDate="2017-05-30T15:23:33.850" UserId="425" Text="Just a quick pointer – I think that you need a LiftOver Chain File to transform your coordinates. You can obtain this file using `bcftools consensus` with the `-c` parameter:&#xD;&#xA;&#xD;&#xA;    -c, --chain &lt;file&gt;         write a chain file for liftover&#xD;&#xA;&#xD;&#xA;With this file in hand, you can transform coordinates in various genomic formats using [CrossMap](http://crossmap.sourceforge.net/)." />
  <row Id="856" PostHistoryTypeId="5" PostId="124" RevisionGUID="cb0d6019-97d4-40d1-9438-be1c75e5c711" CreationDate="2017-05-30T15:24:35.630" UserId="57" Comment="added description" Text="This python script takes a fasta file and tsv file with counts and prints the sequences in the fasta files that many times as it is specified in the tsv file (assuming the format in the question). So if `bar.tsv` and `foo.fasta` will be your files:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    &#xD;&#xA;    repeat = {}&#xD;&#xA;    for line in open(&quot;bar.tsv&quot;):&#xD;&#xA;        seq_id, coverage = line.split()&#xD;&#xA;        repeat[seq_id] = int(coverage)&#xD;&#xA;    &#xD;&#xA;    for seq_record in SeqIO.parse(foo.fasta, &quot;fasta&quot;):&#xD;&#xA;        for i in range(repeat.get(seq_record.name, 0)):&#xD;&#xA;            print(&quot;&gt;&quot;,seq_record.name,&quot;_&quot;,i,sep='')&#xD;&#xA;            print(seq_record.seq)" />
  <row Id="857" PostHistoryTypeId="6" PostId="81" RevisionGUID="b6e45fe5-b0a1-4bde-9a4b-1b0ceba9c15b" CreationDate="2017-05-30T15:28:48.750" UserId="57" Comment="adding format related tag `fasta`" Text="&lt;ngs&gt;&lt;fasta&gt;&lt;genome&gt;&lt;simulation&gt;" />
  <row Id="858" PostHistoryTypeId="5" PostId="274" RevisionGUID="7322df82-87a3-416a-86d1-18d902163c68" CreationDate="2017-05-30T15:38:20.387" UserId="425" Comment="Typo" Text="You can split your FASTA file sequence-by-sequence using&#xD;&#xA;&#xD;&#xA;    split -a 6 -p '^&gt;' your_file.fa seq_&#xD;&#xA;&#xD;&#xA;and then use any existing read simulator supporting coverage (ART, DWGsim, etc.). If you want to have all the reads mixed (not ordered by the original sequence), you can use RNFtools.&#xD;&#xA;&#xD;&#xA;**Edit 1:**&#xD;&#xA;&#xD;&#xA;As @terdon pointed out, the previous command works on OS X only. An analogical one liner for Linux (but with a slightly different naming scheme using numbers rather than letters) can be&#xD;&#xA;&#xD;&#xA;    csplit -f seq_ -n 6 your_file.fa '/^&gt;/' {*}&#xD;&#xA;&#xD;&#xA;To make this command work also on OS X, one needs to install coreutils (e.g., using brew) and then use `gcsplit` instead of `csplit`.&#xD;&#xA;&#xD;&#xA;**Edit 2:**&#xD;&#xA;&#xD;&#xA;Once FASTA is split by sequences, the simulation becomes straightforward and many different approaches can be used. My favorite one is using GNU Parallel. Imagine that you have your coverages in a text file called `covs.txt` on separate lines and in the same order as the sequences in `your_file.fa`, e.g.,&#xD;&#xA;&#xD;&#xA;    40&#xD;&#xA;    30&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;Then you can simulate reads from the original sequences using DWGsim by&#xD;&#xA;&#xD;&#xA;    ls -1 seq_* | paste covs.txt - \&#xD;&#xA;        | parallel -v --colsep '\t' dwgsim -1 100 -2 100 -C {1} {2} sim_{2}&#xD;&#xA;&#xD;&#xA;and merge the obtained FASTQ files using:&#xD;&#xA;&#xD;&#xA;    cat sim_seq_*.bwa.read1.fastq &gt; reads.1.fq&#xD;&#xA;    cat sim_seq_*.bwa.read2.fastq &gt; reads.2.fq&#xD;&#xA;&#xD;&#xA;One possible danger of this approach is that we have assumed that the number of seq_* files is the same as the number of lines in `covs.txt`, which might not be true. We should check this prior to the simulation step, e.g., by:&#xD;&#xA;&#xD;&#xA;    [[ &quot;$(ls -1 seq_* | wc -l)&quot; == &quot;$(cat covs.txt | wc -l)&quot; ]] \&#xD;&#xA;        || echo &quot;Incorrent number of lines in covs.txt&quot;&#xD;&#xA;&#xD;&#xA;Another caveat is that the simulated reads are not in a random order (they are grouped by source sequences).&#xD;&#xA;" />
  <row Id="859" PostHistoryTypeId="5" PostId="286" RevisionGUID="3be55abf-7063-4bee-8c85-3994b536df53" CreationDate="2017-05-30T15:51:07.057" UserId="425" Comment="fix" Text="Just a quick pointer – I think that you need a LiftOver Chain file to transform your coordinates. You can obtain such a file using `bcftools consensus` with the `-c` parameter:&#xD;&#xA;&#xD;&#xA;    -c, --chain &lt;file&gt;         write a chain file for liftover&#xD;&#xA;&#xD;&#xA;Then you can use it to transform coordinates in various genomic formats using [CrossMap](http://crossmap.sourceforge.net/)." />
  <row Id="860" PostHistoryTypeId="6" PostId="283" RevisionGUID="1d4ff953-13b5-49ce-b49a-cbaddb9622f9" CreationDate="2017-05-30T15:53:09.807" UserId="93" Comment="add data-request  tag" Text="&lt;multi-omics&gt;&lt;data-request&gt;" />
  <row Id="861" PostHistoryTypeId="24" PostId="283" RevisionGUID="1d4ff953-13b5-49ce-b49a-cbaddb9622f9" CreationDate="2017-05-30T15:53:09.807" Comment="Proposed by 93 approved by 274 edit id of 82" />
  <row Id="863" PostHistoryTypeId="5" PostId="283" RevisionGUID="4e38c5d5-8be8-43d2-a167-1132afe6be43" CreationDate="2017-05-30T16:15:53.907" UserId="274" Comment="Clarified the question " Text="I am searching for a multi-omics dataset which may include genomics, transcriptomics and proteomics (eg. [snyderome][1]).&#xD;&#xA;I need such a dataset for an introductory data-exploration purpose. So it can be from any host organism and any experimental methods/conditions.&#xD;&#xA;&#xD;&#xA;Is there any database/repository where I can find it?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://snyderome.stanford.edu/" />
  <row Id="865" PostHistoryTypeId="5" PostId="274" RevisionGUID="58c3fd51-41fd-4504-a9cc-a0c48c77927c" CreationDate="2017-05-30T18:09:35.290" UserId="425" Comment="fix" Text="You can split your FASTA file sequence-by-sequence using&#xD;&#xA;&#xD;&#xA;    split -a 6 -p '^&gt;' your_file.fa seq_&#xD;&#xA;&#xD;&#xA;and then use any existing read simulator supporting coverage (ART, DWGsim, etc.). If you want to have all the reads mixed (not ordered by the original sequence), you can use RNFtools.&#xD;&#xA;&#xD;&#xA;**Edit 1:**&#xD;&#xA;&#xD;&#xA;As @terdon pointed out, the previous command works on OS X only. An analogical one liner for Linux (but with a slightly different naming scheme using numbers rather than letters) can be&#xD;&#xA;&#xD;&#xA;    csplit -f seq_ -n 6 your_file.fa '/^&gt;/' {*}&#xD;&#xA;&#xD;&#xA;To make this command work also on OS X, one needs to install coreutils (e.g., using brew) and then use `gcsplit` instead of `csplit`.&#xD;&#xA;&#xD;&#xA;**Edit 2:**&#xD;&#xA;&#xD;&#xA;Once FASTA is split by sequences, the simulation becomes straightforward and many different approaches can be used. My favorite one is using GNU Parallel. Imagine that you have your coverages in a text file called `covs.txt` on separate lines and in the same order as the sequences in `your_file.fa`, e.g.,&#xD;&#xA;&#xD;&#xA;    40&#xD;&#xA;    30&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;Then you can simulate reads from the original sequences using DWGsim by&#xD;&#xA;&#xD;&#xA;    ls -1 seq_* | paste covs.txt - \&#xD;&#xA;        | parallel -v --colsep '\t' dwgsim -1 100 -2 100 -C {1} {2} sim_{2}&#xD;&#xA;&#xD;&#xA;and merge the obtained FASTQ files using:&#xD;&#xA;&#xD;&#xA;    cat sim_seq_*.bwa.read1.fastq &gt; reads.1.fq&#xD;&#xA;    cat sim_seq_*.bwa.read2.fastq &gt; reads.2.fq&#xD;&#xA;&#xD;&#xA;One possible danger of this approach is that we have assumed that the number of seq_* files is the same as the number of lines in `covs.txt`, which might not be true (by mistake). We should check this prior to the simulation step, e.g., by:&#xD;&#xA;&#xD;&#xA;    [[ &quot;$(ls -1 seq_* | wc -l)&quot; == &quot;$(cat covs.txt | wc -l)&quot; ]] \&#xD;&#xA;        || echo &quot;Incorrent number of lines in covs.txt&quot;&#xD;&#xA;&#xD;&#xA;Another caveat is that the simulated reads are not in a random order (they are grouped by source sequences).&#xD;&#xA;" />
  <row Id="866" PostHistoryTypeId="4" PostId="283" RevisionGUID="1c14b61c-9127-4403-9b70-8ca91d78fc7f" CreationDate="2017-05-30T19:07:49.063" UserId="57" Comment="The quesiton rephrased to have an answer." Text="Is there any publicly available multi-omics dataset?" />
  <row Id="868" PostHistoryTypeId="2" PostId="287" RevisionGUID="c4f87b88-dc7d-4845-b989-7f038e45a0ff" CreationDate="2017-05-30T19:18:22.740" UserId="318" Text="It is one of my favorite stories. &#xD;&#xA;&#xD;&#xA;Drop a glance to StereoGene software, it for genomic track correlation.&#xD;&#xA;http://biorxiv.org/content/early/2017/05/25/059584 (http://stereogene.bioinf.fbb.msu.ru/)&#xD;&#xA;&#xD;&#xA;You also can run MACS or another peak caller and estimate the correlation of two interval sets using the GenomtriCorr package&#xD;&#xA;http://genometricorr.sourceforge.net/.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="869" PostHistoryTypeId="2" PostId="288" RevisionGUID="39c127a9-7087-4b6f-af80-431d587f6ddd" CreationDate="2017-05-30T21:18:07.883" UserId="250" Text="I have about 200 short nucleotide motifs (6-12 bp in length) from the human genome, and I'm trying to see how conserved they are across vertebrates. I was thinking that I'd need to make a bed file for each motif that lists all of its occurrences in the human genome. From there, I could map the beds to a bigwig files of PhastCons scores. Does that sound like the best approach? I'm getting stuck at the step of going from motifs to bed files. I've tried using BLAST to find all occurrences of motifs, but their short length is causing issues. I've tried messing with the e-value threshold, word size, and filter parameters, but I still don't get any hits. Is there a work-around for this issue, or should I just rethink my entire approach?" />
  <row Id="870" PostHistoryTypeId="1" PostId="288" RevisionGUID="39c127a9-7087-4b6f-af80-431d587f6ddd" CreationDate="2017-05-30T21:18:07.883" UserId="250" Text="Have DNA motifs 6-12bp long, trying to get conservation scores" />
  <row Id="871" PostHistoryTypeId="3" PostId="288" RevisionGUID="39c127a9-7087-4b6f-af80-431d587f6ddd" CreationDate="2017-05-30T21:18:07.883" UserId="250" Text="&lt;motifs&gt;&lt;conservation&gt;" />
  <row Id="872" PostHistoryTypeId="2" PostId="289" RevisionGUID="06f01ce4-70a0-4a79-835e-6a68a6a75a46" CreationDate="2017-05-30T21:36:46.213" UserId="425" Text="Is there any resource (paper, blogpost, Github gist, etc.) describing the BWA-MEM algorithm for assigning mapping qualities? I believe that a reimplementation of this algorithm in some scripting language could be very useful for the bioinfo community.&#xD;&#xA;&#xD;&#xA;For instance, I sometimes test various mapping methods and some of them tend to find good alignments, but fail in assigning good qualities. Therefore, I would like to re-assign all the mapping qualities in a SAM file with this algorithm.&#xD;&#xA;&#xD;&#xA;I vaguely remember that I have somewhere seen a formula for SE reads. It looked like&#xD;&#xA;&#xD;&#xA;    C * (s_1 - s2)/ s_1,&#xD;&#xA;    &#xD;&#xA;where `s_1` and `s_2` denoted the alignment scores of two best alignments and `C` was a constant.&#xD;&#xA;&#xD;&#xA;Btw. This algorithm must already have been implemented outside BWA, see the BWA-MEM paper: &#xD;&#xA;&gt; GEM does not compute mapping quality. Its&#xD;&#xA;mapping quality is estimated with a BWA-like algorithm with suboptimal&#xD;&#xA;alignments available.&#xD;&#xA;&#xD;&#xA;Nevertheless, the [BWA-MEM paper repo](https://github.com/lh3/mem-paper/) contains only the resulting `.eval` files." />
  <row Id="873" PostHistoryTypeId="1" PostId="289" RevisionGUID="06f01ce4-70a0-4a79-835e-6a68a6a75a46" CreationDate="2017-05-30T21:36:46.213" UserId="425" Text="BWA-MEM mapping qualities" />
  <row Id="874" PostHistoryTypeId="3" PostId="289" RevisionGUID="06f01ce4-70a0-4a79-835e-6a68a6a75a46" CreationDate="2017-05-30T21:36:46.213" UserId="425" Text="&lt;alignment&gt;&lt;mapping&gt;&lt;bwa&gt;" />
  <row Id="875" PostHistoryTypeId="5" PostId="288" RevisionGUID="639b1c13-6f9c-4757-a4bd-37953d8bf49f" CreationDate="2017-05-30T21:40:36.410" UserId="48" Comment="Improved paragraphing, adding link" Text="I have about 200 short nucleotide motifs (6-12 bp in length) from the human genome, and I'm trying to see how conserved they are across vertebrates. &#xD;&#xA;&#xD;&#xA;I was thinking that I'd need to make a bed file for each motif that lists all of its occurrences in the human genome. From there, I could map the beds to a bigwig files of [PhastCons][1] scores. Does that sound like the best approach? &#xD;&#xA;&#xD;&#xA;I'm getting stuck at the step of going from motifs to bed files. I've tried using BLAST to find all occurrences of motifs, but their short length is causing issues.  &#xD;&#xA;I've tried messing with the e-value threshold, word size, and filter parameters, but I still don't get any hits. &#xD;&#xA;&#xD;&#xA;Is there a work-around for this issue, or should I just rethink my entire approach?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://compgen.cshl.edu/phast/phastCons-HOWTO.html" />
  <row Id="876" PostHistoryTypeId="24" PostId="288" RevisionGUID="639b1c13-6f9c-4757-a4bd-37953d8bf49f" CreationDate="2017-05-30T21:40:36.410" Comment="Proposed by 48 approved by 57, 77 edit id of 83" />
  <row Id="877" PostHistoryTypeId="4" PostId="289" RevisionGUID="39286d0b-158d-4d87-af83-1b676cb60057" CreationDate="2017-05-30T21:43:51.863" UserId="425" Comment="edited title" Text="BWA-MEM algorithm for assigning mapping qualities" />
  <row Id="878" PostHistoryTypeId="5" PostId="288" RevisionGUID="12663039-cfc5-42d9-bcac-ef19e8b9e474" CreationDate="2017-05-30T21:52:54.963" UserId="250" Comment="added 82 characters in body" Text="I have about 200 short nucleotide motifs (6-12 bp in length) from the human genome, and I'm trying to see how conserved they are across vertebrates. &#xD;&#xA;&#xD;&#xA;I was thinking that I'd need to make a bed file for each motif that lists all of its occurrences in the human genome. From there, I could map the beds to a bigwig files of [PhastCons][1] scores (essentially doing the reverse of what the PhastCons software was designed to do). Does that sound like the best approach? &#xD;&#xA;&#xD;&#xA;I'm getting stuck at the step of going from motifs to bed files. I've tried using BLAST to find all occurrences of motifs, but their short length is causing issues.  &#xD;&#xA;I've tried messing with the e-value threshold, word size, and filter parameters, but I still don't get any hits. &#xD;&#xA;&#xD;&#xA;Is there a work-around for this issue, or should I just rethink my entire approach?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://compgen.cshl.edu/phast/phastCons-HOWTO.html" />
  <row Id="879" PostHistoryTypeId="2" PostId="290" RevisionGUID="9f6d36ca-0a6f-44a2-8dfe-022f0eb7a133" CreationDate="2017-05-30T21:53:30.923" UserId="44" Text="**In case you have only ACGT in your motifs**&#xD;&#xA;&#xD;&#xA;The short motifs make it sound as if you are in the business of looking for a kmer counter. You can choose to use existing software or build your own.&#xD;&#xA;&#xD;&#xA; 1. Using existing software might be your easiest path. An older post from 2014 will probably give you a first idea what's out there: http://homolog.us/blogs/blog/2014/04/07/kmer-counting-a-2014-recap/ . Note that a couple of algorithms mentioned there got successors, so it is worthwhile digging a bit around. The small kmer size will make most of them usable for your needs.&#xD;&#xA; 2. As the maximum size of your kmers is comparatively small (12 nt need 24 bits, i.e., max 16.7 million entries in your kmer table), you should be able to easily roll your own kmer counting in about any language you like and on about any of nowadays computer. The pseudocode section on the [Wikipedia][1] entry for kmers will give you first pointers for that. Might be a bit more work, but maybe more flexible depending on your needs.&#xD;&#xA;&#xD;&#xA;**In case you have IUPAC bases (N, W, etc.) in your motifs**&#xD;&#xA;&#xD;&#xA;I don't know any any pre-existing software doing what you need. I could imagine that the short motifs make using [regular expressions][2] doable for this kind of search, but I may be wrong. Testing this should be easy in a simple script as all major programming languages have modules or libraries for REs. Even if it should take a couple of hours to run on your data set, that would be good enough for a one-off calculation.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/K-mer&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Regular_expression" />
  <row Id="880" PostHistoryTypeId="5" PostId="289" RevisionGUID="87c9a1b4-f340-4691-9a0f-eec1b9f44359" CreationDate="2017-05-30T21:55:58.467" UserId="425" Comment="Improved question" Text="Is there any resource (paper, blogpost, Github gist, etc.) describing the BWA-MEM algorithm for assigning mapping qualities? I vaguely remember that I have somewhere seen a formula for SE reads, which looked like&#xD;&#xA;&#xD;&#xA;    C * (s_1 - s2)/ s_1,&#xD;&#xA;    &#xD;&#xA;where `s_1` and `s_2` denoted the alignment scores of two best alignments and `C` was a constant.&#xD;&#xA;&#xD;&#xA;I believe that a reimplementation of this algorithm in some scripting language could be very useful for the bioinfo community. For instance, I sometimes test various mapping methods and some of them tend to find good alignments, but fail in assigning good qualities. Therefore, I would like to re-assign all the mapping qualities in a SAM file with the BWA-MEM algorithm.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Btw. This algorithm must already have been implemented outside BWA, see the BWA-MEM paper: &#xD;&#xA;&gt; GEM does not compute mapping quality. Its&#xD;&#xA;mapping quality is estimated with a BWA-like algorithm with suboptimal&#xD;&#xA;alignments available.&#xD;&#xA;&#xD;&#xA;Unfortunately, the [BWA-MEM paper repo](https://github.com/lh3/mem-paper/) contains only the resulting `.eval` files." />
  <row Id="881" PostHistoryTypeId="2" PostId="291" RevisionGUID="a31dfb6f-9704-409c-b5cc-db5bc793ff20" CreationDate="2017-05-30T22:15:53.533" UserId="57" Text="Yes, there bwa-mem was published as a preprint: https://arxiv.org/pdf/1303.3997.pdf" />
  <row Id="882" PostHistoryTypeId="5" PostId="291" RevisionGUID="fe5e1195-29da-4bf7-b1e9-be988328f207" CreationDate="2017-05-30T22:22:34.557" UserId="57" Comment="added second resource" Text="Yes, there bwa-mem was published as a preprint: https://arxiv.org/pdf/1303.3997.pdf&#xD;&#xA;&#xD;&#xA;and there is a decent description of the scoring algorithm directly in the source code of bwa-mem : https://github.com/lh3/bwa/blob/master/bwamem.c" />
  <row Id="883" PostHistoryTypeId="5" PostId="289" RevisionGUID="de6312cc-6526-4189-887a-d61313412990" CreationDate="2017-05-30T22:25:11.557" UserId="57" Comment="fixed typo in equation" Text="Is there any resource (paper, blogpost, Github gist, etc.) describing the BWA-MEM algorithm for assigning mapping qualities? I vaguely remember that I have somewhere seen a formula for SE reads, which looked like&#xD;&#xA;&#xD;&#xA;    C * (s_1 - s_2) / s_1,&#xD;&#xA;    &#xD;&#xA;where `s_1` and `s_2` denoted the alignment scores of two best alignments and `C` was a constant.&#xD;&#xA;&#xD;&#xA;I believe that a reimplementation of this algorithm in some scripting language could be very useful for the bioinfo community. For instance, I sometimes test various mapping methods and some of them tend to find good alignments, but fail in assigning good qualities. Therefore, I would like to re-assign all the mapping qualities in a SAM file with the BWA-MEM algorithm.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Btw. This algorithm must already have been implemented outside BWA, see the BWA-MEM paper: &#xD;&#xA;&gt; GEM does not compute mapping quality. Its&#xD;&#xA;mapping quality is estimated with a BWA-like algorithm with suboptimal&#xD;&#xA;alignments available.&#xD;&#xA;&#xD;&#xA;Unfortunately, the [BWA-MEM paper repo](https://github.com/lh3/mem-paper/) contains only the resulting `.eval` files." />
  <row Id="884" PostHistoryTypeId="2" PostId="292" RevisionGUID="81c749e9-84b7-4c1b-b6ce-bb325619b688" CreationDate="2017-05-30T22:46:52.913" UserId="161" Text="To scan motifs in a sequence (or database) I would use [FIMO](http://meme-suite.org/tools/fimo) which will give you the exact locations of these motifs in your genome.&#xD;&#xA;&#xD;&#xA;Once you have the locations, you can use a `phastCons` bigiwig from UCSC to calculate the basewise conservation scores. However, please remember that `phastCons` scores are smoothed across windows and it might not be the best metric if you are trying to compare the conservation levels at your motif matching sites as compared to the sequences flanking them.&#xD;&#xA;&#xD;&#xA;I wrote a [package](https://github.com/saketkc/moca) a while back to do this, including doing de-novo motif discovery. However, it might be an overkill for your use case. " />
  <row Id="885" PostHistoryTypeId="5" PostId="291" RevisionGUID="425fc470-3f83-459b-8277-0a062c046b29" CreationDate="2017-05-30T22:51:38.550" UserId="48" Comment="Include some description of the algorithm, shorter links" Text="Yes, there bwa-mem was published as a [preprint][1]&#xD;&#xA;&#xD;&#xA;&gt; BWA-MEM’s seed extension differs from the standard seed extension in two aspects. Firstly, suppose at a certain extension step we come to reference&#xD;&#xA;position x with the best extension score achieved at query position y.&#xD;&#xA;&#xD;&#xA;&gt; ...&#xD;&#xA;&#xD;&#xA;&gt; Secondly, while extending a seed, BWA-MEM tries to keep track of the&#xD;&#xA;best extension score reaching the end of the query sequence&#xD;&#xA;&#xD;&#xA;And there is a decent description of the scoring algorithm directly in the source code of [bwa-mem][2] &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://arxiv.org/pdf/1303.3997.pdf&#xD;&#xA;  [2]: https://github.com/lh3/bwa/blob/master/bwamem.c" />
  <row Id="886" PostHistoryTypeId="24" PostId="291" RevisionGUID="425fc470-3f83-459b-8277-0a062c046b29" CreationDate="2017-05-30T22:51:38.550" Comment="Proposed by 48 approved by 57 edit id of 85" />
  <row Id="887" PostHistoryTypeId="6" PostId="288" RevisionGUID="694d879e-c5b4-4df0-8f8b-194c28db177b" CreationDate="2017-05-30T23:42:19.883" UserId="44" Comment="Added k-mer tag" Text="&lt;k-mer&gt;&lt;motifs&gt;&lt;conservation&gt;" />
  <row Id="888" PostHistoryTypeId="24" PostId="288" RevisionGUID="694d879e-c5b4-4df0-8f8b-194c28db177b" CreationDate="2017-05-30T23:42:19.883" Comment="Proposed by 44 approved by 37, 57 edit id of 84" />
  <row Id="889" PostHistoryTypeId="5" PostId="289" RevisionGUID="b5c0ea05-bc33-47dc-a17c-7d63e6249379" CreationDate="2017-05-31T01:01:37.897" UserId="425" Comment="Update: MAQ vs. score" Text="Is there any resource (paper, blogpost, Github gist, etc.) describing the BWA-MEM algorithm for assigning mapping qualities? I vaguely remember that I have somewhere seen a formula for SE reads, which looked like&#xD;&#xA;&#xD;&#xA;    C * (s_1 - s_2) / s_1,&#xD;&#xA;    &#xD;&#xA;where `s_1` and `s_2` denoted the alignment scores of two best alignments and `C` was some constant.&#xD;&#xA;&#xD;&#xA;I believe that a reimplementation of this algorithm in some scripting language could be very useful for the bioinfo community. For instance, I sometimes test various mapping methods and some of them tend to find good alignments, but fail in assigning good qualities. Therefore, I would like to re-assign all the mapping qualities in a SAM file with the BWA-MEM algorithm.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Btw. This algorithm must already have been implemented outside BWA, see the BWA-MEM paper: &#xD;&#xA;&gt; GEM does not compute mapping quality. Its&#xD;&#xA;mapping quality is estimated with a BWA-like algorithm with suboptimal&#xD;&#xA;alignments available.&#xD;&#xA;&#xD;&#xA;Unfortunately, the [BWA-MEM paper repo](https://github.com/lh3/mem-paper/) contains only the resulting `.eval` files.&#xD;&#xA;&#xD;&#xA;**Update:** The question is *not* about the algorithm for computing alignment scores. Mapping qualities and alignment scores are two different things:&#xD;&#xA;&#xD;&#xA; * Alignment score quantifies the similarity between two sequences (e.g., a read and a reference sequence)&#xD;&#xA; * Mapping quality (MAQ) quantifies the probability that a read is aligned to a wrong position.&#xD;&#xA;&#xD;&#xA;Even alignments with high scores can have a very low mapping quality." />
  <row Id="890" PostHistoryTypeId="5" PostId="286" RevisionGUID="655d5d77-9e18-42fc-9054-cfa513f7b597" CreationDate="2017-05-31T01:03:59.797" UserId="425" Comment="Remove &quot;Just a quick pointer&quot;" Text="I think that you need a LiftOver Chain file to transform your coordinates. You can obtain such a file using `bcftools consensus` with the `-c` parameter:&#xD;&#xA;&#xD;&#xA;    -c, --chain &lt;file&gt;         write a chain file for liftover&#xD;&#xA;&#xD;&#xA;Then you can use it to transform coordinates in various genomic formats using [CrossMap](http://crossmap.sourceforge.net/)." />
  <row Id="891" PostHistoryTypeId="5" PostId="292" RevisionGUID="61130dd4-7939-42b4-80ce-b486e52e900a" CreationDate="2017-05-31T01:51:14.620" UserId="161" Comment="deleted 2 characters in body" Text="To scan motifs in a genome (or database) I would use [FIMO](http://meme-suite.org/tools/fimo) which will give you the exact locations of these motifs in your genome.&#xD;&#xA;&#xD;&#xA;Once you have the locations, you can use a `phastCons` bigiwig from UCSC to calculate the basewise conservation scores. However, please remember that `phastCons` scores are smoothed across windows and it might not be the best metric if you are trying to compare the conservation levels at your motif matching sites as compared to the sequences flanking them.&#xD;&#xA;&#xD;&#xA;I wrote a [package](https://github.com/saketkc/moca) a while back to do this, including doing de-novo motif discovery. However, it might be an overkill for your use case. " />
  <row Id="892" PostHistoryTypeId="2" PostId="293" RevisionGUID="9e92878c-6510-4d3b-921e-e20849ff2b0e" CreationDate="2017-05-31T02:30:14.807" UserId="425" Text="A rolling hash function for DNA sequences called [ntHash](http://www.bcgsc.ca/platform/bioinfo/software/nthash) has recently been [published in Bioinformatics](https://doi.org/10.1093/bioinformatics/btw397) and the authors dealt with reverse complements:&#xD;&#xA;&#xD;&#xA;&gt; Using this table, we can easily compute the hash value for the reverse-complement (as well as the canonical form) of a sequence efficiently, without actually reverse- complementing the input sequence, as follows:&#xD;&#xA;..." />
  <row Id="893" PostHistoryTypeId="2" PostId="294" RevisionGUID="d046b816-73bd-42f2-8592-72dd18149375" CreationDate="2017-05-31T06:22:09.180" UserId="191" Text="I vaguely remember, that the original plan of Oxford Nanopore was to provide cheap sequencers (MinION), but charge for base-calling. For that reason the base-calling was performed in the cloud, and the plan was to make it commercial once the technology is established.&#xD;&#xA;&#xD;&#xA;Of course, this limits the potential uses of MinION in the field, since huge areas do not have decent internet connection. Also, not all the data can be legally transferred to a third-party company in the clinical studies.&#xD;&#xA;&#xD;&#xA;For example for the [Ebola paper][1], they had to create a special version of their software:&#xD;&#xA;&#xD;&#xA;&gt; An offline-capable version of MinKNOW, with internet ‘ping’ disabled&#xD;&#xA;&gt; and online updates disabled was made available to us by Oxford&#xD;&#xA;&gt; Nanopore Technologies specifically for the project&#xD;&#xA;&#xD;&#xA;There are couple of third-party base-callers available today. I am aware of [Nanocall][2] and [DeepNano][3], but since they are not official, it can be hard for them to keep-up with the latest versions of sequencers and cells.&#xD;&#xA;&#xD;&#xA;1. Is it possible as of today to use an official base-caller offline without a special arrangement (like the Ebola one).&#xD;&#xA;2. If not, what's the policy of Oxford Nanopore toward third-party base-callers? Are they going to help them, or to sue them eventually?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://dx.doi.org/10.1038/nature16996&#xD;&#xA;  [2]: https://nanoporetech.com/publications/nanocall-open-source-basecaller-oxford-nanopore-sequencing-data&#xD;&#xA;  [3]: https://nanoporetech.com/publications/deepnano-deep-recurrent-neural-networks-base-calling-minion-nanopore-reads" />
  <row Id="894" PostHistoryTypeId="1" PostId="294" RevisionGUID="d046b816-73bd-42f2-8592-72dd18149375" CreationDate="2017-05-31T06:22:09.180" UserId="191" Text="Is it possible to perform MinION sequencing offline?" />
  <row Id="895" PostHistoryTypeId="3" PostId="294" RevisionGUID="d046b816-73bd-42f2-8592-72dd18149375" CreationDate="2017-05-31T06:22:09.180" UserId="191" Text="&lt;nanopore&gt;&lt;minion&gt;&lt;sequencing&gt;" />
  <row Id="896" PostHistoryTypeId="2" PostId="295" RevisionGUID="a0843cbf-3925-45b7-92ad-b3813f390510" CreationDate="2017-05-31T07:09:09.510" UserId="179" Text="In short, yes offline basecalling is supported.&#xD;&#xA;&#xD;&#xA;Oxford nanopore has made available an official offline basecaller, called albacore. If you have an account to the nanpore community website you can download it from [here][1].&#xD;&#xA;&#xD;&#xA;Concerning running it in the field, last time I checked live basecalling was still requiring a lot of computing power, so you might want to run the MinION in the field, but basecall once back a your lab / computing facility.&#xD;&#xA;&#xD;&#xA;I'm not aware of any plans to sue the third-party basecallers, I think Oxford Nanopore is pretty positive about them :-) &#xD;&#xA;&#xD;&#xA;  [1]: https://community.nanoporetech.com/downloads" />
  <row Id="897" PostHistoryTypeId="2" PostId="296" RevisionGUID="c913a29c-47ec-4880-8677-2f543f2bc15e" CreationDate="2017-05-31T09:48:18.563" UserId="73" Text="This is a frequently-asked question within the nanopore community. Oxford Nanopore currently claims that they are able to generate run yields of 10-15 gigabases (e.g. see [here][1] and [here][2]), and yet it's more common to see users only managing in the 1-5 gigabase range.&#xD;&#xA;&#xD;&#xA;  [1]: https://twitter.com/Clive_G_Brown/status/865211116208697344&#xD;&#xA;  [2]: https://twitter.com/Clive_G_Brown/status/860106072081739778" />
  <row Id="898" PostHistoryTypeId="1" PostId="296" RevisionGUID="c913a29c-47ec-4880-8677-2f543f2bc15e" CreationDate="2017-05-31T09:48:18.563" UserId="73" Text="How can I improve the yield of MinION sequencing runs?" />
  <row Id="899" PostHistoryTypeId="3" PostId="296" RevisionGUID="c913a29c-47ec-4880-8677-2f543f2bc15e" CreationDate="2017-05-31T09:48:18.563" UserId="73" Text="&lt;nanopore&gt;&lt;minion&gt;&lt;fastq&gt;&lt;yield&gt;" />
  <row Id="900" PostHistoryTypeId="2" PostId="297" RevisionGUID="0752eee6-675d-4366-a18c-5d699bc71c3c" CreationDate="2017-05-31T09:49:29.980" UserId="383" Text="Often I downloaded datasets from the SRA where the authors failed to mention which adapters were trimmed during the processing?&#xD;&#xA;&#xD;&#xA;Local alignments tend to overcome this obstacle but it feels a bit barbaric.&#xD;&#xA;&#xD;&#xA;fastQC works occasionally to pick them up but sometimes fails to find the actual adapter sequences.&#xD;&#xA;&#xD;&#xA;Usually I ended up looking up the kits they used and trying to grep for all the possible barcodes.&#xD;&#xA;&#xD;&#xA;Is there a more robust/efficient way to do this?" />
  <row Id="901" PostHistoryTypeId="1" PostId="297" RevisionGUID="0752eee6-675d-4366-a18c-5d699bc71c3c" CreationDate="2017-05-31T09:49:29.980" UserId="383" Text="How to systematically detect unknown barcode/adapter sequences within a set of samples?" />
  <row Id="902" PostHistoryTypeId="3" PostId="297" RevisionGUID="0752eee6-675d-4366-a18c-5d699bc71c3c" CreationDate="2017-05-31T09:49:29.980" UserId="383" Text="&lt;barcode&gt;&lt;adapter&gt;&lt;trim&gt;&lt;trimming&gt;&lt;clipping&gt;" />
  <row Id="903" PostHistoryTypeId="2" PostId="298" RevisionGUID="8bb32cff-0a33-4c62-bfb0-8f87024e60d3" CreationDate="2017-05-31T10:02:35.193" UserId="73" Text="Our lab so far hasn't achieved any run yields over 1 gigabase, but we've had trouble extracting good DNA, and all of our runs were carried out before the individual pore blocking software fixes happened (in March 2017). I am hopeful that our next run will be a good one.&#xD;&#xA;&#xD;&#xA;I attended a talk by Josh Quick at PoreCampAU 2017, in which he discussed some common barriers to getting both good sequencing yield and long read length. It mostly boils down to being more careful with the sample preparation. Bear in mind that the MinION will still sequence a dirty sample, it will just be at a reduced yield. Here are my notes from that talk:&#xD;&#xA;&#xD;&#xA; - The hardest thing about MinION sequencing is getting the sample in the first place&#xD;&#xA; - There are lots of different sample types and extraction methods&#xD;&#xA; - You can't get longer reads than what you put in; shit in leads to shit out&#xD;&#xA; - DNA is very stable when not moving, but very sensitive to lateral damage&#xD;&#xA; - The phenol chloroform method of DNA extraction is very good, and can be used with a phase-locked gel to make extraction easier&#xD;&#xA; - A simple salt + alcohol extraction might be the best method for extraction (because it involves the least amount of work on the DNA)&#xD;&#xA; - EDTA (e.g. as found in TE buffer, and many extraction kits) is not compatible with the rapid kit&#xD;&#xA; - The most consistently good Nanopore runs produced by Josh's lab were 1D ligations runs on R9.4; the best overall run was a phenol-chloroform extraction + rapid kit&#xD;&#xA; - John Tyson can tune himself out of a low-yield hole (via software)&#xD;&#xA; - Getting small numbers of short reads is very important&#xD;&#xA; - Suggested (and mostly untested) purification techniques: spin column (60-100kb); ethanol extraction (100-150kb), dialysis (150-250kb); low melting-point agarose plug (~1Mb, DNA extraction in situ)&#xD;&#xA; - The nanopore protocol input is in nanograms, but should really be stated as molarity; the kit expects about 0.2 pmol input&#xD;&#xA; - Picture molecules tethered to the surface of the membrane. You can then see that the flow cell density is independent of the sequence length&#xD;&#xA; - Tapestation, Qubit and Nanodrop are all a good idea; a DNA sample that can pass all three tests will work well: no short-length shoulders by Tapestation, sufficient DNA by Qubit, high purity by Nanodrop&#xD;&#xA; - RNA can interfere with sequencing; digesting RNA is recommended&#xD;&#xA; - Freezing DNA is a really bad idea. The ice crystals are very good at chopping DNA up into small pieces&#xD;&#xA; - DNA that is kept in the fridge is remarkably stable; can be kept for over two years (and probably indefinitely)" />
  <row Id="904" PostHistoryTypeId="5" PostId="296" RevisionGUID="deae6cdd-f9c4-42b1-b47e-c1688ea603ca" CreationDate="2017-05-31T10:03:03.410" UserId="73" Comment="added 39 characters in body" Text="This is a frequently-asked question within the nanopore community. Oxford Nanopore currently claims that they are able to generate run yields of 10-15 gigabases (e.g. see [here][1] and [here][2]), and yet it's more common to see users only managing in the 1-5 gigabase range.&#xD;&#xA;&#xD;&#xA;So why the big difference in yield?&#xD;&#xA;&#xD;&#xA;  [1]: https://twitter.com/Clive_G_Brown/status/865211116208697344&#xD;&#xA;  [2]: https://twitter.com/Clive_G_Brown/status/860106072081739778" />
  <row Id="905" PostHistoryTypeId="2" PostId="299" RevisionGUID="521fef10-b15d-4eae-86ab-2b5abba4d46d" CreationDate="2017-05-31T10:05:01.120" UserId="283" Text="Given WGS data or RNA-seq data, which tools can I use to detect gene fusions?" />
  <row Id="906" PostHistoryTypeId="1" PostId="299" RevisionGUID="521fef10-b15d-4eae-86ab-2b5abba4d46d" CreationDate="2017-05-31T10:05:01.120" UserId="283" Text="Which tools can detect chimeric RNA (fusion genes) from WGS or RNA-Seq data?" />
  <row Id="907" PostHistoryTypeId="3" PostId="299" RevisionGUID="521fef10-b15d-4eae-86ab-2b5abba4d46d" CreationDate="2017-05-31T10:05:01.120" UserId="283" Text="&lt;rna-seq&gt;&lt;wgs&gt;&lt;chimeric-rna&gt;" />
  <row Id="908" PostHistoryTypeId="2" PostId="300" RevisionGUID="c54d57af-101d-4809-88c2-884aa807263f" CreationDate="2017-05-31T10:05:01.120" UserId="283" Text="Most of these use RNA-seq data, some use WGS data, and some use both. They are listed alphabetically. I will add to the list when I discover more.&#xD;&#xA;&#xD;&#xA;1. Barnacle: http://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-14-550&#xD;&#xA;2. Bellerophontes: http://bioinformatics.oxfordjournals.org/content/28/16/2114.long&#xD;&#xA;3. BreakDancer: http://www.nature.com/nmeth/journal/v6/n9/abs/nmeth.1363.html&#xD;&#xA;4. BreakFusion: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3389765/&#xD;&#xA;5. BreakPointer: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3561864/&#xD;&#xA;6. ChimeraScan: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3187648/&#xD;&#xA;7. Comrad: http://bioinformatics.oxfordjournals.org/content/27/11/1481.long&#xD;&#xA;8. CRAC: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4053775/&#xD;&#xA;9. deFuse: http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001138&#xD;&#xA;10. Dissect: http://bioinformatics.oxfordjournals.org/content/28/12/i179.abstract&#xD;&#xA;11. EBARDenovo: http://bioinformatics.oxfordjournals.org/content/early/2013/03/01/bioinformatics.btt092&#xD;&#xA;12. EricScript: http://bioinformatics.oxfordjournals.org/content/28/24/3232&#xD;&#xA;13. FusionAnalyser: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3439881/&#xD;&#xA;14. FusionCatcher: http://biorxiv.org/content/early/2014/11/19/011650.full-text.pdf+html&#xD;&#xA;15. FusionFinder: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3384600/&#xD;&#xA;16. FusionHunter: http://bioinformatics.oxfordjournals.org/content/27/12/1708.long&#xD;&#xA;17. FusionMap: http://bioinformatics.oxfordjournals.org/content/27/14/1922&#xD;&#xA;18. FusionQ: http://www.biomedcentral.com/1471-2105/14/193&#xD;&#xA;19. FusionSeq: http://www.genomebiology.com/2010/11/10/R104&#xD;&#xA;20. IDP-fusion: http://nar.oxfordjournals.org/content/early/2015/06/03/nar.gkv562.full&#xD;&#xA;21. iFUSE: http://bioinformatics.oxfordjournals.org/content/29/13/1700.long&#xD;&#xA;22. InFusion: https://bitbucket.org/kokonech/infusion/wiki/Home&#xD;&#xA;23. INTEGRATE: http://www.ncbi.nlm.nih.gov/pubmed/26556708&#xD;&#xA;24. JAFFA: http://www.genomemedicine.com/content/7/1/43&#xD;&#xA;25. LifeScope: http://www.thermofisher.com/no/en/home/life-science/sequencing/next-generation-sequencing/solid-next-generation-sequencing/solid-next-generation-sequencing-data-analysis-solutions/lifescope-data-analysis-solid-next-generation-sequencing/lifescope-genomic-analysis-software-solid-next-generation-sequencing.html&#xD;&#xA;26. MapSplice: http://www.ncbi.nlm.nih.gov/pubmed/20802226&#xD;&#xA;27. MOJO https://github.com/cband/MOJO&#xD;&#xA;28. nFuse: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3483554/&#xD;&#xA;29. Pegasus: http://www.biomedcentral.com/1752-0509/8/97&#xD;&#xA;30. PRADA: http://www.ncbi.nlm.nih.gov/pubmed/24695405&#xD;&#xA;31. ShortFuse: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3072550/&#xD;&#xA;32. SnowShoes-FTD: http://nar.oxfordjournals.org/content/39/15/e100&#xD;&#xA;33. SOAPFuse: http://www.genomebiology.com/2013/14/2/R12&#xD;&#xA;34. SOAPFusion: http://www.ncbi.nlm.nih.gov/pubmed/24123671&#xD;&#xA;35. STAR: http://bioinformatics.oxfordjournals.org/content/29/1/15&#xD;&#xA;36. STAR-Fusion: https://github.com/STAR-Fusion/STAR-Fusion/wiki&#xD;&#xA;37. TopHat-Fusion: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3245612/&#xD;&#xA;38. TRUP: http://www.genomebiology.com/2015/16/1/7&#xD;&#xA;39. ViralFusionSeq: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3582262/&#xD;&#xA;&#xD;&#xA;**Other useful programs:**&#xD;&#xA;&#xD;&#xA;Chimeraviz (visualization tools for gene fusions): https://www.ncbi.nlm.nih.gov/pubmed/28525538 (disclaimer: I created this)&#xD;&#xA;&#xD;&#xA;Chimera: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4253834/  &#xD;&#xA;OncoFuse: http://bioinformatics.oxfordjournals.org/content/29/20/2539.long  &#xD;&#xA;FuMa: http://bioinformatics.oxfordjournals.org/content/early/2015/12/09/bioinformatics.btv721.abstract" />
  <row Id="909" PostHistoryTypeId="5" PostId="282" RevisionGUID="3e656cdf-890e-4e07-8afd-6a5d06089e7c" CreationDate="2017-05-31T10:23:02.817" UserId="416" Comment="added 41 characters in body" Text="Is there a way to use R for manipulating STRING-DB using bioconductor package STRINGdb to extract a network of about 20.000 proteins with max interactions ? without having to cite the proteins of interest ? &#xD;&#xA;If not is there another way to do so ? &#xD;&#xA;Thank you" />
  <row Id="910" PostHistoryTypeId="2" PostId="301" RevisionGUID="f0cbd10e-0581-4aa7-822b-0ecc3d3b524c" CreationDate="2017-05-31T10:28:26.410" UserId="292" Text="If you happen to know a sequence that should be highly abundant in the library, you can grep its beginning or end and see if the same sequence comes just before or just after respectively." />
  <row Id="911" PostHistoryTypeId="2" PostId="302" RevisionGUID="a1554318-691c-464f-87fe-f2b3890afc79" CreationDate="2017-05-31T10:45:23.277" UserId="73" Text="I'm not aware of any existing methods to do this, but here are a couple of ideas about how it might be done:&#xD;&#xA;&#xD;&#xA;Canu has a method of adapter trimming which involves looking for the absence of overlap for reads. If there are no other reads which share sequence across a particular region, then the read is broken up at the point of low coverage, and small pieces are discarded. It would be possible to use a method like this to hunt for possible adapter/barcode sequences by preserving the short reads.&#xD;&#xA;&#xD;&#xA;Another option is to do a kmer search at the start of reads, and see if any of the high-abundance kmers can be assembled together and/or matched to existing known adapters or barcodes." />
  <row Id="912" PostHistoryTypeId="5" PostId="282" RevisionGUID="39d3d819-6e7b-4356-a059-499c1fc3b52d" CreationDate="2017-05-31T10:53:40.120" UserId="416" Comment="added 81 characters in body" Text="Is there a way to use R for manipulating STRING-DB using bioconductor package STRINGdb to extract a network of about 20.000 proteins with max interactions ? &#xD;&#xA;I have downloaded the whole graph for Homosapiens from STRING.&#xD;&#xA;As I'm newbie in R I don't know :&#xD;&#xA;How do I read the file using that package? &#xD;&#xA;how do I filter things I don't need ?" />
  <row Id="913" PostHistoryTypeId="2" PostId="303" RevisionGUID="512b70ff-601b-4695-aa05-e1e164ba0caa" CreationDate="2017-05-31T11:01:10.113" UserId="442" Text="Microbial genomes can contain extensive duplications. Often we'd like to transfer annotations from an annotated species to one that is newly sequenced. &#xD;&#xA;&#xD;&#xA;Existing tools (e.g. RATT, LiftOver, Kraken) either make specific assumptions about how closely related the species are or fail to transfer when multiple matches are found in the new genome, especially if the sequences are highly similar. &#xD;&#xA;&#xD;&#xA;Are there any pre-existing tools or software that transfer annotations in this scenario? Ideas for ways to do this robustly?" />
  <row Id="914" PostHistoryTypeId="1" PostId="303" RevisionGUID="512b70ff-601b-4695-aa05-e1e164ba0caa" CreationDate="2017-05-31T11:01:10.113" UserId="442" Text="How to transfer gff annotations in genome with extensive duplications?" />
  <row Id="915" PostHistoryTypeId="3" PostId="303" RevisionGUID="512b70ff-601b-4695-aa05-e1e164ba0caa" CreationDate="2017-05-31T11:01:10.113" UserId="442" Text="&lt;annotation&gt;&lt;sequencing&gt;" />
  <row Id="917" PostHistoryTypeId="2" PostId="304" RevisionGUID="cdc8f1cd-21ef-4414-a4fb-c1603065aec8" CreationDate="2017-05-31T11:09:16.597" UserId="73" Text="Short answer: yes, but you need to get permission (and modified software) from ONT before doing that.&#xD;&#xA;&#xD;&#xA;... but that doesn't tell the whole story. This question has the potential to be very confusing, and that's through no fault of the questioner. The question that has been asked in the title is actually slightly different from what's being asked in the body of the question, and I think the &quot;title&quot; question is probably more relevant when considering the field applications of the MinION. The issue is that for the MinION, sequencing (or more specifically, generating the raw data in the form of an electrical signal trace) is distinct and separable from base calling. Many other sequencers also have distinct raw data and base-calling phases, but they're not democratised to the degree they are on the MinION.&#xD;&#xA;&#xD;&#xA;The &quot;sequencing&quot; part of MinION sequencing is carried out by ONT software, namely MinKNOW. As explained to me during PoreCampAU 2017, when the MinION is initially plugged into a computer it is missing the firmware necessary to carry out the sequencing. The most recent version of this firmware is usually downloaded at the start of a sequencing run by sending a request to ONT servers. In the usual case, you can't do sequencing without being able to access those servers, and you can't do sequencing without ONT knowing about it. However, ONT acknowledge that there are people out there who won't have Internet access when sequencing (e.g. sequencing Ebola in Africa, or metagenomic sequencing in the middle of the ocean), and an email to &lt;support@nanoporetech.com&gt; with reasons is likely to result in a quick software fix to the local sequencing problem.&#xD;&#xA;&#xD;&#xA;Once the raw signals are acquired, the &quot;base-calling&quot; part of MinION sequencing can be done anywhere. The ONT-maintained basecaller is Albacore, and this will get the first model updates whenever the sequencing technology is changed (which happens a lot). Albacore is a local basecaller which can be obtained from ONT by browsing through their community pages (available to anyone who has a MinION); ONT switched to only allowing people to do basecalling locally in about April 2017, after establishing that using AWS servers was just too expensive. Albacore is open source and free-as-in-beer, but has a restrictive licensing agreement which limits the distribution (and modification) of the program. However, Albacore is not the only available basecaller. ONT provide a FOSS basecaller called [nanonet][1]. It's a little bit behind Albacore on technology, but ONT have said that all useful Albacore changes will eventually propagate through to nanonet. There is another non-ONT basecaller that I'm aware of which uses a neural network for basecalling: [deepnano][2]. Other basecallers exist, each varying distances away technology-wise, and I expect that more will appear in the future as the technology stabilises and more change-resistant computer scientists get in on the act.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/nanoporetech/nanonet&#xD;&#xA;  [2]: https://bitbucket.org/vboza/deepnano" />
  <row Id="918" PostHistoryTypeId="2" PostId="305" RevisionGUID="b00e7c4a-d7af-4d33-8cb9-eea59e9b5cd5" CreationDate="2017-05-31T11:12:06.970" UserId="191" Text="First, you can get a graph with `STRINGdb` using `get_graph()` method.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;    full.graph &lt;- string_db$get_graph()&#xD;&#xA;&#xD;&#xA;Once this is done, you can use [`igraph`][1], to manipulate the graph. It is not clear from your question, which proteins you want to keep (it would be nice to clarify this in the question).&#xD;&#xA;&#xD;&#xA;Let's assume you want to take 200 proteins with the highest degree, i.e. number of edges they have.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(igraph)&#xD;&#xA;    &#xD;&#xA;    # see how many proteins do you have    &#xD;&#xA;    vcount(full.graph)&#xD;&#xA;    &#xD;&#xA;    # find top 200 proteins with the highest degree&#xD;&#xA;    top.degree.verticies &lt;- tail(sort(degree(full.graph)), 200)&#xD;&#xA;&#xD;&#xA;    # extract the relevant subgraph&#xD;&#xA;    top.subgraph &lt;- induced_subgraph(graph, top.degree.verticies)&#xD;&#xA;    &#xD;&#xA;    # count the number of proteins in it&#xD;&#xA;    vcount(top.subgraph)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://igraph.org/r/&#xD;&#xA;" />
  <row Id="919" PostHistoryTypeId="5" PostId="305" RevisionGUID="fa0d2d5d-9926-4b1a-b58a-f3bd7ab0c14c" CreationDate="2017-05-31T11:17:33.123" UserId="191" Comment="correct R code" Text="First, you can get a graph with `STRINGdb` using `get_graph()` method.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;    full.graph &lt;- string_db$get_graph()&#xD;&#xA;&#xD;&#xA;Once this is done, you can use [`igraph`][1], to manipulate the graph. It is not clear from your question, which proteins you want to keep (it would be nice to clarify this in the question).&#xD;&#xA;&#xD;&#xA;Let's assume you want to take 200 proteins with the highest degree, i.e. number of edges they have.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(igraph)&#xD;&#xA;    &#xD;&#xA;    # see how many proteins do you have    &#xD;&#xA;    vcount(full.graph)&#xD;&#xA;    &#xD;&#xA;    # find top 200 proteins with the highest degree&#xD;&#xA;    top.degree.verticies &lt;- names(tail(sort(degree(full.graph)), 200))&#xD;&#xA;&#xD;&#xA;    # extract the relevant subgraph&#xD;&#xA;    top.subgraph &lt;- induced_subgraph(graph, top.degree.verticies)&#xD;&#xA;    &#xD;&#xA;    # count the number of proteins in it&#xD;&#xA;    vcount(top.subgraph)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://igraph.org/r/&#xD;&#xA;" />
  <row Id="920" PostHistoryTypeId="5" PostId="282" RevisionGUID="033a4a7d-c08d-44b6-a498-43d831076fc9" CreationDate="2017-05-31T11:27:43.870" UserId="416" Comment="added 54 characters in body" Text="Is there a way to use R for manipulating STRING-DB using bioconductor package STRINGdb to extract a network of about 20.000 proteins with max interactions ? &#xD;&#xA;I have downloaded the whole graph for Homosapiens from STRING.&#xD;&#xA;As I'm newbie in R I don't know :&#xD;&#xA;How do I read the file using that package? &#xD;&#xA;how do I filter things I don't need ? Supposing that we want to keep tumor data for example" />
  <row Id="921" PostHistoryTypeId="5" PostId="305" RevisionGUID="e793bd15-1480-462b-af6f-90a5a02f3391" CreationDate="2017-05-31T12:11:00.467" UserId="191" Comment="How to get disease specific genes" Text="First, you can get a graph with `STRINGdb` using `get_graph()` method.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;    full.graph &lt;- string_db$get_graph()&#xD;&#xA;&#xD;&#xA;Once this is done, you can use [`igraph`][1], to manipulate the graph. It is not clear from your question, which proteins you want to keep (it would be nice to clarify this in the question).&#xD;&#xA;&#xD;&#xA;Let's assume you want to take 200 proteins with the highest degree, i.e. number of edges they have.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(igraph)&#xD;&#xA;    &#xD;&#xA;    # see how many proteins do you have    &#xD;&#xA;    vcount(full.graph)&#xD;&#xA;    &#xD;&#xA;    # find top 200 proteins with the highest degree&#xD;&#xA;    top.degree.verticies &lt;- names(tail(sort(degree(full.graph)), 200))&#xD;&#xA;&#xD;&#xA;    # extract the relevant subgraph&#xD;&#xA;    top.subgraph &lt;- induced_subgraph(graph, top.degree.verticies)&#xD;&#xA;    &#xD;&#xA;    # count the number of proteins in it&#xD;&#xA;    vcount(top.subgraph)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Update: How to get disease specific genes?**&#xD;&#xA;&#xD;&#xA;There's no GO annotation for cancer or Alzheimer's disease. It is out of scope of the GO consortium.&#xD;&#xA;&#xD;&#xA;What you can do, you can either take KEGG Pathways annotation, or manually select list of relevant GO-terms. Or acquire the list from one of the papers. For example annotation term `05200` corresponds to the cancer KEGG pathway. You can easily retrieve proteins associated with the annotation:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    cancer.pathway.proteins &lt;-&#xD;&#xA;        string_db$get_term_proteins('05200')$STRING_id&#xD;&#xA;&#xD;&#xA;And then perform subgraphing as described above.&#xD;&#xA;&#xD;&#xA;The problem is that you cannot specify how many genes do you need. And, realistically, I do not think there exists 20k genes associated with cancer, Alzheimer's or any other disease.&#xD;&#xA;&#xD;&#xA;Alternatively you can try to get an enrichment score for an every gene given it's neighbors (the way enrichment is shown on the string-db website). But again, you won't get anything even remotely close to 20k. But at least you can keep only those having top enrichment scores. Probably `get_ppi_enrichment_full` or `get_ppi_enrichment` functions will help you to do that.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://igraph.org/r/&#xD;&#xA;" />
  <row Id="922" PostHistoryTypeId="2" PostId="306" RevisionGUID="23e5c075-afe6-4bcf-955e-837e19e47dcf" CreationDate="2017-05-31T12:17:46.297" UserId="445" Text="I recently used the minION (Nanopore, 9.4 flow cell, RAD001 kit) to generate a metagenome out of environmental samples.&#xD;&#xA;Passed reads weren't brilliant (196, average 1,594bp lenght), but working with [centrifuge][1] the classification outputs turned out to have quite low hitLenght to queryLenght scores (average 2%, max 14%). This plus score values don't give me a lot of confidence towards the results I got.&#xD;&#xA;&#xD;&#xA;Has anyone else used *centrifuge* and experienced the same?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/infphilo/centrifuge" />
  <row Id="923" PostHistoryTypeId="1" PostId="306" RevisionGUID="23e5c075-afe6-4bcf-955e-837e19e47dcf" CreationDate="2017-05-31T12:17:46.297" UserId="445" Text="Rapid metagenomics classifiers on long read data" />
  <row Id="924" PostHistoryTypeId="3" PostId="306" RevisionGUID="23e5c075-afe6-4bcf-955e-837e19e47dcf" CreationDate="2017-05-31T12:17:46.297" UserId="445" Text="&lt;nanopore&gt;&lt;long-reads&gt;&lt;minion&gt;&lt;metagenome&gt;&lt;centrifuge&gt;" />
  <row Id="925" PostHistoryTypeId="5" PostId="306" RevisionGUID="16cdecbd-2c5b-4f2c-a18a-65a4368539a5" CreationDate="2017-05-31T12:22:34.797" UserId="77" Comment="Fixed some small typos" Text="I recently used the minION (Nanopore, 9.4 flow cell, RAD001 kit) to generate a metagenome out of environmental samples.&#xD;&#xA;Passed reads weren't brilliant (196, average 1,594bp lenght), but working with [centrifuge][1] the classification outputs turned out to have quite low hitLength to queryLength scores (average 2%, max 14%). This plus score values don't give me a lot of confidence towards the results I got.&#xD;&#xA;&#xD;&#xA;Has anyone else used *centrifuge* and experienced the same?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/infphilo/centrifuge" />
  <row Id="926" PostHistoryTypeId="5" PostId="297" RevisionGUID="8671e1eb-4750-4ae0-bc8f-6266a692078e" CreationDate="2017-05-31T13:17:49.717" UserId="298" Comment="Copy edited" Text="I have often downloaded datasets from the SRA where the authors failed to mention which adapters were trimmed during the processing.&#xD;&#xA;&#xD;&#xA;Local alignments tend to overcome this obstacle, but it feels a bit barbaric.&#xD;&#xA;&#xD;&#xA;fastQC works occasionally to pick them up, but sometimes fails to find the actual adapter sequences.&#xD;&#xA;&#xD;&#xA;Usually, I ended up looking up the kits they used and trying to grep for all the possible barcodes.&#xD;&#xA;&#xD;&#xA;Is there a more robust/efficient way to do this?" />
  <row Id="927" PostHistoryTypeId="4" PostId="297" RevisionGUID="8671e1eb-4750-4ae0-bc8f-6266a692078e" CreationDate="2017-05-31T13:17:49.717" UserId="298" Comment="Copy edited" Text="How can I systematically detect unknown barcode/adapter sequences within a set of samples?" />
  <row Id="928" PostHistoryTypeId="24" PostId="297" RevisionGUID="8671e1eb-4750-4ae0-bc8f-6266a692078e" CreationDate="2017-05-31T13:17:49.717" Comment="Proposed by 298 approved by 77, 191 edit id of 86" />
  <row Id="929" PostHistoryTypeId="2" PostId="307" RevisionGUID="b60fc77b-f3f2-4dbd-81c0-7ea290821098" CreationDate="2017-05-31T13:40:38.773" UserId="450" Text="I have the following FASTA file, `original.fasta`:&#xD;&#xA;&#xD;&#xA;    &gt;foo&#xD;&#xA;    GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xD;&#xA;&#xD;&#xA;I need to change the record id from `foo` to `bar`, so I wrote the following code:&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    &#xD;&#xA;    original_file = r&quot;path\to\original.fasta&quot;&#xD;&#xA;    corrected_file = r&quot;path\to\corrected.fasta&quot;&#xD;&#xA;    &#xD;&#xA;    with open(original_file) as original, open(corrected_file, 'w') as corrected:&#xD;&#xA;        records = SeqIO.parse(original_file, 'fasta')&#xD;&#xA;        for record in records:&#xD;&#xA;            print record.id             # prints 'foo'&#xD;&#xA;            if record.id == 'foo':&#xD;&#xA;                record.id = 'bar'&#xD;&#xA;            print record.id             # prints 'bar' as expected&#xD;&#xA;            SeqIO.write(record, corrected, 'fasta')&#xD;&#xA;&#xD;&#xA;We printed the record id before and after the change, and get the expected result. We can even doublecheck by reading in the corrected file again with BioPython and printing out the record id:&#xD;&#xA;&#xD;&#xA;    with open(corrected_file) as corrected:&#xD;&#xA;        for record in SeqIO.parse(corrected, 'fasta'):&#xD;&#xA;            print record.id                  # prints 'bar', as expected&#xD;&#xA;      &#xD;&#xA;&#xD;&#xA;However, if we open the corrected file in a text editor, we see that the record id is not `bar` but  `bar foo`:&#xD;&#xA;&#xD;&#xA;    &gt;bar foo&#xD;&#xA;    GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xD;&#xA;&#xD;&#xA;We can confirm that this is what is written to the file if we read the file using plain Python:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    with open(corrected_file) as corrected:&#xD;&#xA;        print corrected.readlines()[0][1:] # prints 'bar foo'&#xD;&#xA;&#xD;&#xA;Is this a bug in BioPython? And if not, what did I do wrong and how do I change the record id in a FASTA file using BioPython?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="930" PostHistoryTypeId="1" PostId="307" RevisionGUID="b60fc77b-f3f2-4dbd-81c0-7ea290821098" CreationDate="2017-05-31T13:40:38.773" UserId="450" Text="Changing the record id in a FASTA file using BioPython" />
  <row Id="931" PostHistoryTypeId="3" PostId="307" RevisionGUID="b60fc77b-f3f2-4dbd-81c0-7ea290821098" CreationDate="2017-05-31T13:40:38.773" UserId="450" Text="&lt;fasta&gt;&lt;biopython&gt;" />
  <row Id="932" PostHistoryTypeId="5" PostId="246" RevisionGUID="3fded0e2-7e83-44ce-a97d-4f62637bed16" CreationDate="2017-05-31T13:46:57.663" UserId="292" Comment="added 251 characters in body" Text="The tricky art of scaling quantitative data across libraries, typically to account for differences in sequencing depth. This can also be about scaling for read source length, like transcript or gene length, in order to enable comparisons across genes." />
  <row Id="933" PostHistoryTypeId="24" PostId="246" RevisionGUID="3fded0e2-7e83-44ce-a97d-4f62637bed16" CreationDate="2017-05-31T13:46:57.663" Comment="Proposed by 292 approved by 55, 77 edit id of 73" />
  <row Id="934" PostHistoryTypeId="2" PostId="308" RevisionGUID="0990d27c-0df4-4688-b426-43d905e9da7f" CreationDate="2017-05-31T13:47:17.630" UserId="425" Text="[SMALT](http://www.sanger.ac.uk/science/tools/smalt-0) seems to be one of the most used read mappers for bacterial data, see, e.g., [this query](https://scholar.google.com/scholar?q=%22smalt%22+bacteria&amp;as_ylo=2016). I do not say that it is not a great mapper, but I cannot easily see what are its main strengths compared to mappers such as BWA-MEM, Bowtie2, NovoAlign or GEM. Moreover, it is not even published. Could you name some of its distinguishing features (e.g., user support by Sanger Pathogens)? So far I have heard only arguments like &quot;We use SMALT because everyone does it.&quot;, but this is not convincing enough for me." />
  <row Id="935" PostHistoryTypeId="1" PostId="308" RevisionGUID="0990d27c-0df4-4688-b426-43d905e9da7f" CreationDate="2017-05-31T13:47:17.630" UserId="425" Text="Why is SMALT so popular in microbial genomics?" />
  <row Id="936" PostHistoryTypeId="3" PostId="308" RevisionGUID="0990d27c-0df4-4688-b426-43d905e9da7f" CreationDate="2017-05-31T13:47:17.630" UserId="425" Text="&lt;mapping&gt;&lt;smalt&gt;" />
  <row Id="937" PostHistoryTypeId="5" PostId="244" RevisionGUID="71f33718-1ce7-484d-927e-8d22dfc7399d" CreationDate="2017-05-31T13:48:58.563" UserId="292" Comment="added 123 characters in body" Text="When dealing with associating sequencing reads to a matching position in a set of reference sequences (typically a genome)." />
  <row Id="938" PostHistoryTypeId="24" PostId="244" RevisionGUID="71f33718-1ce7-484d-927e-8d22dfc7399d" CreationDate="2017-05-31T13:48:58.563" Comment="Proposed by 292 approved by 55, 77 edit id of 72" />
  <row Id="939" PostHistoryTypeId="2" PostId="309" RevisionGUID="89bd76f2-55f0-4070-a68f-505ca5b22e16" CreationDate="2017-05-31T14:04:22.683" UserId="298" Text="I think you will have to first identify the regions homologous to the ones defined in your GFF and then transfer the annotations. Of course, the assumption there is that the homolog will also have the same annotation which is often not true. However, I don't see how you can do it in any other way since you cannot use genomic coordinates (and you would still be making the same assumption even if you could, anyway) when the genomes are so different. &#xD;&#xA;&#xD;&#xA;For a very simplistic approach (which might be enough if, as you say, your sequences are almost identical), you can do something like:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;1. Collect the sequences of interest from your already annotated species. &#xD;&#xA;2. Use a tool like [`genewise`][1] or [`exonerate`][2] to map these into the target genome. Both tools can return gff-formatted output and both can find multiple hits in the target genome. For what you want, I would suggest using a very high threshold of sequence similarity and query coverage (where the target sequence found covers all or most of the query sequence used). &#xD;&#xA;&#xD;&#xA; Since these are microbial genomes and therefore splicing isn't a problem, you could do the same thing with even a simple BLASTn or tBLASTn if you start from protein sequences. &#xD;&#xA;&#xD;&#xA;3. At this point, you should have a list of homologs (some of which will be [orthologs and others paralogs][3]) and you can transfer the annotations of the query sequence over to the target. &#xD;&#xA;&#xD;&#xA;Again, I stress that this is making a whopping huge assumption: that homologous sequences have the same function and can automatically be annotated as whatever you had in the query genome. This is going to be true for many cases but it will also be false for others. Especially if you are looking at paralogs (genes whose duplication occurred after the speciation event and are therefore likely to have diverged in function). &#xD;&#xA;&#xD;&#xA;However, as I said before, this problem would be exactly the same even if you did manage to transfer annotations just by identifying the syntenic regions of the genomes&lt;sup&gt;1&lt;/sup&gt;, so there's not much difference there. &#xD;&#xA;&#xD;&#xA;----&#xD;&#xA;&#xD;&#xA;&lt;sup&gt;1&lt;/sup&gt; &lt;sub&gt;As I said in the comments, I don't see how this could be possible. By definition, if you have extensive duplications, the genomic coordinates will be completely different and it is impossible to map from one genome into the other. &lt;/sub&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ebi.ac.uk/Tools/psa/genewise/&#xD;&#xA;  [2]: https://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate&#xD;&#xA;  [3]: https://biology.stackexchange.com/a/4964/1306" />
  <row Id="941" PostHistoryTypeId="5" PostId="303" RevisionGUID="5d6b2178-6555-454a-b928-e59ed4bd199c" CreationDate="2017-05-31T14:31:42.897" UserId="298" Comment="Included detail from comments" Text="Microbial genomes can contain extensive duplications. Often we'd like to transfer annotations from an annotated species to one that is newly sequenced. &#xD;&#xA;&#xD;&#xA;Existing tools (e.g. RATT, LiftOver, Kraken) either make specific assumptions about how closely related the species are or fail to transfer when multiple matches are found in the new genome, especially if the sequences are highly similar. &#xD;&#xA;&#xD;&#xA;Specifically, I have a syn bio application where genes can duplicate extensively. they are identical in sequence but duplicated many times and relocated (i.e. not adjacent). None of the above mentioned tools was able to transfer coordinates of annotations to genomes with multiple copies of annotation. &#xD;&#xA;&#xD;&#xA;Are there any pre-existing tools or software that transfer annotations in this scenario? Ideas for ways to do this robustly?" />
  <row Id="942" PostHistoryTypeId="24" PostId="303" RevisionGUID="5d6b2178-6555-454a-b928-e59ed4bd199c" CreationDate="2017-05-31T14:31:42.897" Comment="Proposed by 298 approved by -1 edit id of 87" />
  <row Id="943" PostHistoryTypeId="5" PostId="303" RevisionGUID="d3287999-8a92-4a3b-9fa9-0b3bdf7793e0" CreationDate="2017-05-31T14:31:42.897" UserId="77" Comment="Included detail from comments, improved text a bit" Text="Microbial genomes can contain extensive duplications. Often we'd like to transfer annotations from an annotated species to one that is newly sequenced. &#xD;&#xA;&#xD;&#xA;Existing tools (e.g. RATT, LiftOver, Kraken) either make specific assumptions about how closely related the species are or fail to transfer when multiple matches are found in the new genome, especially if the sequences are highly similar. &#xD;&#xA;&#xD;&#xA;Specifically, I have a synthetic biology application where genes can duplicate extensively. They are identical in sequence but duplicated many times and be relocated (i.e., not just adjacent to each other). None of the above mentioned tools are able to transfer coordinates of annotations to genomes with multiple copies of features.&#xD;&#xA;&#xD;&#xA;Are there any pre-existing tools or software that transfer annotations in this scenario? Ideas for ways to do this robustly?" />
  <row Id="944" PostHistoryTypeId="2" PostId="310" RevisionGUID="d2bc844c-da77-4c08-a31c-9266f99409cb" CreationDate="2017-05-31T14:36:06.500" UserId="450" Text="If you use `SeqIO.parse(filehandle, 'fasta')` to parse a FASTA file, then it will return a `SeqRecord` object where the `id` and `name` are the first word (everything before the first whitespace) of the line beginning with `&gt;` and the `description` is the complete line (all not including the initial `&gt;`). (This behaviour can overruled by providing [a custom `title2ids` function][1]).&#xD;&#xA;&#xD;&#xA;So, if you for example have a `original.fasta` file like:&#xD;&#xA;&#xD;&#xA;    &gt;Lorem|ipsum&gt;dolor sit amet&#xD;&#xA;    GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xD;&#xA;&#xD;&#xA;Then you will get:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; from Bio import SeqIO&#xD;&#xA;    &gt;&gt;&gt; path = r'C:\path\to\original.fasta'&#xD;&#xA;    &gt;&gt;&gt; records = SeqIO.parse(open(path), 'fasta')&#xD;&#xA;    &gt;&gt;&gt; record = next(records)&#xD;&#xA;    &gt;&gt;&gt; record.id&#xD;&#xA;    'Lorem|ipsum&gt;dolor'&#xD;&#xA;    &gt;&gt;&gt; record.name&#xD;&#xA;    'Lorem|ipsum&gt;dolor'&#xD;&#xA;    &gt;&gt;&gt; record.description&#xD;&#xA;    'Lorem|ipsum&gt;dolor sit amet'&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So, what happens in your case when you do &#xD;&#xA;&#xD;&#xA;        if record.id == 'foo':&#xD;&#xA;            record.id = 'bar'&#xD;&#xA;&#xD;&#xA;is that the `record.id` is successfully changed from `foo` to `bar`, but that the `record.description` is not changed and stays `foo`. That's why, when the FASTA file is printed out, you see the described behaviour of &#xD;&#xA;&#xD;&#xA;    &gt;bar foo&#xD;&#xA;    GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xD;&#xA;&#xD;&#xA;So, the solution to your problem is to both change the `id` AND the `description`:&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    &#xD;&#xA;    original_file = r&quot;path\to\original.fasta&quot;&#xD;&#xA;    corrected_file = r&quot;path\to\corrected.fasta&quot;&#xD;&#xA;    &#xD;&#xA;    with open(original_file) as original, open(corrected_file, 'w') as corrected:&#xD;&#xA;        records = SeqIO.parse(original_file, 'fasta')&#xD;&#xA;        for record in records:&#xD;&#xA;            if record.id == 'foo':&#xD;&#xA;                record.id = 'bar'&#xD;&#xA;                record.description = 'bar'&#xD;&#xA;            SeqIO.write(record, corrected, 'fasta')&#xD;&#xA;    &#xD;&#xA;    with open(corrected_file) as corrected:&#xD;&#xA;        records = SeqIO.parse(corrected, 'fasta')&#xD;&#xA;        for record in records:&#xD;&#xA;            print record.id             # prints bar&#xD;&#xA;            print record.name           # prints bar&#xD;&#xA;            print record.description    # prints bar&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/biopython/biopython/blob/dbdf7c155af3e7f203ea966acedc12740271ed73/Bio/SeqIO/FastaIO.py#L102" />
  <row Id="945" PostHistoryTypeId="2" PostId="311" RevisionGUID="bcd58154-a0a3-46cf-8b39-5f2011e64126" CreationDate="2017-05-31T15:34:47.833" UserId="44" Text="There is one very simplistic way I use which *might* work for what you are doing, it is similar to what terdon proposed.&#xD;&#xA;&#xD;&#xA;Take a de-novo gene function annotation tool (I have my own, but you could use/modify prokka). I use a &quot;knowledge&quot; protein database of genes I want to have very strictly annotated as a first line of annotation (e.g. in your case: the annotated genomes). For that I loop through very strict identity/similarity parameters which get gradually relaxed.&#xD;&#xA;&#xD;&#xA;E.g.:&#xD;&#xA;Loop 0: only transfer annotations at 100% DNA identity, same length&#xD;&#xA;Loop 1: only transfer annotations at 100% similarity, same length&#xD;&#xA;Loop 2: only transfer annotations at 99% similarity, length +/- 1%&#xD;&#xA;...&#xD;&#xA;Loop n: only transfer annotations at 100-n% similarity, length +/- n%&#xD;&#xA;&#xD;&#xA;In each loop, obviously only annotated which has not been annotated in previous loops.&#xD;&#xA;&#xD;&#xA;After that, use &quot;normal&quot; annotation pipeline of the tool to annotate the rest." />
  <row Id="946" PostHistoryTypeId="2" PostId="312" RevisionGUID="40c164ca-68ea-44e9-8c01-88fb5b610048" CreationDate="2017-05-31T15:45:09.357" UserId="44" Text="You should also keep in mind that single read accuracy of ONT still is a bit lacking, a 2D or 1D^2 accuracy of 95% still means that on average there's one error every 20 bp, and due to the nature of the data some stretches may be more junky than others. Maybe centrifuge doesn't like that.&#xD;&#xA;&#xD;&#xA;Getting a feel of what the data can tell you (and what not) is pretty important.&#xD;&#xA;&#xD;&#xA;You may want to try other classification tools to get an idea how well centrifuge works, Kraken comes to mind. But also: what about BLASTing some of these reads at NCBI and simply look at the results to get a first impression?" />
  <row Id="947" PostHistoryTypeId="2" PostId="313" RevisionGUID="6b1ee011-2625-4eb9-8569-b0e02df40177" CreationDate="2017-05-31T16:58:30.973" UserId="460" Text="1- Chimerascan + Star&#xD;&#xA;&#xD;&#xA;2- Tophat Fusion&#xD;&#xA;&#xD;&#xA;Two pipelines I have used extensively and recommend. The others listed by @L42 I can't speak to but Star Fusion sounds promising. Star is fast. " />
  <row Id="949" PostHistoryTypeId="10" PostId="283" RevisionGUID="58c212fb-c49f-477c-bcb2-15808f09f48e" CreationDate="2017-05-31T17:11:37.447" UserId="-1" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:104,&quot;DisplayName&quot;:&quot;Chris_Rands&quot;},{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;},{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:176,&quot;DisplayName&quot;:&quot;JonMark Perry&quot;},{&quot;Id&quot;:461,&quot;DisplayName&quot;:&quot;Maximilian Peters&quot;}]}" />
  <row Id="950" PostHistoryTypeId="5" PostId="311" RevisionGUID="d7887acf-0278-42ad-86f9-7465ee2c1189" CreationDate="2017-05-31T17:32:39.963" UserId="44" Comment="Fixed typos" Text="There is one very simplistic way I use which *might* work for what you are doing, it is similar to what terdon proposed.&#xD;&#xA;&#xD;&#xA;Take a de-novo gene function annotation tool (I have my own, but you could use/modify prokka). I use a &quot;knowledge&quot; protein database of genes I want to have very strictly annotated as a first line of annotation (e.g. in your case: the annotated genomes). For that I loop through very strict identity/similarity parameters which get gradually relaxed.&#xD;&#xA;&#xD;&#xA;E.g.:&#xD;&#xA;Loop 0: only transfer annotations at 100% DNA identity, same length.&#xD;&#xA;Loop 1: only transfer annotations at 100% similarity, same length.&#xD;&#xA;Loop 2: only transfer annotations at 99% similarity, length +/- 1%.&#xD;&#xA;...&#xD;&#xA;Loop n: only transfer annotations at 100-(n-1)% similarity, length +/- (n-1)%.&#xD;&#xA;&#xD;&#xA;In each loop, obviously only annotate what has not been annotated in previous loops.&#xD;&#xA;&#xD;&#xA;After that, use &quot;normal&quot; annotation pipeline of the tool to annotate the rest." />
  <row Id="951" PostHistoryTypeId="5" PostId="305" RevisionGUID="0ee752b8-919b-4f62-822c-719c27ca2ce8" CreationDate="2017-05-31T17:34:13.080" UserId="191" Comment="remove 20k reference &amp; make it shorter" Text="Let's say you acquired a graph like using `STRINGdb`.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;    full.graph &lt;- string_db$get_graph()&#xD;&#xA;&#xD;&#xA;Now you can use [`igraph`][1], to manipulate the graph. Let's assume you want to take 200 proteins with the highest degree, i.e. number of edges they have.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(igraph)&#xD;&#xA;    &#xD;&#xA;    # see how many proteins do you have    &#xD;&#xA;    vcount(full.graph)&#xD;&#xA;    &#xD;&#xA;    # find top 200 proteins with the highest degree&#xD;&#xA;    top.degree.verticies &lt;- names(tail(sort(degree(full.graph)), 200))&#xD;&#xA;&#xD;&#xA;    # extract the relevant subgraph&#xD;&#xA;    top.subgraph &lt;- induced_subgraph(graph, top.degree.verticies)&#xD;&#xA;    &#xD;&#xA;    # count the number of proteins in it&#xD;&#xA;    vcount(top.subgraph)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Update: How to get disease specific genes?**&#xD;&#xA;&#xD;&#xA;There's no GO annotation for cancer or Alzheimer's disease. It is out of scope of the GO consortium.&#xD;&#xA;&#xD;&#xA;What you can do, you can either take KEGG Pathways annotation, or manually select list of relevant GO-terms. Or acquire the list from one of the papers. For example annotation term `05200` corresponds to the cancer KEGG pathway. You can easily retrieve proteins associated with the annotation:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    cancer.pathway.proteins &lt;-&#xD;&#xA;        string_db$get_term_proteins('05200')$STRING_id&#xD;&#xA;&#xD;&#xA;And then perform subgraphing as described above.&#xD;&#xA;&#xD;&#xA;Alternatively you can try to get an enrichment score for an every gene given it's neighbors (the way enrichment is shown on the string-db website). Then you can keep only those having top enrichment scores. Probably `get_ppi_enrichment_full` or `get_ppi_enrichment` functions will help you to do that.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://igraph.org/r/&#xD;&#xA;" />
  <row Id="952" PostHistoryTypeId="5" PostId="282" RevisionGUID="def79338-a3af-4d42-9803-253bc1815796" CreationDate="2017-05-31T17:40:17.260" UserId="191" Comment="grammar " Text="How can I manipulate protein-interaction network graph from the String database using `STRINGdb` bioconductor package and R?&#xD;&#xA;&#xD;&#xA;I have downloaded the whole graph for *Homo sapiens* from STRING, which has about 20.000 proteins.&#xD;&#xA;&#xD;&#xA;1. How do I read the file using that package? &#xD;&#xA;2. How do I filter things I don't need? Supposing that I want to keep tumor data, as an example." />
  <row Id="953" PostHistoryTypeId="4" PostId="282" RevisionGUID="def79338-a3af-4d42-9803-253bc1815796" CreationDate="2017-05-31T17:40:17.260" UserId="191" Comment="grammar " Text="How to manipulate protein interaction network from String database in R?" />
  <row Id="954" PostHistoryTypeId="5" PostId="305" RevisionGUID="cd2c5422-ec4c-42a4-8d82-d84797072080" CreationDate="2017-05-31T17:41:49.653" UserId="191" Comment="make the answer match the question :)" Text="I think the easiest way is to download the graph using `STRINGdb`.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;    full.graph &lt;- string_db$get_graph()&#xD;&#xA;&#xD;&#xA;Now you can use [`igraph`][1], to manipulate the graph. Let's assume you want to take 200 proteins with the highest degree, i.e. number of edges they have.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(igraph)&#xD;&#xA;    &#xD;&#xA;    # see how many proteins do you have    &#xD;&#xA;    vcount(full.graph)&#xD;&#xA;    &#xD;&#xA;    # find top 200 proteins with the highest degree&#xD;&#xA;    top.degree.verticies &lt;- names(tail(sort(degree(full.graph)), 200))&#xD;&#xA;&#xD;&#xA;    # extract the relevant subgraph&#xD;&#xA;    top.subgraph &lt;- induced_subgraph(graph, top.degree.verticies)&#xD;&#xA;    &#xD;&#xA;    # count the number of proteins in it&#xD;&#xA;    vcount(top.subgraph)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**How to get disease specific genes?**&#xD;&#xA;&#xD;&#xA;There's no GO annotation for cancer or Alzheimer's disease. It is out of scope of the GO consortium.&#xD;&#xA;&#xD;&#xA;What you can do, you can either take KEGG Pathways annotation, or manually select list of relevant GO-terms. Or acquire the list from one of the papers. For example annotation term `05200` corresponds to the cancer KEGG pathway. You can easily retrieve proteins associated with the annotation:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    cancer.pathway.proteins &lt;-&#xD;&#xA;        string_db$get_term_proteins('05200')$STRING_id&#xD;&#xA;&#xD;&#xA;And then perform subgraphing as described above.&#xD;&#xA;&#xD;&#xA;Alternatively you can try to get an enrichment score for an every gene given it's neighbors (the way enrichment is shown on the string-db website). Then you can keep only those having top enrichment scores. Probably `get_ppi_enrichment_full` or `get_ppi_enrichment` functions will help you to do that.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://igraph.org/r/&#xD;&#xA;" />
  <row Id="955" PostHistoryTypeId="5" PostId="311" RevisionGUID="4c6cc435-3cb1-4632-a388-d013301f3d9a" CreationDate="2017-05-31T17:43:32.240" UserId="44" Comment="Added more info on de-novo microbial annotation" Text="There is one very simplistic way I use which *might* work for what you are doing, it is similar to what terdon proposed.&#xD;&#xA;&#xD;&#xA;Take a de-novo microbial genome annotation tool (I have my own, but you could use/modify [prokka][1]). Tools like these often first predict gene boundaries (with other tools like [prodigal][2] or [glimmer][3]) and then try to assign a function to found genes. This function assignment is often done with BLAST and other tools ... and that is where you can go in and modify to do what you need.&#xD;&#xA;&#xD;&#xA;I use a &quot;knowledge&quot; protein database of genes I want to have very strictly annotated as a first line of annotation (e.g. in your case: the annotated genomes). For that I loop through very strict identity/similarity parameters which get gradually relaxed.&#xD;&#xA;&#xD;&#xA;E.g.:&#xD;&#xA;Loop 0: only transfer annotations at 100% DNA identity, same length.&#xD;&#xA;Loop 1: only transfer annotations at 100% similarity, same length.&#xD;&#xA;Loop 2: only transfer annotations at 99% similarity, length +/- 1%.&#xD;&#xA;...&#xD;&#xA;Loop n: only transfer annotations at 100-(n-1)% similarity, length +/- (n-1)%.&#xD;&#xA;&#xD;&#xA;In each loop, obviously only annotate what has not been annotated in previous loops.&#xD;&#xA;&#xD;&#xA;After that, use &quot;normal&quot; annotation pipeline of the tool to annotate the rest.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.vicbioinformatics.com/software.prokka.shtml&#xD;&#xA;  [2]: http://prodigal.ornl.gov/&#xD;&#xA;  [3]: https://ccb.jhu.edu/software/glimmer/" />
  <row Id="956" PostHistoryTypeId="2" PostId="314" RevisionGUID="67f1d4b3-d51d-4ab4-951d-d2f3e4fb56ab" CreationDate="2017-05-31T18:06:26.410" UserId="177" Text="I have:&#xD;&#xA;&#xD;&#xA;* A list of differentially phosphorylated sites in a knockout condition. Some genes contain as many as 70 possible phosphorylation sites; others contain only one.&#xD;&#xA;* A list of genes belonging to a specific gene set annotation.&#xD;&#xA;&#xD;&#xA;**How can I test the differentially phosphorylated proteins for enrichment of this annotation?**&#xD;&#xA;&#xD;&#xA;A few ideas I’ve considered:&#xD;&#xA;&#xD;&#xA;1. Ignore the number of phosphorylation events detected within a gene and simply count the gene as differentially phosphorylated if it contains at least a single site that is differentially phosphorylated. Compare the enrichment score for this selected set to the enrichment score for the set genes containing at least one site that is not differentially phosphorylated. The problem here is that genes with a large number of phosphorylation sites have almost no influence in the enrichment score, since it’s almost certain that they’ll have at least one site that is differentially phosphorylated and at least one that is not.&#xD;&#xA;2. Mark each candidate phosphorylation site as either “in the set” or “not in the set” based on the protein in which it’s found. Then perform the enrichment analysis using the set of annotated phosphorylation sites instead of using the traditional enrichment analysis performed at the gene level. The potential problem with this approach is that it may place too much influence on genes with many potential phosphorylation sites.&#xD;&#xA;3. Aggregate all candidate phosphorylation sites within a gene and use some numerical threshold to determine whether the gene is differentially phosphorylated or not. (There are various ways that this could be done.) Then perform enrichment analysis using the resulting set of differentially phosphorylated genes. A possible problem here is that some of the phosphorylation sites may be more functionally important than others, so it’s not clear how to weight the relative importance of individual phosphorylation sites within a gene.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I realize the goal here isn’t well-defined mathematically; I’m mainly curious what approach makes the most sense given the biological context. Of the above approaches, I’m currently leaning towards approach 2 because it’s straightforward to implement and it at least *attempts* to account for the variable number of phosphorylation events within a gene.&#xD;&#xA;&#xD;&#xA;NOTE: I also have normalized phosphoprotein and protein abundances for all of these sites, obtained from mass spec. So if the solution requires an alternative method of computing differential phosphorylation, that’s fine.&#xD;&#xA;" />
  <row Id="957" PostHistoryTypeId="1" PostId="314" RevisionGUID="67f1d4b3-d51d-4ab4-951d-d2f3e4fb56ab" CreationDate="2017-05-31T18:06:26.410" UserId="177" Text="Gene set enrichment analysis on differential phosphorylation sites" />
  <row Id="958" PostHistoryTypeId="3" PostId="314" RevisionGUID="67f1d4b3-d51d-4ab4-951d-d2f3e4fb56ab" CreationDate="2017-05-31T18:06:26.410" UserId="177" Text="&lt;gse&gt;&lt;differential-expression&gt;&lt;phosphoproteomics&gt;" />
  <row Id="959" PostHistoryTypeId="2" PostId="315" RevisionGUID="39cbceb6-b8b6-4e93-bbc1-2e322ff7d867" CreationDate="2017-05-31T18:35:14.477" UserId="191" Text="Your dataset looks perfect for the [SUMSTAT][1] enrichment test.&#xD;&#xA;&#xD;&#xA;1. You need to come up with a statistic representing your gene. The simplest ideas here are the number of sites or, probably better, a proportion of phosphorylated sites.&#xD;&#xA;2. Now you can compute a statistic for every gene set, for example just sum (SUMSTAT). You can also have average, sum of squares or something else.&#xD;&#xA;3. Get null distribution of your gene set statistics by permutations. You can either randomize statistics for genes, or just randomly assign phosphorylated sites across the genome keeping the total number of sites.&#xD;&#xA;&#xD;&#xA;Now you can compute p-value just by comparing your value with the null-distribution.&#xD;&#xA;&#xD;&#xA;You have to be cautious of two things:&#xD;&#xA;&#xD;&#xA;1. You are testing multiple gene sets, i.e. you perform a statistical test for every gene set. Therefore correct your significance for multiple testing. I suggest using FDR for this.&#xD;&#xA;2. Beware of potential biases when doing permutations. The obvious one is gene length, i.e. longer genes have higher chance of getting at least one site. But there can be other ones like GC-content, chromosomes, etc. You can overcome this by using more realistic permutations, or by controlling for the potential correlations. You can get a few ideas on fighting biases from [this paper][2] (sorry for the self-advertisement).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://dx.doi.org/10.1186/1753-6561-3-S7-S96&#xD;&#xA;  [2]: https://doi.org/10.1093/molbev/msx083" />
  <row Id="960" PostHistoryTypeId="5" PostId="294" RevisionGUID="06ca0d01-6547-4fb4-93e4-98fb92437bd1" CreationDate="2017-05-31T18:46:43.460" UserId="191" Comment="better question" Text="I vaguely remember, that the original plan of Oxford Nanopore was to provide cheap sequencers (MinION), but charge for base-calling. For that reason the base-calling was performed in the cloud, and the plan was to make it commercial once the technology is established.&#xD;&#xA;&#xD;&#xA;Of course, this limits the potential uses of MinION in the field, since huge areas do not have decent internet connection. Also, not all the data can be legally transferred to a third-party company in the clinical studies.&#xD;&#xA;&#xD;&#xA;For example for the [Ebola paper][1], they had to create a special version of their software:&#xD;&#xA;&#xD;&#xA;&gt; An offline-capable version of MinKNOW, with internet ‘ping’ disabled&#xD;&#xA;&gt; and online updates disabled was made available to us by Oxford&#xD;&#xA;&gt; Nanopore Technologies specifically for the project&#xD;&#xA;&#xD;&#xA;There are couple of third-party base-callers available today. I am aware of [Nanocall][2] and [DeepNano][3], but since they are not official, it can be hard for them to keep-up with the latest versions of sequencers and cells.&#xD;&#xA;&#xD;&#xA;1. Is it possible as of today to sequence offline without a special arrangement (like the Ebola one).&#xD;&#xA;2. If not, what's the policy of Oxford Nanopore toward third-party base-callers? Are they going to help them, or to sue them eventually?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://dx.doi.org/10.1038/nature16996&#xD;&#xA;  [2]: https://nanoporetech.com/publications/nanocall-open-source-basecaller-oxford-nanopore-sequencing-data&#xD;&#xA;  [3]: https://nanoporetech.com/publications/deepnano-deep-recurrent-neural-networks-base-calling-minion-nanopore-reads" />
  <row Id="961" PostHistoryTypeId="5" PostId="315" RevisionGUID="698f10fe-0d1d-4400-9702-9d4b2d126815" CreationDate="2017-05-31T19:05:58.757" UserId="191" Comment="grammar " Text="Your dataset looks perfect for the [SUMSTAT][1] enrichment test.&#xD;&#xA;&#xD;&#xA;1. You need to come up with a statistic representing your gene. The simplest ideas here are the number of sites or, probably better, a proportion of phosphorylated sites.&#xD;&#xA;2. Now you can compute a statistic for every gene set, for example just sum (SUMSTAT). You can also have average, sum of squares or something else.&#xD;&#xA;3. Geta a null distribution of your gene set statistics by permutations. You can either randomize statistics for genes, or just randomly assign phosphorylated sites across the genome keeping the total number of sites.&#xD;&#xA;&#xD;&#xA;Now you can compute p-value just by comparing your value with the null-distribution.&#xD;&#xA;&#xD;&#xA;You have to be cautious of two things:&#xD;&#xA;&#xD;&#xA;1. You are testing multiple gene sets, i.e. you perform a statistical test for every gene set. Therefore correct your significance for multiple testing. I suggest using FDR for this.&#xD;&#xA;2. Beware of potential biases when doing permutations. The obvious one is gene length, i.e. longer genes have higher chance of getting at least one site. But there can be other ones like GC-content, chromosomes, etc. You can overcome this by using more realistic permutations, or by controlling for the potential correlations. You can get a few ideas on fighting biases from [this paper][2] (sorry for the self-advertisement).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://dx.doi.org/10.1186/1753-6561-3-S7-S96&#xD;&#xA;  [2]: https://doi.org/10.1093/molbev/msx083" />
  <row Id="962" PostHistoryTypeId="5" PostId="293" RevisionGUID="614faa87-9bee-4d9a-bb34-c0f7c4870953" CreationDate="2017-05-31T19:21:57.343" UserId="37" Comment="Details about ntHash" Text="A rolling hash function for DNA sequences called [ntHash](http://www.bcgsc.ca/platform/bioinfo/software/nthash) has recently been [published in Bioinformatics](https://doi.org/10.1093/bioinformatics/btw397) and the authors dealt with reverse complements:&#xD;&#xA;&#xD;&#xA;&gt; Using this table, we can easily compute the hash value for the reverse-complement (as well as the canonical form) of a sequence efficiently, without actually reverse- complementing the input sequence, as follows:&#xD;&#xA;...&#xD;&#xA;&#xD;&#xA;EDIT: I will add more details about how ntHash works. The notations used in its paper are somewhat uncommon. The source code is more informative.&#xD;&#xA;&#xD;&#xA;Let's first define rotation functions for 64-bit integers:&#xD;&#xA;&#xD;&#xA;    rol(x,k) := x &lt;&lt; k | x &gt;&gt; (64-k)&#xD;&#xA;    ror(x,k) := x &gt;&gt; k | x &lt;&lt; (64-k)&#xD;&#xA;&#xD;&#xA;We then define a hash function `h()` for each base. In the implementation, the authors are using:&#xD;&#xA;&#xD;&#xA;    h(A) = 0x3c8bfbb395c60474&#xD;&#xA;    h(C) = 0x3193c18562a02b4c&#xD;&#xA;    h(G) = 0x20323ed082572324&#xD;&#xA;    h(T) = 0x295549f54be24456&#xD;&#xA;    h(N) = 0&#xD;&#xA;&#xD;&#xA;The rolling hash function of a forward k-mer `s[i,i+k-1]` is:&#xD;&#xA;&#xD;&#xA;    f(s[i,i+k-1]) := rol(h(s[i]),k-1) ^ rol(h(s[i+1]),k-2) ^ ... ^ h(s[i+k-1])&#xD;&#xA;&#xD;&#xA;where `^` is the XOR operator. The hash function of its reverse complement is:&#xD;&#xA;&#xD;&#xA;    r(s[i,i+k-1]) := f(~s[i,i+k-1])&#xD;&#xA;                   = rol(h(~s[i+k-1]),k-1) ^ rol(h(~s[i+k-2]),k-2) ^ ... ^ h(~s[i])&#xD;&#xA;&#xD;&#xA;where `~` gives the reverse complement of a DNA sequence. Knowing `f(s[i,i+k-1])` and `r(s[i,i+k-1])`, we can compute their values for the next k-mer:&#xD;&#xA;&#xD;&#xA;    f(s[i+1,i+k]) = rol(f(s[i,i+k-1]),1) ^ rol(h(s[i]),k)  ^ h(s[i+k])&#xD;&#xA;    r(s[i+1,i+k]) = ror(r(s[i,i+k-1]),1) ^ ror(h(~s[i]),1) ^ rol(h(~s[i+k]),k-1)&#xD;&#xA;&#xD;&#xA;This works because `rol`, `ror` and `^` can all be switched in order. Finally, for a k-mer `s`, the hash function considering both strands is the smaller between `f(s)` and `r(s)`:&#xD;&#xA;&#xD;&#xA;    h(s) = min(f(s),r(s))&#xD;&#xA;&#xD;&#xA;This is a linear algorithm regardless of the k-mer length. It only uses simple arithmetic operations, so should be fairly fast. I have briefly tested its randomness. It seems comparable to murmur. ntHash is probably the best algorithm so far if you want to hash an arbitrarily long k-mer into 64 bits." />
  <row Id="963" PostHistoryTypeId="5" PostId="310" RevisionGUID="9aa871c3-edb5-4f41-b7f6-782dd977f7f7" CreationDate="2017-05-31T19:39:46.323" UserId="280" Comment="proper use of pronomes" Text="If I use `SeqIO.parse(filehandle, 'fasta')` to parse a FASTA file, then it will return a `SeqRecord` object where the `id` and `name` are the first word (everything before the first whitespace) of the line beginning with `&gt;` and the `description` is the complete line (all not including the initial `&gt;`). (This behaviour can overruled by providing [a custom `title2ids` function][1]).&#xD;&#xA;&#xD;&#xA;So, if I for example have a `original.fasta` file like:&#xD;&#xA;&#xD;&#xA;    &gt;Lorem|ipsum&gt;dolor sit amet&#xD;&#xA;    GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xD;&#xA;&#xD;&#xA;Then I will get:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; from Bio import SeqIO&#xD;&#xA;    &gt;&gt;&gt; path = r'C:\path\to\original.fasta'&#xD;&#xA;    &gt;&gt;&gt; records = SeqIO.parse(open(path), 'fasta')&#xD;&#xA;    &gt;&gt;&gt; record = next(records)&#xD;&#xA;    &gt;&gt;&gt; record.id&#xD;&#xA;    'Lorem|ipsum&gt;dolor'&#xD;&#xA;    &gt;&gt;&gt; record.name&#xD;&#xA;    'Lorem|ipsum&gt;dolor'&#xD;&#xA;    &gt;&gt;&gt; record.description&#xD;&#xA;    'Lorem|ipsum&gt;dolor sit amet'&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So, what happens in my case when you do &#xD;&#xA;&#xD;&#xA;        if record.id == 'foo':&#xD;&#xA;            record.id = 'bar'&#xD;&#xA;&#xD;&#xA;is that the `record.id` is successfully changed from `foo` to `bar`, but that the `record.description` is not changed and stays `foo`. That's why, when the FASTA file is printed out, I see the described behaviour of &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    &gt;bar foo&#xD;&#xA;    GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So, the solution to my problem is to both change the `id` AND the `description`:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    &#xD;&#xA;    original_file = r&quot;path\to\original.fasta&quot;&#xD;&#xA;    corrected_file = r&quot;path\to\corrected.fasta&quot;&#xD;&#xA;    &#xD;&#xA;    with open(original_file) as original, open(corrected_file, 'w') as corrected:&#xD;&#xA;        records = SeqIO.parse(original_file, 'fasta')&#xD;&#xA;        for record in records:&#xD;&#xA;            if record.id == 'foo':&#xD;&#xA;                record.id = 'bar'&#xD;&#xA;                record.description = 'bar'&#xD;&#xA;            SeqIO.write(record, corrected, 'fasta')&#xD;&#xA;    &#xD;&#xA;    with open(corrected_file) as corrected:&#xD;&#xA;        records = SeqIO.parse(corrected, 'fasta')&#xD;&#xA;        for record in records:&#xD;&#xA;            print record.id             # prints bar&#xD;&#xA;            print record.name           # prints bar&#xD;&#xA;            print record.description    # prints bar&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/biopython/biopython/blob/dbdf7c155af3e7f203ea966acedc12740271ed73/Bio/SeqIO/FastaIO.py#L102" />
  <row Id="964" PostHistoryTypeId="24" PostId="310" RevisionGUID="9aa871c3-edb5-4f41-b7f6-782dd977f7f7" CreationDate="2017-05-31T19:39:46.323" Comment="Proposed by 280 approved by 57, 191 edit id of 88" />
  <row Id="965" PostHistoryTypeId="5" PostId="304" RevisionGUID="a905a477-852e-4224-b643-2dcea1619352" CreationDate="2017-05-31T19:51:55.210" UserId="73" Comment="added 6 characters in body" Text="Short answer: yes, but you need to get permission (and modified software) from ONT before doing that.&#xD;&#xA;&#xD;&#xA;... but that doesn't tell the whole story. This question has the potential to be very confusing, and that's through no fault of the questioner. The question that has been asked in the title is actually slightly different from what's being asked in the body of the question, and I think the &quot;title&quot; question is probably more relevant when considering the field applications of the MinION. The issue is that for the MinION, sequencing (or more specifically, generating the raw data in the form of an electrical signal trace) is distinct and separable from base calling. Many other sequencers also have distinct raw data and base-calling phases, but they're not democratised to the degree they are on the MinION.&#xD;&#xA;&#xD;&#xA;The &quot;sequencing&quot; part of MinION sequencing is carried out by ONT software, namely MinKNOW. As explained to me during PoreCampAU 2017, when the MinION is initially plugged into a computer it is missing the firmware necessary to carry out the sequencing. The most recent version of this firmware is usually downloaded at the start of a sequencing run by sending a request to ONT servers. In the usual case, you can't do sequencing without being able to access those servers, and you can't do sequencing without ONT knowing about it. However, ONT acknowledge that there are people out there who won't have Internet access when sequencing (e.g. sequencing Ebola in Africa, or metagenomic sequencing in the middle of the ocean), and an email to ```&lt;support@nanoporetech.com&gt;``` with reasons is likely to result in a quick software fix to the local sequencing problem.&#xD;&#xA;&#xD;&#xA;Once the raw signals are acquired, the &quot;base-calling&quot; part of MinION sequencing can be done anywhere. The ONT-maintained basecaller is Albacore, and this will get the first model updates whenever the sequencing technology is changed (which happens a lot). Albacore is a local basecaller which can be obtained from ONT by browsing through their community pages (available to anyone who has a MinION); ONT switched to only allowing people to do basecalling locally in about April 2017, after establishing that using AWS servers was just too expensive. Albacore is open source and free-as-in-beer, but has a restrictive licensing agreement which limits the distribution (and modification) of the program. However, Albacore is not the only available basecaller. ONT provide a FOSS basecaller called [nanonet][1]. It's a little bit behind Albacore on technology, but ONT have said that all useful Albacore changes will eventually propagate through to nanonet. There is another non-ONT basecaller that I'm aware of which uses a neural network for basecalling: [deepnano][2]. Other basecallers exist, each varying distances away technology-wise, and I expect that more will appear in the future as the technology stabilises and more change-resistant computer scientists get in on the act.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/nanoporetech/nanonet&#xD;&#xA;  [2]: https://bitbucket.org/vboza/deepnano" />
  <row Id="966" PostHistoryTypeId="5" PostId="293" RevisionGUID="f8ebfb97-2559-4efd-b03e-1240a5709aab" CreationDate="2017-05-31T19:53:13.073" UserId="425" Comment="Add credit" Text="A rolling hash function for DNA sequences called [ntHash](http://www.bcgsc.ca/platform/bioinfo/software/nthash) has recently been [published in Bioinformatics](https://doi.org/10.1093/bioinformatics/btw397) and the authors dealt with reverse complements:&#xD;&#xA;&#xD;&#xA;&gt; Using this table, we can easily compute the hash value for the reverse-complement (as well as the canonical form) of a sequence efficiently, without actually reverse- complementing the input sequence, as follows:&#xD;&#xA;...&#xD;&#xA;&#xD;&#xA;EDIT (by @user172818): I will add more details about how ntHash works. The notations used in its paper are somewhat uncommon. The source code is more informative.&#xD;&#xA;&#xD;&#xA;Let's first define rotation functions for 64-bit integers:&#xD;&#xA;&#xD;&#xA;    rol(x,k) := x &lt;&lt; k | x &gt;&gt; (64-k)&#xD;&#xA;    ror(x,k) := x &gt;&gt; k | x &lt;&lt; (64-k)&#xD;&#xA;&#xD;&#xA;We then define a hash function `h()` for each base. In the implementation, the authors are using:&#xD;&#xA;&#xD;&#xA;    h(A) = 0x3c8bfbb395c60474&#xD;&#xA;    h(C) = 0x3193c18562a02b4c&#xD;&#xA;    h(G) = 0x20323ed082572324&#xD;&#xA;    h(T) = 0x295549f54be24456&#xD;&#xA;    h(N) = 0&#xD;&#xA;&#xD;&#xA;The rolling hash function of a forward k-mer `s[i,i+k-1]` is:&#xD;&#xA;&#xD;&#xA;    f(s[i,i+k-1]) := rol(h(s[i]),k-1) ^ rol(h(s[i+1]),k-2) ^ ... ^ h(s[i+k-1])&#xD;&#xA;&#xD;&#xA;where `^` is the XOR operator. The hash function of its reverse complement is:&#xD;&#xA;&#xD;&#xA;    r(s[i,i+k-1]) := f(~s[i,i+k-1])&#xD;&#xA;                   = rol(h(~s[i+k-1]),k-1) ^ rol(h(~s[i+k-2]),k-2) ^ ... ^ h(~s[i])&#xD;&#xA;&#xD;&#xA;where `~` gives the reverse complement of a DNA sequence. Knowing `f(s[i,i+k-1])` and `r(s[i,i+k-1])`, we can compute their values for the next k-mer:&#xD;&#xA;&#xD;&#xA;    f(s[i+1,i+k]) = rol(f(s[i,i+k-1]),1) ^ rol(h(s[i]),k)  ^ h(s[i+k])&#xD;&#xA;    r(s[i+1,i+k]) = ror(r(s[i,i+k-1]),1) ^ ror(h(~s[i]),1) ^ rol(h(~s[i+k]),k-1)&#xD;&#xA;&#xD;&#xA;This works because `rol`, `ror` and `^` can all be switched in order. Finally, for a k-mer `s`, the hash function considering both strands is the smaller between `f(s)` and `r(s)`:&#xD;&#xA;&#xD;&#xA;    h(s) = min(f(s),r(s))&#xD;&#xA;&#xD;&#xA;This is a linear algorithm regardless of the k-mer length. It only uses simple arithmetic operations, so should be fairly fast. I have briefly tested its randomness. It seems comparable to murmur. ntHash is probably the best algorithm so far if you want to hash an arbitrarily long k-mer into 64 bits." />
  <row Id="967" PostHistoryTypeId="2" PostId="316" RevisionGUID="16c41288-51fe-4356-8f8d-6ea316f535b0" CreationDate="2017-05-31T20:27:51.153" UserId="131" Text="We have arrayCGH (aCGH) results for one sample. There is a 0.5 Mb terminal duplication on one of the chromosomes. It is rare, from literature review there are only 3-4 samples with clinical information.&#xD;&#xA;&#xD;&#xA;What are the steps to validate the results, how do we ascertain that this duplication is the cause of the clinical symptoms? Some ideas:&#xD;&#xA; &#xD;&#xA; - aCGH the parents. Not sure how this would help.   &#xD;&#xA; - whole genome exome sequencing, worried this might make it more difficult to pinpoint genetic cause.&#xD;&#xA; - whole genome sequencing?&#xD;&#xA; - other ideas?&#xD;&#xA;&#xD;&#xA;**Note:** I am new to aCGH, and next gen, any advice is welcome." />
  <row Id="968" PostHistoryTypeId="1" PostId="316" RevisionGUID="16c41288-51fe-4356-8f8d-6ea316f535b0" CreationDate="2017-05-31T20:27:51.153" UserId="131" Text="ArrayCGH single sample result validation" />
  <row Id="969" PostHistoryTypeId="3" PostId="316" RevisionGUID="16c41288-51fe-4356-8f8d-6ea316f535b0" CreationDate="2017-05-31T20:27:51.153" UserId="131" Text="&lt;sequencing&gt;&lt;array-cgh&gt;&lt;exome&gt;" />
  <row Id="970" PostHistoryTypeId="2" PostId="317" RevisionGUID="ff1da865-11dc-4fbb-a10e-e996c35b7b7e" CreationDate="2017-05-31T20:39:57.870" UserId="131" Text="There are many posts on the web regarding QC steps pre and post-imputation.&#xD;&#xA;&#xD;&#xA;Does applying below (new?) 10% [MAF](https://en.wikipedia.org/wiki/Minor_allele_frequency) difference rule make sense, pitfalls?&#xD;&#xA;&#xD;&#xA;Here is the process:&#xD;&#xA;&#xD;&#xA; 1. Get MAF for imputed set, using SNPTEST with flag `-summary_stats_only`&#xD;&#xA; 2. Convert imputed set to hard-calls using [gtools](http://www.well.ox.ac.uk/~cfreeman/software/gwas/gtool.html) with flag `--threshold 0.9`&#xD;&#xA; 3. If the MAF from step 1 and step 2 differs more than 10% than exclude the variant.&#xD;&#xA;&#xD;&#xA;More info, GWAS is 50K vs 50K case control samples." />
  <row Id="971" PostHistoryTypeId="1" PostId="317" RevisionGUID="ff1da865-11dc-4fbb-a10e-e996c35b7b7e" CreationDate="2017-05-31T20:39:57.870" UserId="131" Text="GWAS imputation QC step MAF difference 10%" />
  <row Id="972" PostHistoryTypeId="3" PostId="317" RevisionGUID="ff1da865-11dc-4fbb-a10e-e996c35b7b7e" CreationDate="2017-05-31T20:39:57.870" UserId="131" Text="&lt;gwas&gt;&lt;imputation&gt;&lt;gtools&gt;&lt;maf&gt;&lt;qc&gt;" />
  <row Id="973" PostHistoryTypeId="2" PostId="318" RevisionGUID="fdcf3cbc-d1ce-402d-a1e7-5b3065082c0e" CreationDate="2017-05-31T20:47:18.167" UserId="131" Text="Is working with and relying on old genome builds still valid?&#xD;&#xA;&#xD;&#xA;For example [NCBI36/hg18](https://genome-euro.ucsc.edu/cgi-bin/hgGateway?db=hg18&amp;redirect=manual&amp;source=genome.ucsc.edu). Would results from papers based on old builds require [LiftOver](https://genome.ucsc.edu/cgi-bin/hgLiftOver) and re-analysis to be useful?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-----------&#xD;&#xA;&#xD;&#xA;**^** This post might be too broad, off topic, but would like to know your views." />
  <row Id="974" PostHistoryTypeId="1" PostId="318" RevisionGUID="fdcf3cbc-d1ce-402d-a1e7-5b3065082c0e" CreationDate="2017-05-31T20:47:18.167" UserId="131" Text="Working with old genome builds" />
  <row Id="975" PostHistoryTypeId="3" PostId="318" RevisionGUID="fdcf3cbc-d1ce-402d-a1e7-5b3065082c0e" CreationDate="2017-05-31T20:47:18.167" UserId="131" Text="&lt;liftover&gt;&lt;build&gt;&lt;hg18&gt;&lt;ncbi36&gt;" />
  <row Id="976" PostHistoryTypeId="2" PostId="319" RevisionGUID="b2a427ba-608b-4983-af8c-719119cb18cf" CreationDate="2017-05-31T21:03:50.680" UserId="425" Text="In my opinion, it is not very reliable. LiftOver is very limited in terms of transformations it can support. The LiftOver Chain format can capture only matching regions in the same order. It means that it can account for indels, but even simple structural variations become problematic.&#xD;&#xA;&#xD;&#xA;For instance, when a newer assembly is available, it is usually a recommended practice to remap all the reads rather than transform the existing alignments." />
  <row Id="977" PostHistoryTypeId="5" PostId="316" RevisionGUID="ae81c705-4d3f-4272-abe2-c5c8841047f4" CreationDate="2017-05-31T21:26:26.280" UserId="131" Comment="added 97 characters in body" Text="We have arrayCGH (aCGH) results for one sample. There is a 0.5 Mb terminal duplication on one of the chromosomes. It is rare, from literature review there are only 3-4 samples with clinical information.&#xD;&#xA;&#xD;&#xA;What are the steps to validate the results, how do we ascertain that this duplication is the cause of the clinical symptoms? Some ideas:&#xD;&#xA; &#xD;&#xA; - aCGH the parents. Not sure how this would help.   &#xD;&#xA; - whole genome exome sequencing, worried this might make it more difficult to pinpoint genetic cause.&#xD;&#xA; - whole genome sequencing?&#xD;&#xA; - other ideas?&#xD;&#xA;&#xD;&#xA;Terminal duplication:&#xD;&#xA; &#xD;&#xA;NCBI36/hg18  &#xD;&#xA;start: 62995490  &#xD;&#xA;end: 63407936  &#xD;&#xA;size: 412446  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Note:** I am new to aCGH, and next gen, any advice is welcome." />
  <row Id="978" PostHistoryTypeId="5" PostId="316" RevisionGUID="1fb1212e-4c17-414c-9fbc-62844af554d2" CreationDate="2017-05-31T21:26:54.530" UserId="131" Comment="deleted 9 characters in body" Text="We have arrayCGH (aCGH) results for one sample. There is a 0.5 Mb terminal duplication on one of the chromosomes. It is rare, from literature review there are only 3-4 samples with clinical information.&#xD;&#xA;&#xD;&#xA;What are the steps to validate the results, how do we ascertain that this duplication is the cause of the clinical symptoms? Some ideas:&#xD;&#xA; &#xD;&#xA; - aCGH the parents. Not sure how this would help.   &#xD;&#xA; - whole genome exome sequencing, worried this might make it more difficult to pinpoint genetic cause.&#xD;&#xA; - whole genome sequencing?&#xD;&#xA; - other ideas?&#xD;&#xA;&#xD;&#xA;Terminal duplication:&#xD;&#xA; &#xD;&#xA;NCBI36/hg18  &#xD;&#xA;chr19:62995490-63407936  &#xD;&#xA;size: 412446  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Note:** I am new to aCGH, and next gen, any advice is welcome." />
  <row Id="979" PostHistoryTypeId="2" PostId="320" RevisionGUID="5300dcf8-47b1-48bd-9e99-fbbd18f0b21b" CreationDate="2017-05-31T21:28:07.403" UserId="77" Text="Why not just good old qPCR? That's (A) quick, (B) cheap and (C) easy to analyze. If you care about the exact location of the break point (I'm guessing from the context that you don't), then targeted sequencing with a custom capture kit would work.&#xD;&#xA;&#xD;&#xA;Regarding validating the biological relevance of this, there are multiple (parallel) routes one can take. Firstly, screen unaffected family members for this same alteration. If you find this in unaffected individuals then it's obviously not the causative alteration. Ideally, one would also do either a cell-line experiment or a mouse (or other model organism) experiment to see if you can reconstitute at least some component of the clinical phenotype. This may not always be possible, of course." />
  <row Id="980" PostHistoryTypeId="2" PostId="321" RevisionGUID="b7f95ac1-998d-4842-8de5-b2d497bad532" CreationDate="2017-05-31T22:32:59.663" UserId="57" Text="The most of the variant calling pipelines (like GATK) have a step for recalibration of the quality scores of Illumina reads, which is dependent on the known variants. Recently some work as been done for reference-free recalibration of scores as well: [Lancer](http://biorxiv.org/content/early/2017/04/27/130732) and [atlas](http://www.genetics.org/content/early/2016/11/07/genetics.116.189985), which is motivated by making the most for aDNA and low coverage datasets. The importance for aDNA is explained in [this lecture][1], but it is not clear to me if / how much it is important for fresh DNA samples with decent (&gt;15x) coverage. Especially when I work with non-model organisms and I can not simply use the standard tools.&#xD;&#xA;&#xD;&#xA;Q: How big impact has recalibration of scores on variant calling? Is there a rule of thumb for which it is / it is not worth the effort?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.youtube.com/watch?v=oJ9pbQsyaUg&amp;list=PLoCxWrRWjqB1JUCntl4X09ezmOtKx1Gke&amp;index=4" />
  <row Id="981" PostHistoryTypeId="1" PostId="321" RevisionGUID="b7f95ac1-998d-4842-8de5-b2d497bad532" CreationDate="2017-05-31T22:32:59.663" UserId="57" Text="Is there a point in recalibration of scores for variant calling?" />
  <row Id="982" PostHistoryTypeId="3" PostId="321" RevisionGUID="b7f95ac1-998d-4842-8de5-b2d497bad532" CreationDate="2017-05-31T22:32:59.663" UserId="57" Text="&lt;ngs&gt;&lt;variant-calling&gt;" />
  <row Id="983" PostHistoryTypeId="2" PostId="322" RevisionGUID="ae1bbd5d-dbcf-4e11-ab51-a367026d06f1" CreationDate="2017-05-31T22:56:26.803" UserId="33" Text="How do I download a reference genome that I can use with bowtie2? Specifically HG19. On UCSC there are a lot of file options." />
  <row Id="984" PostHistoryTypeId="1" PostId="322" RevisionGUID="ae1bbd5d-dbcf-4e11-ab51-a367026d06f1" CreationDate="2017-05-31T22:56:26.803" UserId="33" Text="Downloading a reference Genome" />
  <row Id="985" PostHistoryTypeId="3" PostId="322" RevisionGUID="ae1bbd5d-dbcf-4e11-ab51-a367026d06f1" CreationDate="2017-05-31T22:56:26.803" UserId="33" Text="&lt;human-genome&gt;&lt;reference&gt;&lt;genome-sequencing&gt;" />
  <row Id="986" PostHistoryTypeId="2" PostId="323" RevisionGUID="68616e6c-f1d8-441d-b7db-e43c08c7d0e3" CreationDate="2017-05-31T23:21:53.943" UserId="247" Text="tl;dr: Just use the either the downloads on the [Bowtie2 homepage][1] or the [Illumina iGenomes][2]. Or just uncompress and concatenate the [FASTA files found on UCSC goldenpath][3] and then build the index.&#xD;&#xA;&#xD;&#xA;A bit longer answer:&#xD;&#xA;&#xD;&#xA;There are two components to &quot;genome for a read mapper&quot; such as Bowtie or BWA.&#xD;&#xA;&#xD;&#xA;First, you need to choose the actual sequence (genome release such as GRCh37/hg19 or GRCh38/hg38). There are patch releases such as GRCh37.p3 where some bases might be exchanged and depending on the release, some &quot;unmapped&quot; loci contigs might be added, but generally GRCh37.p1 is roughly the same as GRCh37.p2, for example. Usually, people have agreed on some specific patch version for each read and use this for read mapping.&#xD;&#xA;&#xD;&#xA;Generally, there is the UCSC flavour hg19/hg38 etc. and the NCBI/GRC flavour GRCh37, GRCh38 etc. (similar with mouse). UCSC has no versioning besides the genome release and (to the best of my knowledge) does not update the genome sequence after releasing a hg19 FASTA file.&#xD;&#xA;&#xD;&#xA;Second, you have to build the index files for each genome. Depending on the read mapper you use, you might or might not need the original FASTA files for the alignment. For Bowtie and Bowtie 2, you don't need the original FASTA files after building the index as Bowtie 1/2 can reconstruct the sequence &quot;on the fly&quot; from the index files.&#xD;&#xA;&#xD;&#xA;HTH&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bowtie-bio.sourceforge.net/bowtie2/index.shtml&#xD;&#xA;  [2]: https://support.illumina.com/sequencing/sequencing_software/igenome.html&#xD;&#xA;  [3]: http://hgdownload.soe.ucsc.edu/goldenPath/hg19/chromosomes/" />
  <row Id="987" PostHistoryTypeId="2" PostId="324" RevisionGUID="856c3794-c2b1-4ad8-9f8c-d9e58b07171f" CreationDate="2017-05-31T23:27:28.080" UserId="247" Text="That's a good question.&#xD;&#xA;&#xD;&#xA;I'd say that you don't need to bother with variant recalibration for&#xD;&#xA;&#xD;&#xA;- low number of samples (e.g., just two trios); I could not get GTAK recalibration of variant scores to work anyway&#xD;&#xA;- high-coverage samples (e.g., X Ten genomes with 30x coverage) where the DNA samples themselves are of high, comparable quality and have been sequenced with consistent technology.&#xD;&#xA;&#xD;&#xA;Generally, it is my impression that a lot of the thoughts and advanced statistical models built into GATK come from the earlier phases of the 1000 Genomes project. This means (1) low-coverage, (2) different coverage genomes (3) sequenced with varying technology versions by (4) different samples and (5) population sequencing.&#xD;&#xA;&#xD;&#xA;If you are in a clinical setting where you do 30x sequencing on X Ten platforms only anyway, then variant recalibration will probably not help you that much.&#xD;&#xA;&#xD;&#xA;On the other hand, if you are integrating many data sets from different data centers and machine versions etc., variant recalibration might be worth a shot.&#xD;&#xA;&#xD;&#xA;A good check would be looking at genotype quality distributions and other variant/quality related metrics before and after recalibration.&#xD;&#xA;&#xD;&#xA;Anyone: please correct me if I'm wrong!" />
  <row Id="988" PostHistoryTypeId="2" PostId="325" RevisionGUID="12a41b59-0d61-4951-9c99-989245916c34" CreationDate="2017-05-31T23:34:31.387" UserId="247" Text="I think that right now, the only human builds that are worth considering are hg19/GRCh37 as many data bases such as gnomAD still exclusively use this release. On the other hand, hg38/GRCh8 has many important fixes and the useful (but yet underused) feature of alternative loci.&#xD;&#xA;&#xD;&#xA;Anything from older releases should be remapped to a more recent one." />
  <row Id="989" PostHistoryTypeId="2" PostId="326" RevisionGUID="25c08c13-fe9d-4372-a445-41860bb10c2d" CreationDate="2017-05-31T23:36:32.283" UserId="247" Text="What is the &quot;best&quot; assembly for the popular model organisms:&#xD;&#xA;&#xD;&#xA;- human (GRCh37 and GRCh38 are obvious, I'd pick whatever bwakit uses)&#xD;&#xA;- mouse (GRCm37/GRCm38, OK)&#xD;&#xA;&#xD;&#xA;but what about non-human/mouse ones?&#xD;&#xA;&#xD;&#xA;- fruit fly&#xD;&#xA;- zebrafish&#xD;&#xA;- E. coli&#xD;&#xA;- any other idea?&#xD;&#xA;" />
  <row Id="990" PostHistoryTypeId="1" PostId="326" RevisionGUID="25c08c13-fe9d-4372-a445-41860bb10c2d" CreationDate="2017-05-31T23:36:32.283" UserId="247" Text="Which reference to use for read mapping for popular model organisms" />
  <row Id="991" PostHistoryTypeId="3" PostId="326" RevisionGUID="25c08c13-fe9d-4372-a445-41860bb10c2d" CreationDate="2017-05-31T23:36:32.283" UserId="247" Text="&lt;read-mapping&gt;&lt;reference-genome&gt;" />
  <row Id="992" PostHistoryTypeId="2" PostId="327" RevisionGUID="5a72b127-09de-4e6d-a383-0c4154411aa7" CreationDate="2017-05-31T23:38:44.793" UserId="247" Text="What are good means for performing QC or NGS reads?&#xD;&#xA;&#xD;&#xA;I'm aware of:&#xD;&#xA;&#xD;&#xA;- FastQC&#xD;&#xA;- NGS Screen&#xD;&#xA;- Kraken (e.g., for screening against contaminants)&#xD;&#xA;&#xD;&#xA;What are other popular means for such QC?" />
  <row Id="993" PostHistoryTypeId="1" PostId="327" RevisionGUID="5a72b127-09de-4e6d-a383-0c4154411aa7" CreationDate="2017-05-31T23:38:44.793" UserId="247" Text="QC measures for NGS sequencing" />
  <row Id="994" PostHistoryTypeId="3" PostId="327" RevisionGUID="5a72b127-09de-4e6d-a383-0c4154411aa7" CreationDate="2017-05-31T23:38:44.793" UserId="247" Text="&lt;ngs&gt;&lt;quality-control&gt;" />
  <row Id="995" PostHistoryTypeId="2" PostId="328" RevisionGUID="d3f072e0-3f7f-449b-9e9c-1ec4f6480a60" CreationDate="2017-05-31T23:40:03.393" UserId="247" Text="One big problem that I'm regularly facing is that URLs for downloading Bioinformatics data (e.g., RefSeq releases or NCBI genome releases) disappear.&#xD;&#xA;&#xD;&#xA;Does anyone have any good solution for this?" />
  <row Id="996" PostHistoryTypeId="1" PostId="328" RevisionGUID="d3f072e0-3f7f-449b-9e9c-1ec4f6480a60" CreationDate="2017-05-31T23:40:03.393" UserId="247" Text="Stable download URLs" />
  <row Id="997" PostHistoryTypeId="3" PostId="328" RevisionGUID="d3f072e0-3f7f-449b-9e9c-1ec4f6480a60" CreationDate="2017-05-31T23:40:03.393" UserId="247" Text="&lt;data-download&gt;" />
  <row Id="998" PostHistoryTypeId="2" PostId="329" RevisionGUID="aabecc18-cf49-4cea-8f0b-4d57f7a6f932" CreationDate="2017-05-31T23:46:41.107" UserId="247" Text="PacBio is selling ~10x PacBio SEQUEL long reads as an upgrade to Illumina data for SV discovery.&#xD;&#xA;&#xD;&#xA;In a clinical setting, the main requirements are proper sensitivity and specificity but also the processing of cohorts, at least families. This requires a genotyping step, such that it can be identified whether a given variant is shared by two or more individuals or whether it is not.&#xD;&#xA;&#xD;&#xA;What are the tools of the trade for this task?&#xD;&#xA;&#xD;&#xA;As having 50-60x PacBio reads is not an option from a economic point of view, one has to make do with 10x coverage." />
  <row Id="999" PostHistoryTypeId="1" PostId="329" RevisionGUID="aabecc18-cf49-4cea-8f0b-4d57f7a6f932" CreationDate="2017-05-31T23:46:41.107" UserId="247" Text="Structural variant calling for low-coverage PacBio data" />
  <row Id="1000" PostHistoryTypeId="3" PostId="329" RevisionGUID="aabecc18-cf49-4cea-8f0b-4d57f7a6f932" CreationDate="2017-05-31T23:46:41.107" UserId="247" Text="&lt;structural-variation&gt;&lt;long-reads&gt;" />
  <row Id="1001" PostHistoryTypeId="2" PostId="330" RevisionGUID="acb458bd-3163-497b-997e-08435c31374c" CreationDate="2017-05-31T23:48:37.513" UserId="247" Text="I'm aware of the following (very few and suboptimal) options:&#xD;&#xA;&#xD;&#xA;- [Sniffles](https://github.com/fritzsedlazeck/Sniffles/releases) -- sadly not very reliable in my experience, also no genotyping step or multi-sample support&#xD;&#xA;- [PB Honey](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-180) -- no genotyping step or multi-sample support" />
  <row Id="1003" PostHistoryTypeId="2" PostId="332" RevisionGUID="90eece72-4a5a-49fa-88d3-d33f5c2b44c6" CreationDate="2017-06-01T02:02:50.910" UserId="73" Text="If the programs you are using allow for it, take the most recent available genome, as it will be most likely to have the fewest errors.&#xD;&#xA;&#xD;&#xA;Many of the well-known model organisms have an official release site (e.g. [flybase][1] for drosophila, [wormbase][2] for nematodes) from which genomes are fairly promptly propagated through to larger public databases, of which the most well-known are probably [Ensembl][3] and [RefSeq Genome][4]. There's usually enough cross-talk between the large data repositories that it doesn't matter all that much where the source is, as long as it is at least a few months since the last genome release.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://flybase.org/&#xD;&#xA;  [2]: http://www.wormbase.org/&#xD;&#xA;  [3]: http://www.ensembl.org/info/about/species.html&#xD;&#xA;  [4]: https://www.ncbi.nlm.nih.gov/genome/browse/" />
  <row Id="1004" PostHistoryTypeId="2" PostId="333" RevisionGUID="369ebcef-ea4c-4ed3-9d0d-415f6b83081b" CreationDate="2017-06-01T03:27:11.300" UserId="269" Text="I would like to show that certain types of RBP motifs are enriched in RNA editing islands (i.e. clusters of RNA editing). However, I am unsure about how to think about sequence motifs with respect to their occurrence in other genomic features.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I understand how to find the probability of a motif at a location. i.e.&#xD;&#xA;&#xD;&#xA;P(sequence is at position i) = P(A)^[A] * P(C)^[C] * P(G)^[G] * P(T)^[T]&#xD;&#xA;&#xD;&#xA;where (.) is the base pair and [.] is the number of bps. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Instead what I would like to find is: &#xD;&#xA;&#xD;&#xA;P(sequence S is contained in a feature type T) = ???&#xD;&#xA;&#xD;&#xA;where feature type T is an gene, intron, editing island, etc. I think I should incorporate length since I will mainly be comparing genes or introns vs. editing islands. Also, I am not sure what to do about the editing islands being located within genes. How can I keep from counting the same motif twice?&#xD;&#xA;&#xD;&#xA;Any ideas would be greatly appreciated. Thank you for your time. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1005" PostHistoryTypeId="1" PostId="333" RevisionGUID="369ebcef-ea4c-4ed3-9d0d-415f6b83081b" CreationDate="2017-06-01T03:27:11.300" UserId="269" Text="How to calculate statistical significance of sequence motifs" />
  <row Id="1006" PostHistoryTypeId="3" PostId="333" RevisionGUID="369ebcef-ea4c-4ed3-9d0d-415f6b83081b" CreationDate="2017-06-01T03:27:11.300" UserId="269" Text="&lt;statistics&gt;&lt;motifs&gt;" />
  <row Id="1007" PostHistoryTypeId="2" PostId="334" RevisionGUID="45ad2485-e501-4960-a380-027dc1bb2266" CreationDate="2017-06-01T04:40:50.010" UserId="425" Text="I will answer one of the points – E.coli.&#xD;&#xA;&#xD;&#xA;**TL;DR** Bacteria, and in particular E.coli, are highly variable and there is usually no single best assembly. Large scale WGS studies should come with multiple assemblies for individual monophyletic clusters.&#xD;&#xA;&#xD;&#xA;**Long answer:** Whereas a single reference sequence can make sense for human or mice (to a certain extent), bacteria are not the case. The variability between them even within a single species can be enormous and various lineages can contain very different genes. This is the reason why biologists usually distinguish a *core genome* consisting of the genes shared among a vast majority (typically 95%) of individuals within a given phylogenetic clade, (i.e., of those genes which are essential for this clade – imagine a CPU and RAM in a computer), and an *accessory genome* consisting of the other, unnecessary, genes – imagine a joystick in our analogy). The size of the accessory genome can vary a lot for different bacteria. For instance, Escherichia coli is said to have an *open* accessory genome (i.e., much bigger than the core genome; imagine a Rapsberry-PI) whereas Chlamydia trachomatis has a *closed* accessory genome (i.e., small compared to the core genome; imagine an ATM).&#xD;&#xA;&#xD;&#xA;For species having a small accessory genome, a single reference might be sufficient. However, if a core genome is large, we can use either pan-genomes, or multiple references. The word pan-genome tends to be used for different things: the [computational biologists](https://www.ncbi.nlm.nih.gov/pubmed/27769991) usually use it as a representation of all genomic content in a certain phylogenetic clade (which can be mathematically modeled using finite automata, i.e., graphs) whereas some biologists define a pan-genome as a concatenation of all (core and accessory) genes from the clade (without considering SNPs, etc.). Unfortunately, methods for graph references (especially for read mapping) are still not sufficiently developed, therefore, researchers still have to resort to reference sequence-based methods and use either concatenated genes or multiple reference sequences.&#xD;&#xA;&#xD;&#xA;When particular bacterial species are studied using massive whole-genome sequencing of many isolates, researchers usually cluster the isolates to monophyletic sequence clusters and infer a special reference sequence for each of these clusters (see, e.g., this [example](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3725542/) for the pneumococcus). All of the inferred references should be then published together with the paper or in a separate data paper (see, e.g., the [sequences from the previous example](http://datadryad.org/resource/doi:10.5061/dryad.t55gq/8)).&#xD;&#xA;&#xD;&#xA;**Disclaimer:** I am not a biologist. If anything in this post is too simplified or incorrect (which is likely), please, leave me a comment and I will try to fix it. I tried to make this text accessible for my younger self." />
  <row Id="1008" PostHistoryTypeId="4" PostId="322" RevisionGUID="6105ccb0-4dbb-4fa5-9b0a-b163e278d5ac" CreationDate="2017-06-01T06:47:21.743" UserId="73" Comment="Improved clarity of title" Text="Downloading a reference Genome for Bowtie2" />
  <row Id="1009" PostHistoryTypeId="24" PostId="322" RevisionGUID="6105ccb0-4dbb-4fa5-9b0a-b163e278d5ac" CreationDate="2017-06-01T06:47:21.743" Comment="Proposed by 73 approved by 77, 163 edit id of 89" />
  <row Id="1010" PostHistoryTypeId="2" PostId="335" RevisionGUID="26aae06d-b44b-46fb-8dbf-ba986c5dab84" CreationDate="2017-06-01T07:05:29.963" UserId="476" Text="One big advantage of Reactome, in my opinion, is its visualization using the web interface. &#xD;&#xA;&#xD;&#xA;Many pathways (in Reactome and KEGG) consist of genes / proteins that are up- and down-regulated through the respective pathway. If you do a simple overrepresentation analysis this is not taken into consideration. Therefore, you might end up seeing a pathway as &quot;overexpressed&quot; although only the down-regulated genes were observed more frequently.&#xD;&#xA;&#xD;&#xA;In Reactome, you can zoom in on the different pathways very conveniently and then pick up these inconsistencies. I haven't really found a public database and tool that can take these different regulations into consideration. Therefore, you'll probably always will need some manual investigation of your data. In my opinion, this is easier with Reactome than with KEGG." />
  <row Id="1011" PostHistoryTypeId="2" PostId="336" RevisionGUID="0b051a69-3140-43ae-82b3-8d08bfaf3669" CreationDate="2017-06-01T07:27:25.760" UserId="59" Text="What is the current standard for imputing missing genotypes between two genotyping panels?  I have two populations genotyped using two different panels (A &amp; B), and I would like to impute all the genotypes in population B for those positions on used in panel A.  &#xD;&#xA;&#xD;&#xA;I've read the examples for impute2, and I think the closest thing to what I am looking for is [this example](https://mathgen.stats.ox.ac.uk/impute/impute_v2.html#ex5), &quot;Imputation with one unphased reference panel&quot;.  &#xD;&#xA;&#xD;&#xA;Simply put, I want to provide a list of SNPs, some variant file for population B, and haplotype information from 1,000 Genomes and get imputed genotypes for each SNP in the list.  Is impute2 the state of the art for this?" />
  <row Id="1012" PostHistoryTypeId="1" PostId="336" RevisionGUID="0b051a69-3140-43ae-82b3-8d08bfaf3669" CreationDate="2017-06-01T07:27:25.760" UserId="59" Text="Missing genotype imputation" />
  <row Id="1013" PostHistoryTypeId="3" PostId="336" RevisionGUID="0b051a69-3140-43ae-82b3-8d08bfaf3669" CreationDate="2017-06-01T07:27:25.760" UserId="59" Text="&lt;imputation&gt;&lt;impute2&gt;" />
  <row Id="1015" PostHistoryTypeId="2" PostId="338" RevisionGUID="9e3b0d15-0214-4bff-9fba-7b6a2c7f3f44" CreationDate="2017-06-01T07:33:33.863" UserId="163" Text="I find that in many contexts, the terms computational biology, bioinformatics and biostatistics are often treated as functionally equivalent, and yet for students selecting PhD programs and the like the difference could be quite significant. Is there a standard or rigorous definition of these terms and the difference between them?" />
  <row Id="1016" PostHistoryTypeId="1" PostId="338" RevisionGUID="9e3b0d15-0214-4bff-9fba-7b6a2c7f3f44" CreationDate="2017-06-01T07:33:33.863" UserId="163" Text="Difference between computational biology, bioinformatics and biostatistics" />
  <row Id="1017" PostHistoryTypeId="3" PostId="338" RevisionGUID="9e3b0d15-0214-4bff-9fba-7b6a2c7f3f44" CreationDate="2017-06-01T07:33:33.863" UserId="163" Text="&lt;statistics&gt;&lt;computation&gt;" />
  <row Id="1018" PostHistoryTypeId="10" PostId="308" RevisionGUID="f507e814-83f5-4423-9d04-7f7d9d189454" CreationDate="2017-06-01T07:42:54.877" UserId="-1" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:215,&quot;DisplayName&quot;:&quot;SamStudio8&quot;},{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;},{&quot;Id&quot;:2,&quot;DisplayName&quot;:&quot;Henry WH Hack v2.0&quot;},{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;}]}" />
  <row Id="1019" PostHistoryTypeId="5" PostId="318" RevisionGUID="b2de4cbc-48f2-4140-9200-86d631704048" CreationDate="2017-06-01T07:54:39.567" UserId="131" Comment="added 175 characters in body; edited tags" Text="Is working with and relying on old genome builds still valid?&#xD;&#xA;&#xD;&#xA;For example [NCBI36/hg18](https://genome-euro.ucsc.edu/cgi-bin/hgGateway?db=hg18&amp;redirect=manual&amp;source=genome.ucsc.edu). Would results from papers based on old builds require [LiftOver](https://genome.ucsc.edu/cgi-bin/hgLiftOver) and re-analysis to be useful?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-----------&#xD;&#xA;&#xD;&#xA;**^** This post might be too broad, off topic, but would like to know your views.&#xD;&#xA;&#xD;&#xA;--------&#xD;&#xA;**EDIT:**&#xD;&#xA;&#xD;&#xA;A bit of context, this is related to other post, where we have aCGH results based on old build: https://bioinformatics.stackexchange.com/q/316/131&#xD;&#xA;" />
  <row Id="1020" PostHistoryTypeId="6" PostId="318" RevisionGUID="b2de4cbc-48f2-4140-9200-86d631704048" CreationDate="2017-06-01T07:54:39.567" UserId="131" Comment="added 175 characters in body; edited tags" Text="&lt;array-cgh&gt;&lt;liftover&gt;&lt;build&gt;&lt;hg18&gt;&lt;ncbi36&gt;" />
  <row Id="1021" PostHistoryTypeId="10" PostId="196" RevisionGUID="4b0d5df5-87d2-420d-a62c-5bf09fef52c5" CreationDate="2017-06-01T08:07:26.680" UserId="-1" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:389,&quot;DisplayName&quot;:&quot;Uwe Keim&quot;},{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:176,&quot;DisplayName&quot;:&quot;JonMark Perry&quot;},{&quot;Id&quot;:355,&quot;DisplayName&quot;:&quot;Nikita&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;}]}" />
  <row Id="1022" PostHistoryTypeId="2" PostId="339" RevisionGUID="9d187739-d29c-4ff3-a03a-e9b82f408941" CreationDate="2017-06-01T08:22:35.933" UserId="191" Text="As far as I remember the exact probability computation is an open problem. The reason is that potential motifs can overlap, which makes probability computations for an arbitrary string non-trivial, and depends on the motif.&#xD;&#xA;&#xD;&#xA;For example if he have a binary string of four digits, the probability of &quot;01&quot; will be 11/16, while the probability of &quot;11&quot; will be 8/16=1/2.&#xD;&#xA;&#xD;&#xA;The simplest approximation is to assume that probability of the motif on every position of the sequence is equal and independent. In [such case][1] (this comes from [this course][2]):&#xD;&#xA;&#xD;&#xA;[![Pr(N,A,Pattern,t)][3]][3]&#xD;&#xA;&#xD;&#xA;(Probability of text of length N having at least t occurrences of a k-mer Pattern, A is the number of letters in the alphabet, n = N – t * k)&#xD;&#xA;&#xD;&#xA;This approximation is very rough, there are better [here][4].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://stepik.org/lesson/Some-Hidden-Messages-are-More-Surprising-than-Others-3/step/5?course=Bioinformatics-Algorithms&amp;unit=7&#xD;&#xA;  [2]: https://stepik.org/course/Bioinformatics-Algorithms-2&#xD;&#xA;  [3]: https://i.stack.imgur.com/WZH86.png&#xD;&#xA;  [4]: https://doi.org/10.1016/0097-3165(81)90005-4" />
  <row Id="1023" PostHistoryTypeId="10" PostId="338" RevisionGUID="225bf8dc-ff86-400a-800d-abfa5b3a4885" CreationDate="2017-06-01T08:33:53.843" UserId="-1" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:104,&quot;DisplayName&quot;:&quot;Chris_Rands&quot;},{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;},{&quot;Id&quot;:176,&quot;DisplayName&quot;:&quot;JonMark Perry&quot;},{&quot;Id&quot;:191,&quot;DisplayName&quot;:&quot;Iakov Davydov&quot;}]}" />
  <row Id="1024" PostHistoryTypeId="10" PostId="328" RevisionGUID="57aa2227-4979-4c6a-8a6d-1a4b63dfe152" CreationDate="2017-06-01T08:45:46.243" UserId="-1" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:104,&quot;DisplayName&quot;:&quot;Chris_Rands&quot;},{&quot;Id&quot;:59,&quot;DisplayName&quot;:&quot;Greg&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;},{&quot;Id&quot;:176,&quot;DisplayName&quot;:&quot;JonMark Perry&quot;},{&quot;Id&quot;:191,&quot;DisplayName&quot;:&quot;Iakov Davydov&quot;}]}" />
  <row Id="1025" PostHistoryTypeId="2" PostId="340" RevisionGUID="8f6658b1-0f98-4c0a-8b7f-fb4bca45eead" CreationDate="2017-06-01T09:01:04.053" UserId="191" Text="There are couple of computational methods which try to do this (I never used them, so no experience):&#xD;&#xA;&#xD;&#xA;1. [CellMix][1], based on sets of marker gene lists&#xD;&#xA;2. [Subset Prediction from Enrichment Correlation][2], which is based on correlations with subset-specific genes across a set of samples.&#xD;&#xA;3. [Cell type enrichment][3], which uses our highly expressed, cell specific gene database&#xD;&#xA;4. [Cell type-specific significance analysis][4] using differential gene expression for each cell type&#xD;&#xA;&#xD;&#xA;You might have to get some reference expression levels from public databases or papers for some of the methods.&#xD;&#xA;&#xD;&#xA;One thing to keep in mind: you cannot really compute the cells proportion, only RNA proportion. If you have a good reason to assume that RNA quantity &#xD;&#xA;per cell is very similar, this is a good proxy for the cells proportion in a tissue.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://doi.org/10.1093/bioinformatics/btt351&#xD;&#xA;  [2]: https://dx.doi.org/10.1186/1471-2105-12-258&#xD;&#xA;  [3]: https://dx.doi.org/10.1186/1471-2164-13-460&#xD;&#xA;  [4]: https://dx.doi.org/10.1038/nmeth.1439" />
  <row Id="1027" PostHistoryTypeId="2" PostId="341" RevisionGUID="1e6072d6-ff92-4941-b84a-1d1c7bfb286a" CreationDate="2017-06-01T09:38:53.087" UserId="29" Text="It’s a matter of preference I guess but I recommend the [**Ensembl** builds](http://grch37.ensembl.org/index.html). Decide whether you want the toplevel or primary assembly, and whether you want soft-masked, repeat-masked or unmasked files. The naming schema is very straightforward; the combinations are described in the [`README` file](ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/dna/README), and all files [reside in one directory](ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/dna/).&#xD;&#xA;&#xD;&#xA;For example, if you want the unmasked primary assembly, the file to download would be [`Homo_sapiens.GRCh37.75.dna.primary_assembly.fa.gz`](ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa.gz).&#xD;&#xA;&#xD;&#xA;As for **GoldenPath/UCSC**, there’s no need to download and concatenate separate chromosomes (contrary to what the other answer said); you can download the whole reference [from the `bigZips` directory](http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/); from the `README`:&#xD;&#xA;&#xD;&#xA;&gt; This directory contains the Feb. 2009 assembly of the human genome (hg19,&#xD;&#xA;GRCh37 Genome Reference Consortium Human Reference 37 (GCA_000001405.1)),&#xD;&#xA;as well as repeat annotations and GenBank sequences.&#xD;&#xA;&#xD;&#xA;There are essentially three options here:&#xD;&#xA;&#xD;&#xA;1. `chromFa.tar.gz`, which contains the whole genome in one chromosome per file;&#xD;&#xA;2. `chromFaMasked.tar.gz`, the same with repeats masked by `N`;&#xD;&#xA;3. `hg19.2bit`, which is the whole genome in one file, but needs to be extracted using the utility program `twoBitToFa`, which needs to be downloaded [separately](http://hgdownload.cse.ucsc.edu/admin/exe).&#xD;&#xA;&#xD;&#xA;In any case, I always download the reference and build my own index for mapping, since this allows me more control; not everybody might need this much control, but then building the index once is fairly fast anyway." />
  <row Id="1028" PostHistoryTypeId="5" PostId="341" RevisionGUID="36b2bfed-87fd-45db-89fd-b4cb25434b15" CreationDate="2017-06-01T09:49:08.560" UserId="29" Comment="added 11 characters in body" Text="It’s a matter of preference I guess but I recommend the [**Ensembl** builds](http://grch37.ensembl.org/index.html). Decide whether you want the toplevel or primary assembly, and whether you want soft-masked, repeat-masked or unmasked files. The naming schema is very straightforward; the combinations are described in the [`README` file](ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/dna/README), and all files [reside in one directory](ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/dna/).&#xD;&#xA;&#xD;&#xA;For example, if you want the unmasked primary assembly, the file to download would be [`Homo_sapiens.GRCh37.75.dna.primary_assembly.fa.gz`](ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa.gz).&#xD;&#xA;&#xD;&#xA;As for **GoldenPath/UCSC**, there’s no need to download and concatenate separate chromosomes (contrary to what the other answer said); you can download the whole (toplevel) reference [from the `bigZips` directory](http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/); from the `README`:&#xD;&#xA;&#xD;&#xA;&gt; This directory contains the Feb. 2009 assembly of the human genome (hg19,&#xD;&#xA;GRCh37 Genome Reference Consortium Human Reference 37 (GCA_000001405.1)),&#xD;&#xA;as well as repeat annotations and GenBank sequences.&#xD;&#xA;&#xD;&#xA;There are essentially three options here:&#xD;&#xA;&#xD;&#xA;1. `chromFa.tar.gz`, which contains the whole genome in one chromosome per file;&#xD;&#xA;2. `chromFaMasked.tar.gz`, the same with repeats masked by `N`;&#xD;&#xA;3. `hg19.2bit`, which is the whole genome in one file, but needs to be extracted using the utility program `twoBitToFa`, which needs to be downloaded [separately](http://hgdownload.cse.ucsc.edu/admin/exe).&#xD;&#xA;&#xD;&#xA;In any case, I always download the reference and build my own index for mapping, since this allows me more control; not everybody might need this much control, but then building the index once is fairly fast anyway." />
  <row Id="1029" PostHistoryTypeId="2" PostId="342" RevisionGUID="90d3624b-0252-44f9-bc87-2064b59bc192" CreationDate="2017-06-01T09:56:56.927" UserId="485" Text="[MultiQC](http://multiqc.info/) can merge all your different reports into a single one.&#xD;&#xA;Which could be useful once you manage to know which QC tools to use." />
  <row Id="1030" PostHistoryTypeId="10" PostId="327" RevisionGUID="6e725f95-fef8-402b-b0b8-5bfd499e4ca8" CreationDate="2017-06-01T10:57:08.637" UserId="-1" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;},{&quot;Id&quot;:176,&quot;DisplayName&quot;:&quot;JonMark Perry&quot;},{&quot;Id&quot;:104,&quot;DisplayName&quot;:&quot;Chris_Rands&quot;},{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;}]}" />
  <row Id="1031" PostHistoryTypeId="2" PostId="343" RevisionGUID="f45bdc83-39a2-473e-801d-bd057cb3debb" CreationDate="2017-06-01T11:11:09.893" UserId="29" Text="I’m using the [RepBase libraries](http://www.girinst.org/server/RepBase/index.php) in conjunction with RepeatMasker to get genome-wide repeat element annotations.&#xD;&#xA;&#xD;&#xA;This works well enough, and seems to be the de facto standard in the field.&#xD;&#xA;&#xD;&#xA;However, there are two issues with the use of RepBase, which is why I (and others) have been (so far without success) for alternatives:&#xD;&#xA;&#xD;&#xA;1. [RepBase isn’t open data](http://www.girinst.org/accountservices/register.php?commercial=0). Their academic license agreement includes a clause that *explicitly forbids dissemination of data derived from RepBase*. It’s unclear to what extent this is binding/enforceable, but it effectively prevents publishing at least some of the data I’m using and generating. This is unacceptable for [open science](https://en.wikipedia.org/wiki/Open_science).&#xD;&#xA;&#xD;&#xA;   * Subordinate to this, the subscription model of RepBase also makes it impossible to integrate RepBase into fully automated pipelines, because user interaction is required to subscribe to RepBase provide the login credentials.&#xD;&#xA;&#xD;&#xA;2. RepBase is heavily manually curated. This is both good and bad. Good, because manual curation of sequence data is often the most reliable form of curation. On the flip side, manual curation is inherently biased; and worse, it’s hard to quantify this bias — [this is acknowledged by the RepBase maintainers](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-474)." />
  <row Id="1032" PostHistoryTypeId="1" PostId="343" RevisionGUID="f45bdc83-39a2-473e-801d-bd057cb3debb" CreationDate="2017-06-01T11:11:09.893" UserId="29" Text="Are there any RepBase alternatives?" />
  <row Id="1033" PostHistoryTypeId="3" PostId="343" RevisionGUID="f45bdc83-39a2-473e-801d-bd057cb3debb" CreationDate="2017-06-01T11:11:09.893" UserId="29" Text="&lt;annotation&gt;&lt;database&gt;&lt;repeat-elements&gt;" />
  <row Id="1034" PostHistoryTypeId="2" PostId="344" RevisionGUID="0ef39e8c-e5ee-4e06-8d41-2893cac28b39" CreationDate="2017-06-01T11:15:55.563" UserId="61" Text="What are the key differences between VCF versions 4.1 and 4.2?&#xD;&#xA; It looks like v4.3 contains a changelog (specs available [here](http://samtools.github.io/hts-specs/)) but earlier specifications do not. &#xD;&#xA;&#xD;&#xA;[This](https://www.biostars.org/p/211152/) biostar post points out one difference: the introduction of `Number=R` for fields with one value per allele including REF — can anyone enumerate the other changes between these two versions?" />
  <row Id="1035" PostHistoryTypeId="1" PostId="344" RevisionGUID="0ef39e8c-e5ee-4e06-8d41-2893cac28b39" CreationDate="2017-06-01T11:15:55.563" UserId="61" Text="What's the difference between VCF spec versions 4.1 and 4.2?" />
  <row Id="1036" PostHistoryTypeId="3" PostId="344" RevisionGUID="0ef39e8c-e5ee-4e06-8d41-2893cac28b39" CreationDate="2017-06-01T11:15:55.563" UserId="61" Text="&lt;formats&gt;&lt;vcf&gt;&lt;htslib&gt;" />
  <row Id="1037" PostHistoryTypeId="5" PostId="289" RevisionGUID="0faa3421-b2ee-41a2-af24-31d16e56ed1c" CreationDate="2017-06-01T11:37:36.920" UserId="425" Comment="Fixed word" Text="Is there any resource (paper, blogpost, Github gist, etc.) describing the BWA-MEM algorithm for assigning mapping qualities? I vaguely remember that I have somewhere seen a formula for SE reads, which looked like&#xD;&#xA;&#xD;&#xA;    C * (s_1 - s_2) / s_1,&#xD;&#xA;    &#xD;&#xA;where `s_1` and `s_2` denoted the alignment scores of two best alignments and `C` was some constant.&#xD;&#xA;&#xD;&#xA;I believe that a reimplementation of this algorithm in some scripting language could be very useful for the bioinfo community. For instance, I sometimes test various mapping methods and some of them tend to find good alignments, but fail in assigning appropriate qualities. Therefore, I would like to re-assign all the mapping qualities in a SAM file with the BWA-MEM algorithm.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Btw. This algorithm must already have been implemented outside BWA, see the BWA-MEM paper: &#xD;&#xA;&gt; GEM does not compute mapping quality. Its&#xD;&#xA;mapping quality is estimated with a BWA-like algorithm with suboptimal&#xD;&#xA;alignments available.&#xD;&#xA;&#xD;&#xA;Unfortunately, the [BWA-MEM paper repo](https://github.com/lh3/mem-paper/) contains only the resulting `.eval` files.&#xD;&#xA;&#xD;&#xA;**Update:** The question is *not* about the algorithm for computing alignment scores. Mapping qualities and alignment scores are two different things:&#xD;&#xA;&#xD;&#xA; * Alignment score quantifies the similarity between two sequences (e.g., a read and a reference sequence)&#xD;&#xA; * Mapping quality (MAQ) quantifies the probability that a read is aligned to a wrong position.&#xD;&#xA;&#xD;&#xA;Even alignments with high scores can have a very low mapping quality." />
  <row Id="1039" PostHistoryTypeId="2" PostId="345" RevisionGUID="016f350b-bce5-4017-808c-99da62f17ca0" CreationDate="2017-06-01T11:40:33.550" UserId="191" Text="This is easy to check, you can download both specs in .tex format and do `diff`.&#xD;&#xA;&#xD;&#xA;Changes to the v4.2 compared to v4.1:&#xD;&#xA;&#xD;&#xA;1. Information field format: adding source and version as recommended fields.&#xD;&#xA;2. INFO field can have one value for each possible allele (code `R`).&#xD;&#xA;3. For all of the ##INFO, ##FORMAT, ##FILTER, and ##ALT metainformation, extra fields can be included after the default fields.&#xD;&#xA;4. Alternate base (ALT) can include `*`: missing due to a upstream deletion.&#xD;&#xA;5. Quality scores, a sentence removed: *High QUAL scores indicate high confidence calls. Although traditionally people use integer phred scores, this field is permitted to be a floating point to enable higher resolution for low confidence calls if desired.*&#xD;&#xA;6. Examples changed a bit.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1040" PostHistoryTypeId="2" PostId="346" RevisionGUID="26fe9adc-d6af-454f-baa5-25ab9256dcdb" CreationDate="2017-06-01T11:52:11.200" UserId="490" Text="mauve documentation have a similar command but the usage is `--id-matrix=&lt;file&gt;`. So very similar to the progressivemauve command (just doesn't include --input). Try it and see if it works. &#xD;&#xA;&#xD;&#xA;I think it's a bug for sure as several other people on biostar have complained. &#xD;&#xA;" />
  <row Id="1041" PostHistoryTypeId="2" PostId="347" RevisionGUID="75b78dfc-f663-4483-bb98-c535cda7e479" CreationDate="2017-06-01T12:09:58.040" UserId="104" Text="You could use [RepeatScout][1], which has defined repeat libraries for a limited number of species (including human, mouse, and rat). If your taxon is not represented, you can also do de novo repeat prediction with RepeatScout to build your own library to feed to RepeatMasker. The [RepeatScout publication][2] includes some comparisons with RepBase. Another related tool is [RepeatModeler][3], which wraps RepeatScout with [RECON][4] and some other programs, and shares authors with the RepeatMasker team.&#xD;&#xA;&#xD;&#xA;On the plus side RepeatScout/RepeatModeler are [open source][5] and do not use manual curation, meeting your criteria. On the negative, I'm not sure exactly how RepeatModeler and the component tools are maintained. The RepeatScout web and github pages have not been updated for several years, although the RepeatModeler page shows its latest release was in 2017. Anyway, I know that some combination of RepeatScout/RepeatModeler have been used to annotate repeats for some *fairly recent* newly sequenced genomes, e.g. for [cichlids][6], [coelacanth][7], and [Darwin's finch][8], so I think it's fair to say this kind of approach is accepted in the field, at least for vertebrate genome projects.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bix.ucsd.edu/repeatscout/&#xD;&#xA;  [2]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.517.8206&amp;rep=rep1&amp;type=pdf&#xD;&#xA;  [3]: http://www.repeatmasker.org/RepeatModeler/&#xD;&#xA;  [4]: http://eddylab.org/software/recon/&#xD;&#xA;  [5]: https://github.com/mmcco/RepeatScout&#xD;&#xA;  [6]: https://www.nature.com/nature/journal/v513/n7518/full/nature13726.html&#xD;&#xA;  [7]: https://www.nature.com/nature/journal/v496/n7445/full/nature12027.html&#xD;&#xA;  [8]: https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-14-95" />
  <row Id="1042" PostHistoryTypeId="5" PostId="321" RevisionGUID="fd2ecedc-0455-43b4-968d-a65dfc1249bd" CreationDate="2017-06-01T12:11:39.370" UserId="57" Comment="specified that I talk about BQSR" Text="The most of the variant calling pipelines have a step for recalibration of the quality scores of bases in Illumina reads ([GATK good practises](https://software.broadinstitute.org/gatk/best-practices/bp_3step.php?case=GermShortWGS&amp;p=1)), which is dependent on the known variants. Recently some work as been done for reference-free recalibration of scores as well: [Lancer](http://biorxiv.org/content/early/2017/04/27/130732) and [atlas](http://www.genetics.org/content/early/2016/11/07/genetics.116.189985), which is motivated by making the most for aDNA and low coverage datasets. The importance for aDNA is explained in [this lecture][1], but it is not clear to me if / how much it is important for fresh DNA samples with decent (&gt;15x) coverage. Especially when I work with non-model organisms and I can not simply use the standard tools.&#xD;&#xA;&#xD;&#xA;Q: How big impact has recalibration of scores on variant calling? Is there a rule of thumb for which it is / it is not worth the effort?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.youtube.com/watch?v=oJ9pbQsyaUg&amp;list=PLoCxWrRWjqB1JUCntl4X09ezmOtKx1Gke&amp;index=4" />
  <row Id="1043" PostHistoryTypeId="2" PostId="348" RevisionGUID="09b2558b-0859-4ef6-8b0a-003341e62e92" CreationDate="2017-06-01T12:15:47.873" UserId="29" Text="Dfam has recently launched a sister resource, [**Dfam_consensus**](http://www.dfam-consensus.org/), whose [stated aim](https://xfam.wordpress.com/2017/05/18/introducing-dfam_consensus-dfams-consensus-sequence-twin/) is to replace RepBase. From the annoucement:&#xD;&#xA;&#xD;&#xA;&gt; Dfam_consensus provides an open framework for the community to store both seed alignments (multiple alignments of instances for a given family) and the corresponding consensus sequence model.&#xD;&#xA;&#xD;&#xA;Both RepeatMasker and RepeatModeler have been updated to support Dfam_consensus.&#xD;&#xA;&#xD;&#xA;I haven’t tried it yet but it looks promising." />
  <row Id="1044" PostHistoryTypeId="4" PostId="343" RevisionGUID="f3c62630-3fef-4d1c-823d-c615e9036428" CreationDate="2017-06-01T12:25:17.213" UserId="73" Comment="added more description into the question title" Text="Are there any RepBase alternatives for genome-wide repeat element annotations?" />
  <row Id="1045" PostHistoryTypeId="4" PostId="289" RevisionGUID="10c196db-55a8-4600-bd95-d60dbbbba73f" CreationDate="2017-06-01T12:26:58.313" UserId="73" Comment="clarification of question" Text="How does the BWA-MEM algorithm assign its mapping qualities?" />
  <row Id="1046" PostHistoryTypeId="4" PostId="278" RevisionGUID="d69a321b-853d-43ab-8a17-38222c42aeca" CreationDate="2017-06-01T12:28:58.280" UserId="73" Comment="edited title" Text="How can the cell line contribution be estimated from RNASeq data?" />
  <row Id="1047" PostHistoryTypeId="2" PostId="349" RevisionGUID="083fc293-c699-4991-bb97-745b91806ae1" CreationDate="2017-06-01T12:48:19.277" UserId="73" Text="Cell deconvolution is mentioned in [this Biostars post][1], which mentions CIBERSORT for immune cell mixes, and the Bioconductor package [DeconRNASeq][2].&#xD;&#xA;&#xD;&#xA;As far as I'm aware, it is only possible at best to get proportional representation for transcript expression from standard high-throughput sequencing results, because the sequencers and sample preparation workflow are designed in such a way that the same number of reads are output regardless of the input amount.&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/160961/&#xD;&#xA;  [2]: http://www.bioconductor.org/packages/release/bioc/html/DeconRNASeq.html" />
  <row Id="1048" PostHistoryTypeId="2" PostId="350" RevisionGUID="a98e7c36-1548-4040-a20c-7cffa42a06d6" CreationDate="2017-06-01T13:04:41.200" UserId="266" Text="Models of structures deposited in the Protein Data Bank vary in the quality, depending both on the data quality and expertise and patience of the person who built the model. Is there a well-accepted subset of the PDB entries that has only &quot;high quality&quot; structures? Ideally these structures would be representative for classes of proteins in the whole PDB.&#xD;&#xA;&#xD;&#xA;&lt;sub&gt;based on a [real question][1] from biology.SE&lt;/sub&gt;&#xD;&#xA;&#xD;&#xA;[1]: https://biology.stackexchange.com/questions/56946/subset-of-protein-crystal-structures-from-pdb/" />
  <row Id="1049" PostHistoryTypeId="1" PostId="350" RevisionGUID="a98e7c36-1548-4040-a20c-7cffa42a06d6" CreationDate="2017-06-01T13:04:41.200" UserId="266" Text="How to select high quality structures from the Protein Data Bank?" />
  <row Id="1050" PostHistoryTypeId="3" PostId="350" RevisionGUID="a98e7c36-1548-4040-a20c-7cffa42a06d6" CreationDate="2017-06-01T13:04:41.200" UserId="266" Text="&lt;protein-structure&gt;&lt;pdb&gt;" />
  <row Id="1051" PostHistoryTypeId="2" PostId="351" RevisionGUID="0c6dc538-9868-4885-9d7c-144671ebce24" CreationDate="2017-06-01T13:08:19.863" UserId="374" Text="Can anyone recommend a good tool for estimating the tumor content given a matched tumor and normal file for DNA NGS whole genome sequencing data or whole exome data?&#xD;&#xA;&#xD;&#xA;Is it possible to estimate this without a normal sample as well?" />
  <row Id="1052" PostHistoryTypeId="1" PostId="351" RevisionGUID="0c6dc538-9868-4885-9d7c-144671ebce24" CreationDate="2017-06-01T13:08:19.863" UserId="374" Text="Tumor purity/contamination estimation" />
  <row Id="1053" PostHistoryTypeId="3" PostId="351" RevisionGUID="0c6dc538-9868-4885-9d7c-144671ebce24" CreationDate="2017-06-01T13:08:19.863" UserId="374" Text="&lt;cancer&gt;&lt;wgs&gt;&lt;wes&gt;&lt;contamination&gt;&lt;purity&gt;" />
  <row Id="1055" PostHistoryTypeId="4" PostId="351" RevisionGUID="36455c0b-6c1b-430e-a18e-7ee5b2600923" CreationDate="2017-06-01T13:20:48.980" UserId="77" Comment="Added the &quot;admixture&quot; term to the title and tags. That's the correct term to describe what's being talked about here." Text="Tumor purity/contamination (admixture) estimation" />
  <row Id="1056" PostHistoryTypeId="6" PostId="351" RevisionGUID="36455c0b-6c1b-430e-a18e-7ee5b2600923" CreationDate="2017-06-01T13:20:48.980" UserId="77" Comment="Added the &quot;admixture&quot; term to the title and tags. That's the correct term to describe what's being talked about here." Text="&lt;cancer&gt;&lt;wgs&gt;&lt;wes&gt;&lt;contamination&gt;&lt;admixture&gt;" />
  <row Id="1057" PostHistoryTypeId="2" PostId="352" RevisionGUID="19b636e3-4703-4a0b-b861-b0dee709d434" CreationDate="2017-06-01T13:38:07.977" UserId="180" Text="It's usually CNV callers that make use of Tumour/Normal WGS pairs to estimate purity. It can also be done with WES (exome) Tumour/Normal pairs.&#xD;&#xA;&#xD;&#xA;There are several tools out there, I have some experience with the one written by Illumina (public on Github):&#xD;&#xA;&#xD;&#xA;https://github.com/Illumina/canvas&#xD;&#xA;&#xD;&#xA;It requires realigning things with bowtie2, so I don't think it can take existing data aligned with other aligners. Not sure why.&#xD;&#xA;" />
  <row Id="1058" PostHistoryTypeId="2" PostId="353" RevisionGUID="e6028e94-a6b7-4f22-a6f5-5bbf909f7e2d" CreationDate="2017-06-01T13:55:32.567" UserId="501" Text="There is a very nice database, [pdbcull][1] (also known as the PISCES server in the literature). It filters the PDB for high resolution and reduced sequence identity. It also seems to be updated regularly. Depending on the cut-offs, you get between 3000 and 35000 structures.&#xD;&#xA;&#xD;&#xA;If you are specifically interested in rotamers, you may want to look at [top8000][2] instead, where they have checked for high resolution, and good MolProbity scores. They also provide a rotamer database.&#xD;&#xA;&#xD;&#xA;   [1]:http://dunbrack.fccc.edu/Guoli/pisces_download.php&#xD;&#xA;   [2]:http://kinemage.biochem.duke.edu/databases/top8000.php" />
  <row Id="1059" PostHistoryTypeId="5" PostId="321" RevisionGUID="84d7f11b-7f07-44b8-ab72-4df23824bbd6" CreationDate="2017-06-01T14:17:04.697" UserId="298" Comment="Clarified a bit more; linked to the BQSR page (the link to best practices doesn't take you to the right page); minor gramar corrections" Text="Most variant calling pipelines include a [Base Quality Score Recalibration (BQSR)][1] which requires a list of known variants. Recently, some work has been done for reference-free recalibration of scores as well: [Lancer](http://biorxiv.org/content/early/2017/04/27/130732) and [atlas](http://www.genetics.org/content/early/2016/11/07/genetics.116.189985), which is motivated by making the most for aDNA and low coverage datasets. &#xD;&#xA;&#xD;&#xA;The importance for aDNA is explained in [this lecture][2], but it is not clear to me if / how is important this is for fresh DNA samples with decent (&gt;15x) coverage. Especially when I work with non-model organisms and I can not simply use the standard tools.&#xD;&#xA;&#xD;&#xA;Q: How big an impact does recalibration of scores have on variant calling? Is there a rule of thumb for which it is / it is not worth the effort?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://software.broadinstitute.org/gatk/documentation/article?id=44&#xD;&#xA;  [2]: https://www.youtube.com/watch?v=oJ9pbQsyaUg&amp;list=PLoCxWrRWjqB1JUCntl4X09ezmOtKx1Gke&amp;index=4" />
  <row Id="1060" PostHistoryTypeId="24" PostId="321" RevisionGUID="84d7f11b-7f07-44b8-ab72-4df23824bbd6" CreationDate="2017-06-01T14:17:04.697" Comment="Proposed by 298 approved by -1 edit id of 90" />
  <row Id="1061" PostHistoryTypeId="5" PostId="321" RevisionGUID="33d303d7-648b-44a2-b590-61fdf2f41e22" CreationDate="2017-06-01T14:17:04.697" UserId="57" Comment="Clarified a bit more; linked to the BQSR page (the link to best practices doesn't take you to the right page); minor gramar corrections; added the explicit mention of GATK (because recalibration based on variants is not everywhere)" Text="The most variant calling pipeline GATK include a [Base Quality Score Recalibration (BQSR)][1] which requires a list of known variants. Recently, some work has been done for reference-free recalibration of scores as well: [Lancer](http://biorxiv.org/content/early/2017/04/27/130732) and [atlas](http://www.genetics.org/content/early/2016/11/07/genetics.116.189985), which is motivated by making the most for aDNA and low coverage datasets. &#xD;&#xA;&#xD;&#xA;The importance for aDNA is explained in [this lecture][2], but it is not clear to me if / how is important BQSR is for fresh DNA samples with decent (&gt;15x) coverage. Especially when I work with non-model organisms and I can not simply use the standard tools.&#xD;&#xA;&#xD;&#xA;Q: How big an impact does recalibration of scores have on variant calling? Is there a rule of thumb for which it is / it is not worth the effort?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://software.broadinstitute.org/gatk/documentation/article?id=44&#xD;&#xA;  [2]: https://www.youtube.com/watch?v=oJ9pbQsyaUg&amp;list=PLoCxWrRWjqB1JUCntl4X09ezmOtKx1Gke&amp;index=4" />
  <row Id="1062" PostHistoryTypeId="5" PostId="343" RevisionGUID="4e5fc89c-24fc-44a2-b189-9a49fa957f62" CreationDate="2017-06-01T14:34:50.127" UserId="29" Comment="specify interest in TEs" Text="I’m using the [RepBase libraries](http://www.girinst.org/server/RepBase/index.php) in conjunction with RepeatMasker to get genome-wide repeat element annotations, in particular for transposable elements.&#xD;&#xA;&#xD;&#xA;This works well enough, and seems to be the de facto standard in the field.&#xD;&#xA;&#xD;&#xA;However, there are two issues with the use of RepBase, which is why I (and others) have been (so far without success) for alternatives:&#xD;&#xA;&#xD;&#xA;1. [RepBase isn’t open data](http://www.girinst.org/accountservices/register.php?commercial=0). Their academic license agreement includes a clause that *explicitly forbids dissemination of data derived from RepBase*. It’s unclear to what extent this is binding/enforceable, but it effectively prevents publishing at least some of the data I’m using and generating. This is unacceptable for [open science](https://en.wikipedia.org/wiki/Open_science).&#xD;&#xA;&#xD;&#xA;   * Subordinate to this, the subscription model of RepBase also makes it impossible to integrate RepBase into fully automated pipelines, because user interaction is required to subscribe to RepBase provide the login credentials.&#xD;&#xA;&#xD;&#xA;2. RepBase is heavily manually curated. This is both good and bad. Good, because manual curation of sequence data is often the most reliable form of curation. On the flip side, manual curation is inherently biased; and worse, it’s hard to quantify this bias — [this is acknowledged by the RepBase maintainers](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-474)." />
  <row Id="1063" PostHistoryTypeId="6" PostId="343" RevisionGUID="4e5fc89c-24fc-44a2-b189-9a49fa957f62" CreationDate="2017-06-01T14:34:50.127" UserId="29" Comment="specify interest in TEs" Text="&lt;annotation&gt;&lt;database&gt;&lt;repeat-elements&gt;&lt;transposable-elements&gt;" />
  <row Id="1068" PostHistoryTypeId="10" PostId="306" RevisionGUID="8fb45fba-d41a-4dc9-9a86-3b18e367df47" CreationDate="2017-06-01T15:05:02.230" UserId="-1" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:215,&quot;DisplayName&quot;:&quot;SamStudio8&quot;},{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;},{&quot;Id&quot;:176,&quot;DisplayName&quot;:&quot;JonMark Perry&quot;},{&quot;Id&quot;:355,&quot;DisplayName&quot;:&quot;Nikita&quot;}]}" />
  <row Id="1069" PostHistoryTypeId="2" PostId="355" RevisionGUID="8d0e5997-01e7-4add-ab6b-d59bbfa245bc" CreationDate="2017-06-01T15:11:07.053" UserId="57" Text="There is an evaluation of PB Honey and Sniffles algorithms for low coverage PacBio datasets in [this preprint][1] and another evaluation is shown on [this poster][2]. Both reports agree that optimal is (surprisingly) combination of [PB Honey][3] and [Sniffles][4].&#xD;&#xA;&#xD;&#xA;Author of Sniffles have benchmarked Sniffles against PB Honey, where he shown that Sniffles performs significantly better. Take a look on [this presentation (slide 15)][5].&#xD;&#xA;&#xD;&#xA;Another option is [SMRT-SV][6], but I am not aware of any benchmarking.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.biorxiv.org/content/early/2016/12/17/092544.full.pdf+html&#xD;&#xA;  [2]: http://www.pacb.com/wp-content/uploads/2016-ashg-poster-andrew-carroll-dnanexus.pdf&#xD;&#xA;  [3]: https://sourceforge.net/projects/pb-jelly/&#xD;&#xA;  [4]: https://github.com/fritzsedlazeck/Sniffles&#xD;&#xA;  [5]: http://schatzlab.cshl.edu/presentations/2016/2016.10.28.BIODATA.PacBioSV.pdf&#xD;&#xA;  [6]: https://github.com/EichlerLab/pacbio_variant_caller" />
  <row Id="1070" PostHistoryTypeId="6" PostId="329" RevisionGUID="bf05f511-ae86-4799-95b5-389578757b42" CreationDate="2017-06-01T15:11:31.337" UserId="57" Comment="edited tags" Text="&lt;structural-variation&gt;&lt;long-reads&gt;&lt;pacbio&gt;" />
  <row Id="1072" PostHistoryTypeId="2" PostId="356" RevisionGUID="b511f2c9-de2d-4d77-a4bf-27b63caa0a5b" CreationDate="2017-06-01T15:22:21.113" UserId="492" Text="Given that you mention wanting to use 1000 Genomes as a reference panel for imputing genotypes into your two SNP chip panels, I am going to assume that you are working with human data. &#xD;&#xA;&#xD;&#xA;In that case there are several options you can go with:&#xD;&#xA;&#xD;&#xA;- If your two panels are of European descent, then you are probably best off using the [HRC reference panel](http://www.haplotype-reference-consortium.org/) together with a fast genotype imputation tool such as [Beagle 4.1](http://www.cell.com/ajhg/fulltext/S0002-9297(15)00491-7) to impute genotypes in each of your two SNP chip panels separately.&#xD;&#xA;- If your panels are not of European descent, then you will likely want to use the 1000 Genomes phase 3 reference panel with Beagle 4.1, Impute2, or Minimac3.&#xD;&#xA;&#xD;&#xA;In either case, there are two phasing services available that will do much of the heavy lifting for you [1],[2].&#xD;&#xA;&#xD;&#xA;The second [Wellcome Trust Case-Control Consortium paper](https://www.wtccc.org.uk/ccc2/) performed a cross-imputation analysis as you describe.  I don't see many studies using multiple SNP chip panels.  You will need to take care in your analysis that you are not hit by batch effects from using two different SNP chip panels.&#xD;&#xA;&#xD;&#xA;Also, none of these methods will work if the region you are imputing into has too few variants.  I'm not sure what the minimum number of variants is, but if you impute a whole chromosome, then you'll definitely be ok...&#xD;&#xA;&#xD;&#xA;[1]: https://imputationserver.sph.umich.edu/index.html&#xD;&#xA;[2]: https://imputation.sanger.ac.uk/" />
  <row Id="1073" PostHistoryTypeId="2" PostId="357" RevisionGUID="6d9afae4-d075-4d79-9689-f62466128ab3" CreationDate="2017-06-01T15:29:48.250" UserId="33" Text="Let me preface this and say this is not meant to be opinionated.&#xD;&#xA;&#xD;&#xA;As someone who's beginning to delve into this, I'm noticing that like biology there are industry standards here, similar to Illumina in genomics and bowtie for alignment. &#xD;&#xA;&#xD;&#xA;Is using a shell besides bash going to cause issues for me? Is bash the industry standard? &#xD;&#xA;" />
  <row Id="1074" PostHistoryTypeId="1" PostId="357" RevisionGUID="6d9afae4-d075-4d79-9689-f62466128ab3" CreationDate="2017-06-01T15:29:48.250" UserId="33" Text="What Shell is the Industry Standard?" />
  <row Id="1075" PostHistoryTypeId="3" PostId="357" RevisionGUID="6d9afae4-d075-4d79-9689-f62466128ab3" CreationDate="2017-06-01T15:29:48.250" UserId="33" Text="&lt;linux&gt;&lt;shell&gt;" />
  <row Id="1076" PostHistoryTypeId="5" PostId="334" RevisionGUID="092f4818-c766-44f9-863f-0f8f399a69c7" CreationDate="2017-06-01T15:41:32.643" UserId="425" Comment="Several small improvements" Text="I will answer one of the points – E.coli.&#xD;&#xA;&#xD;&#xA;**TL;DR** Bacteria, and in particular E.coli, are highly variable and there is usually no single best assembly. Large scale WGS studies should come with multiple assemblies for individual monophyletic clusters.&#xD;&#xA;&#xD;&#xA;**Long answer:** Whereas a single reference sequence can make sense for human or mice (to a certain extent), bacteria are not the case. The variability between them even within a single species can be enormous and various lineages can contain very different genes. This is the reason why biologists usually distinguish a *core genome* consisting of the genes shared among a vast majority (typically 95%) of individuals within a given phylogenetic clade, (i.e., of those genes which are essential for this clade – imagine a CPU and RAM in a computer), and an *accessory genome* consisting of the other, unnecessary, genes – imagine a joystick in our analogy). The size of the accessory genome can vary a lot for different bacteria. For instance, Escherichia coli is said to have an *open* accessory genome (i.e., much bigger than the core genome; imagine a Raspberry-PI and all the extension kits) whereas Chlamydia trachomatis has a *closed* accessory genome (i.e., small compared to the core genome; imagine an ATM).&#xD;&#xA;&#xD;&#xA;For species having a small accessory genome, a single reference might be sufficient. However, if a core genome is large, we can use either pan-genomes, or multiple references. The word pan-genome tends to be used for different things: the [computational biologists](https://www.ncbi.nlm.nih.gov/pubmed/27769991) usually use it as a representation of all genomic content in a certain phylogenetic clade (which can be mathematically modeled using finite automata, hence as graphs) whereas some biologists define it as a concatenation of all (core and accessory) genes from the clade (without considering SNPs, etc.). Unfortunately, methods for graph references (especially for read mapping) are still not sufficiently developed, therefore, researchers still have to resort to reference sequence-based methods and use either concatenated genes or multiple reference sequences.&#xD;&#xA;&#xD;&#xA;When particular bacterial species are studied using massive whole-genome sequencing of many isolates, researchers usually cluster the isolates to monophyletic sequence clusters and infer a special reference sequence for each of these clusters (see, e.g., this [example](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3725542/) for the pneumococcus). All of the inferred references should be then published together with the paper or in a separate data paper (see, e.g., the [sequences from the previous example](http://datadryad.org/resource/doi:10.5061/dryad.t55gq/8)).&#xD;&#xA;&#xD;&#xA;**Disclaimer:** I am not a biologist. If anything in this post is too simplified or incorrect, please, leave me a comment and I will try to fix it. I tried to make this text accessible for my younger self." />
  <row Id="1077" PostHistoryTypeId="2" PostId="358" RevisionGUID="05bca449-0825-44e1-9a39-3b82c34e3ad1" CreationDate="2017-06-01T15:44:33.430" UserId="37" Text="I would not say bash as a &quot;standard&quot;, but it is indeed likely to be the most widely used unix shell and available by default on most modern unix/linux distros. There are a few other more convenient shells like [zsh](http://www.zsh.org) that are broadly compatible with `/bin/sh`, but they are not as widely available. There is also C-shell and in particular its open-source implementation [tcsh](https://en.wikipedia.org/wiki/Tcsh). C-shell is quite different from bash. Over ten years ago, I saw it was used from time to time, but nowadays, I rarely see its use, except by programmers from older generations." />
  <row Id="1078" PostHistoryTypeId="2" PostId="359" RevisionGUID="a4544174-8e3b-4a6d-bada-67705fdd5803" CreationDate="2017-06-01T15:53:21.003" UserId="134" Text="Bioinformatics tools written in shell and other shell scripts generally specify the shell they want to use (via `#!/bin/sh` or e.g. `#!/bin/bash` if it matters), so won't be affected by your choice of user shell.&#xD;&#xA;&#xD;&#xA;If you are writing significant shell scripts yourself, there are reasons to do it in a Bourne-style shell.  See _Csh Programming Considered Harmful_ and other essays/polemics.&#xD;&#xA;&#xD;&#xA;A Bourne-style shell is pretty much the industry standard, and if you choose a substantially different shell you'll have to translate some of the documentation of your bioinformatics tools.  It's not uncommon to have things like&#xD;&#xA;&#xD;&#xA;&gt; Set some variables pointing at reference data and add the script to your PATH to run it:&#xD;&#xA;&gt;&#xD;&#xA;&gt;     export FOO_REF=/path/to/stuff&#xD;&#xA;&gt;     export PATH=/path/to/foo-x.y:$PATH&#xD;&#xA;&gt;     foo blah blah&#xD;&#xA;&#xD;&#xA;These will typically be shown in Bourne-shell syntax.  By using a different shell you have to translate the `export` commands to your local syntax, and especially PATH munging is somewhat shell-dependent.&#xD;&#xA;&#xD;&#xA;If you're experienced in Unix, this will be only a minor niggle.  If you're a beginner, IMHO this will add a non-negligible amount of friction on top of all the other things you're learning." />
  <row Id="1079" PostHistoryTypeId="2" PostId="360" RevisionGUID="aad22fe7-ceea-4550-88a6-74f5973dff49" CreationDate="2017-06-01T15:59:22.890" UserId="425" Text="I think that BASH is considered by many computer scientists and software engineers a standard. Since it is nowadays present on almost every Unix machine, it is probably the safest choice (from the viewpoint of availability, but not [security](https://www.symantec.com/connect/blogs/shellshock-all-you-need-know-about-bash-bug-vulnerability)).&#xD;&#xA;&#xD;&#xA;However, various versions of BASH can be installed and scripts working on modern Linux machines with Bash 4.x might not work on OS X, which is still based on Bash 3.x. If you are interested in a real industrial standard, [Bash with the POSIX switcher](https://www.gnu.org/software/bash/manual/html_node/Bash-POSIX-Mode.html) might be an option." />
  <row Id="1080" PostHistoryTypeId="5" PostId="360" RevisionGUID="5b03bd88-d50b-48e9-93cc-6fdfa64feb3f" CreationDate="2017-06-01T16:06:14.263" UserId="425" Comment="deleted 2 characters in body" Text="I think that BASH is considered by many computer scientists and software engineers a standard. Since it is nowadays present on almost every Unix machine, it is probably the safest choice (from the viewpoint of availability, but not [security](https://www.symantec.com/connect/blogs/shellshock-all-you-need-know-about-bash-bug-vulnerability)).&#xD;&#xA;&#xD;&#xA;However, various versions of BASH can be installed and scripts working on modern Linux machines with Bash 4.x might not work on OS X, which is still based on Bash 3.x. If you are interested in a real industry standard, [Bash with the POSIX switcher](https://www.gnu.org/software/bash/manual/html_node/Bash-POSIX-Mode.html) might be an option." />
  <row Id="1081" PostHistoryTypeId="5" PostId="360" RevisionGUID="69c7a925-14b2-49f5-bad9-d90d7eb81dad" CreationDate="2017-06-01T16:13:48.360" UserId="425" Comment="added 34 characters in body" Text="I think that BASH is considered by many computer scientists and software engineers a standard. Since it is nowadays present on almost every Unix machine, it is probably the safest choice (from the viewpoint of availability, but not [security](https://www.symantec.com/connect/blogs/shellshock-all-you-need-know-about-bash-bug-vulnerability)).&#xD;&#xA;&#xD;&#xA;However, various versions of BASH can be installed and scripts working on modern Linux machines with [BASH 4](http://wiki.bash-hackers.org/bash4) might not work on OS X, which is still based on BASH 3. If you are interested in a real industry standard, [BASH with the POSIX switcher](https://www.gnu.org/software/bash/manual/html_node/Bash-POSIX-Mode.html) might be an option." />
  <row Id="1082" PostHistoryTypeId="2" PostId="361" RevisionGUID="bedb227e-2947-4480-91b0-26be365efd9b" CreationDate="2017-06-01T16:47:02.727" UserId="57" Text="What is the fastest way how to calculate number of unknown nucleotides (Ns) in fasta and fastq files?" />
  <row Id="1083" PostHistoryTypeId="1" PostId="361" RevisionGUID="bedb227e-2947-4480-91b0-26be365efd9b" CreationDate="2017-06-01T16:47:02.727" UserId="57" Text="What is the fastest way how to calculate number of unknown nucleotides in FASTA / FASTQ files?" />
  <row Id="1084" PostHistoryTypeId="3" PostId="361" RevisionGUID="bedb227e-2947-4480-91b0-26be365efd9b" CreationDate="2017-06-01T16:47:02.727" UserId="57" Text="&lt;fasta&gt;&lt;fastq&gt;" />
  <row Id="1085" PostHistoryTypeId="2" PostId="362" RevisionGUID="0b75f193-8dd4-472e-a313-026736b247f6" CreationDate="2017-06-01T16:50:34.150" UserId="35" Text="The &quot;best&quot; assembly depends on the species. For more common species, Ensembl/UCSC/NCBI has their own version that corresponds to the most popular assembly. Usually the actual genetic sequence is identical across all of them, but the chromosome names and gene annotations will vary. If those three sources agree for a species, there is a &quot;standard&quot; reference.&#xD;&#xA;&#xD;&#xA;For fruit fly specifically, FlyBase Consortium/Berkeley Drosophila Genome Project is the gold standard. Other species have their own organizations." />
  <row Id="1087" PostHistoryTypeId="2" PostId="363" RevisionGUID="e1ef5b3f-5ad2-46cc-b393-6c22c75824e0" CreationDate="2017-06-01T17:00:25.523" UserId="191" Text="Not sure it's the fastest way, but one possible solution in bash is:&#xD;&#xA;&#xD;&#xA;    sed -n '2~4p' seqs.file | grep -o N | wc -l&#xD;&#xA;&#xD;&#xA;- `sed -n '2~4p'` will print every fourth line&#xD;&#xA;- `grep -o N` will output a line with `N` for every matching symbol&#xD;&#xA;- `wc -l` will count the lines&#xD;&#xA;&#xD;&#xA;I suspect this python approach will work faster:&#xD;&#xA;&#xD;&#xA;    cat seqs.fastq | python3 -c &quot;import sys; print(sum(line.count('N') for line in sys.stdin))&quot;" />
  <row Id="1088" PostHistoryTypeId="5" PostId="363" RevisionGUID="a42917ce-7ba7-4719-9314-ec4b2645d1fc" CreationDate="2017-06-01T17:04:29.377" UserId="191" Comment="small/capital" Text="**FASTQ**&#xD;&#xA;&#xD;&#xA;Not sure it's the fastest way, but one possible solution in bash is:&#xD;&#xA;&#xD;&#xA;    sed -n '2~4p' seqs.fastq | grep -io N | wc -l&#xD;&#xA;&#xD;&#xA;- `sed -n '2~4p'` will print every fourth line&#xD;&#xA;- `grep -o N` will output a line with `N` for every matching symbol&#xD;&#xA;- `wc -l` will count the lines&#xD;&#xA;&#xD;&#xA;I suspect this python approach will work faster:&#xD;&#xA;&#xD;&#xA;    cat seqs.fastq | python3 -c &quot;import sys; print(sum(line.upper().count('N') for line in sys.stdin))&quot;&#xD;&#xA;&#xD;&#xA;**FASTA**&#xD;&#xA;&#xD;&#xA;Bash:&#xD;&#xA;&#xD;&#xA;    grep -v &quot;^&gt;&quot; seqs.fasta | grep -io N | wc -l&#xD;&#xA;&#xD;&#xA;Python:&#xD;&#xA;&#xD;&#xA;    cat seqs.fasta | python3 -c &quot;import sys; print(sum(line.upper().count('N') for line in sys.stdin if not line.startswith('&gt;')))&quot;&#xD;&#xA;" />
  <row Id="1089" PostHistoryTypeId="2" PostId="364" RevisionGUID="9de6f70c-7f7d-4a6a-b645-7c7a5357ed3e" CreationDate="2017-06-01T17:10:02.993" UserId="29" Text="Honestly, the easiest way is probably to use R/Bioconductor:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(ShortRead)&#xD;&#xA;    &#xD;&#xA;    seq = readFasta(commandArgs(TRUE)[1]) # or readFastq&#xD;&#xA;    cat(colSums(oligonucleotideFrequency(sread(seq), 1))[['N']], '\n')&#xD;&#xA;&#xD;&#xA;This may not be the fastest, but it’s *pretty fast*, since the relevant methods are highly optimised." />
  <row Id="1090" PostHistoryTypeId="2" PostId="365" RevisionGUID="624698b5-d979-42cc-993e-95c98c8738a8" CreationDate="2017-06-01T17:15:22.907" UserId="44" Text="If it is raw speed you're after, then writing an own little C/C++ program is probably what you need to do.&#xD;&#xA;&#xD;&#xA;Fortunately, the worst part (a fast and reliable parser) has already been tackled: the [readfq][1] from Heng Li is probably the fastest FASTA/FASTQ parser around.&#xD;&#xA;&#xD;&#xA;And it's easy to use, the example on GitHub can easily be expanded to do what you need. Just add in a parameter parser (for the filename you want to analyse), program a simple 'N'-counter loop and have the results printed to stdout. Done.&#xD;&#xA;&#xD;&#xA;Else, if it is simplicity you are after, see Konrad's answer for an R based solution. Or a very simple script using Biopython.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/lh3/readfq" />
  <row Id="1091" PostHistoryTypeId="2" PostId="366" RevisionGUID="0c33e48d-3cae-4918-ab17-d7d42f94ddd8" CreationDate="2017-06-01T17:22:20.160" UserId="425" Text="I think that this should be pretty fast:&#xD;&#xA;&#xD;&#xA;FASTA:&#xD;&#xA;&#xD;&#xA;    grep -v &quot;^&gt;&quot; seqs.fa | tr -cd N | wc -c&#xD;&#xA;&#xD;&#xA;FASTQ:&#xD;&#xA;&#xD;&#xA;    sed -n '1d;N;N;N;P;d' seqs.fq | tr -cd N | wc -c&#xD;&#xA;&#xD;&#xA;See this [answer on SO](https://stackoverflow.com/a/16679640/4641846) about how to count characters in BASH using different approaches." />
  <row Id="1092" PostHistoryTypeId="5" PostId="363" RevisionGUID="a4a05883-5600-463e-b9be-2da513f94a15" CreationDate="2017-06-01T18:08:54.297" UserId="191" Comment="Mention that this is a simple case" Text="**FASTQ**&#xD;&#xA;&#xD;&#xA;As it was pointed out, fastq can be complicated. But in a simple case when you have four lines per record, one possible solution in bash is:&#xD;&#xA;&#xD;&#xA;    sed -n '2~4p' seqs.fastq | grep -io N | wc -l&#xD;&#xA;&#xD;&#xA;- `sed -n '2~4p'` will print every fourth line&#xD;&#xA;- `grep -o N` will output a line with `N` for every matching symbol&#xD;&#xA;- `wc -l` will count the lines&#xD;&#xA;&#xD;&#xA;I suspect this python approach will work faster:&#xD;&#xA;&#xD;&#xA;    cat seqs.fastq | python3 -c &quot;import sys; print(sum(line.upper().count('N') for line in sys.stdin))&quot;&#xD;&#xA;&#xD;&#xA;**FASTA**&#xD;&#xA;&#xD;&#xA;Bash:&#xD;&#xA;&#xD;&#xA;    grep -v &quot;^&gt;&quot; seqs.fasta | grep -io N | wc -l&#xD;&#xA;&#xD;&#xA;Python:&#xD;&#xA;&#xD;&#xA;    cat seqs.fasta | python3 -c &quot;import sys; print(sum(line.upper().count('N') for line in sys.stdin if not line.startswith('&gt;')))&quot;&#xD;&#xA;" />
  <row Id="1094" PostHistoryTypeId="2" PostId="367" RevisionGUID="fbfe48c9-e640-45fc-8f86-45cbb7bb18ca" CreationDate="2017-06-01T19:49:22.177" UserId="37" Text="According to [this famous blog post](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/), the effective transcript length is:&#xD;&#xA;&#xD;&#xA;    \tilde{l}_i = l_i - \mu&#xD;&#xA;&#xD;&#xA;where `l_i` is the length of transcript and `\mu` is the average fragment length. However, typically fragment length is about 300bp. What if when the transcript  `l_i` is smaller than 300? How do you compute the effective length in this case?&#xD;&#xA;&#xD;&#xA;A related question: when computing the FPKM of a gene, how to choose a transcript? Do we choose a &quot;canonical&quot; transcript (how?) or combine the signals from all transcripts to a gene-level FPKM?" />
  <row Id="1095" PostHistoryTypeId="1" PostId="367" RevisionGUID="fbfe48c9-e640-45fc-8f86-45cbb7bb18ca" CreationDate="2017-06-01T19:49:22.177" UserId="37" Text="How exactly is &quot;effective length&quot; used in FPKM calculated?" />
  <row Id="1096" PostHistoryTypeId="3" PostId="367" RevisionGUID="fbfe48c9-e640-45fc-8f86-45cbb7bb18ca" CreationDate="2017-06-01T19:49:22.177" UserId="37" Text="&lt;rna-seq&gt;" />
  <row Id="1097" PostHistoryTypeId="4" PostId="336" RevisionGUID="9cff77b8-0293-4dec-b6bb-596e75ddf4c2" CreationDate="2017-06-01T19:55:30.320" UserId="73" Comment="clarify question" Text="Imputing missing genotypes from separate genotyping panels" />
  <row Id="1098" PostHistoryTypeId="4" PostId="191" RevisionGUID="bbf5a41a-a3a4-4325-9c7c-012d67155f40" CreationDate="2017-06-01T19:58:14.903" UserId="73" Comment="clarify question" Text="Expected allele frequency distribution of SNVs in real NGS data" />
  <row Id="1100" PostHistoryTypeId="2" PostId="368" RevisionGUID="54b76a1b-063f-4258-9cf0-a140c1120ca3" CreationDate="2017-06-01T20:03:41.287" UserId="77" Text="The effective length is `\tilde{l}_i = l_i - \mu + 1` (note the R code at the bottom of Harold's blog post), which in the case of `\mu &lt; l_i` should be 1. Ideally, you'd use the mean fragment length mapped to the particular feature, rather than a global `\mu`, but that's a lot more work for likely 0 benefit.&#xD;&#xA;&#xD;&#xA;Regarding choosing a particular transcript, ideally one would use a method like salmon or kallisto (or RSEM if you have time to kill). Otherwise, your options are (A) choose the major isoform (if it's known in your tissue and condition) or (B) use a &quot;union gene model&quot; (sum the non-redundant exon lengths) or (C) take the median transcript length. None of those three options make much of a difference if you're comparing between samples, though they're all inferior to a salmon/kallisto/etc. metric." />
  <row Id="1101" PostHistoryTypeId="4" PostId="317" RevisionGUID="c0fb3e10-d15f-40c3-8042-a8e14534b779" CreationDate="2017-06-01T20:06:12.513" UserId="73" Comment="clarify question" Text="Filtering imputed GWAS SNPs based on a MAF difference of 10%" />
  <row Id="1102" PostHistoryTypeId="5" PostId="317" RevisionGUID="b4bdc66e-5554-43ee-9dbd-81f32d01f961" CreationDate="2017-06-01T20:59:10.287" UserId="131" Comment="added 73 characters in body" Text="There are many posts on the web regarding QC steps pre and post-imputation.&#xD;&#xA;&#xD;&#xA;Does applying below (new?) 10% [MAF](https://en.wikipedia.org/wiki/Minor_allele_frequency) difference rule make sense, pitfalls?&#xD;&#xA;&#xD;&#xA;Here is the process:&#xD;&#xA;&#xD;&#xA; 1. Get MAF for imputed set, using [SNPTEST](https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html) with flag `-summary_stats_only`&#xD;&#xA; 2. Convert imputed set to hard-calls using [gtools](http://www.well.ox.ac.uk/~cfreeman/software/gwas/gtool.html) with flag `--threshold 0.9`&#xD;&#xA; 3. If the MAF from step 1 and step 2 differs more than 10% than exclude the variant.&#xD;&#xA;&#xD;&#xA;More info, GWAS is 50K vs 50K case control samples." />
  <row Id="1105" PostHistoryTypeId="5" PostId="356" RevisionGUID="eb47bc68-04db-4e44-879f-d90ad6f9a4a9" CreationDate="2017-06-01T21:36:32.873" UserId="492" Comment="added 72 characters in body" Text="Given that you mention wanting to use 1000 Genomes as a reference panel for imputing genotypes into your two SNP chip panels, I am going to assume that you are working with human data. &#xD;&#xA;&#xD;&#xA;In that case there are several options you can go with:&#xD;&#xA;&#xD;&#xA;- If your two panels are of European descent, then you are probably best off using the [HRC reference panel](http://www.haplotype-reference-consortium.org/) together with a fast genotype imputation tool such as [Beagle 4.1](http://www.cell.com/ajhg/fulltext/S0002-9297(15)00491-7) to impute genotypes in each of your two SNP chip panels separately.&#xD;&#xA;- If your panels are not of European descent, then you will likely want to use the 1000 Genomes phase 3 reference panel with Beagle 4.1, Impute2, or Minimac3.&#xD;&#xA;&#xD;&#xA;In either case, there are two phasing services available that will do much of the heavy lifting for you [1],[2].&#xD;&#xA;&#xD;&#xA;The second [Wellcome Trust Case-Control Consortium paper](https://www.wtccc.org.uk/ccc2/) performed a cross-imputation analysis as you describe.  I don't see many studies using multiple SNP chip panels.  You will need to take care in your analysis that you are not hit by batch effects from using two different SNP chip panels.&#xD;&#xA;&#xD;&#xA;Also, none of these methods will work if the region you are imputing into has too few variants.  I'm not sure what the minimum number of variants is, but if you are using a whole genome genotyping panel of at least 500k SNPs, then you should be ok if you impute a whole chromosome at a time.&#xD;&#xA;&#xD;&#xA;[1]: https://imputationserver.sph.umich.edu/index.html&#xD;&#xA;[2]: https://imputation.sanger.ac.uk/" />
  <row Id="1107" PostHistoryTypeId="2" PostId="369" RevisionGUID="49900561-1232-4416-8d9f-2d9af85ba0ab" CreationDate="2017-06-01T21:46:22.750" UserId="37" Text="For FASTQ:&#xD;&#xA;&#xD;&#xA;    seqtk fqchk in.fq | head -2&#xD;&#xA;&#xD;&#xA;It gives you percentage of &quot;N&quot; bases, not the exact count, though.&#xD;&#xA;&#xD;&#xA;For FASTA:&#xD;&#xA;&#xD;&#xA;    seqtk comp in.fa | awk '{x+=$9}END{print x}'&#xD;&#xA;&#xD;&#xA;This command line also works with FASTQ, but it will be slower as awk is slow." />
  <row Id="1108" PostHistoryTypeId="5" PostId="368" RevisionGUID="4037c58b-109a-4df6-b2c5-ff23bf667aa5" CreationDate="2017-06-01T21:50:18.107" UserId="77" Comment="Expand on salmon et al." Text="The effective length is `\tilde{l}_i = l_i - \mu + 1` (note the R code at the bottom of Harold's blog post), which in the case of `\mu &lt; l_i` should be 1. Ideally, you'd use the mean fragment length mapped to the particular feature, rather than a global `\mu`, but that's a lot more work for likely 0 benefit.&#xD;&#xA;&#xD;&#xA;Regarding choosing a particular transcript, ideally one would use a method like salmon or kallisto (or RSEM if you have time to kill). Otherwise, your options are (A) choose the major isoform (if it's known in your tissue and condition) or (B) use a &quot;union gene model&quot; (sum the non-redundant exon lengths) or (C) take the median transcript length. None of those three options make much of a difference if you're comparing between samples, though they're all inferior to a salmon/kallisto/etc. metric.&#xD;&#xA;&#xD;&#xA;Why are salmon et al. better methods? They don't use arbitrary metrics that will be the same across samples to determine the feature length. Instead, they use expectation maximization (or similarish, since at least salmon doesn't actually use EM) to quantify individual isoform usage. The effective gene length in a sample is then the average of the transcript lengths after weighting for their relative expression (yes, one should remove `\mu` in there). This can then vary between samples, which is quite useful if you have isoform switching between samples/groups in such a way that methods A-C above would miss (think of cases where the switch is to a smaller transcript with higher coverage over it...resulting in the coverage/length in methods A-C to be tamped down)." />
  <row Id="1109" PostHistoryTypeId="5" PostId="369" RevisionGUID="c81b555f-95f1-4baf-a44e-10fe177b209a" CreationDate="2017-06-01T21:59:23.130" UserId="37" Comment="Added a C program" Text="For FASTQ:&#xD;&#xA;&#xD;&#xA;    seqtk fqchk in.fq | head -2&#xD;&#xA;&#xD;&#xA;It gives you percentage of &quot;N&quot; bases, not the exact count, though.&#xD;&#xA;&#xD;&#xA;For FASTA:&#xD;&#xA;&#xD;&#xA;    seqtk comp in.fa | awk '{x+=$9}END{print x}'&#xD;&#xA;&#xD;&#xA;This command line also works with FASTQ, but it will be slower as awk is slow.&#xD;&#xA;&#xD;&#xA;EDIT: ok, based on @BaCH's reminder, here we go (you need [kseq.h](https://github.com/lh3/readfq/blob/master/kseq.h) to compile):&#xD;&#xA;&#xD;&#xA;    // to compile: gcc -O2 this-prog.c -o count-N&#xD;&#xA;    #include &lt;zlib.h&gt;&#xD;&#xA;    #include &lt;stdio.h&gt;&#xD;&#xA;    #include &lt;ctype.h&gt;&#xD;&#xA;    #include &quot;kseq.h&quot;&#xD;&#xA;    KSEQ_INIT(gzFile, gzread)&#xD;&#xA;    &#xD;&#xA;    int main(int argc, char *argv[]) {&#xD;&#xA;    	long i, n_n = 0, n_acgt = 0, n_gap = 0;&#xD;&#xA;    	gzFile fp;&#xD;&#xA;    	kseq_t *seq;&#xD;&#xA;    	if (argc == 1) {&#xD;&#xA;    		fprintf(stderr, &quot;Usage: count-N &lt;in.fa&gt;\n&quot;);&#xD;&#xA;    		return 1;&#xD;&#xA;    	}&#xD;&#xA;    	if ((fp = gzopen(argv[1], &quot;r&quot;)) == 0) {&#xD;&#xA;    		fprintf(stderr, &quot;ERROR: fail to open the input file\n&quot;);&#xD;&#xA;    		return 1;&#xD;&#xA;    	}&#xD;&#xA;    	seq = kseq_init(fp);&#xD;&#xA;    	while (kseq_read(seq) &gt;= 0) {&#xD;&#xA;    		for (i = 0; i &lt; seq-&gt;seq.l; ++i) {&#xD;&#xA;    			int c = tolower(seq-&gt;seq.s[i]);&#xD;&#xA;    			if (c == '-') ++n_gap;&#xD;&#xA;    			else if (c == 'a' || c == 'c' || c == 'g' || c == 't') ++n_acgt;&#xD;&#xA;    			else if (isalpha(c)) ++n_n; // else: don't count&#xD;&#xA;    		}&#xD;&#xA;    	}&#xD;&#xA;    	kseq_destroy(seq);&#xD;&#xA;    	gzclose(fp);&#xD;&#xA;    	printf(&quot;%ld\t%ld\t%ld\n&quot;, n_acgt, n_n, n_gap);&#xD;&#xA;    	return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;It works for both FASTA/Q and gzip'ed FASTA/Q. You can probably make it faster with a lookup table." />
  <row Id="1110" PostHistoryTypeId="5" PostId="46" RevisionGUID="2aaac2ea-503e-485b-b30d-95c52107caf7" CreationDate="2017-06-01T21:59:34.363" UserId="73" Comment="Reordered question statements, clarified questions slightly" Text="I have a computer engineering background, not biology.&#xD;&#xA;&#xD;&#xA;I started working on a bioinformatics project recently, which involves *de-novo* assembly. I came to know the terms `Transcriptome` and `Genome`, but I cannot identify the difference between these two.&#xD;&#xA;&#xD;&#xA;I know a transcriptome is the set of all messenger RNA molecules in a cell, but am not sure how this is different from a genome.&#xD;&#xA;&#xD;&#xA; " />
  <row Id="1111" PostHistoryTypeId="4" PostId="46" RevisionGUID="2aaac2ea-503e-485b-b30d-95c52107caf7" CreationDate="2017-06-01T21:59:34.363" UserId="73" Comment="Reordered question statements, clarified questions slightly" Text="What is the difference between a transcriptome and a genome?" />
  <row Id="1112" PostHistoryTypeId="6" PostId="46" RevisionGUID="2aaac2ea-503e-485b-b30d-95c52107caf7" CreationDate="2017-06-01T21:59:34.363" UserId="73" Comment="Reordered question statements, clarified questions slightly" Text="&lt;transcriptome&gt;&lt;genome&gt;&lt;definition&gt;&lt;background-compsci&gt;" />
  <row Id="1113" PostHistoryTypeId="2" PostId="370" RevisionGUID="8a017610-472c-41b2-a810-ff4465d327d6" CreationDate="2017-06-01T22:07:27.130" UserId="35" Text="I personally don't think BQSR has a huge impact on variant calling, but you don't really need to guess. If you run GATK BQSR, it outputs a table and charts of exactly how much quality scores are adjusted. The adjustment will vary depending on the position in the read and genomic context (previous and following base). In my experience, the difference is a few points at most, but it's certainly noticeable.&#xD;&#xA;&#xD;&#xA;GATK recommends BQSR for both genome and exome data, which is normally much higher than 15x." />
  <row Id="1114" PostHistoryTypeId="5" PostId="150" RevisionGUID="d24d21e4-651b-441c-a55b-9cfbe0f97c32" CreationDate="2017-06-01T22:17:07.003" UserId="73" Comment="reworded to clarify and make the question more obviously bioinformaticsy" Text="I have an [obitools script](http://datadryad.org/bitstream/handle/10255/dryad.122664/obitools_script.txt?sequence=1) that I would like to run faster. How would you run it in parallel to cut down on time?&#xD;&#xA;&#xD;&#xA;    illuminapairedend -r rawdata_scandinavia_R2.fastq rawdata_scandinavia_R1.fastq | tee rawdata_scandinavia.fastq | obiannotate -S goodAli:'&quot;Alignement&quot; if score&gt;40.00 else &quot;Bad&quot;' | obisplit -t goodAli -p rawdata_scandinavia.&#xD;&#xA;    touch rawdata_scandinavia.Bad.fastq&#xD;&#xA;    touch rawdata_scandinavia.Alignement.fastq&#xD;&#xA;    touch rawdata_scandinavia.fastq&#xD;&#xA;    ngsfilter -t rawdata_scandinavia.ngsfilter -u rawdata_scandinavia.unidentified.fastq rawdata_scandinavia.Alignement.fastq &gt; rawdata_scandinavia.filtered.fastq&#xD;&#xA;    obisplit -p MICROSAT.PCR_ -t experiment rawdata_scandinavia.filtered.fastq&#xD;&#xA;&#xD;&#xA;Can someone describe what tools and steps one needs to take to run this job in parallel, given that the functions are not designed to be run in parallel out of the box?" />
  <row Id="1115" PostHistoryTypeId="4" PostId="150" RevisionGUID="d24d21e4-651b-441c-a55b-9cfbe0f97c32" CreationDate="2017-06-01T22:17:07.003" UserId="73" Comment="reworded to clarify and make the question more obviously bioinformaticsy" Text="Parallel processing of scripts that use obitools" />
  <row Id="1116" PostHistoryTypeId="2" PostId="371" RevisionGUID="60c3d9f5-7252-4749-9ac6-332d1fe3b419" CreationDate="2017-06-01T22:20:21.293" UserId="77" Text="5 hours and no benchmarks posted? I am sorely disappointed.&#xD;&#xA;&#xD;&#xA;I'll restrict the comparison to just be fasta files, since fastq will end up being the same. So far, the contenders are:&#xD;&#xA;&#xD;&#xA;1. R with the `ShortRead` package (even if not the fastest, certainly a super convenient method).&#xD;&#xA;2. A pipeline of `grep -v &quot;^&gt;&quot; | tr -cd A | wc -c`&#xD;&#xA;3. A pipeline of `grep -v &quot;^&gt;&quot; | grep -io A | wc -l`&#xD;&#xA;4. A pipeline of `seqtk comp | awk`&#xD;&#xA;5. Something custom in C/C++&#xD;&#xA;&#xD;&#xA;The R code can't actually count N records for some reason when I try it, so I counted `A` for everything.&#xD;&#xA;&#xD;&#xA;I'll exclude all python solutions, because there's no conceivable universe in which python is fast. For the C/C++ solution I just used what @user172818 posted (everything I tried writing took the same amount of time).&#xD;&#xA;&#xD;&#xA;Repeating 100 times and taking the average (yes, one should take the median):&#xD;&#xA;&#xD;&#xA;1. 1.7 seconds&#xD;&#xA;2. 0.65 seconds&#xD;&#xA;3. 15 seconds&#xD;&#xA;4. 0.54 seconds&#xD;&#xA;5. 0.48 seconds&#xD;&#xA;&#xD;&#xA;Unsurprisingly, anything in straight C or C++ is going to win. `grep` with `tr` is surprisingly good, which surprises me since even though `grep` is very very optimized I still expected it to have more overhead. Piping to `grep -io` is a performance killer. The R solution is surprisingly good given the typical R overhead. That seqtk and the C solution are close in time is unsurprising given that `seqtk` is probably not doing much more." />
  <row Id="1117" PostHistoryTypeId="5" PostId="371" RevisionGUID="c575e28e-9e91-4e3d-9bdb-51ab5038df1b" CreationDate="2017-06-01T22:52:13.167" UserId="77" Comment="The seqtk metrics were wrong!" Text="5 hours and no benchmarks posted? I am sorely disappointed.&#xD;&#xA;&#xD;&#xA;I'll restrict the comparison to just be fasta files, since fastq will end up being the same. So far, the contenders are:&#xD;&#xA;&#xD;&#xA;1. R with the `ShortRead` package (even if not the fastest, certainly a super convenient method).&#xD;&#xA;2. A pipeline of `grep -v &quot;^&gt;&quot; | tr -cd A | wc -c`&#xD;&#xA;3. A pipeline of `grep -v &quot;^&gt;&quot; | grep -io A | wc -l`&#xD;&#xA;4. A pipeline of `seqtk comp | awk`&#xD;&#xA;5. Something custom in C/C++&#xD;&#xA;&#xD;&#xA;The R code can't actually count N records for some reason when I try it, so I counted `A` for everything.&#xD;&#xA;&#xD;&#xA;I'll exclude all python solutions, because there's no conceivable universe in which python is fast. For the C/C++ solution I just used what @user172818 posted (everything I tried writing took the same amount of time).&#xD;&#xA;&#xD;&#xA;Repeating 100 times and taking the average (yes, one should take the median):&#xD;&#xA;&#xD;&#xA;1. 1.7 seconds&#xD;&#xA;2. 0.65 seconds&#xD;&#xA;3. 15 seconds&#xD;&#xA;4. 1.2 seconds&#xD;&#xA;5. 0.48 seconds&#xD;&#xA;&#xD;&#xA;Unsurprisingly, anything in straight C or C++ is going to win. `grep` with `tr` is surprisingly good, which surprises me since even though `grep` is very very optimized I still expected it to have more overhead. Piping to `grep -io` is a performance killer. The R solution is surprisingly good given the typical R overhead." />
  <row Id="1118" PostHistoryTypeId="2" PostId="372" RevisionGUID="1ef01042-acda-476b-b7c9-39a03a52899e" CreationDate="2017-06-01T22:57:13.273" UserId="506" Text="Without going into too much background, I just joined up with a lab as a bioinformatics intern while I'm completing my masters degree in the field. The lab has data from an RNA-seq they outsourced, but the only problem is that the only data they have is preprocessed from the company that did the sequencing: filtering the reads, aligning them, and putting the aligned reads through RSEM. I currently have output from RSEM for each of the four samples consisting of: gene id, transcript id(s), length, expected count, and FPKM. I am attempting to get the FASTQ files from the sequencing, but for now, this is what I have, and I'm trying to get something out of it if possible.&#xD;&#xA;&#xD;&#xA;I found this article (https://biowize.wordpress.com/2014/03/04/understanding-rsem-raw-read-counts-vs-expected-counts/) that talks about how expected read counts can be better than raw read counts when analyzing differential expression using EBSeq; it's just one guy's opinion, and it's from 2014, so it may be wrong or outdated, but I thought I'd give it a try since I have the expected counts.&#xD;&#xA;&#xD;&#xA;However, I have just a couple of questions about running EBSeq that I can't find the answers to:&#xD;&#xA;&#xD;&#xA;1: In the output RSEM files I have, not all genes are represented in each, about 80% of them are, but for the ones that aren't, should I remove them before analysis with EBSeq? It runs when I do, but I'm not sure if it is correct.&#xD;&#xA;&#xD;&#xA;2: How do I know which normalization factor to use when running EBSeq? This is more of a conceptual question rather than a technical question.&#xD;&#xA;&#xD;&#xA;Thanks!&#xD;&#xA;" />
  <row Id="1119" PostHistoryTypeId="1" PostId="372" RevisionGUID="1ef01042-acda-476b-b7c9-39a03a52899e" CreationDate="2017-06-01T22:57:13.273" UserId="506" Text="Technical questions on running EBSeq on RSEM output?" />
  <row Id="1120" PostHistoryTypeId="3" PostId="372" RevisionGUID="1ef01042-acda-476b-b7c9-39a03a52899e" CreationDate="2017-06-01T22:57:13.273" UserId="506" Text="&lt;rna-seq&gt;&lt;differential-expression&gt;&lt;ebseq&gt;&lt;rsem&gt;" />
  <row Id="1121" PostHistoryTypeId="5" PostId="369" RevisionGUID="e5ca4a83-7a7b-4fc4-bf53-c7e4dc60c499" CreationDate="2017-06-02T00:12:41.490" UserId="37" Comment="Added seqan" Text="For FASTQ:&#xD;&#xA;&#xD;&#xA;    seqtk fqchk in.fq | head -2&#xD;&#xA;&#xD;&#xA;It gives you percentage of &quot;N&quot; bases, not the exact count, though.&#xD;&#xA;&#xD;&#xA;For FASTA:&#xD;&#xA;&#xD;&#xA;    seqtk comp in.fa | awk '{x+=$9}END{print x}'&#xD;&#xA;&#xD;&#xA;This command line also works with FASTQ, but it will be slower as awk is slow.&#xD;&#xA;&#xD;&#xA;EDIT: ok, based on @BaCH's reminder, here we go (you need [kseq.h](https://github.com/lh3/readfq/blob/master/kseq.h) to compile):&#xD;&#xA;&#xD;&#xA;    // to compile: gcc -O2 -o count-N this-prog.c&#xD;&#xA;    #include &lt;zlib.h&gt;&#xD;&#xA;    #include &lt;stdio.h&gt;&#xD;&#xA;    #include &quot;kseq.h&quot;&#xD;&#xA;    KSEQ_INIT(gzFile, gzread)&#xD;&#xA;    &#xD;&#xA;    unsigned char dna5tbl[256] = {&#xD;&#xA;    	0, 1, 2, 3,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 5, 4, 4,&#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 0, 4, 1,  4, 4, 4, 2,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  3, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 0, 4, 1,  4, 4, 4, 2,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  3, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4&#xD;&#xA;    };&#xD;&#xA;    &#xD;&#xA;    int main(int argc, char *argv[]) {&#xD;&#xA;    	long i, n_n = 0, n_acgt = 0, n_gap = 0;&#xD;&#xA;    	gzFile fp;&#xD;&#xA;    	kseq_t *seq;&#xD;&#xA;    	if (argc == 1) {&#xD;&#xA;    		fprintf(stderr, &quot;Usage: count-N &lt;in.fa&gt;\n&quot;);&#xD;&#xA;    		return 1;&#xD;&#xA;    	}&#xD;&#xA;    	if ((fp = gzopen(argv[1], &quot;r&quot;)) == 0) {&#xD;&#xA;    		fprintf(stderr, &quot;ERROR: fail to open the input file\n&quot;);&#xD;&#xA;    		return 1;&#xD;&#xA;    	}&#xD;&#xA;    	seq = kseq_init(fp);&#xD;&#xA;    	while (kseq_read(seq) &gt;= 0) {&#xD;&#xA;    		for (i = 0; i &lt; seq-&gt;seq.l; ++i) {&#xD;&#xA;    			int c = dna5tbl[(unsigned char)seq-&gt;seq.s[i]];&#xD;&#xA;    			if (c &lt; 4) ++n_acgt;&#xD;&#xA;    			else if (c == 4) ++n_n;&#xD;&#xA;    			else ++n_gap;&#xD;&#xA;    		}&#xD;&#xA;    	}&#xD;&#xA;    	kseq_destroy(seq);&#xD;&#xA;    	gzclose(fp);&#xD;&#xA;    	printf(&quot;%ld\t%ld\t%ld\n&quot;, n_acgt, n_n, n_gap);&#xD;&#xA;    	return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;It works for both FASTA/Q and gzip'ed FASTA/Q. The following uses SeqAn:&#xD;&#xA;&#xD;&#xA;    #include &lt;seqan/seq_io.h&gt;&#xD;&#xA;&#xD;&#xA;    using namespace seqan;&#xD;&#xA;&#xD;&#xA;    int main(int argc, char *argv[]) {&#xD;&#xA;        if (argc == 1) {&#xD;&#xA;            std::cerr &lt;&lt; &quot;Usage: count-N &lt;in.fastq&gt;&quot; &lt;&lt; std::endl;&#xD;&#xA;            return 1;&#xD;&#xA;        }&#xD;&#xA;        CharString id;&#xD;&#xA;        Dna5String seq;&#xD;&#xA;        SeqFileIn seqFileIn(argv[1]);&#xD;&#xA;        long i, n_n = 0, n_acgt = 0;&#xD;&#xA;        while (!atEnd(seqFileIn)) {&#xD;&#xA;            readRecord(id, seq, seqFileIn);&#xD;&#xA;            for (i = beginPosition(seq); i &lt; endPosition(seq); ++i) {&#xD;&#xA;                int c = seq[i];&#xD;&#xA;                if (c &lt; 4) ++n_acgt;&#xD;&#xA;                else ++n_n;&#xD;&#xA;            }&#xD;&#xA;        }&#xD;&#xA;        std::cout &lt;&lt; n_acgt &lt;&lt; '\t' &lt;&lt; n_n &lt;&lt; std::endl;&#xD;&#xA;        return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;On a FASTQ with 4-million 150bp reads:&#xD;&#xA;&#xD;&#xA;* The C version: ~0.75 sec&#xD;&#xA;* The C++ version: ~2.26 sec&#xD;&#xA;* An older C version without a lookup table (see previous edit): ~2.65 sec" />
  <row Id="1122" PostHistoryTypeId="2" PostId="373" RevisionGUID="3eb88378-2481-46aa-b216-6635c72d9250" CreationDate="2017-06-02T00:14:20.617" UserId="96" Text="Yes, that blog post *does* represent just one guy's opinion (hi!) and it does date *all the way back to 2014*, which is, like, decades in genomics years. :-) By the way, there is quite a bit of literature discussing the improvements that expected read counts derived from an Expectation Maximization algorithm provide over raw read counts. I'd suggest reading the RSEM papers for a start&lt;sup&gt;[[1](http://dx.doi.org/10.1093/bioinformatics/btp692)][[2](http://dx.doi.org/10.1186/1471-2105-12-323)]&lt;/sup&gt;.&#xD;&#xA;&#xD;&#xA;But your main question is about the mechanics of running RSEM and EBSeq. First, RSEM was written explicitly to be compatible with EBSeq, so I'd be very surprised if it does not work correctly out-of-the-box. Second, EBSeq's `MedianNorm` function worked very well in my experience for normalizing the library counts. Along those lines, the blog you mentioned above has [another post](https://biowize.wordpress.com/2013/12/12/normalization-for-differential-expression-analysis/) that you may find useful.&#xD;&#xA;&#xD;&#xA;But all joking aside, these tools *are indeed* dated. Alignment-free RNA-Seq tools provide orders-of-magnitude improvements in runtime over the older alignment-based alternatives, with comparable accuracy. [Sailfish](https://github.com/kingsfordgroup/sailfish) was the first in a growing list of tools that now includes [Salmon](https://github.com/COMBINE-lab/salmon) and [Kallisto](https://github.com/pachterlab/kallisto). There's really no good reason not to estimate expression using these much faster tools, followed by a differential expression analysis with DESeq2, edgeR, or sleuth.&#xD;&#xA;&#xD;&#xA;--------&#xD;&#xA;&#xD;&#xA;&lt;sup&gt;1&lt;/sup&gt;**Li B, Ruotti V, Stewart RM, Thomson JA, Dewey CN** (2010) RNA-Seq gene expression estimation with read mapping uncertainty. *Bioinformatics*, 26(4):493–500, [doi:10.1093/bioinformatics/btp692](http://dx.doi.org/10.1093/bioinformatics/btp692).&#xD;&#xA;&#xD;&#xA;&lt;sup&gt;2&lt;/sup&gt;**Li B, Dewey C** (2011) RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome. *BMC Bioinformatics*, 12:323, [doi:10.1186/1471-2105-12-323](http://dx.doi.org/10.1186/1471-2105-12-323)." />
  <row Id="1123" PostHistoryTypeId="5" PostId="361" RevisionGUID="7add0f63-c122-410c-9c6b-5e2d93432cb0" CreationDate="2017-06-02T00:38:22.653" UserId="73" Comment="fixed grammar" Text="What is the fastest way to to calculate the number of unknown nucleotides (Ns) in fasta and fastq files?" />
  <row Id="1124" PostHistoryTypeId="4" PostId="361" RevisionGUID="7add0f63-c122-410c-9c6b-5e2d93432cb0" CreationDate="2017-06-02T00:38:22.653" UserId="73" Comment="fixed grammar" Text="What is the fastest way to calculate the number of unknown nucleotides in FASTA / FASTQ files?" />
  <row Id="1125" PostHistoryTypeId="6" PostId="361" RevisionGUID="7add0f63-c122-410c-9c6b-5e2d93432cb0" CreationDate="2017-06-02T00:38:22.653" UserId="73" Comment="fixed grammar" Text="&lt;fasta&gt;&lt;fastq&gt;&lt;benchmarking&gt;" />
  <row Id="1126" PostHistoryTypeId="5" PostId="372" RevisionGUID="d7ab0802-0693-481f-8b3f-9f56ab522aef" CreationDate="2017-06-02T00:40:24.217" UserId="96" Comment="fix link and title text" Text="Without going into too much background, I just joined up with a lab as a bioinformatics intern while I'm completing my masters degree in the field. The lab has data from an RNA-seq they outsourced, but the only problem is that the only data they have is preprocessed from the company that did the sequencing: filtering the reads, aligning them, and putting the aligned reads through RSEM. I currently have output from RSEM for each of the four samples consisting of: gene id, transcript id(s), length, expected count, and FPKM. I am attempting to get the FASTQ files from the sequencing, but for now, this is what I have, and I'm trying to get something out of it if possible.&#xD;&#xA;&#xD;&#xA;I found [this article](https://biowize.wordpress.com/2014/03/04/understanding-rsem-raw-read-counts-vs-expected-counts/) that talks about how expected read counts can be better than raw read counts when analyzing differential expression using EBSeq; it's just one guy's opinion, and it's from 2014, so it may be wrong or outdated, but I thought I'd give it a try since I have the expected counts.&#xD;&#xA;&#xD;&#xA;However, I have just a couple of questions about running EBSeq that I can't find the answers to:&#xD;&#xA;&#xD;&#xA;1: In the output RSEM files I have, not all genes are represented in each, about 80% of them are, but for the ones that aren't, should I remove them before analysis with EBSeq? It runs when I do, but I'm not sure if it is correct.&#xD;&#xA;&#xD;&#xA;2: How do I know which normalization factor to use when running EBSeq? This is more of a conceptual question rather than a technical question.&#xD;&#xA;&#xD;&#xA;Thanks!&#xD;&#xA;" />
  <row Id="1127" PostHistoryTypeId="4" PostId="372" RevisionGUID="d7ab0802-0693-481f-8b3f-9f56ab522aef" CreationDate="2017-06-02T00:40:24.217" UserId="96" Comment="fix link and title text" Text="Technical questions about running EBSeq on RSEM output" />
  <row Id="1128" PostHistoryTypeId="24" PostId="372" RevisionGUID="d7ab0802-0693-481f-8b3f-9f56ab522aef" CreationDate="2017-06-02T00:40:24.217" Comment="Proposed by 96 approved by 73, 163 edit id of 91" />
  <row Id="1129" PostHistoryTypeId="2" PostId="374" RevisionGUID="eac3e48c-5580-4ded-a79e-574e1671ab44" CreationDate="2017-06-02T01:40:28.860" UserId="73" Text="1. Include all genes/transcripts in your analysis.&#xD;&#xA;&#xD;&#xA;A transcript that is not detected could be undetected through sampling error (i.e. the sequencer / library prep just happened to miss that transcript), or it could be because the transcript isn't generated in a particular sample. It's not uncommon for genes to be switched off in response to different biological factors, so zero-count genes shouldn't be ignored. I can't speak from experience with EBSeq, but as long as the analysis package treats a zero count as &quot;unobserved&quot; rather than &quot;absent&quot; (and makes relevant corrections), it's a good idea to keep them in." />
  <row Id="1130" PostHistoryTypeId="2" PostId="375" RevisionGUID="23b9fbae-f2e0-4e03-a7eb-78962096a142" CreationDate="2017-06-02T02:48:04.570" UserId="73" Text="After discovering a few difficulties with genome assembly, I've taken an interest in finding and categorising repetitive DNA sequences, such as this one from *Nippostrongylus brasiliensis* [each base is colour-coded as A: green; C: blue; G: yellow; T: red]:&#xD;&#xA;&#xD;&#xA;[![Repeat sequence represented in a rectangular fashion][1]][1]&#xD;&#xA;[![Repeat sequence represented in a circular fashion][2]][2]&#xD;&#xA;&#xD;&#xA;[FASTA file associated with this sequence can be found [here]&#xD;&#xA;[3]]&#xD;&#xA;&#xD;&#xA;These sequences with large repeat unit sizes are only detectable (and assembleable) using long reads (e.g. PacBio, nanopore) because any subsequence smaller than the unit length will not be able to distinguish between sequencing error and hitting a different location within the repeat structure. I have been tracking these sequences down in a bulk fashion by two methods:&#xD;&#xA;&#xD;&#xA; 1. Running an all-vs-all mapping, and looking for sequences that map to themselves lots of times&#xD;&#xA; 2. Carrying out a compression of the sequence (e.g. bzip2), and finding sequences that have a compression rate that is substantially higher than normal&#xD;&#xA;&#xD;&#xA;After I've found a suspicious sequence, I then want to be able to categorise the repeat (e.g. major repeat length, number of tandem repeats, repetitive sequence). This is where I'm getting stuck.&#xD;&#xA;&#xD;&#xA;For doing a &quot;look, shiny&quot; demonstration, I currently have a very manual process of getting these sequences into a format that I can visualise. My process is as follows:&#xD;&#xA;&#xD;&#xA; 1. Use LAST to produce a dot plot of self-mapping for the mapping&#xD;&#xA; 2. Visually identify the repetitive region, and extract out the region from the sequence&#xD;&#xA; 3. Use a combination of ```fold -w &lt;width&gt;``` and ```less -S``` to visually inspect the sequence with various potential repeat unit widths to find the most likely repeat unit size&#xD;&#xA; 4. Display the sequence in a rectangular and circular fashion using [my own script][4], wrapping at the repeat unit length&#xD;&#xA;&#xD;&#xA;But that process is by no means feasible when I've got thousands of potential repetitive sequences to fish through.&#xD;&#xA;&#xD;&#xA;Is there any better way to do this? Given an arbitrary DNA sequence of length &gt;10kb, how can I (in an automated fashion) find both the location of the repeat region, and also the unit length (bearing in mind that there might be multiple repeat structures, with unit lengths from 30bp to 10kb)?&#xD;&#xA;&#xD;&#xA;An example sequence can be found [here][5], which has a ~21kb repeat region with ~171bp repeat units about 1/3 of the way into the sequence.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/QSk09.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/RjVLd.png&#xD;&#xA;  [3]: http://www.gringene.org/data/longRepeat_ch192_r13894.fa&#xD;&#xA;  [4]: https://github.com/gringer/bioinfscripts/blob/master/seqmat.r&#xD;&#xA;  [5]: http://www.gringene.org/data/fd2.fa" />
  <row Id="1131" PostHistoryTypeId="1" PostId="375" RevisionGUID="23b9fbae-f2e0-4e03-a7eb-78962096a142" CreationDate="2017-06-02T02:48:04.570" UserId="73" Text="Finding the location and unit length of repetitive sequences within a long read" />
  <row Id="1132" PostHistoryTypeId="3" PostId="375" RevisionGUID="23b9fbae-f2e0-4e03-a7eb-78962096a142" CreationDate="2017-06-02T02:48:04.570" UserId="73" Text="&lt;nanopore&gt;&lt;long-reads&gt;&lt;repeat-elements&gt;" />
  <row Id="1133" PostHistoryTypeId="5" PostId="373" RevisionGUID="68e348c6-dd84-456c-a8b7-1ac9c7da1889" CreationDate="2017-06-02T02:54:54.193" UserId="96" Comment="missed that he didn't have access to FASTQs" Text="Yes, that blog post *does* represent just one guy's opinion (hi!) and it does date *all the way back to 2014*, which is, like, decades in genomics years. :-) By the way, there is quite a bit of literature discussing the improvements that expected read counts derived from an Expectation Maximization algorithm provide over raw read counts. I'd suggest reading the RSEM papers for a start&lt;sup&gt;[[1](http://dx.doi.org/10.1093/bioinformatics/btp692)][[2](http://dx.doi.org/10.1186/1471-2105-12-323)]&lt;/sup&gt;.&#xD;&#xA;&#xD;&#xA;But your main question is about the mechanics of running RSEM and EBSeq. First, RSEM was written explicitly to be compatible with EBSeq, so I'd be very surprised if it does not work correctly out-of-the-box. Second, EBSeq's `MedianNorm` function worked very well in my experience for normalizing the library counts. Along those lines, the blog you mentioned above has [another post](https://biowize.wordpress.com/2013/12/12/normalization-for-differential-expression-analysis/) that you may find useful.&#xD;&#xA;&#xD;&#xA;But all joking aside, these tools *are indeed* dated. Alignment-free RNA-Seq tools provide orders-of-magnitude improvements in runtime over the older alignment-based alternatives, with comparable accuracy. [Sailfish](https://github.com/kingsfordgroup/sailfish) was the first in a growing list of tools that now includes [Salmon](https://github.com/COMBINE-lab/salmon) and [Kallisto](https://github.com/pachterlab/kallisto). When starting a new analysis from scratch (i.e. if you ever get the original FASTQ files), there's really no good reason not to estimate expression using these much faster tools, followed by a differential expression analysis with DESeq2, edgeR, or sleuth.&#xD;&#xA;&#xD;&#xA;--------&#xD;&#xA;&#xD;&#xA;&lt;sup&gt;1&lt;/sup&gt;**Li B, Ruotti V, Stewart RM, Thomson JA, Dewey CN** (2010) RNA-Seq gene expression estimation with read mapping uncertainty. *Bioinformatics*, 26(4):493–500, [doi:10.1093/bioinformatics/btp692](http://dx.doi.org/10.1093/bioinformatics/btp692).&#xD;&#xA;&#xD;&#xA;&lt;sup&gt;2&lt;/sup&gt;**Li B, Dewey C** (2011) RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome. *BMC Bioinformatics*, 12:323, [doi:10.1186/1471-2105-12-323](http://dx.doi.org/10.1186/1471-2105-12-323)." />
  <row Id="1134" PostHistoryTypeId="2" PostId="376" RevisionGUID="99ee62e7-54a3-48b1-af9c-dd6cb9527170" CreationDate="2017-06-02T03:42:33.323" UserId="73" Text="The generic command ```sh``` is quite literally an industry standard, a POSIX standard, to be precise (IEEE 1003.2 and 1003.2a, available for purchase for hundreds of dollars at various websites). In theory, any script that starts with ```#!/bin/sh``` should conform to this standard. In practise, most Linux systems have a shell that is close to this standard, but has a few quirks and extensions.&#xD;&#xA;&#xD;&#xA;Problems crop up when these quirks and extensions become standard practise in shell scripts. The Debian operating system changed to ```dash``` as their default shell to encourage people to stop using &quot;bashisms&quot; in their shell scripts, particularly those that began with ```#!/bin/sh```. The ```dash``` shell tries to be as standards-compliant as possible:&#xD;&#xA;&#xD;&#xA;&gt; dash is the standard command interpreter for the system.  The current version of dash is in the process of being changed to&#xD;&#xA;     conform with the POSIX 1003.2 and 1003.2a specifications for the shell.  This version has many features which make it&#xD;&#xA;     appear similar in some respects to the Korn shell, but it is not a Korn shell clone (see ksh(1)).  Only features designated&#xD;&#xA;     by POSIX, plus a few Berkeley extensions, are being incorporated into this shell.  This man page is not intended to be a&#xD;&#xA;     tutorial or a complete specification of the shell.&#xD;&#xA;&#xD;&#xA;I'm not familiar with the differences, and generally try to stick to the ```sh``` man pages to instruct me with regards to correct standards-compliant shell scripts." />
  <row Id="1135" PostHistoryTypeId="5" PostId="316" RevisionGUID="39f836c6-5337-4771-9549-9c1834acfe38" CreationDate="2017-06-02T03:49:26.877" UserId="73" Comment="rearranged question, reworded question title to make it more specific" Text="We have arrayCGH (aCGH) results for one sample. There is a 0.5 Mb terminal duplication on chromosome 19 (62995490-63407936, according to NCBI36/hg18). The duplication is rare: a literature review suggests there are only 3-4 samples with clinical information.&#xD;&#xA;&#xD;&#xA;What are the steps to validate the results? How do we ascertain that this duplication is the cause of the clinical symptoms?&#xD;&#xA;&#xD;&#xA;I have some ideas:&#xD;&#xA; &#xD;&#xA; - aCGH the parents. Not sure how this would help.   &#xD;&#xA; - whole genome exome sequencing. Worried this might make it more difficult to pinpoint genetic cause.&#xD;&#xA; - whole genome sequencing?&#xD;&#xA; - other ideas?&#xD;&#xA;&#xD;&#xA;**Note:** I am new to aCGH and high-throughput sequencing, any advice is welcome." />
  <row Id="1136" PostHistoryTypeId="4" PostId="316" RevisionGUID="39f836c6-5337-4771-9549-9c1834acfe38" CreationDate="2017-06-02T03:49:26.877" UserId="73" Comment="rearranged question, reworded question title to make it more specific" Text="How do I validate a single sample ArrayCGH result?" />
  <row Id="1137" PostHistoryTypeId="2" PostId="377" RevisionGUID="d9987976-210c-45c5-bffc-41818852b8b7" CreationDate="2017-06-02T06:54:02.307" UserId="434" Text="[The Open Group Base Specifications Issue 7&#xD;&#xA;IEEE Std 1003.1™-2008, 2016 Edition](http://pubs.opengroup.org/onlinepubs/9699919799.2016edition/), or &quot;The POSIX Standard&quot; for short, is the standard that defines the interfaces and utilities provided by a Unix system.  Among these is the command line shell language and tools (see &quot;Shell &amp; Utilities&quot; in the main index above).&#xD;&#xA;&#xD;&#xA;As far as I know, there is no shell that implement _exactly_ what's specified by the standard, but both `bash` and `ksh93` does a pretty good job of adhering to the standard along with their own, sometimes conflicting, extensions.&#xD;&#xA;&#xD;&#xA;The `bash` shell is pretty much ubiquitous on Linux systems, and may be installed on all other Unices too. `ksh93` is also available for most Unices but is usually not installed by default on Linux.&#xD;&#xA;&#xD;&#xA;If you are concerned about portability when writing a shell script (which is IMHO a good thing to be concerned about), you should make sure that you use only the POSIX utilities and their POSIX command line flags, as well as only use POSIX shell syntax. You should then ensure that you script is executed by `/bin/sh` which is supposed to be a shell that understands the POSIX specification.  `/bin/sh` is often implemented by `bash` running in &quot;POSIX mode&quot;, but it may also be `dash`, `ash` or `pdksh` depending on what Unix you are using.&#xD;&#xA;&#xD;&#xA;For a Linux user, the most difficult bit in writing a portable script is not the shell per se, but the multitude of non-standard command line flags provided by the GNU implementation of the many shell utilities." />
  <row Id="1138" PostHistoryTypeId="2" PostId="378" RevisionGUID="93e96e41-065a-46f5-beb2-2f231e1dc1b9" CreationDate="2017-06-02T07:13:13.107" UserId="96" Text="Hidden Markov models (HMMs) are used extensively in bioinformatics, and have been adapted for gene prediction, protein family classification, and a variety of other problems. Indeed, [the treatise by Durbin, Eddy and colleagues](http://www.cambridge.org/gb/academic/subjects/life-sciences/genomics-bioinformatics-and-systems-biology/biological-sequence-analysis-probabilistic-models-proteins-and-nucleic-acids) is one of the defining volumes in this field.&#xD;&#xA;&#xD;&#xA;Although the details of each of these different applications of HMMs differ, the core mathematical model remains unchanged, and there are efficient algorithms for computing the probability of the observed sequence given the model, or (perhaps more useful) the most likely hidden sequence given the sequence of observed states.&#xD;&#xA;&#xD;&#xA;Accordingly, it seems plausible that there could be a generic software library for solving HMMs. As far as I can tell that's not the case, and most bioinformaticians end up writing HMMs from scratch. Perhaps there's a good reason for this? (Aside from the obvious fact that it's already difficult, nigh impossible, to get funding to build and provide long-term support for open source science software. Academic pressures incentivize building a new tool that you can publish a paper on much more than building on and extending existing tools.)&#xD;&#xA;&#xD;&#xA;Do any generic HMM solver libraries exist? If so, would this be tempting enough for bioinformaticians to use rather than writing their own from scratch?" />
  <row Id="1139" PostHistoryTypeId="1" PostId="378" RevisionGUID="93e96e41-065a-46f5-beb2-2f231e1dc1b9" CreationDate="2017-06-02T07:13:13.107" UserId="96" Text="Generic HMM solvers in bioinformatics?" />
  <row Id="1140" PostHistoryTypeId="3" PostId="378" RevisionGUID="93e96e41-065a-46f5-beb2-2f231e1dc1b9" CreationDate="2017-06-02T07:13:13.107" UserId="96" Text="&lt;hidden-markov-models&gt;&lt;sequence-analysis&gt;" />
  <row Id="1141" PostHistoryTypeId="11" PostId="308" RevisionGUID="50b2d6a8-1716-4785-9d68-0a5416f71a67" CreationDate="2017-06-02T07:31:12.803" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:37,&quot;DisplayName&quot;:&quot;user172818&quot;},{&quot;Id&quot;:425,&quot;DisplayName&quot;:&quot;Karel&quot;},{&quot;Id&quot;:35,&quot;DisplayName&quot;:&quot;burger&quot;},{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;}]}" />
  <row Id="1142" PostHistoryTypeId="11" PostId="46" RevisionGUID="a381117a-53f0-4810-aa6d-d1d887d77a17" CreationDate="2017-06-02T07:43:28.910" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;},{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:311,&quot;DisplayName&quot;:&quot;Michael&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;}]}" />
  <row Id="1143" PostHistoryTypeId="11" PostId="14" RevisionGUID="5237f0b3-6ace-4b1f-ad32-a616c143bd22" CreationDate="2017-06-02T07:49:40.483" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:37,&quot;DisplayName&quot;:&quot;user172818&quot;},{&quot;Id&quot;:35,&quot;DisplayName&quot;:&quot;burger&quot;},{&quot;Id&quot;:425,&quot;DisplayName&quot;:&quot;Karel&quot;},{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;}]}" />
  <row Id="1144" PostHistoryTypeId="4" PostId="372" RevisionGUID="b960d4df-dcb8-4792-b52d-db4dcafb20e5" CreationDate="2017-06-02T07:50:21.887" UserId="73" Comment="reworded question to remove unnecessary &quot;technical questions&quot; statement" Text="Missing genes and normalisation of RSEM output using EBSeq" />
  <row Id="1145" PostHistoryTypeId="2" PostId="379" RevisionGUID="e45267aa-eace-4550-b427-dd7588c76072" CreationDate="2017-06-02T07:52:08.220" UserId="224" Text="You mention that FastQC _&quot;fails to find the actual adapter sequences&quot;_ - I guess you mean in the Adapter Sequence Contamination plot. However, the kmer and Sequence Content Plots are often useful even when the former fails. I've used these in the past - you can sometimes just read off the adapter sequence from the start of the Sequence Content Plot (or at least see how many bases to trim)." />
  <row Id="1146" PostHistoryTypeId="5" PostId="284" RevisionGUID="718752c6-926c-4cb8-9d19-d77d46062dfc" CreationDate="2017-06-02T08:05:19.020" UserId="48" Comment="Giving the reference to the bioarxive file, and hidding links from the text, also removing the context" Text="There is a database called [OmicsDI][1], where one can search for multi-omics datasets.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Here's a [link][2] of the associated publication (Perez-Riverol, Yasset, et al. &quot;Omics Discovery Index-Discovering and Linking Public Omics Datasets.&quot; bioRxiv (2016): 049205.) for more details. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.omicsdi.org/search?q=omics_type:%22Multi-Omics%22&#xD;&#xA;  [2]: http://biorxiv.org/content/early/2016/04/18/049205" />
  <row Id="1147" PostHistoryTypeId="24" PostId="284" RevisionGUID="718752c6-926c-4cb8-9d19-d77d46062dfc" CreationDate="2017-06-02T08:05:19.020" Comment="Proposed by 48 approved by -1, 73 edit id of 92" />
  <row Id="1148" PostHistoryTypeId="5" PostId="284" RevisionGUID="a3b428e0-84c2-4a4b-aec8-dbd02d7674df" CreationDate="2017-06-02T08:05:19.020" UserId="191" Comment="Giving the reference to the bioarxive file, and hidding links from the text, also removing the context" Text="There is a database called [OmicsDI][1], where one can search for multi-omics datasets.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Here's a [link][2] of the associated publication (Perez-Riverol, Yasset, et al. &quot;Omics Discovery Index-Discovering and Linking Public Omics Datasets.&quot; bioRxiv (2016): 049205.) for more details. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.omicsdi.org/search?q=omics_type:%22Multi-Omics%22&#xD;&#xA;  [2]: https://doi.org/10.1101/049205" />
  <row Id="1149" PostHistoryTypeId="4" PostId="351" RevisionGUID="363cca36-3f66-405b-b227-ca996b04e1ad" CreationDate="2017-06-02T08:06:06.787" UserId="374" Comment="edited title" Text="Tumor purity/contamination/admixture estimation" />
  <row Id="1150" PostHistoryTypeId="5" PostId="371" RevisionGUID="61653560-f5bd-4415-b5e0-d4b19b00679c" CreationDate="2017-06-02T08:07:34.220" UserId="77" Comment="Add python timings" Text="5 hours and no benchmarks posted? I am sorely disappointed.&#xD;&#xA;&#xD;&#xA;I'll restrict the comparison to just be fasta files, since fastq will end up being the same. So far, the contenders are:&#xD;&#xA;&#xD;&#xA;1. R with the `ShortRead` package (even if not the fastest, certainly a super convenient method).&#xD;&#xA;2. A pipeline of `grep -v &quot;^&gt;&quot; | tr -cd A | wc -c`&#xD;&#xA;3. A pipeline of `grep -v &quot;^&gt;&quot; | grep -io A | wc -l`&#xD;&#xA;4. A pipeline of `seqtk comp | awk`&#xD;&#xA;5. Something custom in C/C++&#xD;&#xA;&#xD;&#xA;The R code can't actually count N records for some reason when I try it, so I counted `A` for everything.&#xD;&#xA;&#xD;&#xA;I'll exclude all python solutions, because there's no conceivable universe in which python is fast. For the C/C++ solution I just used what @user172818 posted (everything I tried writing took the same amount of time).&#xD;&#xA;&#xD;&#xA;Repeating 100 times and taking the average (yes, one should take the median):&#xD;&#xA;&#xD;&#xA;1. 1.7 seconds&#xD;&#xA;2. 0.65 seconds&#xD;&#xA;3. 15 seconds&#xD;&#xA;4. 1.2 seconds&#xD;&#xA;5. 0.48 seconds&#xD;&#xA;&#xD;&#xA;Unsurprisingly, anything in straight C or C++ is going to win. `grep` with `tr` is surprisingly good, which surprises me since even though `grep` is very very optimized I still expected it to have more overhead. Piping to `grep -io` is a performance killer. The R solution is surprisingly good given the typical R overhead.&#xD;&#xA;&#xD;&#xA;**Edit:** As suggested in the comments&#xD;&#xA;&#xD;&#xA;6. `sum(x.count('A') for x in open('seqs.fa','r') if x[0] != '&gt;')` in python&#xD;&#xA;&#xD;&#xA;This yields a time of&#xD;&#xA;&#xD;&#xA;6. 1.6 seconds (I'm at work now, so I've adjusted the time as best I can to account for the different computers)" />
  <row Id="1151" PostHistoryTypeId="5" PostId="14" RevisionGUID="ccaad8fc-7891-4c3d-ab82-334e0994e621" CreationDate="2017-06-02T08:14:35.087" UserId="48" Comment="Add comment to the question, describing the problem of the OP" Text="I'd like to learn the differences between 3 common formats such as [FASTA][1], [FASTQ][2] and [SAM][3]. How they are different? Are there any benefits of using one over another?&#xD;&#xA;&#xD;&#xA;Based on Wikipedia pages, I can't tell the differences between them.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/FASTA_format&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/FASTQ_format&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/SAM_(file_format)" />
  <row Id="1152" PostHistoryTypeId="24" PostId="14" RevisionGUID="ccaad8fc-7891-4c3d-ab82-334e0994e621" CreationDate="2017-06-02T08:14:35.087" Comment="Proposed by 48 approved by 43 edit id of 94" />
  <row Id="1153" PostHistoryTypeId="6" PostId="153" RevisionGUID="0504a142-41fa-45b5-a681-2e90de0cf308" CreationDate="2017-06-02T08:21:30.523" UserId="292" Comment="Added differential-expression tag" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;deseq2&gt;&lt;differential-expression&gt;" />
  <row Id="1154" PostHistoryTypeId="5" PostId="361" RevisionGUID="90d1779f-5216-403d-978d-c3c201532a74" CreationDate="2017-06-02T08:28:52.683" UserId="57" Comment="I added motivation and my preferences about the solution." Text="I used to work with publicly available genomic references, where basic statistics are usually available and if they are not, you have to compute them only once so there is no reason to worry about performance.&#xD;&#xA;&#xD;&#xA;Recently I started sequencing project of couple of different species with mid-sized genomes (~Gbp) and during testing of different assembly pipelines I had compute number of unknown nucleotides many times in both raw reads (in fastq) and assembly scaffolds (in fasta), therefore I thought that I would like to optimize the computation.&#xD;&#xA;&#xD;&#xA;- For me it is reasonable to expect 4-line formatted fastq files, but general solution is still prefered&#xD;&#xA;- It would be nice if solution would work on gzipped files as well&#xD;&#xA;&#xD;&#xA;Q : What is the fastest way (performance-wise) to compute the number of unknown nucleotides (Ns) in fasta and fastq files?" />
  <row Id="1155" PostHistoryTypeId="5" PostId="371" RevisionGUID="aef3408c-e498-428e-9a3e-07a3df1997ce" CreationDate="2017-06-02T08:36:45.603" UserId="77" Comment="Add perl &amp; awk" Text="5 hours and no benchmarks posted? I am sorely disappointed.&#xD;&#xA;&#xD;&#xA;I'll restrict the comparison to just be fasta files, since fastq will end up being the same. So far, the contenders are:&#xD;&#xA;&#xD;&#xA;1. R with the `ShortRead` package (even if not the fastest, certainly a super convenient method).&#xD;&#xA;2. A pipeline of `grep -v &quot;^&gt;&quot; | tr -cd A | wc -c`&#xD;&#xA;3. A pipeline of `grep -v &quot;^&gt;&quot; | grep -io A | wc -l`&#xD;&#xA;4. A pipeline of `seqtk comp | awk`&#xD;&#xA;5. Something custom in C/C++&#xD;&#xA;&#xD;&#xA;The R code can't actually count N records for some reason when I try it, so I counted `A` for everything.&#xD;&#xA;&#xD;&#xA;I'll exclude all python solutions, because there's no conceivable universe in which python is fast. For the C/C++ solution I just used what @user172818 posted (everything I tried writing took the same amount of time).&#xD;&#xA;&#xD;&#xA;Repeating 100 times and taking the average (yes, one should take the median):&#xD;&#xA;&#xD;&#xA;1. 1.7 seconds&#xD;&#xA;2. 0.65 seconds&#xD;&#xA;3. 15 seconds&#xD;&#xA;4. 1.2 seconds&#xD;&#xA;5. 0.48 seconds&#xD;&#xA;&#xD;&#xA;Unsurprisingly, anything in straight C or C++ is going to win. `grep` with `tr` is surprisingly good, which surprises me since even though `grep` is very very optimized I still expected it to have more overhead. Piping to `grep -io` is a performance killer. The R solution is surprisingly good given the typical R overhead.&#xD;&#xA;&#xD;&#xA;**Update1:** As suggested in the comments&#xD;&#xA;&#xD;&#xA;6. `sum(x.count('A') for x in open('seqs.fa','r') if x[0] != '&gt;')` in python&#xD;&#xA;&#xD;&#xA;This yields a time of&#xD;&#xA;&#xD;&#xA;6. 1.6 seconds (I'm at work now, so I've adjusted the time as best I can to account for the different computers)&#xD;&#xA;&#xD;&#xA;**Update2:** More benchmarks from the comments&#xD;&#xA;&#xD;&#xA;7. `awk -F A '!/^&gt;&quot;/ {cnt+=NF-1}END{print cnt}' foo.fa`&#xD;&#xA;8. `perl -ne 'if(!/^&gt;/){$count += tr/A//} END{print &quot;$count\n&quot;}' foo.fa`&#xD;&#xA;&#xD;&#xA;These yield times of:&#xD;&#xA;&#xD;&#xA;7. 5.2 seconds&#xD;&#xA;8. 1.18 seconds&#xD;&#xA;&#xD;&#xA;The awk version is the best hack I could come up with, there are likely better ways." />
  <row Id="1156" PostHistoryTypeId="5" PostId="377" RevisionGUID="0249ffc5-8b93-401b-927a-2608a20bb5d2" CreationDate="2017-06-02T08:42:10.833" UserId="434" Comment="added 119 characters in body" Text="[The Open Group Base Specifications Issue 7&#xD;&#xA;IEEE Std 1003.1™-2008, 2016 Edition](http://pubs.opengroup.org/onlinepubs/9699919799.2016edition/), or &quot;The POSIX Standard&quot; for short, is the standard that defines the interfaces and utilities provided by a Unix system.  Among these is the command line shell language and tools (see &quot;Shell &amp; Utilities&quot; in the main index on the page linked above).&#xD;&#xA;&#xD;&#xA;As far as I know, there is no shell that implement _exactly_ what's specified by the standard, but both `bash` and `ksh93` does a pretty good job of adhering to the standard along with their own, sometimes conflicting, extensions. The `ksh93` shell in particular has had a big impact on the past development of the POSIX shell specification, but future POSIX specifications _may_ borrow more from `bash` due to its wide use on Linux.&#xD;&#xA;&#xD;&#xA;The `bash` shell is pretty much ubiquitous on Linux systems, and may be installed on all other Unices too. `ksh93` is also available for most Unices but is usually not installed by default on Linux. `ksh93` is available by default on at least macOS (as `ksh`) and Solaris.&#xD;&#xA;&#xD;&#xA;If you are concerned about portability when writing a shell script (which is IMHO a good thing to be concerned about), you should make sure that you use only the POSIX utilities and their POSIX command line flags, as well as only use POSIX shell syntax. You should then ensure that you script is executed by `/bin/sh` which is supposed to be a shell that understands the POSIX specification.  `/bin/sh` is often implemented by `bash` running in &quot;POSIX mode&quot;, but it may also be `dash`, `ash` or `pdksh` (or something else) depending on what Unix you are using.&#xD;&#xA;&#xD;&#xA;For a Linux user, the most difficult bit in writing a portable script is often not the shell per se, but the multitude of non-standard command line flags provided by the GNU implementation of the many shell utilities.  Thu GNU coreutils (basic shell utilities) may be installed on all Unices though." />
  <row Id="1157" PostHistoryTypeId="5" PostId="308" RevisionGUID="7290d80f-1535-4678-ac85-02061c834b43" CreationDate="2017-06-02T09:06:31.513" UserId="48" Comment="Improving visual appealing, changing the title of the question" Text="[SMALT](http://www.sanger.ac.uk/science/tools/smalt-0) seems to be one of the most used read mappers for bacterial data, see, e.g., [this query](https://scholar.google.com/scholar?q=%22smalt%22+bacteria&amp;as_ylo=2016). I do not say that it is not a great mapper, but I cannot easily see what are its main strengths compared to mappers such as BWA-MEM, Bowtie2, NovoAlign or GEM. Moreover, it is not even published. &#xD;&#xA;&#xD;&#xA;Could you name some of its distinguishing features (e.g., user support by Sanger Pathogens)? &#xD;&#xA;&#xD;&#xA;So far I have heard only arguments like &quot;We use SMALT because everyone does it.&quot;, but this is not convincing enough for me." />
  <row Id="1158" PostHistoryTypeId="4" PostId="308" RevisionGUID="7290d80f-1535-4678-ac85-02061c834b43" CreationDate="2017-06-02T09:06:31.513" UserId="48" Comment="Improving visual appealing, changing the title of the question" Text="Why is SMALT better for microbial genomics than other mappers?" />
  <row Id="1159" PostHistoryTypeId="24" PostId="308" RevisionGUID="7290d80f-1535-4678-ac85-02061c834b43" CreationDate="2017-06-02T09:06:31.513" Comment="Proposed by 48 approved by 73, 77 edit id of 93" />
  <row Id="1160" PostHistoryTypeId="11" PostId="150" RevisionGUID="1b10faaa-e63f-4f53-bc55-9f6a4707460c" CreationDate="2017-06-02T09:10:20.343" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;},{&quot;Id&quot;:200,&quot;DisplayName&quot;:&quot;Roman Luštrik&quot;},{&quot;Id&quot;:52,&quot;DisplayName&quot;:&quot;Axeman&quot;}]}" />
  <row Id="1161" PostHistoryTypeId="2" PostId="380" RevisionGUID="64b38ed2-1ff7-4301-b464-a7015a129d77" CreationDate="2017-06-02T09:28:46.290" UserId="292" Text="With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (including gzipped versions), so the above command should also handle fastq format robustly.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters. I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)" />
  <row Id="1165" PostHistoryTypeId="2" PostId="382" RevisionGUID="0e11f40e-08ae-4ca8-b478-56cb12690709" CreationDate="2017-06-02T09:35:00.113" UserId="200" Text="Here is a bash script and GNU `parallel` along with some other tools to split-apply-combine the data. It may or may not work out of the box, but it's not far off and it should give the general idea of how one way of how to approached this.&#xD;&#xA;&#xD;&#xA;    #!/bin/bash&#xD;&#xA;    &#xD;&#xA;    # ====== USER PROVIDED PARAMETERS ========&#xD;&#xA;    N=20 # number of cores to use&#xD;&#xA;    ngsfilter=&quot;rawdata_scandinavia.ngsfilter&quot;&#xD;&#xA;    unidentified=&quot;unidentified_samples.fastq&quot;&#xD;&#xA;    &#xD;&#xA;    R1=&quot;rawdata_scandinavia_R1.fastq&quot;&#xD;&#xA;    R2=&quot;rawdata_scandinavia_R2.fastq&quot;&#xD;&#xA;    # ==== END USER PROVIDED PARAMETERS =====&#xD;&#xA;    &#xD;&#xA;    # fastqutils available @ http://ngsutils.org/modules/fastqutils&#xD;&#xA;    fastqutils split $R1 to_illumina_R1_ $N &amp;&#xD;&#xA;    fastqutils split $R2 to_illumina_R2_ $N &amp;&#xD;&#xA;    wait&#xD;&#xA;    &#xD;&#xA;    R1=$( ls | grep -P &quot;to_illumina_R1_&quot; )&#xD;&#xA;    R2=$( ls | grep -P &quot;to_illumina_R2_&quot; )&#xD;&#xA;    &#xD;&#xA;    # Do alignment of two reads.&#xD;&#xA;    for i in $( seq $N )&#xD;&#xA;    do&#xD;&#xA;    	partR1=$( ls | grep -P &quot;^to_illumina_R1_\\.$i\\.fastq$&quot; )&#xD;&#xA;    	partR2=$( ls | grep -P &quot;^to_illumina_R2_\\.$i\\.fastq$&quot; )&#xD;&#xA;    	illuminapairedend -r $partR1 $partR2 | tee to_ngsfilter_$i.fastq | obiannotate -S goodAli:'&quot;Align&quot; if score&gt;40.00 else &quot;Bad&quot;' | obisplit -t goodAli -p to_ngsfilter_$i. &amp;&#xD;&#xA;    done&#xD;&#xA;    &#xD;&#xA;    # Remove intermediate aligned files.&#xD;&#xA;    echo $R1 | xargs rm -r&#xD;&#xA;    echo $R2 | xargs rm -r&#xD;&#xA;    ls | grep -P &quot;\\.seq$|\\.err$&quot; | xargs rm -r&#xD;&#xA;    &#xD;&#xA;    input=$( ls | grep -P &quot;to_ngsfilter_[0-9]+\\.Align\\.fastq&quot; )&#xD;&#xA;    &#xD;&#xA;    # For some reasons, files need to be &quot;touched&quot; in order to work. Perhaps&#xD;&#xA;    # it's not closing them...&#xD;&#xA;    touch $unidentified&#xD;&#xA;    &#xD;&#xA;    touchedbyangel=$( ls | grep -P &quot;to_ngsfilter_&quot;  )&#xD;&#xA;    for i in $touchedbyangel&#xD;&#xA;    do&#xD;&#xA;    	touch $i&#xD;&#xA;    done&#xD;&#xA;    &#xD;&#xA;    parallel -j$N --result filtered_{} ngsfilter -t $ngsfilter -u $unidentified {} ::: $input&#xD;&#xA;    &#xD;&#xA;    # Remove intermediate results.&#xD;&#xA;    ls | grep -P &quot;\\.seq$|\\.err$&quot; | xargs rm -r&#xD;&#xA;    &#xD;&#xA;    ls | grep -P &quot;filtered_.*\\.fastq$&quot; | xargs cat &gt; filtered_data.fastq&#xD;&#xA;    &#xD;&#xA;    # remove intermediate results to conserve space&#xD;&#xA;    rm to_ngsfilter*&#xD;&#xA;    &#xD;&#xA;    # Split reads by locus.&#xD;&#xA;    filtered=$( ls | grep -P &quot;^filtered_&quot;  )&#xD;&#xA;    parallel -j$N --result to_split_{} obisplit -p MS.PCR_ -t experiment {} ::: $filtered&#xD;&#xA;    &#xD;&#xA;    ls | grep -P &quot;\\.err$|\\.seq$&quot; | xargs rm -r&#xD;&#xA;    ls | grep -P &quot;^to_split_&quot; | xargs rm -r&#xD;&#xA;    &#xD;&#xA;    # Wasn't able to parallelize this, so it's just pushing tasks into the background.&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_03.fastq &gt; MS.PCR_UA_MxRout1_03.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_06.fastq &gt; MS.PCR_UA_MxRout1_06.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_14.fastq &gt; MS.PCR_UA_MxRout1_14.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_16.fastq &gt; MS.PCR_UA_MxRout1_16.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_17.fastq &gt; MS.PCR_UA_MxRout1_17.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_25.fastq &gt; MS.PCR_UA_MxRout1_25.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_51.fastq &gt; MS.PCR_UA_MxRout1_51.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_57.fastq &gt; MS.PCR_UA_MxRout1_57.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_63.fastq &gt; MS.PCR_UA_MxRout1_63.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_64.fastq &gt; MS.PCR_UA_MxRout1_64.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_65.fastq &gt; MS.PCR_UA_MxRout1_65.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_67.fastq &gt; MS.PCR_UA_MxRout1_67.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_68.fastq &gt; MS.PCR_UA_MxRout1_68.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_ZF.fastq &gt; MS.PCR_UA_MxRout1_ZF.uniq.fasta &amp;&#xD;&#xA;    wait&#xD;&#xA;    &#xD;&#xA;    uniqfiles=$( ls | grep -P &quot;.{2}+\\.uniq\\.fasta$&quot; )&#xD;&#xA;    &#xD;&#xA;    for i in $uniqfiles&#xD;&#xA;    do&#xD;&#xA;    	# this section removes the file suffix&#xD;&#xA;    	# https://stackoverflow.com/questions/125281/how-do-i-remove-the-file-suffix-and-path-portion-from-a-path-string-in-bash&#xD;&#xA;    	tab=${i%.fasta}&#xD;&#xA;    	tab=${tab##*/}&#xD;&#xA;    	obigrep -p 'count&gt;1' $i | obiannotate -k merged_sample -k count | obiannotate --length | obisort -r -k seq_length | obitab --no-definition --output-seq &gt; $tab.tab&#xD;&#xA;    done&#xD;&#xA;    &#xD;&#xA;    ls | grep -P &quot;\\.uniq\\.fasta&quot; | xargs rm -r&#xD;&#xA;    &#xD;&#xA;    echo &quot;Done. I think.&quot;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1167" PostHistoryTypeId="5" PostId="364" RevisionGUID="3569d819-7ed9-459e-8764-399dda1e6c47" CreationDate="2017-06-02T10:27:46.013" UserId="29" Comment="fix counting of “N”s" Text="Honestly, the easiest way is probably to use R/Bioconductor:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    suppressMessages(library(ShortRead))&#xD;&#xA;    &#xD;&#xA;    seq = readFasta(commandArgs(TRUE)[1]) # or readFastq&#xD;&#xA;    cat(colSums(alphabetFrequency(sread(seq))[, 'N', drop = FALSE]), '\n')&#xD;&#xA;&#xD;&#xA;This may not be the fastest, but it’s *pretty fast*, since the relevant methods are highly optimised. The biggest overhead is actually probably from loading `ShortRead`, which is an unjustifiable slog." />
  <row Id="1168" PostHistoryTypeId="5" PostId="25" RevisionGUID="afcb8b3c-11f2-43d9-9537-88ed3498e7ba" CreationDate="2017-06-02T10:45:37.963" UserId="29" Comment="correct the wrong ID specifiers" Text="FASTA and FATSQ formats are both file formats that contain sequencing reads while SAM files are these reads aligned to a reference sequence. In other words, FASTA and FASTQ are the &quot;raw data&quot; of sequencing while SAM is the product of aligning the sequencing reads to a refseq. &#xD;&#xA;&#xD;&#xA;A FASTA file contains a read name followed by the sequence. An example of one of these reads for RNASeq might be: &#xD;&#xA;&#xD;&#xA;    &gt;Flow cell number: lane number: chip coordinates etc.&#xD;&#xA;    ATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTA&#xD;&#xA;&#xD;&#xA;The FASTQ version of this read will have two more lines, one + as a space holder and then a line of quality scores for the base calls. The qualities are given as characters with '!' being the lowest and '~' being the highest, in increasing ASCII value. It would look something like this&#xD;&#xA;     &#xD;&#xA;    @Flow cell number: lane number: chip coordinates etc.&#xD;&#xA;    ATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTA&#xD;&#xA;    +&#xD;&#xA;    !''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&gt;&gt;&gt;&gt;&gt;&gt;CCCCCCC65&#xD;&#xA;&#xD;&#xA;A SAM file has many fields for each alignment, the header begins with the @ character. The alignment contains 11 mandatory fields and various optional ones. You can find the spec file here: https://samtools.github.io/hts-specs/SAMv1.pdf .&#xD;&#xA;&#xD;&#xA;Often you'll see BAM files which are just compressed binary versions of SAM files. You can view these alignment files using various tools, such as SAMtools, IGV or USCS Genome browser. &#xD;&#xA;&#xD;&#xA;As to the benefits, FASTA/FASTQ vs. SAM/BAM is comparing apples and oranges. I do a lot of RNASeq work so generally we take the FASTQ files and align them the a refseq using an aligner such as STAR which outputs SAM/BAM files. There's a lot you can do with just these alignment files, looking at expression, but usually I'll use a tool such as RSEM to &quot;count&quot; the reads from various genes to create an expression matrix, samples as columns and genes as rows. Whether you get FASTQ or FASTA files just depends on your sequencing platform. I've never heard of anybody really using the quality scores. &#xD;&#xA;" />
  <row Id="1169" PostHistoryTypeId="2" PostId="383" RevisionGUID="1a68879a-3d30-4620-b182-5d72ad6745ee" CreationDate="2017-06-02T10:59:48.487" UserId="29" Text="There are certainly software libraries for working with HMMs. For a general-purpose implementation in C++, take a look at the [SeqAn HMM algorithms](http://docs.seqan.de/seqan/2.1.1/group_HmmAlgorithms.html).&#xD;&#xA;&#xD;&#xA;For your purposes, i.e. “computing … the most likely hidden sequence given the sequence of observed states”, you’d invoke `viterbiAlgorithm` with your observed sequence and the HMM graph.&#xD;&#xA;&#xD;&#xA;More fundamentally I think that most existing, mature implementations are probably found in the domain of signal processing, which has been using them longer than biology, and where most of the underlying theory was developed." />
  <row Id="1170" PostHistoryTypeId="5" PostId="380" RevisionGUID="d0044285-c84d-4e8e-b83e-a3ea373d2a5a" CreationDate="2017-06-02T11:04:19.740" UserId="292" Comment="Added python-based versions and link to where I got the gsub hack from" Text="With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (including gzipped versions), so the above command should also handle fastq format robustly.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters (got it from &lt;https://unix.stackexchange.com/a/169575/55127&gt;). I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)&#xD;&#xA;&#xD;&#xA;### Edit&#xD;&#xA;&#xD;&#xA;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;https://bioinformatics.stackexchange.com/a/365/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &lt; /Genomes/C_elegans/Caenorhabditis_elegans/Ensembl/WBcel235/Sequence/WholeGenomeFasta/genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.976s&#xD;&#xA;    user	0m0.876s&#xD;&#xA;    sys 	0m0.100s&#xD;&#xA;&#xD;&#xA;Comparing with something adapted from this answer: &lt;https://bioinformatics.stackexchange.com/a/363/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&gt;')))&quot; &lt; /Genomes/C_elegans/Caenorhabditis_elegans/Ensembl/WBcel235/Sequence/WholeGenomeFasta/genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.143s&#xD;&#xA;    user	0m1.100s&#xD;&#xA;    sys 	0m0.040s&#xD;&#xA;&#xD;&#xA;(It is slower with python3.6 than with python 2.7 on my computer)" />
  <row Id="1171" PostHistoryTypeId="5" PostId="343" RevisionGUID="b724aee1-d642-49ea-a85b-eddd37c0a16d" CreationDate="2017-06-02T11:08:54.493" UserId="29" Comment="grammer &amp; speling" Text="I’m using the [RepBase libraries](http://www.girinst.org/server/RepBase/index.php) in conjunction with RepeatMasker to get genome-wide repeat element annotations, in particular for transposable elements.&#xD;&#xA;&#xD;&#xA;This works well enough, and seems to be the de facto standard in the field.&#xD;&#xA;&#xD;&#xA;However, there are two issues with the use of RepBase, which is why I (and others) have been (so far without success) for alternatives:&#xD;&#xA;&#xD;&#xA;1. [RepBase isn’t open data](http://www.girinst.org/accountservices/register.php?commercial=0). Their academic license agreement includes a clause that *explicitly forbids dissemination of data derived from RepBase*. It’s unclear to what extent this is binding/enforceable, but it effectively prevents publishing at least some of the data I’m using and generating. This is unacceptable for [open science](https://en.wikipedia.org/wiki/Open_science).&#xD;&#xA;&#xD;&#xA;   * Subordinate to this, the subscription model of RepBase also makes it impossible to integrate RepBase into fully automated pipelines, because user interaction is required to subscribe to RepBase, and to provide the login credentials.&#xD;&#xA;&#xD;&#xA;2. RepBase is heavily manually curated. This is both good and bad. Good, because manual curation of sequence data is often the most reliable form of curation. On the flip side, manual curation is inherently biased; and worse, it’s hard to quantify this bias — [this is acknowledged by the RepBase maintainers](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-474)." />
  <row Id="1172" PostHistoryTypeId="5" PostId="371" RevisionGUID="eaf745fe-8d21-4869-a902-e0b23975a40c" CreationDate="2017-06-02T11:33:54.600" UserId="77" Comment="More comparisons, this time on fastq files" Text="5 hours and no benchmarks posted? I am sorely disappointed.&#xD;&#xA;&#xD;&#xA;I'll restrict the comparison to just be fasta files, since fastq will end up being the same. So far, the contenders are:&#xD;&#xA;&#xD;&#xA;1. R with the `ShortRead` package (even if not the fastest, certainly a super convenient method).&#xD;&#xA;2. A pipeline of `grep -v &quot;^&gt;&quot; | tr -cd A | wc -c`&#xD;&#xA;3. A pipeline of `grep -v &quot;^&gt;&quot; | grep -io A | wc -l`&#xD;&#xA;4. A pipeline of `seqtk comp | awk`&#xD;&#xA;5. Something custom in C/C++&#xD;&#xA;&#xD;&#xA;The R code can't actually count N records for some reason when I try it, so I counted `A` for everything.&#xD;&#xA;&#xD;&#xA;I'll exclude all python solutions, because there's no conceivable universe in which python is fast. For the C/C++ solution I just used what @user172818 posted (everything I tried writing took the same amount of time).&#xD;&#xA;&#xD;&#xA;Repeating 100 times and taking the average (yes, one should take the median):&#xD;&#xA;&#xD;&#xA;1. 1.7 seconds&#xD;&#xA;2. 0.65 seconds&#xD;&#xA;3. 15 seconds&#xD;&#xA;4. 1.2 seconds&#xD;&#xA;5. 0.48 seconds&#xD;&#xA;&#xD;&#xA;Unsurprisingly, anything in straight C or C++ is going to win. `grep` with `tr` is surprisingly good, which surprises me since even though `grep` is very very optimized I still expected it to have more overhead. Piping to `grep -io` is a performance killer. The R solution is surprisingly good given the typical R overhead.&#xD;&#xA;&#xD;&#xA;**Update1:** As suggested in the comments&#xD;&#xA;&#xD;&#xA;6. `sum(x.count('A') for x in open('seqs.fa','r') if x[0] != '&gt;')` in python&#xD;&#xA;&#xD;&#xA;This yields a time of&#xD;&#xA;&#xD;&#xA;6. 1.6 seconds (I'm at work now, so I've adjusted the time as best I can to account for the different computers)&#xD;&#xA;&#xD;&#xA;**Update2:** More benchmarks from the comments&#xD;&#xA;&#xD;&#xA;7. `awk -F A '!/^&gt;&quot;/ {cnt+=NF-1}END{print cnt}' foo.fa`&#xD;&#xA;8. `perl -ne 'if(!/^&gt;/){$count += tr/A//} END{print &quot;$count\n&quot;}' foo.fa`&#xD;&#xA;&#xD;&#xA;These yield times of:&#xD;&#xA;&#xD;&#xA;7. 5.2 seconds&#xD;&#xA;8. 1.18 seconds&#xD;&#xA;&#xD;&#xA;The awk version is the best hack I could come up with, there are likely better ways.&#xD;&#xA;&#xD;&#xA;**Update 3**: Right, so regarding fastq files, I'm running this on a 3.3GB gzipped file with 10 repetitions (this takes a bit of time to run), so I'm not going to initially limit tests to commands that I can trivially modify to handle compressed files (after all, uncompressed fastq files are an abomination).&#xD;&#xA;&#xD;&#xA;1. `seqtk fqchk`&#xD;&#xA;2. `sed -n '2~4p' &lt;(zcat foo.fastq.gz) | grep -io A | wc -l`&#xD;&#xA;3. `sed -n '1d;N;N;N;P;d' &lt;(zcat Undetermined_S0_R1_001.fastq.gz) | tr -cd A | wc -c`&#xD;&#xA;4. The C example from user1728181 (the comparison to C++ is already provided there, so no need to include it).&#xD;&#xA;5. `bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' &lt;(zcat foo.fastq.gz)`&#xD;&#xA;&#xD;&#xA;The average times are:&#xD;&#xA;&#xD;&#xA;1. 1 minute 44 seconds&#xD;&#xA;2. 2 minutes 6 seconds&#xD;&#xA;3. 1 minute 52 seconds&#xD;&#xA;4. 1 minute 15 seconds&#xD;&#xA;5. 3 minutes 8 seconds&#xD;&#xA;&#xD;&#xA;N.B., these will often not handle fastq files with entries spanning more than 4 lines. However, such files are the dodo birds of bioinformatics, so that's not practically important. Also, I couldn't get `ShortRead` to work with the compressed fastq file. I suspect there's a way to fix that." />
  <row Id="1173" PostHistoryTypeId="2" PostId="384" RevisionGUID="c573eee3-afbc-45a8-acaf-4d3601ca4641" CreationDate="2017-06-02T11:41:15.883" UserId="306" Text="The `minion` utility from the kraken/reaper toolkit may be helpful for this: http://wwwdev.ebi.ac.uk/enright-dev/kraken/reaper/src/reaper-latest/doc/minion.html" />
  <row Id="1174" PostHistoryTypeId="5" PostId="371" RevisionGUID="63e8b382-d4b5-49a2-ac43-4ef7fba64f60" CreationDate="2017-06-02T11:41:38.503" UserId="77" Comment="Correct bioawk command" Text="5 hours and no benchmarks posted? I am sorely disappointed.&#xD;&#xA;&#xD;&#xA;I'll restrict the comparison to just be fasta files, since fastq will end up being the same. So far, the contenders are:&#xD;&#xA;&#xD;&#xA;1. R with the `ShortRead` package (even if not the fastest, certainly a super convenient method).&#xD;&#xA;2. A pipeline of `grep -v &quot;^&gt;&quot; | tr -cd A | wc -c`&#xD;&#xA;3. A pipeline of `grep -v &quot;^&gt;&quot; | grep -io A | wc -l`&#xD;&#xA;4. A pipeline of `seqtk comp | awk`&#xD;&#xA;5. Something custom in C/C++&#xD;&#xA;&#xD;&#xA;The R code can't actually count N records for some reason when I try it, so I counted `A` for everything.&#xD;&#xA;&#xD;&#xA;I'll exclude all python solutions, because there's no conceivable universe in which python is fast. For the C/C++ solution I just used what @user172818 posted (everything I tried writing took the same amount of time).&#xD;&#xA;&#xD;&#xA;Repeating 100 times and taking the average (yes, one should take the median):&#xD;&#xA;&#xD;&#xA;1. 1.7 seconds&#xD;&#xA;2. 0.65 seconds&#xD;&#xA;3. 15 seconds&#xD;&#xA;4. 1.2 seconds&#xD;&#xA;5. 0.48 seconds&#xD;&#xA;&#xD;&#xA;Unsurprisingly, anything in straight C or C++ is going to win. `grep` with `tr` is surprisingly good, which surprises me since even though `grep` is very very optimized I still expected it to have more overhead. Piping to `grep -io` is a performance killer. The R solution is surprisingly good given the typical R overhead.&#xD;&#xA;&#xD;&#xA;**Update1:** As suggested in the comments&#xD;&#xA;&#xD;&#xA;6. `sum(x.count('A') for x in open('seqs.fa','r') if x[0] != '&gt;')` in python&#xD;&#xA;&#xD;&#xA;This yields a time of&#xD;&#xA;&#xD;&#xA;6. 1.6 seconds (I'm at work now, so I've adjusted the time as best I can to account for the different computers)&#xD;&#xA;&#xD;&#xA;**Update2:** More benchmarks from the comments&#xD;&#xA;&#xD;&#xA;7. `awk -F A '!/^&gt;&quot;/ {cnt+=NF-1}END{print cnt}' foo.fa`&#xD;&#xA;8. `perl -ne 'if(!/^&gt;/){$count += tr/A//} END{print &quot;$count\n&quot;}' foo.fa`&#xD;&#xA;&#xD;&#xA;These yield times of:&#xD;&#xA;&#xD;&#xA;7. 5.2 seconds&#xD;&#xA;8. 1.18 seconds&#xD;&#xA;&#xD;&#xA;The awk version is the best hack I could come up with, there are likely better ways.&#xD;&#xA;&#xD;&#xA;**Update 3**: Right, so regarding fastq files, I'm running this on a 3.3GB gzipped file with 10 repetitions (this takes a bit of time to run), so I'm not going to initially limit tests to commands that I can trivially modify to handle compressed files (after all, uncompressed fastq files are an abomination).&#xD;&#xA;&#xD;&#xA;1. `seqtk fqchk`&#xD;&#xA;2. `sed -n '2~4p' &lt;(zcat foo.fastq.gz) | grep -io A | wc -l`&#xD;&#xA;3. `sed -n '1d;N;N;N;P;d' &lt;(zcat Undetermined_S0_R1_001.fastq.gz) | tr -cd A | wc -c`&#xD;&#xA;4. The C example from user1728181 (the comparison to C++ is already provided there, so no need to include it).&#xD;&#xA;5. `bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' foo.fastq.gz`&#xD;&#xA;&#xD;&#xA;The average times are:&#xD;&#xA;&#xD;&#xA;1. 1 minute 44 seconds&#xD;&#xA;2. 2 minutes 6 seconds&#xD;&#xA;3. 1 minute 52 seconds&#xD;&#xA;4. 1 minute 15 seconds&#xD;&#xA;5. 3 minutes 8 seconds&#xD;&#xA;&#xD;&#xA;N.B., these will often not handle fastq files with entries spanning more than 4 lines. However, such files are the dodo birds of bioinformatics, so that's not practically important. Also, I couldn't get `ShortRead` to work with the compressed fastq file. I suspect there's a way to fix that." />
  <row Id="1175" PostHistoryTypeId="10" PostId="357" RevisionGUID="11653299-6f5b-4498-be86-dcb569fc82a9" CreationDate="2017-06-02T11:55:46.697" UserId="-1" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;},{&quot;Id&quot;:311,&quot;DisplayName&quot;:&quot;Michael&quot;},{&quot;Id&quot;:52,&quot;DisplayName&quot;:&quot;Axeman&quot;},{&quot;Id&quot;:215,&quot;DisplayName&quot;:&quot;SamStudio8&quot;}]}" />
  <row Id="1176" PostHistoryTypeId="5" PostId="360" RevisionGUID="f431052b-db5f-4db3-a1e7-0ae75f59537c" CreationDate="2017-06-02T11:57:39.400" UserId="425" Comment="readability" Text="I think that BASH is considered by many computer scientists and software engineers to be a standard. Since it is nowadays present on almost every Unix machine, it is probably the safest choice (from the viewpoint of availability, but not [security](https://www.symantec.com/connect/blogs/shellshock-all-you-need-know-about-bash-bug-vulnerability)).&#xD;&#xA;&#xD;&#xA;However, various BASH versions exist and scripts working on modern Linux machines with [BASH 4](http://wiki.bash-hackers.org/bash4) might not work on OS X, which is still based on BASH 3. If you are interested in a real industry standard, [BASH with the POSIX switcher](https://www.gnu.org/software/bash/manual/html_node/Bash-POSIX-Mode.html) might be an option." />
  <row Id="1179" PostHistoryTypeId="2" PostId="385" RevisionGUID="888ec0aa-f2d4-4c1b-99cf-8d0db1851d0a" CreationDate="2017-06-02T12:16:24.783" UserId="29" Text="Let’s start with what they have in common: All three formats store&#xD;&#xA;&#xD;&#xA;1. sequence data, and&#xD;&#xA;2. sequence metadata.&#xD;&#xA;&#xD;&#xA;Furthermore, all three formats are text-based.&#xD;&#xA;&#xD;&#xA;However, beyond that all three formats are different and serve different purposes.&#xD;&#xA;&#xD;&#xA;Let’s start with the simplest format:&#xD;&#xA;&#xD;&#xA;### FASTA&#xD;&#xA;&#xD;&#xA;FASTA stores a variable number of sequence records, and for each record it stores the sequence itself, and a sequence ID. Each record starts with a header line whose first character is `&gt;`, followed by the sequence ID. The next lines of a record contain the actual sequence.&#xD;&#xA;&#xD;&#xA;The [Wikipedia artice](https://en.wikipedia.org/wiki/FASTA_format) gives several examples for peptide sequences, but since FASTQ and SAM are used exclusively (?) for nucleotide sequences, here’s a nucleotide example:&#xD;&#xA;&#xD;&#xA;    &gt;Mus_musculus_tRNA-Ala-AGC-1-1 (chr13.trna34-AlaAGC)&#xD;&#xA;    GGGGGTGTAGCTCAGTGGTAGAGCGCGTGCTTAGCATGCACGAGGcCCTGGGTTCGATCC&#xD;&#xA;    CCAGCACCTCCA&#xD;&#xA;    &gt;Mus_musculus_tRNA-Ala-AGC-10-1 (chr13.trna457-AlaAGC)&#xD;&#xA;    GGGGGATTAGCTCAAATGGTAGAGCGCTCGCTTAGCATGCAAGAGGtAGTGGGATCGATG&#xD;&#xA;    CCCACATCCTCCA&#xD;&#xA;&#xD;&#xA;The ID can be in any arbitrary format, although [several conventions exist](https://en.wikipedia.org/wiki/FASTA_format#Sequence_identifiers).&#xD;&#xA;&#xD;&#xA;In the context of nucleotide sequences, FASTA is mostly used to store reference data; that is, data extracted from a curated database; the above is adapted from [GtRNAdb](http://gtrnadb.ucsc.edu/) (a database of tRNA sequences).&#xD;&#xA;&#xD;&#xA;### FASTQ&#xD;&#xA;&#xD;&#xA;FASTQ was conceived to solve a specific problem of FASTA files: when sequencing, the confidence in a given [base call](https://biology.stackexchange.com/a/1873/166) (that is, the identity of a nucleotide) varies. This is expressed in the [Phred quality score](https://en.wikipedia.org/wiki/Phred_quality_score). FASTA had no standardised way of encoding the this. By contrast, the a FASTQ record [contains a sequence of quality scores](https://en.wikipedia.org/wiki/FASTQ_format#Encoding) for each nucleotide.&#xD;&#xA;&#xD;&#xA;A FASTQ record has the following format:&#xD;&#xA;&#xD;&#xA;1. A line starting with `@`, containing the sequence ID.&#xD;&#xA;2. One or more lines that contain the sequence.&#xD;&#xA;3. A new line starting with the character `+`, and being either empty or repeating the sequence ID.&#xD;&#xA;4. One or more lines that contain the quality scores.&#xD;&#xA;&#xD;&#xA;Here’s an example of a FASTQ file with two records:&#xD;&#xA;&#xD;&#xA;    @071112_SLXA-EAS1_s_7:5:1:817:345&#xD;&#xA;    GGGTGATGGCCGCTGCCGATGGCGTC&#xD;&#xA;    AAATCCCACC&#xD;&#xA;    +&#xD;&#xA;    IIIIIIIIIIIIIIIIIIIIIIIIII&#xD;&#xA;    IIII9IG9IC&#xD;&#xA;    @071112_SLXA-EAS1_s_7:5:1:801:338&#xD;&#xA;    GTTCAGGGATACGACGTTTGTATTTTAAGAATCTGA&#xD;&#xA;    +&#xD;&#xA;    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII6IBI&#xD;&#xA;&#xD;&#xA;FASTQ files are (almost?) exclusively used to store short-read data from high-throughput sequencing experiments. As a consequence, the sequence and quality scores are usually put into a single line each, and indeed many tools assume that each record in a FASTQ file is exactly four lines long, even though this isn’t guaranteed.&#xD;&#xA;&#xD;&#xA;As for FASTA, the format of the sequence ID isn’t standardised, but different producers of FASTQ use [fixed notations that follow strict conventions](https://en.wikipedia.org/wiki/FASTQ_format#Illumina_sequence_identifiers).&#xD;&#xA;&#xD;&#xA;### SAM&#xD;&#xA;&#xD;&#xA;SAM files are so complex that a [complete description](https://samtools.github.io/hts-specs/SAMv1.pdf) &lt;sup&gt;[PDF]&lt;/sup&gt; takes 15 pages. So here’s the short version.&#xD;&#xA;&#xD;&#xA;The original purpose of SAM files is to store mapping information for sequences from high-throughput sequencing. As a consequence, a SAM record needs to store more than just the sequence and its quality, it also needs to store information about where and how a sequence maps into the reference.&#xD;&#xA;&#xD;&#xA;Unlike the previous formats, SAM is tab-based, and each record, consisting of either 11 or 12 fields, fills exactly one line. Here’s an example (tabs replaced by fixed-width spacing):&#xD;&#xA;&#xD;&#xA;    r001  99  chr1  7 30  17M         =  37  39  TTAGATAAAGGATACTG   IIIIIIIIIIIIIIIII&#xD;&#xA;    r002  0   chrX  9 30  3S6M1P1I4M  *  0   0   AAAAGATAAGGATA      IIIIIIIIII6IBI    NM:i:1&#xD;&#xA;&#xD;&#xA;For a description of the individual fields, refer to the documentation. The relevant bit is this: SAM can express exactly the same information as FASTQ, plus, as mentioned, the mapping information. However, SAM is also used to store read data *without* mapping information.&#xD;&#xA;&#xD;&#xA;In addition to sequence records, SAM files can also contain a *header*, which stores information about the reference that the sequences were mapped to, and the tool used to create the SAM file. Header information precede the sequence records, and consist of lines starting with `@`.&#xD;&#xA;&#xD;&#xA;SAM itself is almost never used as a storage format; instead, files are stored in BAM format, which is a compact binary representation of SAM. It stores the same information, just more efficiently, and in conjunction with a [search index](https://en.wikipedia.org/wiki/Database_index), allows fast retrieval of individual records from the middle of the file (= fast [random access](https://en.wikipedia.org/wiki/Random_access)). BAM files are also much more compact than compressed FASTQ or FASTA files.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;The above implies a *hierarchy* in what the formats can store: FASTA ⊂ FASTQ ⊂ SAM.&#xD;&#xA;&#xD;&#xA;In a typical high-throughput analysis workflow, you will encounter all three file types:&#xD;&#xA;&#xD;&#xA;1. FASTA to store the reference genome/transcriptome that the sequence fragments will be mapped to.&#xD;&#xA;2. FASTQ to store the sequence fragments before mapping.&#xD;&#xA;3. SAM/BAM to store the sequence fragments after mapping." />
  <row Id="1180" PostHistoryTypeId="11" PostId="283" RevisionGUID="a2593b9d-0e23-4c4d-9c9d-b2b67a691843" CreationDate="2017-06-02T12:21:39.440" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:37,&quot;DisplayName&quot;:&quot;user172818&quot;},{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:311,&quot;DisplayName&quot;:&quot;Michael&quot;},{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:292,&quot;DisplayName&quot;:&quot;bli&quot;}]}" />
  <row Id="1181" PostHistoryTypeId="11" PostId="196" RevisionGUID="b060eaf3-8ce7-448a-bcf4-8ff5b6629d1b" CreationDate="2017-06-02T12:27:05.573" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;},{&quot;Id&quot;:311,&quot;DisplayName&quot;:&quot;Michael&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;},{&quot;Id&quot;:292,&quot;DisplayName&quot;:&quot;bli&quot;}]}" />
  <row Id="1182" PostHistoryTypeId="5" PostId="385" RevisionGUID="bee862ae-85c9-4e79-b51c-68fb22bb48f9" CreationDate="2017-06-02T12:32:37.677" UserId="19" Comment="Fixing a couple of typos" Text="Let’s start with what they have in common: All three formats store&#xD;&#xA;&#xD;&#xA;1. sequence data, and&#xD;&#xA;2. sequence metadata.&#xD;&#xA;&#xD;&#xA;Furthermore, all three formats are text-based.&#xD;&#xA;&#xD;&#xA;However, beyond that all three formats are different and serve different purposes.&#xD;&#xA;&#xD;&#xA;Let’s start with the simplest format:&#xD;&#xA;&#xD;&#xA;### FASTA&#xD;&#xA;&#xD;&#xA;FASTA stores a variable number of sequence records, and for each record it stores the sequence itself, and a sequence ID. Each record starts with a header line whose first character is `&gt;`, followed by the sequence ID. The next lines of a record contain the actual sequence.&#xD;&#xA;&#xD;&#xA;The [Wikipedia artice](https://en.wikipedia.org/wiki/FASTA_format) gives several examples for peptide sequences, but since FASTQ and SAM are used exclusively (?) for nucleotide sequences, here’s a nucleotide example:&#xD;&#xA;&#xD;&#xA;    &gt;Mus_musculus_tRNA-Ala-AGC-1-1 (chr13.trna34-AlaAGC)&#xD;&#xA;    GGGGGTGTAGCTCAGTGGTAGAGCGCGTGCTTAGCATGCACGAGGcCCTGGGTTCGATCC&#xD;&#xA;    CCAGCACCTCCA&#xD;&#xA;    &gt;Mus_musculus_tRNA-Ala-AGC-10-1 (chr13.trna457-AlaAGC)&#xD;&#xA;    GGGGGATTAGCTCAAATGGTAGAGCGCTCGCTTAGCATGCAAGAGGtAGTGGGATCGATG&#xD;&#xA;    CCCACATCCTCCA&#xD;&#xA;&#xD;&#xA;The ID can be in any arbitrary format, although [several conventions exist](https://en.wikipedia.org/wiki/FASTA_format#Sequence_identifiers).&#xD;&#xA;&#xD;&#xA;In the context of nucleotide sequences, FASTA is mostly used to store reference data; that is, data extracted from a curated database; the above is adapted from [GtRNAdb](http://gtrnadb.ucsc.edu/) (a database of tRNA sequences).&#xD;&#xA;&#xD;&#xA;### FASTQ&#xD;&#xA;&#xD;&#xA;FASTQ was conceived to solve a specific problem of FASTA files: when sequencing, the confidence in a given [base call](https://biology.stackexchange.com/a/1873/166) (that is, the identity of a nucleotide) varies. This is expressed in the [Phred quality score](https://en.wikipedia.org/wiki/Phred_quality_score). FASTA had no standardised way of encoding this. By contrast, a FASTQ record [contains a sequence of quality scores](https://en.wikipedia.org/wiki/FASTQ_format#Encoding) for each nucleotide.&#xD;&#xA;&#xD;&#xA;A FASTQ record has the following format:&#xD;&#xA;&#xD;&#xA;1. A line starting with `@`, containing the sequence ID.&#xD;&#xA;2. One or more lines that contain the sequence.&#xD;&#xA;3. A new line starting with the character `+`, and being either empty or repeating the sequence ID.&#xD;&#xA;4. One or more lines that contain the quality scores.&#xD;&#xA;&#xD;&#xA;Here’s an example of a FASTQ file with two records:&#xD;&#xA;&#xD;&#xA;    @071112_SLXA-EAS1_s_7:5:1:817:345&#xD;&#xA;    GGGTGATGGCCGCTGCCGATGGCGTC&#xD;&#xA;    AAATCCCACC&#xD;&#xA;    +&#xD;&#xA;    IIIIIIIIIIIIIIIIIIIIIIIIII&#xD;&#xA;    IIII9IG9IC&#xD;&#xA;    @071112_SLXA-EAS1_s_7:5:1:801:338&#xD;&#xA;    GTTCAGGGATACGACGTTTGTATTTTAAGAATCTGA&#xD;&#xA;    +&#xD;&#xA;    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII6IBI&#xD;&#xA;&#xD;&#xA;FASTQ files are (almost?) exclusively used to store short-read data from high-throughput sequencing experiments. As a consequence, the sequence and quality scores are usually put into a single line each, and indeed many tools assume that each record in a FASTQ file is exactly four lines long, even though this isn’t guaranteed.&#xD;&#xA;&#xD;&#xA;As for FASTA, the format of the sequence ID isn’t standardised, but different producers of FASTQ use [fixed notations that follow strict conventions](https://en.wikipedia.org/wiki/FASTQ_format#Illumina_sequence_identifiers).&#xD;&#xA;&#xD;&#xA;### SAM&#xD;&#xA;&#xD;&#xA;SAM files are so complex that a [complete description](https://samtools.github.io/hts-specs/SAMv1.pdf) &lt;sup&gt;[PDF]&lt;/sup&gt; takes 15 pages. So here’s the short version.&#xD;&#xA;&#xD;&#xA;The original purpose of SAM files is to store mapping information for sequences from high-throughput sequencing. As a consequence, a SAM record needs to store more than just the sequence and its quality, it also needs to store information about where and how a sequence maps into the reference.&#xD;&#xA;&#xD;&#xA;Unlike the previous formats, SAM is tab-based, and each record, consisting of either 11 or 12 fields, fills exactly one line. Here’s an example (tabs replaced by fixed-width spacing):&#xD;&#xA;&#xD;&#xA;    r001  99  chr1  7 30  17M         =  37  39  TTAGATAAAGGATACTG   IIIIIIIIIIIIIIIII&#xD;&#xA;    r002  0   chrX  9 30  3S6M1P1I4M  *  0   0   AAAAGATAAGGATA      IIIIIIIIII6IBI    NM:i:1&#xD;&#xA;&#xD;&#xA;For a description of the individual fields, refer to the documentation. The relevant bit is this: SAM can express exactly the same information as FASTQ, plus, as mentioned, the mapping information. However, SAM is also used to store read data *without* mapping information.&#xD;&#xA;&#xD;&#xA;In addition to sequence records, SAM files can also contain a *header*, which stores information about the reference that the sequences were mapped to, and the tool used to create the SAM file. Header information precede the sequence records, and consist of lines starting with `@`.&#xD;&#xA;&#xD;&#xA;SAM itself is almost never used as a storage format; instead, files are stored in BAM format, which is a compact binary representation of SAM. It stores the same information, just more efficiently, and in conjunction with a [search index](https://en.wikipedia.org/wiki/Database_index), allows fast retrieval of individual records from the middle of the file (= fast [random access](https://en.wikipedia.org/wiki/Random_access)). BAM files are also much more compact than compressed FASTQ or FASTA files.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;The above implies a *hierarchy* in what the formats can store: FASTA ⊂ FASTQ ⊂ SAM.&#xD;&#xA;&#xD;&#xA;In a typical high-throughput analysis workflow, you will encounter all three file types:&#xD;&#xA;&#xD;&#xA;1. FASTA to store the reference genome/transcriptome that the sequence fragments will be mapped to.&#xD;&#xA;2. FASTQ to store the sequence fragments before mapping.&#xD;&#xA;3. SAM/BAM to store the sequence fragments after mapping." />
  <row Id="1183" PostHistoryTypeId="24" PostId="385" RevisionGUID="bee862ae-85c9-4e79-b51c-68fb22bb48f9" CreationDate="2017-06-02T12:32:37.677" Comment="Proposed by 19 approved by 29 edit id of 96" />
  <row Id="1186" PostHistoryTypeId="2" PostId="386" RevisionGUID="1cb01683-d13f-4d6a-b195-cbbab04dd238" CreationDate="2017-06-02T13:16:00.703" UserId="81" Text="I would also recommend to take a look at [pomegranate](https://pomegranate.readthedocs.io/en/latest/), a nice Python package for probabilistic graphical models. It includes solvers for HMMs and much more. Under the hood it uses Cythonised code, so it's also quite fast." />
  <row Id="1187" PostHistoryTypeId="5" PostId="382" RevisionGUID="92d8dd88-7a91-45cd-83c3-43d75e35dbcb" CreationDate="2017-06-02T13:18:13.977" UserId="292" Comment="Trying to improve formatting and language. (highlighting doesn't seem to work yet)" Text="Here is a bash script and GNU `parallel` along with some other tools to split-apply-combine the data. It may or may not work out of the box, but it's not far off and it should give the general idea of how one way of how to approach this.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    #!/bin/bash&#xD;&#xA;    &#xD;&#xA;    # ====== USER PROVIDED PARAMETERS ========&#xD;&#xA;    N=20 # number of cores to use&#xD;&#xA;    ngsfilter=&quot;rawdata_scandinavia.ngsfilter&quot;&#xD;&#xA;    unidentified=&quot;unidentified_samples.fastq&quot;&#xD;&#xA;    &#xD;&#xA;    R1=&quot;rawdata_scandinavia_R1.fastq&quot;&#xD;&#xA;    R2=&quot;rawdata_scandinavia_R2.fastq&quot;&#xD;&#xA;    # ==== END USER PROVIDED PARAMETERS =====&#xD;&#xA;    &#xD;&#xA;    # fastqutils available @ http://ngsutils.org/modules/fastqutils&#xD;&#xA;    fastqutils split $R1 to_illumina_R1_ $N &amp;&#xD;&#xA;    fastqutils split $R2 to_illumina_R2_ $N &amp;&#xD;&#xA;    wait&#xD;&#xA;    &#xD;&#xA;    R1=$( ls | grep -P &quot;to_illumina_R1_&quot; )&#xD;&#xA;    R2=$( ls | grep -P &quot;to_illumina_R2_&quot; )&#xD;&#xA;    &#xD;&#xA;    # Do alignment of two reads.&#xD;&#xA;    for i in $( seq $N )&#xD;&#xA;    do&#xD;&#xA;    	partR1=$( ls | grep -P &quot;^to_illumina_R1_\\.$i\\.fastq$&quot; )&#xD;&#xA;    	partR2=$( ls | grep -P &quot;^to_illumina_R2_\\.$i\\.fastq$&quot; )&#xD;&#xA;    	illuminapairedend -r $partR1 $partR2 | tee to_ngsfilter_$i.fastq | obiannotate -S goodAli:'&quot;Align&quot; if score&gt;40.00 else &quot;Bad&quot;' | obisplit -t goodAli -p to_ngsfilter_$i. &amp;&#xD;&#xA;    done&#xD;&#xA;    &#xD;&#xA;    # Remove intermediate aligned files.&#xD;&#xA;    echo $R1 | xargs rm -r&#xD;&#xA;    echo $R2 | xargs rm -r&#xD;&#xA;    ls | grep -P &quot;\\.seq$|\\.err$&quot; | xargs rm -r&#xD;&#xA;    &#xD;&#xA;    input=$( ls | grep -P &quot;to_ngsfilter_[0-9]+\\.Align\\.fastq&quot; )&#xD;&#xA;    &#xD;&#xA;    # For some reasons, files need to be &quot;touched&quot; in order to work. Perhaps&#xD;&#xA;    # it's not closing them...&#xD;&#xA;    touch $unidentified&#xD;&#xA;    &#xD;&#xA;    touchedbyangel=$( ls | grep -P &quot;to_ngsfilter_&quot;  )&#xD;&#xA;    for i in $touchedbyangel&#xD;&#xA;    do&#xD;&#xA;    	touch $i&#xD;&#xA;    done&#xD;&#xA;    &#xD;&#xA;    parallel -j$N --result filtered_{} ngsfilter -t $ngsfilter -u $unidentified {} ::: $input&#xD;&#xA;    &#xD;&#xA;    # Remove intermediate results.&#xD;&#xA;    ls | grep -P &quot;\\.seq$|\\.err$&quot; | xargs rm -r&#xD;&#xA;    &#xD;&#xA;    ls | grep -P &quot;filtered_.*\\.fastq$&quot; | xargs cat &gt; filtered_data.fastq&#xD;&#xA;    &#xD;&#xA;    # remove intermediate results to conserve space&#xD;&#xA;    rm to_ngsfilter*&#xD;&#xA;    &#xD;&#xA;    # Split reads by locus.&#xD;&#xA;    filtered=$( ls | grep -P &quot;^filtered_&quot;  )&#xD;&#xA;    parallel -j$N --result to_split_{} obisplit -p MS.PCR_ -t experiment {} ::: $filtered&#xD;&#xA;    &#xD;&#xA;    ls | grep -P &quot;\\.err$|\\.seq$&quot; | xargs rm -r&#xD;&#xA;    ls | grep -P &quot;^to_split_&quot; | xargs rm -r&#xD;&#xA;    &#xD;&#xA;    # Wasn't able to parallelize this, so it's just pushing tasks into the background.&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_03.fastq &gt; MS.PCR_UA_MxRout1_03.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_06.fastq &gt; MS.PCR_UA_MxRout1_06.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_14.fastq &gt; MS.PCR_UA_MxRout1_14.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_16.fastq &gt; MS.PCR_UA_MxRout1_16.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_17.fastq &gt; MS.PCR_UA_MxRout1_17.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_25.fastq &gt; MS.PCR_UA_MxRout1_25.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_51.fastq &gt; MS.PCR_UA_MxRout1_51.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_57.fastq &gt; MS.PCR_UA_MxRout1_57.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_63.fastq &gt; MS.PCR_UA_MxRout1_63.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_64.fastq &gt; MS.PCR_UA_MxRout1_64.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_65.fastq &gt; MS.PCR_UA_MxRout1_65.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_67.fastq &gt; MS.PCR_UA_MxRout1_67.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_68.fastq &gt; MS.PCR_UA_MxRout1_68.uniq.fasta &amp;&#xD;&#xA;    obiuniq -m sample MS.PCR_UA_MxRout1_ZF.fastq &gt; MS.PCR_UA_MxRout1_ZF.uniq.fasta &amp;&#xD;&#xA;    wait&#xD;&#xA;    &#xD;&#xA;    uniqfiles=$( ls | grep -P &quot;.{2}+\\.uniq\\.fasta$&quot; )&#xD;&#xA;    &#xD;&#xA;    for i in $uniqfiles&#xD;&#xA;    do&#xD;&#xA;    	# this section removes the file suffix&#xD;&#xA;    	# https://stackoverflow.com/questions/125281/how-do-i-remove-the-file-suffix-and-path-portion-from-a-path-string-in-bash&#xD;&#xA;    	tab=${i%.fasta}&#xD;&#xA;    	tab=${tab##*/}&#xD;&#xA;    	obigrep -p 'count&gt;1' $i | obiannotate -k merged_sample -k count | obiannotate --length | obisort -r -k seq_length | obitab --no-definition --output-seq &gt; $tab.tab&#xD;&#xA;    done&#xD;&#xA;    &#xD;&#xA;    ls | grep -P &quot;\\.uniq\\.fasta&quot; | xargs rm -r&#xD;&#xA;    &#xD;&#xA;    echo &quot;Done. I think.&quot;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1188" PostHistoryTypeId="24" PostId="382" RevisionGUID="92d8dd88-7a91-45cd-83c3-43d75e35dbcb" CreationDate="2017-06-02T13:18:13.977" Comment="Proposed by 292 approved by 77, 200 edit id of 98" />
  <row Id="1189" PostHistoryTypeId="5" PostId="363" RevisionGUID="a76c045d-0f8c-4214-96f4-5fdbae7233ce" CreationDate="2017-06-02T13:41:57.900" UserId="298" Comment="There's nothing bash-specific or even bash-related there. " Text="**FASTQ**&#xD;&#xA;&#xD;&#xA;As it was pointed out, fastq can be complicated. But in a simple case when you have four lines per record, one possible solution in bash is:&#xD;&#xA;&#xD;&#xA;    sed -n '2~4p' seqs.fastq | grep -io N | wc -l&#xD;&#xA;&#xD;&#xA;- `sed -n '2~4p'` will print every fourth line&#xD;&#xA;- `grep -o N` will output a line with `N` for every matching symbol&#xD;&#xA;- `wc -l` will count the lines&#xD;&#xA;&#xD;&#xA;I suspect this python approach will work faster:&#xD;&#xA;&#xD;&#xA;    cat seqs.fastq | python3 -c &quot;import sys; print(sum(line.upper().count('N') for line in sys.stdin))&quot;&#xD;&#xA;&#xD;&#xA;**FASTA**&#xD;&#xA;&#xD;&#xA;Coreutils:&#xD;&#xA;&#xD;&#xA;    grep -v &quot;^&gt;&quot; seqs.fasta | grep -io N | wc -l&#xD;&#xA;&#xD;&#xA;Python:&#xD;&#xA;&#xD;&#xA;    cat seqs.fasta | python3 -c &quot;import sys; print(sum(line.upper().count('N') for line in sys.stdin if not line.startswith('&gt;')))&quot;&#xD;&#xA;" />
  <row Id="1190" PostHistoryTypeId="24" PostId="363" RevisionGUID="a76c045d-0f8c-4214-96f4-5fdbae7233ce" CreationDate="2017-06-02T13:41:57.900" Comment="Proposed by 298 approved by 73, 191 edit id of 97" />
  <row Id="1194" PostHistoryTypeId="2" PostId="388" RevisionGUID="16d9022c-bd63-4026-87d6-8de7fd7d6df1" CreationDate="2017-06-02T14:08:49.563" UserId="104" Text="I want some templates of different file formats that I can use to test my scripts and identify possible bugs in my code.&#xD;&#xA;&#xD;&#xA;For example, consider FASTA, a simple but often abused format, I would want templates to capture regular and irregular formats, like I have seen all of these:&#xD;&#xA;&#xD;&#xA;1) Single line sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    ATG&#xD;&#xA;&#xD;&#xA;2) Multi-line sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    AT&#xD;&#xA;    G&#xD;&#xA;&#xD;&#xA;3) Upper and lower case letters in sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    Atg&#xD;&#xA;&#xD;&#xA;4) Ns and Xs in sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    ANnxX&#xD;&#xA;&#xD;&#xA;5) Unusual headers (sometimes even non-ASCI characters in headers)&#xD;&#xA;&#xD;&#xA;    &gt;ATG | &gt;Name @species | (MULTI_SPECIES)[taxa]&#xD;&#xA;    ATG&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;etc.&#xD;&#xA;&#xD;&#xA;There should be separate templates for nucleotide and protein FASTA, and separate ones for aligned FASTA.&#xD;&#xA;&#xD;&#xA;It would ideally include other aspects too, like different compression formats (such as `.gz`, `.bzip2`) and different file extensions (such as `.fa`, `.fasta`).&#xD;&#xA;&#xD;&#xA;I have never seen resources that provide templates covering these, but I think it would be useful. Of course I could build my own templates but it would take time to capture all the likely variations of the formats, particularly for more complex file formats.&#xD;&#xA;&#xD;&#xA;Note, I am not just interested in FASTA format, it was an example.&#xD;&#xA;&#xD;&#xA;Also note, I know about tools (such as `BioPython`) that should handle many formats well, but they may also have bugs. Anyway, in practice sometimes I end up parsing files myself directly because I don't want the overhead or dependency of an external package.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1195" PostHistoryTypeId="1" PostId="388" RevisionGUID="16d9022c-bd63-4026-87d6-8de7fd7d6df1" CreationDate="2017-06-02T14:08:49.563" UserId="104" Text="Are there any databases of templates for common bioinformatic file formats?" />
  <row Id="1196" PostHistoryTypeId="3" PostId="388" RevisionGUID="16d9022c-bd63-4026-87d6-8de7fd7d6df1" CreationDate="2017-06-02T14:08:49.563" UserId="104" Text="&lt;fasta&gt;&lt;database&gt;&lt;formats&gt;" />
  <row Id="1198" PostHistoryTypeId="5" PostId="369" RevisionGUID="cd764e7f-8490-4ec8-ae47-a7d5a2fe1b33" CreationDate="2017-06-02T14:14:13.903" UserId="37" Comment="Modifications based on comments below" Text="For FASTQ:&#xD;&#xA;&#xD;&#xA;    seqtk fqchk in.fq | head -2&#xD;&#xA;&#xD;&#xA;It gives you percentage of &quot;N&quot; bases, not the exact count, though.&#xD;&#xA;&#xD;&#xA;For FASTA:&#xD;&#xA;&#xD;&#xA;    seqtk comp in.fa | awk '{x+=$9}END{print x}'&#xD;&#xA;&#xD;&#xA;This command line also works with FASTQ, but it will be slower as awk is slow.&#xD;&#xA;&#xD;&#xA;EDIT: ok, based on @BaCH's reminder, here we go (you need [kseq.h](https://github.com/lh3/readfq/blob/master/kseq.h) to compile):&#xD;&#xA;&#xD;&#xA;    // to compile: gcc -O2 -o count-N this-prog.c&#xD;&#xA;    #include &lt;zlib.h&gt;&#xD;&#xA;    #include &lt;stdio.h&gt;&#xD;&#xA;    #include &lt;stdint.h&gt;&#xD;&#xA;    #include &quot;kseq.h&quot;&#xD;&#xA;    KSEQ_INIT(gzFile, gzread)&#xD;&#xA;    &#xD;&#xA;    unsigned char dna5tbl[256] = {&#xD;&#xA;    	0, 1, 2, 3,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 5, 4, 4,&#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 0, 4, 1,  4, 4, 4, 2,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  3, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 0, 4, 1,  4, 4, 4, 2,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  3, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4&#xD;&#xA;    };&#xD;&#xA;    &#xD;&#xA;    int main(int argc, char *argv[]) {&#xD;&#xA;    	long i, n_n = 0, n_acgt = 0, n_gap = 0;&#xD;&#xA;    	gzFile fp;&#xD;&#xA;    	kseq_t *seq;&#xD;&#xA;    	if (argc == 1) {&#xD;&#xA;    		fprintf(stderr, &quot;Usage: count-N &lt;in.fa&gt;\n&quot;);&#xD;&#xA;    		return 1;&#xD;&#xA;    	}&#xD;&#xA;    	if ((fp = gzopen(argv[1], &quot;r&quot;)) == 0) {&#xD;&#xA;    		fprintf(stderr, &quot;ERROR: fail to open the input file\n&quot;);&#xD;&#xA;    		return 1;&#xD;&#xA;    	}&#xD;&#xA;    	seq = kseq_init(fp);&#xD;&#xA;    	while (kseq_read(seq) &gt;= 0) {&#xD;&#xA;    		for (i = 0; i &lt; seq-&gt;seq.l; ++i) {&#xD;&#xA;    			int c = dna5tbl[(unsigned char)seq-&gt;seq.s[i]];&#xD;&#xA;    			if (c &lt; 4) ++n_acgt;&#xD;&#xA;    			else if (c == 4) ++n_n;&#xD;&#xA;    			else ++n_gap;&#xD;&#xA;    		}&#xD;&#xA;    	}&#xD;&#xA;    	kseq_destroy(seq);&#xD;&#xA;    	gzclose(fp);&#xD;&#xA;    	printf(&quot;%ld\t%ld\t%ld\n&quot;, n_acgt, n_n, n_gap);&#xD;&#xA;    	return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;It works for both FASTA/Q and gzip'ed FASTA/Q. The following uses SeqAn:&#xD;&#xA;&#xD;&#xA;    #include &lt;seqan/seq_io.h&gt;&#xD;&#xA;&#xD;&#xA;    using namespace seqan;&#xD;&#xA;&#xD;&#xA;    int main(int argc, char *argv[]) {&#xD;&#xA;        if (argc == 1) {&#xD;&#xA;            std::cerr &lt;&lt; &quot;Usage: count-N &lt;in.fastq&gt;&quot; &lt;&lt; std::endl;&#xD;&#xA;            return 1;&#xD;&#xA;        }&#xD;&#xA;        std::ios::sync_with_stdio(false);&#xD;&#xA;        CharString id;&#xD;&#xA;        Dna5String seq;&#xD;&#xA;        SeqFileIn seqFileIn(argv[1]);&#xD;&#xA;        long i, n_n = 0, n_acgt = 0;&#xD;&#xA;        while (!atEnd(seqFileIn)) {&#xD;&#xA;            readRecord(id, seq, seqFileIn);&#xD;&#xA;            for (i = beginPosition(seq); i &lt; endPosition(seq); ++i)&#xD;&#xA;                if (seq[i] &lt; 4) ++n_acgt;&#xD;&#xA;                else ++n_n;&#xD;&#xA;        }&#xD;&#xA;        std::cout &lt;&lt; n_acgt &lt;&lt; '\t' &lt;&lt; n_n &lt;&lt; std::endl;&#xD;&#xA;        return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;On a FASTQ with 4-million 150bp reads:&#xD;&#xA;&#xD;&#xA;* The C version: ~0.74 sec&#xD;&#xA;* The C++ version: ~2.15 sec&#xD;&#xA;* An older C version without a lookup table (see previous edit): ~2.65 sec" />
  <row Id="1199" PostHistoryTypeId="2" PostId="389" RevisionGUID="a8c375e1-3e22-4685-b52f-faa97f6dc678" CreationDate="2017-06-02T14:20:08.550" UserId="37" Text="I have used smalt a couple of times. Smalt is more flexible than bwa/bowtie and can be tuned to be more sensitive to divergent hits, which is useful to certain applications. I heard Sanger did evaluate a few mappers for some non-typical applications (not sure what). They found smalt to perform better. Also smalt was developed at Sanger. I guess they could modify smalt based on their needs, though I don't know the details." />
  <row Id="1200" PostHistoryTypeId="2" PostId="390" RevisionGUID="3783ad37-7955-4534-9446-5b2477490a3a" CreationDate="2017-06-02T14:20:35.313" UserId="446" Text="It could be an idea to fragment the long reads into small sequences, like simulating Illumina reads of 150 bp, and then map these small sequences against the original long reads and extract regions with a high coverage? " />
  <row Id="1201" PostHistoryTypeId="2" PostId="391" RevisionGUID="cfec2db5-7eb4-4422-950d-6c1beaf72be6" CreationDate="2017-06-02T14:24:46.343" UserId="269" Text="These may not be exactly what your are looking for, but they do contain a wide range of formats with examples. If you want more, you might try searching other major databases hosting other types of into e.g. Uniprot, PDB, NCBI. &#xD;&#xA;&#xD;&#xA;https://genome.ucsc.edu/FAQ/FAQformat.html&#xD;&#xA;&#xD;&#xA;http://www.ensembl.org/info/website/upload/bed.html" />
  <row Id="1202" PostHistoryTypeId="2" PostId="392" RevisionGUID="0b0500ab-22fb-49b9-b368-c41905ca0ddd" CreationDate="2017-06-02T14:24:55.750" UserId="298" Text="No. At least none that I've heard of and I doubt there ever will be. There is no central repository for formats and each tool, community, field etc have their own. &#xD;&#xA;&#xD;&#xA;The best you can do is look up the official standard for each format and hope they include examples. Having a truly comprehensive collection of all possible variations is basically impossible for &lt;del&gt;horrible&lt;/del&gt; complex formats like VCF. Just consider the simple fact that it allows user-defined `INFO` fields with arbitrary contents. &#xD;&#xA;&#xD;&#xA;So the best you can do is make sure your scripts conform with the standard and hope your input does as well. " />
  <row Id="1204" PostHistoryTypeId="5" PostId="364" RevisionGUID="9cae6342-68bf-4d84-a4e6-910e01254e19" CreationDate="2017-06-02T14:53:35.553" UserId="29" Comment="added 60 characters in body" Text="Honestly, the easiest way (especially for FASTQ) is probably to use a dedicated parsing library, such as R/Bioconductor:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    suppressMessages(library(ShortRead))&#xD;&#xA;    &#xD;&#xA;    seq = readFasta(commandArgs(TRUE)[1]) # or readFastq&#xD;&#xA;    cat(colSums(alphabetFrequency(sread(seq))[, 'N', drop = FALSE]), '\n')&#xD;&#xA;&#xD;&#xA;This may not be the fastest, but it’s *pretty fast*, since the relevant methods are highly optimised. The biggest overhead is actually probably from loading `ShortRead`, which is an unjustifiable slog." />
  <row Id="1205" PostHistoryTypeId="5" PostId="153" RevisionGUID="15c02a44-7b2d-46b2-8a4b-a582c432c190" CreationDate="2017-06-02T16:04:30.200" UserId="292" Comment="deleted 3 characters in body" Text="I have a set of high-troughput experiments with 2 genotypes (&quot;WT&quot; and &quot;prg1&quot;) and 3 treatments (&quot;RT&quot;, &quot;HS30&quot; and &quot;HS30RT120&quot;), and there are 2 replicates for each of the genotype x treatment combinations.&#xD;&#xA;&#xD;&#xA;The read counts for the genes are summarized in a file that I load as follows in R:&#xD;&#xA;&#xD;&#xA;    &gt; counts_data &lt;- read.table(&quot;path/to/my/file&quot;, header=TRUE, row.names=&quot;gene&quot;)&#xD;&#xA;    &gt; colnames(counts_data)&#xD;&#xA;     [1] &quot;WT_RT_1&quot;          &quot;WT_HS30_1&quot;        &quot;WT_HS30RT120_1&quot;   &quot;prg1_RT_1&quot;       &#xD;&#xA;     [5] &quot;prg1_HS30_1&quot;      &quot;prg1_HS30RT120_1&quot; &quot;WT_RT_2&quot;          &quot;WT_HS30_2&quot;       &#xD;&#xA;     [9] &quot;WT_HS30RT120_2&quot;   &quot;prg1_RT_2&quot;        &quot;prg1_HS30_2&quot;      &quot;prg1_HS30RT120_2&quot;&#xD;&#xA;&#xD;&#xA;I describe the experiments as follows:&#xD;&#xA;&#xD;&#xA;    &gt; col_data &lt;- DataFrame(&#xD;&#xA;        geno = c(rep(&quot;WT&quot;, times=3), rep(&quot;prg1&quot;, times=3), rep(&quot;WT&quot;, times=3), rep(&quot;prg1&quot;, times=3)),&#xD;&#xA;        treat = rep(c(&quot;RT&quot;, &quot;HS30&quot;, &quot;HS30RT120&quot;), times=4),&#xD;&#xA;        rep = c(rep(&quot;1&quot;, times=6), rep(&quot;2&quot;, times=6)),&#xD;&#xA;        row.names = colnames(counts_data))&#xD;&#xA;    &gt; col_data&#xD;&#xA;    DataFrame with 12 rows and 3 columns&#xD;&#xA;                            geno       treat         rep&#xD;&#xA;                     &lt;character&gt; &lt;character&gt; &lt;character&gt;&#xD;&#xA;    WT_RT_1                   WT          RT           1&#xD;&#xA;    WT_HS30_1                 WT        HS30           1&#xD;&#xA;    WT_HS30RT120_1            WT   HS30RT120           1&#xD;&#xA;    prg1_RT_1               prg1          RT           1&#xD;&#xA;    prg1_HS30_1             prg1        HS30           1&#xD;&#xA;    ...                      ...         ...         ...&#xD;&#xA;    WT_HS30_2                 WT        HS30           2&#xD;&#xA;    WT_HS30RT120_2            WT   HS30RT120           2&#xD;&#xA;    prg1_RT_2               prg1          RT           2&#xD;&#xA;    prg1_HS30_2             prg1        HS30           2&#xD;&#xA;    prg1_HS30RT120_2        prg1   HS30RT120           2&#xD;&#xA;&#xD;&#xA;I want to build a DESeq2 object that I could use to either:&#xD;&#xA;&#xD;&#xA;- **find differentially expressed genes when the treatment varies for a given fixed genotype**&#xD;&#xA;&#xD;&#xA;or:&#xD;&#xA;&#xD;&#xA;- **find differentially expressed genes when the genotype varies for a given fixed treatment**&#xD;&#xA;&#xD;&#xA;In the [bioconductor help forum](https://support.bioconductor.org/p/74101/#74163) I think I've found a somewhat similar situation, and I read the following:&#xD;&#xA;&#xD;&#xA;&gt; Try a design of ~ genotype + genotype:condition&#xD;&#xA;&#xD;&#xA;&gt; Then you will have a condition effect for each level of genotype, including the reference level.&#xD;&#xA;&#xD;&#xA;&gt; You can constrast pairs of them using the list style of the 'contrast' argument.&#xD;&#xA;&#xD;&#xA;However, this doesn't explain how to apply this &quot;list style&quot; to the &quot;contrast&quot; argument. And the above situation seems to be asymmetrical. By that I mean that genotype and condition do not seem to have an interchangeable role.&#xD;&#xA;&#xD;&#xA;So I tried the following more symmetric formula:&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeqDataSetFromMatrix(&#xD;&#xA;                                    countData = counts_data,&#xD;&#xA;                                    colData = col_data,&#xD;&#xA;                                    design = ~ geno + treat + geno:treat)&#xD;&#xA;    &gt; dds &lt;- DESeq(dds)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Now, can I for instance **get the differential expression results when comparing treatment &quot;HS30&quot; against &quot;RT&quot; as a reference, in genotype &quot;prg1&quot;**?&#xD;&#xA;&#xD;&#xA;And how?&#xD;&#xA;&#xD;&#xA;If I understand correctly, the above-mentioned &quot;list style&quot; uses names given by the `resultsNames` function. In my case, I have the following:&#xD;&#xA;&#xD;&#xA;    &gt; resultsNames(dds)&#xD;&#xA;    [1] &quot;Intercept&quot;               &quot;geno_WT_vs_prg1&quot;        &#xD;&#xA;    [3] &quot;treat_HS30RT120_vs_HS30&quot; &quot;treat_RT_vs_HS30&quot;       &#xD;&#xA;    [5] &quot;genoWT.treatHS30RT120&quot;   &quot;genoWT.treatRT&quot;&#xD;&#xA;&#xD;&#xA;I guess I would need a contrast between &quot;genoprg1.treatRT&quot; and a &quot;genoprg1.treatHS30&quot;, but these are not in the above results names.&#xD;&#xA;&#xD;&#xA;I'm lost." />
  <row Id="1206" PostHistoryTypeId="5" PostId="380" RevisionGUID="d13f5579-d9b7-47f8-bf19-f1f9c7c4a896" CreationDate="2017-06-02T16:05:08.813" UserId="292" Comment="added 1 character in body" Text="With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (including gzipped versions), so the above command should also handle fastq format robustly.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters (got it from &lt;https://unix.stackexchange.com/a/169575/55127&gt;). I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)&#xD;&#xA;&#xD;&#xA;### Edit&#xD;&#xA;&#xD;&#xA;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;https://bioinformatics.stackexchange.com/a/365/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &lt; /Genomes/C_elegans/Caenorhabditis_elegans/Ensembl/WBcel235/Sequence/WholeGenomeFasta/genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.976s&#xD;&#xA;    user	0m0.876s&#xD;&#xA;    sys 	0m0.100s&#xD;&#xA;&#xD;&#xA;Comparing with something adapted from this answer: &lt;https://bioinformatics.stackexchange.com/a/363/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&gt;')))&quot; &lt; /Genomes/C_elegans/Caenorhabditis_elegans/Ensembl/WBcel235/Sequence/WholeGenomeFasta/genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.143s&#xD;&#xA;    user	0m1.100s&#xD;&#xA;    sys 	0m0.040s&#xD;&#xA;&#xD;&#xA;(It is slower with python 3.6 than with python 2.7 on my computer)" />
  <row Id="1208" PostHistoryTypeId="2" PostId="393" RevisionGUID="3cedd405-b102-49d8-9792-86697903c483" CreationDate="2017-06-02T16:18:48.537" UserId="292" Text="You mention Biopython, which contains tests: &lt;https://github.com/biopython/biopython/tree/master/Tests&gt;.&#xD;&#xA;&#xD;&#xA;Some of the tests consist in reading files present in the folders listed in the above link. These files could be a starting point for a database of test files. Whenever one comes across a test case not covered with these files, one could construct a new test file and contribute it to Biopython, along with a test, or at least file an issue: &lt;https://github.com/biopython/biopython/issues&gt;.&#xD;&#xA;&#xD;&#xA;That would be a way to contribute to Biopython while constituting a database of test files." />
  <row Id="1209" PostHistoryTypeId="2" PostId="394" RevisionGUID="53fdde39-2efc-435f-b6cc-64f9c752cb21" CreationDate="2017-06-02T17:08:55.260" UserId="47" Text="As far as I know, there is no single repository that collects all of the common data formats used in bioinformatics.  Typically, you have to go to the source to find the specifications for each format.  There are a few places that collect descriptions of file formats, though:&#xD;&#xA;&#xD;&#xA;- [IGV File Formats](https://software.broadinstitute.org/software/igv/FileFormats), coveres all of the formats usable in the Broad Institute's Integrative Genomics Viewer software (which is a lot).&#xD;&#xA;- [NCI File Formats](https://wiki.nci.nih.gov/display/TCGA/File+Format+Specifications), mostly formats used by TGCA (including MAF and VCF).&#xD;&#xA;- [UCSC Genomics](https://cgwb.nci.nih.gov/FAQ/FAQformat.html), covers BED, MAF, and a few others.&#xD;&#xA;- [GenePattern](http://software.broadinstitute.org/cancer/software/genepattern/file-formats-guide), covers many of the file formats related to microarray data.&#xD;&#xA;- [GSEA](http://software.broadinstitute.org/cancer/software/gsea/wiki/index.php/Data_formats), Broad Gene Set Enrichment Analysis documentation." />
  <row Id="1210" PostHistoryTypeId="5" PostId="388" RevisionGUID="6c5ace34-8f8b-40bd-bfef-75a98894bb23" CreationDate="2017-06-02T18:05:26.423" UserId="104" Comment="added 381 characters in body" Text="I want some templates of different file formats that I can use to test my scripts and identify possible bugs in my code.&#xD;&#xA;&#xD;&#xA;For example, consider FASTA, a simple but often abused format, I would want templates to capture regular and irregular formats, like I have seen all of these:&#xD;&#xA;&#xD;&#xA;1) Single line sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    ATG&#xD;&#xA;&#xD;&#xA;2) Multi-line sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    AT&#xD;&#xA;    G&#xD;&#xA;&#xD;&#xA;3) Upper and lower case letters in sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    Atg&#xD;&#xA;&#xD;&#xA;4) Ns and Xs in sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    ANnxX&#xD;&#xA;&#xD;&#xA;5) Unusual headers (sometimes even non-ASCI characters in headers)&#xD;&#xA;&#xD;&#xA;    &gt;ATG | &gt;Name @species | (MULTI_SPECIES)[taxa]&#xD;&#xA;    ATG&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;etc.&#xD;&#xA;&#xD;&#xA;There should be separate templates for nucleotide and protein FASTA, and separate ones for aligned FASTA.&#xD;&#xA;&#xD;&#xA;It would ideally include other aspects too, like different compression formats (such as `.gz`, `.bzip2`) and different file extensions (such as `.fa`, `.fasta`).&#xD;&#xA;&#xD;&#xA;I have never seen resources that provide templates covering these, but I think it would be useful. Of course I could build my own templates but it would take time to capture all the likely variations of the formats, particularly for more complex file formats.&#xD;&#xA;&#xD;&#xA;Note, I am not just interested in FASTA format, it was an example.&#xD;&#xA;&#xD;&#xA;Also note, I know about tools (such as `BioPython`) that should handle many formats well, but they may also have bugs. Anyway, in practice sometimes I end up parsing files myself directly because I don't want the overhead or dependency of an external package.&#xD;&#xA;&#xD;&#xA;*EDIT: Please don't answer this question to say you don't know of any such resources, me neither, hence the question. [bli's helpful answer][1] shows that there is at least one test suite that could be used as a starting point. I know that it is normally easy to look up the specification of any particular file format.*&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/393/104" />
  <row Id="1211" PostHistoryTypeId="5" PostId="291" RevisionGUID="0899f431-e870-4113-acda-3872c9c0d903" CreationDate="2017-06-02T18:39:54.083" UserId="57" Comment="added location of descrition with ackowledgement that maybe it is not complete" Text="Yes, there bwa-mem was published as a [preprint][1]&#xD;&#xA;&#xD;&#xA;&gt; BWA-MEM’s seed extension differs from the standard seed extension in two aspects. Firstly, suppose at a certain extension step we come to reference&#xD;&#xA;position x with the best extension score achieved at query position y.&#xD;&#xA;&#xD;&#xA;&gt; ...&#xD;&#xA;&#xD;&#xA;&gt; Secondly, while extending a seed, BWA-MEM tries to keep track of the&#xD;&#xA;best extension score reaching the end of the query sequence&#xD;&#xA;&#xD;&#xA;And there is a description of the scoring algorithm directly in the source code of [bwa-mem][2] (lines 22 - 44), but maybe the only solution is really to go though the source code.&#xD;&#xA;&#xD;&#xA;  [1]: https://arxiv.org/pdf/1303.3997.pdf&#xD;&#xA;  [2]: https://github.com/lh3/bwa/blob/master/bwamem.c" />
  <row Id="1212" PostHistoryTypeId="2" PostId="395" RevisionGUID="79dc68ef-d420-45ab-9803-84d2de259082" CreationDate="2017-06-02T19:22:34.750" UserId="467" Text="In the analyses of scRNA-seq data there are different unsupervised approaches to identify putative subpopulations (e.g. as available with Suerat or SCDE packages). Is there a good way of computationally validating the cluster solutions? Different methods may results in slightly different clustering results. How to know which one is the best i.e.representative of biological sub-populations?" />
  <row Id="1213" PostHistoryTypeId="1" PostId="395" RevisionGUID="79dc68ef-d420-45ab-9803-84d2de259082" CreationDate="2017-06-02T19:22:34.750" UserId="467" Text="validating identified sub-populations of cells in scRNA-seq" />
  <row Id="1214" PostHistoryTypeId="3" PostId="395" RevisionGUID="79dc68ef-d420-45ab-9803-84d2de259082" CreationDate="2017-06-02T19:22:34.750" UserId="467" Text="&lt;scrnaseq&gt;&lt;clustering&gt;&lt;unsupervised&gt;" />
  <row Id="1215" PostHistoryTypeId="2" PostId="396" RevisionGUID="910027cf-c38a-456d-8348-be13ae0ac0a8" CreationDate="2017-06-02T19:36:07.733" UserId="272" Text="I have indexed a gzipped reference with bwa: `bwa index reference.fa.gz`, which produces a series of other files `reference.fa.gz.{amb,ann,bwt,pac,sa}`. These are working fine with bwa alignment.&#xD;&#xA;&#xD;&#xA;I have discovered that samtools does not take a gzipped reference, so I am planning to use an unzipped version of the reference for my workflow instead of dealing with two separate representations of the reference. &#xD;&#xA;&#xD;&#xA;Are the files `reference.fa.gz.{amb,ann,bwt,pac,sa}` dependent upon the reference being gzipped? Do I need to reindex the unzipped reference.fa with bwa, or can I just rename the current files to remove the `.gz` portion of the filename? " />
  <row Id="1216" PostHistoryTypeId="1" PostId="396" RevisionGUID="910027cf-c38a-456d-8348-be13ae0ac0a8" CreationDate="2017-06-02T19:36:07.733" UserId="272" Text="What are all the reference files produced by bwa index, and are these dependent upon whether the reference is zipped?" />
  <row Id="1217" PostHistoryTypeId="3" PostId="396" RevisionGUID="910027cf-c38a-456d-8348-be13ae0ac0a8" CreationDate="2017-06-02T19:36:07.733" UserId="272" Text="&lt;samtools&gt;&lt;reference&gt;&lt;bwa&gt;" />
  <row Id="1218" PostHistoryTypeId="5" PostId="395" RevisionGUID="a4619387-071d-490b-a0ce-217b6e359a5b" CreationDate="2017-06-02T19:37:06.107" UserId="57" Comment="scRNA is abiguitous abreveation &gt; single-cell RNA to make it more clear" Text="In the analyses of single-cell RNA-seq data there are different unsupervised approaches to identify putative subpopulations (e.g. as available with Suerat or SCDE packages). &#xD;&#xA;&#xD;&#xA;Is there a good way of computationally validating the cluster solutions? Different methods may results in slightly different clustering results. How to know which one is the best i.e. representative of biological sub-populations?" />
  <row Id="1219" PostHistoryTypeId="2" PostId="397" RevisionGUID="96fa3172-fc7d-41ca-8cdd-39b58d1389a8" CreationDate="2017-06-02T19:44:00.670" UserId="375" Text="Not that I am aware. It is best to go with formats specifications when coding.&#xD;&#xA;&#xD;&#xA;Also it may be good to look at the example files that come together with various tools performing file conversions and handling. E.g. &#xD;&#xA;&#xD;&#xA; - Trimmomatic comes with few .fa files (&lt;http://www.usadellab.org/cms/?page=trimmomatic&gt;)&#xD;&#xA; - Samtools with a toy.sam (&lt;https://github.com/lh3/samtools-legacy/blob/master/examples/toy.sam&gt;)" />
  <row Id="1220" PostHistoryTypeId="2" PostId="398" RevisionGUID="ae4ee13b-473c-42b7-9fd5-5f778b1e97d8" CreationDate="2017-06-02T19:46:16.340" UserId="77" Text="You'll get the exact same index (the `amb`, `ann`, `bwt`, `pac` and `sa` files) whether the reference is gzipped or not. BWA also makes its own packed reference sequence (the .pac file) so you don't even need the genome around after you index." />
  <row Id="1221" PostHistoryTypeId="2" PostId="399" RevisionGUID="13624aca-eb7e-43e7-9dc1-98122f52b3dc" CreationDate="2017-06-02T19:50:21.880" UserId="375" Text="A SC3, single-cell consensus clustering, approach could be helpful here. It aims at achieving &quot;high accuracy and robustness by combining multiple clustering solutions through a consensus approach&quot; &lt;https://www.nature.com/nmeth/journal/v14/n5/full/nmeth.4236.html&gt;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1222" PostHistoryTypeId="5" PostId="397" RevisionGUID="226a7326-5b52-40be-b4f1-b26d1144947a" CreationDate="2017-06-02T19:58:28.837" UserId="77" Comment="Spelling fix" Text="Not that I am aware. It is best to go with format specifications when coding.&#xD;&#xA;&#xD;&#xA;Also it may be good to look at the example files that come together with various tools performing file conversions and handling. E.g. &#xD;&#xA;&#xD;&#xA; - Trimmomatic comes with few .fa files (&lt;http://www.usadellab.org/cms/?page=trimmomatic&gt;)&#xD;&#xA; - Samtools with a toy.sam (&lt;https://github.com/lh3/samtools-legacy/blob/master/examples/toy.sam&gt;)" />
  <row Id="1223" PostHistoryTypeId="2" PostId="400" RevisionGUID="3f1d7c3b-3e7a-4eb4-8f1e-d5688e426d9d" CreationDate="2017-06-02T20:28:26.517" UserId="532" Text="If we take real world gzipped FASTQ (which as the OP suggested would be beneficial) rather than trivial FASTA as the starting point then the real issue is actually decompressing the file not counting the Ns and in this case the C program count-N is no longer the fastest solution.&#xD;&#xA;&#xD;&#xA;Additionally it would good to use the a specific file for benchmarking which actually has Ns, because you'll get some quite interesting execution time differences with some methods counting the more frequently occurring As rather than Ns.&#xD;&#xA;&#xD;&#xA;One such file is:&#xD;&#xA;&#xD;&#xA;http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/SRR077487_2.filt.fastq.gz&#xD;&#xA;&#xD;&#xA;It's also worth checking the various solutions return the correct answer there should be 306072 Ns in the above file.&#xD;&#xA;&#xD;&#xA;Next note that decompression of this file redirected to `/dev/null` is slower with `zcat` and `gzip` (which are both gzip 1.6 on my system) than say a parallel implementation of gzip like [Mark Adler's][1] `pigz`, which appears to use 4 threads for decompression.  All timings represent an average of 10 runs reporting real (wall clock time).&#xD;&#xA;&#xD;&#xA;    time pigz -dc SRR077487_2.filt.fastq.gz &gt; /dev/null&#xD;&#xA;        &#xD;&#xA;    real    0m29.0132s&#xD;&#xA;&#xD;&#xA;    time gzip -dc SRR077487_2.filt.fastq.gz &gt; /dev/null&#xD;&#xA;    &#xD;&#xA;    real    0m40.6996s&#xD;&#xA;&#xD;&#xA;There is an ~11.7 second difference between the two.  Next if I then try to benchmark a one-liner which performs on FASTQ and gives the correct answer (Note I've yet to encounter FASTQ which is not 4 line, and seriously who generates these files!)&#xD;&#xA;&#xD;&#xA;    time pigz -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | tr -cd N | wc -c&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    0m34.793s&#xD;&#xA;&#xD;&#xA;As you can see the counting adds ~5.8 second to the total run time versus `pigz` based decompression.  Additionally this time delta is higher when using `gzip` ~6.7 seconds above gzip decompression alone.   &#xD;&#xA;&#xD;&#xA;    time gzip -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | tr -cd N | wc -c                                                                &#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    0m44.399s&#xD;&#xA;&#xD;&#xA;The `pigz awk tr wc` based solution is however ~4.5 seconds faster than the `count-N` based C code solution:&#xD;&#xA;&#xD;&#xA;    time count-N SRR077487_2.filt.fastq.gz &#xD;&#xA;    2385855128      306072  0&#xD;&#xA;    &#xD;&#xA;    real    0m39.266s&#xD;&#xA;&#xD;&#xA;This difference appears to be robust to re-running as many times as I like.  I expect if you could use pthread in the C based solution or alter it to take the standard out from pigz it would also show an increase in performance.&#xD;&#xA;&#xD;&#xA;Benchmarking another alternative `pigz grep` variant appears to take more or less the same time as the `tr` based variant:&#xD;&#xA;&#xD;&#xA;    time pigz -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | grep -o N | wc -l&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    0m34.869s&#xD;&#xA;&#xD;&#xA;Note that the `seqtk` based solution discussed above is noticeably slower,&#xD;&#xA;&#xD;&#xA;    time seqtk comp  SRR077487_2.filt.fastq.gz | awk '{x+=$9}END{print x}'&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    1m42.062s &#xD;&#xA;&#xD;&#xA;However it's worth noting `seqtk comp` is doing a bit more than the other solutions.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://zlib.net/pigz/" />
  <row Id="1224" PostHistoryTypeId="5" PostId="400" RevisionGUID="689e1fd1-ff9b-4e94-921b-e8ac6ce18fbc" CreationDate="2017-06-02T20:33:33.187" UserId="532" Comment="deleted 2 characters in body" Text="If we take real world gzipped FASTQ (which as the OP suggested would be beneficial) rather than trivial FASTA as the starting point then the real issue is actually decompressing the file not counting the Ns and in this case the C program `count-N` is no longer the fastest solution.&#xD;&#xA;&#xD;&#xA;Additionally it would good to use a specific file for benchmarking which actually has Ns, because you'll get some quite interesting execution time differences with some methods counting the more frequently occurring As rather than Ns.&#xD;&#xA;&#xD;&#xA;One such file is:&#xD;&#xA;&#xD;&#xA;http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/SRR077487_2.filt.fastq.gz&#xD;&#xA;&#xD;&#xA;It's also worth checking the various solutions return the correct answer there should be 306072 Ns in the above file.&#xD;&#xA;&#xD;&#xA;Next note that decompression of this file redirected to `/dev/null` is slower with `zcat` and `gzip` (which are both gzip 1.6 on my system) than say a parallel implementation of gzip like [Mark Adler's][1] `pigz`, which appears to use 4 threads for decompression.  All timings represent an average of 10 runs reporting real (wall clock time).&#xD;&#xA;&#xD;&#xA;    time pigz -dc SRR077487_2.filt.fastq.gz &gt; /dev/null&#xD;&#xA;        &#xD;&#xA;    real    0m29.0132s&#xD;&#xA;&#xD;&#xA;    time gzip -dc SRR077487_2.filt.fastq.gz &gt; /dev/null&#xD;&#xA;    &#xD;&#xA;    real    0m40.6996s&#xD;&#xA;&#xD;&#xA;There is an ~11.7 second difference between the two.  Next if I then try to benchmark a one-liner which performs on FASTQ and gives the correct answer (Note I've yet to encounter FASTQ which is not 4 line, and seriously who generates these files!)&#xD;&#xA;&#xD;&#xA;    time pigz -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | tr -cd N | wc -c&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    0m34.793s&#xD;&#xA;&#xD;&#xA;As you can see the counting adds ~5.8 second to the total run time versus `pigz` based decompression.  Additionally this time delta is higher when using `gzip` ~6.7 seconds above gzip decompression alone.   &#xD;&#xA;&#xD;&#xA;    time gzip -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | tr -cd N | wc -c                                                                &#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    0m44.399s&#xD;&#xA;&#xD;&#xA;The `pigz awk tr wc` based solution is however ~4.5 seconds faster than the `count-N` based C code solution:&#xD;&#xA;&#xD;&#xA;    time count-N SRR077487_2.filt.fastq.gz &#xD;&#xA;    2385855128      306072  0&#xD;&#xA;    &#xD;&#xA;    real    0m39.266s&#xD;&#xA;&#xD;&#xA;This difference appears to be robust to re-running as many times as I like.  I expect if you could use pthread in the C based solution or alter it to take the standard out from pigz it would also show an increase in performance.&#xD;&#xA;&#xD;&#xA;Benchmarking another alternative `pigz grep` variant appears to take more or less the same time as the `tr` based variant:&#xD;&#xA;&#xD;&#xA;    time pigz -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | grep -o N | wc -l&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    0m34.869s&#xD;&#xA;&#xD;&#xA;Note that the `seqtk` based solution discussed above is noticeably slower,&#xD;&#xA;&#xD;&#xA;    time seqtk comp  SRR077487_2.filt.fastq.gz | awk '{x+=$9}END{print x}'&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    1m42.062s &#xD;&#xA;&#xD;&#xA;However it's worth noting `seqtk comp` is doing a bit more than the other solutions.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://zlib.net/pigz/" />
  <row Id="1225" PostHistoryTypeId="5" PostId="400" RevisionGUID="470cd5b2-ac9a-463e-8719-edd8605d80a0" CreationDate="2017-06-02T20:57:56.203" UserId="532" Comment="missing &quot;be&quot;" Text="### Decompression of gzipped FASTQ is the main issue ###&#xD;&#xA;&#xD;&#xA;If we take real world gzipped FASTQ (which as the OP suggested would be beneficial) rather than trivial FASTA as the starting point then the real issue is actually decompressing the file not counting the Ns and in this case the C program `count-N` is no longer the fastest solution.&#xD;&#xA;&#xD;&#xA;Additionally it would be good to use a specific file for benchmarking which actually has Ns, because you'll get some quite interesting execution time differences with some methods counting the more frequently occurring As rather than Ns.&#xD;&#xA;&#xD;&#xA;One such file is:&#xD;&#xA;&#xD;&#xA;http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/SRR077487_2.filt.fastq.gz&#xD;&#xA;&#xD;&#xA;It's also worth checking the various solutions return the correct answer there should be 306072 Ns in the above file.&#xD;&#xA;&#xD;&#xA;Next note that decompression of this file redirected to `/dev/null` is slower with `zcat` and `gzip` (which are both gzip 1.6 on my system) than say a parallel implementation of gzip like [Mark Adler's][1] `pigz`, which appears to use 4 threads for decompression.  All timings represent an average of 10 runs reporting real (wall clock time).&#xD;&#xA;&#xD;&#xA;    time pigz -dc SRR077487_2.filt.fastq.gz &gt; /dev/null&#xD;&#xA;        &#xD;&#xA;    real    0m29.0132s&#xD;&#xA;&#xD;&#xA;    time gzip -dc SRR077487_2.filt.fastq.gz &gt; /dev/null&#xD;&#xA;    &#xD;&#xA;    real    0m40.6996s&#xD;&#xA;&#xD;&#xA;There is an ~11.7 second difference between the two.  Next if I then try to benchmark a one-liner which performs on FASTQ and gives the correct answer (Note I've yet to encounter FASTQ which is not 4 line, and seriously who generates these files!)&#xD;&#xA;&#xD;&#xA;    time pigz -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | tr -cd N | wc -c&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    0m34.793s&#xD;&#xA;&#xD;&#xA;As you can see the counting adds ~5.8 second to the total run time versus `pigz` based decompression.  Additionally this time delta is higher when using `gzip` ~6.7 seconds above gzip decompression alone.   &#xD;&#xA;&#xD;&#xA;    time gzip -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | tr -cd N | wc -c                                                                &#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    0m44.399s&#xD;&#xA;&#xD;&#xA;The `pigz awk tr wc` based solution is however ~4.5 seconds faster than the `count-N` based C code solution:&#xD;&#xA;&#xD;&#xA;    time count-N SRR077487_2.filt.fastq.gz &#xD;&#xA;    2385855128      306072  0&#xD;&#xA;    &#xD;&#xA;    real    0m39.266s&#xD;&#xA;&#xD;&#xA;This difference appears to be robust to re-running as many times as I like.  I expect if you could use pthread in the C based solution or alter it to take the standard out from pigz it would also show an increase in performance.&#xD;&#xA;&#xD;&#xA;Benchmarking another alternative `pigz grep` variant appears to take more or less the same time as the `tr` based variant:&#xD;&#xA;&#xD;&#xA;    time pigz -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | grep -o N | wc -l&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    0m34.869s&#xD;&#xA;&#xD;&#xA;Note that the `seqtk` based solution discussed above is noticeably slower,&#xD;&#xA;&#xD;&#xA;    time seqtk comp  SRR077487_2.filt.fastq.gz | awk '{x+=$9}END{print x}'&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real    1m42.062s &#xD;&#xA;&#xD;&#xA;However it's worth noting `seqtk comp` is doing a bit more than the other solutions.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://zlib.net/pigz/" />
  <row Id="1226" PostHistoryTypeId="2" PostId="401" RevisionGUID="500bf21c-5275-4d5b-8167-788dfd0c7f80" CreationDate="2017-06-02T23:38:13.547" UserId="549" Text="I have a BAM created by Picard. I want to filter alignments by flags with `samtools view`. However, I noticed that even if I apply no filters, the output BAM is different from my input BAM. Are BAMs produced by different tools also different in size? How can I check if two BAMs are the same?" />
  <row Id="1227" PostHistoryTypeId="1" PostId="401" RevisionGUID="500bf21c-5275-4d5b-8167-788dfd0c7f80" CreationDate="2017-06-02T23:38:13.547" UserId="549" Text="Why two BAMs created by different tools are different in size?" />
  <row Id="1228" PostHistoryTypeId="3" PostId="401" RevisionGUID="500bf21c-5275-4d5b-8167-788dfd0c7f80" CreationDate="2017-06-02T23:38:13.547" UserId="549" Text="&lt;samtools&gt;" />
  <row Id="1229" PostHistoryTypeId="2" PostId="402" RevisionGUID="39ac27ac-f123-4921-97cd-d3cb61486606" CreationDate="2017-06-02T23:44:44.637" UserId="549" Text="I know how to downsample a BAM file to lower coverage. I know I can randomly select lines in SAM, but this procedure can't guarantee two reads in a pair are always sampled the same time. Is there a way to downsample BAM while keeping pairing information intact?" />
  <row Id="1230" PostHistoryTypeId="1" PostId="402" RevisionGUID="39ac27ac-f123-4921-97cd-d3cb61486606" CreationDate="2017-06-02T23:44:44.637" UserId="549" Text="How can I downsample a BAM file while keeping both reads in pairs?" />
  <row Id="1231" PostHistoryTypeId="3" PostId="402" RevisionGUID="39ac27ac-f123-4921-97cd-d3cb61486606" CreationDate="2017-06-02T23:44:44.637" UserId="549" Text="&lt;bam&gt;" />
  <row Id="1232" PostHistoryTypeId="2" PostId="403" RevisionGUID="5a4e02ea-ea21-456a-a411-81dad5705bc2" CreationDate="2017-06-02T23:50:11.167" UserId="549" Text="After some google searches, I found multiple tools with overlapping functionality for viewing, merging, pileuping, etc. I have not got time to try these tools, so will just see if anyone already know the answer: what is the difference between them? Performance? Features? Or something else? Which one is generally preferred? Samtools?" />
  <row Id="1233" PostHistoryTypeId="1" PostId="403" RevisionGUID="5a4e02ea-ea21-456a-a411-81dad5705bc2" CreationDate="2017-06-02T23:50:11.167" UserId="549" Text="What is the difference between samtools, bamtools, picard, sambamba and biobambam?" />
  <row Id="1234" PostHistoryTypeId="3" PostId="403" RevisionGUID="5a4e02ea-ea21-456a-a411-81dad5705bc2" CreationDate="2017-06-02T23:50:11.167" UserId="549" Text="&lt;bam&gt;" />
  <row Id="1235" PostHistoryTypeId="6" PostId="401" RevisionGUID="3cc344b7-7250-4d0f-a201-1f03e9b162c5" CreationDate="2017-06-02T23:50:32.717" UserId="549" Comment="Changed the tag" Text="&lt;bam&gt;" />
  <row Id="1236" PostHistoryTypeId="2" PostId="404" RevisionGUID="dae19826-0ecc-4d61-9940-3f6d49444d27" CreationDate="2017-06-02T23:58:10.703" UserId="549" Text="As I read the SAM spec, the &quot;X&quot; CIGAR operator represents a mismatch. This seems useful as we can know where are the mismatches without looking at the reference genome. However, many popular aligners such as BWA do not output &quot;X&quot;. Why do they omit &quot;X&quot;?" />
  <row Id="1237" PostHistoryTypeId="1" PostId="404" RevisionGUID="dae19826-0ecc-4d61-9940-3f6d49444d27" CreationDate="2017-06-02T23:58:10.703" UserId="549" Text="Why most aligners do not output the &quot;X&quot; CIGAR operation?" />
  <row Id="1238" PostHistoryTypeId="3" PostId="404" RevisionGUID="dae19826-0ecc-4d61-9940-3f6d49444d27" CreationDate="2017-06-02T23:58:10.703" UserId="549" Text="&lt;bwa&gt;&lt;sam&gt;" />
  <row Id="1239" PostHistoryTypeId="2" PostId="405" RevisionGUID="f821b97b-7f73-4d36-a23e-26de694c04bc" CreationDate="2017-06-03T00:20:02.360" UserId="73" Text="It's unlikely that two different mapping tools will give exactly the same alignment, scores, and match strings for the same sequence mapped to the same reference. For some sequence/reference alignments, it's impossible to determine which is the &quot;best&quot; alignment, and slight differences in code can have large effects on the chosen alignment.&#xD;&#xA;&#xD;&#xA;However, even if the actual mapping location and match string are exactly the same (for example when using a tool like Picard to filter BAM/SAM files), different tools will incorporate different metadata with each mapping. This is allowed in the [SAM file format specification][1] by the addition of optional fields beyond the 11th column. There are a few standard optional tags that can be used in these fields, and additional custom non-standard tags can be used as well. It is very likely that Picard is adding additional metadata to the alignments in the BAM/SAM file.&#xD;&#xA;&#xD;&#xA;There's an additional complication in that the underlying *SAM* alignment (and metadata) could be identical, but the *BAM* file can still have different file sizes. One reason for this is that BAM file compression methods can be changed. For example, alignment tools might choose a quick compression method, while filtering tools might choose a method that results in better compression.&#xD;&#xA;&#xD;&#xA;Checking for alignment similarity is more difficult than just comparing files at a binary level, and your particular application (or context, or story) will change what the best method of comparison is. It would be helpful to know why you want to compare BAM files in order to provide a better answer to your question.&#xD;&#xA;&#xD;&#xA;  [1]: https://samtools.github.io/hts-specs/SAMv1.pdf" />
  <row Id="1240" PostHistoryTypeId="11" PostId="328" RevisionGUID="905307b1-420f-4872-adf5-f3a4fac3665f" CreationDate="2017-06-03T00:44:31.203" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:29,&quot;DisplayName&quot;:&quot;Konrad Rudolph&quot;},{&quot;Id&quot;:57,&quot;DisplayName&quot;:&quot;Kamil S Jaron&quot;},{&quot;Id&quot;:37,&quot;DisplayName&quot;:&quot;user172818&quot;},{&quot;Id&quot;:292,&quot;DisplayName&quot;:&quot;bli&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;Franck Dernoncourt&quot;}]}" />
  <row Id="1241" PostHistoryTypeId="11" PostId="327" RevisionGUID="d7b9597c-eec7-4371-9908-ec6abaccd7e1" CreationDate="2017-06-03T00:44:52.537" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:37,&quot;DisplayName&quot;:&quot;user172818&quot;},{&quot;Id&quot;:35,&quot;DisplayName&quot;:&quot;burger&quot;},{&quot;Id&quot;:131,&quot;DisplayName&quot;:&quot;zx8754&quot;},{&quot;Id&quot;:29,&quot;DisplayName&quot;:&quot;Konrad Rudolph&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;Franck Dernoncourt&quot;}]}" />
  <row Id="1242" PostHistoryTypeId="2" PostId="406" RevisionGUID="4ceca3b3-df5f-44b5-b3e7-05655eb12eea" CreationDate="2017-06-03T01:08:48.127" UserId="161" Text="[samtools](http://www.htslib.org/doc/samtools.html) has a subsampling option:&#xD;&#xA;&#xD;&#xA;&gt; -s FLOAT: &#xD;&#xA;Integer part is used to seed the random number generator [0]. Part after the decimal point sets the fraction of templates/pairs to subsample [no subsampling]&#xD;&#xA;&#xD;&#xA;    samtools view -bs 42.1 in.bam &gt; subsampled.bam&#xD;&#xA;&#xD;&#xA;will subsample 10 percent mapped reads with 42 as the seed for the random number generator." />
  <row Id="1243" PostHistoryTypeId="2" PostId="407" RevisionGUID="6fc383a7-1a5e-4167-b684-29ad85525fcf" CreationDate="2017-06-03T01:22:37.273" UserId="549" Text="I am calling SNPs from WGS samples produced at my lab. I am currently using bwa-mem for mapping Illumina reads as it is recommended by GATK best practice. However, bwa is a bit slow. I heard from my colleague that [SNAP](https://github.com/amplab/snap) is much faster than bwa. I tried it on a small set of reads and it is indeed faster. However, I am not sure how it works with downstream SNP callers, so here are my questions: have you used SNAP for short-read mapping? What is your experience? Does SNAP work well with SNP callers like GATK and freebayes? Thanks!" />
  <row Id="1244" PostHistoryTypeId="1" PostId="407" RevisionGUID="6fc383a7-1a5e-4167-b684-29ad85525fcf" CreationDate="2017-06-03T01:22:37.273" UserId="549" Text="Do you use SNAP for short-read mapping?" />
  <row Id="1245" PostHistoryTypeId="3" PostId="407" RevisionGUID="6fc383a7-1a5e-4167-b684-29ad85525fcf" CreationDate="2017-06-03T01:22:37.273" UserId="549" Text="&lt;bwa&gt;&lt;snap&gt;" />
  <row Id="1246" PostHistoryTypeId="2" PostId="408" RevisionGUID="f35f6e8d-5345-4164-b78b-a58c78a15887" CreationDate="2017-06-03T01:50:55.283" UserId="73" Text="The obvious answer is that different people wrote them. It's fairly common in bioinformatics for people with a computer science background to get frustrated with existing tools and create their own alternative tool (rather than improving an existing tool). Over time, tools with similar initial aims will have popular functionality implemented in them (and eventually have bugs fixed), such that it matters less which particular tool is used for common methods.&#xD;&#xA;&#xD;&#xA;Here's my impression of the tools:&#xD;&#xA;&#xD;&#xA;1. [samtools][1] -- originally written by Heng Li (who also wrote BWA). The people who now work on samtools also maintain the alignment file format specification for [SAM, BAM, and CRAM][2], so any new file format features are likely to be implemented in samtools first.&#xD;&#xA;&#xD;&#xA;2. [bamtools][3] -- this looks like it was written by Derek Barnett, Erik Garrison, Gabor Marth, Michael Stromberg to mirror the samtools toolkit, but using C++ instead of C&#xD;&#xA;&#xD;&#xA;3. [picard][4] -- Java tools written by the Broad Institute for manipulating BAM/SAM files. Being written in Java makes it easier to port to other operating systems, so it may work better on Windows systems. I'm more familiar with picard being used at a filtering level (e.g. removing PCR duplicates), and for statistical analysis, but it links in with the Java HTS library from samtools, so probably shares a lot of the functionality.&#xD;&#xA;&#xD;&#xA;4. [sambamba][5] -- a GPL2-licensed toolkit written in the D programming language (presumably by Artem Tarasov and Pjotr Prins). I haven't used it (and don't know people who have used it), but the github page suggests &quot;For almost 5 years the main advantage over samtools was parallelized BAM reading. Finally in March 2017 samtools 1.4 was released, reaching parity on this.&quot;&#xD;&#xA;&#xD;&#xA;5. [biobambam][6] -- written by German Tischler in C++. I also have no experience with this toolkit. This seems to have some multithreading capability, but is otherwise similar to other toolkits.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/samtools/&#xD;&#xA;  [2]: https://samtools.github.io/hts-specs/SAMv1.pdf&#xD;&#xA;  [3]: https://github.com/pezmaster31/bamtools&#xD;&#xA;  [4]: http://broadinstitute.github.io/picard/&#xD;&#xA;  [5]: https://github.com/lomereiter/sambamba&#xD;&#xA;  [6]: https://github.com/gt1/biobambam2" />
  <row Id="1247" PostHistoryTypeId="4" PostId="401" RevisionGUID="1145a71d-b7a2-44a2-bf28-e56e7d448ed7" CreationDate="2017-06-03T02:05:48.400" UserId="73" Comment="fixed grammar, clarified question" Text="Why do BAM files created by different tools have different file sizes?" />
  <row Id="1248" PostHistoryTypeId="2" PostId="409" RevisionGUID="0cb363fe-5f77-4b86-ac1e-6486aa3eb816" CreationDate="2017-06-03T09:06:32.320" UserId="224" Text="We routinely run both [FastQC](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and [FastQ Screen](http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/) on all of our raw sequencing reads. FastQ Screen is a tool for detecting cross-species contamination. [MGA](https://github.com/crukci-bioinformatics/MGA) is another similar tool.&#xD;&#xA;&#xD;&#xA;There are then lots of QC tools specific to different types of data, most of which run after alignment. For example [RSeQC](http://rseqc.sourceforge.net/) (RNA data), [Qualimap](http://qualimap.bioinfo.cipf.es/) and many many others. Without specifying what kind of data you have this is a bit difficult to make recommendations for though.&#xD;&#xA;&#xD;&#xA;Phil" />
  <row Id="1249" PostHistoryTypeId="5" PostId="301" RevisionGUID="039c1db8-7fab-4d5f-929f-ccf61aaaf9cc" CreationDate="2017-06-03T09:09:53.957" UserId="292" Comment="Tried to improve the description of the method." Text="If you happen to know a sequence that should be highly abundant in the library, you can grep its beginning or end (with pattern match highlighting) and see if the same sequence systematically comes just before or just after respectively. This kind of visual inspection can help you finding the adaptor.&#xD;&#xA;&#xD;&#xA;For instance, in a previous lab, we were working on *D. melanogaster* small RNA sequencing data and my colleague knew from previous experience with this kind of data that the following small RNA was likely to be abundant: &lt;http://flybase.org/reports/FBgn0065042.html&gt;&#xD;&#xA;&#xD;&#xA;We just had to grep it in the fastq file to see many lines with this sequence, next to another sequence that happened to be always the same: the unknown adapter." />
  <row Id="1250" PostHistoryTypeId="5" PostId="270" RevisionGUID="5301ed07-0196-40d6-8f79-379c8d75758b" CreationDate="2017-06-03T09:46:11.493" UserId="57" Comment="I added the solution from comment" Text="The &quot;my own parser&quot; solutions. The information I was searching for in part of column `INFO`, namely in variables `SVLEN` and `SVTYPE`.&#xD;&#xA;&#xD;&#xA;very fast SV types + counts (by @user172818 in commnent) :&#xD;&#xA;&#xD;&#xA;    zcat var.vcf.gz | perl -ne 'print &quot;$1\n&quot; if /[;\t]SVTYPE=([^;\t]+)/' | sort | uniq -c&#xD;&#xA;&#xD;&#xA;quite slow SV types + counts + sizes :&#xD;&#xA;&#xD;&#xA;    SV_colnames &lt;- c('CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO', 'FORMAT', 'SAMPLE1')&#xD;&#xA;&#xD;&#xA;    ssplit &lt;- function(s, split = '='){&#xD;&#xA;        unlist(strsplit(s, split = split))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;    # note, capital letters just respect original naming conventions of the VCF file&#xD;&#xA;    getSVTYPE &lt;- function(info){&#xD;&#xA;        ssplit(grep(&quot;SVTYPE&quot;, info, value = T))[2]&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    getSVLEN &lt;- function(info){&#xD;&#xA;        SVLEN &lt;- ssplit(grep(&quot;SVLEN&quot;, info, value = T))&#xD;&#xA;        ifelse(length(SVLEN) == 0, NA, as.numeric(SVLEN[2]))&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    load_sv &lt;- function(file){&#xD;&#xA;        vcf_sv_table &lt;- read.table(file, stringsAsFactors = F)&#xD;&#xA;        colnames(vcf_sv_table) &lt;- SV_colnames&#xD;&#xA;        # possible filtering&#xD;&#xA;        # vcf_sv_table &lt;- vcf_sv_table[vcf_sv_table$FILTER == 'PASS',]&#xD;&#xA;        vcf_sv_table_info &lt;- strsplit(vcf_sv_table$INFO, ';')&#xD;&#xA;        vcf_sv_table$SVTYPE &lt;- unlist(lapply(vcf_sv_table_info, getSVTYPE))&#xD;&#xA;        vcf_sv_table$SVLEN &lt;- unlist(lapply(vcf_sv_table_info, getSVLEN))&#xD;&#xA;        return(vcf_sv_table)&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;    sv_df &lt;- load_sv('my_sv_calls.vcf')&#xD;&#xA;    table(sv_df$SVTYPE)" />
  <row Id="1251" PostHistoryTypeId="5" PostId="264" RevisionGUID="a0d69cd3-56f0-4790-b53a-1920ca0a6901" CreationDate="2017-06-03T11:03:39.833" UserId="57" Comment="I explained why other answers are not really working" Text="I got a bunch of vcf files (v4.1) with structural variations of bunch of non-model organisms (i.e. there are no known variants). I found there are quite a some tools to manipulate vcf files like [VCFtools][1], R package [vcfR][2] or python library [PyVCF][3]. However none of them seems to provide a quick summary, something like (preferably categorised by size as well):&#xD;&#xA;&#xD;&#xA;    type    count&#xD;&#xA;    DEL     x&#xD;&#xA;    INS     y&#xD;&#xA;    INV     z&#xD;&#xA;    ....&#xD;&#xA;&#xD;&#xA;Is there any tool or a function I overlooked that produces summaries of this style?&#xD;&#xA;&#xD;&#xA;I know that vcf file is just a plain text file and if I will dissect `REF` and `ALT` columns I should be able to write a script that will do the job, but I hoped that I could avoid to write my own parser.&#xD;&#xA;&#xD;&#xA;--- edit ---&#xD;&#xA;&#xD;&#xA;So far it seems that only tool that aims to do summaries (@gringer answer) is not working on vcf v4.1. Other tools would provide just partial solution by filtering certain variant type. Therefore I accept my own parser perl/R solutions, till there will be a **working** tool for **stats** of vcf with **structural variants**.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://vcftools.github.io/&#xD;&#xA;  [2]: https://cran.r-project.org/web/packages/vcfR/index.html&#xD;&#xA;  [3]: https://pypi.python.org/pypi/PyVCF" />
  <row Id="1252" PostHistoryTypeId="5" PostId="388" RevisionGUID="52089ae0-4e2d-4c16-9901-90e3cb9d4db2" CreationDate="2017-06-03T11:16:03.760" UserId="104" Comment="added 132 characters in body" Text="I want some templates of different file formats that I can use to test my scripts and identify possible bugs in my code.&#xD;&#xA;&#xD;&#xA;For example, consider FASTA, a simple but often abused format, I would want templates to capture regular and irregular formats, like I have seen all of these:&#xD;&#xA;&#xD;&#xA;1) Single line sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    ATG&#xD;&#xA;&#xD;&#xA;2) Multi-line sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    AT&#xD;&#xA;    G&#xD;&#xA;&#xD;&#xA;3) Upper and lower case letters in sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    Atg&#xD;&#xA;&#xD;&#xA;4) Ns and Xs in sequence&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    ANnxX&#xD;&#xA;&#xD;&#xA;5) Unusual headers (sometimes even non-ASCI characters in headers)&#xD;&#xA;&#xD;&#xA;    &gt;ATG | &gt;Name @species | (MULTI_SPECIES)[taxa]&#xD;&#xA;    ATG&#xD;&#xA;&#xD;&#xA;6) Whitespace between records&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    ATG&#xD;&#xA;    &#xD;&#xA;    &gt;2&#xD;&#xA;    ATG&#xD;&#xA;&#xD;&#xA;7) Duplicated headers&#xD;&#xA;&#xD;&#xA;    &gt;1&#xD;&#xA;    ATG&#xD;&#xA;    &gt;1&#xD;&#xA;    ATC&#xD;&#xA;&#xD;&#xA;etc.&#xD;&#xA;&#xD;&#xA;There should be separate templates for nucleotide and protein FASTA, and separate ones for aligned FASTA.&#xD;&#xA;&#xD;&#xA;It would ideally include other aspects too, like different compression formats (such as `.gz`, `.bzip2`) and different file extensions (such as `.fa`, `.fasta`).&#xD;&#xA;&#xD;&#xA;I have never seen resources that provide templates covering these, but I think it would be useful. Of course I could build my own templates but it would take time to capture all the likely variations of the formats, particularly for more complex file formats.&#xD;&#xA;&#xD;&#xA;Note, I am not just interested in FASTA format, it was an example.&#xD;&#xA;&#xD;&#xA;Also note, I know about tools (such as `BioPython`) that should handle many formats well, but they may also have bugs. Anyway, in practice sometimes I end up parsing files myself directly because I don't want the overhead or dependency of an external package.&#xD;&#xA;&#xD;&#xA;*EDIT: Please don't answer this question to say you don't know of any such resources, me neither, hence the question. [bli's helpful answer][1] shows that there is at least one test suite that could be used as a starting point. I know that it is normally easy to look up the specification of any particular file format.*&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/393/104" />
  <row Id="1253" PostHistoryTypeId="2" PostId="410" RevisionGUID="cffc8838-555a-44a0-8296-0a13b64d2882" CreationDate="2017-06-03T11:31:03.630" UserId="476" Text="Next to OmicsDI the EBI has a special repository for multi-omics datasets: https://www.ebi.ac.uk/biosamples&#xD;&#xA;&#xD;&#xA;It links the different datasets between repositories, ie. PRIDE for MS/MS based data and ArrayExpress for RNASeq data." />
  <row Id="1254" PostHistoryTypeId="2" PostId="411" RevisionGUID="55bc955f-fe00-4278-9eb7-63c41625efcf" CreationDate="2017-06-03T11:43:30.357" UserId="556" Text="I'm trying to download three WGS datasets from the SRA that are each between 60 and 100GB in size.  So far I've tried:&#xD;&#xA;&#xD;&#xA; - Fetching the .sra files directly from NCBI's ftp site&#xD;&#xA; - Fetching the .sra files directly using the aspera command line (`ascp`)&#xD;&#xA; - Using the SRA toolkit's `fastqdump` and `samdump` tools&#xD;&#xA;&#xD;&#xA;It's excruciatingly slow.  I've had three `fastqdump` processes running in parallel now for approximately 18 hours.  They're running on a large AWS instance in the US east (Virginia) region, which I figure is about as close to NCBI as I can get.  In 18 hours they've downloaded a total of 33GB of data.  By my calculation that's ~500kb/s.  They do appear to still be running - the fastq files continue to grow and their timestamps continue to update.&#xD;&#xA;&#xD;&#xA;At this rate it's going to take me days or weeks just to download the datasets.  Surely the SRA must be capable of moving data at higher rates that this?  I've also looked, and unfortunately the datasets I'm interested have not been mirrored out to ENA or the Japanese archive, so it looks like I'm stuck working with the SRA.&#xD;&#xA;&#xD;&#xA;Is there a better way to fetch this data that wouldn't take multiple days?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1255" PostHistoryTypeId="1" PostId="411" RevisionGUID="55bc955f-fe00-4278-9eb7-63c41625efcf" CreationDate="2017-06-03T11:43:30.357" UserId="556" Text="What's the best way to download data from the SRA? Is it really this slow?" />
  <row Id="1256" PostHistoryTypeId="3" PostId="411" RevisionGUID="55bc955f-fe00-4278-9eb7-63c41625efcf" CreationDate="2017-06-03T11:43:30.357" UserId="556" Text="&lt;archive&gt;" />
  <row Id="1257" PostHistoryTypeId="2" PostId="412" RevisionGUID="0e145def-725f-46d2-be8d-a70aba6d253b" CreationDate="2017-06-03T12:10:48.387" UserId="73" Text="Proximity to NCBI may not necessarily give you the fastest transfer speed. AWS may be deliberately throttling the Internet connection to limit the likelihood that people will use it for undesirable things. There's a chance that a home network might be faster, but you're likely to get the fastest connection to NCBI by using an academic system that is linked to NCBI via a research network.&#xD;&#xA;&#xD;&#xA;Another possibility is using Aspera for downloads. This is unlikely to help if bandwidth is being throttled, but it might help if there's a bit of congestion through the regular methods:&#xD;&#xA;&#xD;&#xA;https://www.ncbi.nlm.nih.gov/public/&#xD;&#xA;&#xD;&#xA;NCBI also has an online [book][1] about best practises for downloading data from their servers.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/books/NBK51062/?report=reader" />
  <row Id="1258" PostHistoryTypeId="2" PostId="413" RevisionGUID="637d31fe-157f-46ea-832d-c1092ba246ce" CreationDate="2017-06-03T12:14:11.773" UserId="532" Text="It's worth bearing in mind that when outputting compressed BAM, as most tools do by default, they may well be using different levels of compression and/or different libraries, or versions of said, libraries for doing (de)compression which will result in different file sizes.  Additionally coordinate sorted BAM will compress more than unsorted BAM. The current version of Picard uses [HTSJDK][1] which in turn uses java.util.zip.Deflater/Inflater, current versions of samtools should be using the [HTSlib][2] library which in turn depends on the [standard zlib library][3]. You can see the effect of different implementations of zlib have on file size and execution time in benchmarking done by the [samtools team][4].&#xD;&#xA;&#xD;&#xA;However, in your case the best way to see if there is any difference between the BAM files is to rule out the effect of different levels of compression or libraries used for compression and save both BAM files as uncompressed.  Both samtools and Picard have options for disabling or changing the levels of compression, since the BAM compression standard BGZF is implemented on top of the gzip format it has inherited the ability, just like with gzip, to change the compression level from 0 to 9.&#xD;&#xA;&#xD;&#xA;`samtools view -bu` will allow you to produce uncompressed BAM output (which is also handy for piping into other programs as it saves time wasted compressing decompressing what is essentially a stream).  Also note that `samtools sort` has a `-l INT` setting where INT can be set between 0 (compression off, as with `-u`) 1 (for fastest compression, but increased file size) or -9 (for maximal compression, with increased run time).  Some of the effects of increased runtime for higher compression settings might be ameliorated using the `-@` argument which allows you to set the number of extra threads used for BAM compression, by default samtools won't use any.  &#xD;&#xA;&#xD;&#xA;Picard tools has a general setting `COMPRESSION_LEVEL` which is applicable to most of its tools setting this to 0, `COMPRESSION_LEVEL=0` should disable compression. &#xD;&#xA;&#xD;&#xA;So re-running whatever Picard tool you used in the first instance with `COMPRESSION_LEVEL=0` will then enable you to check that the file is not further modified by `samtools view -bu`. The assumption here being that if both files have exactly the same content they should be the same size uncompressed, of course if they have trivial differences with regard to white space formatting things may still differ.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/samtools/htsjdk&#xD;&#xA;  [2]: https://github.com/samtools/htslib&#xD;&#xA;  [3]: http://zlib.net/&#xD;&#xA;  [4]: http://www.htslib.org/benchmarks/zlib.html" />
  <row Id="1260" PostHistoryTypeId="2" PostId="414" RevisionGUID="e43297dc-1102-490b-b87e-baf40b1e44a4" CreationDate="2017-06-03T12:42:57.640" UserId="375" Text="I need to merge sequencing data from different sequencing runs but for the same ChiP-seq library (HiSeq 2000).&#xD;&#xA;&#xD;&#xA;Are there any potential advantages or disadvantages when merging files at .fastq or .BAM stage (alignment with Bowite/1.1.2)?&#xD;&#xA;" />
  <row Id="1261" PostHistoryTypeId="1" PostId="414" RevisionGUID="e43297dc-1102-490b-b87e-baf40b1e44a4" CreationDate="2017-06-03T12:42:57.640" UserId="375" Text="Merging sequencing data for ChIP-seq experiments" />
  <row Id="1262" PostHistoryTypeId="3" PostId="414" RevisionGUID="e43297dc-1102-490b-b87e-baf40b1e44a4" CreationDate="2017-06-03T12:42:57.640" UserId="375" Text="&lt;chip-seq&gt;" />
  <row Id="1263" PostHistoryTypeId="2" PostId="415" RevisionGUID="751a82ec-efc8-4877-9795-45f1c5cce3ec" CreationDate="2017-06-03T12:48:05.047" UserId="29" Text="I don’t think it matters. Both are easy to merge (BAM via `samtools merge`, and (gzipped) FASTQ via `cat`), and neither method has specific disadvantages, unless your FASTQ files are sorted for some reason (but they generally shouldn’t be).&#xD;&#xA;&#xD;&#xA;One advantage of keeping the FASTQ files separate is that it makes it slightly easier to parallelise the mapping step: just run the mapper in parallel on the separate FASTQ files. Although `bowtie` has an option (`-p`) for this, throughput from that is slightly worse than running the mapping on split files." />
  <row Id="1264" PostHistoryTypeId="2" PostId="416" RevisionGUID="3bc86b01-703b-464d-887c-8c9b49f61e57" CreationDate="2017-06-03T13:07:50.953" UserId="532" Text="GATK best practices are explicably meant to consume BWA MEM generated BAM.  Whilst SNAP may be faster, the Broad will not have tested it for compatibility with GATK as such you can't guaranty using it won't have unexpected consequences.  &#xD;&#xA;&#xD;&#xA;As such you'd be better off using BWA MEM because I assume accurately called variation is always better than fast and incorrectly called variation.  The main issue you'll have is ensuring shorter split hits and mapping quality are reported in the sam way as `bwa MEM -M` which GATK/Picard is expecting.  Ultimately however you'd be better off posting this question on the [GATK forum][1]. &#xD;&#xA;&#xD;&#xA;It's also worth noting that the soon to be released GATK 4 will utilise bwaspark which can distribute it's alignment processes across Apache Spark for increase performance.  Consequently I can't see SNAP being adopted anytime soon.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://gatkforums.broadinstitute.org/gatk" />
  <row Id="1265" PostHistoryTypeId="2" PostId="417" RevisionGUID="22e35927-cf72-4c86-b123-acb1af2a2bec" CreationDate="2017-06-03T13:36:28.887" UserId="532" Text="The [Persistent uniform resource locator or PURL][1] is one such solution, these are designed to be a bit more robust than permalinks in so much as they are supposed to survive the change of domain name.  The bio ontology community already use them http://purl.bioontology.org/docs/index.html &#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Persistent_uniform_resource_locator" />
  <row Id="1266" PostHistoryTypeId="2" PostId="418" RevisionGUID="47487cdf-22d5-4113-af22-e416bde92417" CreationDate="2017-06-03T13:58:26.980" UserId="298" Text="The industry standard for aligning short reads seems to be bwa-mem. However, in my tests I have seen that using bwa-sampe + bwa-samse performs better. It is slightly slower, but gives significantly better results in terms of both sensitivity and specificity. I have tested it using the genome in a bottle data and public samples (NA12878 and NA12877 among others) and found that bwa-samse + bwa-sampe consistently outperformed bwa-mem. &#xD;&#xA;&#xD;&#xA;So why is bwa-mem the standard? Am I wrong and other tests have shown the opposite? I don't really see how since I tested using the most common datasets and validation data. Is it that the slight increase in efficiency outweighs the decrease in performance? " />
  <row Id="1267" PostHistoryTypeId="1" PostId="418" RevisionGUID="47487cdf-22d5-4113-af22-e416bde92417" CreationDate="2017-06-03T13:58:26.980" UserId="298" Text="Why is bwa-mem the standard algorithm when using bwa?" />
  <row Id="1268" PostHistoryTypeId="3" PostId="418" RevisionGUID="47487cdf-22d5-4113-af22-e416bde92417" CreationDate="2017-06-03T13:58:26.980" UserId="298" Text="&lt;ngs&gt;&lt;alignment&gt;&lt;bwa&gt;" />
  <row Id="1272" PostHistoryTypeId="5" PostId="418" RevisionGUID="5ae186e4-5f48-4bf9-8aa8-8487950076b1" CreationDate="2017-06-03T15:26:15.993" UserId="298" Comment="added 571 characters in body" Text="The industry standard for aligning short reads seems to be bwa-mem. However, in my tests I have seen that using bwa backtrack (bwa-aln + bwa-sampe + bwa-samse) performs better. It is slightly slower, but gives significantly better results in terms of both sensitivity and specificity. I have tested it using the genome in a bottle data and public samples (NA12878 and NA12877 among others) and found that backtrack consistently outperformed bwa-mem. &#xD;&#xA;&#xD;&#xA;So why is bwa-mem the standard? Am I wrong and other tests have shown the opposite? I don't really see how since I tested using the most common datasets and validation data. Is it that the slight increase in efficiency outweighs the decrease in performance? &#xD;&#xA;&#xD;&#xA;The only other explanation I can see is that bwa backtrack is designed specifically for Illumina reads and all my tests have been on Illumina data. Is it just that bwa-mem is &quot;sequencer agnostic&quot;? So that we can use the same algorithm irrespective of what sequencing platform is used? In that case, it makes sense to use backtrack for if we only deal with Illumina data and mem if we can have different sequencers. But, if so, seeing as Illumina is so widespread, why isn't backtrack used more often on Illumina data? I feel I must be missing something." />
  <row Id="1274" PostHistoryTypeId="2" PostId="420" RevisionGUID="d325949f-ea4e-4a99-8326-d1629c6fe99a" CreationDate="2017-06-03T16:18:46.493" UserId="532" Text="I have previously estimated tumour purity with the [EXPANDS][1] an inferred tumour heterogeneity program which is designed to calculate the number of clonal subpopulations in matched tumour/normal samples.  The purity is essentially the size of the largest subpopulation identified in that sample - this is discussed in the programs [FAQ][2].  In addition to a matched tumour / normal exome or genome sequencing you'll also need match somatic copy number as a seg file as input.  Other programs also exist for this sort of inferred heterogeneity analysis - some of which may also give you a measure of purity are: [Absolute][3], [ThetA][4], [SciClone][5], [CHAT][6], [PyClone][7] and [Canopy][8]. A more complete list looks to be [here][9].  &#xD;&#xA;&#xD;&#xA;The only other thing I'd suggest is have an estimate via some orthogonal measure of the purity to try at least judge how the different methods are performing with you data, _e_._g_. you might have clonality indications from [cytogenetics/FISH][10] or histology work, or perhaps [FACS][11].  Picking samples known to be pure / impure via one or more of these might help you get a handle on how well various methods are performing.&#xD;&#xA;&#xD;&#xA;As regards estimation without a normal sample the only program which looks like it might help is [QuantumClone][12], but that requires more than one tumour sample form said patient either spatially or temporally distinct to perform the analysis. &#xD;&#xA;&#xD;&#xA;Also If you have low coverage DNA sequencing [CNAnorm][13] looks like it could be useful with just a single sample. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://dna-discovery.stanford.edu/software/expands/&#xD;&#xA;  [2]: http://dna-discovery.stanford.edu/software/expands/expands_faq.html&#xD;&#xA;  [3]: https://www.nature.com/nbt/journal/v30/n5/full/nbt.2203.html&#xD;&#xA;  [4]: https://genomebiology.biomedcentral.com/articles/10.1186/gb-2013-14-7-r80&#xD;&#xA;  [5]: http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003665&#xD;&#xA;  [6]: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0473-4&#xD;&#xA;  [7]: https://www.nature.com/nmeth/journal/v11/n4/full/nmeth.2883.html&#xD;&#xA;  [8]: http://www.pnas.org/content/113/37/E5528.full&#xD;&#xA;  [9]: https://omictools.com/tumor-purity-and-heterogeneity-category&#xD;&#xA;  [10]: https://en.wikipedia.org/wiki/Cytogenetics&#xD;&#xA;  [11]: https://en.wikipedia.org/wiki/Flow_cytometry#Fluorescence-activated_cell_sorting_.28FACS.29&#xD;&#xA;  [12]: https://github.com/DeveauP/QuantumClone/&#xD;&#xA;  [13]: http://www.precancer.leeds.ac.uk/software-and-datasets/cnanorm/" />
  <row Id="1275" PostHistoryTypeId="6" PostId="338" RevisionGUID="5bbdab37-1b74-4a7d-ad0f-0004f936e424" CreationDate="2017-06-03T17:33:30.207" UserId="93" Comment="add terminology tag" Text="&lt;statistics&gt;&lt;terminology&gt;&lt;computation&gt;" />
  <row Id="1276" PostHistoryTypeId="24" PostId="338" RevisionGUID="5bbdab37-1b74-4a7d-ad0f-0004f936e424" CreationDate="2017-06-03T17:33:30.207" Comment="Proposed by 93 approved by 57, 77 edit id of 99" />
  <row Id="1277" PostHistoryTypeId="2" PostId="421" RevisionGUID="d7918a54-19e1-46a6-889f-a2d7f232eacc" CreationDate="2017-06-03T18:39:37.780" UserId="134" Text="The SAM format originally had only **M**, **I**, **D**, **N**, **S**, **H**, and **P** CIGAR operators.  See the [original SAM specification](https://github.com/samtools/hts-specs/tree/a9b16d4c54d8b2bf900b210b8319474163d7ce2a) (if you can view Apple Pages documents) and Table 1 in [_The Sequence Alignment/Map format and SAMtools_](https://www.ncbi.nlm.nih.gov/pubmed/19505943) (Li _et al_, 2009).  This was in line with previous tools using CIGAR strings, notably [_exonerate_](http://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate) which introduced them with just the **M**, **I**, and **D** operators.&#xD;&#xA;&#xD;&#xA;BWA-backtrack was written contemporaneously with the SAM format in 2009 (and [published in May 2009](https://www.ncbi.nlm.nih.gov/pubmed/19451168)).  Its _ChangeLog_ shows that it was outputting mismatch information in an **MD** tag from January 2009 and that **MD** was defined in the SAM specification of the time (and that the tag value's syntax was somewhat in flux in February 2009).&#xD;&#xA;&#xD;&#xA;The **=** and **X** CIGAR operators were introduced later in [SAM v1.3](https://github.com/samtools/hts-specs/commit/07dc1c67a717a6c5cc9d65eeb8b3c99612744cde) as a result of [this lengthy samtools-devel mailing list thread](https://sourceforge.net/p/samtools/mailman/samtools-devel/thread/ee957a250907301016y22c96c5x2907a33c2e719da2%40mail.gmail.com/).  The characters used for the operators and the initial implementations were essentially in place by November 2009.&#xD;&#xA;&#xD;&#xA;Since then, the **=**/**X** operators have not really taken over from **M**, as you've seen.  There's probably a number of factors contributing to this:&#xD;&#xA;&#xD;&#xA;* Their later introduction, long after the basic **M**/**I**/**D** operators and **MD** tag were well-established;&#xD;&#xA;* Conceivably their being specific to SAM and unavailable in other CIGAR flavours;&#xD;&#xA;* The **MD** tag still provides more information — **X** doesn't tell you what the mismatched reference bases were." />
  <row Id="1278" PostHistoryTypeId="5" PostId="180" RevisionGUID="a8e14483-b152-4cbb-9617-b8cb32e0234c" CreationDate="2017-06-03T18:40:03.477" UserId="559" Comment="Added summary per Wikipedia and link to home page." Text="Bioconductor is &quot;is a free, open source and open development software project for the analysis and comprehension of genomic data generated by wet lab experiments in molecular biology.&quot; ([Wikipedia][1])&#xD;&#xA;&#xD;&#xA;Learn more on the project's [home page][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Bioconductor&#xD;&#xA;  [2]: https://www.bioconductor.org/" />
  <row Id="1279" PostHistoryTypeId="24" PostId="180" RevisionGUID="a8e14483-b152-4cbb-9617-b8cb32e0234c" CreationDate="2017-06-03T18:40:03.477" Comment="Proposed by 559 approved by 57, 77 edit id of 100" />
  <row Id="1280" PostHistoryTypeId="2" PostId="422" RevisionGUID="83799733-0bfb-4460-99af-79b540f4d91e" CreationDate="2017-06-03T20:57:33.417" UserId="73" Text="I've come across a bit of confusion about the initialism NGS, so think it would be a good idea to clarify this term (and similar terms like 2GS, SBS, and HTS) for this site with a bit of discussion.&#xD;&#xA;&#xD;&#xA;What are the most common initialisms used to describe different sequencing technologies?" />
  <row Id="1281" PostHistoryTypeId="1" PostId="422" RevisionGUID="83799733-0bfb-4460-99af-79b540f4d91e" CreationDate="2017-06-03T20:57:33.417" UserId="73" Text="What is the difference between NGS, 2GS, SBS and HTS?" />
  <row Id="1282" PostHistoryTypeId="3" PostId="422" RevisionGUID="83799733-0bfb-4460-99af-79b540f4d91e" CreationDate="2017-06-03T20:57:33.417" UserId="73" Text="&lt;terminology&gt;&lt;nbs&gt;&lt;sbs&gt;&lt;hts&gt;" />
  <row Id="1283" PostHistoryTypeId="2" PostId="423" RevisionGUID="f9cae712-43b2-4145-b445-210834a36eb6" CreationDate="2017-06-03T20:57:33.417" UserId="73" Text="Here are my attempts at definitions:&#xD;&#xA;&#xD;&#xA;[Sanger][1]: A method of sequencing that depends on chain-terminatiing dideoxynucleotides. This sequencing uses the differential flow of DNA sequences of different lengths through a gel to determine the original DNA sequence, producing a single sequence per reaction container.&#xD;&#xA;&#xD;&#xA;NGS: Next-generation sequencing, also referred to as 2GS (second-generation sequencing). This term is used to describe the first wave of sequencing technologies that followed Sanger sequencing technology. The use of NGS has become more confusing with the advent of long-read sequencing, because it's a common assumption that &quot;next-generation&quot; refers to the most recent technology (which is incorrect in this case).&#xD;&#xA;&#xD;&#xA;HTS: High-throughput sequencing. This term describes any type of sequencing technology that produces large amounts of data, usually in the form of millions of different sequences produced from the same sequencing run.&#xD;&#xA;&#xD;&#xA;SBS: Sequencing by synthesis. This term describes a method of sequencing that depends on the synthesis of [DNA] bases in order for sequencing to be carried out. This definition can extend into long-read sequencing (e.g. PacBio sequencers depend on synthesis during sequencing), but is more typically associated with only the second-generation sequencing technology.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Sanger_sequencing" />
  <row Id="1284" PostHistoryTypeId="5" PostId="421" RevisionGUID="fc0d38db-7df0-48d9-9d9b-d508fe41b741" CreationDate="2017-06-03T21:34:45.437" UserId="134" Comment="added 88 characters in body" Text="The SAM format originally had only **M**, **I**, **D**, **N**, **S**, **H**, and **P** CIGAR operators.  See the [original SAM specification](https://github.com/samtools/hts-specs/tree/a9b16d4c54d8b2bf900b210b8319474163d7ce2a) (if you can view Apple Pages documents) and Table 1 in [_The Sequence Alignment/Map format and SAMtools_](https://www.ncbi.nlm.nih.gov/pubmed/19505943) (Li _et al_, 2009).  This was in line with previous tools using CIGAR strings, notably [_exonerate_](http://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate) which introduced them with just the **M**, **I**, and **D** operators.&#xD;&#xA;&#xD;&#xA;BWA-backtrack was written contemporaneously with the SAM format in 2009 (and [published in May 2009](https://www.ncbi.nlm.nih.gov/pubmed/19451168)).  Its _ChangeLog_ shows that it was outputting mismatch information in an **MD** tag from January 2009 and that **MD** was defined in the SAM specification of the time (and that the tag value's syntax was somewhat in flux in February 2009).  The **MD** tag is also described in that early v1.0 Pages-formatted SAM specification.&#xD;&#xA;&#xD;&#xA;The **=** and **X** CIGAR operators were introduced later in [SAM v1.3](https://github.com/samtools/hts-specs/commit/07dc1c67a717a6c5cc9d65eeb8b3c99612744cde) as a result of [this lengthy samtools-devel mailing list thread](https://sourceforge.net/p/samtools/mailman/samtools-devel/thread/ee957a250907301016y22c96c5x2907a33c2e719da2%40mail.gmail.com/).  The characters used for the operators and the initial implementations were essentially in place by November 2009.&#xD;&#xA;&#xD;&#xA;Since then, the **=**/**X** operators have not really taken over from **M**, as you've seen.  There's probably a number of factors contributing to this:&#xD;&#xA;&#xD;&#xA;* Their later introduction, long after the basic **M**/**I**/**D** operators and **MD** tag were well-established;&#xD;&#xA;* Conceivably their being specific to SAM and unavailable in other CIGAR flavours;&#xD;&#xA;* The **MD** tag still provides more information — **X** doesn't tell you what the mismatched reference bases were." />
  <row Id="1286" PostHistoryTypeId="2" PostId="424" RevisionGUID="89e36e18-dafb-492e-b27c-b1c6f3af043d" CreationDate="2017-06-03T23:01:33.863" UserId="549" Text="Blast reports E-values, but short-read mappers report mapping qualities. Are they the same thing? Can they be converted to each other? If not, why blast doesn't report mapping quality while short-read mappers do not report E-values?" />
  <row Id="1287" PostHistoryTypeId="1" PostId="424" RevisionGUID="89e36e18-dafb-492e-b27c-b1c6f3af043d" CreationDate="2017-06-03T23:01:33.863" UserId="549" Text="What is the difference between SAM mapping quality and Blast E-value?" />
  <row Id="1288" PostHistoryTypeId="3" PostId="424" RevisionGUID="89e36e18-dafb-492e-b27c-b1c6f3af043d" CreationDate="2017-06-03T23:01:33.863" UserId="549" Text="&lt;blast&gt;&lt;read-mapping&gt;" />
  <row Id="1289" PostHistoryTypeId="2" PostId="425" RevisionGUID="cca7140b-7a9b-4366-85c3-7954192faf38" CreationDate="2017-06-03T23:06:42.720" UserId="563" Text="There are several [datasets available on GEO][1], though you do have to search for them. For example, here are three data sets that have both Illumina methylation and gene expression microarray profiling:&#xD;&#xA;&#xD;&#xA; - [GSE87650][2]&#xD;&#xA; - [GSE72874][3]&#xD;&#xA; - [GSE56047][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/geo/browse/?view=series&amp;display=20&amp;zsort=samples&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE87650&#xD;&#xA;  [3]: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72874&#xD;&#xA;  [4]: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE56047" />
  <row Id="1290" PostHistoryTypeId="2" PostId="426" RevisionGUID="6a33a52f-cd1c-4ba3-b291-ac9b1c63ea12" CreationDate="2017-06-03T23:10:32.853" UserId="549" Text="I learned that GATK 4 is using Spark for parallelization. I googled around, though I am still not quite sure how spark really works and how to use it in practice. Besides GATK 4, are any other bioinformatics tool using spark? Generally, is spark widely used? Is it a necessary skill to learn? Thanks in advance." />
  <row Id="1291" PostHistoryTypeId="1" PostId="426" RevisionGUID="6a33a52f-cd1c-4ba3-b291-ac9b1c63ea12" CreationDate="2017-06-03T23:10:32.853" UserId="549" Text="Is spark widely used in bioinformatics?" />
  <row Id="1292" PostHistoryTypeId="3" PostId="426" RevisionGUID="6a33a52f-cd1c-4ba3-b291-ac9b1c63ea12" CreationDate="2017-06-03T23:10:32.853" UserId="549" Text="&lt;gatk&gt;&lt;spark&gt;" />
  <row Id="1293" PostHistoryTypeId="2" PostId="427" RevisionGUID="2a664d3b-03b7-4e29-a4c0-3bda609ddcb7" CreationDate="2017-06-03T23:19:39.530" UserId="549" Text="I was given a list of target regions in BED and many exome alignments in BAM. I was asked to extract on-target alignments from these BAMs to save disk space. I know I can use bedtools to extract sub-BAMs. I am thinking to write a script to apply bedtools to all BAMs at my hand, but I speculate there may be some more convenient command lines to achieve my goal. How would you do? Thanks." />
  <row Id="1294" PostHistoryTypeId="1" PostId="427" RevisionGUID="2a664d3b-03b7-4e29-a4c0-3bda609ddcb7" CreationDate="2017-06-03T23:19:39.530" UserId="549" Text="How to extract exome on-target reads in batch?" />
  <row Id="1295" PostHistoryTypeId="3" PostId="427" RevisionGUID="2a664d3b-03b7-4e29-a4c0-3bda609ddcb7" CreationDate="2017-06-03T23:19:39.530" UserId="549" Text="&lt;bam&gt;&lt;linux&gt;&lt;bedtools&gt;" />
  <row Id="1296" PostHistoryTypeId="5" PostId="416" RevisionGUID="3dfd41f2-c8a6-4a33-b88f-fa3d7db5da68" CreationDate="2017-06-04T00:35:38.640" UserId="532" Comment="typo" Text="GATK best practices are explicably meant to consume BWA MEM generated BAM.  Whilst SNAP may be faster, the Broad will not have tested it for compatibility with GATK as such you can't guaranty using it won't have unexpected consequences.  &#xD;&#xA;&#xD;&#xA;As such you'd be better off using BWA MEM because I assume accurately called variation is always better than fast and incorrectly called variation.  The main issue you'll have is ensuring shorter split hits and mapping quality are reported in the same way as `bwa MEM -M` which GATK/Picard is expecting.  Ultimately however you'd be better off posting this question on the [GATK forum][1]. &#xD;&#xA;&#xD;&#xA;It's also worth noting that the soon to be released GATK 4 will utilise bwaspark which can distribute it's alignment processes across Apache Spark for increase performance.  Consequently I can't see SNAP being adopted anytime soon.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://gatkforums.broadinstitute.org/gatk" />
  <row Id="1299" PostHistoryTypeId="2" PostId="428" RevisionGUID="9c0763cf-4f7e-4373-ab67-6079fa233b49" CreationDate="2017-06-04T01:17:18.017" UserId="492" Text="I don't have enough experience to answer which probabilistic distribution should be used.&#xD;&#xA;&#xD;&#xA;However, this questions also also asks how to estimate parameters of the distributions. If a binomial distribution is chosen, then Heng Li's paper titled &quot;A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data&quot; [1] is probably the definitive one. Section 2.3.1 of that paper describes an EM algorithm for estimating allele frequencies from multiple samples under the assumption of Hardy-Weinberg equilibrium for arbitrary but constant ploidy.&#xD;&#xA;&#xD;&#xA;[1]: https://doi.org/10.1093/bioinformatics/btr509" />
  <row Id="1300" PostHistoryTypeId="2" PostId="429" RevisionGUID="fe761008-484c-4c28-be3a-8c1b7eb62f7e" CreationDate="2017-06-04T01:31:45.110" UserId="73" Text="It sounds like ```bedtools intersect``` will work for you:&#xD;&#xA;&#xD;&#xA;    bedtools intersect -wa -a &lt;alignment.bam&gt; -b &lt;region.bed&gt; &gt; &lt;intersect_alignment.bam&gt;&#xD;&#xA;&#xD;&#xA;&gt; When a BAM file is used for the A file, the alignment is retained if overlaps exist, and exlcuded if an overlap cannot be found.  If multiple overlaps exist, they are not reported, as we are only testing for one or more overlaps.&#xD;&#xA;&#xD;&#xA;It's easy enough to wrap this up into a ```for``` loop for batch processing (line breaks can be removed if desired):&#xD;&#xA;&#xD;&#xA;     for alnBam in alignmentFiles*.bam;&#xD;&#xA;       do echo ${x};&#xD;&#xA;       bedtools intersect -wa -a ${alnBam} -b &lt;region.bed&gt; &gt; intersect_${alnBam};&#xD;&#xA;     done&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;There are a lot of other options which should cover most related uses. For more details, just run &quot;bedtools intersect&quot; (with no arguments)." />
  <row Id="1301" PostHistoryTypeId="2" PostId="430" RevisionGUID="43b2bb2d-0936-4d0a-b430-bd7ec7be2369" CreationDate="2017-06-04T01:33:49.913" UserId="492" Text="The only bioinformatics tool other than GATK4 that I am aware of that uses spark is [Hail](https://github.com/hail-is/hail) (a Spark based replacement for Plink). Hail is also supported by researchers at the Broad.&#xD;&#xA;&#xD;&#xA;Most places I have worked at have not switched over to Spark.  As such, I don't think it is widely used generally. Therefore, I don't think knowing Spark qualifies as a necessary skill to learn for bioinformatics at this time.&#xD;&#xA;&#xD;&#xA;However, I think Spark is a superior paradigm for handling the large amount of data that bioinformaticians routinely deal with, and I think we will soon see the field move towards using it more.  Any bioinformatician would do well to acquaint themselves with Spark and to play around with Hail." />
  <row Id="1302" PostHistoryTypeId="2" PostId="431" RevisionGUID="0ea60efd-703e-47ec-9381-c82cf0f349f4" CreationDate="2017-06-04T07:23:24.257" UserId="-1" Text="" />
  <row Id="1303" PostHistoryTypeId="1" PostId="431" RevisionGUID="0ea60efd-703e-47ec-9381-c82cf0f349f4" CreationDate="2017-06-04T07:23:24.257" UserId="-1" />
  <row Id="1304" PostHistoryTypeId="2" PostId="432" RevisionGUID="51f89508-c905-4328-88f8-ff77b7812a49" CreationDate="2017-06-04T07:23:24.257" UserId="-1" Text="" />
  <row Id="1305" PostHistoryTypeId="1" PostId="432" RevisionGUID="51f89508-c905-4328-88f8-ff77b7812a49" CreationDate="2017-06-04T07:23:24.257" UserId="-1" />
  <row Id="1306" PostHistoryTypeId="2" PostId="433" RevisionGUID="e9304d36-98be-498d-8762-43ea504bd25f" CreationDate="2017-06-04T07:38:44.210" UserId="73" Text="```bwa mem``` is newer, faster, and [should be] more accurate, particularly for longer reads.&#xD;&#xA;&#xD;&#xA;From the ```bwa``` man page (presumably in Heng Li's own words):&#xD;&#xA;&#xD;&#xA;&gt; BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms:  BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm  is designed for Illumina sequence reads up to 100bp, while the rest two for longer sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar  features such as long-read support and split alignment, but ***BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and  more  accurate.*** BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads." />
  <row Id="1307" PostHistoryTypeId="16" PostId="423" RevisionGUID="2a3c27f5-9622-4b07-bd99-ff2e53235d33" CreationDate="2017-06-04T07:42:56.527" UserId="73" />
  <row Id="1308" PostHistoryTypeId="2" PostId="434" RevisionGUID="a49ef17c-e56d-400a-9ee3-846470ec3c22" CreationDate="2017-06-04T09:28:02.797" UserId="73" Text="Our research institute processes a lot of flow cytometry data, but the produced data is under-utilised due to the effort required to process it. A typical run will produce 5 million events (ideally one event per cell), with up to 14 dimensions of [ideally] log-normal fluorescence values for particular groups of cells (&quot;populations&quot;). Due to various systematic errors, negative values, scatter, and a non-zero &quot;zero&quot; value can happen, but I'm going to ignore those for the purpose of this question and assume that the data are well-distributed.&#xD;&#xA;&#xD;&#xA;Researchers will typically probe these data using manual filters (set up specifically for each experiment) to find populations of interest. I suppose a picture might help. This one shows three identifiable cell populations, in order of size one at about (X/Ly6C:2,Y/CD86:2), one at about (4.5,3), and one small population at (2,4).&#xD;&#xA;&#xD;&#xA;[![CD86 vs Ly6C flow plot][1]][1]&#xD;&#xA;&#xD;&#xA;Here's another plot that has two cell populations that are close to each other, such that the population &quot;humps&quot; overlap substantially.&#xD;&#xA;&#xD;&#xA;[![B220 vs CD11b flow plot][2]][2] &#xD;&#xA;&#xD;&#xA;Manual filters are typically used because it can be very difficult to distinguish between a noisy data point from a large cell population and a less noisy data point from a small cell population, particularly when considering populations that make up about 0.01% of the total cells.&#xD;&#xA;&#xD;&#xA;As an additional complication, counting these populations can be difficult when populations overlap (as in the second image). A filter/slice through the plot that separates populations could count many cells as being members of the wrong population.&#xD;&#xA;&#xD;&#xA;Can these populations be detected in an automated way? If it is assumed that the populations are spread in a gaussian fashion at some point in each dimension, is there some method that can be used to approximate the number of cells in each population, even when populations are close by?&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/ErbhE.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/jJ72F.png" />
  <row Id="1309" PostHistoryTypeId="1" PostId="434" RevisionGUID="a49ef17c-e56d-400a-9ee3-846470ec3c22" CreationDate="2017-06-04T09:28:02.797" UserId="73" Text="Finding peaks and estimating cell population sizes in multi-dimensional flow cytometry data" />
  <row Id="1310" PostHistoryTypeId="3" PostId="434" RevisionGUID="a49ef17c-e56d-400a-9ee3-846470ec3c22" CreationDate="2017-06-04T09:28:02.797" UserId="73" Text="&lt;flow-cytometry&gt;&lt;gaussian&gt;&lt;fluorescence&gt;" />
  <row Id="1311" PostHistoryTypeId="2" PostId="435" RevisionGUID="8e19dc63-a4df-4362-977a-7d2b36ded3e2" CreationDate="2017-06-04T10:24:42.397" UserId="156" Text="The E-value and the mapping qualities are two very different things.&#xD;&#xA;&#xD;&#xA;The E-value is &quot;a parameter that describes the number of hits one can 'expect' to see by chance when searching a database of a particular size&quot;. More details can be found here: https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=FAQ#expect&#xD;&#xA;&#xD;&#xA;The mapping quality is an attempt to estimate the probability that a given base from a given read is mapped correctly to a particular place in the reference genome. Different aligners calculate mapping qualities very differently, so there is no simple way to compare them across aligners. But in general the mapping quality will consider things like: (i) the quality of the base call; (ii) the repeat structure of the reference; (iii) the alignment algorithm; (iv) whether the read has a mapped pair; (v) anything else the author of the mapping software thought might help. &#xD;&#xA;&#xD;&#xA;E-values and mapping qualities are, therefore, measuring two fundamentally different properties with very different uses." />
  <row Id="1312" PostHistoryTypeId="6" PostId="424" RevisionGUID="8dc9ae4d-9a23-49d7-a80b-4f31c1ecd564" CreationDate="2017-06-04T10:57:51.070" UserId="73" Comment="edited tags" Text="&lt;terminology&gt;&lt;blast&gt;&lt;read-mapping&gt;" />
  <row Id="1313" PostHistoryTypeId="2" PostId="436" RevisionGUID="e99875d9-fd17-40a0-b0da-bddc1f142cc5" CreationDate="2017-06-04T11:13:06.240" UserId="45" Text="There are quite a few algorithms developed for the automatic classification of multidimensional flow cytometry data, you can see a (not so recent?) review here: http://www.nature.com/nmeth/journal/v10/n3/full/nmeth.2365.html. In your case, you are interested in the **unsupervised** methods, since you do not have data from single populations (which would provide you &quot;labels&quot; associated to each sample), as I understood. &#xD;&#xA;&#xD;&#xA;In other words, if you have your data set as a matrix of 5 million samples (cells) with 14 dimensions (FC values) each, then you are looking basically for algorithms that will do a *clustering* of the samples into groups. By the way your description of each sample belonging to a multivariate Normal distribution leads to a **Gaussian Mixture Model**, which can be estimated from your data using e.g. the scikit-learn library for python: http://scikit-learn.org/stable/modules/mixture.html" />
  <row Id="1314" PostHistoryTypeId="5" PostId="429" RevisionGUID="9b8ad96d-e9ed-4879-a0a5-32b48936ea00" CreationDate="2017-06-04T11:30:33.870" UserId="73" Comment="variable bug" Text="It sounds like ```bedtools intersect``` will work for you:&#xD;&#xA;&#xD;&#xA;    bedtools intersect -wa -a &lt;alignment.bam&gt; -b &lt;region.bed&gt; &gt; &lt;intersect_alignment.bam&gt;&#xD;&#xA;&#xD;&#xA;&gt; When a BAM file is used for the A file, the alignment is retained if overlaps exist, and exlcuded if an overlap cannot be found.  If multiple overlaps exist, they are not reported, as we are only testing for one or more overlaps.&#xD;&#xA;&#xD;&#xA;It's easy enough to wrap this up into a ```for``` loop for batch processing (line breaks can be removed if desired):&#xD;&#xA;&#xD;&#xA;     for alnBam in alignmentFiles*.bam;&#xD;&#xA;       do echo ${alnBam};&#xD;&#xA;       bedtools intersect -wa -a ${alnBam} -b &lt;region.bed&gt; &gt; intersect_${alnBam};&#xD;&#xA;     done&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;There are a lot of other options which should cover most related uses. For more details, just run &quot;bedtools intersect&quot; (with no arguments)." />
  <row Id="1315" PostHistoryTypeId="2" PostId="437" RevisionGUID="57f966ba-e9c5-45fa-8ebb-e47f19fa2632" CreationDate="2017-06-04T12:09:34.170" UserId="532" Text="Automated gating methods are gaining popularity over manual and time consuming analysis in say [FlowJo][1], you should take a look at the [openCyto][2] bioconductor package this is a framework which builds on top of existing Bioconductor infrastructure for flow cytometry [(PLOS compbio paper)][3] with the aim of making various analysis routes more accessible to the researcher by assisting getting data into and out various gating algorithms. You can find a nice intro [here][4] which starts with importing a manual gating scheme from FlowJo and a how-to on 1D and 2D gating can also be found [here][5]. &#xD;&#xA;&#xD;&#xA;An excellent YouTube intro to analysing flow data in R by Ryan Duggan [Cytometry on Air][6] is worth watching especially for those which have no R experience and are curious as to what can be achieved.  Additionally it's worth having a look at the growing list of Bioconductor packages and datasets that exists for flow data: https://www.bioconductor.org/help/search/index.html?q=cytometry/&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.flowjo.com/&#xD;&#xA;  [2]: http://bioconductor.org/packages/release/bioc/html/openCyto.html&#xD;&#xA;  [3]: http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003806&#xD;&#xA;  [4]: http://bioconductor.org/packages/release/bioc/vignettes/openCyto/inst/doc/openCytoVignette.html&#xD;&#xA;  [5]: https://www.bioconductor.org/packages/devel/bioc/vignettes/openCyto/inst/doc/HowToAutoGating.html&#xD;&#xA;  [6]: https://www.youtube.com/watch?v=_B7mo6dB3BU" />
  <row Id="1316" PostHistoryTypeId="2" PostId="438" RevisionGUID="266abc89-7e71-4f6e-89dd-68ea7ea36e99" CreationDate="2017-06-04T13:21:05.390" UserId="532" Text="Working on various cancers I have an interest in detecting structural variation (SV) in human, we've successfully used various tools like [Pindel][1], [SVDetect][2], [Manta][3], and [LUMPY][4], to name a few for detecting SVs in illumina short-read sequencing.  I curious if anyone has successfully used ONTs [MinION][5] sequencer for detecting SV, as there are many cases where longer reads would be beneficial for SV detection especially where events are long and/or occur in repetitive regions.  Has anyone tried or had success with tools previously designed for PacBio data such as [Sniffles][6]? &#xD;&#xA;&#xD;&#xA;Supplementary question, obviously the MinION has not quite got the throughput of say the PacBio SEQUEL so can't generate low-pass coverage for the whole human genome as easily, has anyone got experience trying to generate reads targeting specific areas of the genome in areas that are known for translations, long inversions, duplications, *etc*?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://gmt.genome.wustl.edu/packages/pindel/&#xD;&#xA;  [2]: http://svdetect.sourceforge.net/Site/Home.html&#xD;&#xA;  [3]: https://github.com/Illumina/manta&#xD;&#xA;  [4]: https://github.com/arq5x/lumpy-sv&#xD;&#xA;  [5]: https://nanoporetech.com/products/minion&#xD;&#xA;  [6]: https://github.com/fritzsedlazeck/Sniffles" />
  <row Id="1317" PostHistoryTypeId="1" PostId="438" RevisionGUID="266abc89-7e71-4f6e-89dd-68ea7ea36e99" CreationDate="2017-06-04T13:21:05.390" UserId="532" Text="Detecting structural variants with MinION data" />
  <row Id="1318" PostHistoryTypeId="3" PostId="438" RevisionGUID="266abc89-7e71-4f6e-89dd-68ea7ea36e99" CreationDate="2017-06-04T13:21:05.390" UserId="532" Text="&lt;long-reads&gt;&lt;structural-variation&gt;&lt;minion&gt;&lt;cancer&gt;" />
  <row Id="1319" PostHistoryTypeId="2" PostId="439" RevisionGUID="53e9df4d-9650-4c82-96d2-ee0b144e6816" CreationDate="2017-06-04T13:31:13.200" UserId="532" Text="If I remember correctly Ewan Birney's Dynamite (a [compiler-compiler][1]) as presented at [ISMB 1997][2] had this functionality, there is also some code here on GitHub https://github.com/birney/wise3 which at least mentions Dynamite.  Suspect Ewan is too busy these days to work on this, although he has tweeted about blowing dust of his old Dynamite, sorry Dynamite code https://twitter.com/ewanbirney/status/788121636973142016&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Compiler-compiler&#xD;&#xA;  [2]: https://pdfs.semanticscholar.org/2345/f68bb0b2f079f9f201034da462d69a7817b1.pdf" />
  <row Id="1320" PostHistoryTypeId="2" PostId="440" RevisionGUID="c5b75014-c6b8-4a11-9648-c8cec45fb47c" CreationDate="2017-06-04T13:37:19.093" UserId="64" Text="[Celluloid](https://github.com/mathieu-lemire/celluloid_0.11) emits copy number profiles and tumour purity / ploidy information.  There's a nice tutorial [https://github.com/mathieu-lemire/celluloid_0.11_tutorial](here). One thing to keep in mind is that celluloid will find multiple solutions and taking advantage of its plotting tools is essential to determining which solution is correct. Typically the first solution is correct, but occasionally it may not be.&#xD;&#xA;&#xD;&#xA;Another tool similar is [TITAN](http://genome.cshlp.org/content/early/2014/07/24/gr.180281.114), which may also require hand annotated the ideal solution." />
  <row Id="1321" PostHistoryTypeId="5" PostId="439" RevisionGUID="8e46f4dd-239a-4881-96c0-e62edf1d6ea1" CreationDate="2017-06-04T13:37:40.803" UserId="532" Comment="added 1 character in body" Text="If I remember correctly Ewan Birney's Dynamite (a [compiler-compiler][1]) as presented at [ISMB 1997][2] had this functionality, there is also some code here on GitHub https://github.com/birney/wise3 which at least mentions Dynamite.  Suspect Ewan is too busy these days to work on this, although he has tweeted about blowing dust of his old Dynamite, sorry Dynamite code: https://twitter.com/ewanbirney/status/788121636973142016&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Compiler-compiler&#xD;&#xA;  [2]: https://pdfs.semanticscholar.org/2345/f68bb0b2f079f9f201034da462d69a7817b1.pdf" />
  <row Id="1323" PostHistoryTypeId="2" PostId="441" RevisionGUID="fd2f7c6c-8bf6-4a2c-8b19-b8501a62d745" CreationDate="2017-06-04T13:50:23.627" UserId="569" Text="Some of the work in our lab requires a comparison of a strain across several experimental conditions. We are looking to identify most similar experimental conditions based on the gene transcription response similarity from the cell.&#xD;&#xA;&#xD;&#xA;While we could easily invent and create home-grown methods to do it, their implementation and testing are a laborious project in themselves and are outside the scope of our current work.&#xD;&#xA;&#xD;&#xA;Are there any methods for RNA expression profile similarity computation that has been already published? If yes, what is your experience using them?" />
  <row Id="1324" PostHistoryTypeId="1" PostId="441" RevisionGUID="fd2f7c6c-8bf6-4a2c-8b19-b8501a62d745" CreationDate="2017-06-04T13:50:23.627" UserId="569" Text="What methods exist to calculate RNA expression profile similarity" />
  <row Id="1325" PostHistoryTypeId="3" PostId="441" RevisionGUID="fd2f7c6c-8bf6-4a2c-8b19-b8501a62d745" CreationDate="2017-06-04T13:50:23.627" UserId="569" Text="&lt;rna-seq&gt;&lt;rna&gt;" />
  <row Id="1326" PostHistoryTypeId="6" PostId="120" RevisionGUID="f144edd3-09e5-4c35-a4f1-799810811e74" CreationDate="2017-06-04T14:08:11.640" UserId="57" Comment="removed poitless tag genomics, added structural-variants tag" Text="&lt;ngs&gt;&lt;structural-variation&gt;&lt;variant-calling&gt;" />
  <row Id="1327" PostHistoryTypeId="2" PostId="442" RevisionGUID="e9d204dc-8350-47d7-adf7-2911672563dc" CreationDate="2017-06-04T14:13:44.537" UserId="570" Text="I use SoapDenovo to assemble a large genome (4.8G). With ~20X paired-end reads, the results show contig is 6.3G while scaffold is 2.7G. Note that this is a haploid genome, so there is no issue of heterozygosity issue for scaffolding. &#xD;&#xA;&#xD;&#xA;I am wondering what happened during the scaffolding procedure? and why the result is like so?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1328" PostHistoryTypeId="1" PostId="442" RevisionGUID="e9d204dc-8350-47d7-adf7-2911672563dc" CreationDate="2017-06-04T14:13:44.537" UserId="570" Text="For a de novo assembled large genome, how is contig size and scaffold size related?" />
  <row Id="1329" PostHistoryTypeId="3" PostId="442" RevisionGUID="e9d204dc-8350-47d7-adf7-2911672563dc" CreationDate="2017-06-04T14:13:44.537" UserId="570" Text="&lt;contigsize&gt;&lt;scaffold&gt;&lt;size&gt;&lt;inconsistence&gt;" />
  <row Id="1330" PostHistoryTypeId="5" PostId="442" RevisionGUID="5d4df7fb-3de0-4bb5-af08-e00904eeb7e2" CreationDate="2017-06-04T14:21:20.323" UserId="57" Comment="reformulating the question" Text="I use SOAPdenovo2 to assemble a large genome (4.8G) using ~20X paired-end reads. The sum of contig sizes is 6.3G while sum of scaffolds is 2.7G. Note that this is a haploid genome, so there is no issue of heterozygosity issue for scaffolding. &#xD;&#xA;&#xD;&#xA;I am wondering what happened during the scaffolding procedure? How comes that sum of scaffolds is so much smaller than sum of contigs?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1331" PostHistoryTypeId="4" PostId="442" RevisionGUID="5d4df7fb-3de0-4bb5-af08-e00904eeb7e2" CreationDate="2017-06-04T14:21:20.323" UserId="57" Comment="reformulating the question" Text="What causes the difference in total sum of assembled contigs and scaffolds?" />
  <row Id="1332" PostHistoryTypeId="6" PostId="442" RevisionGUID="5d4df7fb-3de0-4bb5-af08-e00904eeb7e2" CreationDate="2017-06-04T14:21:20.323" UserId="57" Comment="reformulating the question" Text="&lt;assembly&gt;&lt;soapdenovo2&gt;" />
  <row Id="1333" PostHistoryTypeId="5" PostId="431" RevisionGUID="43feb33d-cb3d-41b1-9be0-e977f93004f2" CreationDate="2017-06-04T16:42:18.767" UserId="73" Comment="added 2850 characters in body" Text="```minion``` -- The Oxford Nanopore MinION&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;A small, portable sequencer that uses electrical current flowing through small molecules (e.g. DNA nucleotides) to determine the underlying sequence.&#xD;&#xA;&#xD;&#xA;What questions should have this tag?&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;Anything that is associated with the generation or analysis of data from the MinION, e.g. long-read FASTQ files, raw signal data, nanopore sample prep QC.&#xD;&#xA;&#xD;&#xA;Brief Introduction&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;The MinION's flow cell is comprised of 2048 wells containing a membrane perforated by nanopores. Ligated with a molecular motor, a single stranded DNA molecule passes through the pore, altering the recorded current. After the electronic sequencing is carried out, a software basecalling algorithm transforms the current trace into a modelled DNA sequence.&#xD;&#xA;&#xD;&#xA;The advantages of the MinION are rapid library preparation, portability ([Walter *et al.*, 2016][1]; [Castro-Wallace *et al.*, 2016][2]), long molecule sequencing ([Urban *et al.*, 2015][3]), and sequencing of non-model modifications of the DNA strand ([Simpson *et al.*, 2017][4]). With the recent improvement in the chemistry of the MinION, Oxford Nanopore has overcome the majority of issues associated with low yield and high error rates that have limited the range of its application. The MinION sequencing device has now been successfully applied to sequence genomes of a wide range of sizes, from bacterial and viral genomes ([Deschamps *et al.*, 2016][5]; [Quick *et al.*, 2017][6]), amplicon sequencing like bacterial 16S rRNA sequencing ([Benitez-paez *et al.*, 2016][7]), and more recently a human genome ([Jain *et al.*, 2017][8]). The MinION has also been used for cDNA sequencing ([Hargreaves *et al.*, 2015][9]), for detecting DNA methylation patterns without chemical treatment ([Simpson *et al.*, 2017][4]; [Rand *et al.*, 2017][10]), and for direct RNA sequencing with detection of modified 16S rRNA nucleotides ([Smith *et al.*, 2017][11]).&#xD;&#xA;&#xD;&#xA;[CC-by-4.0 text source by White *et al.*, 2017 [here][12]]&#xD;&#xA;&#xD;&#xA;Important links for learning more&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA; - [Summary of nanopore DNA sequencing][13] (official ONT website)&#xD;&#xA; - [F1000 Research Nanopore Analysis gateway][14]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://dx.doi.org/10.1016/j.jbiotec.2016.12.006&#xD;&#xA;  [2]: http://biorxiv.org/content/early/2016/09/27/077651&#xD;&#xA;  [3]: http://dx.doi.org/10.1101/019281&#xD;&#xA;  [4]: http://dx.doi.org/10.1038/nmeth.4184&#xD;&#xA;  [5]: http://dx.doi.org/10.1038/srep28625&#xD;&#xA;  [6]: http://dx.doi.org/10.1101/098913&#xD;&#xA;  [7]: http://dx.doi.org/10.1186/s13742-016-0111-z&#xD;&#xA;  [8]: http://dx.doi.org/10.1101/128835&#xD;&#xA;  [9]: http://dx.doi.org/10.7717/peerj.1441&#xD;&#xA;  [10]: http://dx.doi.org/10.1038/nmeth.4189&#xD;&#xA;  [11]: http://dx.doi.org/10.1101/132274&#xD;&#xA;  [12]: https://f1000research.com/articles/6-631/v1&#xD;&#xA;  [13]: https://nanoporetech.com/how-it-works&#xD;&#xA;  [14]: https://f1000research.com/gateways/nanoporeanalysis" />
  <row Id="1334" PostHistoryTypeId="24" PostId="431" RevisionGUID="43feb33d-cb3d-41b1-9be0-e977f93004f2" CreationDate="2017-06-04T16:42:18.767" Comment="Proposed by 73 approved by 37, 77 edit id of 102" />
  <row Id="1335" PostHistoryTypeId="5" PostId="432" RevisionGUID="f7928d0c-d0b1-4a87-8090-130f016886bc" CreationDate="2017-06-04T16:42:35.993" UserId="73" Comment="added 149 characters in body" Text="a small, portable sequencer that uses electrical current flowing through small molecules (e.g. DNA nucleotides) to determine the underlying sequence." />
  <row Id="1336" PostHistoryTypeId="24" PostId="432" RevisionGUID="f7928d0c-d0b1-4a87-8090-130f016886bc" CreationDate="2017-06-04T16:42:35.993" Comment="Proposed by 73 approved by 37, 77 edit id of 103" />
  <row Id="1337" PostHistoryTypeId="2" PostId="443" RevisionGUID="7c6c8924-7313-401a-9f08-3f560c5a7c2d" CreationDate="2017-06-04T17:58:23.480" UserId="57" Text="The quality control of ngs reads is heavily dependent on type of the project. &#xD;&#xA;&#xD;&#xA;**For genome assembly** projects based on short reads, beside already covered checking quality of sequencing, you would like to look at the kmer spectra to find out, if your reads are going to make sense when they will be translated to De Brujin graph. You will get also a clues about genome coverage, genome size, repetitive content and small clue about heterozygocity. A lot of useful info about interpretation you can find in the README of GenomeScope.&#xD;&#xA;&#xD;&#xA;A list tools I used:&#xD;&#xA;&#xD;&#xA; - [jellyfish][1] - to count k-mer frequencies&#xD;&#xA; - [GenomeScope][2] - a package for analysis and visualisation of k-mer frequencies (they recommend to use jellyfish for counting k-mer frequencies)&#xD;&#xA; - [kmergenie][3] - for prediction of optimal kmer for assembly&#xD;&#xA;&#xD;&#xA;Using these tools can save you a lot of frustration if you accidentally sequence a contaminated sample or if your colleagues / a sequencing company have sent you a wrong file!&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.genome.umd.edu/jellyfish.html&#xD;&#xA;  [2]: https://github.com/schatzlab/genomescope&#xD;&#xA;  [3]: http://kmergenie.bx.psu.edu/" />
  <row Id="1338" PostHistoryTypeId="5" PostId="183" RevisionGUID="451ee526-7788-4797-a87b-1e919892835d" CreationDate="2017-06-04T19:17:01.873" UserId="57" Comment="suggested ngs description" Text="ngs (next generation sequencing) is a term refering to all high-throughput nucleotide sequencing methods." />
  <row Id="1339" PostHistoryTypeId="24" PostId="183" RevisionGUID="451ee526-7788-4797-a87b-1e919892835d" CreationDate="2017-06-04T19:17:01.873" Comment="Proposed by 57 approved by 77, 55 edit id of 101" />
  <row Id="1340" PostHistoryTypeId="5" PostId="327" RevisionGUID="58e82dac-5839-4729-87f9-3da66c7245a4" CreationDate="2017-06-04T20:02:08.037" UserId="57" Comment="expanded QC abbreveation (to be more explicit)" Text="What are good means for performing quality control (QC) or NGS reads?&#xD;&#xA;&#xD;&#xA;I'm aware of:&#xD;&#xA;&#xD;&#xA;- FastQC&#xD;&#xA;- NGS Screen&#xD;&#xA;- Kraken (e.g., for screening against contaminants)&#xD;&#xA;&#xD;&#xA;What are other popular means for such QC?" />
  <row Id="1341" PostHistoryTypeId="5" PostId="442" RevisionGUID="686fac71-911a-43b8-ba0a-cd92b02392bf" CreationDate="2017-06-04T20:06:35.887" UserId="57" Comment="correction of common term : total sum -&gt; total length; fixed typo" Text="I use SOAPdenovo2 to assemble a large genome (4.8G) using ~20X paired-end reads. The  total length of contig sizes is 6.3G while total length of scaffolds is 2.7G. Note that this is a haploid genome, so there is no issue of heterozygosity for scaffolding. &#xD;&#xA;&#xD;&#xA;I am wondering what happened during the scaffolding procedure? How comes that sum of scaffolds is so much smaller than sum of contigs?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1342" PostHistoryTypeId="4" PostId="442" RevisionGUID="686fac71-911a-43b8-ba0a-cd92b02392bf" CreationDate="2017-06-04T20:06:35.887" UserId="57" Comment="correction of common term : total sum -&gt; total length; fixed typo" Text="What causes the difference in total length of assembled contigs and scaffolds?" />
  <row Id="1343" PostHistoryTypeId="2" PostId="444" RevisionGUID="8e7fbb76-2e20-411c-a721-76382a89a7b0" CreationDate="2017-06-04T20:22:22.863" UserId="462" Text="[ADAM][1] and [avocado][2] are Spark-based alignment and variant calling tools under active development, but I do not believe they have wide adoption.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/bigdatagenomics/adam&#xD;&#xA;  [2]: https://github.com/bigdatagenomics/avocado&#xD;&#xA;  [3]: http://bdgenomics.org/" />
  <row Id="1344" PostHistoryTypeId="5" PostId="444" RevisionGUID="b981bfce-e6ad-4733-b7f0-8443f887ee9c" CreationDate="2017-06-04T21:17:09.970" UserId="462" Comment="added 87 characters in body" Text="[ADAM][1] and [avocado][2] are Spark-based alignment and variant calling tools under active development by a collaboration ([http://bdgenomics.org][3]) that also includes the Broad, but I do not believe they have wide adoption.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/bigdatagenomics/adam&#xD;&#xA;  [2]: https://github.com/bigdatagenomics/avocado&#xD;&#xA;  [3]: http://bdgenomics.org/" />
  <row Id="1345" PostHistoryTypeId="2" PostId="445" RevisionGUID="382faec8-fa37-4f0b-b57f-acfe76334f44" CreationDate="2017-06-04T22:35:10.487" UserId="73" Text="There's a new paper that's just appeared on the subject of &quot;proportionality&quot;, including a method by which RNA expression might be compared:&#xD;&#xA;&#xD;&#xA;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004075&#xD;&#xA;&#xD;&#xA;This is a new concept for me, and the article is not easy enough for me to read that I can write up a quick summary; the authors don't seem to devote a section in the paper to defining &quot;proportionality&quot;. However, here's an interesting chunk from the article:&#xD;&#xA;&#xD;&#xA;&gt; We graphed the network of relationships between these mRNAs (S5 Fig.), an approach similar to gene co-expression network [12] or weighted gene co-expression analysis [13] but founded on proportionality and therefore valid for relative data. The network revealed one cluster of 96, and many other smaller clusters of mRNAs behaving proportionally across conditions.  &#xD;&#xA;...  &#xD;&#xA;We are also keen to raise awareness that correlation (and other statistical methods that assume measurements come from real coordinate space) should not be applied to relative abundances. This is highly relevant to gene coexpression networks [12]. Correlation is at the heart of methods like Weighted Gene Co-expression Network Analysis [13] and heatmap visualization [14]. These methods are potentially misleading if applied to relative data.&#xD;&#xA;" />
  <row Id="1346" PostHistoryTypeId="2" PostId="446" RevisionGUID="fd0f8c4d-6d63-4e65-9c99-cfea84a23e31" CreationDate="2017-06-04T22:42:23.850" UserId="73" Text="If you have a lot of samples with very different environmental conditions, then a Weighted Gene Correlation Network Analysis (WGCNA) might be appropriate:&#xD;&#xA;&#xD;&#xA;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631488/&#xD;&#xA;&#xD;&#xA;This type of analysis looks for genes that trace similar (or opposing) expression patterns throughout different conditions (e.g. high-medium-medium-low-absent-high would be highly negatively correlated with low-medium-medium-high-very high-low).&#xD;&#xA;&#xD;&#xA;That particular paper introduces the concept of &quot;modules&quot;, which are groups of genes that share similar expression patterns. Functions are available for plotting how the expression of canonical module members changes throughout the different conditions, and for identifying which module (or modules) a particular gene is likely to be a member of.&#xD;&#xA;&#xD;&#xA;WGCNA works best when there's a lot of different expression changes in the different conditions, which sounds like it would fit well with your project. However, it concentrates more on the genes, rather than the conditions (which seems like it would be less useful for you)." />
  <row Id="1347" PostHistoryTypeId="2" PostId="447" RevisionGUID="1c4dd36e-eee9-4085-b6c2-31effafccc0f" CreationDate="2017-06-04T23:02:47.750" UserId="73" Text="There was a Structural Variant breakout session at the London Calling conference this year. Unfortunately I didn't attend that session, but MinION community members have access to Constance Donnell's summary of that:&#xD;&#xA;&#xD;&#xA;https://community.nanoporetech.com/posts/breakout-structural-varia&#xD;&#xA;&#xD;&#xA;Here are my attempts at grabbing non-creative chunks from those notes:&#xD;&#xA;&#xD;&#xA; - Professor Wigard Kloosterman has been developing a bioinformatics pipeline called NanoSV for mapping genomic structural variants in patients with congenital abnormalities&#xD;&#xA;&#xD;&#xA; - Tomas Sesani at the University of Utah studied the precise copy number of duplicated genes in individual virus genes over time, and tracked the SNP within copy number variable regions of the genome [no public tool mentioned]&#xD;&#xA;&#xD;&#xA; - Dr. Sudha Rao from Genotypic Technology in India used the MinION sequencer with Sniffles for structural variation calling&#xD;&#xA;&#xD;&#xA;ONT have said that the sessions/talks from London Calling 2017 will all be made publicly available at some point in the future." />
  <row Id="1348" PostHistoryTypeId="2" PostId="448" RevisionGUID="0c70858f-a317-4813-8d87-f1ebfdbce3a7" CreationDate="2017-06-04T23:11:39.370" UserId="492" Text="I cannot think of any principled rationale for choosing this filtering strategy. &#xD;&#xA;&#xD;&#xA;However, I am going to take a guess that this filtering strategy is supposed to filter out SNPs for which imputation did not work well?  In that case the appropriate statistic to filter on is the INFO score as described [here](https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html#data_summaries)." />
  <row Id="1349" PostHistoryTypeId="2" PostId="449" RevisionGUID="3812920b-2962-4584-bd68-12e39a902373" CreationDate="2017-06-04T23:25:12.173" UserId="73" Text="The ```samtools mpileup``` command has quite a neat feature that it is able to correct mapping errors associated with misalignment of INDELs. By default, the ```mpileup``` command will not work for reads that have more than 250X coverage of the reference genome. While this limit can be increased, very high coverage causes the mpileup program to grind to a halt, so it'd be nice to know if there's some easy way to make that faster.&#xD;&#xA;&#xD;&#xA;To add a bit more context, I've been doing this with mitochondrial genome reads that were extracted from both Illumina whole-genome sequencing (coverage ~1000X), and from targeted amplicon sequencing done on the IonTorrent (coverage up to ~4000X).&#xD;&#xA;&#xD;&#xA;I see that @rightskewed has mentioned the downsampling ability of samtools with ```samtools view -s &lt;float&gt;``` (see [here](https://bioinformatics.stackexchange.com/a/406/73)), which seems like it might work as a solution for this if used prior to the mpileup operation." />
  <row Id="1350" PostHistoryTypeId="1" PostId="449" RevisionGUID="3812920b-2962-4584-bd68-12e39a902373" CreationDate="2017-06-04T23:25:12.173" UserId="73" Text="How can I speed up INDEL calling/correction on BAM files?" />
  <row Id="1351" PostHistoryTypeId="3" PostId="449" RevisionGUID="3812920b-2962-4584-bd68-12e39a902373" CreationDate="2017-06-04T23:25:12.173" UserId="73" Text="&lt;bam&gt;&lt;samtools&gt;&lt;variant-calling&gt;&lt;indel&gt;" />
  <row Id="1352" PostHistoryTypeId="2" PostId="450" RevisionGUID="6fe69134-f331-461b-9886-a9461af68e0e" CreationDate="2017-06-04T23:34:21.633" UserId="73" Text="I wasn't aware of the samtools subsampling when I had this problem a couple of years ago, so ended up writing my own digital normalisation method to deal with *mapped* reads. This method reduces the genome coverage, but preserves reads where coverage is low.&#xD;&#xA;&#xD;&#xA;Because I was working with IonTorrent reads (which have variable length), I came up with the idea of selecting the longest read that mapped to each location in the genome (assuming such a read existed). This meant that the highly variable coverage for different samples (sometimes as low as 200X, sometimes as high as 4000X) was flattened out to a much more consistent coverage of about 100-200X. Here's the core of the code that I wrote:&#xD;&#xA;&#xD;&#xA;      if(($F[2] ne $seqName) || ($F[3] != $pos) || (length($bestSeq) &lt;= length($F[9]))){&#xD;&#xA;        if(length($bestSeq) == length($F[9])){&#xD;&#xA;          ## reservoir sampling with a reservoir size of 1&#xD;&#xA;          ## See https://en.wikipedia.org/wiki/Reservoir_sampling&#xD;&#xA;          ## * with probability 1/i, keep the new item instead of the current item&#xD;&#xA;          $seenCount++;&#xD;&#xA;          if(!rand($seenCount)){&#xD;&#xA;            ## i.e. if rand($seenCount) == 0, then continue with replacement&#xD;&#xA;            next;&#xD;&#xA;          }&#xD;&#xA;        } else {&#xD;&#xA;          $seenCount = 1;&#xD;&#xA;        }&#xD;&#xA;        if(($F[2] ne $seqName) || ($F[3] != $pos)){&#xD;&#xA;          if($output eq &quot;fastq&quot;){&#xD;&#xA;            printSeq($bestID, $bestSeq, $bestQual);&#xD;&#xA;          } elsif($output eq &quot;sam&quot;){&#xD;&#xA;            print($bestLine);&#xD;&#xA;          }&#xD;&#xA;        }&#xD;&#xA;        $seqName = $F[2];&#xD;&#xA;        $pos = $F[3];&#xD;&#xA;        $bestLine = $line;&#xD;&#xA;        $bestID = $F[0];&#xD;&#xA;        $bestFlags = $F[1];&#xD;&#xA;        $bestSeq = $F[9];&#xD;&#xA;        $bestQual = $F[10];&#xD;&#xA;&#xD;&#xA;The full code (which works as a filter on uncompressed SAM files) can be found [here](https://github.com/gringer/bioinfscripts/blob/master/sam2LongestBase.pl)." />
  <row Id="1353" PostHistoryTypeId="5" PostId="412" RevisionGUID="fbe5b5c8-a1da-4da9-a383-cc96a17ea0a2" CreationDate="2017-06-05T02:20:43.573" UserId="73" Comment="Added EBI/ENA ascp command line" Text="Proximity to NCBI may not necessarily give you the fastest transfer speed. AWS may be deliberately throttling the Internet connection to limit the likelihood that people will use it for undesirable things. There's a chance that a home network might be faster, but you're likely to get the fastest connection to NCBI by using an academic system that is linked to NCBI via a research network.&#xD;&#xA;&#xD;&#xA;Another possibility is using Aspera for downloads. This is unlikely to help if bandwidth is being throttled, but it might help if there's a bit of congestion through the regular methods:&#xD;&#xA;&#xD;&#xA;https://www.ncbi.nlm.nih.gov/public/&#xD;&#xA;&#xD;&#xA;NCBI also has an online [book][1] about best practises for downloading data from their servers.&#xD;&#xA;&#xD;&#xA;On a related note, just in case someone sees this and EBI/ENA is an option, there's a great guide for how to do file transfer using Aspera on the EBI web site:&#xD;&#xA;&#xD;&#xA;https://www.ebi.ac.uk/ena/browse/read-download#downloading_files_aspera&#xD;&#xA;&#xD;&#xA;&gt; Your command should look similar to this on Unix:&#xD;&#xA;&#xD;&#xA;    ascp -QT -l 300m -i &lt;aspera connect installation directory&gt;/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:&lt;file or files to download&gt; &lt;download location&gt;&#xD;&#xA;&#xD;&#xA;In my case, I've just started downloading some files from a MinION sequencing run. The estimated completion time via standard FTP was 12 hours for about 32GB of data; ```ascp``` has reduced that estimated download time to about an hour. Here's the command I used for downloading:&#xD;&#xA;&#xD;&#xA;    ascp -QT -l 300m -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:/vol1/ERA932/ERA932268/oxfordnanopore_native/20160804_Mock.tar.gz .&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/books/NBK51062/?report=reader" />
  <row Id="1354" PostHistoryTypeId="4" PostId="442" RevisionGUID="1b940ba2-8828-4af3-9c55-94d90c88f0e3" CreationDate="2017-06-05T02:22:40.943" UserId="73" Comment="added program name to question, removed from tag" Text="What causes the difference in total length of assembled contigs and scaffolds in SOAPdenovo2?" />
  <row Id="1355" PostHistoryTypeId="6" PostId="442" RevisionGUID="1b940ba2-8828-4af3-9c55-94d90c88f0e3" CreationDate="2017-06-05T02:22:40.943" UserId="73" Comment="added program name to question, removed from tag" Text="&lt;assembly&gt;&lt;scaffold&gt;" />
  <row Id="1356" PostHistoryTypeId="2" PostId="451" RevisionGUID="aa00ce56-8dd3-4d58-9c82-1dc4cc69c19c" CreationDate="2017-06-05T02:34:11.050" UserId="73" Text="It's possible that contigs that are being collapsed into &quot;heterozygous&quot; groups. This would be a particular problem when a genome has a substantial amount of repeated sequence. Digging deep into the supplementary information of the [SOAPdenovo2 paper][1], I've found the following information:&#xD;&#xA;&#xD;&#xA;&gt; In SOAPdenovo2, heterozygous contig pairs are recognized by utilizing the information of contig depth and the locality of contig. The recognized heterozygous contig pairs should obey the following rules: 1) the similarity between contigs should be high enough, for example, ≥ 95%; 2) the depth of both contigs should be near half of the average depth or all contigs, complying Poisson distribution; 3) the two contigs should be located adjacently in a scaffold and have no relationship to each other inferred by paired-end reads information. The normal contigs neighboring the heterozygous regions, if they exist, could be connected to both of the heterozygous contig pairs (H1 and H2). Only the contig with relatively higher depth in a heterozygous contig pair were kept for scaffolding. The method reduces the influence of genome heterozygosity on final scaffold length. All heterozygous contig pairs were outputted to a file to facilitate further analysis. **However, the trade-off of this method is that it might incorrectly remove paralogous contigs**. This problem could be relieved by a gap-filling procedure while the removed copy of paralogous contigs would be represented by gaps during scaffolding.&#xD;&#xA;&#xD;&#xA;  [1]: https://academic.oup.com/gigascience/article-lookup/doi/10.1186/2047-217X-1-18" />
  <row Id="1357" PostHistoryTypeId="2" PostId="452" RevisionGUID="905a6112-ed14-4289-82a4-7b217b677129" CreationDate="2017-06-05T04:50:53.453" UserId="73" Text="The Albertsen lab has recently put out a competition/challenge for read error correction:&#xD;&#xA;&#xD;&#xA;http://albertsenlab.org/can-you-beat-our-nanopore-read-error-correction-we-hope-so/&#xD;&#xA;&#xD;&#xA;I only found out about this today, and I think the conference where high-ranking participants were going to be mentioned has just finished, but the data is all public and there's no reason why this can't be continued in the future as a benchmarking test for nanopore base calling and/or read error correction.&#xD;&#xA;&#xD;&#xA;Nanopore reads (as called FAST5 files) can be found here:&#xD;&#xA;&#xD;&#xA;http://www.ebi.ac.uk/ena/data/view/PRJEB20906&#xD;&#xA;&#xD;&#xA;The initial called FASTQ files can be found here:&#xD;&#xA;&#xD;&#xA;https://www.dropbox.com/sh/cw8n7df1z61lkcj/AACc4ElVSefVfD5csnv0Klc_a?dl=0&#xD;&#xA;&#xD;&#xA;Reference sequences (from which the reads were generated) can be found here:&#xD;&#xA;&#xD;&#xA;https://www.dropbox.com/s/u6w993jrca05w8u/mockrRNAall.fasta?dl=0&#xD;&#xA;&#xD;&#xA;Every [passed] read should be a 2D nanopore read with the following sequence structure, where the decamer, **NNNNNNNNNN**, is the unique molecular identifier and is attached to common primer sites at the read extremities:&#xD;&#xA;&#xD;&#xA;&gt; ***AAAGATGAAGAT***–**NNNNNNNNNN** CGTACTAGACTTGCCTGTCGCTCTATCTTCTTTTTTTTTTTTTTTTTTTT  &#xD;&#xA;&lt;—- fragment of SSU cDNA molecule—-&gt;  &#xD;&#xA;GGGCAATATCAGCACCAACAGAAATAGATCGC**NNNNNNNNNN**–***ATGGATGAGTCT***&#xD;&#xA;&#xD;&#xA;So... what's the best consensus accuracy you can get from these reads? How did you get that accuracy?" />
  <row Id="1358" PostHistoryTypeId="1" PostId="452" RevisionGUID="905a6112-ed14-4289-82a4-7b217b677129" CreationDate="2017-06-05T04:50:53.453" UserId="73" Text="Improving consensus assembly from UMI-tagged nanopore reads" />
  <row Id="1359" PostHistoryTypeId="3" PostId="452" RevisionGUID="905a6112-ed14-4289-82a4-7b217b677129" CreationDate="2017-06-05T04:50:53.453" UserId="73" Text="&lt;minion&gt;&lt;benchmarking&gt;&lt;consensus&gt;" />
  <row Id="1360" PostHistoryTypeId="5" PostId="317" RevisionGUID="49507196-eed8-4024-9fe8-9921b55e2dcb" CreationDate="2017-06-05T06:30:45.717" UserId="131" Comment="added 62 characters in body" Text="There are many posts on the web regarding QC steps pre and post-imputation.&#xD;&#xA;&#xD;&#xA;Does applying below (new?) 10% [MAF](https://en.wikipedia.org/wiki/Minor_allele_frequency) difference rule make sense, pitfalls?&#xD;&#xA;&#xD;&#xA;Here is the process:&#xD;&#xA;&#xD;&#xA; 1. Get MAF for imputed set, using [SNPTEST](https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html) with flag `-summary_stats_only`&#xD;&#xA; 2. Convert imputed set to hard-calls using [gtools](http://www.well.ox.ac.uk/~cfreeman/software/gwas/gtool.html) with flag `--threshold 0.9`&#xD;&#xA; 3. If the MAF from step 1 and step 2 differs more than 10% than exclude the variant.&#xD;&#xA;&#xD;&#xA;More info:&#xD;&#xA;&#xD;&#xA; - GWAS is 50K vs 50K case control samples.&#xD;&#xA; - This step is applied after `info &gt; 0.4` filter.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1362" PostHistoryTypeId="5" PostId="426" RevisionGUID="52a54249-4261-47e6-993f-7de952bcda40" CreationDate="2017-06-05T10:13:33.140" UserId="292" Comment="Added link to spark website" Text="I learned that GATK 4 is using [Spark](https://spark.apache.org/) for parallelization. I googled around, though I am still not quite sure how spark really works and how to use it in practice. Besides GATK 4, are any other bioinformatics tool using spark? Generally, is spark widely used? Is it a necessary skill to learn? Thanks in advance." />
  <row Id="1363" PostHistoryTypeId="24" PostId="426" RevisionGUID="52a54249-4261-47e6-993f-7de952bcda40" CreationDate="2017-06-05T10:13:33.140" Comment="Proposed by 292 approved by 73, 57 edit id of 104" />
  <row Id="1364" PostHistoryTypeId="5" PostId="304" RevisionGUID="3e6c3e30-2e10-45aa-9365-81cb60e4bd72" CreationDate="2017-06-05T10:25:12.697" UserId="73" Comment="added ONT data release update" Text="Short answer: yes, but you need to get permission (and modified software) from ONT before doing that.&#xD;&#xA;&#xD;&#xA;... but that doesn't tell the whole story. This question has the potential to be very confusing, and that's through no fault of the questioner. The question that has been asked in the title is actually slightly different from what's being asked in the body of the question, and I think the &quot;title&quot; question is probably more relevant when considering the field applications of the MinION. The issue is that for the MinION, sequencing (or more specifically, generating the raw data in the form of an electrical signal trace) is distinct and separable from base calling. Many other sequencers also have distinct raw data and base-calling phases, but they're not democratised to the degree they are on the MinION.&#xD;&#xA;&#xD;&#xA;The &quot;sequencing&quot; part of MinION sequencing is carried out by ONT software, namely MinKNOW. As explained to me during PoreCampAU 2017, when the MinION is initially plugged into a computer it is missing the firmware necessary to carry out the sequencing. The most recent version of this firmware is usually downloaded at the start of a sequencing run by sending a request to ONT servers. In the usual case, you can't do sequencing without being able to access those servers, and you can't do sequencing without ONT knowing about it. However, ONT acknowledge that there are people out there who won't have Internet access when sequencing (e.g. sequencing Ebola in Africa, or metagenomic sequencing in the middle of the ocean), and an email to ```&lt;support@nanoporetech.com&gt;``` with reasons is likely to result in a quick software fix to the local sequencing problem.&#xD;&#xA;&#xD;&#xA;Once the raw signals are acquired, the &quot;base-calling&quot; part of MinION sequencing can be done anywhere. The ONT-maintained basecaller is Albacore, and this will get the first model updates whenever the sequencing technology is changed (which happens a lot). Albacore is a local basecaller which can be obtained from ONT by browsing through their community pages (available to anyone who has a MinION); ONT switched to only allowing people to do basecalling locally in about April 2017, after establishing that using AWS servers was just too expensive. Albacore is open source and free-as-in-beer, but has a restrictive licensing agreement which limits the distribution (and modification) of the program. However, Albacore is not the only available basecaller. ONT provide a FOSS basecaller called [nanonet][1]. It's a little bit behind Albacore on technology, but ONT have said that all useful Albacore changes will eventually propagate through to nanonet. There is another non-ONT basecaller that I'm aware of which uses a neural network for basecalling: [deepnano][2]. Other basecallers exist, each varying distances away technology-wise, and I expect that more will appear in the future as the technology stabilises and more change-resistant computer scientists get in on the act.&#xD;&#xA;&#xD;&#xA;Edit: ONT has just pulled back the curtain on their basecalling software; all the repositories that I've looked at so far (except for the Cliveome) have been released under the Mozilla Public License (free and open source, with some conditions and limitations). Included in that software repository is Scrappie, which is their testing / bleeding-edge version of Albacore.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/nanoporetech/nanonet&#xD;&#xA;  [2]: https://bitbucket.org/vboza/deepnano" />
  <row Id="1365" PostHistoryTypeId="5" PostId="450" RevisionGUID="eaa6fb0f-0eae-46d6-8572-d71c4f48c27f" CreationDate="2017-06-05T10:26:20.287" UserId="73" Comment="clarified as Perl code" Text="I wasn't aware of the samtools subsampling when I had this problem a couple of years ago, so ended up writing my own digital normalisation method to deal with *mapped* reads. This method reduces the genome coverage, but preserves reads where coverage is low.&#xD;&#xA;&#xD;&#xA;Because I was working with IonTorrent reads (which have variable length), I came up with the idea of selecting the longest read that mapped to each location in the genome (assuming such a read existed). This meant that the highly variable coverage for different samples (sometimes as low as 200X, sometimes as high as 4000X) was flattened out to a much more consistent coverage of about 100-200X. Here's the core of the Perl code that I wrote:&#xD;&#xA;&#xD;&#xA;      if(($F[2] ne $seqName) || ($F[3] != $pos) || (length($bestSeq) &lt;= length($F[9]))){&#xD;&#xA;        if(length($bestSeq) == length($F[9])){&#xD;&#xA;          ## reservoir sampling with a reservoir size of 1&#xD;&#xA;          ## See https://en.wikipedia.org/wiki/Reservoir_sampling&#xD;&#xA;          ## * with probability 1/i, keep the new item instead of the current item&#xD;&#xA;          $seenCount++;&#xD;&#xA;          if(!rand($seenCount)){&#xD;&#xA;            ## i.e. if rand($seenCount) == 0, then continue with replacement&#xD;&#xA;            next;&#xD;&#xA;          }&#xD;&#xA;        } else {&#xD;&#xA;          $seenCount = 1;&#xD;&#xA;        }&#xD;&#xA;        if(($F[2] ne $seqName) || ($F[3] != $pos)){&#xD;&#xA;          if($output eq &quot;fastq&quot;){&#xD;&#xA;            printSeq($bestID, $bestSeq, $bestQual);&#xD;&#xA;          } elsif($output eq &quot;sam&quot;){&#xD;&#xA;            print($bestLine);&#xD;&#xA;          }&#xD;&#xA;        }&#xD;&#xA;        $seqName = $F[2];&#xD;&#xA;        $pos = $F[3];&#xD;&#xA;        $bestLine = $line;&#xD;&#xA;        $bestID = $F[0];&#xD;&#xA;        $bestFlags = $F[1];&#xD;&#xA;        $bestSeq = $F[9];&#xD;&#xA;        $bestQual = $F[10];&#xD;&#xA;&#xD;&#xA;The full code (which works as a filter on uncompressed SAM files) can be found [here](https://github.com/gringer/bioinfscripts/blob/master/sam2LongestBase.pl)." />
  <row Id="1366" PostHistoryTypeId="5" PostId="451" RevisionGUID="c02d5640-278f-4042-8873-9490fdc62707" CreationDate="2017-06-05T10:36:44.423" UserId="73" Comment="clarify as not the only answer" Text="Something else that could be happening is that contigs that are being collapsed into &quot;heterozygous&quot; groups. This would be a particular problem when a genome has a substantial amount of repeated sequence. Digging deep into the supplementary information of the [SOAPdenovo2 paper][1], I've found the following information:&#xD;&#xA;&#xD;&#xA;&gt; In SOAPdenovo2, heterozygous contig pairs are recognized by utilizing the information of contig depth and the locality of contig. The recognized heterozygous contig pairs should obey the following rules: 1) the similarity between contigs should be high enough, for example, ≥ 95%; 2) the depth of both contigs should be near half of the average depth or all contigs, complying Poisson distribution; 3) the two contigs should be located adjacently in a scaffold and have no relationship to each other inferred by paired-end reads information. The normal contigs neighboring the heterozygous regions, if they exist, could be connected to both of the heterozygous contig pairs (H1 and H2). Only the contig with relatively higher depth in a heterozygous contig pair were kept for scaffolding. The method reduces the influence of genome heterozygosity on final scaffold length. All heterozygous contig pairs were outputted to a file to facilitate further analysis. **However, the trade-off of this method is that it might incorrectly remove paralogous contigs**. This problem could be relieved by a gap-filling procedure while the removed copy of paralogous contigs would be represented by gaps during scaffolding.&#xD;&#xA;&#xD;&#xA;  [1]: https://academic.oup.com/gigascience/article-lookup/doi/10.1186/2047-217X-1-18" />
  <row Id="1367" PostHistoryTypeId="2" PostId="453" RevisionGUID="61016ca8-0fcf-4da7-8986-d14ee7e12201" CreationDate="2017-06-05T12:49:24.763" UserId="29" Text="This has come up repeatedly recently: I have a very large text file (in the order of several GiB) and I need to perform line-based subsetting for around 10,000 lines. There exist solutions for specific scenarios (e.g. `samtools view -s` for randomly sampling BAM files) but sometimes my use-case doesn’t fit into these categories.&#xD;&#xA;&#xD;&#xA;Unfortunately a naïve `sed`-based solution is *extremely* slow:&#xD;&#xA;&#xD;&#xA;    time sed -n -f &lt;(awk -vOFS='' '{print $0, &quot;p&quot;}' lines.txt) input_file &gt; selected_lines.txt&#xD;&#xA;&#xD;&#xA;Forget running this for 10,000 lines; it’s already grinding to a halt for a mere 1000.&#xD;&#xA;&#xD;&#xA;How can I speed this up, ideally so that it scales only with the size of the input file, and has more or less constant runtime n the number of lines that I subset?" />
  <row Id="1368" PostHistoryTypeId="1" PostId="453" RevisionGUID="61016ca8-0fcf-4da7-8986-d14ee7e12201" CreationDate="2017-06-05T12:49:24.763" UserId="29" Text="How do I efficiently subset a very large line-based file?" />
  <row Id="1369" PostHistoryTypeId="3" PostId="453" RevisionGUID="61016ca8-0fcf-4da7-8986-d14ee7e12201" CreationDate="2017-06-05T12:49:24.763" UserId="29" Text="&lt;shell&gt;&lt;subset&gt;&lt;text&gt;" />
  <row Id="1370" PostHistoryTypeId="2" PostId="454" RevisionGUID="99f3b1b4-89f3-43b0-8be6-08addf551d98" CreationDate="2017-06-05T12:49:24.763" UserId="29" Text="Turns out, simply keeping track of the next candidate line (after sorting the sample line numbers) fixes the performance issue, and most of the remaining slowness seems to be due to the overhead of actually reading the file so there’s not very much to improve.&#xD;&#xA;&#xD;&#xA;Since I don’t know how how to do this in `sed`, and it’s not trivial in `awk` either, here’s a Perl script:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-perl --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $file = $ARGV[0];&#xD;&#xA;    my $lines_file = $ARGV[1];&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    chomp (my @lines = &lt;$lines_fh&gt;);&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    @lines = sort {$a &lt;=&gt; $b} @lines;&#xD;&#xA;    &#xD;&#xA;    open my $fh, '&lt;', $file or die &quot;Cannot read file $file&quot;;&#xD;&#xA;    my $line = 1;&#xD;&#xA;    my $next_line = 0;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;        last if $next_line == scalar @lines;&#xD;&#xA;        if ($line++ == $lines[$next_line]) {&#xD;&#xA;            $next_line++;&#xD;&#xA;            print;&#xD;&#xA;        }&#xD;&#xA;    }&#xD;&#xA;    close $fh;&#xD;&#xA;&#xD;&#xA;For an [R package](https://github.com/HannahVMeyer/PhenotypeSimulator), I’ve implemented a [similar function in C++](https://gist.github.com/klmr/a3e88c0ce102be0d3b2ce52ae83825e5) that’s only slightly longer than the Perl script. It is ~3 times faster than the Perl script on my test file." />
  <row Id="1371" PostHistoryTypeId="5" PostId="453" RevisionGUID="31a006d5-85eb-4902-8761-78ede2a2448b" CreationDate="2017-06-05T12:59:13.543" UserId="29" Comment="clarification" Text="This has come up repeatedly recently: I have a very large text file (in the order of several GiB) and I need to perform line-based subsetting for around 10,000 lines. There exist solutions for specific scenarios (e.g. `samtools view -s` for randomly sampling BAM files) but sometimes my use-case doesn’t fit into these categories.&#xD;&#xA;&#xD;&#xA;Unfortunately a naïve `sed`-based solution is *extremely* slow:&#xD;&#xA;&#xD;&#xA;    time sed -n -f &lt;(awk -vOFS='' '{print $0, &quot;p&quot;}' line_numbers.txt) input_file &gt; selected_lines.txt&#xD;&#xA;&#xD;&#xA;Where `line_numbers.txt` is a file containing one line number per line.&#xD;&#xA;&#xD;&#xA;Forget running this for 10,000 lines; it’s already grinding to a halt for a mere 1000.&#xD;&#xA;&#xD;&#xA;How can I speed this up, ideally so that it scales only with the size of the input file, and has more or less constant runtime n the number of lines that I subset?" />
  <row Id="1372" PostHistoryTypeId="6" PostId="422" RevisionGUID="1e4c6003-a87a-4f42-a2b2-23c1909b499d" CreationDate="2017-06-05T13:19:33.923" UserId="73" Comment="got the NGS tag wrong" Text="&lt;ngs&gt;&lt;terminology&gt;&lt;sbs&gt;&lt;hts&gt;" />
  <row Id="1373" PostHistoryTypeId="2" PostId="455" RevisionGUID="26ced694-dab0-4011-8200-bdf4a0dfd498" CreationDate="2017-06-05T14:17:42.013" UserId="292" Text="Some related questions appear in other sites, with potentially interesting solutions, which I report here:&#xD;&#xA;&#xD;&#xA;To sample approximately 1% of the non-empty lines:&#xD;&#xA;&#xD;&#xA;    awk 'BEGIN {srand()} !/^$/ { if (rand() &lt;= .01) print $0}' input_file&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(from &lt;https://stackoverflow.com/a/692321/1878788&gt;)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;To select 1000 random lines:&#xD;&#xA;&#xD;&#xA;    shuf -n 1000 input_file&#xD;&#xA;&#xD;&#xA;(from &lt;https://stackoverflow.com/a/15065490/1878788&gt;, and &lt;https://unix.stackexchange.com/a/108604/55127&gt;)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1375" PostHistoryTypeId="2" PostId="456" RevisionGUID="c9d2d894-f903-4cc0-91d7-5a855c9df1a9" CreationDate="2017-06-05T16:36:54.550" UserId="573" Text="We are designing a CRISPR/Cas9 experiment and thinking of the down-stream data analyses. &#xD;&#xA;&#xD;&#xA;Are there any R packages for analysing raw NGS read count data from pooled genetic screens using CRIPSR/Cas9 to disrupt gene expression in a population of cells? I guess we will need to start with basics, i.e. sequences processing, data exploration, visualisation etc. &#xD;&#xA;" />
  <row Id="1376" PostHistoryTypeId="1" PostId="456" RevisionGUID="c9d2d894-f903-4cc0-91d7-5a855c9df1a9" CreationDate="2017-06-05T16:36:54.550" UserId="573" Text="R packages for data analyses of pooled CRISPR screens" />
  <row Id="1377" PostHistoryTypeId="3" PostId="456" RevisionGUID="c9d2d894-f903-4cc0-91d7-5a855c9df1a9" CreationDate="2017-06-05T16:36:54.550" UserId="573" Text="&lt;r&gt;&lt;crispr-cas9&gt;" />
  <row Id="1378" PostHistoryTypeId="2" PostId="457" RevisionGUID="d9c06c41-6829-45f6-8a7a-ece0abc10e1a" CreationDate="2017-06-05T16:51:28.363" UserId="467" Text="Is it possible to draw e.g. 95% confidence ellipses around samples from the same group on the results from the plotMDS function under edgeR? If so, how?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1379" PostHistoryTypeId="1" PostId="457" RevisionGUID="d9c06c41-6829-45f6-8a7a-ece0abc10e1a" CreationDate="2017-06-05T16:51:28.363" UserId="467" Text="confidence ellipses for MDS plot in edgeR?" />
  <row Id="1380" PostHistoryTypeId="3" PostId="457" RevisionGUID="d9c06c41-6829-45f6-8a7a-ece0abc10e1a" CreationDate="2017-06-05T16:51:28.363" UserId="467" Text="&lt;r&gt;&lt;edger&gt;" />
  <row Id="1381" PostHistoryTypeId="2" PostId="458" RevisionGUID="d948db51-9691-4f23-865c-91f76d0a6fe9" CreationDate="2017-06-05T17:15:39.767" UserId="375" Text="We have heard in the group that it is important to keep track of and to filter artifact regions when analysing data from functional genomics experiments, especially ChIP-seq. &#xD;&#xA;&#xD;&#xA;Here, we have seen pipelines that remove the ENCODE tracks i) before cross-correlations QCs, ii) after cross-correlations QC but before peak calling and iii) after peak calling. &#xD;&#xA;&#xD;&#xA;We have noticed that removal of the tracks does not affect significantly cross-correlation and peak-independent QCs. However, we are not sure whether peak calling should be done on the filtered tracks or not?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1382" PostHistoryTypeId="1" PostId="458" RevisionGUID="d948db51-9691-4f23-865c-91f76d0a6fe9" CreationDate="2017-06-05T17:15:39.767" UserId="375" Text="When to account for the blacklisted genomic regions in ChIP-seq data analyses?" />
  <row Id="1383" PostHistoryTypeId="3" PostId="458" RevisionGUID="d948db51-9691-4f23-865c-91f76d0a6fe9" CreationDate="2017-06-05T17:15:39.767" UserId="375" Text="&lt;chip-seq&gt;" />
  <row Id="1384" PostHistoryTypeId="5" PostId="455" RevisionGUID="c4b25895-c1aa-4bae-b3d6-8333ffa22403" CreationDate="2017-06-05T17:37:39.233" UserId="292" Comment="Added python solutions to the case when selection on line numbers" Text="Some related questions appear in other sites, with potentially interesting solutions, which I report here:&#xD;&#xA;&#xD;&#xA;To sample approximately 1% of the non-empty lines:&#xD;&#xA;&#xD;&#xA;    awk 'BEGIN {srand()} !/^$/ { if (rand() &lt;= .01) print $0}' input_file&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(from &lt;https://stackoverflow.com/a/692321/1878788&gt;)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;To select 1000 random lines:&#xD;&#xA;&#xD;&#xA;    shuf -n 1000 input_file&#xD;&#xA;&#xD;&#xA;(from &lt;https://stackoverflow.com/a/15065490/1878788&gt;, and &lt;https://unix.stackexchange.com/a/108604/55127&gt;)&#xD;&#xA;&#xD;&#xA;### Edit: Python solutions using a list of lines&#xD;&#xA;&#xD;&#xA;Using a set of line indices and selecting lines by testing set membership:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python3&#xD;&#xA;    &#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    with open(sys.argv[2], &quot;r&quot;) as line_numbers_file:&#xD;&#xA;        line_indices = set(int(line.strip()) - 1 for line in line_numbers_file)&#xD;&#xA;    &#xD;&#xA;    with open(sys.argv[1], &quot;r&quot;) as input_file:&#xD;&#xA;        print(*(line.strip() for (idx, line) in enumerate(input_file)&#xD;&#xA;                if idx in line_indices), sep=&quot;\n&quot;)&#xD;&#xA;&#xD;&#xA;Using a numpy boolean array together with `itertools.compress`:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python3&#xD;&#xA;    &#xD;&#xA;    import sys&#xD;&#xA;    from itertools import compress&#xD;&#xA;    from numpy import zeros&#xD;&#xA;    &#xD;&#xA;    with open(sys.argv[2], &quot;r&quot;) as line_numbers_file:&#xD;&#xA;        line_indices = [int(line.strip()) - 1 for line in line_numbers_file]&#xD;&#xA;    &#xD;&#xA;    selector = zeros(max(line_indices) + 1, dtype=bool)&#xD;&#xA;    selector[line_indices] = 1&#xD;&#xA;    &#xD;&#xA;    with open(sys.argv[1], &quot;r&quot;) as input_file:&#xD;&#xA;        print(*(line.strip() for line in compress(input_file, selector)), sep=&quot;\n&quot;)&#xD;&#xA;&#xD;&#xA;I did some tests on a file containing 15774756 sam records and a list of 10000 pre-generated random line numbers.&#xD;&#xA;&#xD;&#xA;The perl script proposed by Konrad Rudolph (&lt;https://bioinformatics.stackexchange.com/a/454/292&gt;) runs in about 5.3 seconds.&#xD;&#xA;&#xD;&#xA;The set membership testing python solution runs in about 4.45 seconds.&#xD;&#xA;&#xD;&#xA;The compress based solution runs in about 3.4 seconds.&#xD;&#xA;I suspect this may vary a lot depending on the highest line number we want, since the number of iterations will depend on the length of the boolean array. Here the highest line number was 15773768, so pretty high compared with the total number of lines.&#xD;&#xA;&#xD;&#xA;I tried with python 3.6. I suspect that python 2.7 could be slightly faster, but haven't tested." />
  <row Id="1385" PostHistoryTypeId="2" PostId="459" RevisionGUID="52f7ef75-df8b-4d64-a4be-b58c6a82a6e9" CreationDate="2017-06-05T17:41:43.290" UserId="77" Text="Aside: Cross-correlation is largely meaningless, regardless of what some of the ENCODE folks might argue. When we process our DEEP samples we don't even look at that value.&#xD;&#xA;&#xD;&#xA;Regardless, if you're using SPP/phantomPeakQual for cross-correlation then note that it already removes the highest peaks from your dataset before computing the cross-correlation (in fact, it can remove most of the actual peaks too, which makes one further wonder what it's actually telling you). I don't know that this is actually documented anywhere, it's something I noticed when going through the code while pondering whether to implement it in deepTools. But at least it's ignoring these regions already :)&#xD;&#xA;&#xD;&#xA;In general, it's most convenient to just remove peaks overlapping blacklisted regions. In an ideal world you'd filter out the blacklisted reads before peak calling, but (1) this is really inconvenient (more time and disk required) and (2) I've never seen an appreciable gain in peak calling performance. In theory at least you should be losing sensitivity right around blacklisted regions if you don't remove reads overlapping blacklisted regions, but you have to ask yourself whether you want to trust such peaks anyway. For other QC steps, at least with deepTools we provide a parameter with every tool to specify a BED file of blacklisted regions to skip.&#xD;&#xA;&#xD;&#xA;As an aside, there are many fewer blacklisted regions in more recent genome builds (GRCh38 and GRCm38 at least), so this is less of an issue in general with them." />
  <row Id="1386" PostHistoryTypeId="2" PostId="460" RevisionGUID="53675906-9926-4d4e-ad23-70eef34fd1cb" CreationDate="2017-06-05T17:42:37.170" UserId="375" Text="Regarding the first part of the question: scRNA-seq is a rapidly developing field so it may be hard to talk about &quot;widely accepted tool for doing pseudo-temporal ordering from scRNAseq data&quot;. Few of the tools aiming to do just that include Monocole, Waterfall or Sincell (see this paper for references &lt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4122333/&gt;)&#xD;&#xA;&#xD;&#xA;The second part is a bit more complex. Many aspects have come together here, e.g. cell-cycles, sub-populaitons identification and pseudo-temporal ordering, to get a true reflection of the biological processes. There are efforts on all (and more) these fronts (see again e.g. the above paper, though not the latest) and there are most likely people working on their integration. I'm not aware of any studies published yet at this depth though&#xD;&#xA;" />
  <row Id="1387" PostHistoryTypeId="2" PostId="461" RevisionGUID="2fdece85-fde1-41c4-aec2-fa1357f713b7" CreationDate="2017-06-05T17:43:34.723" UserId="35" Text="I am not sure how appropriate it is at this point to still refer to sequencing as next-generation sequencing. The leading NGS technology is Illumina/Solexa that has been around for over 10 years at this point. 454 was around even earlier. It's not really &quot;next&quot; at this point.&#xD;&#xA;&#xD;&#xA;Opinions aside, I would also add &quot;third-generation sequencing&quot; to that list, referring to long read technologies like PacBio and Oxford Nanopore. See [this question on Biology SE][1] for more details.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://biology.stackexchange.com/questions/21080/what-is-the-difference-between-second-and-third-generation-sequencing" />
  <row Id="1388" PostHistoryTypeId="2" PostId="462" RevisionGUID="f36e406d-8ce2-4760-857b-ef6271062467" CreationDate="2017-06-05T18:06:35.393" UserId="589" Text="We have whole genome sequencing data for patients (not-cancer) (n=60) and for healthy controls (n=20). The sequencing centre has provided us with the best practice bioinformatics analyses including reads mapping (.BAM) and variant calling using GATK (.vcf) as well as annotation (annotated .vcf and .gVCF). &#xD;&#xA;&#xD;&#xA;What should be our next steps? We are interested to see if there are any differences (global and/or specific) between the groups. " />
  <row Id="1389" PostHistoryTypeId="1" PostId="462" RevisionGUID="f36e406d-8ce2-4760-857b-ef6271062467" CreationDate="2017-06-05T18:06:35.393" UserId="589" Text="How to compare groups using WGS data?" />
  <row Id="1390" PostHistoryTypeId="3" PostId="462" RevisionGUID="f36e406d-8ce2-4760-857b-ef6271062467" CreationDate="2017-06-05T18:06:35.393" UserId="589" Text="&lt;wgs&gt;" />
  <row Id="1391" PostHistoryTypeId="5" PostId="360" RevisionGUID="1a389731-fec5-454e-837b-8583ec2282b4" CreationDate="2017-06-05T19:06:04.200" UserId="425" Comment="A big update of the whole answer" Text="**TL;DR**&#xD;&#xA;&#xD;&#xA;SH adheres to an official industry standard, but it is not suitable for scientific computing. BASH is considered an informal standard (e.g., by Google). BASH 3 is preferable in most of situations in the world of bioinformatics.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Long answer**&#xD;&#xA;&#xD;&#xA;As already described in other answers, SH (`/bin/sh`) should fully adhere to POSIX which is a real industry standard. However, SH is too limited for scientific computing since many key features were incorporated later in SH successors, especially in BASH (`/bin/bash`): `set -o pipefail`, `[[ ... ]]`, or process substitutions `&lt; ()` to name at least few.&#xD;&#xA;&#xD;&#xA;In practice, it is much more difficult to write &quot;safe&quot; scripts in pure SH and only shell experts are usually capable to prevent unexpected behavior. For example, it may be hard to ensure that no command in a pipeline failed in the middle of computation. For BASH, various easy-to-follow defensive-programming recommendations have been developed and they should prevent all these problems. From this reason, many computer scientists, software engineers and companies use BASH as a kind of a standard. For instance, Google internal policy allows [only BASH](https://google.github.io/styleguide/shell.xml?showone=Which_Shell_to_Use#Which_Shell_to_Use) for writing any shell scripts. &#xD;&#xA;&#xD;&#xA;Even though we cannot expect that BASH is present on completely every Unix machine (e.g., on mobile devices as @terdon pointed out), a vast majority of *nix machines used for scientific computation should have it. We should be also aware of the fact that BASH can be slower than SH and that it has recently suffered from  major [security issues](https://www.symantec.com/connect/blogs/shellshock-all-you-need-know-about-bash-bug-vulnerability). Moreover, various BASH versions exist and scripts working on modern Linux machines with [BASH 4](http://wiki.bash-hackers.org/bash4) might not work on OS X, which is still based on BASH 3. &#xD;&#xA;&#xD;&#xA;To sum up, BASH 3 is probably be the most reasonable choice for scientific computing.&#xD;&#xA;&#xD;&#xA;**Edit**&#xD;&#xA;&#xD;&#xA;I addressed comments from @terdon and @John Marschall. In particular, I added an explanation why I believe that BASH is more suitable for scientific computing than SH." />
  <row Id="1392" PostHistoryTypeId="5" PostId="360" RevisionGUID="e79422bf-e76b-455e-86f5-d3805fadbea8" CreationDate="2017-06-05T19:11:19.653" UserId="425" Comment="added 4 characters in body" Text="**TL;DR**&#xD;&#xA;&#xD;&#xA;SH adheres to an official industry standard, but it is not suitable for scientific computing. BASH is considered an informal standard (e.g., by Google). BASH 3 is preferable in most of situations in the world of bioinformatics.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Long answer**&#xD;&#xA;&#xD;&#xA;As already described in other answers, SH (`/bin/sh`) should fully adhere to POSIX which is a real industry standard. However, SH is too limited for scientific computing since many key features were incorporated later in SH successors, especially in BASH (`/bin/bash`): `set -o pipefail`, `[[ ... ]]`, or process substitutions `&lt; ()` to name at least few.&#xD;&#xA;&#xD;&#xA;In practice, it is much more difficult to write &quot;safe&quot; scripts in pure SH and only shell experts are usually capable to prevent unexpected behavior. For example, it may be hard to ensure that no command in a pipeline failed in the middle of computation. For BASH, various easy-to-follow defensive-programming recommendations have been developed and they should prevent all these problems. From this reason, many computer scientists, software engineers and companies use BASH as a kind of a standard. For instance, Google internal policy allows [only BASH](https://google.github.io/styleguide/shell.xml?showone=Which_Shell_to_Use#Which_Shell_to_Use) for writing any shell scripts. &#xD;&#xA;&#xD;&#xA;Even though we cannot expect that BASH is present on completely every Unix machine (e.g., on mobile devices as @terdon pointed out), a vast majority of *nix machines used for scientific computation should have it. We should be also aware of the fact that BASH can be slower than SH and that it has recently suffered from  major [security issues](https://www.symantec.com/connect/blogs/shellshock-all-you-need-know-about-bash-bug-vulnerability). Moreover, various BASH versions exist and scripts working on modern Linux machines with [BASH 4](http://wiki.bash-hackers.org/bash4) might not work on OS X, which is still based on BASH 3. &#xD;&#xA;&#xD;&#xA;To sum up, BASH 3 is probably be the most reasonable choice for scientific computing.&#xD;&#xA;&#xD;&#xA;**Edit**&#xD;&#xA;&#xD;&#xA;I addressed the comments from @terdon and @John Marschall. In particular, I added an explanation why I believe that BASH is more suitable for scientific computing than SH." />
  <row Id="1393" PostHistoryTypeId="5" PostId="360" RevisionGUID="7b7ab2e1-9890-4138-b6df-69962bc3c7db" CreationDate="2017-06-05T19:16:50.987" UserId="425" Comment="Full names" Text="**TL;DR**&#xD;&#xA;&#xD;&#xA;SH adheres to an official industry standard, but it is not suitable for scientific computing. BASH is considered an informal standard (e.g., by Google). BASH 3 is preferable in most of situations in the world of bioinformatics.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Long answer**&#xD;&#xA;&#xD;&#xA;As already described in other answers, SH (`/bin/sh`, plain Bourne shell, the original UNIX shell) should fully adhere to POSIX which is a real industry standard. However, SH is too limited for scientific computing since many key features were incorporated later in SH successors, especially in BASH (`/bin/bash`, Bourne Again Shell): `set -o pipefail`, `[[ ... ]]`, or process substitutions `&lt; ()` to name at least few.&#xD;&#xA;&#xD;&#xA;In practice, it is much more difficult to write &quot;safe&quot; scripts in pure SH and only shell experts are usually capable to prevent unexpected behavior. For example, it may be hard to ensure that no command in a pipeline failed in the middle of computation. For BASH, various easy-to-follow defensive-programming recommendations have been developed and they should prevent all these problems. From this reason, many computer scientists, software engineers and companies use BASH as a kind of a standard. For instance, Google internal policy allows [only BASH](https://google.github.io/styleguide/shell.xml?showone=Which_Shell_to_Use#Which_Shell_to_Use) for writing any shell scripts. &#xD;&#xA;&#xD;&#xA;Even though we cannot expect that BASH is present on completely every Unix machine (e.g., on mobile devices as @terdon pointed out), a vast majority of *nix machines used for scientific computation should have it. We also should be aware of the fact that BASH can be slower than SH and that it has recently suffered from  major [security issues](https://www.symantec.com/connect/blogs/shellshock-all-you-need-know-about-bash-bug-vulnerability). Moreover, various BASH versions exist and scripts working on modern Linux machines with [BASH 4](http://wiki.bash-hackers.org/bash4) might not work on OS X, which is still based on BASH 3. &#xD;&#xA;&#xD;&#xA;To sum up, BASH 3 is probably be the most reasonable choice for scientific computing.&#xD;&#xA;&#xD;&#xA;**Edit**&#xD;&#xA;&#xD;&#xA;I addressed the comments from @terdon and @John Marshall. In particular, I added an explanation why BASH is more suitable for scientific computing than SH (in my opinion)." />
  <row Id="1394" PostHistoryTypeId="2" PostId="463" RevisionGUID="f4525a03-23a8-4141-bc94-741f98d0184e" CreationDate="2017-06-05T19:19:33.367" UserId="-1" Text="" />
  <row Id="1395" PostHistoryTypeId="1" PostId="463" RevisionGUID="f4525a03-23a8-4141-bc94-741f98d0184e" CreationDate="2017-06-05T19:19:33.367" UserId="-1" />
  <row Id="1396" PostHistoryTypeId="2" PostId="464" RevisionGUID="280476e1-af93-40d2-81fb-cc600bf5ece4" CreationDate="2017-06-05T19:19:33.367" UserId="-1" Text="" />
  <row Id="1397" PostHistoryTypeId="1" PostId="464" RevisionGUID="280476e1-af93-40d2-81fb-cc600bf5ece4" CreationDate="2017-06-05T19:19:33.367" UserId="-1" />
  <row Id="1398" PostHistoryTypeId="6" PostId="453" RevisionGUID="cdea3c0f-e420-4123-a0e7-0cb8fce86031" CreationDate="2017-06-05T19:44:59.737" UserId="73" Comment="added &quot;benchmarking&quot; tag because this is a &quot;which is fastest&quot; question" Text="&lt;shell&gt;&lt;benchmarking&gt;&lt;subset&gt;&lt;text&gt;" />
  <row Id="1399" PostHistoryTypeId="2" PostId="465" RevisionGUID="91516eb0-dff0-4450-a394-e3b4097f5bd5" CreationDate="2017-06-05T19:54:57.300" UserId="73" Text="Perl should be fairly fast with this when using a hash set to store the list of lines:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    my %include_lines = ();&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    while(&lt;$lines_fh&gt;){&#xD;&#xA;      chomp;&#xD;&#xA;      %include_lines{$_} = 1;&#xD;&#xA;    }&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    while(&lt;&gt;){&#xD;&#xA;      if($include_lines{$.}){ # &quot;$.&quot; -- line number of current file&#xD;&#xA;        print;&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Note that according to [this SO answer](https://stackoverflow.com/questions/5920686/how-to-get-the-current-line-number-of-a-file-open-using-perl#comment6821096_5920709), the &quot;$.&quot; operator is not strictly the current line number, and can be influenced by different file operations or other settings." />
  <row Id="1400" PostHistoryTypeId="5" PostId="465" RevisionGUID="0796184a-df17-482b-957a-cbf39d75e678" CreationDate="2017-06-05T20:02:38.157" UserId="73" Comment="added beginning of an alternative answer" Text="Perl should be fairly fast with this when using a hash set to store the list of lines:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    my %include_lines = ();&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    while(&lt;$lines_fh&gt;){&#xD;&#xA;      chomp;&#xD;&#xA;      %include_lines{$_} = 1;&#xD;&#xA;    }&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    while(&lt;&gt;){&#xD;&#xA;      if($include_lines{$.}){ # &quot;$.&quot; -- line number of current file&#xD;&#xA;        print;&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Note that according to [this SO answer](https://stackoverflow.com/questions/5920686/how-to-get-the-current-line-number-of-a-file-open-using-perl#comment6821096_5920709), the &quot;$.&quot; operator is not strictly the current line number, and can be influenced by different file operations or other settings.&#xD;&#xA;&#xD;&#xA;Edit: just saw your comment about speed in your answer, comparing hash sets to a sorted list. The ```$lines[$next_line]``` bit feels a bit odd to me. Have you tried out using ```shift``` or ```pop``` on a sorted list to fetch the next line?" />
  <row Id="1401" PostHistoryTypeId="5" PostId="465" RevisionGUID="9363248f-1237-428e-b4eb-d0fc72bebff1" CreationDate="2017-06-05T20:10:50.013" UserId="73" Comment="added beginning of an alternative answer" Text="Perl should be fairly fast with this when using a hash set to store the list of lines. A structure like this also works for subsetting based on a field value, where the comparison would be with the field rather than &quot;$.&quot;:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    my %include_lines = ();&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    while(&lt;$lines_fh&gt;){&#xD;&#xA;      chomp;&#xD;&#xA;      %include_lines{$_} = 1;&#xD;&#xA;    }&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    while(&lt;&gt;){&#xD;&#xA;      if($include_lines{$.}){ # &quot;$.&quot; -- line number of current file&#xD;&#xA;        print;&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Note that according to [this SO answer](https://stackoverflow.com/questions/5920686/how-to-get-the-current-line-number-of-a-file-open-using-perl#comment6821096_5920709), the &quot;$.&quot; operator is not strictly the current line number, and can be influenced by different file operations or other settings.&#xD;&#xA;&#xD;&#xA;Edit: just saw your comment about speed in your answer, comparing hash sets to a sorted list. The ```$lines[$next_line]``` bit feels a bit odd to me. Have you tried out using ```shift``` or ```pop``` on a sorted list to fetch the next line:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    chomp (my @lines = &lt;$lines_fh&gt;);&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    @lines = sort {$a &lt;=&gt; $b} @lines;&#xD;&#xA;    $next_line = shift(@lines);&#xD;&#xA;    &#xD;&#xA;    while (&lt;&gt;) {&#xD;&#xA;        last if (!@lines);&#xD;&#xA;        if ($. == $next_line) {&#xD;&#xA;            $next_line = shift(@lines);&#xD;&#xA;            print;&#xD;&#xA;        }&#xD;&#xA;    }" />
  <row Id="1402" PostHistoryTypeId="5" PostId="464" RevisionGUID="d433080c-9cca-407c-bfa6-721d7169254c" CreationDate="2017-06-05T20:43:58.390" UserId="77" Comment="added 279 characters in body" Text="The &quot;Binary Alignment Map&quot; (BAM) format is one of the common binary formats used to store sequence alignment information. Questions should include this tag if they directly pertain to the format itself, details of BAM file usage, or errors relating to likely malformed BAM files." />
  <row Id="1403" PostHistoryTypeId="24" PostId="464" RevisionGUID="d433080c-9cca-407c-bfa6-721d7169254c" CreationDate="2017-06-05T20:43:58.390" Comment="Proposed by 77 approved by 73, 37 edit id of 106" />
  <row Id="1404" PostHistoryTypeId="5" PostId="463" RevisionGUID="6926cc4e-4455-40a6-875e-ad8f3107c492" CreationDate="2017-06-05T20:44:04.017" UserId="77" Comment="added 950 characters in body" Text="BAM was the first widely-adopted binary standard for storing NGS alignments. Its specification is openly maintained [here](https://github.com/samtools/hts-specs). BAM is a compressed format, using bgzf to compress reads in blocks. This is convenient, as it allows sorted files to be indexed and then rapid queried.&#xD;&#xA;&#xD;&#xA;Questions with this tag should broadly fall into one of the following categories:&#xD;&#xA;&#xD;&#xA;1. Questions regarding the standard itself (e.g., &quot;What is the relationship between the BAM header and alignments?&quot;).&#xD;&#xA;2. Questions regarding using BAM files (e.g., &quot;How do I sort a BAM file?&quot; or &quot;How do I ensure that all reads in one/more fastq files are contained in a BAM file?).&#xD;&#xA;3. When seeing errors, such as corrupt or incomplete BAM files.&#xD;&#xA;&#xD;&#xA;Particularly for category 3, it is then vital to provide information regarding exactly how the BAM file was made, ideally including the version of upstream tools as well as the exact command used." />
  <row Id="1405" PostHistoryTypeId="24" PostId="463" RevisionGUID="6926cc4e-4455-40a6-875e-ad8f3107c492" CreationDate="2017-06-05T20:44:04.017" Comment="Proposed by 77 approved by 73, 37 edit id of 105" />
  <row Id="1406" PostHistoryTypeId="2" PostId="466" RevisionGUID="52c8028e-5bac-4b0e-b5ec-4e675ad65cde" CreationDate="2017-06-05T21:05:51.263" UserId="77" Text="I just received a reply from 1000Genomes regarding this. I'll post it in its entirety below:&#xD;&#xA;&#xD;&#xA;&gt; Looking at the example you mention, I find it difficult to come up with an&#xD;&#xA;interpretation of the information whereby the stated end seems to be correct,&#xD;&#xA;so believe that this may indeed be an error.&#xD;&#xA;&#xD;&#xA;&gt; Since the v4.0 was created, however, new versions of VCF have been introduced,&#xD;&#xA;improving and correcting the specification. The current version is v4.3&#xD;&#xA;(http://samtools.github.io/hts-specs/). I believe the first record shown on&#xD;&#xA;page 11 provides an accurate example of this type of deletion.&#xD;&#xA;&#xD;&#xA;&gt; I will update the web page to include this information.&#xD;&#xA;&#xD;&#xA;So we can take this as official confirmation that we were all correct in suspecting the example was just wrong." />
  <row Id="1407" PostHistoryTypeId="2" PostId="467" RevisionGUID="c9048606-1e87-470b-98b5-be593ebae0aa" CreationDate="2017-06-05T21:16:01.070" UserId="73" Text="I'm not familiar with the program, but apparently [Hail](https://hail.is/) is setting itself up as a swiss-army chainsaw project for doing downstream analysis on variant-called datasets.&#xD;&#xA;&#xD;&#xA;An overview of Hail can be found here:&#xD;&#xA;&#xD;&#xA;http://blog.cloudera.com/blog/2017/05/hail-scalable-genomics-analysis-with-spark/&#xD;&#xA;&#xD;&#xA;A tutorial on association testing can be found here:&#xD;&#xA;&#xD;&#xA;https://hail.is/hail/tutorial.html#Association-testing" />
  <row Id="1408" PostHistoryTypeId="5" PostId="31" RevisionGUID="841bb088-afdf-435a-8490-eddda4644e1c" CreationDate="2017-06-05T23:08:16.720" UserId="57" Comment="added excerpt" Text="should be used for questions specific to sequence file format `.fasta`.&#xD;&#xA;&#xD;&#xA;should **not** be used if you just used fasta file, or if the question is more general about sequence formats." />
  <row Id="1409" PostHistoryTypeId="24" PostId="31" RevisionGUID="841bb088-afdf-435a-8490-eddda4644e1c" CreationDate="2017-06-05T23:08:16.720" Comment="Proposed by 57 approved by 77, 37 edit id of 107" />
  <row Id="1410" PostHistoryTypeId="2" PostId="468" RevisionGUID="b324ba4a-74c8-426d-8bf3-f27c7738a1b6" CreationDate="2017-06-06T02:47:36.653" UserId="37" Text="In general, the best way to download SRA data is: don't download from SRA. However, as ENA has not be sync'd yet, I would recommend to download from SRA ftp and then convert to fastq locally. You can find files in the SRA format [here](https://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/). Downloading and then converting locally is much faster than direct retrieval from NCBI for some mysterious reasons." />
  <row Id="1411" PostHistoryTypeId="5" PostId="455" RevisionGUID="fb71b603-292e-4d47-9a24-1b4827f9b1c2" CreationDate="2017-06-06T07:10:27.073" UserId="292" Comment="Removed useless strip()" Text="Some related questions appear in other sites, with potentially interesting solutions, which I report here:&#xD;&#xA;&#xD;&#xA;To sample approximately 1% of the non-empty lines:&#xD;&#xA;&#xD;&#xA;    awk 'BEGIN {srand()} !/^$/ { if (rand() &lt;= .01) print $0}' input_file&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(from &lt;https://stackoverflow.com/a/692321/1878788&gt;)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;To select 1000 random lines:&#xD;&#xA;&#xD;&#xA;    shuf -n 1000 input_file&#xD;&#xA;&#xD;&#xA;(from &lt;https://stackoverflow.com/a/15065490/1878788&gt;, and &lt;https://unix.stackexchange.com/a/108604/55127&gt;)&#xD;&#xA;&#xD;&#xA;### Edit: Python solutions using a list of lines&#xD;&#xA;&#xD;&#xA;Using a set of line indices and selecting lines by testing set membership:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python3&#xD;&#xA;    &#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    with open(sys.argv[2], &quot;r&quot;) as line_numbers_file:&#xD;&#xA;        line_indices = set(int(line) - 1 for line in line_numbers_file)&#xD;&#xA;    &#xD;&#xA;    with open(sys.argv[1], &quot;r&quot;) as input_file:&#xD;&#xA;        print(*(line.strip() for (idx, line) in enumerate(input_file)&#xD;&#xA;                if idx in line_indices), sep=&quot;\n&quot;)&#xD;&#xA;&#xD;&#xA;Using a numpy boolean array together with `itertools.compress`:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python3&#xD;&#xA;    &#xD;&#xA;    import sys&#xD;&#xA;    from itertools import compress&#xD;&#xA;    from numpy import zeros&#xD;&#xA;    &#xD;&#xA;    with open(sys.argv[2], &quot;r&quot;) as line_numbers_file:&#xD;&#xA;        line_indices = [int(line) - 1 for line in line_numbers_file]&#xD;&#xA;    &#xD;&#xA;    selector = zeros(max(line_indices) + 1, dtype=bool)&#xD;&#xA;    selector[line_indices] = 1&#xD;&#xA;    &#xD;&#xA;    with open(sys.argv[1], &quot;r&quot;) as input_file:&#xD;&#xA;        print(*(line.strip() for line in compress(input_file, selector)), sep=&quot;\n&quot;)&#xD;&#xA;&#xD;&#xA;I did some tests on a file containing 15774756 sam records and a list of 10000 pre-generated random line numbers.&#xD;&#xA;&#xD;&#xA;The perl script proposed by Konrad Rudolph (&lt;https://bioinformatics.stackexchange.com/a/454/292&gt;) runs in about 5.3 seconds.&#xD;&#xA;&#xD;&#xA;The set membership testing python solution runs in about 4.45 seconds.&#xD;&#xA;&#xD;&#xA;The compress based solution runs in about 3.4 seconds.&#xD;&#xA;I suspect this may vary a lot depending on the highest line number we want, since the number of iterations will depend on the length of the boolean array. Here the highest line number was 15773768, so pretty high compared with the total number of lines.&#xD;&#xA;&#xD;&#xA;I tried with python 3.6. I suspect that python 2.7 could be slightly faster, but haven't tested." />
  <row Id="1412" PostHistoryTypeId="2" PostId="469" RevisionGUID="402e76d1-478b-4e4b-acdb-f279afaca221" CreationDate="2017-06-06T07:56:48.763" UserId="182" Text="By far the fastest method in my experience has been to use the [SRAdb](http://bioconductor.org/packages/release/bioc/html/SRAdb.html) library in R. For most entries, you can download fastq files directly. Some older experiments don't have them, but I've still found it much faster to download SRA files via `getSRAfile()` and then to convert them using `fastqdump` than to use `fastqdump` directly. " />
  <row Id="1413" PostHistoryTypeId="2" PostId="470" RevisionGUID="d8525373-7950-48ff-a065-f0f1fea718a5" CreationDate="2017-06-06T08:05:14.003" UserId="-1" Text="" />
  <row Id="1414" PostHistoryTypeId="1" PostId="470" RevisionGUID="d8525373-7950-48ff-a065-f0f1fea718a5" CreationDate="2017-06-06T08:05:14.003" UserId="-1" />
  <row Id="1415" PostHistoryTypeId="2" PostId="471" RevisionGUID="4a3f1b61-3afd-4b95-aca3-3f4f3364a4fa" CreationDate="2017-06-06T08:05:14.003" UserId="-1" Text="" />
  <row Id="1416" PostHistoryTypeId="1" PostId="471" RevisionGUID="4a3f1b61-3afd-4b95-aca3-3f4f3364a4fa" CreationDate="2017-06-06T08:05:14.003" UserId="-1" />
  <row Id="1417" PostHistoryTypeId="5" PostId="465" RevisionGUID="c97f4fc1-f4b9-486f-b850-16ba12bdb5c6" CreationDate="2017-06-06T08:05:44.387" UserId="73" Comment="fixed bugs in code" Text="Perl should be fairly fast with this when using a hash set to store the list of lines. A structure like this also works for subsetting based on a field value, where the comparison would be with the field rather than &quot;$.&quot;:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    my %include_lines = ();&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    while(&lt;$lines_fh&gt;){&#xD;&#xA;      chomp;&#xD;&#xA;      $include_lines{$_} = 1;&#xD;&#xA;    }&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    while(&lt;&gt;){&#xD;&#xA;      if($include_lines{$.}){ # &quot;$.&quot; -- line number of current file&#xD;&#xA;        print;&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Note that according to [this SO answer](https://stackoverflow.com/questions/5920686/how-to-get-the-current-line-number-of-a-file-open-using-perl#comment6821096_5920709), the &quot;$.&quot; operator is not strictly the current line number, and can be influenced by different file operations or other settings.&#xD;&#xA;&#xD;&#xA;Edit: just saw your comment about speed in your answer, comparing hash sets to a sorted list. The ```$lines[$next_line]``` bit feels a bit odd to me. Have you tried out using ```shift``` or ```pop``` on a sorted list to fetch the next line:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    chomp (my @lines = &lt;$lines_fh&gt;);&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    @lines = sort {$a &lt;=&gt; $b} @lines;&#xD;&#xA;    my $next_line = shift(@lines);&#xD;&#xA;    &#xD;&#xA;    while (&lt;&gt;) {&#xD;&#xA;        last if (!@lines);&#xD;&#xA;        if ($. == $next_line) {&#xD;&#xA;            $next_line = shift(@lines);&#xD;&#xA;            print;&#xD;&#xA;        }&#xD;&#xA;    }" />
  <row Id="1418" PostHistoryTypeId="2" PostId="472" RevisionGUID="f94c38b1-50d9-4859-82cb-4a2feec7a0a7" CreationDate="2017-06-06T08:08:22.230" UserId="-1" Text="" />
  <row Id="1419" PostHistoryTypeId="1" PostId="472" RevisionGUID="f94c38b1-50d9-4859-82cb-4a2feec7a0a7" CreationDate="2017-06-06T08:08:22.230" UserId="-1" />
  <row Id="1420" PostHistoryTypeId="2" PostId="473" RevisionGUID="a0e621d0-3396-44fc-8ee6-c36a1bfca277" CreationDate="2017-06-06T08:08:22.230" UserId="-1" Text="" />
  <row Id="1421" PostHistoryTypeId="1" PostId="473" RevisionGUID="a0e621d0-3396-44fc-8ee6-c36a1bfca277" CreationDate="2017-06-06T08:08:22.230" UserId="-1" />
  <row Id="1422" PostHistoryTypeId="2" PostId="474" RevisionGUID="e75252dc-d8ba-4695-81c9-089307032df5" CreationDate="2017-06-06T08:10:34.667" UserId="-1" Text="" />
  <row Id="1423" PostHistoryTypeId="1" PostId="474" RevisionGUID="e75252dc-d8ba-4695-81c9-089307032df5" CreationDate="2017-06-06T08:10:34.667" UserId="-1" />
  <row Id="1424" PostHistoryTypeId="2" PostId="475" RevisionGUID="711b3838-c12b-45a4-8426-76780db2a579" CreationDate="2017-06-06T08:10:34.667" UserId="-1" Text="" />
  <row Id="1425" PostHistoryTypeId="1" PostId="475" RevisionGUID="711b3838-c12b-45a4-8426-76780db2a579" CreationDate="2017-06-06T08:10:34.667" UserId="-1" />
  <row Id="1426" PostHistoryTypeId="2" PostId="476" RevisionGUID="9b5faa56-e334-4237-bed2-f679c6051840" CreationDate="2017-06-06T08:19:37.980" UserId="182" Text="For ChIP-seq it shouldn't really matter. But do be aware that by default, `samtools merge` retains read group information (the `@RG` field in the header) from each input file. This could pose a problem for some downstream analyses (e.g. for the GATK HaplotypeCaller) if you want the merged data to be considered as all part of the same sample. You can change this behaviour using the `-c` option. " />
  <row Id="1427" PostHistoryTypeId="5" PostId="465" RevisionGUID="3f607937-f0cd-4e22-b75e-9909b0bddab2" CreationDate="2017-06-06T08:43:18.097" UserId="73" Comment="changed sort order, changed shift to pop" Text="Perl should be fairly fast with this when using a hash set to store the list of lines. A structure like this also works for subsetting based on a field value, where the comparison would be with the field rather than &quot;$.&quot;:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    my %include_lines = ();&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    while(&lt;$lines_fh&gt;){&#xD;&#xA;      chomp;&#xD;&#xA;      $include_lines{$_} = 1;&#xD;&#xA;    }&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    while(&lt;&gt;){&#xD;&#xA;      if($include_lines{$.}){ # &quot;$.&quot; -- line number of current file&#xD;&#xA;        print;&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Note that according to [this SO answer](https://stackoverflow.com/questions/5920686/how-to-get-the-current-line-number-of-a-file-open-using-perl#comment6821096_5920709), the &quot;$.&quot; operator is not strictly the current line number, and can be influenced by different file operations or other settings.&#xD;&#xA;&#xD;&#xA;Edit: just saw your comment about speed in your answer, comparing hash sets to a sorted list. The ```$lines[$next_line]``` bit feels a bit odd to me. Have you tried out using ```shift``` or ```pop``` on a sorted list to fetch the next line:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    chomp (my @lines = &lt;$lines_fh&gt;);&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    @lines = sort {$b &lt;=&gt; $a} @lines;&#xD;&#xA;    my $next_line = pop(@lines);&#xD;&#xA;    &#xD;&#xA;    while (&lt;&gt;) {&#xD;&#xA;        if ($. == $next_line) {&#xD;&#xA;            $next_line = pop(@lines);&#xD;&#xA;            print;&#xD;&#xA;        }&#xD;&#xA;        last if (!@lines);&#xD;&#xA;    }" />
  <row Id="1428" PostHistoryTypeId="5" PostId="465" RevisionGUID="60bd3d40-ce2b-4313-8ca0-075582825dc1" CreationDate="2017-06-06T08:49:52.693" UserId="73" Comment="shifted last check into test loop" Text="Perl should be fairly fast with this when using a hash set to store the list of lines. A structure like this also works for subsetting based on a field value, where the comparison would be with the field rather than &quot;$.&quot;:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    my %include_lines = ();&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    while(&lt;$lines_fh&gt;){&#xD;&#xA;      chomp;&#xD;&#xA;      $include_lines{$_} = 1;&#xD;&#xA;    }&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    while(&lt;&gt;){&#xD;&#xA;      if($include_lines{$.}){ # &quot;$.&quot; -- line number of current file&#xD;&#xA;        print;&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Note that according to [this SO answer](https://stackoverflow.com/questions/5920686/how-to-get-the-current-line-number-of-a-file-open-using-perl#comment6821096_5920709), the &quot;$.&quot; operator is not strictly the current line number, and can be influenced by different file operations or other settings.&#xD;&#xA;&#xD;&#xA;Edit: just saw your comment about speed in your answer, comparing hash sets to a sorted list. The ```$lines[$next_line]``` bit feels a bit odd to me. Have you tried out using ```shift``` or ```pop``` on a sorted list to fetch the next line:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    chomp (my @lines = &lt;$lines_fh&gt;);&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    @lines = sort {$b &lt;=&gt; $a} @lines;&#xD;&#xA;    my $next_line = pop(@lines);&#xD;&#xA;    &#xD;&#xA;    while (&lt;&gt;) {&#xD;&#xA;        if ($. == $next_line) {&#xD;&#xA;            $next_line = pop(@lines);&#xD;&#xA;            print;&#xD;&#xA;            last if (!@lines);&#xD;&#xA;        }&#xD;&#xA;    }" />
  <row Id="1429" PostHistoryTypeId="11" PostId="357" RevisionGUID="68597cfe-00ab-4f11-acdf-6ee04c597798" CreationDate="2017-06-06T08:51:05.200" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:37,&quot;DisplayName&quot;:&quot;user172818&quot;},{&quot;Id&quot;:93,&quot;DisplayName&quot;:&quot;Franck Dernoncourt&quot;},{&quot;Id&quot;:134,&quot;DisplayName&quot;:&quot;John Marshall&quot;},{&quot;Id&quot;:425,&quot;DisplayName&quot;:&quot;Karel&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;Llopis&quot;}]}" />
  <row Id="1430" PostHistoryTypeId="5" PostId="465" RevisionGUID="d68f5ac7-7fd6-4871-bed1-d6df331755d0" CreationDate="2017-06-06T08:58:49.697" UserId="73" Comment="changed back to shift; it's slightly easier to understand and doesn't affect the speed" Text="Perl should be fairly fast with this when using a hash set to store the list of lines. A structure like this also works for subsetting based on a field value, where the comparison would be with the field rather than &quot;$.&quot;:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    my %include_lines = ();&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    while(&lt;$lines_fh&gt;){&#xD;&#xA;      chomp;&#xD;&#xA;      $include_lines{$_} = 1;&#xD;&#xA;    }&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    while(&lt;&gt;){&#xD;&#xA;      if($include_lines{$.}){ # &quot;$.&quot; -- line number of current file&#xD;&#xA;        print;&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Note that according to [this SO answer](https://stackoverflow.com/questions/5920686/how-to-get-the-current-line-number-of-a-file-open-using-perl#comment6821096_5920709), the &quot;$.&quot; operator is not strictly the current line number, and can be influenced by different file operations or other settings.&#xD;&#xA;&#xD;&#xA;Edit: just saw your comment about speed in your answer, comparing hash sets to a sorted list. The ```$lines[$next_line]``` bit feels a bit odd to me. Have you tried out using ```shift``` or ```pop``` on a sorted list to fetch the next line:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $lines_file = $ARGV[0];&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    chomp (my @lines = &lt;$lines_fh&gt;);&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    @lines = sort {$a &lt;=&gt; $b} @lines;&#xD;&#xA;    my $next_line = shift(@lines);&#xD;&#xA;    &#xD;&#xA;    while (&lt;&gt;) {&#xD;&#xA;        if ($. == $next_line) {&#xD;&#xA;            $next_line = shift(@lines);&#xD;&#xA;            print;&#xD;&#xA;            last if (!@lines);&#xD;&#xA;        }&#xD;&#xA;    }" />
  <row Id="1431" PostHistoryTypeId="5" PostId="452" RevisionGUID="9a66a611-a057-4cc5-8df1-fc746598e9de" CreationDate="2017-06-06T09:00:32.853" UserId="48" Comment="Improve readability by hidding the links" Text="The Albertsen lab has recently put out a [competition/challenge][1] for read error correction&#xD;&#xA;&#xD;&#xA;I only found out about this today, and I think the conference where high-ranking participants were going to be mentioned has just finished, but the data is all public and there's no reason why this can't be continued in the future as a benchmarking test for nanopore base calling and/or read error correction.&#xD;&#xA;&#xD;&#xA;Data: Nanopore reads (as called FAST5 files) can be found [here][2]. The initial called FASTQ files can be found [here][3]. Reference sequences (from which the reads were generated) can be found [here][4].&#xD;&#xA;&#xD;&#xA;Every [passed] read should be a 2D nanopore read with the following sequence structure, where the decamer, **NNNNNNNNNN**, is the unique molecular identifier and is attached to common primer sites at the read extremities:&#xD;&#xA;&#xD;&#xA;&gt; ***AAAGATGAAGAT***–**NNNNNNNNNN** CGTACTAGACTTGCCTGTCGCTCTATCTTCTTTTTTTTTTTTTTTTTTTT  &#xD;&#xA;&lt;—- fragment of SSU cDNA molecule—-&gt;  &#xD;&#xA;GGGCAATATCAGCACCAACAGAAATAGATCGC**NNNNNNNNNN**–***ATGGATGAGTCT***&#xD;&#xA;&#xD;&#xA;So... what's the best consensus accuracy you can get from these reads? How did you get that accuracy?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://albertsenlab.org/can-you-beat-our-nanopore-read-error-correction-we-hope-so/&#xD;&#xA;  [2]: http://www.ebi.ac.uk/ena/data/view/PRJEB20906&#xD;&#xA;  [3]: https://www.dropbox.com/sh/cw8n7df1z61lkcj/AACc4ElVSefVfD5csnv0Klc_a?dl=0&#xD;&#xA;  [4]: https://www.dropbox.com/s/u6w993jrca05w8u/mockrRNAall.fasta?dl=0" />
  <row Id="1432" PostHistoryTypeId="24" PostId="452" RevisionGUID="9a66a611-a057-4cc5-8df1-fc746598e9de" CreationDate="2017-06-06T09:00:32.853" Comment="Proposed by 48 approved by 77, 73 edit id of 111" />
  <row Id="1433" PostHistoryTypeId="2" PostId="477" RevisionGUID="fa974386-a917-4c12-89e0-361219d88027" CreationDate="2017-06-06T09:05:07.703" UserId="595" Text="Assuming you mean CRISPR screens targetting many loci (for example using the GeCKO library) there is an R package here: &#xD;&#xA;&#xD;&#xA;https://git.embl.de/msmith/geckoR&#xD;&#xA;&#xD;&#xA;It uses a linear mixed effect model to compare guide counts before and after a selection step, allowing for multiple guides/gene and multiple replicates. &#xD;&#xA;It can also do the initial read alignment afaik and the author should be quite responsive in case you have questions.&#xD;&#xA;&#xD;&#xA;Another opiton could be the *python* program MAGeCK (https://bitbucket.org/liulab/mageck-vispr)&#xD;&#xA;" />
  <row Id="1434" PostHistoryTypeId="5" PostId="357" RevisionGUID="02b5e987-57df-4ede-be2a-4704c518b526" CreationDate="2017-06-06T09:09:06.957" UserId="48" Comment="Adapting title of the question to content, adding description of the problem from the OP" Text="As someone who's beginning to delve into bioinformatics, I'm noticing that like biology there are industry standards here, similar to Illumina in genomics and bowtie for alignment, many people use bash as shell. &#xD;&#xA;&#xD;&#xA;Is using a shell besides bash going to cause issues for me?&#xD;&#xA;" />
  <row Id="1435" PostHistoryTypeId="4" PostId="357" RevisionGUID="02b5e987-57df-4ede-be2a-4704c518b526" CreationDate="2017-06-06T09:09:06.957" UserId="48" Comment="Adapting title of the question to content, adding description of the problem from the OP" Text="Using other shell than bash" />
  <row Id="1436" PostHistoryTypeId="24" PostId="357" RevisionGUID="02b5e987-57df-4ede-be2a-4704c518b526" CreationDate="2017-06-06T09:09:06.957" Comment="Proposed by 48 approved by -1 edit id of 112" />
  <row Id="1437" PostHistoryTypeId="4" PostId="357" RevisionGUID="7c266c39-37e1-4674-9f6a-a95a7fba25e0" CreationDate="2017-06-06T09:09:06.957" UserId="77" Comment="Adapting title of the question to content, adding description of the problem from the OP" Text="Using shells other than bash" />
  <row Id="1438" PostHistoryTypeId="5" PostId="475" RevisionGUID="c92f5687-58e1-482c-9196-1cde5908fa4e" CreationDate="2017-06-06T09:16:33.847" UserId="450" Comment="added 201 characters in body" Text="Questions specific to interacting with and post-processing short DNA sequence read alignments in the SAM (Sequence Alignment/Map), BAM (Binary Alignment/Map) or CRAM formats, using the SAMtools package" />
  <row Id="1439" PostHistoryTypeId="24" PostId="475" RevisionGUID="c92f5687-58e1-482c-9196-1cde5908fa4e" CreationDate="2017-06-06T09:16:33.847" Comment="Proposed by 450 approved by 77, 73 edit id of 110" />
  <row Id="1440" PostHistoryTypeId="5" PostId="473" RevisionGUID="0c952a8e-04c7-4e28-90cd-ea49c2bfbd48" CreationDate="2017-06-06T09:16:46.830" UserId="450" Comment="added 79 characters in body" Text="Questions specific to products by Oxford Nanopore Technologies like the MinION." />
  <row Id="1441" PostHistoryTypeId="24" PostId="473" RevisionGUID="0c952a8e-04c7-4e28-90cd-ea49c2bfbd48" CreationDate="2017-06-06T09:16:46.830" Comment="Proposed by 450 approved by 77, 73 edit id of 109" />
  <row Id="1442" PostHistoryTypeId="5" PostId="471" RevisionGUID="4629a654-e0cd-4bd7-b770-a3ab2b5230bf" CreationDate="2017-06-06T09:16:52.090" UserId="450" Comment="added 46 characters in body" Text="These questions are about sequence alignment. " />
  <row Id="1443" PostHistoryTypeId="24" PostId="471" RevisionGUID="4629a654-e0cd-4bd7-b770-a3ab2b5230bf" CreationDate="2017-06-06T09:16:52.090" Comment="Proposed by 450 approved by 77, 73 edit id of 108" />
  <row Id="1444" PostHistoryTypeId="2" PostId="478" RevisionGUID="331d35ce-a298-457e-b636-0f72341852ed" CreationDate="2017-06-06T10:57:34.970" UserId="235" Text="The suggestion to use proportionality is probably the correct one if you are interested in similar *patterns* between samples. But not if you are interested in *absolute* differences. &#xD;&#xA;&#xD;&#xA;For example: the following two samples are similar in pattern, but not similar on absolute levels:&#xD;&#xA;&#xD;&#xA;            Sample 1    Sample 2    Sample 3&#xD;&#xA;    Gene A    10          100           80&#xD;&#xA;    Gene B     8           80          100  &#xD;&#xA;    Gene C    12          120          120&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Samples 1 and 2 have a perfect proportionality (phi is 0) and also a perfect correlation (as a side note any pair with a perfect proportionality will always have a perfect correlation). However, in terms of logfold changes, samples 2 and 3 are more like each other. &#xD;&#xA;&#xD;&#xA;Of course in real life you would never see a comparison like the sample 1 - sample 2 one because normalisation would have removed the scale difference. This was exactly the point brought up by the proportionality paper. But normalisation methods don't normally guarantee that the sum of expression for each sample is identical, and such differences can still occur. &#xD;&#xA;&#xD;&#xA;An alternative that might suit more in the second case is either the euclidean distance between the samples, or the euclidean distance on the first two components of a principle component or multi-dimensional scaling. The latter is effectively using mean logFC between samples.&#xD;&#xA;&#xD;&#xA;Assuming that `x` is a matrix containing normalised, log transformed expression values, you could use R and limma to calculate distance in multi-dimensional scaled space as follows:&#xD;&#xA;&#xD;&#xA;    library(limma)&#xD;&#xA;    mds &lt;- plotMDS(x, plot=FALSE)&#xD;&#xA;    mds &lt;- data.frame(mds$x, mds$y)&#xD;&#xA;    distances &lt;- dist(mds)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    " />
  <row Id="1445" PostHistoryTypeId="2" PostId="479" RevisionGUID="8c48654e-1684-4a11-8782-323dcba581ad" CreationDate="2017-06-06T11:07:55.663" UserId="451" Text="Ideally these BQSR methods were made keeping in mind how technical errors will actually screw up the base quality calls and when the machines were still more on development phase while being used for the 1000G project. As of now machines are more powerful and strong where it will be unlikely to use it but still we use with listed SNPs to find the covariates and build a model around the data using the information with machine learning trciks to improve the quality of those base calls. Ideally it should be more appropriate when old machines from Illumina or other standard companies are being used but with new machines which are much powerful and having high throughput they should tend to go down. I do not recall if such tests have been made but obviously I know new sequencing machine always make such tests to show that they have reduced such errors but still recommend such BQSR for variant calls. Now the problem is the list of SNPs, this to me is the real problem since the list we use is far from being Gold standard and if that is not properly taken care of everything we infer about quality is still shaky. This link is pretty informative [http://gatkforums.broadinstitute.org/gatk/discussion/44/base-quality-score-recalibration-bqsr][1] but its an old one. I would really see improvements with new sequencers.  However very less people care about such tests in academic research and also translational lab will really not invest time and money on such unless the facility has some bioinformaticians who always does such testing while buying a new sequencer for the institute. In terms of clinical genomics for finding variants I reckon most powerful and up-to-date sequencers should be used but not sure if they still use BQSR and if so what is the list they use to build model of covariation around the data.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://Ideally%20these%20BQSR%20methods%20were%20made%20keeping%20in%20mind%20how%20technical%20errors%20will%20actually%20screw%20up%20the%20base%20quality%20calls%20and%20when%20the%20machines%20were%20still%20more%20on%20development%20phase%20while%20being%20used%20for%20the%201000G%20project.%20As%20of%20now%20machines%20are%20more%20powerful%20and%20strong%20where%20it%20will%20be%20unlikely%20to%20use%20it%20but%20still%20we%20use%20with%20listed%20SNPs%20to%20find%20the%20covariates%20and%20build%20a%20model%20around%20that%20to%20improve%20the%20quality%20of%20those%20base%20calls.%20Ideally%20it%20should%20be%20more%20appropriate%20when%20old%20machines%20from%20Illumina%20or%20other%20standard%20companies%20are%20being%20used%20but%20with%20new%20machines%20which%20are%20much%20powerful%20and%20having%20high%20throughput%20they%20should%20tend%20to%20go%20down.%20I%20do%20not%20recall%20if%20such%20tests%20have%20been%20made%20but%20obviously%20I%20know%20new%20sequencing%20machine%20always%20make%20such%20tests%20to%20show%20that%20they%20have%20reduced%20such%20errors%20but%20still%20recommend%20such%20BQSR%20for%20variant%20calls.%20Now%20the%20problem%20is%20the%20list%20of%20SNPs,%20this%20to%20me%20is%20the%20real%20problem%20since%20the%20list%20we%20use%20is%20far%20from%20being%20Gold%20standard%20and%20if%20that%20is%20not%20properly%20taken%20care%20of%20everything%20we%20infer%20about%20quality%20is%20still%20shaky.%20This%20link%20is%20pretty%20informative%20http://gatkforums.broadinstitute.org/gatk/discussion/44/base-quality-score-recalibration-bqsr%20but%20its%20an%20old%20one.%20I%20would%20really%20see%20improvements%20with%20new%20sequencers." />
  <row Id="1446" PostHistoryTypeId="2" PostId="480" RevisionGUID="75192d31-7036-4a60-9cb1-a8375d8a0648" CreationDate="2017-06-06T11:28:46.080" UserId="235" Text="I can see that there would be a three step process to doing this:&#xD;&#xA;&#xD;&#xA;1. Merge counts from all samples in the group and then resample pseudo-replicates from this. If `x` is a matrix with samples in the group being columns and genes being rows&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    `s &lt;- sample(row.names(x), n = mean(colSums(x)), probs=rowSums(x)/sum(rowSums(x))&#xD;&#xA;     stab &lt;- table(s)&#xD;&#xA;     s &lt;- as.vector(stab)&#xD;&#xA;     names(s) &lt;- names(stab)`&#xD;&#xA;&#xD;&#xA;   Do this thousands of times. &#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;2. You would then want to project those onto your MDS plot space - i'm less clear about how you could do this without perturbing the space itself.&#xD;&#xA;&#xD;&#xA;3. Calculate the parameter for an ellipise that would contain 95% of these points. Again, I'm not so sure about how to do this, but I think the `car` pacakge might be a good place to start looking.&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;" />
  <row Id="1448" PostHistoryTypeId="2" PostId="481" RevisionGUID="e857f3d4-dfb4-4934-8088-b629f435e54b" CreationDate="2017-06-06T12:44:58.087" UserId="73" Text="*This is a question from [/u/beneficii9](https://www.reddit.com/user/beneficii9) on reddit. The original post can be found [here](https://www.reddit.com/r/bioinformatics/comments/6f040g/how_do_you_do_admixture_testing_with_a_whole/).*&#xD;&#xA;&#xD;&#xA;Through the Personal Genome Project, I have had my whole genome sequenced by Veritas, and have it in the form of a single VCF file for the whole genome and one BAS file for each chromosome. The reference genome associated with the VCF file is hg19. It has been helpful in health data; for example, I discovered I'm homozygous for the non-functional variant CYP-2D6 gene ([rs3892097](https://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=3892097)), which can render several common medications useless, and helps explain why some medicines didn't really work for me. My doctor has found this information very helpful.&#xD;&#xA;&#xD;&#xA;Unfortunately, I can't find any way of looking at admixture or ancestry. I've tried setting everything up using a combination of VCFTools, Plink1.9, and ADMIXTURE, but I can't get it to work. I think for ADMIXTURE you have to have a bunch of genomes sorted by geographical origin to compare your genome against, but I'm not sure how to do that, and what's online isn't very clear to me. So scratch that one off.&#xD;&#xA;&#xD;&#xA;I've tried converting the file to 23andme format (and at this [/u/psychosomaticism](https://www.reddit.com/u/psychosomaticism) has been very helpful). I did that (though it seems there were problems because of the way the VCF file was set up). But the websites that take the data want you to point them to your 23andme account, and that doesn't really work if you only have the file. 23andme doesn't provide for people who had their whole genomes sequenced. They want you to give them a saliva sample like everyone else.&#xD;&#xA;&#xD;&#xA;So, what can I do?" />
  <row Id="1449" PostHistoryTypeId="1" PostId="481" RevisionGUID="e857f3d4-dfb4-4934-8088-b629f435e54b" CreationDate="2017-06-06T12:44:58.087" UserId="73" Text="How do I carry out an ancestry/admixture test on a single VCF file?" />
  <row Id="1450" PostHistoryTypeId="3" PostId="481" RevisionGUID="e857f3d4-dfb4-4934-8088-b629f435e54b" CreationDate="2017-06-06T12:44:58.087" UserId="73" Text="&lt;vcf&gt;&lt;reddit&gt;&lt;ancestry&gt;&lt;personal-genomics&gt;" />
  <row Id="1451" PostHistoryTypeId="2" PostId="482" RevisionGUID="bcefd2f9-4234-4e1b-8f87-d427a4911062" CreationDate="2017-06-06T12:47:47.490" UserId="-1" Text="" />
  <row Id="1452" PostHistoryTypeId="1" PostId="482" RevisionGUID="bcefd2f9-4234-4e1b-8f87-d427a4911062" CreationDate="2017-06-06T12:47:47.490" UserId="-1" />
  <row Id="1453" PostHistoryTypeId="2" PostId="483" RevisionGUID="47420d32-439c-4e44-a324-69cd893ffd61" CreationDate="2017-06-06T12:47:47.490" UserId="-1" Text="" />
  <row Id="1454" PostHistoryTypeId="1" PostId="483" RevisionGUID="47420d32-439c-4e44-a324-69cd893ffd61" CreationDate="2017-06-06T12:47:47.490" UserId="-1" />
  <row Id="1455" PostHistoryTypeId="5" PostId="483" RevisionGUID="da56a029-25f8-41c1-9859-d0c095f030a6" CreationDate="2017-06-06T12:56:19.700" UserId="73" Comment="added 211 characters in body" Text="Posts originally from reddit, where the question is well-structured and fits in with the StackExchange question/answer format. StackExchange questions should link to the original post at the top of the question." />
  <row Id="1456" PostHistoryTypeId="24" PostId="483" RevisionGUID="da56a029-25f8-41c1-9859-d0c095f030a6" CreationDate="2017-06-06T12:56:19.700" Comment="Proposed by 73 approved by 29, 57 edit id of 113" />
  <row Id="1457" PostHistoryTypeId="2" PostId="484" RevisionGUID="010abffd-be66-44c5-a075-da74b1ea0aa6" CreationDate="2017-06-06T13:03:11.313" UserId="601" Text="A suggestion from the UK Biobank QC methods white paper:&#xD;&#xA;&#xD;&#xA;1. Create a set of SNPs common to your VCF and the 1000 genomes phase 3 call set.&#xD;&#xA;2. Perform PCA of the 1000 genomes samples using eigenstrat smartpca. You might have to convert to binary plink format.&#xD;&#xA;3. Project your genotypes onto this pre-computed PCA space and visualize using ggplot to see which cluster you fall in." />
  <row Id="1458" PostHistoryTypeId="2" PostId="485" RevisionGUID="e016a6b2-ab09-4a62-a016-10b1394f1e9f" CreationDate="2017-06-06T13:16:05.703" UserId="73" Text="Ancestry testing is a tricky subject. I spent a good chunk of [my PhD project](http://researcharchive.vuw.ac.nz/handle/10063/1987) on questions similar to this, and haven't really found a good answer for how to detect unmodelled ancestry.&#xD;&#xA;&#xD;&#xA;The general idea of how I tried to approach ancestry determination was to create a model set containing well-defined groups of individuals with a known, specific ancestral background. A query individual (or individuals) were then added to this group, and an ancestry-estimating program was run (specifically [Structure](http://web.stanford.edu/group/pritchardlab/structure.html)) to work out what proportion of the unknown individual was attributable to each of the known groups. I would expect that most genetic ancestry tests follow a similar approach to this, although perhaps with a little bit less care about the definition of the model population groups.&#xD;&#xA;&#xD;&#xA;Problems arise when individuals are incorrectly assigned to a particular group, when groups are present in the model set that represent a larger proportion of individuals than most other groups, when groups are closely-related to other groups, and when an ancestral history is present in a test individual that doesn't match any of the model groups. And all that assumes that the marker set used for ancestry determination is perfect: no bias towards any particular group, and no systematic genotyping error.&#xD;&#xA;&#xD;&#xA;This doesn't mean that ancestry testing won't work, but it's a good idea to take the results with a large grain of salt. There's a [good example](https://www.theguardian.com/world/2017/apr/13/a-dna-test-showed-im-100-maori-many-thought-there-were-none-of-us-left) of a media personality in New Zealand who was told that she had a very high probability of being 100% Māori, despite having a good knowledge of her own family history that indicated one European ancestor a few generations back on both sides of her family." />
  <row Id="1459" PostHistoryTypeId="2" PostId="486" RevisionGUID="11dd75ef-1396-4a62-a644-03431190a5a9" CreationDate="2017-06-06T13:27:44.753" UserId="73" Text="*This is a question from [/u/wipeyourmit](https://www.reddit.com/user/wipeyourmit) on reddit. The original post can be found [here](https://www.reddit.com/r/bioinformatics/comments/6dfu3c/question_about_eggnog_mapper_and_annotation_in/).*&#xD;&#xA;&#xD;&#xA;If I have a metagenomic dataset that contains reads from both eukaryotes and prokaryotes and then I annotate by running DIAMOND or HMMER against a bacterial database how much of a risk do I run of eukaryotic reads being annotated in the process?&#xD;&#xA;&#xD;&#xA;I was hoping to use the eggNOG mapper to search against the bacterial and archaeal databases and to exclude the eukaryotic portion of my dataset. Is the eukaryotic filtering something that I would need to do in a step prior to this?" />
  <row Id="1460" PostHistoryTypeId="1" PostId="486" RevisionGUID="11dd75ef-1396-4a62-a644-03431190a5a9" CreationDate="2017-06-06T13:27:44.753" UserId="73" Text="Accidental mapping of eukaryotic reads in a metagenomic dataset" />
  <row Id="1461" PostHistoryTypeId="3" PostId="486" RevisionGUID="11dd75ef-1396-4a62-a644-03431190a5a9" CreationDate="2017-06-06T13:27:44.753" UserId="73" Text="&lt;mapping&gt;&lt;metagenome&gt;&lt;reddit&gt;" />
  <row Id="1462" PostHistoryTypeId="2" PostId="487" RevisionGUID="170b64ac-87df-4b1c-8f6d-fb936513002e" CreationDate="2017-06-06T13:31:44.300" UserId="73" Text="*Answer from [/u/Romanticon](https://www.reddit.com/user/Romanticon) on reddit. The original answer can be found [here](https://www.reddit.com/r/bioinformatics/comments/6dfu3c/question_about_eggnog_mapper_and_annotation_in/di2qv4a/)*&#xD;&#xA;&#xD;&#xA;Depending on the length of your reads, the level of error in the reads, the quality of the database, and the specificity settings you give DIAMOND, you can change the level of &quot;bleed&quot; that you get - but you'll always have a couple reads, especially at metagenome dataset sizes, that will be annotated incorrectly.&#xD;&#xA;&#xD;&#xA;But you'll always have this happen, whenever you annotate any large dataset. The best way to handle it is to set a threshold after annotation that removes wrongly annotated reads (looking for 'clearly wrong' organisms in your annotated dataset can help you figure out where to set the threshold level - there's probably no Bos taurus reads in a mouse metagenome).&#xD;&#xA;&#xD;&#xA;You can do things like enable the sensitive flag on the DIAMOND annotation search to help improve annotation accuracy." />
  <row Id="1463" PostHistoryTypeId="16" PostId="487" RevisionGUID="11dafe38-bd67-45a0-a3c8-45aa6ce8d804" CreationDate="2017-06-06T13:31:44.300" UserId="73" />
  <row Id="1464" PostHistoryTypeId="2" PostId="488" RevisionGUID="e5e73475-5306-46fb-85f4-922e97eadb75" CreationDate="2017-06-06T13:38:19.540" UserId="73" Text="*This is a question from [/u/apivan19](https://www.reddit.com/user/apivan19) on reddit. The original post can be found [here](https://www.reddit.com/r/bioinformatics/comments/6ddvkv/converting_gene_names/).*&#xD;&#xA;&#xD;&#xA;I have some proteomics data that was given to me with the UniProt gene identifiers in column 1. I've been trying to convert these to normal gene symbols using various programs, but it is proving to be difficult.&#xD;&#xA;&#xD;&#xA;The Uniprot website does it fairly decently, but it is not able to convert all of them and then adds some unknown genes into my list.&#xD;&#xA;&#xD;&#xA;For example, I will give it 5439 genes in UniProt notation, and will say &quot;5420 of 5439 UniProt identifiers have been converted to 5450 gene symbols&quot;... which is ridiculous.&#xD;&#xA;&#xD;&#xA;I tried using David to change the symbols, but it returns them to me in some ridiculous, random order and there's no way I can sort... actually there might be but it'll take a second.&#xD;&#xA;&#xD;&#xA;What are some of the easiest ways to do this? It's already very time consuming and am looking for simpler solutions" />
  <row Id="1465" PostHistoryTypeId="1" PostId="488" RevisionGUID="e5e73475-5306-46fb-85f4-922e97eadb75" CreationDate="2017-06-06T13:38:19.540" UserId="73" Text="Converting gene names from one public database format to another" />
  <row Id="1466" PostHistoryTypeId="3" PostId="488" RevisionGUID="e5e73475-5306-46fb-85f4-922e97eadb75" CreationDate="2017-06-06T13:38:19.540" UserId="73" Text="&lt;database&gt;&lt;reddit&gt;&lt;gene&gt;&lt;conversion&gt;" />
  <row Id="1467" PostHistoryTypeId="2" PostId="489" RevisionGUID="b00a5f0f-e95f-4aab-8583-785b22a7243f" CreationDate="2017-06-06T13:46:44.057" UserId="73" Text="My favourite gene database conversion site is [db2db](https://biodbnet-abcc.ncifcrf.gov/db/db2db.php). You provide a list of IDs in one of a large number of different public formats, and can select one or more IDs as translation targets. It will then walk through various known paths to do the translation, picking what it determines to be the most reliable route to get the information you have asked for. Results appear in the browser as a table, but can also be exported as an Excel file, or as a tab-separated text file.&#xD;&#xA;&#xD;&#xA;Note that the mapping of genes from one database to another is not a one-to-one mapping. It is likely the case that there will be some genes in the source database that map to multiple genes in the target database (and *vice versa*), and some genes that aren't present in the target database. These phenomena probably account for the &quot;ridiculous&quot; results that have been seen here." />
  <row Id="1468" PostHistoryTypeId="2" PostId="490" RevisionGUID="55f08a0f-0996-4c03-b178-da54b5158229" CreationDate="2017-06-06T14:16:38.920" UserId="29" Text="I tend to use [Ensembl Biomart](http://www.ensembl.org/biomart) for such queries since there are APIs for various programming languages, e.g. [biomaRt](https://bioconductor.org/packages/release/bioc/html/biomaRt.html), and, maybe more interestingly, via a [REST API](http://www.ensembl.org/info/data/biomart/biomart_restful.html) (although it’s a pretty terrible one).&#xD;&#xA;&#xD;&#xA;To translate identifiers from different databases, proceed as follows:&#xD;&#xA;&#xD;&#xA;1. Choose database “Ensembl genes”&#xD;&#xA;2. Choose dataset *your desired oganism*&#xD;&#xA;3. Go on “Filters” › “Gene:” › “Input external reference ID list”&#xD;&#xA;    1. Select the chosen source database&#xD;&#xA;    2. Provide a list of IDs, delimited by newline&#xD;&#xA;4. Go to “Attributes” › “Gene:” › untick “Transcript stable ID”&#xD;&#xA;    1. If Ensembl IDs are desired, leave “Gene stable ID” ticked …&#xD;&#xA;    2. Otherwise untick it; go to “External:”, tick your desired identifier format&#xD;&#xA;5. Click “Results” at the top left. This gives a preview that can be exported into various formats; alternatively the top centre buttons “XML” and “Perl” provide the query in XML (for SOAP/REST requests) and as a (horrendously formatted) executable Perl script." />
  <row Id="1469" PostHistoryTypeId="5" PostId="480" RevisionGUID="f6b96760-373a-4a16-aa99-1e0a17bba0e9" CreationDate="2017-06-06T14:18:36.500" UserId="29" Comment="formatting" Text="I can see that there would be a three step process to doing this:&#xD;&#xA;&#xD;&#xA;1. Merge counts from all samples in the group and then resample pseudo-replicates from this. If `x` is a matrix with samples in the group being columns and genes being rows&#xD;&#xA;&#xD;&#xA;    &lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;        s &lt;- sample(row.names(x), n = mean(colSums(x)), probs=rowSums(x)/sum(rowSums(x))&#xD;&#xA;        stab &lt;- table(s)&#xD;&#xA;        s &lt;- as.vector(stab)&#xD;&#xA;        names(s) &lt;- names(stab)&#xD;&#xA;&#xD;&#xA;   Do this thousands of times.&#xD;&#xA;&#xD;&#xA;2. You would then want to project those onto your MDS plot space - i'm less clear about how you could do this without perturbing the space itself.&#xD;&#xA;&#xD;&#xA;3. Calculate the parameter for an ellipise that would contain 95% of these points. Again, I'm not so sure about how to do this, but I think the `car` pacakge might be a good place to start looking.&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1470" PostHistoryTypeId="2" PostId="491" RevisionGUID="136e408d-38e7-4be6-95cf-f880a872e35f" CreationDate="2017-06-06T14:29:59.870" UserId="104" Text="I'm fairly new to [snakemake][1] and I'm trying to understand the difference between the `--cluster` and `--drmaa` flags, both of which are used to submit jobs to compute clusters/nodes.&#xD;&#xA;&#xD;&#xA;The docs give a few hints about the advantages of using `--drmaa` [here][2]:&#xD;&#xA;&#xD;&#xA;&gt; If your cluster system supports DRMAA, Snakemake can make use of that&#xD;&#xA;&gt; to increase the control over jobs. E.g. jobs can be cancelled upon&#xD;&#xA;&gt; pressing Ctrl+C, which is not possible with the generic --cluster&#xD;&#xA;&gt; support.&#xD;&#xA;&#xD;&#xA;And [here][3]:&#xD;&#xA;&#xD;&#xA;&gt; If available, DRMAA is preferable over the generic cluster modes&#xD;&#xA;&gt; because it provides better control and error handling.&#xD;&#xA;&#xD;&#xA;However, I don't consider this a very complete explanation, can someone elaborate?&#xD;&#xA;&#xD;&#xA;Note, that although this could be considered a more general programming question, snakemake is predominantly used in bioinformatics; I was persuaded that this question would be considered on-topic because of [this meta-post][4] and [this answer][5]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://snakemake.readthedocs.io/en/stable/&#xD;&#xA;  [2]: http://snakemake.readthedocs.io/en/stable/executable.html&#xD;&#xA;  [3]: http://snakemake.readthedocs.io/en/stable/tutorial/additional_features.html&#xD;&#xA;  [4]: https://bioinformatics.meta.stackexchange.com/questions/61/serving-as-a-support-forum-for-specific-tools&#xD;&#xA;  [5]: https://bioinformatics.meta.stackexchange.com/a/66/104" />
  <row Id="1471" PostHistoryTypeId="1" PostId="491" RevisionGUID="136e408d-38e7-4be6-95cf-f880a872e35f" CreationDate="2017-06-06T14:29:59.870" UserId="104" Text="What is the difference between snakemake's --cluster and --drmaa flags?" />
  <row Id="1472" PostHistoryTypeId="3" PostId="491" RevisionGUID="136e408d-38e7-4be6-95cf-f880a872e35f" CreationDate="2017-06-06T14:29:59.870" UserId="104" Text="&lt;snakemake&gt;" />
  <row Id="1473" PostHistoryTypeId="2" PostId="492" RevisionGUID="26d17024-1adc-4865-a130-bdd21f7513b4" CreationDate="2017-06-06T14:37:23.240" UserId="110" Text="I'm not a huge fan of the Ensembl BioMart system because I find it difficult to use. The [Synergizer](http://llama.mshri.on.ca/synergizer/doc/) has a very straightforward interface and works pretty well for most lists. Note: it hasn't been updated in a while." />
  <row Id="1474" PostHistoryTypeId="2" PostId="493" RevisionGUID="36088a7c-314c-4473-a6d9-ea72a7c23132" CreationDate="2017-06-06T15:04:52.107" UserId="110" Text="The kind of QC you do routinely depends on what your lab's focus is. We do a lot of low-quality, multiplexed DNA and RNA. If you routinely do fresh frozen whole genomes, your QC will be different.&#xD;&#xA;&#xD;&#xA;Weighing in from the resequencing side of things (i.e. sequencing an organism that has a good reference), there are several things we are testing with quality control:&#xD;&#xA;&#xD;&#xA; - Did the library preparation succeed? Was the capture successful? Is our input material of high enough quality? &#xD;&#xA; - Did the sequencing work? For Illumina, did enough clusters pass filter? What's our Q30? Did we sequence into adapters?&#xD;&#xA; - Can we proceed with analysis? Are the indices on multiplexed samples correct?&#xD;&#xA; Do we have the correct reference sequence? Do we have the right targets?&#xD;&#xA;&#xD;&#xA;You may use a combination of tools:&#xD;&#xA;&#xD;&#xA;* [bcl2fastq](https://support.illumina.com/sequencing/sequencing_software/bcl2fastq-conversion-software.html) : if you multiplexed, check the undetermined indices file after converting to fastq to make sure you caught all of the indexed reads&#xD;&#xA;* [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/): General sequencing quality, sequencing into adapters, handy flagging for obvious problems&#xD;&#xA;* [Illumina interop](https://github.com/Illumina/interop)/SAV: Cluster pass filter, Q30&#xD;&#xA;* Alignment (with [BWA](http://bio-bwa.sourceforge.net/), bowtie, etc), [bedtools](http://bedtools.readthedocs.io/en/latest/), [samtools](http://www.htslib.org/), [Picard](https://broadinstitute.github.io/picard/): check for contamination, make sure we're using the right reference, overlap with target regions, depth of coverage, soft-clipping for low-quality sequence&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1475" PostHistoryTypeId="2" PostId="494" RevisionGUID="38bd5ea9-eca0-40ab-863d-985aa2284d6b" CreationDate="2017-06-06T15:26:23.913" UserId="110" Text="If you have gVCFs, the first thing you should try is joint variant calling. According to GATK, joint variant calling &quot;empowers variant discovery by providing the ability to leverage population-wide information from a cohort of multiple sample[sic], allowing us to detect variants with great sensitivity and genotype samples as accurately as possible.&quot; [source](https://software.broadinstitute.org/gatk/documentation/article.php?id=4150). &#xD;&#xA;&#xD;&#xA;Once you have two sets of high-quality variants, what you do next depends on your research question. Are you looking for druggable mutations? Underlying causes? Biomarkers? Patient prognosis? You can look at the most frequently mutated positions in the patient cohort and compare them to your reference cohort, check for co-occurring mutations, cluster them, do principal component analysis, do some machine learning to stratify them, etc. " />
  <row Id="1476" PostHistoryTypeId="2" PostId="495" RevisionGUID="86025a7a-34b5-4cce-b49b-bea975e3329e" CreationDate="2017-06-06T15:31:11.623" UserId="215" Text="**DRMAA** (Distributed Resource Management Application API) appears to be an open API that describes a specification for submission and management of work submitted to some grid/cluster. [If your scheduler is DRMAA compliant](https://en.wikipedia.org/wiki/DRMAA#Implementations), my guess would be that using `snakemake`'s `--drmaa` flag will afford you the additional controls exposed by that API.&#xD;&#xA;&#xD;&#xA;As mentioned in your question with DRMAA support, a Ctrl+C on one's console would be passed on to the grid to kill or stop submitted jobs. A task that would otherwise require use of the `qdel` command on `Sun Grid Engine`, for example.&#xD;&#xA;&#xD;&#xA;I could see why the suggestion would be to enable `--drmaa` where possible as it's likely to expose functionality (such as control of jobs, and availability of logs and errors) that makes the submission and management of jobs a little easier for you and `snakemake`.&#xD;&#xA;&#xD;&#xA;Hope this helps?" />
  <row Id="1477" PostHistoryTypeId="5" PostId="495" RevisionGUID="730ec7a9-fa7e-4866-a7ae-2b857f6b2792" CreationDate="2017-06-06T15:36:38.690" UserId="215" Comment="added 414 characters in body" Text="**DRMAA** (Distributed Resource Management Application API) appears to be an open API that describes a specification for submission and management of work submitted to some grid/cluster. [If your scheduler is DRMAA compliant](https://en.wikipedia.org/wiki/DRMAA#Implementations), my guess would be that using `snakemake`'s `--drmaa` flag will afford you the additional controls exposed by that API.&#xD;&#xA;&#xD;&#xA;As mentioned in your question with DRMAA support, a Ctrl+C on one's console would be passed on to the grid to kill or stop submitted jobs. A task that would otherwise require use of the `qdel` command on `Sun Grid Engine`, for example.&#xD;&#xA;&#xD;&#xA;I could see why the suggestion would be to enable `--drmaa` where possible as it's likely to expose functionality (such as control of jobs, and availability of logs and errors) that makes the submission and management of jobs a little easier for you and `snakemake`. Perhaps someone with `snakemake` experience can explain a little more, but I hope this helps?&#xD;&#xA;&#xD;&#xA;For what it's worth, I use Sun Grid Engine without DRMAA and it's quite a pain to get good information out of `qstat` and its `qacct` log format is quite possible the worst file format I've ever seen. I suspect DRMAA provides a nice (or at least more reasonable) API for `snakemake` (and others) to get at this information more readily." />
  <row Id="1478" PostHistoryTypeId="5" PostId="491" RevisionGUID="f3afde6e-f90d-4829-bb7a-aef35c7eefa5" CreationDate="2017-06-06T15:39:47.827" UserId="104" Comment="added 158 characters in body; edited title" Text="I'm fairly new to [snakemake][1] and I'm trying to understand the difference between the `--cluster` and `--drmaa` flags, both of which are used to submit jobs to compute clusters/nodes.&#xD;&#xA;&#xD;&#xA;The docs give a few hints about the advantages of using `--drmaa` [here][2]:&#xD;&#xA;&#xD;&#xA;&gt; If your cluster system supports DRMAA, Snakemake can make use of that&#xD;&#xA;&gt; to increase the control over jobs. E.g. jobs can be cancelled upon&#xD;&#xA;&gt; pressing Ctrl+C, which is not possible with the generic --cluster&#xD;&#xA;&gt; support.&#xD;&#xA;&#xD;&#xA;And [here][3]:&#xD;&#xA;&#xD;&#xA;&gt; If available, DRMAA is preferable over the generic cluster modes&#xD;&#xA;&gt; because it provides better control and error handling.&#xD;&#xA;&#xD;&#xA;So I have a conceptual understanding of the advantages of using `--drmaa`. However, I don't consider the above a very complete explanation, and I don't know how these flags are implemented in snakemake under the hood, can someone elaborate?&#xD;&#xA;&#xD;&#xA;Note, that although this could be considered a more general programming question, snakemake is predominantly used in bioinformatics; I was persuaded that this question would be considered on-topic because of [this meta-post][4] and [this answer][5]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://snakemake.readthedocs.io/en/stable/&#xD;&#xA;  [2]: http://snakemake.readthedocs.io/en/stable/executable.html&#xD;&#xA;  [3]: http://snakemake.readthedocs.io/en/stable/tutorial/additional_features.html&#xD;&#xA;  [4]: https://bioinformatics.meta.stackexchange.com/questions/61/serving-as-a-support-forum-for-specific-tools&#xD;&#xA;  [5]: https://bioinformatics.meta.stackexchange.com/a/66/104" />
  <row Id="1479" PostHistoryTypeId="4" PostId="491" RevisionGUID="f3afde6e-f90d-4829-bb7a-aef35c7eefa5" CreationDate="2017-06-06T15:39:47.827" UserId="104" Comment="added 158 characters in body; edited title" Text="How are snakemake's --cluster and --drmaa options implemented?" />
  <row Id="1480" PostHistoryTypeId="5" PostId="495" RevisionGUID="39c1d6d8-fce5-4305-bd40-e771a9cd7a45" CreationDate="2017-06-06T16:03:19.040" UserId="215" Comment="spelling" Text="**DRMAA** (Distributed Resource Management Application API) appears to be an open API that describes a specification for submission and management of work submitted to some grid/cluster. [If your scheduler is DRMAA compliant](https://en.wikipedia.org/wiki/DRMAA#Implementations), my guess would be that using `snakemake`'s `--drmaa` flag will afford you the additional controls exposed by that API.&#xD;&#xA;&#xD;&#xA;As mentioned in your question with DRMAA support, a Ctrl+C on one's console would be passed on to the grid to kill or stop submitted jobs. A task that would otherwise require use of the `qdel` command on `Sun Grid Engine`, for example.&#xD;&#xA;&#xD;&#xA;I could see why the suggestion would be to enable `--drmaa` where possible as it's likely to expose functionality (such as control of jobs, and availability of logs and errors) that makes the submission and management of jobs a little easier for you and `snakemake`. Perhaps someone with `snakemake` experience can explain a little more, but I hope this helps?&#xD;&#xA;&#xD;&#xA;For what it's worth, I use Sun Grid Engine without DRMAA and it's quite a pain to get good information out of `qstat` and its `qacct` log format is quite possible the worst file format I've ever seen. I suspect DRMAA provides a nice (or at least more reasonable) API for `snakemake` (and others) to get at this information more readily.&#xD;&#xA;&#xD;&#xA;**Appended:**&#xD;&#xA;&#xD;&#xA;Under the hood, the flags are parsed by Python's `argparse` module in the `__init__.py`. [A mutually exclusive group of options](https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/__init__.py?fileviewer=file-view-default#__init__.py-747) forces a selection of `--cluster` or `--drmaa` (or `--cluster-sync`).&#xD;&#xA;&#xD;&#xA;`--cluster` or `-c` requires you to specify an argument; the command for which to submit a job on your cluster (the example names `qsub`). The `--drmaa` flag just seems to indicate to `snakemake` that DRMAA is to be used, which doesn't change a lot in terms of how the command you run from your console is processed in the `__init__.py` or [`workflow.py`](https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/workflow.py?at=master&amp;fileviewer=file-view-default).&#xD;&#xA;&#xD;&#xA;However, when it is time to interface with your scheduler (as specified in [`scheduler.py`](https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/scheduler.py?at=master&amp;fileviewer=file-view-default)), an `elif` statement checks whether you're in some form of cluster mode and the `else` catches the case where you've raised the `--drmaa` flag instead.&#xD;&#xA;&#xD;&#xA;Here's where the magic happens, as now your job is submitted with the `DRMAAExecutor` as specified in [executors.py](https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/executors.py?at=master&amp;fileviewer=file-view-default), instead of the default executor.&#xD;&#xA;&#xD;&#xA;Without investigating much more, I can see that the `DRMAAExecutor` features some more class attributes and exposes additional functions when compared to the others.&#xD;&#xA;&#xD;&#xA;Hope this is of more use? Again, I've only just looked at the code myself but in lieu of a `snakemake` user chiming in, I thought I'd have a nose." />
  <row Id="1482" PostHistoryTypeId="2" PostId="496" RevisionGUID="c458b0f6-bd9f-4b5a-b09e-7f7da97f6a6a" CreationDate="2017-06-06T17:40:13.550" UserId="224" Text="Agree with the others that it doesn't really matter. One thing to note though - if you're deduplicating your BAM files (you probably should for ChIP-seq data), make sure that you do this _after_ merging.. :)" />
  <row Id="1483" PostHistoryTypeId="2" PostId="497" RevisionGUID="f7fc49b1-8a38-4065-8ff6-0c0b151391ff" CreationDate="2017-06-06T17:52:17.670" UserId="612" Text="If you're comfortable doing a little programming, check out mygene.info (web services for gene annotations of all sorts).  ID translation is specifically one of the use cases addressed in the [bioconductor client][2] (see the vignette), and there is a [python client][4] as well available through pypi.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://mygene.info&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/bioc/html/mygene.html&#xD;&#xA;  [3]: https://bioconductor.org/packages/release/bioc/vignettes/mygene/inst/doc/mygene.pdf&#xD;&#xA;  [4]: https://pypi.python.org/pypi/mygene&#xD;&#xA;  [5]: http://mygene-py.readthedocs.io/en/latest/" />
  <row Id="1485" PostHistoryTypeId="2" PostId="498" RevisionGUID="d55fbcfe-4d24-4d3a-b797-b43c3611ec82" CreationDate="2017-06-06T18:32:15.170" UserId="206" Text="You can do the same using [AnnotationDbi][1] Package from Bioconductor. Download the organism specific annotation file like [org.Mm.eg.db][2] for mouse and map current gene ids to the gene names/gene symbols.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.bioconductor.org/packages/release/bioc/html/AnnotationDbi.html&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/data/annotation/html/org.Mm.eg.db.html" />
  <row Id="1486" PostHistoryTypeId="5" PostId="495" RevisionGUID="e9831a98-6df1-4185-8fc8-475ccf6d286e" CreationDate="2017-06-06T18:54:34.823" UserId="215" Comment="Spag" Text="**DRMAA** (Distributed Resource Management Application API) appears to be an open API that describes a specification for submission and management of work submitted to some grid/cluster. [If your scheduler is DRMAA compliant](https://en.wikipedia.org/wiki/DRMAA#Implementations), my guess would be that using `snakemake`'s `--drmaa` flag will afford you the additional controls exposed by that API.&#xD;&#xA;&#xD;&#xA;As mentioned in your question with DRMAA support, a Ctrl+C on one's console would be passed on to the grid to kill or stop submitted jobs. A task that would otherwise require use of the `qdel` command on `Sun Grid Engine`, for example.&#xD;&#xA;&#xD;&#xA;I could see why the suggestion would be to enable `--drmaa` where possible as it's likely to expose functionality (such as control of jobs, and availability of logs and errors) that makes the submission and management of jobs a little easier for you and `snakemake`. Perhaps someone with `snakemake` experience can explain a little more, but I hope this helps?&#xD;&#xA;&#xD;&#xA;For what it's worth, I use Sun Grid Engine without DRMAA and it's quite a pain to get good information out of `qstat`, and its `qacct` log format is quite possibly the worst file format I have ever encountered. I suspect DRMAA provides a nice (or at least more reasonable) API for `snakemake` (and others) to get at this information more readily.&#xD;&#xA;&#xD;&#xA;**Appended:**&#xD;&#xA;&#xD;&#xA;Under the hood, the flags are parsed by Python's `argparse` module in the `__init__.py`. [A mutually exclusive group of options](https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/__init__.py?fileviewer=file-view-default#__init__.py-747) forces a selection of `--cluster` or `--drmaa` (or `--cluster-sync`).&#xD;&#xA;&#xD;&#xA;`--cluster` or `-c` requires you to specify an argument; the command for which to submit a job on your cluster (the example names `qsub`). The `--drmaa` flag just seems to indicate to `snakemake` that DRMAA is to be used, which doesn't change a lot in terms of how the command you run from your console is processed in the `__init__.py` or [`workflow.py`](https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/workflow.py?at=master&amp;fileviewer=file-view-default).&#xD;&#xA;&#xD;&#xA;However, when it is time to interface with your scheduler (as specified in [`scheduler.py`](https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/scheduler.py?at=master&amp;fileviewer=file-view-default)), an `elif` statement checks whether you're in some form of cluster mode and the `else` catches the case where you've raised the `--drmaa` flag instead.&#xD;&#xA;&#xD;&#xA;Here's where the magic happens, as now your job is submitted with the `DRMAAExecutor` as specified in [executors.py](https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/executors.py?at=master&amp;fileviewer=file-view-default), instead of the default executor.&#xD;&#xA;&#xD;&#xA;Without investigating much more, I can see that the `DRMAAExecutor` features some more class attributes and exposes additional functions when compared to the others.&#xD;&#xA;&#xD;&#xA;Hope this is of more use? Again, I've only just looked at the code myself but in lieu of a `snakemake` user chiming in, I thought I'd have a nose." />
  <row Id="1487" PostHistoryTypeId="2" PostId="499" RevisionGUID="8cdcba54-18a1-461e-b27c-8f6aedcea703" CreationDate="2017-06-06T19:05:40.117" UserId="96" Text="The euclidean distance is probably the simplest, both conceptually and in terms of implementation. It's by no means an elegant solution, and it may not perform well in certain circumstances.&#xD;&#xA;&#xD;&#xA;Euclidean distance is easiest to conceptualize as the distance between two points in a two dimensional space.&#xD;&#xA;&#xD;&#xA;    Y&#xD;&#xA;    ^&#xD;&#xA;    |&#xD;&#xA;    |&#xD;&#xA;    |&#xD;&#xA;    |            * p = (3, 3)&#xD;&#xA;    |&#xD;&#xA;    |&#xD;&#xA;    |&#xD;&#xA;    |        * q = (2, 1)&#xD;&#xA;    |&#xD;&#xA;    ----------------------------&gt; X&#xD;&#xA;&#xD;&#xA;In this example, the distance between the two points is&#xD;&#xA;&#xD;&#xA;    d(p, q) = sqrt( (p_x-q_x)^2 + (p_y-q_y)^2 )&#xD;&#xA;            = sqrt( (2-3)^2 + (1-3)^2 )&#xD;&#xA;            = sqrt(5)&#xD;&#xA;            ≈ 2.24&#xD;&#xA;&#xD;&#xA;For a gene expression profile with two genes, this is exactly how the euclidean distance would be calculated, using expression values from one gene as the X axis and expression values from the other gene as the Y axis. Realistically, though, gene expression profiles typically contain thousands or tens of thousands of genes, so instead we use the generalization of the distance calculation for N dimensions.&#xD;&#xA;&#xD;&#xA;    d(p, q) = sqrt( (p_1-q_1)^2 + (p_2-q_2)^2 + ... + (p_N-q_N)^2 )&#xD;&#xA;&#xD;&#xA;Packages for R and Python make these types of calculations trivial once you have the data loaded into the correct data structure. See Ian's answer for some example R code." />
  <row Id="1489" PostHistoryTypeId="2" PostId="500" RevisionGUID="cd8ab545-7300-4056-a270-4c8ca2b7fe5d" CreationDate="2017-06-06T19:19:47.827" UserId="96" Text="I am writing a software tool to which I would like to add the ability to compute alignments using the efficient Burrows-Wheeler Transform (BWT) approach made popular by tools such as BWA and Bowtie. As far as I can tell, though, both of these tools and their derivatives are invoked strictly via a command-line interface. Are there any libraries that implement BWT-based read alignment with a C/C++ API?&#xD;&#xA;&#xD;&#xA;Python bindings would also be great, but probably too much to expect." />
  <row Id="1490" PostHistoryTypeId="1" PostId="500" RevisionGUID="cd8ab545-7300-4056-a270-4c8ca2b7fe5d" CreationDate="2017-06-06T19:19:47.827" UserId="96" Text="Library for computing BWT-based alignments" />
  <row Id="1491" PostHistoryTypeId="3" PostId="500" RevisionGUID="cd8ab545-7300-4056-a270-4c8ca2b7fe5d" CreationDate="2017-06-06T19:19:47.827" UserId="96" Text="&lt;alignment&gt;&lt;bwa&gt;&lt;mapping&gt;&lt;api&gt;" />
  <row Id="1492" PostHistoryTypeId="2" PostId="501" RevisionGUID="b2dc5834-a460-4b0a-83b3-56f09901684e" CreationDate="2017-06-06T19:38:15.237" UserId="77" Text="I'd always kind of wondered how this worked too, so I took this as an excuse to look into the snakemake code. At the end of the day this becomes a question of (1) how are jobs actually submitted and (2) how is it determined if jobs are done (and then whether they failed)?&#xD;&#xA;&#xD;&#xA;For DRMAA, python has a module (appropriately named &quot;drmaa&quot;) that wraps around the libdrmaa library that comes with most schedulers. This is a very popular route, for example the Galaxy project uses this for dealing with most clusters (e.g., I use it to connect our internal Galaxy instance to our slurm cluster). The huge benefit here is that DRMAA does magic to allow you to simply submit commands to your cluster without needing to know whether you should execute `qsub` or `srun` or something else. Further, it then provides methods to simply query whether a job is running or not and what its exit status was.&#xD;&#xA;&#xD;&#xA;Using the `--cluster` command requires a LOT more magic on the side of snakemake. At the end of the day it creates a shell script that it then submits using the command you provided. Importantly, it includes some secret files in that script that it can then watch for (ever noticed that `.snakemake` directory where you execute a script? That appears to be among the things it's used for.). These are named `{jobid}.jobfinished` and `{jobid}.jobfailed` and one of them will get touched, depending on the exit status of your command/script. Once one of those is there, then snakemake can move on in its DAG (or not, if there's a failure). This is obviously a LOT more to keep track of and it's then not possible for snakemake to cancel running jobs, which is something one can do easily with DRMAA." />
  <row Id="1493" PostHistoryTypeId="2" PostId="502" RevisionGUID="cc45c107-e855-4719-87d6-2a937d8d864e" CreationDate="2017-06-06T19:44:56.890" UserId="29" Text="[SeqAn](http://seqan.de/) supports BTW tables for use with their parametrisable alignment algorithms.&#xD;&#xA;&#xD;&#xA;To use it, [follow the general outline for building a SeqAn short-read aligner](http://seqan.readthedocs.io/en/seqan-v1.4.2/Tutorial/SimpleReadMapping.html), and use the [`FMIndex`](http://docs.seqan.de/seqan/2.2.0/index.html?p=FMIndex) specialisation instead of — as in the example — the `IndexQGram`." />
  <row Id="1494" PostHistoryTypeId="5" PostId="376" RevisionGUID="9691482d-fbb7-4511-a45f-5693507796a4" CreationDate="2017-06-06T19:45:54.183" UserId="73" Comment="clarified Debian default, as per Matthew Bashton's answer" Text="The generic command ```sh``` is quite literally an industry standard, a POSIX standard, to be precise (IEEE 1003.2 and 1003.2a, available for purchase for hundreds of dollars at various websites). In theory, any script that starts with ```#!/bin/sh``` should conform to this standard. In practise, most Linux systems have a shell that is close to this standard, but has a few quirks and extensions.&#xD;&#xA;&#xD;&#xA;Problems crop up when these quirks and extensions become standard practise in shell scripts. The Debian operating system changed to ```dash``` as their ```sh``` shell to encourage people to stop using &quot;bashisms&quot; in shell scripts that didn't specify a particular shell, i.e. those that began with ```#!/bin/sh```. The ```dash``` shell tries to be as standards-compliant as possible:&#xD;&#xA;&#xD;&#xA;&gt; dash is the standard command interpreter for the system.  The current version of dash is in the process of being changed to&#xD;&#xA;     conform with the POSIX 1003.2 and 1003.2a specifications for the shell.  This version has many features which make it&#xD;&#xA;     appear similar in some respects to the Korn shell, but it is not a Korn shell clone (see ksh(1)).  Only features designated&#xD;&#xA;     by POSIX, plus a few Berkeley extensions, are being incorporated into this shell.  This man page is not intended to be a&#xD;&#xA;     tutorial or a complete specification of the shell.&#xD;&#xA;&#xD;&#xA;I'm not familiar with the differences, and generally try to stick to the ```sh``` man pages to instruct me with regards to correct standards-compliant shell scripts." />
  <row Id="1495" PostHistoryTypeId="5" PostId="502" RevisionGUID="dd4d6bf7-6a54-4772-b7fb-5b51ec99003b" CreationDate="2017-06-06T19:50:47.123" UserId="73" Comment="initialism fix" Text="[SeqAn](http://seqan.de/) supports BWT tables for use with their parametrisable alignment algorithms.&#xD;&#xA;&#xD;&#xA;To use it, [follow the general outline for building a SeqAn short-read aligner](http://seqan.readthedocs.io/en/seqan-v1.4.2/Tutorial/SimpleReadMapping.html), and use the [`FMIndex`](http://docs.seqan.de/seqan/2.2.0/index.html?p=FMIndex) specialisation instead of — as in the example — the `IndexQGram`." />
  <row Id="1496" PostHistoryTypeId="5" PostId="497" RevisionGUID="a00aaf14-c5c9-40b5-8d55-a2c8ac6f8ec6" CreationDate="2017-06-06T19:53:55.077" UserId="73" Comment="put in anchors for included hyperlinks" Text="If you're comfortable doing a little programming, check out [mygene.info][1] (web services for gene annotations of all sorts).  ID translation is specifically one of the use cases addressed in the [bioconductor client][2] (see the [vignette][3]), and there is a [python client][4] as well available through pypi. The documentation for mygene can be found [here][5].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://mygene.info&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/bioc/html/mygene.html&#xD;&#xA;  [3]: https://bioconductor.org/packages/release/bioc/vignettes/mygene/inst/doc/mygene.pdf&#xD;&#xA;  [4]: https://pypi.python.org/pypi/mygene&#xD;&#xA;  [5]: http://mygene-py.readthedocs.io/en/latest/" />
  <row Id="1497" PostHistoryTypeId="2" PostId="503" RevisionGUID="3c1a91f7-8bb5-4a0b-9618-eb1e89482871" CreationDate="2017-06-06T20:17:34.890" UserId="425" Text="First, let us remark that there exist several hundred read mappers, most of which have been even published (see, e.g., pages 25-29 of [this thesis](http://brinda.cz/publications/brinda_phd.pdf)). Developing a new mapper probably makes sense only as a programming exercise. Whereas developing a quick proof-of-concept read mapper is usually easy, turning it into a real competitor of existing and well-tuned mappers can last for years.&#xD;&#xA;&#xD;&#xA;It is not clear from the provided description how long is your reference, how many alignments you need to compute, etc. In certain situations, it may be useful to write a wrapper over existing mappers; while in some other situations, standard dynamic programming may be sufficient.&#xD;&#xA;&#xD;&#xA;Let assume that you want to develop a toy read mapper. Most of read mappers are based on a so called seed-and-extend paradigm. Simply speaking, first you detect candidates for alignments, usually as exact matches between a read and the reference (using either a hash table or some full-text index – e.g., BWT-index). Then you would need to compute alignments for these candidates, typically using some algorithm based on dynamic programming, and report the best obtained alignments (e.g., the ones with the highest alignment score)&#xD;&#xA;&#xD;&#xA;There exist two big, powerful and well debugged libraries implementing BWT-indexes which can be easily used for building a read mapper:&#xD;&#xA;&#xD;&#xA;* [SeqAn](http://seqan.rtfd.io). See the [FMIndex tutorial](http://seqan.readthedocs.io/en/master/Tutorial/DataStructures/Indices/FMIndex.html?highlight=bwt) and the [Pairwise Sequence Alignment tutorial](http://seqan.readthedocs.io/en/master/Tutorial/Algorithms/Alignment/PairwiseSequenceAlignment.html) for quick examples of how to detect exact matches and how to do pairwise alignments. Also, they provide a tutorial about a [quick development of a read mapper](http://seqan.readthedocs.io/en/seqan-v1.4.2/Tutorial/SimpleReadMapping.html), but the resulting mapper seems to be hash-table based, not BWT-based. SeqAn is used, e.g., in [YARA](https://github.com/seqan/seqan/tree/master/apps/yara).&#xD;&#xA;&#xD;&#xA;* [SDSL-Lite](https://github.com/simongog/sdsl-lite). This is a general library for compact data structures. See the [tutorial slides](http://simongog.github.io/assets/data/sdsl-slides/tutorial#1) for an idea how it works. For instance, [GramTools](https://github.com/iqbal-lab/gramtools) are based on SDSL." />
  <row Id="1498" PostHistoryTypeId="5" PostId="503" RevisionGUID="704d49ea-6010-4678-9669-c9eb3c0528cf" CreationDate="2017-06-06T20:36:18.473" UserId="425" Comment="Add some other links" Text="First, let us remark that there exist several hundred read mappers, most of which have been even published (see, e.g., pages 25-29 of [this thesis](http://brinda.cz/publications/brinda_phd.pdf)). Developing a new mapper probably makes sense only as a programming exercise. Whereas developing a quick proof-of-concept read mapper is usually easy, turning it into a real competitor of existing and well-tuned mappers can last for years.&#xD;&#xA;&#xD;&#xA;It is not clear from the provided description how long is your reference, how many alignments you need to compute, etc. In certain situations, it may be useful to write a wrapper over existing mappers (e.g., using [subprocess.Popen](https://docs.python.org/3/library/subprocess.html#subprocess.Popen) for running the mapper and [PySam](http://pysam.readthedocs.io/) for parsing the output); while in some other situations, standard dynamic programming may be sufficient (e.g., using [SSW](https://github.com/mengyao/complete-striped-smith-waterman-library) with its Python binding).&#xD;&#xA;&#xD;&#xA;Let assume that you want to develop a toy read mapper. Most of read mappers are based on a so called seed-and-extend paradigm. Simply speaking, first you detect candidates for alignments, usually as exact matches between a read and the reference (using either a hash table or some full-text index – e.g., BWT-index). Then you would need to compute alignments for these candidates, typically using some algorithm based on dynamic programming, and report the best obtained alignments (e.g., the ones with the highest alignment score)&#xD;&#xA;&#xD;&#xA;There exist two big, powerful and well debugged libraries implementing BWT-indexes which can be easily used for building a read mapper:&#xD;&#xA;&#xD;&#xA;* [SeqAn](http://seqan.rtfd.io). See the [FMIndex tutorial](http://seqan.readthedocs.io/en/master/Tutorial/DataStructures/Indices/FMIndex.html?highlight=bwt) and the [Pairwise Sequence Alignment tutorial](http://seqan.readthedocs.io/en/master/Tutorial/Algorithms/Alignment/PairwiseSequenceAlignment.html) for quick examples of how to detect exact matches and how to do pairwise alignments. Also, they provide a tutorial about a [quick development of a read mapper](http://seqan.readthedocs.io/en/seqan-v1.4.2/Tutorial/SimpleReadMapping.html), but the resulting mapper seems to be hash-table based, not BWT-based. SeqAn is used, e.g., in [YARA](https://github.com/seqan/seqan/tree/master/apps/yara).&#xD;&#xA;&#xD;&#xA;* [SDSL-Lite](https://github.com/simongog/sdsl-lite). This is a general library for compact data structures. See the [tutorial slides](http://simongog.github.io/assets/data/sdsl-slides/tutorial#1) for an idea how it works. For instance, [GramTools](https://github.com/iqbal-lab/gramtools) are based on SDSL." />
  <row Id="1499" PostHistoryTypeId="5" PostId="503" RevisionGUID="f203632c-f8ce-48b9-98d7-32d09f483888" CreationDate="2017-06-06T20:46:40.420" UserId="425" Comment="compact =&gt; succinct (SDSL implements also non-succinct data structures)" Text="First, let us remark that there exist several hundred read mappers, most of which have been even published (see, e.g., pages 25-29 of [this thesis](http://brinda.cz/publications/brinda_phd.pdf)). Developing a new mapper probably makes sense only as a programming exercise. Whereas developing a quick proof-of-concept read mapper is usually easy, turning it into a real competitor of existing and well-tuned mappers can last for years.&#xD;&#xA;&#xD;&#xA;It is not clear from the provided description how long is your reference, how many alignments you need to compute, etc. In certain situations, it may be useful to write a wrapper over existing mappers (e.g., using [subprocess.Popen](https://docs.python.org/3/library/subprocess.html#subprocess.Popen) for running the mapper and [PySam](http://pysam.readthedocs.io/) for parsing the output); while in some other situations, standard dynamic programming may be sufficient (e.g., using [SSW](https://github.com/mengyao/complete-striped-smith-waterman-library) with its Python binding).&#xD;&#xA;&#xD;&#xA;Let assume that you want to develop a toy read mapper. Most of read mappers are based on a so called seed-and-extend paradigm. Simply speaking, first you detect candidates for alignments, usually as exact matches between a read and the reference (using either a hash table or some full-text index – e.g., BWT-index). Then you would need to compute alignments for these candidates, typically using some algorithm based on dynamic programming, and report the best obtained alignments (e.g., the ones with the highest alignment score).&#xD;&#xA;&#xD;&#xA;There exist two big, powerful and well debugged libraries implementing BWT-indexes which can be easily used for building a read mapper:&#xD;&#xA;&#xD;&#xA;* [SeqAn](http://seqan.rtfd.io). See the [FMIndex tutorial](http://seqan.readthedocs.io/en/master/Tutorial/DataStructures/Indices/FMIndex.html?highlight=bwt) and the [Pairwise Sequence Alignment tutorial](http://seqan.readthedocs.io/en/master/Tutorial/Algorithms/Alignment/PairwiseSequenceAlignment.html) for quick examples of how to detect exact matches and how to do pairwise alignments. Also, they provide a tutorial about a [quick development of a read mapper](http://seqan.readthedocs.io/en/seqan-v1.4.2/Tutorial/SimpleReadMapping.html), but the resulting mapper seems to be hash-table based, not BWT-based. SeqAn is used, e.g., in [YARA](https://github.com/seqan/seqan/tree/master/apps/yara).&#xD;&#xA;&#xD;&#xA;* [SDSL-Lite](https://github.com/simongog/sdsl-lite). This is a general library for succinct data structures. See the [tutorial slides](http://simongog.github.io/assets/data/sdsl-slides/tutorial#1) for an idea of how it works. For instance, [GramTools](https://github.com/iqbal-lab/gramtools) are based on SDSL." />
  <row Id="1500" PostHistoryTypeId="2" PostId="504" RevisionGUID="cffc4bf5-c597-4e1e-ab79-b9979fc32fb9" CreationDate="2017-06-06T20:53:30.430" UserId="630" Text="I think others have provided fine answers to your question already. I just thought it might be relevant to mention about a tool that allows using Spark to distribute computations that re-use existing commandline bioinformatics software ... so, avoiding the need to re-implement proven algorithms in a language that Spark supports, [EasyMapReduce][1]. &#xD;&#xA;&#xD;&#xA;This perceived need to re-implement algorithms, has otherwise been one of the biggest road blocks for wider adoption of Spark in the bioinformatics community in my understanding. &#xD;&#xA;&#xD;&#xA;EasyMapReduce was presented SparkSummit East this year (video link in repo).&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/mcapuccini/EasyMapReduce" />
  <row Id="1501" PostHistoryTypeId="2" PostId="505" RevisionGUID="d8021cf7-ea89-4662-bb39-36d9611d7ba5" CreationDate="2017-06-06T20:58:15.443" UserId="626" Text="I recently got some EPIC DNA methylation data and I was wondering what are some good practices to follow? &#xD;&#xA;I am interested in knowing about normalization and differential analysis. Thank you." />
  <row Id="1502" PostHistoryTypeId="1" PostId="505" RevisionGUID="d8021cf7-ea89-4662-bb39-36d9611d7ba5" CreationDate="2017-06-06T20:58:15.443" UserId="626" Text="What are some good practices to follow during EPIC DNA methylation data analysis?" />
  <row Id="1503" PostHistoryTypeId="3" PostId="505" RevisionGUID="d8021cf7-ea89-4662-bb39-36d9611d7ba5" CreationDate="2017-06-06T20:58:15.443" UserId="626" Text="&lt;r&gt;&lt;normalization&gt;" />
  <row Id="1505" PostHistoryTypeId="2" PostId="506" RevisionGUID="b61609c7-8b28-4f5b-87b3-6a9b3911d0d2" CreationDate="2017-06-06T21:32:30.483" UserId="634" Text="If you know the eukaryotic contaminant present you could use `bbsplit.sh` from [BBMap suite][1] to split/bin reads first using a reference for that contaminant (into one or as many bins as reference sequences you provide). &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://sourceforge.net/projects/bbmap/" />
  <row Id="1507" PostHistoryTypeId="5" PostId="504" RevisionGUID="51fb9178-d29a-490e-bd15-2b72c83e8673" CreationDate="2017-06-06T21:40:05.000" UserId="73" Comment="linked directly to video" Text="I think others have provided fine answers to your question already. I just thought it might be relevant to mention about a tool that allows using Spark to distribute computations that re-use existing commandline bioinformatics software ... so, avoiding the need to re-implement proven algorithms in a language that Spark supports, [EasyMapReduce][1]. &#xD;&#xA;&#xD;&#xA;This perceived need to re-implement algorithms, has otherwise been one of the biggest road blocks for wider adoption of Spark in the bioinformatics community in my understanding. &#xD;&#xA;&#xD;&#xA;EasyMapReduce was presented SparkSummit East this year (video link [here](https://www.youtube.com/watch?v=4C4R9qptUQo), as shown on the repository home page).&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/mcapuccini/EasyMapReduce" />
  <row Id="1508" PostHistoryTypeId="5" PostId="505" RevisionGUID="824b4285-6cb3-469a-ac21-3963502b5ca9" CreationDate="2017-06-06T21:44:17.690" UserId="73" Comment="added EPIC DNA link" Text="I recently got some [EPIC DNA methylation](https://www.illumina.com/products/by-type/microarray-kits/infinium-methylation-epic.html) data and I was wondering what are some good practices to follow? &#xD;&#xA;I am interested in knowing about normalization and differential analysis. Thank you." />
  <row Id="1509" PostHistoryTypeId="6" PostId="505" RevisionGUID="6af60bfe-6e2b-467b-a069-b335e7718144" CreationDate="2017-06-06T21:50:14.550" UserId="73" Comment="edited tags" Text="&lt;r&gt;&lt;normalization&gt;&lt;methylation&gt;&lt;microarray&gt;" />
  <row Id="1510" PostHistoryTypeId="6" PostId="491" RevisionGUID="ce5ad9e5-05f6-4ca4-9dfe-316fe0ef467d" CreationDate="2017-06-06T21:58:34.553" UserId="73" Comment="edited tags" Text="&lt;snakemake&gt;&lt;documentation&gt;" />
  <row Id="1512" PostHistoryTypeId="2" PostId="508" RevisionGUID="0143ea87-86ce-46e3-846e-8e1f294e18f6" CreationDate="2017-06-06T22:23:31.397" UserId="73" Text="*This is a question from [betsy.s.collins](https://www.biostars.org/u/39680/) on BioStars. The original post can be found [here](https://www.biostars.org/p/256448/).*&#xD;&#xA;&#xD;&#xA;Does anyone have any suggestions for other tags or filtering steps I can use for my data so that my reads only map to one location?&#xD;&#xA;&#xD;&#xA;I am pretty new to bioinformatics and to this forum, and would very much appreciate any suggestions you could offer to help me with ways to filter my alignments (obtained using bwa mem) so I only have uniquely mapped reads, other than using q scores (which I have already done and doesn't solve my problem). Here is a bit more explanation:&#xD;&#xA;&#xD;&#xA;I am trying to obtain uniquely mapped reads from my bwa mem alignments to use in downstream analyses. I realize that &quot;uniquely mapped reads&quot; is a loaded term, and that most sources I have found on the topic suggest filtering by q score should do the trick. My problem is that I have tried filtering by progressively higher q scores and I still have a (very small) number of reads that appear to not be uniquely mapped. Because I am aligning my reads from each sample to 96 different loci (in bwa), I want to make sure my reads are only mapped to one locus.&#xD;&#xA;&#xD;&#xA;For the one sample I am using to test out commands, I filtered my data so I only have properly paired reads and mapped reads (in the case of my unpaired reads) and also filtered by q score (I get nearly the same results at q of 10, 20, or 30, so it's not an issue of changing the q score level), merged all my bam files, and obtained 193,998 reads total for that sample. When I open the bam file in Geneious and count the number of reads for each locus they add up to 194, 050. I realize the difference between these two numbers is an incredibly small number of reads, and I am going to only use loci that have ~20 or 30 mapped reads for each sample (haven't yet decided on that threshold) so maybe this doesn't matter terribly much in the grand scheme, but I still would like to have these two numbers match so I am not incorrectly including loci that weren't actually sequenced for a particular sample.&#xD;&#xA;&#xD;&#xA;I have also seen suggestions to filter by XT tags, but none of my files have this tag. I DO have the XS tag (most seem to be XS:i:0). My understanding of what XS:i:0 means is a bit shaky, but I think it means that the secondary alignment has a score of zero so these would be reads that only have one alignment? And maybe this could be a good tag to use for filtering?&#xD;&#xA;&#xD;&#xA;Here is the applicable portion of code that I am running on alignments obtained from bwa (sorted them by coordinate and converted them to bam already in a previous step). The first line is what I'm using on my paired-end file and the second line is what I am using on my unpaired read 1 file (using the same code for my unpaired read 2 file). Note that I used trimmomatic to remove low quality bases from my reads, which is why I have three different read files for each sample.&#xD;&#xA;&#xD;&#xA;    samtools view -q 10 -f 0x02 -b pe001_sorted.bam &gt; pe001_sorted_properlypaired.bam&#xD;&#xA;    samtools view -q 10 -F 0x04 -b unpaired1_sorted.bam &gt; unpaired1_sorted_mapped.bam&#xD;&#xA;&#xD;&#xA;Thank you very much for any help you can offer!&#xD;&#xA;" />
  <row Id="1513" PostHistoryTypeId="1" PostId="508" RevisionGUID="0143ea87-86ce-46e3-846e-8e1f294e18f6" CreationDate="2017-06-06T22:23:31.397" UserId="73" Text="Obtaining uniquely mapped reads from BWA mem alignment" />
  <row Id="1514" PostHistoryTypeId="3" PostId="508" RevisionGUID="0143ea87-86ce-46e3-846e-8e1f294e18f6" CreationDate="2017-06-06T22:23:31.397" UserId="73" Text="&lt;bam&gt;&lt;alignment&gt;&lt;bwa&gt;&lt;biostars&gt;" />
  <row Id="1515" PostHistoryTypeId="2" PostId="509" RevisionGUID="1aa66a58-544e-42be-926b-99b4685544e0" CreationDate="2017-06-06T22:38:50.510" UserId="-1" Text="" />
  <row Id="1516" PostHistoryTypeId="1" PostId="509" RevisionGUID="1aa66a58-544e-42be-926b-99b4685544e0" CreationDate="2017-06-06T22:38:50.510" UserId="-1" />
  <row Id="1517" PostHistoryTypeId="2" PostId="510" RevisionGUID="ed40ffb1-ae90-48ca-91e3-74787fad9e4b" CreationDate="2017-06-06T22:38:50.510" UserId="-1" Text="" />
  <row Id="1518" PostHistoryTypeId="1" PostId="510" RevisionGUID="ed40ffb1-ae90-48ca-91e3-74787fad9e4b" CreationDate="2017-06-06T22:38:50.510" UserId="-1" />
  <row Id="1519" PostHistoryTypeId="2" PostId="511" RevisionGUID="e83b9207-cceb-4d8a-8ed4-e40e63b6ade8" CreationDate="2017-06-06T23:07:51.320" UserId="37" Text="BWA-MEM can be used as a library. File [bwa/example.c](https://github.com/lh3/bwa/blob/master/example.c) shows the basic functionality for single-end mapping. It should give identical mapping to the bwa-mem command line. Header [bwa/bwamem.h](https://github.com/lh3/bwa/blob/master/bwamem.h) contains basic documentation. Paired-end mapping is doable, but not well exposed. Several teams, including GATK and MS genomics, have been using bwa-mem as a library." />
  <row Id="1520" PostHistoryTypeId="2" PostId="512" RevisionGUID="2d208a9b-3fd1-4738-b666-4127032227bb" CreationDate="2017-06-06T23:10:14.527" UserId="643" Text="EPIC data can be processed in the same manner as the previous iteration of methylation array data from Illumina (450k). This means that starting with .idat files, normalization should be performed (for example, via the minfi package). A [recent paper](http://biorxiv.org/content/early/2016/07/23/065490) from the creators of minfi is particularly helpful because it makes clear that normalized EPIC data from their package can be immediately compared against, for example, [level 3 TCGA data](https://wiki.nci.nih.gov/display/TCGA/Data+level).&#xD;&#xA;&#xD;&#xA;After that, I suggest using the manifest to attach genomic coordinates to your probes and segregating them into functional regions. By testing differential methylation in only regulatory regions, for example, you can increase the statistical power by reducing the overall number of tests to the ones you expect to yield major differences.&#xD;&#xA;&#xD;&#xA;There are existing packages out there for differential methylation analysis, but without knowing your replicate structure or aims, it is difficult to point you in the right direction." />
  <row Id="1521" PostHistoryTypeId="2" PostId="513" RevisionGUID="d96434e2-dd4b-4a32-8b0b-c4aa35aed72e" CreationDate="2017-06-06T23:44:21.290" UserId="548" Text="I'm working on a university project of predicting Translation Initiation Sites in human DNA. I searched the net for papers and documentation to get guidelines and inspiration, but I feel uncertain that I was able to find the state of the art in solving this problem, so I thought of asking the community here: What's the state of the art method of predicting Translation Initiation Sites?&#xD;&#xA;&#xD;&#xA;(I'm also posting the question to help this nascent Q&amp;A site by participating, please close it if it completely misses the mark of being a good question.)" />
  <row Id="1522" PostHistoryTypeId="1" PostId="513" RevisionGUID="d96434e2-dd4b-4a32-8b0b-c4aa35aed72e" CreationDate="2017-06-06T23:44:21.290" UserId="548" Text="State of the art in predicting Translation Initiation Sites" />
  <row Id="1523" PostHistoryTypeId="3" PostId="513" RevisionGUID="d96434e2-dd4b-4a32-8b0b-c4aa35aed72e" CreationDate="2017-06-06T23:44:21.290" UserId="548" Text="&lt;human-genome&gt;&lt;dna&gt;" />
  <row Id="1525" PostHistoryTypeId="2" PostId="515" RevisionGUID="af5e54f0-354c-4c2e-9c8b-21452c16c879" CreationDate="2017-06-07T01:53:56.240" UserId="161" Text="[Ribosome Profiling](http://science.sciencemag.org/content/324/5924/218) experiments profile the positions of ribosome-protected mRNA. a cycloheximide treatment fixes the translating ribosomes. A naive approach to define Translation Initiation Sites (TIS) would be to look at positions where you see distinct 'peaks' in the distribution of these fragments since the ribosome density will be higher at the start of the CDS (at the TIS).&#xD;&#xA;&#xD;&#xA;[TISDB](http://tisdb.human.cornell.edu/) is a database comprised of [GTI-Seq](http://www.pnas.org/content/109/37/E2424.full) global mapping of TIS codons at nearly single-nucleotide resolution. GTI-Seq uses two 'fixers' that fix ribosomes where one of them specifically fixes the initiating ribosomes. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1526" PostHistoryTypeId="2" PostId="516" RevisionGUID="682be769-258f-42d9-81cf-a33132c54851" CreationDate="2017-06-07T03:39:25.560" UserId="651" Text="I'm trying to use HOMER to make a metagene profile over gene bodies using a bedgraph file I've generated. The problem is that every time I do, I get really weird scaling on the y-axis. I should be getting average values across the gene body on the order of 5-10, but instead I'm getting values on the order of 0.03-0.05 or less. The weird thing is, when I don't use metagene script or when I don't use histogram with -size given, I get perfectly normal values -- but that's not what I want, unfortunately, for the metagene profile.&#xD;&#xA;&#xD;&#xA;What am I missing?" />
  <row Id="1527" PostHistoryTypeId="1" PostId="516" RevisionGUID="682be769-258f-42d9-81cf-a33132c54851" CreationDate="2017-06-07T03:39:25.560" UserId="651" Text="What's the scaling for HOMER metagenes?" />
  <row Id="1528" PostHistoryTypeId="3" PostId="516" RevisionGUID="682be769-258f-42d9-81cf-a33132c54851" CreationDate="2017-06-07T03:39:25.560" UserId="651" Text="&lt;ngs&gt;" />
  <row Id="1529" PostHistoryTypeId="2" PostId="517" RevisionGUID="95c8d448-e76d-41b3-91f6-889393ade533" CreationDate="2017-06-07T03:49:20.207" UserId="652" Text="You can use [centrifuge][1] with the NT library to profile the taxonomy, and then remove the reads from eukaryotes. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://ccb.jhu.edu/software/centrifuge/" />
  <row Id="1530" PostHistoryTypeId="2" PostId="518" RevisionGUID="d2244356-03c9-4fdd-ae13-5bb33b160214" CreationDate="2017-06-07T04:20:05.333" UserId="163" Text="I am aligning a dataset of 1,000,000 reads oh human mRNA sequenced on Oxford Nanopore Technologies' MinION, and would like to use the STAR aligner, using the [parameters recommended by Pacific Biosciences][1] for long reads. &#xD;&#xA;&#xD;&#xA;According to [this Google Groups thread][2], in setting up the genome index for short reads, the parameter `sjdbOverhang` should be set to 1 less than the read length. Obviously, with long (mean 1.7Kb, max &gt;50Kb) reads, this doesn't make sense.&#xD;&#xA;&#xD;&#xA;Running `STARlong --runMode genomeGenerate` without setting `sjdbOverhang` sets the parameter to `0`. Does anyone know what this means, how it might affect mapping, and what I should set it to for my long reads?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/PacificBiosciences/cDNA_primer/wiki/Bioinfx-study:-Optimizing-STAR-aligner-for-Iso-Seq-data#param&#xD;&#xA;  [2]: https://groups.google.com/forum/#!searchin/rna-star/sjdboverhang%7Csort:relevance/rna-star/h9oh10UlvhI/ZwAs1xFVCAAJ" />
  <row Id="1531" PostHistoryTypeId="1" PostId="518" RevisionGUID="d2244356-03c9-4fdd-ae13-5bb33b160214" CreationDate="2017-06-07T04:20:05.333" UserId="163" Text="Building STAR Genome Index for nanopore RNA sequencing" />
  <row Id="1532" PostHistoryTypeId="3" PostId="518" RevisionGUID="d2244356-03c9-4fdd-ae13-5bb33b160214" CreationDate="2017-06-07T04:20:05.333" UserId="163" Text="&lt;rna-seq&gt;&lt;alignment&gt;&lt;nanopore&gt;" />
  <row Id="1533" PostHistoryTypeId="2" PostId="519" RevisionGUID="7405535a-496c-4dfd-9ca3-00d0cbf8d75a" CreationDate="2017-06-07T04:21:28.520" UserId="73" Text="I'm going to assume a situation in which a bioinformatician is presented with a mapped BAM file produced by BWA, and has no way of getting the original reads. One high-effort solution would be to extract the mapped reads from the BAM file and re-map with a different mapper that uses the MAPQ score to indicate multiple mappings.&#xD;&#xA;&#xD;&#xA;... but what if that were not possible?&#xD;&#xA;&#xD;&#xA;My understanding of BWA's output is that if a read maps perfectly to multiple genomic locations, it will be given a high mapping quality (MAPQ) score for both locations. Many people expect that a read that maps to at least two locations can have (at best) a 50% probability of mapping to one of those locations (i.e. MAPQ = 3). Because BWA doesn't do this, it makes it difficult to filter out multiply-mapped reads from BWA results using the MAPQ filter that works for other aligners; this is likely to be why the current answer on Biostars [```samtools view -bq 1```] probably won't work.&#xD;&#xA;&#xD;&#xA;Here is an example line from a BWA mem alignment that I've just made. These are Illumina reads mapped to a parasite genome that has a *lot* of repetitive sequence. I might be interested, for example, to find uniquely-mapped regions so that I can generate seeds so that the [TULIP assembler/scaffolder](https://github.com/Generade-nl/TULIP) can do its thing:&#xD;&#xA;&#xD;&#xA;    ERR063640.7     16      tig00019544     79974   21      21M2I56M1I20M   *       0       0       TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC    &lt;AABBEGABFJKKKIM7GHKKJK&gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&gt;D    NM:i:4  MD:Z:83A13      AS:i:77 XS:i:67 XA:Z:tig00019544,-78808,21M2I56M1I20M,6;tig00019544,-84624,79M1I20M,6;tig00019544,-79312,33M4I42M1I20M,8;&#xD;&#xA;&#xD;&#xA;BWA mem has found that this particular read, ERR063460.7, maps to at least three different locations: tig00019544, tig00019544, and tig00019544. Note that the MAPQ for this read is 21, so even though the read maps to multiple locations, MAPQ can't be used to determine that.&#xD;&#xA;&#xD;&#xA;However, the alternative locations are shown by the presence of the ```XA``` tag in the custom fields section of the SAM output. Perhaps just filtering on lines that contain the ```XA``` tag will be able to exclude multiply-mapped reads. The ```samtools view``` man page suggests that ```-x``` will filter out a particular tag:&#xD;&#xA;&#xD;&#xA;    $ samtools view -x XA output.bam | grep '^ERR063640\.7[[:space:]]'&#xD;&#xA;    ERR063640.7	16	tig00019544	79974	21	21M2I56M1I20M	*	0	0	TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC	&lt;AABBEGABFJKKKIM7GHKKJK&gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&gt;D	NM:i:4	MD:Z:83A13	AS:i:77	XS:i:67&#xD;&#xA;&#xD;&#xA;... so it filtered out the tag (i.e. the tag no longer exists in the SAM output), but not the *read*. There are no useful bits in the FLAG field to indicate multiple genomic mappings (which I know can be filtered to exclude the read as well), so I have to resort to other measures.&#xD;&#xA;&#xD;&#xA;In this particular case, I can use ```grep -v``` on the uncompressed SAM output to exclude alignment lines that have the ```XA``` tag (and re-compress to BAM afterwards, just to be tidy):&#xD;&#xA;&#xD;&#xA;    $ samtools view -h output.bam | grep -v 'XA:Z:' | samtools view -b &gt; output_filtered.bam&#xD;&#xA;    $ samtools view output_filtered.bam | grep '^ERR063640\.7[[:space:]]'&#xD;&#xA;    &lt;no output&gt;&#xD;&#xA;&#xD;&#xA;Hurray! reads filtered. As a little aside, this ```grep``` search has a fairly substantial computational load: it's looking for some string with the text ```XA:Z:``` somewhere in the line, and doesn't actually capture every situation. Some masochistic person might come along at a later date and decide that they're going to call all their reads ```HAXXA:Z:AWESOME!:&lt;readNumber&gt;```, in which case a tweak to this grep search would be needed to make sure that there's a space (or more specifically a tab character) prior to the ```XA:Z:```.&#xD;&#xA;&#xD;&#xA;Now I do a check for *any* duplicated read names, just to be sure:&#xD;&#xA;&#xD;&#xA;    $ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xD;&#xA;    ERR063640.1194&#xD;&#xA;    ERR063640.1429&#xD;&#xA;    ERR063640.1761&#xD;&#xA;    ERR063640.2336&#xD;&#xA;    ERR063640.2825&#xD;&#xA;    ERR063640.3458&#xD;&#xA;    ERR063640.4421&#xD;&#xA;    ERR063640.4474&#xD;&#xA;    ERR063640.4888&#xD;&#xA;    ERR063640.49&#xD;&#xA;    ERR063640.4974&#xD;&#xA;    ERR063640.5070&#xD;&#xA;    ERR063640.5130&#xD;&#xA;    ERR063640.5300&#xD;&#xA;    ERR063640.5868&#xD;&#xA;    ERR063640.6116&#xD;&#xA;    ERR063640.6198&#xD;&#xA;    ERR063640.6468&#xD;&#xA;    ERR063640.6717&#xD;&#xA;    ERR063640.6797&#xD;&#xA;    ERR063640.7322&#xD;&#xA;    ERR063640.750&#xD;&#xA;    ERR063640.7570&#xD;&#xA;    ERR063640.7900&#xD;&#xA;    ERR063640.8115&#xD;&#xA;    ERR063640.8405&#xD;&#xA;    ERR063640.911&#xD;&#xA;    ERR063640.9206&#xD;&#xA;    ERR063640.9765&#xD;&#xA;    ERR063640.9986&#xD;&#xA;&#xD;&#xA;Oh... damn. I wonder what they are:&#xD;&#xA;&#xD;&#xA;    $ samtools view output_filtered.bam | grep '^ERR063640.3458[[:space:]]'&#xD;&#xA;    ERR063640.3458	16	tig00002961	5402	60	58S38M	*	0	0	AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATATCCGTATCGCGAAGATCAGGACTTACTCCGCAGAAGAA	DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJDHLNKCK;KMKGMFKJILIIIMKI9JLKKHEJFII?CC	NM:i:0	MD:Z:38	AS:i:38	XS:i:0	SA:Z:tig00002377,202353,-,14M3I5M1I35M38S,19,5;&#xD;&#xA;    ERR063640.3458	2064	tig00002377	202353	19	14M3I5M1I35M38H	*	0	0	AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATA	DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJ	NM:i:5	MD:Z:5G48	AS:i:35	XS:i:27	SA:Z:tig00002961,5402,-,58S38M,60,0;&#xD;&#xA;&#xD;&#xA;Aha! Secondary alignments, which use the *official* ```SA``` tag [other canonical alignments in a chimeric alignment]. These appear to be situations where a single read has been split up, and maps to multiple locations. Note that in this case, ***both of the alignments still have MAPQ scores of over 3***. It sounds like the questioner would also want to get rid of these situations as well. This time, there are standard flag fields as well to deal with these situations (0x100: secondary alignment). Except it's not enough to just filter the secondary alignment, because *both* read mappings should be removed, rather than just the one (or ones) that happened to be tagged as secondary.&#xD;&#xA;&#xD;&#xA;Luckily, BWA appears to put the ```SA``` tag into all reads containing secondary alignments (if this is not the case, I'm sure someone will correct me on that). So, I add in the ```SA``` search as an additional ```grep``` filter:&#xD;&#xA;&#xD;&#xA;    $ samtools view -h output.bam | grep -v -e 'XA:Z:' -e 'SA:Z:' | samtools view -b &gt; output_filtered.bam&#xD;&#xA;    $ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xD;&#xA;    &lt;no output&gt;&#xD;&#xA;&#xD;&#xA;Done. Easy peasy! ```&lt;/sarcasm&gt;```&#xD;&#xA;&#xD;&#xA;... that original &quot;high-effort&quot; solution of using a different aligner doesn't look so bad now." />
  <row Id="1534" PostHistoryTypeId="6" PostId="516" RevisionGUID="8cdd6ec5-0832-4079-9c47-1571f041edb7" CreationDate="2017-06-07T04:27:00.783" UserId="73" Comment="updated tags" Text="&lt;ngs&gt;&lt;bed&gt;&lt;metagenome&gt;&lt;homer&gt;" />
  <row Id="1535" PostHistoryTypeId="6" PostId="518" RevisionGUID="021d31f3-4d03-4dfe-bbf9-33d4ad890dbf" CreationDate="2017-06-07T04:50:06.280" UserId="73" Comment="added 'star' tag" Text="&lt;rna-seq&gt;&lt;alignment&gt;&lt;nanopore&gt;&lt;star&gt;" />
  <row Id="1536" PostHistoryTypeId="2" PostId="520" RevisionGUID="4503385a-ce95-4403-a077-d727be6917ac" CreationDate="2017-06-07T04:50:54.920" UserId="64" Text="The parameter is used to determine how much sequence STAR indexes on each side of a splice junction to improve its alignment accuracy.  For very long reads, this may not be ideal. I am not sure if STAR is capable of including multiple splice junctions since a long read is more than likely to span more than one.  &#xD;&#xA;&#xD;&#xA;IT may be worthwhile to consider aligning your reads directly to transcripts and taking the reads that align poorly and using them to look for novel transcripts." />
  <row Id="1537" PostHistoryTypeId="2" PostId="521" RevisionGUID="f92abcf9-85ec-444d-b910-e9d5d20ad416" CreationDate="2017-06-07T04:58:25.327" UserId="73" Text="I've found the [STAR manual](https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf) to be incredibly helpful with trying to navigate my way round all the STAR command line parameters. Here's the section on the ```sjdbOverhang``` parameter:&#xD;&#xA;&#xD;&#xA;&gt; ```--sjdbOverhang``` specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the *ReadLength-1*, where *ReadLength* is the length of the reads. For instance, for Illumina 2x100b paired-end reads, the ideal value is 100-1=99. In case of reads of varying length, the ideal value is *max(ReadLength)-1*. **In most cases, the default value of 100 will work as well as the ideal value.**&#xD;&#xA;&#xD;&#xA;*[emphasis preserved from the original text]*&#xD;&#xA;&#xD;&#xA;I would recommend trying out running STAR with this value unset, or set to 100, and seeing how it goes. If you want to be particularly adventurous, you could set it to 1000 and see if anything changes." />
  <row Id="1538" PostHistoryTypeId="2" PostId="522" RevisionGUID="c8f498ad-b088-4a2a-a20c-c38b6bc84649" CreationDate="2017-06-07T05:25:27.387" UserId="657" Text="I'm using several datasets that are encoded using either Entrez or Ensembl IDs to specify genes, and need to decide on which to standardise on.&#xA;&#xA;Are there any major reasons to use one over the other?&#xA;&#xA;Which tools and conversion tables are best to use for this?" />
  <row Id="1539" PostHistoryTypeId="1" PostId="522" RevisionGUID="c8f498ad-b088-4a2a-a20c-c38b6bc84649" CreationDate="2017-06-07T05:25:27.387" UserId="657" Text="Entrez or Ensembl gene IDs?" />
  <row Id="1540" PostHistoryTypeId="3" PostId="522" RevisionGUID="c8f498ad-b088-4a2a-a20c-c38b6bc84649" CreationDate="2017-06-07T05:25:27.387" UserId="657" Text="&lt;gene&gt;&lt;conversion&gt;" />
  <row Id="1541" PostHistoryTypeId="2" PostId="523" RevisionGUID="76ad01b7-3c47-4534-ba62-d596128b7070" CreationDate="2017-06-07T06:15:01.453" UserId="651" Text="Figured it out! For anyone who finds this thread in the future and is wondering what is going on: HOMER, when using the -size given option, normalizes the y-axis to the number of basepairs in the intervals -- in this case, the length of the gene body. To get the &quot;raw&quot; values, as I wanted, you need to multiply by the length of the gene body to &quot;undo&quot; the normalization." />
  <row Id="1542" PostHistoryTypeId="6" PostId="441" RevisionGUID="ec5a9700-4c4b-4892-93df-c708fb8a5df0" CreationDate="2017-06-07T06:19:11.957" UserId="163" Comment="removed singleton rna tag" Text="&lt;rna-seq&gt;" />
  <row Id="1543" PostHistoryTypeId="24" PostId="441" RevisionGUID="ec5a9700-4c4b-4892-93df-c708fb8a5df0" CreationDate="2017-06-07T06:19:11.957" Comment="Proposed by 163 approved by 73, 77 edit id of 117" />
  <row Id="1544" PostHistoryTypeId="2" PostId="524" RevisionGUID="fc2dc1eb-f639-49fb-9bad-2bdfbee47206" CreationDate="2017-06-07T06:21:23.533" UserId="186" Text="I have performed RNA-seq analysis using HISAT2 &amp; StringTie workflow suggested in: [Transcript-level expression analysis of RNA-seq experiments with HISAT, StringTie and Ballgown][1]. &#xD;&#xA;&#xD;&#xA;Some of the transcripts/exons have decimal coverage values (eg., 1.1 or 2.59).  &#xD;&#xA;&#xD;&#xA;**My question is:** How can these tools report decimal coverage? If only half of the read overlaps exon will this exon have 0.5 coverage?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.nature.com/nprot/journal/v11/n9/full/nprot.2016.095.html" />
  <row Id="1545" PostHistoryTypeId="1" PostId="524" RevisionGUID="fc2dc1eb-f639-49fb-9bad-2bdfbee47206" CreationDate="2017-06-07T06:21:23.533" UserId="186" Text="How can HISAT2/StringTie report decimal coverage values" />
  <row Id="1546" PostHistoryTypeId="3" PostId="524" RevisionGUID="fc2dc1eb-f639-49fb-9bad-2bdfbee47206" CreationDate="2017-06-07T06:21:23.533" UserId="186" Text="&lt;rna-seq&gt;" />
  <row Id="1547" PostHistoryTypeId="5" PostId="519" RevisionGUID="6caa97bc-9ef0-4eb6-bfdb-2536a4a0a341" CreationDate="2017-06-07T06:54:52.147" UserId="73" Comment="secondary -&gt; supplemental" Text="I'm going to assume a situation in which a bioinformatician is presented with a mapped BAM file produced by BWA, and has no way of getting the original reads. One high-effort solution would be to extract the mapped reads from the BAM file and re-map with a different mapper that uses the MAPQ score to indicate multiple mappings.&#xD;&#xA;&#xD;&#xA;... but what if that were not possible?&#xD;&#xA;&#xD;&#xA;My understanding of BWA's output is that if a read maps perfectly to multiple genomic locations, it will be given a high mapping quality (MAPQ) score for both locations. Many people expect that a read that maps to at least two locations can have (at best) a 50% probability of mapping to one of those locations (i.e. MAPQ = 3). Because BWA doesn't do this, it makes it difficult to filter out multiply-mapped reads from BWA results using the MAPQ filter that works for other aligners; this is likely to be why the current answer on Biostars [```samtools view -bq 1```] probably won't work.&#xD;&#xA;&#xD;&#xA;Here is an example line from a BWA mem alignment that I've just made. These are Illumina reads mapped to a parasite genome that has a *lot* of repetitive sequence. I might be interested, for example, to find uniquely-mapped regions so that I can generate seeds so that the [TULIP assembler/scaffolder](https://github.com/Generade-nl/TULIP) can do its thing:&#xD;&#xA;&#xD;&#xA;    ERR063640.7     16      tig00019544     79974   21      21M2I56M1I20M   *       0       0       TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC    &lt;AABBEGABFJKKKIM7GHKKJK&gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&gt;D    NM:i:4  MD:Z:83A13      AS:i:77 XS:i:67 XA:Z:tig00019544,-78808,21M2I56M1I20M,6;tig00019544,-84624,79M1I20M,6;tig00019544,-79312,33M4I42M1I20M,8;&#xD;&#xA;&#xD;&#xA;BWA mem has found that this particular read, ERR063460.7, maps to at least three different locations: tig00019544, tig00019544, and tig00019544. Note that the MAPQ for this read is 21, so even though the read maps to multiple locations, MAPQ can't be used to determine that.&#xD;&#xA;&#xD;&#xA;However, the alternative locations are shown by the presence of the ```XA``` tag in the custom fields section of the SAM output. Perhaps just filtering on lines that contain the ```XA``` tag will be able to exclude multiply-mapped reads. The ```samtools view``` man page suggests that ```-x``` will filter out a particular tag:&#xD;&#xA;&#xD;&#xA;    $ samtools view -x XA output.bam | grep '^ERR063640\.7[[:space:]]'&#xD;&#xA;    ERR063640.7	16	tig00019544	79974	21	21M2I56M1I20M	*	0	0	TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC	&lt;AABBEGABFJKKKIM7GHKKJK&gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&gt;D	NM:i:4	MD:Z:83A13	AS:i:77	XS:i:67&#xD;&#xA;&#xD;&#xA;... so it filtered out the tag (i.e. the tag no longer exists in the SAM output), but not the *read*. There are no useful bits in the FLAG field to indicate multiple genomic mappings (which I know can be filtered to exclude the read as well), so I have to resort to other measures.&#xD;&#xA;&#xD;&#xA;In this particular case, I can use ```grep -v``` on the uncompressed SAM output to exclude alignment lines that have the ```XA``` tag (and re-compress to BAM afterwards, just to be tidy):&#xD;&#xA;&#xD;&#xA;    $ samtools view -h output.bam | grep -v 'XA:Z:' | samtools view -b &gt; output_filtered.bam&#xD;&#xA;    $ samtools view output_filtered.bam | grep '^ERR063640\.7[[:space:]]'&#xD;&#xA;    &lt;no output&gt;&#xD;&#xA;&#xD;&#xA;Hurray! reads filtered. As a little aside, this ```grep``` search has a fairly substantial computational load: it's looking for some string with the text ```XA:Z:``` somewhere in the line, and doesn't actually capture every situation. Some masochistic person might come along at a later date and decide that they're going to call all their reads ```HAXXA:Z:AWESOME!:&lt;readNumber&gt;```, in which case a tweak to this grep search would be needed to make sure that there's a space (or more specifically a tab character) prior to the ```XA:Z:```.&#xD;&#xA;&#xD;&#xA;Now I do a check for *any* duplicated read names, just to be sure:&#xD;&#xA;&#xD;&#xA;    $ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xD;&#xA;    ERR063640.1194&#xD;&#xA;    ERR063640.1429&#xD;&#xA;    ERR063640.1761&#xD;&#xA;    ERR063640.2336&#xD;&#xA;    ERR063640.2825&#xD;&#xA;    ERR063640.3458&#xD;&#xA;    ERR063640.4421&#xD;&#xA;    ERR063640.4474&#xD;&#xA;    ERR063640.4888&#xD;&#xA;    ERR063640.49&#xD;&#xA;    ERR063640.4974&#xD;&#xA;    ERR063640.5070&#xD;&#xA;    ERR063640.5130&#xD;&#xA;    ERR063640.5300&#xD;&#xA;    ERR063640.5868&#xD;&#xA;    ERR063640.6116&#xD;&#xA;    ERR063640.6198&#xD;&#xA;    ERR063640.6468&#xD;&#xA;    ERR063640.6717&#xD;&#xA;    ERR063640.6797&#xD;&#xA;    ERR063640.7322&#xD;&#xA;    ERR063640.750&#xD;&#xA;    ERR063640.7570&#xD;&#xA;    ERR063640.7900&#xD;&#xA;    ERR063640.8115&#xD;&#xA;    ERR063640.8405&#xD;&#xA;    ERR063640.911&#xD;&#xA;    ERR063640.9206&#xD;&#xA;    ERR063640.9765&#xD;&#xA;    ERR063640.9986&#xD;&#xA;&#xD;&#xA;Oh... damn. I wonder what they are:&#xD;&#xA;&#xD;&#xA;    $ samtools view output_filtered.bam | grep '^ERR063640.3458[[:space:]]'&#xD;&#xA;    ERR063640.3458	16	tig00002961	5402	60	58S38M	*	0	0	AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATATCCGTATCGCGAAGATCAGGACTTACTCCGCAGAAGAA	DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJDHLNKCK;KMKGMFKJILIIIMKI9JLKKHEJFII?CC	NM:i:0	MD:Z:38	AS:i:38	XS:i:0	SA:Z:tig00002377,202353,-,14M3I5M1I35M38S,19,5;&#xD;&#xA;    ERR063640.3458	2064	tig00002377	202353	19	14M3I5M1I35M38H	*	0	0	AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATA	DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJ	NM:i:5	MD:Z:5G48	AS:i:35	XS:i:27	SA:Z:tig00002961,5402,-,58S38M,60,0;&#xD;&#xA;&#xD;&#xA;Aha! Supplemental alignments, which use the *official* ```SA``` tag [other canonical alignments in a chimeric alignment]. These appear to be situations where a single read has been split up, and maps to multiple locations. Note that in this case, ***both of the alignments still have MAPQ scores of over 3***. It sounds like the questioner would also want to get rid of these situations as well. This time, there are standard flag fields as well to deal with these situations (0x800: secondary alignment). Except it's not enough to just filter the supplemental alignment, because *both* read mappings should be removed, rather than just the one (or ones) that happened to be tagged as secondary.&#xD;&#xA;&#xD;&#xA;Luckily, BWA appears to put the ```SA``` tag into all reads containing supplementary alignments (if this is not the case, I'm sure someone will correct me on that). So, I add in the ```SA``` search as an additional ```grep``` filter:&#xD;&#xA;&#xD;&#xA;    $ samtools view -h output.bam | grep -v -e 'XA:Z:' -e 'SA:Z:' | samtools view -b &gt; output_filtered.bam&#xD;&#xA;    $ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xD;&#xA;    &lt;no output&gt;&#xD;&#xA;&#xD;&#xA;Done. Easy peasy! ```&lt;/sarcasm&gt;```&#xD;&#xA;&#xD;&#xA;... that original &quot;high-effort&quot; solution of using a different aligner doesn't look so bad now." />
  <row Id="1548" PostHistoryTypeId="2" PostId="525" RevisionGUID="851e5b9b-5bf1-45e3-9775-b276b377d20c" CreationDate="2017-06-07T07:20:47.170" UserId="400" Text="I have a pipeline for generating a BigWig file from a BAM file:&#xD;&#xA;&#xD;&#xA;    BAM -&gt; BedGraph -&gt; BigWig&#xD;&#xA;&#xD;&#xA;Which uses `bedtools genomecov` for the `BAM -&gt; BedGraph` part and `bedGraphToBigWig` for the `BedGraph -&gt; BigWig` part.&#xD;&#xA;&#xD;&#xA;The use of `bedGraphToBigWig` to create the `BigWig` file requires a BedGraph file to reside on disk in uncompressed form as it performs seeks. This is problematic for large genomes and variable coverage BAM files when there are more step changes/lines in the `BedGraph` file. My `BedGraph` files are in the order of 50 Gbytes in size and all that IO for 10-20 BAM files seems unnecessary.&#xD;&#xA;&#xD;&#xA;**Are there any tools which generate `BigWig` without having to use an uncompressed `BedGraph` file on disk?**&#xD;&#xA;&#xD;&#xA;I have tried the following tools, but they still create/use a `BedGraph` intermediary file:&#xD;&#xA;&#xD;&#xA; - `deepTools`" />
  <row Id="1549" PostHistoryTypeId="1" PostId="525" RevisionGUID="851e5b9b-5bf1-45e3-9775-b276b377d20c" CreationDate="2017-06-07T07:20:47.170" UserId="400" Text="BAM to BigWig without intermediary BedGraph" />
  <row Id="1550" PostHistoryTypeId="3" PostId="525" RevisionGUID="851e5b9b-5bf1-45e3-9775-b276b377d20c" CreationDate="2017-06-07T07:20:47.170" UserId="400" Text="&lt;bam&gt;&lt;conversion&gt;" />
  <row Id="1551" PostHistoryTypeId="2" PostId="526" RevisionGUID="768e3668-721d-4d69-b840-c5800869c6a8" CreationDate="2017-06-07T08:08:25.347" UserId="292" Text="I think there is no general answer to your question.&#xD;&#xA;&#xD;&#xA;Which type of ID you will use depends on how practical it will be for your downstream processing. This depends on what other source of information you have, how much conversion work you will need to do in order to match your data to these information if you use one or the other.&#xD;&#xA;&#xD;&#xA;I think the choice may emerge by trial and error, as you realize that a given choice is actually not convenient in your particular use case. Try to set up easy to use conversion procedures that you can easily insert in some steps of your workflow, so that you can easily switch from one choice to the other.&#xD;&#xA;&#xD;&#xA;Concerning the tools to do the conversion, see the following question and its answers: &lt;https://bioinformatics.stackexchange.com/q/488/292&gt;" />
  <row Id="1552" PostHistoryTypeId="5" PostId="519" RevisionGUID="8a887161-0a39-4675-9bbc-40e22d774c33" CreationDate="2017-06-07T08:09:44.250" UserId="73" Comment="added TLDR" Text="To exclude *all possible* multi-mapped reads from a BWA-mapped BAM file, it looks like you need to use ```grep``` on the uncompressed SAM fields:&#xD;&#xA;&#xD;&#xA;    samtools view -h mapped.bam | grep -v -e 'XA:Z:' -e 'SA:Z:' | samtools view -b &gt; unique_mapped.bam&#xD;&#xA;&#xD;&#xA;Explanation follows...&#xD;&#xA;&#xD;&#xA;I'm going to assume a situation in which a bioinformatician is presented with a mapped BAM file produced by BWA, and has no way of getting the original reads. One high-effort solution would be to extract the mapped reads from the BAM file and re-map with a different mapper that uses the MAPQ score to indicate multiple mappings.&#xD;&#xA;&#xD;&#xA;... but what if that were not possible?&#xD;&#xA;&#xD;&#xA;My understanding of BWA's output is that if a read maps perfectly to multiple genomic locations, it will be given a high mapping quality (MAPQ) score for both locations. Many people expect that a read that maps to at least two locations can have (at best) a 50% probability of mapping to one of those locations (i.e. MAPQ = 3). Because BWA doesn't do this, it makes it difficult to filter out multiply-mapped reads from BWA results using the MAPQ filter that works for other aligners; this is likely to be why the current answer on Biostars [```samtools view -bq 1```] probably won't work.&#xD;&#xA;&#xD;&#xA;Here is an example line from a BWA mem alignment that I've just made. These are Illumina reads mapped to a parasite genome that has a *lot* of repetitive sequence. I might be interested, for example, to find uniquely-mapped regions so that I can generate seeds so that the [TULIP assembler/scaffolder](https://github.com/Generade-nl/TULIP) can do its thing:&#xD;&#xA;&#xD;&#xA;    ERR063640.7     16      tig00019544     79974   21      21M2I56M1I20M   *       0       0       TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC    &lt;AABBEGABFJKKKIM7GHKKJK&gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&gt;D    NM:i:4  MD:Z:83A13      AS:i:77 XS:i:67 XA:Z:tig00019544,-78808,21M2I56M1I20M,6;tig00019544,-84624,79M1I20M,6;tig00019544,-79312,33M4I42M1I20M,8;&#xD;&#xA;&#xD;&#xA;BWA mem has found that this particular read, ERR063460.7, maps to at least three different locations: tig00019544, tig00019544, and tig00019544. Note that the MAPQ for this read is 21, so even though the read maps to multiple locations, MAPQ can't be used to determine that.&#xD;&#xA;&#xD;&#xA;However, the alternative locations are shown by the presence of the ```XA``` tag in the custom fields section of the SAM output. Perhaps just filtering on lines that contain the ```XA``` tag will be able to exclude multiply-mapped reads. The ```samtools view``` man page suggests that ```-x``` will filter out a particular tag:&#xD;&#xA;&#xD;&#xA;    $ samtools view -x XA output.bam | grep '^ERR063640\.7[[:space:]]'&#xD;&#xA;    ERR063640.7	16	tig00019544	79974	21	21M2I56M1I20M	*	0	0	TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC	&lt;AABBEGABFJKKKIM7GHKKJK&gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&gt;D	NM:i:4	MD:Z:83A13	AS:i:77	XS:i:67&#xD;&#xA;&#xD;&#xA;... so it filtered out the tag (i.e. the tag no longer exists in the SAM output), but not the *read*. There are no useful bits in the FLAG field to indicate multiple genomic mappings (which I know can be filtered to exclude the read as well), so I have to resort to other measures.&#xD;&#xA;&#xD;&#xA;In this particular case, I can use ```grep -v``` on the uncompressed SAM output to exclude alignment lines that have the ```XA``` tag (and re-compress to BAM afterwards, just to be tidy):&#xD;&#xA;&#xD;&#xA;    $ samtools view -h output.bam | grep -v 'XA:Z:' | samtools view -b &gt; output_filtered.bam&#xD;&#xA;    $ samtools view output_filtered.bam | grep '^ERR063640\.7[[:space:]]'&#xD;&#xA;    &lt;no output&gt;&#xD;&#xA;&#xD;&#xA;Hurray! reads filtered. As a little aside, this ```grep``` search has a fairly substantial computational load: it's looking for some string with the text ```XA:Z:``` somewhere in the line, and doesn't actually capture every situation. Some masochistic person might come along at a later date and decide that they're going to call all their reads ```HAXXA:Z:AWESOME!:&lt;readNumber&gt;```, in which case a tweak to this grep search would be needed to make sure that there's a space (or more specifically a tab character) prior to the ```XA:Z:```.&#xD;&#xA;&#xD;&#xA;Now I do a check for *any* duplicated read names, just to be sure:&#xD;&#xA;&#xD;&#xA;    $ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xD;&#xA;    ERR063640.1194&#xD;&#xA;    ERR063640.1429&#xD;&#xA;    ERR063640.1761&#xD;&#xA;    ERR063640.2336&#xD;&#xA;    ERR063640.2825&#xD;&#xA;    ERR063640.3458&#xD;&#xA;    ERR063640.4421&#xD;&#xA;    ERR063640.4474&#xD;&#xA;    ERR063640.4888&#xD;&#xA;    ERR063640.49&#xD;&#xA;    ERR063640.4974&#xD;&#xA;    ERR063640.5070&#xD;&#xA;    ERR063640.5130&#xD;&#xA;    ERR063640.5300&#xD;&#xA;    ERR063640.5868&#xD;&#xA;    ERR063640.6116&#xD;&#xA;    ERR063640.6198&#xD;&#xA;    ERR063640.6468&#xD;&#xA;    ERR063640.6717&#xD;&#xA;    ERR063640.6797&#xD;&#xA;    ERR063640.7322&#xD;&#xA;    ERR063640.750&#xD;&#xA;    ERR063640.7570&#xD;&#xA;    ERR063640.7900&#xD;&#xA;    ERR063640.8115&#xD;&#xA;    ERR063640.8405&#xD;&#xA;    ERR063640.911&#xD;&#xA;    ERR063640.9206&#xD;&#xA;    ERR063640.9765&#xD;&#xA;    ERR063640.9986&#xD;&#xA;&#xD;&#xA;Oh... damn. I wonder what they are:&#xD;&#xA;&#xD;&#xA;    $ samtools view output_filtered.bam | grep '^ERR063640.3458[[:space:]]'&#xD;&#xA;    ERR063640.3458	16	tig00002961	5402	60	58S38M	*	0	0	AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATATCCGTATCGCGAAGATCAGGACTTACTCCGCAGAAGAA	DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJDHLNKCK;KMKGMFKJILIIIMKI9JLKKHEJFII?CC	NM:i:0	MD:Z:38	AS:i:38	XS:i:0	SA:Z:tig00002377,202353,-,14M3I5M1I35M38S,19,5;&#xD;&#xA;    ERR063640.3458	2064	tig00002377	202353	19	14M3I5M1I35M38H	*	0	0	AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATA	DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJ	NM:i:5	MD:Z:5G48	AS:i:35	XS:i:27	SA:Z:tig00002961,5402,-,58S38M,60,0;&#xD;&#xA;&#xD;&#xA;Aha! Supplemental alignments, which use the *official* ```SA``` tag [other canonical alignments in a chimeric alignment]. These appear to be situations where a single read has been split up, and maps to multiple locations. Note that in this case, ***both of the alignments still have MAPQ scores of over 3***. It sounds like the questioner would also want to get rid of these situations as well. This time, there are standard flag fields as well to deal with these situations (0x800: secondary alignment). Except it's not enough to just filter the supplemental alignment, because *both* read mappings should be removed, rather than just the one (or ones) that happened to be tagged as secondary.&#xD;&#xA;&#xD;&#xA;Luckily, BWA appears to put the ```SA``` tag into all reads containing supplementary alignments (if this is not the case, I'm sure someone will correct me on that). So, I add in the ```SA``` search as an additional ```grep``` filter:&#xD;&#xA;&#xD;&#xA;    $ samtools view -h output.bam | grep -v -e 'XA:Z:' -e 'SA:Z:' | samtools view -b &gt; output_filtered.bam&#xD;&#xA;    $ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xD;&#xA;    &lt;no output&gt;&#xD;&#xA;&#xD;&#xA;Done. Easy peasy! ```&lt;/sarcasm&gt;```&#xD;&#xA;&#xD;&#xA;... that original &quot;high-effort&quot; solution of using a different aligner doesn't look so bad now." />
  <row Id="1553" PostHistoryTypeId="2" PostId="527" RevisionGUID="bef5a013-e5cd-4819-8383-5de60a50fe63" CreationDate="2017-06-07T08:24:32.383" UserId="391" Text="miRBase 21 was published June 26, 2014 and was still in its growth phase. Why is it not being updated anymore or the project declared officially dead? ENSEMBL also uses miRBase as a starting point (http://www.ensembl.org/info/genome/genebuild/ncrna.html).&#xD;&#xA;&#xD;&#xA;What do people use now as a good reference for miRNAs especially for less well annotated/sequenced species?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1554" PostHistoryTypeId="1" PostId="527" RevisionGUID="bef5a013-e5cd-4819-8383-5de60a50fe63" CreationDate="2017-06-07T08:24:32.383" UserId="391" Text="Where is an up to date miRNA database and what happened to miRBase?" />
  <row Id="1555" PostHistoryTypeId="3" PostId="527" RevisionGUID="bef5a013-e5cd-4819-8383-5de60a50fe63" CreationDate="2017-06-07T08:24:32.383" UserId="391" Text="&lt;annotation&gt;" />
  <row Id="1556" PostHistoryTypeId="2" PostId="528" RevisionGUID="b16e79c3-7277-4f22-852f-9f59573192f4" CreationDate="2017-06-07T08:24:35.987" UserId="682" Text="I'm trying to find a programatic way to automatically extract:&#xD;&#xA;1) RNA sequence &#xD;&#xA;2) Secondary structure restrains ( bracket format `es. (( . ( . ) . )) `) &#xD;&#xA;from a PDB file. &#xD;&#xA;&#xD;&#xA;Do exist softwares that from a pdb input return and generate this two informations ? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; " />
  <row Id="1557" PostHistoryTypeId="1" PostId="528" RevisionGUID="b16e79c3-7277-4f22-852f-9f59573192f4" CreationDate="2017-06-07T08:24:35.987" UserId="682" Text="How to extract RNA sequence and secondary structure restrains from a PDB file." />
  <row Id="1558" PostHistoryTypeId="3" PostId="528" RevisionGUID="b16e79c3-7277-4f22-852f-9f59573192f4" CreationDate="2017-06-07T08:24:35.987" UserId="682" Text="&lt;pdb&gt;&lt;rna&gt;" />
  <row Id="1559" PostHistoryTypeId="2" PostId="529" RevisionGUID="9dc09883-06e9-4edc-94d6-5dec4653dff2" CreationDate="2017-06-07T08:28:15.090" UserId="678" Text="From my experience, there is few about CRISPR pooled screenings in R. One of them is [ScreenBEAM](https://github.com/jyyu/ScreenBEAM). It uses a linear mixture model to determine significance at gene level by considering all the guides together at the same step in the analysis. However, it does not work very well for me. &#xD;&#xA;&#xD;&#xA;There are other ways of analyze these screenings outside of R that are easy to use such the suggested MAGeCK or others such HiTSelect, BAGEL or [CRISPR-Analyzer](http://crispr-analyzer.dkfz.de/).&#xD;&#xA;&#xD;&#xA;CRISPR-Analyzer is an online tool that could be useful and that recapitulates some of the mentioned methods in a single analysis. However, if you want to work from command line I would recommend you BAGEL for gene essentiality experiments and MAGeCK for other experimental setup (i.e, control vs treatment)&#xD;&#xA;&#xD;&#xA;Sorry for not putting all the links but I only can put two at the moment!" />
  <row Id="1560" PostHistoryTypeId="2" PostId="530" RevisionGUID="bd81a55f-f140-4011-97d3-3acc0463ecd4" CreationDate="2017-06-07T08:31:35.480" UserId="81" Text="I suspect this is average coverage across the transcript or exon. &#xD;&#xA;&#xD;&#xA;Formula for average coverage:&#xD;&#xA;&#xD;&#xA;![coverage-formula](https://latex.codecogs.com/gif.latex?Cov%20%3D%20N%20%5Ccdot%20%5Cfrac%7BL%7D%7BT%7D)&#xD;&#xA;&#xD;&#xA;Where:&#xD;&#xA;&#xD;&#xA;* N: number of reads mapping/aligning to your transcript/exon&#xD;&#xA;* L: average read length&#xD;&#xA;* T: Transcript/exon length&#xD;&#xA;" />
  <row Id="1561" PostHistoryTypeId="6" PostId="527" RevisionGUID="3bb5b7c1-6fdf-4563-911e-baafcbb2cf98" CreationDate="2017-06-07T08:34:20.397" UserId="391" Comment="edited tags" Text="&lt;annotation&gt;&lt;database&gt;" />
  <row Id="1562" PostHistoryTypeId="2" PostId="531" RevisionGUID="5282e228-4334-485a-921c-074b6b725497" CreationDate="2017-06-07T08:39:33.900" UserId="180" Text="I am trying to use `samtools depth` (v1.4) with the -a option and a bed file listing the human chromosomes chr1-chr22, chrX, chrY, and chrM to print out the coverage at every position:&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;cat GRCh38.karyo.bed | awk '{print $3}' | datamash sum 1&#xD;&#xA;3088286401&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;I would like to know how to run `samtools depth` so that it produces 3088286401 &#xD;&#xA;entries when run against a GRCh38 bam file:&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;samtools depth -b $bedfile -a $inputfile&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;I tried it for a few bam files that were aligned the same way, and I get differing number of entries:&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;3087003274&#xD;&#xA;3087005666&#xD;&#xA;3087007158&#xD;&#xA;3087009435&#xD;&#xA;3087009439&#xD;&#xA;3087009621&#xD;&#xA;3087009818&#xD;&#xA;3087010065&#xD;&#xA;3087010408&#xD;&#xA;3087010477&#xD;&#xA;3087010481&#xD;&#xA;3087012115&#xD;&#xA;3087013147&#xD;&#xA;3087013186&#xD;&#xA;3087013500&#xD;&#xA;3087149616&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;Is there a special flag in `samtools depth` so that it reports all entries from the bed file?&#xD;&#xA;&#xD;&#xA;If `samtools depth` is not the best tool for this, what would be the equivalent with `sambamba depth base`?&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;sambamba depth base --min-coverage=0 --regions $bedfile $inputfile&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;Any other options?&#xD;&#xA;" />
  <row Id="1563" PostHistoryTypeId="1" PostId="531" RevisionGUID="5282e228-4334-485a-921c-074b6b725497" CreationDate="2017-06-07T08:39:33.900" UserId="180" Text="samtools depth print out all positions" />
  <row Id="1564" PostHistoryTypeId="3" PostId="531" RevisionGUID="5282e228-4334-485a-921c-074b6b725497" CreationDate="2017-06-07T08:39:33.900" UserId="180" Text="&lt;bam&gt;&lt;samtools&gt;&lt;sambamba&gt;&lt;depth&gt;" />
  <row Id="1565" PostHistoryTypeId="5" PostId="531" RevisionGUID="c90e9820-8f5f-45a6-943e-82c129003e27" CreationDate="2017-06-07T08:46:35.073" UserId="215" Comment="Added code blocks" Text="I am trying to use `samtools depth` (v1.4) with the -a option and a bed file listing the human chromosomes chr1-chr22, chrX, chrY, and chrM to print out the coverage at every position:&#xD;&#xA;&#xD;&#xA;    cat GRCh38.karyo.bed | awk '{print $3}' | datamash sum 1&#xD;&#xA;    3088286401&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I would like to know how to run `samtools depth` so that it produces 3,088,286,401 entries when run against a GRCh38 bam file:&#xD;&#xA;&#xD;&#xA;    samtools depth -b $bedfile -a $inputfile&#xD;&#xA;&#xD;&#xA;I tried it for a few bam files that were aligned the same way, and I get differing number of entries:&#xD;&#xA;&#xD;&#xA;    3087003274&#xD;&#xA;    3087005666&#xD;&#xA;    3087007158&#xD;&#xA;    3087009435&#xD;&#xA;    3087009439&#xD;&#xA;    3087009621&#xD;&#xA;    3087009818&#xD;&#xA;    3087010065&#xD;&#xA;    3087010408&#xD;&#xA;    3087010477&#xD;&#xA;    3087010481&#xD;&#xA;    3087012115&#xD;&#xA;    3087013147&#xD;&#xA;    3087013186&#xD;&#xA;    3087013500&#xD;&#xA;    3087149616&#xD;&#xA;&#xD;&#xA;Is there a special flag in `samtools depth` so that it reports all entries from the bed file?&#xD;&#xA;&#xD;&#xA;If `samtools depth` is not the best tool for this, what would be the equivalent with `sambamba depth base`?&#xD;&#xA;&#xD;&#xA;    sambamba depth base --min-coverage=0 --regions $bedfile $inputfile&#xD;&#xA;&#xD;&#xA;Any other options?&#xD;&#xA;" />
  <row Id="1566" PostHistoryTypeId="24" PostId="531" RevisionGUID="c90e9820-8f5f-45a6-943e-82c129003e27" CreationDate="2017-06-07T08:46:35.073" Comment="Proposed by 215 approved by 180 edit id of 118" />
  <row Id="1568" PostHistoryTypeId="5" PostId="522" RevisionGUID="0a22899c-41ca-4f18-b36f-5d42f176b025" CreationDate="2017-06-07T09:37:00.570" UserId="73" Comment="added 4 characters in body; edited tags" Text="I'm using several datasets that are encoded using either Entrez or Ensembl IDs to specify genes, and need to decide on which to standardise on.&#xD;&#xA;&#xD;&#xA;Are there any major reasons to use one over the other?&#xD;&#xA;&#xD;&#xA;Which tools and conversion tables are best to use for this?" />
  <row Id="1569" PostHistoryTypeId="6" PostId="522" RevisionGUID="0a22899c-41ca-4f18-b36f-5d42f176b025" CreationDate="2017-06-07T09:37:00.570" UserId="73" Comment="added 4 characters in body; edited tags" Text="&lt;conversion&gt;&lt;gene&gt;&lt;public-databases&gt;" />
  <row Id="1570" PostHistoryTypeId="2" PostId="532" RevisionGUID="e4f6bef1-abc1-451d-995b-dab41e6c783b" CreationDate="2017-06-07T10:04:21.943" UserId="690" Text="I would guess DIANA-Tarbase ( evidences, https://www.ncbi.nlm.nih.gov/pubmed/25416803 ) and TargetScan (predictions, https://www.ncbi.nlm.nih.gov/pubmed/26267216 ) are good starting points.&#xD;&#xA;&#xD;&#xA;Especially for the less annotated species, TargetScan might be helpful." />
  <row Id="1571" PostHistoryTypeId="2" PostId="533" RevisionGUID="e55ef44d-8148-4b30-b9a9-93b1e07597c3" CreationDate="2017-06-07T10:06:32.310" UserId="640" Text="I suggest you take a look at https://github.com/mmagnus/rna-pdb-tools we do way more than you need! :-) The tools can get you a sequence, secondary structure and much more using various algorithms, and all is well documented http://rna-pdb-tools.readthedocs.io/en/latest/" />
  <row Id="1572" PostHistoryTypeId="5" PostId="508" RevisionGUID="db15a09c-bab5-41d4-8774-78c99f4def74" CreationDate="2017-06-07T10:09:43.330" UserId="73" Comment="condensed question" Text="*This is based on a question from [betsy.s.collins](https://www.biostars.org/u/39680/) on BioStars. The original post can be found [here](https://www.biostars.org/p/256448/).*&#xD;&#xA;&#xD;&#xA;Does anyone have any suggestions for other tags or filtering steps on BWA-generated BAM files that can be used so reads only map to one location? One example application would be to find seeds for the [TULIP](https://github.com/Generade-nl/TULIP) assembler/scaffolder, which works best for reads that map to unique genomic locations.&#xD;&#xA;&#xD;&#xA;The concept of &quot;uniquely mapped reads&quot; is a loaded term, and most sources suggest filtering by MAPQ should do the trick. However, this approach doesn't seem to work when using BWA as a read mapper." />
  <row Id="1573" PostHistoryTypeId="5" PostId="519" RevisionGUID="40800ae2-dab4-4f4f-b0f9-fe104d80b20b" CreationDate="2017-06-07T10:11:54.673" UserId="73" Comment="removed TULIP example (shifted to question)" Text="To exclude *all possible* multi-mapped reads from a BWA-mapped BAM file, it looks like you need to use ```grep``` on the uncompressed SAM fields:&#xD;&#xA;&#xD;&#xA;    samtools view -h mapped.bam | grep -v -e 'XA:Z:' -e 'SA:Z:' | samtools view -b &gt; unique_mapped.bam&#xD;&#xA;&#xD;&#xA;Explanation follows...&#xD;&#xA;&#xD;&#xA;I'm going to assume a situation in which a bioinformatician is presented with a mapped BAM file produced by BWA, and has no way of getting the original reads. One high-effort solution would be to extract the mapped reads from the BAM file and re-map with a different mapper that uses the MAPQ score to indicate multiple mappings.&#xD;&#xA;&#xD;&#xA;... but what if that were not possible?&#xD;&#xA;&#xD;&#xA;My understanding of BWA's output is that if a read maps perfectly to multiple genomic locations, it will be given a high mapping quality (MAPQ) score for both locations. Many people expect that a read that maps to at least two locations can have (at best) a 50% probability of mapping to one of those locations (i.e. MAPQ = 3). Because BWA doesn't do this, it makes it difficult to filter out multiply-mapped reads from BWA results using the MAPQ filter that works for other aligners; this is likely to be why the current answer on Biostars [```samtools view -bq 1```] probably won't work.&#xD;&#xA;&#xD;&#xA;Here is an example line from a BWA mem alignment that I've just made. These are Illumina reads mapped to a parasite genome that has a *lot* of repetitive sequence:&#xD;&#xA;&#xD;&#xA;    ERR063640.7     16      tig00019544     79974   21      21M2I56M1I20M   *       0       0       TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC    &lt;AABBEGABFJKKKIM7GHKKJK&gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&gt;D    NM:i:4  MD:Z:83A13      AS:i:77 XS:i:67 XA:Z:tig00019544,-78808,21M2I56M1I20M,6;tig00019544,-84624,79M1I20M,6;tig00019544,-79312,33M4I42M1I20M,8;&#xD;&#xA;&#xD;&#xA;BWA mem has found that this particular read, ERR063460.7, maps to at least three different locations: tig00019544, tig00019544, and tig00019544. Note that the MAPQ for this read is 21, so even though the read maps to multiple locations, MAPQ can't be used to determine that.&#xD;&#xA;&#xD;&#xA;However, the alternative locations are shown by the presence of the ```XA``` tag in the custom fields section of the SAM output. Perhaps just filtering on lines that contain the ```XA``` tag will be able to exclude multiply-mapped reads. The ```samtools view``` man page suggests that ```-x``` will filter out a particular tag:&#xD;&#xA;&#xD;&#xA;    $ samtools view -x XA output.bam | grep '^ERR063640\.7[[:space:]]'&#xD;&#xA;    ERR063640.7	16	tig00019544	79974	21	21M2I56M1I20M	*	0	0	TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC	&lt;AABBEGABFJKKKIM7GHKKJK&gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&gt;D	NM:i:4	MD:Z:83A13	AS:i:77	XS:i:67&#xD;&#xA;&#xD;&#xA;... so it filtered out the tag (i.e. the tag no longer exists in the SAM output), but not the *read*. There are no useful bits in the FLAG field to indicate multiple genomic mappings (which I know can be filtered to exclude the read as well), so I have to resort to other measures.&#xD;&#xA;&#xD;&#xA;In this particular case, I can use ```grep -v``` on the uncompressed SAM output to exclude alignment lines that have the ```XA``` tag (and re-compress to BAM afterwards, just to be tidy):&#xD;&#xA;&#xD;&#xA;    $ samtools view -h output.bam | grep -v 'XA:Z:' | samtools view -b &gt; output_filtered.bam&#xD;&#xA;    $ samtools view output_filtered.bam | grep '^ERR063640\.7[[:space:]]'&#xD;&#xA;    &lt;no output&gt;&#xD;&#xA;&#xD;&#xA;Hurray! reads filtered. As a little aside, this ```grep``` search has a fairly substantial computational load: it's looking for some string with the text ```XA:Z:``` somewhere in the line, and doesn't actually capture every situation. Some masochistic person might come along at a later date and decide that they're going to call all their reads ```HAXXA:Z:AWESOME!:&lt;readNumber&gt;```, in which case a tweak to this grep search would be needed to make sure that there's a space (or more specifically a tab character) prior to the ```XA:Z:```.&#xD;&#xA;&#xD;&#xA;Now I do a check for *any* duplicated read names, just to be sure:&#xD;&#xA;&#xD;&#xA;    $ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xD;&#xA;    ERR063640.1194&#xD;&#xA;    ERR063640.1429&#xD;&#xA;    ERR063640.1761&#xD;&#xA;    ERR063640.2336&#xD;&#xA;    ERR063640.2825&#xD;&#xA;    ERR063640.3458&#xD;&#xA;    ERR063640.4421&#xD;&#xA;    ERR063640.4474&#xD;&#xA;    ERR063640.4888&#xD;&#xA;    ERR063640.49&#xD;&#xA;    ERR063640.4974&#xD;&#xA;    ERR063640.5070&#xD;&#xA;    ERR063640.5130&#xD;&#xA;    ERR063640.5300&#xD;&#xA;    ERR063640.5868&#xD;&#xA;    ERR063640.6116&#xD;&#xA;    ERR063640.6198&#xD;&#xA;    ERR063640.6468&#xD;&#xA;    ERR063640.6717&#xD;&#xA;    ERR063640.6797&#xD;&#xA;    ERR063640.7322&#xD;&#xA;    ERR063640.750&#xD;&#xA;    ERR063640.7570&#xD;&#xA;    ERR063640.7900&#xD;&#xA;    ERR063640.8115&#xD;&#xA;    ERR063640.8405&#xD;&#xA;    ERR063640.911&#xD;&#xA;    ERR063640.9206&#xD;&#xA;    ERR063640.9765&#xD;&#xA;    ERR063640.9986&#xD;&#xA;&#xD;&#xA;Oh... damn. I wonder what they are:&#xD;&#xA;&#xD;&#xA;    $ samtools view output_filtered.bam | grep '^ERR063640.3458[[:space:]]'&#xD;&#xA;    ERR063640.3458	16	tig00002961	5402	60	58S38M	*	0	0	AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATATCCGTATCGCGAAGATCAGGACTTACTCCGCAGAAGAA	DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJDHLNKCK;KMKGMFKJILIIIMKI9JLKKHEJFII?CC	NM:i:0	MD:Z:38	AS:i:38	XS:i:0	SA:Z:tig00002377,202353,-,14M3I5M1I35M38S,19,5;&#xD;&#xA;    ERR063640.3458	2064	tig00002377	202353	19	14M3I5M1I35M38H	*	0	0	AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATA	DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJ	NM:i:5	MD:Z:5G48	AS:i:35	XS:i:27	SA:Z:tig00002961,5402,-,58S38M,60,0;&#xD;&#xA;&#xD;&#xA;Aha! Supplemental alignments, which use the *official* ```SA``` tag [other canonical alignments in a chimeric alignment]. These appear to be situations where a single read has been split up, and maps to multiple locations. Note that in this case, ***both of the alignments still have MAPQ scores of over 3***. It sounds like the questioner would also want to get rid of these situations as well. This time, there are standard flag fields as well to deal with these situations (0x800: secondary alignment). Except it's not enough to just filter the supplemental alignment, because *both* read mappings should be removed, rather than just the one (or ones) that happened to be tagged as secondary.&#xD;&#xA;&#xD;&#xA;Luckily, BWA appears to put the ```SA``` tag into all reads containing supplementary alignments (if this is not the case, I'm sure someone will correct me on that). So, I add in the ```SA``` search as an additional ```grep``` filter:&#xD;&#xA;&#xD;&#xA;    $ samtools view -h output.bam | grep -v -e 'XA:Z:' -e 'SA:Z:' | samtools view -b &gt; output_filtered.bam&#xD;&#xA;    $ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xD;&#xA;    &lt;no output&gt;&#xD;&#xA;&#xD;&#xA;Done. Easy peasy! ```&lt;/sarcasm&gt;```&#xD;&#xA;&#xD;&#xA;... that original &quot;high-effort&quot; solution of using a different aligner doesn't look so bad now." />
  <row Id="1574" PostHistoryTypeId="5" PostId="533" RevisionGUID="36d72912-7094-403a-aad4-741ab8620364" CreationDate="2017-06-07T10:12:19.923" UserId="640" Comment="add a link to get sequence" Text="I suggest you take a look at `rna-pdb-tools` we do way more than you need! :-) The tools can get you a sequence, secondary structure and much more using various algorithms, and all is well documented http://rna-pdb-tools.readthedocs.io/en/latest/&#xD;&#xA;&#xD;&#xA;To get sequence http://rna-pdb-tools.readthedocs.io/en/latest/main.html#get-sequence " />
  <row Id="1575" PostHistoryTypeId="6" PostId="118" RevisionGUID="346db42b-0372-4946-bcf1-257c2a05bffc" CreationDate="2017-06-07T10:43:58.227" UserId="298" Comment="Removed the &quot;mapping&quot; meta tag" Text="&lt;ensembl&gt;&lt;biomart&gt;&lt;affymetrix-arrays&gt;" />
  <row Id="1576" PostHistoryTypeId="24" PostId="118" RevisionGUID="346db42b-0372-4946-bcf1-257c2a05bffc" CreationDate="2017-06-07T10:43:58.227" Comment="Proposed by 298 approved by 23 edit id of 121" />
  <row Id="1577" PostHistoryTypeId="5" PostId="487" RevisionGUID="346cab59-c6fc-46fa-baa6-9f0c7290567a" CreationDate="2017-06-07T10:45:03.643" UserId="298" Comment="If you reproduce content from elsewhere, it need to be in a quote block." Text="*Answer from [/u/Romanticon](https://www.reddit.com/user/Romanticon) on reddit. The original answer can be found [here](https://www.reddit.com/r/bioinformatics/comments/6dfu3c/question_about_eggnog_mapper_and_annotation_in/di2qv4a/)*&#xD;&#xA;&#xD;&#xA;&gt; Depending on the length of your reads, the level of error in the&#xD;&#xA;&gt; reads, the quality of the database, and the specificity settings you&#xD;&#xA;&gt; give DIAMOND, you can change the level of &quot;bleed&quot; that you get - but&#xD;&#xA;&gt; you'll always have a couple reads, especially at metagenome dataset&#xD;&#xA;&gt; sizes, that will be annotated incorrectly.&#xD;&#xA;&gt; &#xD;&#xA;&gt; But you'll always have this happen, whenever you annotate any large&#xD;&#xA;&gt; dataset. The best way to handle it is to set a threshold after&#xD;&#xA;&gt; annotation that removes wrongly annotated reads (looking for 'clearly&#xD;&#xA;&gt; wrong' organisms in your annotated dataset can help you figure out&#xD;&#xA;&gt; where to set the threshold level - there's probably no Bos taurus&#xD;&#xA;&gt; reads in a mouse metagenome).&#xD;&#xA;&gt; &#xD;&#xA;&gt; You can do things like enable the sensitive flag on the DIAMOND&#xD;&#xA;&gt; annotation search to help improve annotation accuracy." />
  <row Id="1578" PostHistoryTypeId="5" PostId="528" RevisionGUID="fc32c6de-330e-4855-8c1d-504145143244" CreationDate="2017-06-07T10:58:05.227" UserId="73" Comment="corrected spelling / grammar" Text="I'm trying to find a programmatic way to automatically extract the following information from a [PDB file](http://www.wwpdb.org/documentation/file-format):&#xD;&#xA;&#xD;&#xA;1. RNA sequence &#xD;&#xA;2. Secondary structure restraints in bracket format, e.g. `. (( . ( . ) . )) `&#xD;&#xA;&#xD;&#xA;Does software exist that can take a PDB file as input and generate these two pieces of information? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA; " />
  <row Id="1579" PostHistoryTypeId="4" PostId="528" RevisionGUID="fc32c6de-330e-4855-8c1d-504145143244" CreationDate="2017-06-07T10:58:05.227" UserId="73" Comment="corrected spelling / grammar" Text="How to extract RNA sequence and secondary structure restrains from a PDB file" />
  <row Id="1580" PostHistoryTypeId="6" PostId="528" RevisionGUID="fc32c6de-330e-4855-8c1d-504145143244" CreationDate="2017-06-07T10:58:05.227" UserId="73" Comment="corrected spelling / grammar" Text="&lt;public-databases&gt;&lt;pdb&gt;&lt;rna&gt;" />
  <row Id="1581" PostHistoryTypeId="4" PostId="528" RevisionGUID="c16ab189-5077-46e3-bbe7-8a400663b8d8" CreationDate="2017-06-07T11:01:07.300" UserId="682" Comment="edited title" Text="How to extract RNA sequence and secondary structure restrains from a PDB file" />
  <row Id="1582" PostHistoryTypeId="2" PostId="534" RevisionGUID="b683ceea-b0c7-4ff9-867a-5f1fc5870518" CreationDate="2017-06-07T11:15:08.160" UserId="-1" Text="" />
  <row Id="1583" PostHistoryTypeId="1" PostId="534" RevisionGUID="b683ceea-b0c7-4ff9-867a-5f1fc5870518" CreationDate="2017-06-07T11:15:08.160" UserId="-1" />
  <row Id="1584" PostHistoryTypeId="2" PostId="535" RevisionGUID="5ecc0c90-814f-49d2-b0df-6b509842d4e7" CreationDate="2017-06-07T11:15:08.160" UserId="-1" Text="" />
  <row Id="1585" PostHistoryTypeId="1" PostId="535" RevisionGUID="5ecc0c90-814f-49d2-b0df-6b509842d4e7" CreationDate="2017-06-07T11:15:08.160" UserId="-1" />
  <row Id="1587" PostHistoryTypeId="2" PostId="537" RevisionGUID="bf8912b7-79ab-41cf-8af3-31a78535bcdf" CreationDate="2017-06-07T12:07:52.207" UserId="182" Text="The [QuasR](http://bioconductor.org/packages/release/bioc/html/QuasR.html) package in R has a useful function called `qExportWig`. &#xD;&#xA;&#xD;&#xA;To use it you first need to create a sample sheet and a `qProject` object - this can be done starting with bam files or fastq files (in which case it also  performs alignments). You can then create wig or bigWig files without writing a bedGraph file to disk.&#xD;&#xA;&#xD;&#xA;It has a lot of flexible options, including scaling or log-transforming counts, shifting reads and changing the bin size." />
  <row Id="1588" PostHistoryTypeId="5" PostId="529" RevisionGUID="2ed0172c-f442-4987-9e81-89b6f87572b9" CreationDate="2017-06-07T12:08:54.520" UserId="678" Comment="spelling" Text="From my experience, there is few about CRISPR pooled screenings in R. One of them is [ScreenBEAM](https://github.com/jyyu/ScreenBEAM). It uses a linear mixture model to determine significance at gene level by considering all the guides together at the same step in the analysis. However, it does not work very well for me. &#xD;&#xA;&#xD;&#xA;There are other ways of analyze these screenings outside of R that are easy to use such the suggested MAGeCK or others such HiTSelect, BAGEL or [CRISPR-Analyzer](http://crispr-analyzer.dkfz.de/).&#xD;&#xA;&#xD;&#xA;CRISPR-Analyzer is an online tool that could be useful and that recapitulates some of the mentioned methods in a single analysis. However, if you want to work from command line I would recommend you BAGEL for gene essentiality experiments and MAGeCK for other experimental setup (i.e, control vs treatment)&#xD;&#xA;&#xD;&#xA;Sorry for not putting all the links but I can only put two at the moment!" />
  <row Id="1590" PostHistoryTypeId="6" PostId="308" RevisionGUID="9face41f-d266-42be-8ac2-2dc861808311" CreationDate="2017-06-07T12:39:23.120" UserId="298" Comment="Replaced the &quot;mapping&quot; meta tag with the more informative &quot;read-mapping&quot;" Text="&lt;read-mapping&gt;&lt;smalt&gt;" />
  <row Id="1591" PostHistoryTypeId="24" PostId="308" RevisionGUID="9face41f-d266-42be-8ac2-2dc861808311" CreationDate="2017-06-07T12:39:23.120" Comment="Proposed by 298 approved by 77, 73 edit id of 123" />
  <row Id="1592" PostHistoryTypeId="6" PostId="500" RevisionGUID="13f6fab7-8c87-4667-ba7e-c589a78cf7a7" CreationDate="2017-06-07T12:39:30.463" UserId="298" Comment="Replaced the &quot;mapping&quot; meta tag with the more informative &quot;read-mapping&quot;" Text="&lt;alignment&gt;&lt;bwa&gt;&lt;read-mapping&gt;&lt;api&gt;" />
  <row Id="1593" PostHistoryTypeId="24" PostId="500" RevisionGUID="13f6fab7-8c87-4667-ba7e-c589a78cf7a7" CreationDate="2017-06-07T12:39:30.463" Comment="Proposed by 298 approved by 77, 73 edit id of 125" />
  <row Id="1594" PostHistoryTypeId="6" PostId="15" RevisionGUID="113e716d-41ad-41b7-8192-47c6f6701b5d" CreationDate="2017-06-07T12:39:34.250" UserId="298" Comment="Replaced the &quot;mapping&quot; meta tag with the more informative &quot;read-mapping&quot;" Text="&lt;alignment&gt;&lt;bwa&gt;&lt;read-mapping&gt;&lt;reads&gt;" />
  <row Id="1595" PostHistoryTypeId="24" PostId="15" RevisionGUID="113e716d-41ad-41b7-8192-47c6f6701b5d" CreationDate="2017-06-07T12:39:34.250" Comment="Proposed by 298 approved by 77, 73 edit id of 120" />
  <row Id="1596" PostHistoryTypeId="6" PostId="289" RevisionGUID="4cfd13e2-f7cd-42ff-a3b7-38117f8ddd23" CreationDate="2017-06-07T12:39:50.217" UserId="298" Comment="Replaced the &quot;mapping&quot; meta tag with the more informative &quot;read-mapping&quot;" Text="&lt;alignment&gt;&lt;bwa&gt;&lt;read-mapping&gt;" />
  <row Id="1597" PostHistoryTypeId="24" PostId="289" RevisionGUID="4cfd13e2-f7cd-42ff-a3b7-38117f8ddd23" CreationDate="2017-06-07T12:39:50.217" Comment="Proposed by 298 approved by 77, 73 edit id of 122" />
  <row Id="1598" PostHistoryTypeId="5" PostId="486" RevisionGUID="121049d0-a5a2-4432-a6f9-518df57f7397" CreationDate="2017-06-07T12:40:05.083" UserId="298" Comment="Copied content must be quoted; replaced &quot;mapping&quot; with the more specific &quot;read-mapping&quot;" Text="*This is a question from [/u/wipeyourmit](https://www.reddit.com/user/wipeyourmit) on reddit. The original post can be found [here](https://www.reddit.com/r/bioinformatics/comments/6dfu3c/question_about_eggnog_mapper_and_annotation_in/).*&#xD;&#xA;&#xD;&#xA;&gt; If I have a metagenomic dataset that contains reads from both&#xD;&#xA;&gt; eukaryotes and prokaryotes and then I annotate by running DIAMOND or&#xD;&#xA;&gt; HMMER against a bacterial database how much of a risk do I run of&#xD;&#xA;&gt; eukaryotic reads being annotated in the process?&#xD;&#xA;&gt; &#xD;&#xA;&gt; I was hoping to use the eggNOG mapper to search against the bacterial&#xD;&#xA;&gt; and archaeal databases and to exclude the eukaryotic portion of my&#xD;&#xA;&gt; dataset. Is the eukaryotic filtering something that I would need to do&#xD;&#xA;&gt; in a step prior to this?" />
  <row Id="1599" PostHistoryTypeId="6" PostId="486" RevisionGUID="121049d0-a5a2-4432-a6f9-518df57f7397" CreationDate="2017-06-07T12:40:05.083" UserId="298" Comment="Copied content must be quoted; replaced &quot;mapping&quot; with the more specific &quot;read-mapping&quot;" Text="&lt;metagenome&gt;&lt;reddit&gt;&lt;read-mapping&gt;" />
  <row Id="1600" PostHistoryTypeId="24" PostId="486" RevisionGUID="121049d0-a5a2-4432-a6f9-518df57f7397" CreationDate="2017-06-07T12:40:05.083" Comment="Proposed by 298 approved by 77, 73 edit id of 124" />
  <row Id="1602" PostHistoryTypeId="5" PostId="533" RevisionGUID="a8e550cb-43d0-4e4b-92d2-52e72da7b991" CreationDate="2017-06-07T12:49:08.503" UserId="73" Comment="added example from the link" Text="I suggest you take a look at `rna-pdb-tools` we do way more than you need! :-) The tools can get you a sequence, secondary structure and much more using various algorithms, and all is well documented http://rna-pdb-tools.readthedocs.io/en/latest/&#xD;&#xA;&#xD;&#xA;To get sequence http://rna-pdb-tools.readthedocs.io/en/latest/main.html#get-sequence &#xD;&#xA;&#xD;&#xA;    $ rna_pdb_tools.py --get_seq 5_solution_1.pdb&#xD;&#xA;    &gt; 5_solution_1.pdb A:1-576&#xD;&#xA;    CAUCCGGUAUCCCAAGACAAUCUCGGGUUGGGUUGGGAAGUAUCAUGGCUAAUCACCAUGAUGCAAUCGGGUUGAACACUUAAUUGGGUUAAAACGGUGGGGGACGAUCCCGUAACAUCCGUCCUAACGGCGACAGACUGCACGGCCCUGCCUCAGGUGUGUCCAAUGAACAGUCGUUCCGAAAGGAAG&#xD;&#xA;" />
  <row Id="1603" PostHistoryTypeId="6" PostId="118" RevisionGUID="feedba09-908e-4084-9a22-4fb724ea5cdc" CreationDate="2017-06-07T13:11:07.443" UserId="73" Comment="replaced affymetrix-arrays with microarray; affymetrix / Illumina arrays are similar enough (for analysis purposes) that they don't need to be separated" Text="&lt;ensembl&gt;&lt;biomart&gt;&lt;microarray&gt;" />
  <row Id="1604" PostHistoryTypeId="2" PostId="538" RevisionGUID="3f6abdae-4f14-4dce-add6-61bb2b5856b9" CreationDate="2017-06-07T13:13:19.417" UserId="-1" Text="" />
  <row Id="1605" PostHistoryTypeId="1" PostId="538" RevisionGUID="3f6abdae-4f14-4dce-add6-61bb2b5856b9" CreationDate="2017-06-07T13:13:19.417" UserId="-1" />
  <row Id="1606" PostHistoryTypeId="2" PostId="539" RevisionGUID="f1ee69e9-ba4c-44e9-926b-4c166de023c4" CreationDate="2017-06-07T13:13:19.417" UserId="-1" Text="" />
  <row Id="1607" PostHistoryTypeId="1" PostId="539" RevisionGUID="f1ee69e9-ba4c-44e9-926b-4c166de023c4" CreationDate="2017-06-07T13:13:19.417" UserId="-1" />
  <row Id="1608" PostHistoryTypeId="2" PostId="540" RevisionGUID="a29f9370-9cee-4b21-8097-766b918cb8da" CreationDate="2017-06-07T13:23:52.380" UserId="383" Text="When you look at all the genome files available from Ensembl. You are presented with a bunch of options. Which one is the best to use/download?&#xD;&#xA;&#xD;&#xA;You have a combination of choices.&#xD;&#xA;&#xD;&#xA;First part options:&#xD;&#xA;&#xD;&#xA; - **dna_sm** - Repeats soft-masked (converts repeat nucleotides to lowercase)&#xD;&#xA; - **dna_rm** - Repeats masked (converts repeats to to N's)&#xD;&#xA; - **dna** - No masking&#xD;&#xA;&#xD;&#xA;Second part options:&#xD;&#xA;&#xD;&#xA; - **.toplevel** - Includes haplotype information (not sure how aligners deal with this)&#xD;&#xA;   &#xD;&#xA; - **.primary_assembly** - Single reference base per position&#xD;&#xA;&#xD;&#xA;Right now I usually use a non-masked primary assembly for analysis, so in the case of humans:&#xD;&#xA;*Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz*&#xD;&#xA;&#xD;&#xA;Does this make sense for standard RNA-Seq, ChIP-Seq, ATAC-Seq, CLIP-Seq, scRNA-Seq, etc... ?&#xD;&#xA;&#xD;&#xA;In what cases would I prefer other genomes? Which tools/aligners take into account softmasked repeat regions?" />
  <row Id="1609" PostHistoryTypeId="1" PostId="540" RevisionGUID="a29f9370-9cee-4b21-8097-766b918cb8da" CreationDate="2017-06-07T13:23:52.380" UserId="383" Text="What Ensembl genome version should I use for alignments? (e.g. toplevel.fa vs. primary_assembly.fa)" />
  <row Id="1610" PostHistoryTypeId="3" PostId="540" RevisionGUID="a29f9370-9cee-4b21-8097-766b918cb8da" CreationDate="2017-06-07T13:23:52.380" UserId="383" Text="&lt;fasta&gt;&lt;genome&gt;&lt;ensembl&gt;&lt;biomart&gt;" />
  <row Id="1611" PostHistoryTypeId="2" PostId="541" RevisionGUID="0f95d2f9-87b7-49b5-9266-8cb927320a62" CreationDate="2017-06-07T13:29:18.277" UserId="77" Text="There's rarely a good reason to use a hard-masked genome (sometimes for blast, but that's it). For that reason, we use soft-masked genomes, which only have the benefit of showing roughly where repeats are (we never make use of this for our *-seq experiments, but it's there in case we ever want to).&#xD;&#xA;&#xD;&#xA;For primary vs. toplevel, very few aligners can properly handle additional haplotypes. If you happen to be using BWA, then the toplevel assembly would benefit you. For STAR/hisat2/bowtie2/BBmap/etc. the haplotypes will just cause you problems due to increasing multimapper rates incorrectly. Note that none of these actually use soft-masking." />
  <row Id="1612" PostHistoryTypeId="2" PostId="542" RevisionGUID="dadc598d-38ce-414e-9acc-36fdd0b9a722" CreationDate="2017-06-07T13:32:48.943" UserId="383" Text="This can be done in R very easily from an indexed .bam file.&#xD;&#xA;&#xD;&#xA;Given single-end file for **sample1**.&#xD;&#xA;&#xD;&#xA;    library(GenomicAlignments)&#xD;&#xA;    library(rtracklayer)&#xD;&#xA;    &#xD;&#xA;    ## read in BAM file (use readGAlignmentPairs for paired-end files)&#xD;&#xA;    gr &lt;- readGAlignments('sample1.bam')&#xD;&#xA;    &#xD;&#xA;    ## convert to coverages&#xD;&#xA;    gr.cov &lt;- coverage(gr)&#xD;&#xA;    &#xD;&#xA;    ## export as bigWig&#xD;&#xA;    export.bw(gr.cov,'sample1.bigwig')&#xD;&#xA;&#xD;&#xA;Be aware that this method doesn't include normalization steps (such as normalizing to total coverage). Most of these additional steps can be added if necessary. &#xD;&#xA;" />
  <row Id="1613" PostHistoryTypeId="5" PostId="525" RevisionGUID="fbc34bdc-95c8-4a34-a28b-5f6443959dba" CreationDate="2017-06-07T13:36:47.160" UserId="400" Comment="Added some benchmarks for posterity" Text="I have a pipeline for generating a BigWig file from a BAM file:&#xD;&#xA;&#xD;&#xA;    BAM -&gt; BedGraph -&gt; BigWig&#xD;&#xA;&#xD;&#xA;Which uses `bedtools genomecov` for the `BAM -&gt; BedGraph` part and `bedGraphToBigWig` for the `BedGraph -&gt; BigWig` part.&#xD;&#xA;&#xD;&#xA;The use of `bedGraphToBigWig` to create the `BigWig` file requires a BedGraph file to reside on disk in uncompressed form as it performs seeks. This is problematic for large genomes and variable coverage BAM files when there are more step changes/lines in the `BedGraph` file. My `BedGraph` files are in the order of 50 Gbytes in size and all that IO for 10-20 BAM files seems unnecessary.&#xD;&#xA;&#xD;&#xA;**Are there any tools which generate `BigWig` without having to use an uncompressed `BedGraph` file on disk?**&#xD;&#xA;&#xD;&#xA;I have tried the following tools, but they still create/use a `BedGraph` intermediary file:&#xD;&#xA;&#xD;&#xA; - `deepTools`&#xD;&#xA;&#xD;&#xA;# Some Benchmarks, Ignoring IO&#xD;&#xA;&#xD;&#xA;Here are some timings I get for creating a `BigWig` file from a `BAM` file using 3 different pipelines. All files reside on a `tmpfs` i.e. in memory.&#xD;&#xA;&#xD;&#xA;## BEDTools and Kent Utils&#xD;&#xA;&#xD;&#xA;This is the approach taken by most.&#xD;&#xA;&#xD;&#xA;    time $(bedtools genomecov -bg -ibam test.bam -split -scale 1.0 &gt; test.bedgraph \&#xD;&#xA;      &amp;&amp; bedGraphToBigWig test.bedgraph test.fasta.chrom.sizes kent.bw \&#xD;&#xA;      &amp;&amp; rm test.bedgraph)&#xD;&#xA;    &#xD;&#xA;    real    1m20.015s&#xD;&#xA;    user    0m56.608s&#xD;&#xA;    sys     0m27.271s&#xD;&#xA;&#xD;&#xA;## SAMtools and Kent Utils&#xD;&#xA;&#xD;&#xA;Replacing `bedtools genomecov` with `samtools depth` and a custom `awk` script ([`depth2bedgraph.awk`][1]) to output `bedgraph` format has a significant performance improvement:&#xD;&#xA;&#xD;&#xA;    time $(samtools depth -Q 1 --reference test.fasta test.bam \&#xD;&#xA;      | mawk -f depth2bedgraph.awk \&#xD;&#xA;      &gt; test.bedgraph \&#xD;&#xA;      &amp;&amp; bedGraphToBigWig test.bedgraph test.fasta.chrom.sizes kent.bw \&#xD;&#xA;      &amp;&amp; rm test.bedgraph)&#xD;&#xA;    &#xD;&#xA;    real    0m28.765s&#xD;&#xA;    user    0m44.999s&#xD;&#xA;    sys     0m1.166s&#xD;&#xA;&#xD;&#xA;Although it is has less features, we used `mawk` here as it's faster than `gawk` (we don't need those extra features here).&#xD;&#xA;&#xD;&#xA;## deepTools&#xD;&#xA;&#xD;&#xA;Let's see how `deepTools` performs.&#xD;&#xA;&#xD;&#xA;    time bamCoverage --numberOfProcessors max \&#xD;&#xA;      --minMappingQuality 1 \&#xD;&#xA;      --bam test.bam --binSize 1 --skipNonCoveredRegions \&#xD;&#xA;      --outFileName deeptools.bw&#xD;&#xA;    &#xD;&#xA;    real    0m40.077s&#xD;&#xA;    user    3m56.032s&#xD;&#xA;    sys     0m9.276s&#xD;&#xA;&#xD;&#xA;  [1]: https://gist.github.com/nathanhaigh/c8322002a42e40c479d3d766aab32547" />
  <row Id="1615" PostHistoryTypeId="2" PostId="543" RevisionGUID="3f6e6877-7eb6-4629-b680-695353d7fba4" CreationDate="2017-06-07T14:08:52.377" UserId="104" Text="&gt; Which tools/aligners take into account softmasked repeat regions?&#xD;&#xA;&#xD;&#xA;If you're doing whole genome - whole genome alignment (rather than read alignment) then using the softmasked genome is definitely best. Tools suitable for such large scale alignments task tend to skip marked repeats completely in their initial steps to prevent the build up of bogus short alignments that can have a massive performance impact in terms of time and memory usage. For example, [LASTZ][1] skips lower case letters during the seeding stage. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.bx.psu.edu/miller_lab/dist/README.lastz-1.02.00/README.lastz-1.02.00a.html" />
  <row Id="1616" PostHistoryTypeId="2" PostId="544" RevisionGUID="9fc9c609-0448-4f7b-8623-92b9cf5a1841" CreationDate="2017-06-07T14:44:42.053" UserId="705" Text="TOPLEVEL&#xD;&#xA;&#xD;&#xA;These files contains all sequence regions flagged as toplevel in an Ensembl&#xD;&#xA;schema. This includes chromsomes, regions not assembled into chromosomes and&#xD;&#xA;N padded haplotype/patch regions.&#xD;&#xA;&#xD;&#xA;E.g: I used the soft masked assemblies for genome annotation pipelines like MAKER, also toplevel unmasked ones for RNA-seq, ChipSeq analysis&#xD;&#xA;&#xD;&#xA;PRIMARY ASSEMBLY&#xD;&#xA;&#xD;&#xA;Primary assembly contains all toplevel sequence regions excluding haplotypes&#xD;&#xA;and patches. This file is best used for performing sequence similarity searches&#xD;&#xA;where patch and haplotype sequences would confuse analysis." />
  <row Id="1617" PostHistoryTypeId="2" PostId="545" RevisionGUID="cea76ff9-6383-4440-8254-3bbe0b0a73f8" CreationDate="2017-06-07T15:04:08.313" UserId="37" Text="Generally, you should use the soft-masked or unmasked primary assembly. Cross-species whole-genome aligners, especially older ones, do need to know soft-masked regions; otherwise they can be impractically slow for mammalian genomes. Modern read aligners are designed to work with repeats efficiently and therefore they don't need to see the soft mask.&#xD;&#xA;&#xD;&#xA;For GRCh38, though, I would recommend to use the official build [at GRC FTP](ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/). Most people will probably choose &quot;no_alt_analysis_set&quot;. Using the Ensembl version is discouraged due to its chromosome naming. We more often use &quot;chr1&quot; instead of &quot;1&quot; for GRCh38. At one point, Ensembl actually agreed to use &quot;chr1&quot; as well, but didn't make that happen due to technical issues, I guess.&#xD;&#xA;&#xD;&#xA;As to alternate haplotypes, most aligners can't work with them; no variant callers can take the advantage of these sequences, either. When you align to a reference genome containing haplotypes with an aligner not supporting these extra sequences, you will get poor mapping results." />
  <row Id="1618" PostHistoryTypeId="2" PostId="546" RevisionGUID="aeb20649-d62a-4e73-b5ce-a2d240e9767b" CreationDate="2017-06-07T15:34:51.743" UserId="425" Text="**TL;DR:**&#xD;&#xA;&#xD;&#xA;BWA-backtrack is based on [backtracking](https://en.wikipedia.org/wiki/Backtracking).&#xD;&#xA;This approach is appropriate only when the dissimilarity between the reads and the reference is low,&#xD;&#xA;or when you want to find all best hits or enumerate all possible alignments up to a specified&#xD;&#xA;number of errors.&#xD;&#xA;&#xD;&#xA;In all other situations, BWA-MEM is preferable as it can,&#xD;&#xA;thanks to its sophisticated strategy based on maximum exact matches,&#xD;&#xA;deal with errors better and also automatically switch between local and&#xD;&#xA;global alignment modes.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Long description:**&#xD;&#xA;&#xD;&#xA;I would like to provide some algorithmic insight since I believe that it&#xD;&#xA;can be very useful in this case.&#xD;&#xA;Both BWA-backtrack and BWA-MEM use the same indexing strategy&#xD;&#xA;(heavily relying on BWT-index), but the actual algorithms are quite different.&#xD;&#xA;&#xD;&#xA;[BWT-index](https://en.wikipedia.org/wiki/FM-index) (and also other full-text indexes such as&#xD;&#xA;[suffix arrays](https://en.wikipedia.org/wiki/Suffix_array) or [suffix trees](https://en.wikipedia.org/wiki/Suffix_tree))&#xD;&#xA;can easily find exact matches (imagine Ctrl+F-like search in a text editor),&#xD;&#xA;but any differences between the read and the reference,&#xD;&#xA;such as sequencing errors or genomic variants,&#xD;&#xA;make the situation complicated. One then needs to&#xD;&#xA;somehow transform inexact matching to exact matching,&#xD;&#xA;and all three BWA mappers (note that there exists also BWA-SW, but it is deprecated)&#xD;&#xA;use quite different strategies.&#xD;&#xA;&#xD;&#xA;**BWA-backtrack** looks for substrings of the reference, which would be similar&#xD;&#xA;to the entire read (end-to-end) using an algorithm&#xD;&#xA;called [backtracking](https://en.wikipedia.org/wiki/Backtracking).&#xD;&#xA;First, it searches occurences of the read without any &quot;corrections&quot;.&#xD;&#xA;If nothing found, it consideres all possible single edits; then two edits, etc.&#xD;&#xA;To make mapping efficient, one usually wants to stop with the first&#xD;&#xA;found alignment as this would be the best one.&#xD;&#xA;When required, it is also possible to find the other equally good alignments&#xD;&#xA;or to enumerate all alignments up to some edit distance or withing some divergance rate&#xD;&#xA;(see the -N option of BWA-backtrack).&#xD;&#xA;&#xD;&#xA;It turns out that&#xD;&#xA;the time required for finding an alignment&#xD;&#xA;can be exponential in the number of errors, which is probably the main problem of&#xD;&#xA;backtracking-based approaches.&#xD;&#xA;To prevent huge overheads due to dissimilar reads,&#xD;&#xA;one needs to limit the number of allowed errors to some reasonable number&#xD;&#xA;(see maxDiff in the [BWA man page](http://bio-bwa.sourceforge.net/bwa.shtml))&#xD;&#xA;and consider the other reads unaligned.&#xD;&#xA;In the case of BWA-backtrack,&#xD;&#xA;the minimum required identity level is ~97% with the default options (see the -n option).&#xD;&#xA;&#xD;&#xA;In fact, the algorithm is more complicated and uses various heuristics such as&#xD;&#xA;seed-and-extend or Z-dropoff in order to make the computation fast enough (at the price of lower accuracy). If you are interested&#xD;&#xA;in more details, all these tricks are well described in the [paper](https://www.ncbi.nlm.nih.gov/pubmed/19451168).&#xD;&#xA;&#xD;&#xA;**BWA-MEM** uses quite a different strategy. It detects long exact matches&#xD;&#xA;between the read and the reference, and then chains them into local or global alignments,&#xD;&#xA;based on what is more appropriate in that specific case.&#xD;&#xA;Such an automatic local-global switching can be very powerful and&#xD;&#xA;BWA-MEM works well with various types of data&#xD;&#xA;(short reads, long reads, low error rates, high error rates, etc.).&#xD;&#xA;" />
  <row Id="1621" PostHistoryTypeId="2" PostId="547" RevisionGUID="3cf529ac-3f14-4068-818a-640d43e83805" CreationDate="2017-06-07T16:20:19.863" UserId="110" Text="You might try using [bedtools genomecov](http://bedtools.readthedocs.io/en/latest/content/tools/genomecov.html) instead. If you provide the -d option, it reports the coverage at every position in the BAM file.&#xD;&#xA;&#xD;&#xA;    bedtools genomecov -d -ibam $inputfile &gt; &quot;${inputfile}.genomecov&quot;&#xD;&#xA;&#xD;&#xA;You can also provide a BED file if you just want to calculate in the target  region." />
  <row Id="1623" PostHistoryTypeId="2" PostId="548" RevisionGUID="7cb9bfbd-fda2-4c73-9538-0c2a0c7d3715" CreationDate="2017-06-07T16:54:42.373" UserId="711" Text="As a small introductory project, I want to compare gene sequencing of viruses for different flu viruses.&#xD;&#xA;&#xD;&#xA;I'm having a hard time locating a source for such database. Are there are publicly available ones that specializes in handling gene sequences?&#xD;&#xA;&#xD;&#xA;Thanks." />
  <row Id="1624" PostHistoryTypeId="1" PostId="548" RevisionGUID="7cb9bfbd-fda2-4c73-9538-0c2a0c7d3715" CreationDate="2017-06-07T16:54:42.373" UserId="711" Text="Publicly available gene sequence database for viruses?" />
  <row Id="1625" PostHistoryTypeId="3" PostId="548" RevisionGUID="7cb9bfbd-fda2-4c73-9538-0c2a0c7d3715" CreationDate="2017-06-07T16:54:42.373" UserId="711" Text="&lt;database&gt;&lt;public-databases&gt;&lt;genome-sequencing&gt;" />
  <row Id="1626" PostHistoryTypeId="2" PostId="549" RevisionGUID="0ea8f9c2-8a3f-403b-99a1-837ebbe5adb7" CreationDate="2017-06-07T17:31:26.533" UserId="634" Text="[Influenza virus resource][1] at NCBI or [FluDB][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/genomes/FLU/Database/nph-select.cgi?go=database&#xD;&#xA;  [2]: https://www.fludb.org/brc/home.spg?decorator=influenza" />
  <row Id="1627" PostHistoryTypeId="2" PostId="550" RevisionGUID="48075879-eb42-485f-9737-8e0b367754e2" CreationDate="2017-06-07T17:45:42.067" UserId="714" Text="There area few different influenza virus database resources:&#xD;&#xA;&#xD;&#xA;- The [Influenza Research Database (IRD)](http://www.fludb.org) &#xD;&#xA;    - A NIAID Bioinformatics Resource Center or BRC which highly curates the data brought in and integrates it with numerous other relevant data types.&#xD;&#xA;&#xD;&#xA;- The [NCBI Influenza Virus Resource](https://www.ncbi.nlm.nih.gov/genomes/FLU/Database/nph-select.cgi?go=database)&#xD;&#xA;    - A sub-project of the NCBI with data curated over and above the GenBank data that is part of the NCBI&#xD;&#xA;&#xD;&#xA;Disclaimer: I used to work for the IRD / VIPR.&#xD;&#xA;Note: Due to reputation restrictions I am only able to post two links at a time right now. Perhaps an editor would be nice enough to consolidate these or wait until I can do it." />
  <row Id="1628" PostHistoryTypeId="5" PostId="550" RevisionGUID="c9f7f01a-7215-4c79-9af3-b962651950f2" CreationDate="2017-06-07T17:55:41.980" UserId="714" Comment="Added other flu databases" Text="There area few different influenza virus database resources:&#xD;&#xA;&#xD;&#xA;- The [Influenza Research Database (IRD)](http://www.fludb.org) &#xD;&#xA;    - A NIAID Bioinformatics Resource Center or BRC which highly curates the data brought in and integrates it with numerous other relevant data types&#xD;&#xA;- The [NCBI Influenza Virus Resource](https://www.ncbi.nlm.nih.gov/genomes/FLU/Database/nph-select.cgi?go=database)&#xD;&#xA;    - A sub-project of the NCBI with data curated over and above the GenBank data that is part of the NCBI&#xD;&#xA;- The [GISAID EpiFlu Database](http://platform.gisaid.org/)&#xD;&#xA;    - A database of sequences from the Global Initiative on Sharing All Influenza Data. Has unique data from many countries but requires user agree to a data sharing policy.&#xD;&#xA;- The [OpenFluDB](http://openflu.vital-it.ch/browse.php)&#xD;&#xA;    - Former GISAID database that contains some sequence data that GenBank does not have.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Disclaimer: I used to work for the IRD / VIPR." />
  <row Id="1629" PostHistoryTypeId="5" PostId="550" RevisionGUID="18f9ef4e-8a48-4304-8df6-e13ee9b9ea10" CreationDate="2017-06-07T18:01:27.783" UserId="714" Comment="Added other databases" Text="There area few different influenza virus database resources:&#xD;&#xA;&#xD;&#xA;- The [Influenza Research Database (IRD)](http://www.fludb.org) &#xD;&#xA;    - A NIAID Bioinformatics Resource Center or BRC which highly curates the data brought in and integrates it with numerous other relevant data types&#xD;&#xA;- The [NCBI Influenza Virus Resource](https://www.ncbi.nlm.nih.gov/genomes/FLU/Database/nph-select.cgi?go=database)&#xD;&#xA;    - A sub-project of the NCBI with data curated over and above the GenBank data that is part of the NCBI&#xD;&#xA;- The [GISAID EpiFlu Database](http://platform.gisaid.org/)&#xD;&#xA;    - A database of sequences from the Global Initiative on Sharing All Influenza Data. Has unique data from many countries but requires user agree to a data sharing policy.&#xD;&#xA;- The [OpenFluDB](http://openflu.vital-it.ch/browse.php)&#xD;&#xA;    - Former GISAID database that contains some sequence data that GenBank does not have.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;For those who also may be interested another virus databases, there are:&#xD;&#xA;&#xD;&#xA;- [Virus Pathogen Resource (VIPR)](https://www.viprbrc.org)&#xD;&#xA;    - A companion portal to the IRD, which hosts curated and integrated data for most other NIAID A-C virus pathogens including (but not limited to) Ebola, Zika, Dengue, Enterovirus, and Hepatitis C&#xD;&#xA;- [LANL HIV database](https://www.hiv.lanl.gov/)&#xD;&#xA;   - Los Alamos National Laboratory HIV database with HIV data and many useful tools for all virus bioinformatics&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Disclaimer: I used to work for the IRD / VIPR." />
  <row Id="1630" PostHistoryTypeId="5" PostId="533" RevisionGUID="fc26d39c-4fc4-49d8-ba45-66da1e7a6bb7" CreationDate="2017-06-07T18:36:43.480" UserId="640" Comment="add secondary structure detection" Text="I suggest you take a look at `rna-pdb-tools` we do way more than you need! :-) The tools can get you a sequence, secondary structure and much more using various algorithms, and all is well documented http://rna-pdb-tools.readthedocs.io/en/latest/&#xD;&#xA;&#xD;&#xA;To get sequence http://rna-pdb-tools.readthedocs.io/en/latest/main.html#get-sequence &#xD;&#xA;&#xD;&#xA;    $ rna_pdb_tools.py --get_seq 5_solution_1.pdb&#xD;&#xA;    &gt; 5_solution_1.pdb A:1-576&#xD;&#xA;    CAUCCGGUAUCCCAAGACAAUCUCGGGUUGGGUUGGGAAGUAUCAUGGCUAAUCACCAUGAUGCAAUCGGGUUGAACACUUAAUUGGGUUAAAACGGUGGGGGACGAUCCCGUAACAUCCGUCCUAACGGCGACAGACUGCACGGCCCUGCCUCAGGUGUGUCCAAUGAACAGUCGUUCCGAAAGGAAG&#xD;&#xA;&#xD;&#xA;or you can get sequence and secondary structure via x3dna (http://x3dna.org/)&#xD;&#xA;&#xD;&#xA;    [mm] py3dna$ git:(master) ✗ ./rna_x3dna.py test_data/1xjr.pdb&#xD;&#xA;    test_data/1xjr.pdb&#xD;&#xA;     &gt;1xjr nts=47 [1xjr] -- secondary structure derived by DSSR&#xD;&#xA;     gGAGUUCACCGAGGCCACGCGGAGUACGAUCGAGGGUACAGUGAAUU&#xD;&#xA;     ..(((((((...((((.((((.....))..))..))).).)))))))&#xD;&#xA;&#xD;&#xA;The problem is not trivial. I hope rna-pdb-tools has stuff that can solve this issue for you (http://rna-pdb-tools.readthedocs.io/en/latest/want.html#module-rna_pdb_tools.utils.rna_x3dna.rna_x3dna)" />
  <row Id="1631" PostHistoryTypeId="2" PostId="551" RevisionGUID="7dbf89e9-d43c-4927-a5a2-f1b28c404ebd" CreationDate="2017-06-07T19:22:15.420" UserId="266" Text="First the context (in response to the comments below the question):&#xD;&#xA;&#xD;&#xA;The **PDB file format** is a fixed-column file format designed in **1970s** for storing structural models of macromolecules. So the format has been around for long time, and as a result it is being used for many things (sometimes in distant fields -- I've seen material scientists using this format for MD simulations of ceramics). The PDB files from the Protein Data Bank strictly follow the [official spec](https://www.wwpdb.org/documentation/file-format-content/format33/v3.3.html), other files in circulation not necessarily. The beef is the list of atoms with coordinates&#xD;&#xA;(the first two lines are added to stress that it's a fixed-column format, they are not part of the file):&#xD;&#xA;```&#xD;&#xA;         1         2         3         4         5         6         7         8&#xD;&#xA;12345678901234567890123456789012345678901234567890123456789012345678901234567890&#xD;&#xA;ATOM      1  N   VAL A   1       3.320  14.780   4.844  1.00 35.53           N&#xD;&#xA;ATOM      2  CA  VAL A   1       3.577  16.239   4.984  1.00 35.39           C&#xD;&#xA;ATOM      3  C   VAL A   1       4.896  16.398   5.727  1.00 30.43           C&#xD;&#xA;ATOM      4  O   VAL A   1       5.143  15.702   6.732  1.00 30.51           O&#xD;&#xA;ATOM      5  CB  VAL A   1       2.343  16.975   5.494  1.00 45.39           C&#xD;&#xA;ATOM      6  CG1 VAL A   1       2.586  18.497   5.590  1.00 60.06           C&#xD;&#xA;ATOM      7  CG2 VAL A   1       1.103  16.811   4.634  1.00 53.93           C&#xD;&#xA;ATOM      8  N   LEU A   2       5.748  17.241   5.158  1.00 28.04           N&#xD;&#xA;ATOM      9  CA  LEU A   2       7.116  17.471   5.661  1.00 24.31           C&#xD;&#xA;ATOM     10  C   LEU A   2       7.166  18.490   6.792  1.00 24.00           C&#xD;&#xA;...&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;or, to show some RNA:&#xD;&#xA;```&#xD;&#xA;ATOM     42  N3    G B   2       9.252  12.871  -1.168  1.00 36.98           N&#xD;&#xA;ATOM     43  C4    G B   2       8.424  12.964  -2.233  1.00 34.09           C&#xD;&#xA;ATOM     44  P     A B   3      10.376   8.321  -4.834  1.00 40.53           P&#xD;&#xA;ATOM     45  OP1   A B   3      11.773   8.279  -5.364  1.00 38.97           O&#xD;&#xA;ATOM     46  OP2   A B   3       9.396   7.283  -5.218  1.00 39.32           O&#xD;&#xA;ATOM     47  O5'   A B   3      10.429   8.473  -3.211  1.00 36.55           O&#xD;&#xA;ATOM     48  C5'   A B   3      11.698   8.232  -2.554  1.00 35.13           C&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;The **Protein Data Bank** (PDB) is an international (dozens people in each of the three sites: US, UK and Japan), tax-payer funded, open-data institution that archives and annotates structural models of biological molecules. It was founded in 1970s, long before the term *open access* was coined. Anyone can [deposit](https://www.wwpdb.org/deposition/deposition-resources) their model. Nowadays it is required to include also experimental data that the model is based on, but PDB does not act as gatekeeper - so the quality of PDB entries vary. Currently, PDB has 100,000+ entries, with 10,000+ added every year, and people usually refer to them using 4-character codes (`1ABC`). Proteins dominate, but 3D models of nucleic acids are also there.&#xD;&#xA;&#xD;&#xA;Interestingly (although it's not relevant to the question), the PDB format is no longer the official (&quot;master&quot;) format used by the PDB organisation. The fixed-column format had inherent limitations (max. 99,999 atoms, but also it was hard to include additional info). Since a few years ago the depositions must come in the **mmCIF format**, which has a [CIF syntax](https://www.iucr.org/resources/cif/spec/version1.1/cifsyntax) (think JSON or XML, but designed way before XML and unfortunately for most people the syntax is not intuitive),&#xD;&#xA;plus ontology defined by [PDBx/mmCIF dictionary](http://mmcif.wwpdb.org/dictionaries/mmcif_pdbx_v50.dic/Groups/index.html) (think XML Schema).&#xD;&#xA;&#xD;&#xA;The mmCIF format took a bit from RDMBS theory (tables and relations between tables), while trying to resemble the PDB format where possible. It is relatively hard to work with and the old PDB format is still more popular.&#xD;&#xA;&#xD;&#xA;Now to answer the question:&#xD;&#xA;&#xD;&#xA;The PDB format has a record called `SEQRES` that explicitly lists the sequence, for example (205D):&#xD;&#xA;```&#xD;&#xA;SEQRES   1 A   12    G   G   A   C   U   U   U   G   G   U   C   C&#xD;&#xA;SEQRES   1 B   12    G   G   A   C   U   U   U   G   G   U   C   C&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;If this record is absent, you may get the sequence from the list of atoms (residue name in columns 18-20). It is more complicated when you care about corner cases. Some residues from SEQRES may be absent in the atom list -- because it was not possible to determine atomic coordinates. The sequence may have two alternative residues in the same place, because part of the sample had a point mutation (microheterogeneity). At least that's what happens in proteins and SEQRES can't represent this case (another limitation of PDB that was solved in mmCIF).&#xD;&#xA;&#xD;&#xA;Now getting to restraints. Usually the experimental data alone is not sufficient to refine a model, and one needs restraints that represent prior knowledge - lengths of atomic bonds, angles, planarity restraints, more complex restraints based on local similarity to other structures that were determined with higher resolution data, etc, whatever can help to make a sensible model. Refinement programs (such as Refmac, BUSTER or Phenix.refine) try to fit model to the data and satisfy geometrical restraints at the same time.&#xD;&#xA;&#xD;&#xA;Macromolecular crystallographic software is quite fragmented and one normally uses several different programs in the process. It happens that the programs to generate secondary structure restraints are separate from the actual refinement programs. (secondary structure restraints are something extra, the most essential are restraints for covalent bonds). A program called LibG (from the CCP4 suite) makes RNA restraints for Refmac. Phenix has a program called [phenix.secondary_structure_restraints](https://www.phenix-online.org/documentation/reference/secondary_structure_restraints.html),&#xD;&#xA;and before that program existed you could use [a server from UCSC](http://rna.ucsc.edu/pdbrestraints/). Not sure about BUSTER.&#xD;&#xA;&#xD;&#xA;I haven't heard about &quot;bracket format&quot;, so it may not be an answer exactly to your question :-)" />
  <row Id="1632" PostHistoryTypeId="2" PostId="552" RevisionGUID="23ca35ae-ce9e-4a5d-b577-dd786b790c4f" CreationDate="2017-06-07T19:58:48.613" UserId="712" Text="To add onto @burkesquires answer [PaVE](https://pave.niaid.nih.gov/) is the best database for papillomavirus sequences. " />
  <row Id="1634" PostHistoryTypeId="5" PostId="551" RevisionGUID="d86b28f6-433d-4264-8738-8c2c970b70f5" CreationDate="2017-06-07T20:29:49.693" UserId="298" Comment="Fixed formatting. The SE sites use a slightly different version of markdown than github and code blocks are defined by leading spaces" Text="First the context (in response to the comments below the question):&#xD;&#xA;&#xD;&#xA;The **PDB file format** is a fixed-column file format designed in **1970s** for storing structural models of macromolecules. So the format has been around for long time, and as a result it is being used for many things (sometimes in distant fields -- I've seen material scientists using this format for MD simulations of ceramics). The PDB files from the Protein Data Bank strictly follow the [official spec](https://www.wwpdb.org/documentation/file-format-content/format33/v3.3.html), other files in circulation not necessarily. The beef is the list of atoms with coordinates&#xD;&#xA;(the first two lines are added to stress that it's a fixed-column format, they are not part of the file):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;             1         2         3         4         5         6         7         8&#xD;&#xA;    12345678901234567890123456789012345678901234567890123456789012345678901234567890&#xD;&#xA;    ATOM      1  N   VAL A   1       3.320  14.780   4.844  1.00 35.53           N&#xD;&#xA;    ATOM      2  CA  VAL A   1       3.577  16.239   4.984  1.00 35.39           C&#xD;&#xA;    ATOM      3  C   VAL A   1       4.896  16.398   5.727  1.00 30.43           C&#xD;&#xA;    ATOM      4  O   VAL A   1       5.143  15.702   6.732  1.00 30.51           O&#xD;&#xA;    ATOM      5  CB  VAL A   1       2.343  16.975   5.494  1.00 45.39           C&#xD;&#xA;    ATOM      6  CG1 VAL A   1       2.586  18.497   5.590  1.00 60.06           C&#xD;&#xA;    ATOM      7  CG2 VAL A   1       1.103  16.811   4.634  1.00 53.93           C&#xD;&#xA;    ATOM      8  N   LEU A   2       5.748  17.241   5.158  1.00 28.04           N&#xD;&#xA;    ATOM      9  CA  LEU A   2       7.116  17.471   5.661  1.00 24.31           C&#xD;&#xA;    ATOM     10  C   LEU A   2       7.166  18.490   6.792  1.00 24.00           C&#xD;&#xA;    ...&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;or, to show some RNA:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    ATOM     42  N3    G B   2       9.252  12.871  -1.168  1.00 36.98           N&#xD;&#xA;    ATOM     43  C4    G B   2       8.424  12.964  -2.233  1.00 34.09           C&#xD;&#xA;    ATOM     44  P     A B   3      10.376   8.321  -4.834  1.00 40.53           P&#xD;&#xA;    ATOM     45  OP1   A B   3      11.773   8.279  -5.364  1.00 38.97           O&#xD;&#xA;    ATOM     46  OP2   A B   3       9.396   7.283  -5.218  1.00 39.32           O&#xD;&#xA;    ATOM     47  O5'   A B   3      10.429   8.473  -3.211  1.00 36.55           O&#xD;&#xA;    ATOM     48  C5'   A B   3      11.698   8.232  -2.554  1.00 35.13           C&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;The **Protein Data Bank** (PDB) is an international (dozens people in each of the three sites: US, UK and Japan), tax-payer funded, open-data institution that archives and annotates structural models of biological molecules. It was founded in 1970s, long before the term *open access* was coined. Anyone can [deposit](https://www.wwpdb.org/deposition/deposition-resources) their model. Nowadays it is required to include also experimental data that the model is based on, but PDB does not act as gatekeeper - so the quality of PDB entries vary. Currently, PDB has 100,000+ entries, with 10,000+ added every year, and people usually refer to them using 4-character codes (`1ABC`). Proteins dominate, but 3D models of nucleic acids are also there.&#xD;&#xA;&#xD;&#xA;Interestingly (although it's not relevant to the question), the PDB format is no longer the official (&quot;master&quot;) format used by the PDB organisation. The fixed-column format had inherent limitations (max. 99,999 atoms, but also it was hard to include additional info). Since a few years ago the depositions must come in the **mmCIF format**, which has a [CIF syntax](https://www.iucr.org/resources/cif/spec/version1.1/cifsyntax) (think JSON or XML, but designed way before XML and unfortunately for most people the syntax is not intuitive),&#xD;&#xA;plus ontology defined by [PDBx/mmCIF dictionary](http://mmcif.wwpdb.org/dictionaries/mmcif_pdbx_v50.dic/Groups/index.html) (think XML Schema).&#xD;&#xA;&#xD;&#xA;The mmCIF format took a bit from RDMBS theory (tables and relations between tables), while trying to resemble the PDB format where possible. It is relatively hard to work with and the old PDB format is still more popular.&#xD;&#xA;&#xD;&#xA;Now to answer the question:&#xD;&#xA;&#xD;&#xA;The PDB format has a record called `SEQRES` that explicitly lists the sequence, for example (205D):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    SEQRES   1 A   12    G   G   A   C   U   U   U   G   G   U   C   C&#xD;&#xA;    SEQRES   1 B   12    G   G   A   C   U   U   U   G   G   U   C   C&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;If this record is absent, you may get the sequence from the list of atoms (residue name in columns 18-20). It is more complicated when you care about corner cases. Some residues from SEQRES may be absent in the atom list -- because it was not possible to determine atomic coordinates. The sequence may have two alternative residues in the same place, because part of the sample had a point mutation (microheterogeneity). At least that's what happens in proteins and SEQRES can't represent this case (another limitation of PDB that was solved in mmCIF).&#xD;&#xA;&#xD;&#xA;Now getting to restraints. Usually the experimental data alone is not sufficient to refine a model, and one needs restraints that represent prior knowledge - lengths of atomic bonds, angles, planarity restraints, more complex restraints based on local similarity to other structures that were determined with higher resolution data, etc, whatever can help to make a sensible model. Refinement programs (such as Refmac, BUSTER or Phenix.refine) try to fit model to the data and satisfy geometrical restraints at the same time.&#xD;&#xA;&#xD;&#xA;Macromolecular crystallographic software is quite fragmented and one normally uses several different programs in the process. It happens that the programs to generate secondary structure restraints are separate from the actual refinement programs. (secondary structure restraints are something extra, the most essential are restraints for covalent bonds). A program called LibG (from the CCP4 suite) makes RNA restraints for Refmac. Phenix has a program called [phenix.secondary_structure_restraints](https://www.phenix-online.org/documentation/reference/secondary_structure_restraints.html),&#xD;&#xA;and before that program existed you could use [a server from UCSC](http://rna.ucsc.edu/pdbrestraints/). Not sure about BUSTER.&#xD;&#xA;&#xD;&#xA;I haven't heard about &quot;bracket format&quot;, so it may not be an answer exactly to your question :-)" />
  <row Id="1635" PostHistoryTypeId="24" PostId="551" RevisionGUID="d86b28f6-433d-4264-8738-8c2c970b70f5" CreationDate="2017-06-07T20:29:49.693" Comment="Proposed by 298 approved by 266 edit id of 135" />
  <row Id="1636" PostHistoryTypeId="5" PostId="319" RevisionGUID="7fdbf863-efb8-49a3-9041-15250106397d" CreationDate="2017-06-07T21:09:58.510" UserId="425" Comment="Add a link to the format" Text="In my opinion, it is not very reliable. LiftOver is very limited in terms of transformations it can support. The [LiftOver Chain format](https://genome.ucsc.edu/goldenpath/help/chain.html) can capture only matching regions in the same order. It means that it can account for indels, but even simple structural variations become problematic.&#xD;&#xA;&#xD;&#xA;For instance, when a newer assembly is available, it is usually a recommended practice to remap all the reads rather than transform the existing alignments." />
  <row Id="1638" PostHistoryTypeId="5" PostId="548" RevisionGUID="5ad19b42-aec6-40f8-bf6e-5e0c3dea37bd" CreationDate="2017-06-07T22:19:26.737" UserId="191" Comment="improve language" Text="As a small introductory project, I want to compare gene sequences of  different strains of flu virus.&#xD;&#xA;&#xD;&#xA;Are there publicly available databases of flu virus gene sequences?" />
  <row Id="1639" PostHistoryTypeId="24" PostId="548" RevisionGUID="5ad19b42-aec6-40f8-bf6e-5e0c3dea37bd" CreationDate="2017-06-07T22:19:26.737" Comment="Proposed by 191 approved by 77, 73 edit id of 132" />
  <row Id="1640" PostHistoryTypeId="6" PostId="531" RevisionGUID="9bed0587-c1ec-46ff-849d-8203cb353fa1" CreationDate="2017-06-07T22:19:56.217" UserId="57" Comment="removed abiguitous tag 'depth'" Text="&lt;bam&gt;&lt;samtools&gt;&lt;sambamba&gt;" />
  <row Id="1641" PostHistoryTypeId="24" PostId="531" RevisionGUID="9bed0587-c1ec-46ff-849d-8203cb353fa1" CreationDate="2017-06-07T22:19:56.217" Comment="Proposed by 57 approved by 77, 73 edit id of 134" />
  <row Id="1642" PostHistoryTypeId="2" PostId="553" RevisionGUID="98123248-ce7b-46ae-9253-9765e01ab86b" CreationDate="2017-06-07T22:30:55.380" UserId="727" Text="I'm a newcomer to the world of bioinformatics, and in need of help solving a problem.&#xD;&#xA;&#xD;&#xA;My goal is to take a list of human proteins, and identify segments (13-17aa in length) with a high degree of microbial homology. Ideally, I would like to start with list of FASTA sequences, and have an easy way to generate an output of the corresponding high homology segments of each protein.&#xD;&#xA;&#xD;&#xA;Are there existing tools or software that I should be aware of that will make my life easier?&#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="1643" PostHistoryTypeId="1" PostId="553" RevisionGUID="98123248-ce7b-46ae-9253-9765e01ab86b" CreationDate="2017-06-07T22:30:55.380" UserId="727" Text="Detecting portions of human proteins with high degree of microbial homology" />
  <row Id="1644" PostHistoryTypeId="3" PostId="553" RevisionGUID="98123248-ce7b-46ae-9253-9765e01ab86b" CreationDate="2017-06-07T22:30:55.380" UserId="727" Text="&lt;fasta&gt;&lt;homology-modelling&gt;" />
  <row Id="1645" PostHistoryTypeId="2" PostId="554" RevisionGUID="75a376ab-c09f-44f4-bdfc-0a1f6b260858" CreationDate="2017-06-07T22:34:32.847" UserId="73" Text="Just replace the ```-a``` option with ```-aa```:&#xD;&#xA;&#xD;&#xA;    samtools depth -b $bedfile -aa $inputfile&#xD;&#xA;&#xD;&#xA;I see that you're using the GRCh38 human reference genome build, which includes alternate scaffolds that represent a wider variety of genomic variation in the human genome. This was quite a substantial change from [previous reference genome builds](https://www.quora.com/What-are-the-differences-between-GRCh38-and-GRCh37), and many bioinformatics tools still haven't been updated to accept the GRCh38 reference.&#xD;&#xA;&#xD;&#xA;The discrepancy in entry size is probably because there are alternate contigs that are uncovered by the reads that have been mapped. This is expected, but can cause a bit of confusion." />
  <row Id="1646" PostHistoryTypeId="6" PostId="525" RevisionGUID="122c0161-dbb8-42bd-b833-be5a32b0fd6e" CreationDate="2017-06-07T22:36:06.587" UserId="96" Comment="Added the &quot;formats&quot; tag" Text="&lt;bam&gt;&lt;formats&gt;&lt;conversion&gt;" />
  <row Id="1647" PostHistoryTypeId="24" PostId="525" RevisionGUID="122c0161-dbb8-42bd-b833-be5a32b0fd6e" CreationDate="2017-06-07T22:36:06.587" Comment="Proposed by 96 approved by 77, 73 edit id of 133" />
  <row Id="1648" PostHistoryTypeId="5" PostId="304" RevisionGUID="5aee32b1-bf62-461e-be34-2eb4b24c401f" CreationDate="2017-06-07T22:36:44.750" UserId="191" Comment="Since I update the question (a while ago), I corrected the answer to correspond to the question; now I think it matches the answer" Text="Short answer: yes, but you need to get permission (and modified software) from ONT before doing that.&#xD;&#xA;&#xD;&#xA;... but that doesn't tell the whole story. This question has the potential to be very confusing, and that's through no fault of the questioner. The issue is that for the MinION, sequencing (or more specifically, generating the raw data in the form of an electrical signal trace) is distinct and separable from base calling. Many other sequencers also have distinct raw data and base-calling phases, but they're not democratised to the degree they are on the MinION.&#xD;&#xA;&#xD;&#xA;The &quot;sequencing&quot; part of MinION sequencing is carried out by ONT software, namely MinKNOW. As explained to me during PoreCampAU 2017, when the MinION is initially plugged into a computer it is missing the firmware necessary to carry out the sequencing. The most recent version of this firmware is usually downloaded at the start of a sequencing run by sending a request to ONT servers. In the usual case, you can't do sequencing without being able to access those servers, and you can't do sequencing without ONT knowing about it. However, ONT acknowledge that there are people out there who won't have Internet access when sequencing (e.g. sequencing Ebola in Africa, or metagenomic sequencing in the middle of the ocean), and an email to ```&lt;support@nanoporetech.com&gt;``` with reasons is likely to result in a quick software fix to the local sequencing problem.&#xD;&#xA;&#xD;&#xA;Once the raw signals are acquired, the &quot;base-calling&quot; part of MinION sequencing can be done anywhere. The ONT-maintained basecaller is Albacore, and this will get the first model updates whenever the sequencing technology is changed (which happens a lot). Albacore is a local basecaller which can be obtained from ONT by browsing through their community pages (available to anyone who has a MinION); ONT switched to only allowing people to do basecalling locally in about April 2017, after establishing that using AWS servers was just too expensive. Albacore is open source and free-as-in-beer, but has a restrictive licensing agreement which limits the distribution (and modification) of the program. However, Albacore is not the only available basecaller. ONT provide a FOSS basecaller called [nanonet][1]. It's a little bit behind Albacore on technology, but ONT have said that all useful Albacore changes will eventually propagate through to nanonet. There is another non-ONT basecaller that I'm aware of which uses a neural network for basecalling: [deepnano][2]. Other basecallers exist, each varying distances away technology-wise, and I expect that more will appear in the future as the technology stabilises and more change-resistant computer scientists get in on the act.&#xD;&#xA;&#xD;&#xA;Edit: ONT has just pulled back the curtain on their basecalling software; all the repositories that I've looked at so far (except for the Cliveome) have been released under the Mozilla Public License (free and open source, with some conditions and limitations). Included in that software repository is Scrappie, which is their testing / bleeding-edge version of Albacore.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/nanoporetech/nanonet&#xD;&#xA;  [2]: https://bitbucket.org/vboza/deepnano" />
  <row Id="1649" PostHistoryTypeId="24" PostId="304" RevisionGUID="5aee32b1-bf62-461e-be34-2eb4b24c401f" CreationDate="2017-06-07T22:36:44.750" Comment="Proposed by 191 approved by 73 edit id of 131" />
  <row Id="1650" PostHistoryTypeId="5" PostId="304" RevisionGUID="cecca65c-7e7c-4829-88f0-b1c867cedf40" CreationDate="2017-06-07T22:47:06.100" UserId="73" Comment="added link to nanopore github" Text="Short answer: yes, but you need to get permission (and modified software) from ONT before doing that.&#xD;&#xA;&#xD;&#xA;... but that doesn't tell the whole story. This question has the potential to be very confusing, and that's through no fault of the questioner. The issue is that for the MinION, sequencing (or more specifically, generating the raw data in the form of an electrical signal trace) is distinct and separable from base calling. Many other sequencers also have distinct raw data and base-calling phases, but they're not democratised to the degree they are on the MinION.&#xD;&#xA;&#xD;&#xA;The &quot;sequencing&quot; part of MinION sequencing is carried out by ONT software, namely MinKNOW. As explained to me during PoreCampAU 2017, when the MinION is initially plugged into a computer it is missing the firmware necessary to carry out the sequencing. The most recent version of this firmware is usually downloaded at the start of a sequencing run by sending a request to ONT servers. In the usual case, you can't do sequencing without being able to access those servers, and you can't do sequencing without ONT knowing about it. However, ONT acknowledge that there are people out there who won't have Internet access when sequencing (e.g. sequencing Ebola in Africa, or metagenomic sequencing in the middle of the ocean), and an email to ```&lt;support@nanoporetech.com&gt;``` with reasons is likely to result in a quick software fix to the local sequencing problem.&#xD;&#xA;&#xD;&#xA;Once the raw signals are acquired, the &quot;base-calling&quot; part of MinION sequencing can be done anywhere. The ONT-maintained basecaller is Albacore, and this will get the first model updates whenever the sequencing technology is changed (which happens a lot). Albacore is a local basecaller which can be obtained from ONT by browsing through their community pages (available to anyone who has a MinION); ONT switched to only allowing people to do basecalling locally in about April 2017, after establishing that using AWS servers was just too expensive. Albacore is open source and free-as-in-beer, but has a restrictive licensing agreement which limits the distribution (and modification) of the program. However, Albacore is not the only available basecaller. ONT provide a FOSS basecaller called [nanonet][1]. It's a little bit behind Albacore on technology, but ONT have said that all useful Albacore changes will eventually propagate through to nanonet. There is another non-ONT basecaller that I'm aware of which uses a neural network for basecalling: [deepnano][2]. Other basecallers exist, each varying distances away technology-wise, and I expect that more will appear in the future as the technology stabilises and more change-resistant computer scientists get in on the act.&#xD;&#xA;&#xD;&#xA;Edit: ONT has just pulled back the curtain on their basecalling software; all the repositories that I've looked at so far (except for the Cliveome) have been released under the Mozilla Public License (free and open source, with some conditions and limitations). Included in [that software repository](https://github.com/nanoporetech/) is Scrappie, which is their testing / bleeding-edge version of Albacore.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/nanoporetech/nanonet&#xD;&#xA;  [2]: https://bitbucket.org/vboza/deepnano" />
  <row Id="1651" PostHistoryTypeId="2" PostId="555" RevisionGUID="5db40b70-5f7d-48fa-b329-cf1f9a9a4011" CreationDate="2017-06-07T22:51:54.867" UserId="298" Text="Sounds like precisely the job BLAST was developed for. Now, which flavor will depend on what you want to do and what data you have available. Some options:&#xD;&#xA;&#xD;&#xA;1. **PSI-BLAST**: this is usually the best choice if you are trying to find protein homologs. It works by building a hidden markov model describing your query sequence and using that model to query a database of proteins. The advantage is that it is run in multiple iterations, giving you the chance to add or remove results (so you add the ones that are true positives and remove false ones), eventually building a pretty good model of your protein. This is far moer powerful than a simple homology-based approach since proteins work via protein domains and simple homology is not as important as specific conserved functional residues. &#xD;&#xA;&#xD;&#xA;2. **BLASTp**: Simple protein-protein blast. It will identify homologous proteins based on sequence similarity. Whether or not that also implies functional homology is not that simple and will depend on each case you investigate. &#xD;&#xA;&#xD;&#xA;3. **tBLASTn**: this is a tool that takes protein sequences as input and compares them to a database of DNA which is dynamically translated in all 6 possible reading frames. Very good for finding homologous sequences when you don't have well annotated protein information for the target species. It has the benefit of being more sensitive and able to find more distant homologies than basic nucleotide BLASTn and the go-to approach when your target species is distant and not well annotated. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;All of these can be run online through the [NCBI's BLAST page][1]. If you want to investigate hundreds of proteins, I suggest you install blast locally. You can then either download the relevant target sequences from NCBI and rebuild the blast database locally (if so, I suggest you ask a new question about how to do that) or, use NCBI's remote blast client which lets you use a locally stored query file and will run blast on the NCBI's servers. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Now, these programs will return what are known as High Scoring Pairs (HSPs), the regions of your query sequence(s) that align well to the target. There are various options you can play with to improve the sensitivity or the specificity but a discussion of those would require far more details about what you're doing and would be best in a new question as well. &#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi" />
  <row Id="1652" PostHistoryTypeId="5" PostId="555" RevisionGUID="7a10b31e-e72b-403c-acbb-b3e87c9196a9" CreationDate="2017-06-07T23:04:27.220" UserId="298" Comment="added 516 characters in body" Text="Sounds like precisely the job BLAST was developed for. Now, which flavor will depend on what you want to do and what data you have available. Some options:&#xD;&#xA;&#xD;&#xA;1. **PSI-BLAST**: this is usually the best choice if you are trying to find protein homologs. It works by building a hidden markov model describing your query sequence and using that model to query a database of proteins. The advantage is that it is run in multiple iterations, giving you the chance to add or remove results (so you add the ones that are true positives and remove false ones), eventually building a pretty good model of your protein. This is far moer powerful than a simple homology-based approach since proteins work via protein domains and simple homology is not as important as specific conserved functional residues. &#xD;&#xA;&#xD;&#xA; For this, go to the NCBI [protein blast page][1] and select PSI-BLAST:&#xD;&#xA;&#xD;&#xA; [![psi-blast option at ncbi][2]][2]&#xD;&#xA;&#xD;&#xA;2. **BLASTp**: Simple protein-protein blast. It will identify homologous proteins based on sequence similarity. Whether or not that also implies functional homology is not that simple and will depend on each case you investigate. &#xD;&#xA;&#xD;&#xA; As above, go to the NCBI [protein blast page][1], but this time use the defaults.&#xD;&#xA;&#xD;&#xA;3. **tBLASTn**: this is a tool that takes protein sequences as input and compares them to a database of DNA which is dynamically translated in all 6 possible reading frames. Very good for finding homologous sequences when you don't have well annotated protein information for the target species. It has the benefit of being more sensitive and able to find more distant homologies than basic nucleotide BLASTn and the go-to approach when your target species is distant and not well annotated. &#xD;&#xA;&#xD;&#xA; [NCBI's tBLASTn page][3].&#xD;&#xA;&#xD;&#xA;All of these can be run online through the [NCBI's BLAST page][4]. If you want to investigate hundreds of proteins, I suggest you install blast locally. You can then either download the relevant target sequences from NCBI and rebuild the blast database locally (if so, I suggest you ask a new question about how to do that) or, use NCBI's remote blast client which lets you use a locally stored query file and will run blast on the NCBI's servers. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Now, these programs will return what are known as High Scoring Pairs (HSPs), the regions of your query sequence(s) that align well to the target. There are various options you can play with to improve the sensitivity or the specificity but a discussion of those would require far more details about what you're doing and would be best in a new question as well. &#xD;&#xA;&#xD;&#xA;Once you have your HSPs, you can relatively easily parse them to select regions with a given range of sequence similarity values and of a specific length. Once again, that would be better discussed in a separate question once you have your results and can show an example. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&amp;PAGE_TYPE=BlastSearch&amp;LINK_LOC=blasthome&#xD;&#xA;  [2]: https://i.stack.imgur.com/q4yAK.png&#xD;&#xA;  [3]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=tblastn&amp;PAGE_TYPE=BlastSearch&amp;BLAST_SPEC=&amp;LINK_LOC=blasttab&amp;LAST_PAGE=blastp&#xD;&#xA;  [4]: https://blast.ncbi.nlm.nih.gov/Blast.cgi" />
  <row Id="1653" PostHistoryTypeId="5" PostId="551" RevisionGUID="b3c101cf-0f30-4718-ba61-9027c41a42a1" CreationDate="2017-06-07T23:21:54.793" UserId="266" Comment="shorten and try biopython" Text="## Context&#xD;&#xA;&#xD;&#xA;The **PDB file format** is a fixed-column file format designed in 1970s for storing structural models of macromolecules. The format has been around for long time, has many uses, and although it has [official spec](https://www.wwpdb.org/documentation/file-format-content/format33/v3.3.html) the files in circulation may not strictly conform to it. It always has a list of atoms with coordinates&#xD;&#xA;(the first two lines are added to stress that it's a fixed-column format, they are not part of the file):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;             1         2         3         4         5         6         7         8&#xD;&#xA;    12345678901234567890123456789012345678901234567890123456789012345678901234567890&#xD;&#xA;    ATOM      1  N   VAL A   1       3.320  14.780   4.844  1.00 35.53           N&#xD;&#xA;    ATOM      2  CA  VAL A   1       3.577  16.239   4.984  1.00 35.39           C&#xD;&#xA;    ATOM      3  C   VAL A   1       4.896  16.398   5.727  1.00 30.43           C&#xD;&#xA;    ATOM      4  O   VAL A   1       5.143  15.702   6.732  1.00 30.51           O&#xD;&#xA;    ATOM      5  CB  VAL A   1       2.343  16.975   5.494  1.00 45.39           C&#xD;&#xA;    ATOM      6  CG1 VAL A   1       2.586  18.497   5.590  1.00 60.06           C&#xD;&#xA;    ATOM      7  CG2 VAL A   1       1.103  16.811   4.634  1.00 53.93           C&#xD;&#xA;    ATOM      8  N   LEU A   2       5.748  17.241   5.158  1.00 28.04           N&#xD;&#xA;    ATOM      9  CA  LEU A   2       7.116  17.471   5.661  1.00 24.31           C&#xD;&#xA;    ATOM     10  C   LEU A   2       7.166  18.490   6.792  1.00 24.00           C&#xD;&#xA;    ...&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;or, to show some RNA:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    ATOM     42  N3    G B   2       9.252  12.871  -1.168  1.00 36.98           N&#xD;&#xA;    ATOM     43  C4    G B   2       8.424  12.964  -2.233  1.00 34.09           C&#xD;&#xA;    ATOM     44  P     A B   3      10.376   8.321  -4.834  1.00 40.53           P&#xD;&#xA;    ATOM     45  OP1   A B   3      11.773   8.279  -5.364  1.00 38.97           O&#xD;&#xA;    ATOM     46  OP2   A B   3       9.396   7.283  -5.218  1.00 39.32           O&#xD;&#xA;    ATOM     47  O5'   A B   3      10.429   8.473  -3.211  1.00 36.55           O&#xD;&#xA;    ATOM     48  C5'   A B   3      11.698   8.232  -2.554  1.00 35.13           C&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;The **Protein Data Bank** -- international institution that archives and annotates structural models of biological molecules that anyone can [deposit](https://www.wwpdb.org/deposition/deposition-resources) -- is now using **mmCIF** as the primary format. The fixed-column PDB format had inherent limitations (max. 99,999 atoms, but also it was hard to include additional info). They still generate PDB files if possible (i.e. except the largest structures), but the only format accepted now for depositions is mmCIF.&#xD;&#xA;&#xD;&#xA;mmCIF has a rather not intuitive [CIF syntax](https://www.iucr.org/resources/cif/spec/version1.1/cifsyntax) (think JSON or XML, but designed before XML),&#xD;&#xA;plus ontology defined by [PDBx/mmCIF dictionary](http://mmcif.wwpdb.org/dictionaries/mmcif_pdbx_v50.dic/Groups/index.html) (think XML Schema).&#xD;&#xA;The content of mmCIF is divided into table-like categories with relations between tables (it was designed at the peak of RDMBS popularity) and is a bit harder to work with, so the old PDB format is more popular.&#xD;&#xA;&#xD;&#xA;## Sequence from PDB&#xD;&#xA;&#xD;&#xA;The PDB format has a record called `SEQRES` that explicitly lists the sequence, for example (5NEO):&#xD;&#xA;&#xD;&#xA;    SEQRES   1 A   18    G   G   U   G   G   G   G   A   C   G   A   C   C          &#xD;&#xA;    SEQRES   2 A   18    C   C   A CBV   C&#xD;&#xA;&#xD;&#xA;The mmCIF format has more details:&#xD;&#xA;&#xD;&#xA;    _entity_poly.pdbx_seq_one_letter_code       'GGUGGGGACGACCCCA(CBV)C' &#xD;&#xA;    _entity_poly.pdbx_seq_one_letter_code_can   GGUGGGGACGACCCCACC &#xD;&#xA;    _entity_poly.pdbx_strand_id                 A &#xD;&#xA;&#xD;&#xA;    loop_&#xD;&#xA;    _entity_poly_seq.entity_id &#xD;&#xA;    _entity_poly_seq.num &#xD;&#xA;    _entity_poly_seq.mon_id &#xD;&#xA;    _entity_poly_seq.hetero &#xD;&#xA;    1 1  G   n &#xD;&#xA;    1 2  G   n &#xD;&#xA;    1 3  U   n &#xD;&#xA;    1 4  G   n &#xD;&#xA;    1 5  G   n &#xD;&#xA;    1 6  G   n &#xD;&#xA;    1 7  G   n &#xD;&#xA;    1 8  A   n &#xD;&#xA;    1 9  C   n &#xD;&#xA;    1 10 G   n &#xD;&#xA;    1 11 A   n &#xD;&#xA;    1 12 C   n &#xD;&#xA;    1 13 C   n &#xD;&#xA;    1 14 C   n &#xD;&#xA;    1 15 C   n &#xD;&#xA;    1 16 A   n &#xD;&#xA;    1 17 CBV n &#xD;&#xA;    1 18 C   n &#xD;&#xA;&#xD;&#xA;The sequence is conveniently extracted for you.&#xD;&#xA;&#xD;&#xA;Additionally, `_entity_poly_seq` in mmCIF includes information about microheterogeneity, and the SEQRES record doesn't. It is relevant when the model has two alternative residues in the same place, because part of the sample had a point mutation.&#xD;&#xA;&#xD;&#xA;PDB files that didn't come from PDB usually don't have the SEQRES record.&#xD;&#xA;You may get the sequence from the list of atoms (residue names in columns 18-20), but some residues may be missing in the atom list if atomic positions could not be determined.&#xD;&#xA;&#xD;&#xA;So it should be easy to extract the sequence either way.&#xD;&#xA;For example, BioPython has module BioSeqIO with two PDB pseudo-formats:&#xD;&#xA;&#xD;&#xA;* pdb-seqres - Reads a Protein Data Bank (PDB) file to determine the complete protein sequence as it appears in the header (no dependencies).&#xD;&#xA;* pdb-atom - Uses Bio.PDB to determine the (partial) protein sequence as it appears in the structure based on the atom coordinate section of the file (requires NumPy for Bio.PDB).&#xD;&#xA;&#xD;&#xA;*I just tried BioPython 1.66 and while `SeqIO.parse()` can extract protein sequence, it fails with RNA, such as the 5NEO above. Ooops.*&#xD;&#xA;&#xD;&#xA;## Restraints&#xD;&#xA;&#xD;&#xA;Usually the experimental data alone is not sufficient to refine a model, and one needs restraints that represent prior knowledge - lengths of atomic bonds, angles, planarity restraints, more complex restraints based on local similarity to other structures that were determined with higher resolution data, etc, whatever can help to make a sensible model. Refinement programs (such as Refmac, BUSTER or Phenix.refine) try to fit model to the data and satisfy geometrical restraints at the same time.&#xD;&#xA;&#xD;&#xA;Macromolecular crystallographic software is quite fragmented and one normally uses several different programs in the process. It happens that the programs to generate secondary structure restraints are separate from the actual refinement programs. (secondary structure restraints are something extra, the most essential are restraints for covalent bonds). A program called LibG (from the CCP4 suite) makes RNA restraints for Refmac. Phenix has a program called [phenix.secondary_structure_restraints](https://www.phenix-online.org/documentation/reference/secondary_structure_restraints.html),&#xD;&#xA;and before that program existed you could use [a server from UCSC](http://rna.ucsc.edu/pdbrestraints/). Not sure about BUSTER.&#xD;&#xA;&#xD;&#xA;I haven't heard about &quot;bracket format&quot;, so it may not be an answer exactly to your question :-)" />
  <row Id="1654" PostHistoryTypeId="2" PostId="556" RevisionGUID="d3eb5ab0-0cda-4d22-864f-c6acd4ce38a0" CreationDate="2017-06-07T23:31:46.327" UserId="729" Text="If you know how to use python:&#xD;&#xA;&#xD;&#xA;1 - Download the ModeRNA module from [here][1] and install&#xD;&#xA;&#xD;&#xA;2 - from a python IDLE execute:&#xD;&#xA;&#xD;&#xA;    from moderna import *&#xD;&#xA;    m = load_model('file.pdb', 'A') #A is the chain&#xD;&#xA;    seq = m.get_sequence()&#xD;&#xA;    sec = m.get_secstruc()&#xD;&#xA;&#xD;&#xA;the variable `sec` store the secondary structure in dot-bracket notation.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/lenarother/moderna" />
  <row Id="1655" PostHistoryTypeId="5" PostId="551" RevisionGUID="b17c446d-6a20-475e-889b-350ef3cfb5c6" CreationDate="2017-06-07T23:53:58.087" UserId="266" Comment="edit section about restraints, adding examples" Text="## Context&#xD;&#xA;&#xD;&#xA;The **PDB file format** is a fixed-column file format designed in 1970s for storing structural models of macromolecules. The format has been around for long time, has many uses, and although it has [official spec](https://www.wwpdb.org/documentation/file-format-content/format33/v3.3.html) the files in circulation may not strictly conform to it. It always has a list of atoms with coordinates&#xD;&#xA;(the first two lines are added to stress that it's a fixed-column format, they are not part of the file):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;             1         2         3         4         5         6         7         8&#xD;&#xA;    12345678901234567890123456789012345678901234567890123456789012345678901234567890&#xD;&#xA;    ATOM      1  N   VAL A   1       3.320  14.780   4.844  1.00 35.53           N&#xD;&#xA;    ATOM      2  CA  VAL A   1       3.577  16.239   4.984  1.00 35.39           C&#xD;&#xA;    ATOM      3  C   VAL A   1       4.896  16.398   5.727  1.00 30.43           C&#xD;&#xA;    ATOM      4  O   VAL A   1       5.143  15.702   6.732  1.00 30.51           O&#xD;&#xA;    ATOM      5  CB  VAL A   1       2.343  16.975   5.494  1.00 45.39           C&#xD;&#xA;    ATOM      6  CG1 VAL A   1       2.586  18.497   5.590  1.00 60.06           C&#xD;&#xA;    ATOM      7  CG2 VAL A   1       1.103  16.811   4.634  1.00 53.93           C&#xD;&#xA;    ATOM      8  N   LEU A   2       5.748  17.241   5.158  1.00 28.04           N&#xD;&#xA;    ATOM      9  CA  LEU A   2       7.116  17.471   5.661  1.00 24.31           C&#xD;&#xA;    ATOM     10  C   LEU A   2       7.166  18.490   6.792  1.00 24.00           C&#xD;&#xA;    ...&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;or, to show some RNA:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    ATOM     42  N3    G B   2       9.252  12.871  -1.168  1.00 36.98           N&#xD;&#xA;    ATOM     43  C4    G B   2       8.424  12.964  -2.233  1.00 34.09           C&#xD;&#xA;    ATOM     44  P     A B   3      10.376   8.321  -4.834  1.00 40.53           P&#xD;&#xA;    ATOM     45  OP1   A B   3      11.773   8.279  -5.364  1.00 38.97           O&#xD;&#xA;    ATOM     46  OP2   A B   3       9.396   7.283  -5.218  1.00 39.32           O&#xD;&#xA;    ATOM     47  O5'   A B   3      10.429   8.473  -3.211  1.00 36.55           O&#xD;&#xA;    ATOM     48  C5'   A B   3      11.698   8.232  -2.554  1.00 35.13           C&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;The **Protein Data Bank** -- international institution that archives and annotates structural models of biological molecules that anyone can [deposit](https://www.wwpdb.org/deposition/deposition-resources) -- is now using **mmCIF** as the primary format. The fixed-column PDB format had inherent limitations (max. 99,999 atoms, but also it was hard to include additional info). They still generate PDB files if possible (i.e. except the largest structures), but the only format accepted now for depositions is mmCIF.&#xD;&#xA;&#xD;&#xA;mmCIF has a rather not intuitive [CIF syntax](https://www.iucr.org/resources/cif/spec/version1.1/cifsyntax) (think JSON or XML, but designed before XML),&#xD;&#xA;plus ontology defined by [PDBx/mmCIF dictionary](http://mmcif.wwpdb.org/dictionaries/mmcif_pdbx_v50.dic/Groups/index.html) (think XML Schema).&#xD;&#xA;The content of mmCIF is divided into table-like categories with relations between tables (it was designed at the peak of RDMBS popularity) and is a bit harder to work with, so the old PDB format is more popular.&#xD;&#xA;&#xD;&#xA;## Sequence from PDB&#xD;&#xA;&#xD;&#xA;The PDB format has a record called `SEQRES` that explicitly lists the sequence, for example (5NEO):&#xD;&#xA;&#xD;&#xA;    SEQRES   1 A   18    G   G   U   G   G   G   G   A   C   G   A   C   C          &#xD;&#xA;    SEQRES   2 A   18    C   C   A CBV   C&#xD;&#xA;&#xD;&#xA;The mmCIF format has more details:&#xD;&#xA;&#xD;&#xA;    _entity_poly.pdbx_seq_one_letter_code       'GGUGGGGACGACCCCA(CBV)C' &#xD;&#xA;    _entity_poly.pdbx_seq_one_letter_code_can   GGUGGGGACGACCCCACC &#xD;&#xA;    _entity_poly.pdbx_strand_id                 A &#xD;&#xA;&#xD;&#xA;    loop_&#xD;&#xA;    _entity_poly_seq.entity_id &#xD;&#xA;    _entity_poly_seq.num &#xD;&#xA;    _entity_poly_seq.mon_id &#xD;&#xA;    _entity_poly_seq.hetero &#xD;&#xA;    1 1  G   n &#xD;&#xA;    1 2  G   n &#xD;&#xA;    1 3  U   n &#xD;&#xA;    1 4  G   n &#xD;&#xA;    1 5  G   n &#xD;&#xA;    1 6  G   n &#xD;&#xA;    1 7  G   n &#xD;&#xA;    1 8  A   n &#xD;&#xA;    1 9  C   n &#xD;&#xA;    1 10 G   n &#xD;&#xA;    1 11 A   n &#xD;&#xA;    1 12 C   n &#xD;&#xA;    1 13 C   n &#xD;&#xA;    1 14 C   n &#xD;&#xA;    1 15 C   n &#xD;&#xA;    1 16 A   n &#xD;&#xA;    1 17 CBV n &#xD;&#xA;    1 18 C   n &#xD;&#xA;&#xD;&#xA;The sequence is conveniently extracted for you.&#xD;&#xA;&#xD;&#xA;Additionally, `_entity_poly_seq` in mmCIF includes information about microheterogeneity, and the SEQRES record doesn't. It is relevant when the model has two alternative residues in the same place, because part of the sample had a point mutation.&#xD;&#xA;&#xD;&#xA;PDB files that didn't come from PDB usually don't have the SEQRES record.&#xD;&#xA;You may get the sequence from the list of atoms (residue names in columns 18-20), but some residues may be missing in the atom list if atomic positions could not be determined.&#xD;&#xA;&#xD;&#xA;So it should be easy to extract the sequence either way.&#xD;&#xA;For example, BioPython has module BioSeqIO with two PDB pseudo-formats:&#xD;&#xA;&#xD;&#xA;* pdb-seqres - Reads a Protein Data Bank (PDB) file to determine the complete protein sequence as it appears in the header (no dependencies).&#xD;&#xA;* pdb-atom - Uses Bio.PDB to determine the (partial) protein sequence as it appears in the structure based on the atom coordinate section of the file (requires NumPy for Bio.PDB).&#xD;&#xA;&#xD;&#xA;*I just tried BioPython 1.66 and while `SeqIO.parse()` can extract protein sequence, it fails with RNA, such as the 5NEO above. Ooops.*&#xD;&#xA;&#xD;&#xA;## Secondary structure restraints&#xD;&#xA;&#xD;&#xA;We may be thinking about different restraints here. I'll write about restraints used in refinement.&#xD;&#xA;The experimental data alone is normally not sufficient to refine a model, so one needs restraints that represent prior knowledge - lengths of atomic bonds, angles, planarity restraints, restraints based on local similarity to other structures that were determined from higher resolution data, etc, whatever can help to make a sensible model. Refinement programs (such as Refmac, BUSTER, Phenix.refine) try to fit the model to the data and satisfy geometrical restraints at the same time.&#xD;&#xA;&#xD;&#xA;Macromolecular crystallographic software is quite fragmented and one normally uses several different programs in the process. It happens that the programs to generate secondary structure restraints are separate from the actual refinement programs. (Secondary structure restraints are less essential than restraints for covalent bonds). CCP4 has a program called LibG that makes DNA/RNA restraints for Refmac. Phenix has a program called [phenix.secondary_structure_restraints](https://www.phenix-online.org/documentation/reference/secondary_structure_restraints.html),&#xD;&#xA;or you can use a [server from UCSC](http://rna.ucsc.edu/pdbrestraints/).&#xD;&#xA;&#xD;&#xA;These restraints don't have a &quot;bracket format&quot;, but they explicitly specify expected distances and angles between atoms. For example (for phenix.refine):&#xD;&#xA;&#xD;&#xA;      bond { &#xD;&#xA;        action = *add &#xD;&#xA;        atom_selection_1 = chain  A and resid    1  and name  O6 &#xD;&#xA;        atom_selection_2 = chain  A and resid   18  and name  N4 &#xD;&#xA;        distance_ideal = 2.91&#xD;&#xA;        sigma = 0.1&#xD;&#xA;      }&#xD;&#xA;&#xD;&#xA;or (for Refmac):&#xD;&#xA;&#xD;&#xA;    exte dist first chain A resi 16 ins  . atom N6 second chain A resi 3 ins . atom O4 value 2.94 sigma 0.15 type 1 &#xD;&#xA;    exte dist first chain A resi 16 ins  . atom N1 second chain A resi 3 ins . atom N3 value 2.84 sigma 0.1 type 1 &#xD;&#xA;    exte torsion first chain A resi 16 ins  . atom C2 second chain A resi 16 ins . atom N1 third chain A resi 3 ins . atom N3 fourth chain A resi 3 ins . atom C4 value 180 sigma 15 type 1&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1656" PostHistoryTypeId="2" PostId="557" RevisionGUID="94733816-92b4-4934-a6e4-ebf079c52d26" CreationDate="2017-06-08T00:09:09.243" UserId="35" Text="For mouse, I still see people using mm9/NCBI37 in high profile publications even though mm10/GRCm38 was released more than 5 years ago (2011). I personally don't think that's a great idea, but it's certainly valid according to the peer reviewers.&#xD;&#xA;&#xD;&#xA;It also depends on your application. If you are working with coding regions (likely well known for a long time) or extracting genome-wide stats (enrichment at TSS, for example), the differences should be negligible." />
  <row Id="1658" PostHistoryTypeId="2" PostId="558" RevisionGUID="8fa0382f-ba3c-42bf-b7e5-028c8fefc0e8" CreationDate="2017-06-08T01:03:40.633" UserId="732" Text="it is supposed that quantum computers would help to find the native state of all human DNA proteins.&#xD;&#xA;&#xD;&#xA;does someone know how to modelate the protein folding problem into a decision problem solvable either by [quantum annealers](https://en.wikipedia.org/wiki/Quantum_annealing) or [universal quantum computers](https://en.wikipedia.org/wiki/Quantum_Turing_machine)?&#xD;&#xA;&#xD;&#xA;it would also be interesting to know, how to modelate a decision problem for classical computers, like [Folding @ Home](http://folding.stanford.edu/)." />
  <row Id="1659" PostHistoryTypeId="1" PostId="558" RevisionGUID="8fa0382f-ba3c-42bf-b7e5-028c8fefc0e8" CreationDate="2017-06-08T01:03:40.633" UserId="732" Text="Protein folding problem model for quantum computers?" />
  <row Id="1660" PostHistoryTypeId="3" PostId="558" RevisionGUID="8fa0382f-ba3c-42bf-b7e5-028c8fefc0e8" CreationDate="2017-06-08T01:03:40.633" UserId="732" Text="&lt;proteins&gt;&lt;computation&gt;" />
  <row Id="1661" PostHistoryTypeId="2" PostId="559" RevisionGUID="5aed46c8-313c-4884-9176-139ddcc7260e" CreationDate="2017-06-08T02:25:46.173" UserId="734" Text="I have a gene expression quantification file from [TCGA][1] that contains the following lines:&#xD;&#xA;&#xD;&#xA;    ENSG00000242268.2	591.041000514&#xD;&#xA;    ENSG00000270112.3	0.0&#xD;&#xA;    ENSG00000167578.15	62780.6543066&#xD;&#xA;    ENSG00000273842.1	0.0&#xD;&#xA;    ENSG00000078237.5	36230.832883&#xD;&#xA;    ENSG00000146083.10	189653.152706&#xD;&#xA;    ENSG00000225275.4	0.0&#xD;&#xA;    ENSG00000158486.12	420.761140072&#xD;&#xA;    ENSG00000198242.12	2914738.3675&#xD;&#xA;    ENSG00000259883.1	1632.83700531&#xD;&#xA;    ENSG00000231981.3	0.0&#xD;&#xA;    ENSG00000269475.2	0.0&#xD;&#xA;    ENSG00000201788.1	0.0&#xD;&#xA;    ENSG00000134108.11	925529.547944&#xD;&#xA;    ENSG00000263089.1	2646.63769677&#xD;&#xA;    ENSG00000172137.17	23162.6989867&#xD;&#xA;    ENSG00000167700.7	291192.25157&#xD;&#xA;&#xD;&#xA;1. What is the `.number` that are added to the Gene e.g. the `.2` in `ENSG00000242268.2`&#xD;&#xA;&#xD;&#xA;2. How the quantification isn't an integer, what does it mean `591.041000514`?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://portal.gdc.cancer.gov/files/92e73892-811b-4edd-b3db-d452bc5d28e0" />
  <row Id="1662" PostHistoryTypeId="1" PostId="559" RevisionGUID="5aed46c8-313c-4884-9176-139ddcc7260e" CreationDate="2017-06-08T02:25:46.173" UserId="734" Text="How to read and interpreter a gene expression quantification file?" />
  <row Id="1663" PostHistoryTypeId="3" PostId="559" RevisionGUID="5aed46c8-313c-4884-9176-139ddcc7260e" CreationDate="2017-06-08T02:25:46.173" UserId="734" Text="&lt;gene&gt;" />
  <row Id="1664" PostHistoryTypeId="5" PostId="559" RevisionGUID="22f89ab1-601a-46e9-bfc6-58e693c5d29a" CreationDate="2017-06-08T02:31:13.820" UserId="734" Comment="added 119 characters in body" Text="I have a gene expression quantification file from [TCGA][1] that contains the following lines:&#xD;&#xA;&#xD;&#xA;    ENSG00000242268.2	591.041000514&#xD;&#xA;    ENSG00000270112.3	0.0&#xD;&#xA;    ENSG00000167578.15	62780.6543066&#xD;&#xA;    ENSG00000273842.1	0.0&#xD;&#xA;    ENSG00000078237.5	36230.832883&#xD;&#xA;    ENSG00000146083.10	189653.152706&#xD;&#xA;    ENSG00000225275.4	0.0&#xD;&#xA;    ENSG00000158486.12	420.761140072&#xD;&#xA;    ENSG00000198242.12	2914738.3675&#xD;&#xA;    ENSG00000259883.1	1632.83700531&#xD;&#xA;    ENSG00000231981.3	0.0&#xD;&#xA;    ENSG00000269475.2	0.0&#xD;&#xA;    ENSG00000201788.1	0.0&#xD;&#xA;    ENSG00000134108.11	925529.547944&#xD;&#xA;    ENSG00000263089.1	2646.63769677&#xD;&#xA;    ENSG00000172137.17	23162.6989867&#xD;&#xA;    ENSG00000167700.7	291192.25157&#xD;&#xA;&#xD;&#xA;1. What is the `.number` that are added to the Gene e.g. the `.2` in `ENSG00000242268.2`&#xD;&#xA;&#xD;&#xA;2. How the quantification isn't an integer, what does it mean `591.041000514`?&#xD;&#xA;&#xD;&#xA;In case one wants more info [about the gene LINC02082][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://portal.gdc.cancer.gov/files/92e73892-811b-4edd-b3db-d452bc5d28e0&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/gene/?term=100507661" />
  <row Id="1665" PostHistoryTypeId="5" PostId="520" RevisionGUID="fa569811-5967-4e5b-b169-c1cd37afba48" CreationDate="2017-06-08T02:58:54.080" UserId="64" Comment="Capitalization issues" Text="The parameter is used to determine how much sequence STAR indexes on each side of a splice junction to improve its alignment accuracy.  For very long reads, this may not be ideal. I am not sure if STAR is capable of including multiple splice junctions since a long read is more than likely to span more than one.  &#xD;&#xA;&#xD;&#xA;It may be worthwhile to consider aligning your reads directly to transcripts and taking the reads that align poorly and using them to look for novel transcripts." />
  <row Id="1666" PostHistoryTypeId="2" PostId="560" RevisionGUID="0357485f-622b-4585-9f80-63393eb175de" CreationDate="2017-06-08T03:05:39.547" UserId="96" Text="1. The first column contains Ensembl gene identifiers, and the suffix is a version number that can be used to track changes to the gene annotations over time. From the [**Ensembl Stable IDs** documentation](http://www.ensembl.org/info/genome/stable_ids/index.html):&#xD;&#xA;  &gt; Ensembl annotation uses a system of stable IDs that have prefixes based on the species scientific name plus the feature type, followed by a series of digits and a version e.g. ENSG00000139618.1. The version may be omitted.&#xD;&#xA;&#xD;&#xA;2. Following the [first link](https://portal.gdc.cancer.gov/files/92e73892-811b-4edd-b3db-d452bc5d28e0) you provided leads to a page with details for the file `2edcaaa7-63b4-40b4-abbe-5d7a84012e60.FPKM-UQ.txt.gz`. The first thing that caught my eye about this filename was `FPKM`, or &quot;fragments per kilobase of exon per million reads&quot;, which is a commonly used unit of RNA expression. Since these are not raw read counts, there's no expectation that these values should be integers.&#xD;&#xA;&#xD;&#xA;  The best explanation I've seen of FPKM comes from [a blog post](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) written by Harold Pimentel of [kallisto](http://pachterlab.github.io/kallisto/) and [sleuth](https://pachterlab.github.io/sleuth/) fame. From the blog post:&#xD;&#xA;&#xD;&#xA;  &gt; The interpretation of FPKM is as follows: if you were to sequence this pool of RNA again, you expect to see `FPKM_i` fragments for each thousand bases in the feature for every `N/10^6` fragments you’ve sequenced. It’s basically just the rate of fragments per base multiplied by a big number (proportional to the number of fragments you sequenced) to make it more convenient." />
  <row Id="1667" PostHistoryTypeId="5" PostId="558" RevisionGUID="748f4faf-4d01-47b1-82ae-09c612ab221d" CreationDate="2017-06-08T03:39:52.487" UserId="96" Comment="clarify the language" Text="It's supposed that quantum computers could help to find the native state of all human DNA proteins.&#xD;&#xA;&#xD;&#xA;Does anyone know how to formulate the protein folding problem into a decision problem solvable either by [quantum annealers](https://en.wikipedia.org/wiki/Quantum_annealing) or [universal quantum computers](https://en.wikipedia.org/wiki/Quantum_Turing_machine)?&#xD;&#xA;&#xD;&#xA;It would also be interesting to know, how to model a decision problem for classical computers, like [Folding@Home](http://folding.stanford.edu/)." />
  <row Id="1668" PostHistoryTypeId="24" PostId="558" RevisionGUID="748f4faf-4d01-47b1-82ae-09c612ab221d" CreationDate="2017-06-08T03:39:52.487" Comment="Proposed by 96 approved by 732 edit id of 137" />
  <row Id="1669" PostHistoryTypeId="4" PostId="559" RevisionGUID="8d6d42f0-da4a-4009-ba56-b70b91981993" CreationDate="2017-06-08T03:56:32.030" UserId="96" Comment="Additional tags, fix title" Text="How to read and interpret a gene expression quantification file?" />
  <row Id="1670" PostHistoryTypeId="6" PostId="559" RevisionGUID="8d6d42f0-da4a-4009-ba56-b70b91981993" CreationDate="2017-06-08T03:56:32.030" UserId="96" Comment="Additional tags, fix title" Text="&lt;gene&gt;&lt;identifiers&gt;&lt;quantification&gt;" />
  <row Id="1671" PostHistoryTypeId="24" PostId="559" RevisionGUID="8d6d42f0-da4a-4009-ba56-b70b91981993" CreationDate="2017-06-08T03:56:32.030" Comment="Proposed by 96 approved by 734 edit id of 136" />
  <row Id="1672" PostHistoryTypeId="5" PostId="525" RevisionGUID="91eaba2b-ad4c-4f04-8d27-e91aa3b90452" CreationDate="2017-06-08T04:10:44.707" UserId="400" Comment="Clarify question so aim is to do the conversion quickly" Text="I have a pipeline for generating a BigWig file from a BAM file:&#xD;&#xA;&#xD;&#xA;    BAM -&gt; BedGraph -&gt; BigWig&#xD;&#xA;&#xD;&#xA;Which uses `bedtools genomecov` for the `BAM -&gt; BedGraph` part and `bedGraphToBigWig` for the `BedGraph -&gt; BigWig` part.&#xD;&#xA;&#xD;&#xA;The use of `bedGraphToBigWig` to create the `BigWig` file requires a BedGraph file to reside on disk in uncompressed form as it performs seeks. This is problematic for large genomes and variable coverage BAM files when there are more step changes/lines in the `BedGraph` file. My `BedGraph` files are in the order of 50 Gbytes in size and all that IO for 10-20 BAM files seems unnecessary.&#xD;&#xA;&#xD;&#xA;**Are there any tools capable generate `BigWig` without having to use an uncompressed `BedGraph` file on disk? I'd like this conversion to hapen as quickly as possible.**&#xD;&#xA;&#xD;&#xA;I have tried the following tools, but they still create/use a `BedGraph` intermediary file:&#xD;&#xA;&#xD;&#xA; - `deepTools`&#xD;&#xA;&#xD;&#xA;# Some Benchmarks, Ignoring IO&#xD;&#xA;&#xD;&#xA;Here are some timings I get for creating a `BigWig` file from a `BAM` file using 3 different pipelines. All files reside on a `tmpfs` i.e. in memory.&#xD;&#xA;&#xD;&#xA;## BEDTools and Kent Utils&#xD;&#xA;&#xD;&#xA;This is the approach taken by most.&#xD;&#xA;&#xD;&#xA;    time $(bedtools genomecov -bg -ibam test.bam -split -scale 1.0 &gt; test.bedgraph \&#xD;&#xA;      &amp;&amp; bedGraphToBigWig test.bedgraph test.fasta.chrom.sizes kent.bw \&#xD;&#xA;      &amp;&amp; rm test.bedgraph)&#xD;&#xA;    &#xD;&#xA;    real    1m20.015s&#xD;&#xA;    user    0m56.608s&#xD;&#xA;    sys     0m27.271s&#xD;&#xA;&#xD;&#xA;## SAMtools and Kent Utils&#xD;&#xA;&#xD;&#xA;Replacing `bedtools genomecov` with `samtools depth` and a custom `awk` script ([`depth2bedgraph.awk`][1]) to output `bedgraph` format has a significant performance improvement:&#xD;&#xA;&#xD;&#xA;    time $(samtools depth -Q 1 --reference test.fasta test.bam \&#xD;&#xA;      | mawk -f depth2bedgraph.awk \&#xD;&#xA;      &gt; test.bedgraph \&#xD;&#xA;      &amp;&amp; bedGraphToBigWig test.bedgraph test.fasta.chrom.sizes kent.bw \&#xD;&#xA;      &amp;&amp; rm test.bedgraph)&#xD;&#xA;    &#xD;&#xA;    real    0m28.765s&#xD;&#xA;    user    0m44.999s&#xD;&#xA;    sys     0m1.166s&#xD;&#xA;&#xD;&#xA;Although it is has less features, we used `mawk` here as it's faster than `gawk` (we don't need those extra features here).&#xD;&#xA;&#xD;&#xA;### Parallelising with xargs&#xD;&#xA;&#xD;&#xA;If you want to have a `BigWig` file per chromosome, you can easily parallelise this across chromosome/reference sequences. We can use `xargs` to run 5 parallel BAM-&gt;BedGraph-&gt;BigWig pipelines, each using the `tmpfs` mounted `/dev/shm` for the intermediary `BedGraph` files.&#xD;&#xA;&#xD;&#xA;    cut -f1 test.fasta.chrom.sizes \&#xD;&#xA;      | xargs -I{} -P 5  bash -c 'mkdir /dev/shm/${1} \&#xD;&#xA;      &amp;&amp; samtools depth -Q 1 --reference test.fasta -r &quot;${1}&quot; test.bam \&#xD;&#xA;      | mawk -f scripts/depth2bedgraph.awk \&#xD;&#xA;      &gt; &quot;/dev/shm/${1}/test.bam.bedgraph&quot; \&#xD;&#xA;      &amp;&amp; mkdir &quot;./${1}&quot; \&#xD;&#xA;      &amp;&amp; bedGraphToBigWig \&#xD;&#xA;        &quot;/dev/shm/${1}/test.bam.bedgraph&quot; \&#xD;&#xA;        test.fasta.chrom.sizes \&#xD;&#xA;        &quot;./${1}/test.bam.bw&quot; \&#xD;&#xA;      &amp;&amp; rm &quot;/dev/shm/${1}/test.bam.bedgraph&quot;' -- {}&#xD;&#xA;&#xD;&#xA;## deepTools&#xD;&#xA;&#xD;&#xA;Let's see how `deepTools` performs.&#xD;&#xA;&#xD;&#xA;    time bamCoverage --numberOfProcessors max \&#xD;&#xA;      --minMappingQuality 1 \&#xD;&#xA;      --bam test.bam --binSize 1 --skipNonCoveredRegions \&#xD;&#xA;      --outFileName deeptools.bw&#xD;&#xA;    &#xD;&#xA;    real    0m40.077s&#xD;&#xA;    user    3m56.032s&#xD;&#xA;    sys     0m9.276s&#xD;&#xA;&#xD;&#xA;  [1]: https://gist.github.com/nathanhaigh/c8322002a42e40c479d3d766aab32547" />
  <row Id="1673" PostHistoryTypeId="2" PostId="561" RevisionGUID="187640cb-9bbe-4120-ac56-ad94f76bb69c" CreationDate="2017-06-08T04:54:36.177" UserId="174" Text="I have some FASTQ sequence files and a FASTA file for some regions I'm interested in.&#xD;&#xA;&#xD;&#xA;I would like:&#xD;&#xA;&#xD;&#xA;  1. Build an index for the FASTA file&#xD;&#xA;  2. Use the index to count number of k-mers occurred in my sequence files&#xD;&#xA;&#xD;&#xA;I know how to do this in many k-mer counting tools, such as Kallisto, Jellyfish, DSK, Salmon. However, I'm more interested in Python (khmer is a Python package).&#xD;&#xA;&#xD;&#xA;Unfortunately, I can't figure out how to do that in khmer from the documentation:&#xD;&#xA;&#xD;&#xA;&gt; http://khmer.readthedocs.io/en/v2.1.1/introduction.html&#xD;&#xA;&#xD;&#xA;**Q:** How to build an index and count k-mers using khmer?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1674" PostHistoryTypeId="1" PostId="561" RevisionGUID="187640cb-9bbe-4120-ac56-ad94f76bb69c" CreationDate="2017-06-08T04:54:36.177" UserId="174" Text="How to use khmer to count k-mers?" />
  <row Id="1675" PostHistoryTypeId="3" PostId="561" RevisionGUID="187640cb-9bbe-4120-ac56-ad94f76bb69c" CreationDate="2017-06-08T04:54:36.177" UserId="174" Text="&lt;genome&gt;&lt;k-mer&gt;&lt;metagenome&gt;" />
  <row Id="1676" PostHistoryTypeId="5" PostId="561" RevisionGUID="4985d8b9-2e27-472e-bfcd-a5dc5fe89f14" CreationDate="2017-06-08T05:01:42.680" UserId="174" Comment="added 168 characters in body" Text="I have some FASTQ sequence files and a FASTA file for some regions I'm interested in.&#xD;&#xA;&#xD;&#xA;I would like:&#xD;&#xA;&#xD;&#xA;  1. Build an index for the FASTA file&#xD;&#xA;  2. Use the index to count number of k-mers occurred in my sequence files&#xD;&#xA;&#xD;&#xA;I know how to do this in many k-mer counting tools, such as Kallisto, Jellyfish, DSK, Salmon. However, I'm more interested in Python (khmer is a Python package).&#xD;&#xA;&#xD;&#xA;Unfortunately, I can't figure out how to do that in khmer from the documentation:&#xD;&#xA;&#xD;&#xA;&gt; http://khmer.readthedocs.io/en/v2.1.1/introduction.html&#xD;&#xA;&#xD;&#xA;It talks about RNA-Seq, metagenomics, de Bruijn graph etc... All I need are just counts. Khmer is a popular k-mer analysis tool, so it should be able to do it, but how?&#xD;&#xA;&#xD;&#xA;**Q:** How to build an index and count k-mers using khmer?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1677" PostHistoryTypeId="2" PostId="562" RevisionGUID="437e3599-3fb8-450b-9abd-d3bc3f7fc48e" CreationDate="2017-06-08T05:03:42.780" UserId="96" Text="If you want to count the number of unique k-mers that occur in your data set, you should use the [unique-kmers.py](http://khmer.readthedocs.io/en/v2.1.1/user/scripts.html#unique-kmers-py) script, which implements a HyperLogLog-based cardinality estimator.&#xD;&#xA;&#xD;&#xA;If the number of unique k-mers is not what you're after, please clarify." />
  <row Id="1678" PostHistoryTypeId="5" PostId="561" RevisionGUID="3b3896ab-4c1d-4d89-9b05-e0037068e216" CreationDate="2017-06-08T05:06:09.087" UserId="174" Comment="added 224 characters in body" Text="I have some FASTQ sequence files and a FASTA file for some regions I'm interested in.&#xD;&#xA;&#xD;&#xA;I would like:&#xD;&#xA;&#xD;&#xA;  1. Build an index for the FASTA file&#xD;&#xA;  2. Use the index to count number of k-mers occurred in my sequence files&#xD;&#xA;&#xD;&#xA;I know how to do this in many k-mer counting tools, such as Kallisto, Jellyfish, DSK, Salmon. However, I'm more interested in Python (khmer is a Python package).&#xD;&#xA;&#xD;&#xA;Unfortunately, I can't figure out how to do that in khmer from the documentation:&#xD;&#xA;&#xD;&#xA;&gt; http://khmer.readthedocs.io/en/v2.1.1/introduction.html&#xD;&#xA;&#xD;&#xA;It talks about RNA-Seq, metagenomics, de Bruijn graph etc... All I need are just counts. Khmer is a popular k-mer tool, so it should be able to do it, but how?&#xD;&#xA;&#xD;&#xA;**Q:** How to build an index and count k-mers using khmer?&#xD;&#xA;&#xD;&#xA;**EDIT:**&#xD;&#xA;&#xD;&#xA;I don't need number of unique k-mers, I'm looking for a solution like Jellyfish where we build an index and count k-mers to the index.&#xD;&#xA;&#xD;&#xA;Something like:&#xD;&#xA;&#xD;&#xA;    MY_SEQ_1  100 counts&#xD;&#xA;    MY_SEQ_2  134 counts&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1679" PostHistoryTypeId="5" PostId="561" RevisionGUID="69a534d9-a9f0-4758-8a7f-e9cc8a845c91" CreationDate="2017-06-08T05:12:12.183" UserId="174" Comment="added 44 characters in body" Text="I have some FASTQ sequence files and a FASTA file for some regions I'm interested in.&#xD;&#xA;&#xD;&#xA;I would like:&#xD;&#xA;&#xD;&#xA;  1. Build an index for the FASTA file&#xD;&#xA;  2. Use the index to count number of k-mers occurred in my sequence files&#xD;&#xA;&#xD;&#xA;I know how to do this in many k-mer counting tools, such as Kallisto, Jellyfish, DSK, Salmon. However, I'm more interested in Python (khmer is a Python package, all the other k-mer counting tools are C++).&#xD;&#xA;&#xD;&#xA;Unfortunately, I can't figure out how to do that in khmer from the documentation:&#xD;&#xA;&#xD;&#xA;&gt; http://khmer.readthedocs.io/en/v2.1.1/introduction.html&#xD;&#xA;&#xD;&#xA;It talks about RNA-Seq, metagenomics, de Bruijn graph etc... All I need are just counts. Khmer is a popular k-mer tool, so it should be able to do it, but how?&#xD;&#xA;&#xD;&#xA;**Q:** How to build an index and count k-mers using khmer?&#xD;&#xA;&#xD;&#xA;**EDIT:**&#xD;&#xA;&#xD;&#xA;I don't need number of unique k-mers, I'm looking for a solution like Jellyfish where we build an index and count k-mers to the index.&#xD;&#xA;&#xD;&#xA;Something like:&#xD;&#xA;&#xD;&#xA;    MY_SEQ_1  100 counts&#xD;&#xA;    MY_SEQ_2  134 counts&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1680" PostHistoryTypeId="6" PostId="438" RevisionGUID="cc63b613-6ef6-4a33-91db-24cefda4552a" CreationDate="2017-06-08T05:32:58.860" UserId="73" Comment="standardise ONT tag" Text="&lt;nanopore&gt;&lt;structural-variation&gt;&lt;long-reads&gt;&lt;cancer&gt;" />
  <row Id="1681" PostHistoryTypeId="6" PostId="296" RevisionGUID="4db4501f-9ff3-4bb8-872f-7be354b864be" CreationDate="2017-06-08T05:33:21.487" UserId="73" Comment="[Edit removed during grace period]" Text="&lt;nanopore&gt;&lt;fastq&gt;&lt;minion&gt;&lt;yield&gt;" />
  <row Id="1682" PostHistoryTypeId="6" PostId="294" RevisionGUID="ea0c0252-85f1-4e29-9ae3-236487bdae7c" CreationDate="2017-06-08T05:33:45.810" UserId="73" Comment="standardise ONT tag" Text="&lt;nanopore&gt;&lt;sequencing&gt;" />
  <row Id="1683" PostHistoryTypeId="6" PostId="452" RevisionGUID="92afa07f-8ca1-4335-99c7-e32b8a4acf12" CreationDate="2017-06-08T05:34:30.047" UserId="73" Comment="standardise ONT tag" Text="&lt;nanopore&gt;&lt;benchmarking&gt;&lt;consensus&gt;" />
  <row Id="1684" PostHistoryTypeId="5" PostId="559" RevisionGUID="b753529b-7ec6-4570-94c6-aa845527793c" CreationDate="2017-06-08T05:40:39.443" UserId="73" Comment="Correct grammar / spelling" Text="I have a gene expression quantification file from [TCGA][1] that contains the following lines:&#xD;&#xA;&#xD;&#xA;    ENSG00000242268.2	591.041000514&#xD;&#xA;    ENSG00000270112.3	0.0&#xD;&#xA;    ENSG00000167578.15	62780.6543066&#xD;&#xA;    ENSG00000273842.1	0.0&#xD;&#xA;    ENSG00000078237.5	36230.832883&#xD;&#xA;    ENSG00000146083.10	189653.152706&#xD;&#xA;    ENSG00000225275.4	0.0&#xD;&#xA;    ENSG00000158486.12	420.761140072&#xD;&#xA;    ENSG00000198242.12	2914738.3675&#xD;&#xA;    ENSG00000259883.1	1632.83700531&#xD;&#xA;    ENSG00000231981.3	0.0&#xD;&#xA;    ENSG00000269475.2	0.0&#xD;&#xA;    ENSG00000201788.1	0.0&#xD;&#xA;    ENSG00000134108.11	925529.547944&#xD;&#xA;    ENSG00000263089.1	2646.63769677&#xD;&#xA;    ENSG00000172137.17	23162.6989867&#xD;&#xA;    ENSG00000167700.7	291192.25157&#xD;&#xA;&#xD;&#xA;1. What is the `.number` that are added to the Gene e.g. the `.2` in `ENSG00000242268.2`&#xD;&#xA;&#xD;&#xA;2. Why isn't the quantified value an integer; what does `591.041000514` mean?&#xD;&#xA;&#xD;&#xA;In case someone one wants more info [about the gene LINC02082][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://portal.gdc.cancer.gov/files/92e73892-811b-4edd-b3db-d452bc5d28e0&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/gene/?term=100507661" />
  <row Id="1685" PostHistoryTypeId="2" PostId="563" RevisionGUID="79adfefb-0ab8-42cd-940a-76fd4fc5ced4" CreationDate="2017-06-08T05:48:07.953" UserId="161" Text="I mostly use TargetScan predictions. But here are few curated databases:&#xD;&#xA;&#xD;&#xA;- [miRGate](http://mirgate.bioinfo.cnio.es/miRGate/) &#xD;&#xA;&gt; miRGate is a curated database of human, mouse and rat miRNAs/mRNAs targets. It is designed to analyze miRNA and gene isoforms lists under a common and consistent space of annotations. Including all existing 3 UTR and the entirely known miRNAs. All Havana biotypes and ENCODE principal isoforms for the three organisms are also included. [Publication](https://www.ncbi.nlm.nih.gov/pubmed/25858286)&#xD;&#xA;&#xD;&#xA;- [miR2Disease](http://www.mir2disease.org/) : &#xD;&#xA;&gt;  a manually curated database, aims at providing a comprehensive resource of miRNA deregulation in various human diseases. [Publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2686559/)&#xD;&#xA;&#xD;&#xA;- [PASmiR](http://pcsb.ahau.edu.cn:8080/PASmiR/f)&#xD;&#xA;&gt; PASmiR, a literature-curated and web-accessible database, was thus developed to provide a solid platform for the collection, standardization, and searching of these miRNA-stress regulation data in plants. [Publication](https://bmcplantbiol.biomedcentral.com/articles/10.1186/1471-2229-13-33)" />
  <row Id="1686" PostHistoryTypeId="6" PostId="318" RevisionGUID="f1167370-e054-4798-b98e-0677c5462d5b" CreationDate="2017-06-08T05:55:17.747" UserId="96" Comment="Tag editing" Text="&lt;human-genome&gt;&lt;array-cgh&gt;&lt;liftover&gt;&lt;hg18&gt;" />
  <row Id="1687" PostHistoryTypeId="24" PostId="318" RevisionGUID="f1167370-e054-4798-b98e-0677c5462d5b" CreationDate="2017-06-08T05:55:17.747" Comment="Proposed by 96 approved by 73, 131 edit id of 139" />
  <row Id="1688" PostHistoryTypeId="5" PostId="562" RevisionGUID="9b94e4da-7a38-42fa-812b-7e82552ca0a7" CreationDate="2017-06-08T06:04:19.577" UserId="96" Comment="responding to OP's update" Text="If you want to count the number of unique k-mers that occur in your data set, you should use the [unique-kmers.py](http://khmer.readthedocs.io/en/v2.1.1/user/scripts.html#unique-kmers-py) script, which implements a HyperLogLog-based cardinality estimator.&#xD;&#xA;&#xD;&#xA;If the number of unique k-mers is not what you're after, please clarify.&#xD;&#xA;&#xD;&#xA;**EDIT**:&#xD;&#xA;&#xD;&#xA;khmer uses probabilistic data structures internally, which store k-mers as hashed values that cannot be un-hashed back into k-mer sequences uniquely. Querying a k-mer's abundance requires you to know which k-mer(s) you're looking for, which in this case would require a second pass over the reads.&#xD;&#xA;&#xD;&#xA;The closest thing to what you're asking provided in khmer's command-line scripts is [abundance-dist.py](http://khmer.readthedocs.io/en/v2.1.1/user/scripts.html#abundance-dist-py) (or the alternative abundance-dist-single.py), which will produce a k-mer abundance histogram but not per-kmer abundance.&#xD;&#xA;&#xD;&#xA;The [load-into-counting.py](http://khmer.readthedocs.io/en/v2.1.1/user/scripts.html#load-into-counting-py) scripts computes the k-mer abundances and stores them in a probabilistic data structure, which is written to disk and can subsequently be re-loaded into memory quickly. Many of the scripts in khmer expect the reads to have been pre-processed by `load-into-counting.py`, while others will invoke the counting routines directly themselve. It's also fairly easy to do this with the Python API. In every case, the accuracy of the k-mer counts is dependant on [memory considerations](http://khmer.readthedocs.io/en/v2.1.1/user/choosing-table-sizes.html).&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; import khmer&#xD;&#xA;    &gt;&gt;&gt; counts = khmer.Counttable(31, 1e7, 4)&#xD;&#xA;    &gt;&gt;&gt; counts.consume_seqfile('reads.fq.gz')&#xD;&#xA;    (25000, 1150000)&#xD;&#xA;    &gt;&gt;&gt; counts.get('TGACTTTCTTCGCTTCCTGACGGCTTATGCC')&#xD;&#xA;    117&#xD;&#xA;&#xD;&#xA;To check the false positive rate:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; fpr = khmer.calc_expected_collisions(counts)&#xD;&#xA;    &gt;&gt;&gt; fpr&#xD;&#xA;&#xD;&#xA;If you *must* have the counts of each k-mer, it's not much more work...although reporting each k-mer's abundance only once would consume a lot of memory with a naive approach.&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; outfile = open('outfile.txt', 'w')&#xD;&#xA;    &gt;&gt;&gt; seenkmers = set()  # Consumes a lot of memory for large input!!!&#xD;&#xA;    &gt;&gt;&gt; for read in khmer.ReadParser('reads.fq.gz):&#xD;&#xA;    ...     for kmer in counts.get_kmers(read.sequence):&#xD;&#xA;    ...         if kmer not in seenkmers:&#xD;&#xA;    ...             print(kmer, counttable.get(kmer), file=outfile)&#xD;&#xA;    ...             seenkmers.add(kmer)" />
  <row Id="1690" PostHistoryTypeId="2" PostId="564" RevisionGUID="6488ac83-87ef-4cc7-a59b-acf1d11d5485" CreationDate="2017-06-08T06:38:40.213" UserId="383" Text="You could use liftOver which isn't always great.&#xD;&#xA;&#xD;&#xA;Whenever I encounter this (especially NGS data readily available on the SRA), I often just get the raw files (e.g. fastqs) and re-align/re-map.&#xD;&#xA;&#xD;&#xA;In your case (arrays) it may be a bit tough. Not impossible though, as I recently took some old yeast DNA/RNA microarray data and updated it to the newest genome. Just requires the right data (like DNA for normalization) and a good understanding of the entire process.&#xD;&#xA;&#xD;&#xA;A last resort/alternative is to align your **new** data to the old genome to be able to make comparisons. This isn't ideal but works in cases where upgrading one source isn't possible or is a HUGE amount of time/effort. I've done this for a few fly experiments where all the available/previous data was done in dm3. All the old genomes can generally be found on http://archive.ensembl.org ." />
  <row Id="1691" PostHistoryTypeId="6" PostId="553" RevisionGUID="5b69dfb9-b85c-4156-990d-19a5313c1e3d" CreationDate="2017-06-08T06:41:54.973" UserId="96" Comment="homology modeling has to do with (usually protein) structure" Text="&lt;fasta&gt;&lt;sequence-homology&gt;" />
  <row Id="1692" PostHistoryTypeId="24" PostId="553" RevisionGUID="5b69dfb9-b85c-4156-990d-19a5313c1e3d" CreationDate="2017-06-08T06:41:54.973" Comment="Proposed by 96 approved by 73, 77 edit id of 140" />
  <row Id="1693" PostHistoryTypeId="6" PostId="388" RevisionGUID="03558dd6-8372-4337-8753-af1b02b87f1c" CreationDate="2017-06-08T06:42:47.810" UserId="96" Comment="tag normalization" Text="&lt;fasta&gt;&lt;database&gt;&lt;file-formats&gt;" />
  <row Id="1694" PostHistoryTypeId="24" PostId="388" RevisionGUID="03558dd6-8372-4337-8753-af1b02b87f1c" CreationDate="2017-06-08T06:42:47.810" Comment="Proposed by 96 approved by 77, 104 edit id of 142" />
  <row Id="1695" PostHistoryTypeId="5" PostId="560" RevisionGUID="52e3a2d5-ad11-44af-bd4b-e196cd3b4bad" CreationDate="2017-06-08T06:45:32.780" UserId="96" Comment="add final caveat" Text="1. The first column contains Ensembl gene identifiers, and the suffix is a version number that can be used to track changes to the gene annotations over time. From the [**Ensembl Stable IDs** documentation](http://www.ensembl.org/info/genome/stable_ids/index.html):&#xD;&#xA;  &gt; Ensembl annotation uses a system of stable IDs that have prefixes based on the species scientific name plus the feature type, followed by a series of digits and a version e.g. ENSG00000139618.1. The version may be omitted.&#xD;&#xA;&#xD;&#xA;2. Following the [first link](https://portal.gdc.cancer.gov/files/92e73892-811b-4edd-b3db-d452bc5d28e0) you provided leads to a page with details for the file `2edcaaa7-63b4-40b4-abbe-5d7a84012e60.FPKM-UQ.txt.gz`. The first thing that caught my eye about this filename was `FPKM`, or &quot;fragments per kilobase of exon per million reads&quot;, which is a commonly used unit of RNA expression. Since these are not raw read counts, there's no expectation that these values should be integers.&#xD;&#xA;&#xD;&#xA;  The best explanation I've seen of FPKM comes from [a blog post](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) written by Harold Pimentel of [kallisto](http://pachterlab.github.io/kallisto/) and [sleuth](https://pachterlab.github.io/sleuth/) fame. From the blog post:&#xD;&#xA;&#xD;&#xA;  &gt; The interpretation of FPKM is as follows: if you were to sequence this pool of RNA again, you expect to see `FPKM_i` fragments for each thousand bases in the feature for every `N/10^6` fragments you’ve sequenced. It’s basically just the rate of fragments per base multiplied by a big number (proportional to the number of fragments you sequenced) to make it more convenient.&#xD;&#xA;&#xD;&#xA;-------&#xD;&#xA;&#xD;&#xA;More generally though, even when FPKM is not the unit of expression abundance used, most quantification methods and associated units of expression will not produce integer estimates." />
  <row Id="1696" PostHistoryTypeId="5" PostId="552" RevisionGUID="bcdcfca2-f554-4553-9c0b-1e180cbc52c9" CreationDate="2017-06-08T06:47:19.023" UserId="77" Comment="Expand on what PaVE is, to hopefully give this answer a bit more meat." Text="In addition to what others have suggested, I would also recommend [PaVE](https://pave.niaid.nih.gov/) as a resource. This is a curated database maintained by the NIAID and current holds over 300 papilloma virus genomes." />
  <row Id="1697" PostHistoryTypeId="5" PostId="537" RevisionGUID="498ea48d-8743-43c7-b81d-ac0a5bd455cf" CreationDate="2017-06-08T06:56:00.910" UserId="182" Comment="Added code example" Text="The [QuasR](http://bioconductor.org/packages/release/bioc/html/QuasR.html) package in R has a useful function called `qExportWig`. &#xD;&#xA;&#xD;&#xA;To use it you first need to create a sample sheet and a `qProject` object - this can be done starting with bam files or fastq files (in which case it also  performs alignments). You can then create wig or bigWig files without writing a bedGraph file to disk.&#xD;&#xA;&#xD;&#xA;An example would look something like this:&#xD;&#xA;&#xD;&#xA;    library(QuasR)&#xD;&#xA;    &#xD;&#xA;    # Create qProject object using qAlign&#xD;&#xA;    # Sample file should contain tab-separated file names (BAM or fastq) and sample names&#xD;&#xA;    myproj &lt;- qAlign(&quot;samples.txt&quot;, genome=mygenome, paired=&quot;no&quot;)&#xD;&#xA;&#xD;&#xA;    # Export all samples in project as bigWig, scaling to mean number of alignments by default&#xD;&#xA;    qExportWig(myproj, binsize=50, createBigWig=T)&#xD;&#xA;&#xD;&#xA;It has a lot of flexible options, including scaling or log-transforming counts, shifting reads and changing the bin size." />
  <row Id="1698" PostHistoryTypeId="6" PostId="91" RevisionGUID="3a1fe2ec-ffb2-434e-807a-581c4be5289b" CreationDate="2017-06-08T07:04:56.403" UserId="96" Comment="tag normalization" Text="&lt;fasta&gt;&lt;bed&gt;&lt;file-formats&gt;&lt;format-conversion&gt;" />
  <row Id="1699" PostHistoryTypeId="24" PostId="91" RevisionGUID="3a1fe2ec-ffb2-434e-807a-581c4be5289b" CreationDate="2017-06-08T07:04:56.403" Comment="Proposed by 96 approved by 77, 174 edit id of 144" />
  <row Id="1700" PostHistoryTypeId="2" PostId="565" RevisionGUID="a0a3ff0e-84ea-442b-8c14-89ace32f1f5e" CreationDate="2017-06-08T07:06:29.710" UserId="96" Text="Bashing file formats is a favorite pastime in bioinformatics, and annotation file formats such as GFF and BED seem to get special attention. A lot of this frustration stems from community's shockingly inconsistent adherence to specifications and conventions, but there are also some (dare I say objectively) problematic design choices in each of these formats.&#xD;&#xA;&#xD;&#xA;- GFF (and its more common derivatives GTF and GFF3) use 1-based closed interval notation, which optimizes for human comprehension but is far inferior to 0-based half-open interval notation (such as used by BED) for computations involving interval arithmetic.&#xD;&#xA;&#xD;&#xA;- Although BED and GTF were designed for very specific use cases (visualization and gene prediction, respectively), they have been used and abused in a much wider set of contexts. For example, the BED fields related to the **thick part** are irrelevant if you're not plotting them in a genome browser.&#xD;&#xA;&#xD;&#xA;- BED supports a single level of feature decomposition (a feature can be broken up into blocks). GTF supports two levels (exons grouped by transcript_id, transcripts grouped by gene_id). In contrast, GFF3 supports an arbitrary number of levels, and uses parent/child relationships defined by `ID` and `Parent` attributes to declare a directed acyclic graph of features.&#xD;&#xA;&#xD;&#xA;- Data that does not fit into mandatory pre-defined fields must be relegated to optional fields or free-form attribute key/value pairs. While this flexibility is powerful, a common complaint is that &quot;all the action&quot; happens in these optional/free-form fields.&#xD;&#xA;&#xD;&#xA;- There is a dearth of validation tools, and those that do exist focus primarily on validating syntax and not semantics. To use an aging analogy, it's one thing to say an XML file is valid, but it's completely different to validate it against a schema. There are essentially no widely used tools that do the latter for annotation files.&#xD;&#xA;&#xD;&#xA;If we were tasked with creating a new annotation format, and if we were guaranteed the resources needed to develop it, and interest and wide adoption from the wider community (one can dream!), what design criteria should be considered in the development of this new format? **What, if anything, makes an objectively good annotation data format?**" />
  <row Id="1701" PostHistoryTypeId="1" PostId="565" RevisionGUID="a0a3ff0e-84ea-442b-8c14-89ace32f1f5e" CreationDate="2017-06-08T07:06:29.710" UserId="96" Text="Annotation format design" />
  <row Id="1702" PostHistoryTypeId="3" PostId="565" RevisionGUID="a0a3ff0e-84ea-442b-8c14-89ace32f1f5e" CreationDate="2017-06-08T07:06:29.710" UserId="96" Text="&lt;annotation&gt;&lt;bed&gt;&lt;file-formats&gt;&lt;gff3&gt;" />
  <row Id="1703" PostHistoryTypeId="2" PostId="566" RevisionGUID="59e92217-57f9-48c9-a515-8a07f7ad04bf" CreationDate="2017-06-08T07:18:13.830" UserId="77" Text="Presuming we consider &quot;human readable&quot;, &quot;easily parsable&quot;, and &quot;quickly queryable&quot; to be objectively good qualities (and if not, I worry for the future):&#xD;&#xA;&#xD;&#xA;1. Text-based: It's absurdly common to want to use `grep` or `awk` on annotations. Sure, one could make variants of these that are binary-format aware, but why reinvent the wheel. Of course text files don't innately allow region-based queries of their contents, so on to point 2...&#xD;&#xA;2. A strictly defined line order: This is one of my personal pet-peeves about BED/GFF/GTF files. If you're going to make a text-based annotation format, everyone would end up ahead if said format made explicit that it should be sorted. This then allows things like tabix to be used so the &quot;query a region&quot; problem is then solved. But I would go further than that. The issue with things like GFF is that there are multiple inter-dependent lines and there's no strict rule about whether a parent line absolutely must come before a child line. This just makes implementing things a nightmare and more often than not a randomly ordered file will just break tools. Since GTF or GFF-style interdependent lines likely be how any annotation format works, this ordering between lines with the same start position should be made explicit in the format." />
  <row Id="1704" PostHistoryTypeId="2" PostId="567" RevisionGUID="3cda2e0c-081d-4831-a935-573301deed43" CreationDate="2017-06-08T07:21:27.027" UserId="734" Text="Assume I have found the top 0.01% most frequent genes from a [gene expression file][1]. &#xD;&#xA;&#xD;&#xA;Let's say, these are 10 genes and I want to study the [protein protein interactions][2], the protein network and pathway.&#xD;&#xA; &#xD;&#xA;I thought to use [string-db][3] and [interactome][4], but I am not sure what would be a plausible way to approach this problem and to build the protein network etc. Are there more suitable databases?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/559/how-to-read-and-interpret-a-gene-expression-quantification-file&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Protein%E2%80%93protein_interaction&#xD;&#xA;  [3]: https://string-db.org&#xD;&#xA;  [4]: http://interactome.dfci.harvard.edu" />
  <row Id="1705" PostHistoryTypeId="1" PostId="567" RevisionGUID="3cda2e0c-081d-4831-a935-573301deed43" CreationDate="2017-06-08T07:21:27.027" UserId="734" Text="How to build protein network pathway from gene expression quantification file?" />
  <row Id="1706" PostHistoryTypeId="3" PostId="567" RevisionGUID="3cda2e0c-081d-4831-a935-573301deed43" CreationDate="2017-06-08T07:21:27.027" UserId="734" Text="&lt;proteins&gt;&lt;gene&gt;" />
  <row Id="1707" PostHistoryTypeId="2" PostId="568" RevisionGUID="38f32a85-b8d7-44c3-8407-1ca96694ef74" CreationDate="2017-06-08T07:57:21.707" UserId="311" Text="This question is based on the following question on BioStars https://www.biostars.org/p/129057/ posted &gt;2 years ago by User jack.&#xD;&#xA;&#xD;&#xA;It describes a very frequent problem of generating GO annotations for non-model organisms. While it is based on some specific format and single application (Ontologizer), it would be useful to have a general description of the pathway to getting to a GAF file. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I want to do the Gene enrichment using Ontologizer without a predefined association file(it's not model organism). &#xD;&#xA;&#xD;&#xA;I have parsed a file with two columns for that organism like this : &#xD;&#xA;&#xD;&#xA;    geneA  GO:0006950,GO:0005737&#xD;&#xA;    geneB  GO:0016020,GO:0005524,GO:0006468,GO:0005737,GO:0004674,GO:0006914,GO:0016021,GO:0015031&#xD;&#xA;    geneC  GO:0003779,GO:0006941,GO:0005524,GO:0003774,GO:0005516,GO:0005737,GO:0005863&#xD;&#xA;    geneD  GO:0005634,GO:0003677,GO:0030154,GO:0006350,GO:0006355,GO:0007275,GO:0030528&#xD;&#xA;     &#xD;&#xA;&#xD;&#xA;also I have downloaded the .ob file from Gene ontology file which contain this information (http://www.geneontology.org/doc/GO.terms_and_ids) : &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    !&#xD;&#xA;    ! GO IDs (primary only) and name text strings&#xD;&#xA;    ! GO:0000000 [tab] text string [tab] F|P|C&#xD;&#xA;    ! where F = molecular function, P = biological process, C = cellular component&#xD;&#xA;    !&#xD;&#xA;    GO:0000001  mitochondrion inheritance   P&#xD;&#xA;    GO:0000002  mitochondrial genome maintenance    P&#xD;&#xA;    GO:0000003  reproduction    P&#xD;&#xA;    GO:0000005  ribosomal chaperone activity    F&#xD;&#xA;    GO:0000006  high affinity zinc uptake transmembrane transporter activity    F&#xD;&#xA;    GO:0000007  low-affinity zinc ion transmembrane transporter activity    F&#xD;&#xA;    GO:0000008  thioredoxin F&#xD;&#xA;    GO:0000009  alpha-1,6-mannosyltransferase activity  F&#xD;&#xA;    GO:0000010  trans-hexaprenyltranstransferase activity   F&#xD;&#xA;    GO:0000011  vacuole inheritance P&#xD;&#xA;     &#xD;&#xA;&#xD;&#xA;what I need as output is .gaf file in the following format (in the format of the files at here: http://geneontology.org/page/download-annotations):&#xD;&#xA;&#xD;&#xA;    !gaf-version: 2.0&#xD;&#xA;    &#xD;&#xA;    !Project_name: Leishmania major GeneDB&#xD;&#xA;    &#xD;&#xA;    !URL: http://www.genedb.org/leish&#xD;&#xA;    &#xD;&#xA;    !Contact Email: mb4@sanger.ac.uk&#xD;&#xA;    &#xD;&#xA;    GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0003723    PMID:22396527    ISO    GeneDB:Tb927.10.10130    F    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;    &#xD;&#xA;    GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0044429    PMID:20660476    ISS        C    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20100803    GeneDB_Lmajor       &#xD;&#xA;    &#xD;&#xA;    GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0016554    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;    &#xD;&#xA;    GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0048255    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor &#xD;&#xA;&#xD;&#xA;" />
  <row Id="1708" PostHistoryTypeId="1" PostId="568" RevisionGUID="38f32a85-b8d7-44c3-8407-1ca96694ef74" CreationDate="2017-06-08T07:57:21.707" UserId="311" Text="How to create your own GO association file (gaf) for gene ontology enrichment analysis?" />
  <row Id="1709" PostHistoryTypeId="3" PostId="568" RevisionGUID="38f32a85-b8d7-44c3-8407-1ca96694ef74" CreationDate="2017-06-08T07:57:21.707" UserId="311" Text="&lt;public-databases&gt;" />
  <row Id="1710" PostHistoryTypeId="6" PostId="568" RevisionGUID="782e1067-1c05-4fdc-9675-7809219a27b8" CreationDate="2017-06-08T08:07:13.163" UserId="77" Comment="Add a couple tags" Text="&lt;public-databases&gt;&lt;gene-ontology&gt;&lt;gaf&gt;" />
  <row Id="1711" PostHistoryTypeId="5" PostId="305" RevisionGUID="f39ff402-2c81-4ed3-8d42-cd505cc1184a" CreationDate="2017-06-08T08:20:19.730" UserId="191" Comment="fix code" Text="I think the easiest way is to download the graph using `STRINGdb`.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;    full.graph &lt;- string_db$get_graph()&#xD;&#xA;&#xD;&#xA;Now you can use [`igraph`][1], to manipulate the graph. Let's assume you want to take 200 proteins with the highest degree, i.e. number of edges they have.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    library(igraph)&#xD;&#xA;    &#xD;&#xA;    # see how many proteins do you have    &#xD;&#xA;    vcount(full.graph)&#xD;&#xA;    &#xD;&#xA;    # find top 200 proteins with the highest degree&#xD;&#xA;    top.degree.verticies &lt;- names(tail(sort(degree(full.graph)), 200))&#xD;&#xA;&#xD;&#xA;    # extract the relevant subgraph&#xD;&#xA;    top.subgraph &lt;- induced_subgraph(full.graph, top.degree.verticies)&#xD;&#xA;    &#xD;&#xA;    # count the number of proteins in it&#xD;&#xA;    vcount(top.subgraph)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**How to get disease specific genes?**&#xD;&#xA;&#xD;&#xA;There's no GO annotation for cancer or Alzheimer's disease. It is out of scope of the GO consortium.&#xD;&#xA;&#xD;&#xA;What you can do, you can either take KEGG Pathways annotation, or manually select list of relevant GO-terms. Or acquire the list from one of the papers. For example annotation term `05200` corresponds to the cancer KEGG pathway. You can easily retrieve proteins associated with the annotation:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    cancer.pathway.proteins &lt;-&#xD;&#xA;        string_db$get_term_proteins('05200')$STRING_id&#xD;&#xA;&#xD;&#xA;And then perform subgraphing as described above.&#xD;&#xA;&#xD;&#xA;Alternatively you can try to get an enrichment score for an every gene given it's neighbors (the way enrichment is shown on the string-db website). Then you can keep only those having top enrichment scores. Probably `get_ppi_enrichment_full` or `get_ppi_enrichment` functions will help you to do that.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://igraph.org/r/&#xD;&#xA;" />
  <row Id="1712" PostHistoryTypeId="2" PostId="569" RevisionGUID="e680aaf7-14b1-4761-8f49-5e1ce06ee33d" CreationDate="2017-06-08T08:24:57.997" UserId="191" Text="There are multiple ways to do this, and multiple protein interaction databases besides the ones you mentioned, such as [BioGRID][1] or [IntAct][2]. Interaction databases are different in how interactions are defined, sometimes it can be experimental evidence of interaction, sometimes coexpression, orthology-based predictions, etc. &#xD;&#xA;&#xD;&#xA;There is no single solution to your problem. For String-DB you can use their R package [STRINGdb][3].&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    # string-db&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    # library for working with graphs&#xD;&#xA;    library(ighaph)&#xD;&#xA;    &#xD;&#xA;    # retrieve full graph for H. sapiens&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;    full.graph &lt;- string_db$get_graph()&#xD;&#xA;    &#xD;&#xA;    # see how your graph looks like (i.e. how many genes and interactions)&#xD;&#xA;    full.graph&#xD;&#xA;    &#xD;&#xA;    # define genes of interest&#xD;&#xA;    genes.of.interest &lt;- ...&#xD;&#xA;    &#xD;&#xA;    # get their neighbors&#xD;&#xA;    neighbors &lt;- string_db$get_neighbors(genes.of.interest)&#xD;&#xA;    &#xD;&#xA;    # get the subgraph of interest (your genes + their neighbors)&#xD;&#xA;    my.subgraph &lt;- induced_subgraph(full.graph, c(genes.of.interest, neighbors))&#xD;&#xA;&#xD;&#xA;    # look how many genes and interactions you have&#xD;&#xA;    my.subgraph&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://thebiogrid.org/&#xD;&#xA;  [2]: http://www.ebi.ac.uk/intact/&#xD;&#xA;  [3]: http://bioconductor.org/packages/release/bioc/html/STRINGdb.html" />
  <row Id="1713" PostHistoryTypeId="5" PostId="528" RevisionGUID="4f7f15b4-9cba-4a2a-b2d3-6a72dbe3eb05" CreationDate="2017-06-08T08:28:25.240" UserId="682" Comment="added 441 characters in body" Text="I'm trying to find a programmatic way to automatically extract the following information from a [PDB file](http://www.wwpdb.org/documentation/file-format):&#xD;&#xA;&#xD;&#xA;1. RNA sequence &#xD;&#xA;2. Secondary structure restraints in bracket format, e.g. `. (( . ( . ) . )) `&#xD;&#xA;&#xD;&#xA;Does software exist that can take a PDB file as input and generate these two pieces of information? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;e.g. file. [3NDB_ba.pdb][1]&#xD;&#xA;&#xD;&#xA;The nucleotide sequence is: &#xD;&#xA; &#xD;&#xA;`gUCUCGUCCCGUGGGGCUCGGCGGUGGGGGAGCAUCUCCUGUAGGGGAGAUGUAACCCCCUUUACCUGCCGAACCCCGCCAGGCCCGGAAGGGAGCAACGGUAGGCAGGACGUCGGCGCUCACGGGGGUGCGGGAC`.&#xD;&#xA;&#xD;&#xA;And the secondary structure :&#xD;&#xA;&#xD;&#xA;`.(((.(..(((((((((.(((((..(((((.(((((((((....)))))))))..)))))....((((((.....(((.....(((....))).....)))..)))))).))))))).))))))).....).))).`&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.rcsb.org/pdb/explore.do?structureId=3NDB" />
  <row Id="1714" PostHistoryTypeId="5" PostId="569" RevisionGUID="e99cbd5a-fba3-481a-ac1e-17ec2c788102" CreationDate="2017-06-08T08:31:47.370" UserId="191" Comment="add cytoscape" Text="There are multiple ways to do this, and multiple protein interaction databases besides the ones you mentioned, such as [BioGRID][1] or [IntAct][2]. Interaction databases are different in how interactions are defined, sometimes it can be experimental evidence of interaction, sometimes coexpression, orthology-based predictions, etc. &#xD;&#xA;&#xD;&#xA;There is no single solution to your problem. For String-DB you can use their R package [STRINGdb][3].&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    # string-db&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    # library for working with graphs&#xD;&#xA;    library(ighaph)&#xD;&#xA;    &#xD;&#xA;    # retrieve full graph for H. sapiens&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;    full.graph &lt;- string_db$get_graph()&#xD;&#xA;    &#xD;&#xA;    # see how your graph looks like (i.e. how many genes and interactions)&#xD;&#xA;    full.graph&#xD;&#xA;    &#xD;&#xA;    # define genes of interest&#xD;&#xA;    genes.of.interest &lt;- ...&#xD;&#xA;    &#xD;&#xA;    # get their neighbors&#xD;&#xA;    neighbors &lt;- string_db$get_neighbors(genes.of.interest)&#xD;&#xA;    &#xD;&#xA;    # get the subgraph of interest (your genes + their neighbors)&#xD;&#xA;    my.subgraph &lt;- induced_subgraph(full.graph, c(genes.of.interest, neighbors))&#xD;&#xA;&#xD;&#xA;    # look how many genes and interactions you have&#xD;&#xA;    my.subgraph&#xD;&#xA;&#xD;&#xA;If you prefer GUI, you might consider using [Cytoscape][4]. They have [multiple addons for interactions databases][5], e.g. [for STRINGdb][6]. There you can just provide your gene list and get a network in a few clicks.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://thebiogrid.org/&#xD;&#xA;  [2]: http://www.ebi.ac.uk/intact/&#xD;&#xA;  [3]: http://bioconductor.org/packages/release/bioc/html/STRINGdb.html&#xD;&#xA;  [4]: http://www.cytoscape.org/&#xD;&#xA;  [5]: http://apps.cytoscape.org/apps/with_tag/interactiondatabase&#xD;&#xA;  [6]: http://apps.cytoscape.org/apps/stringapp" />
  <row Id="1715" PostHistoryTypeId="6" PostId="1" RevisionGUID="4c019d46-2634-4848-9b77-569bfde90ca2" CreationDate="2017-06-08T08:35:23.083" UserId="96" Comment="tag normalization" Text="&lt;human-genome&gt;&lt;storage&gt;&lt;file-formats&gt;" />
  <row Id="1716" PostHistoryTypeId="24" PostId="1" RevisionGUID="4c019d46-2634-4848-9b77-569bfde90ca2" CreationDate="2017-06-08T08:35:23.083" Comment="Proposed by 96 approved by 77, 43 edit id of 146" />
  <row Id="1717" PostHistoryTypeId="5" PostId="569" RevisionGUID="b96fd897-a897-4b80-9f3a-daa1c957019e" CreationDate="2017-06-08T08:39:57.720" UserId="191" Comment="simplify the code" Text="There are multiple ways to do this, and multiple protein interaction databases besides the ones you mentioned, such as [BioGRID][1] or [IntAct][2]. Interaction databases are different in how interactions are defined, sometimes it can be experimental evidence of interaction, sometimes coexpression, orthology-based predictions, etc. &#xD;&#xA;&#xD;&#xA;There is no single solution to your problem. For String-DB you can use their R package [STRINGdb][3].&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    # string-db&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    &#xD;&#xA;    # retrieve full graph for H. sapiens&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;   &#xD;&#xA;    # define genes of interest&#xD;&#xA;    genes.of.interest &lt;- ...&#xD;&#xA;    &#xD;&#xA;    # get their neighbors&#xD;&#xA;    neighbors &lt;- string_db$get_neighbors(genes.of.interest)&#xD;&#xA;    &#xD;&#xA;    # get the subgraph of interest (your genes + their neighbors)&#xD;&#xA;    my.subgraph &lt;- string_db$get_subnetwork(c(genes.of.interest, neighbors))&#xD;&#xA;&#xD;&#xA;    # look how many genes and interactions you have&#xD;&#xA;    my.subgraph&#xD;&#xA;&#xD;&#xA;If you prefer GUI, you might consider using [Cytoscape][4]. They have [multiple addons for interactions databases][5], e.g. [for STRINGdb][6]. There you can just provide your gene list and get a network in a few clicks.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://thebiogrid.org/&#xD;&#xA;  [2]: http://www.ebi.ac.uk/intact/&#xD;&#xA;  [3]: http://bioconductor.org/packages/release/bioc/html/STRINGdb.html&#xD;&#xA;  [4]: http://www.cytoscape.org/&#xD;&#xA;  [5]: http://apps.cytoscape.org/apps/with_tag/interactiondatabase&#xD;&#xA;  [6]: http://apps.cytoscape.org/apps/stringapp" />
  <row Id="1718" PostHistoryTypeId="5" PostId="566" RevisionGUID="44007677-3c6a-495d-a5d0-7c313881edad" CreationDate="2017-06-08T08:43:05.753" UserId="77" Comment="Line-based too" Text="Presuming we consider &quot;human readable&quot;, &quot;easily parsable&quot;, and &quot;quickly queryable&quot; to be objectively good qualities (and if not, I worry for the future):&#xD;&#xA;&#xD;&#xA;1. Text-based: It's absurdly common to want to use `grep` or `awk` on annotations. Sure, one could make variants of these that are binary-format aware, but why reinvent the wheel. Of course text files don't innately allow region-based queries of their contents, so on to point 2...&#xD;&#xA;  * This should further explicitly be line-based. Fields would need to be tab-separated (ideally it's use the ascii record separator character instead, but I fear that ship has long ago sailed).&#xD;&#xA;2. A strictly defined line order: This is one of my personal pet-peeves about BED/GFF/GTF files. If you're going to make a text-based annotation format, everyone would end up ahead if said format made explicit that it should be sorted. This then allows things like tabix to be used so the &quot;query a region&quot; problem is then solved. But I would go further than that. The issue with things like GFF is that there are multiple inter-dependent lines and there's no strict rule about whether a parent line absolutely must come before a child line. This just makes implementing things a nightmare and more often than not a randomly ordered file will just break tools. Since GTF or GFF-style interdependent lines likely be how any annotation format works, this ordering between lines with the same start position should be made explicit in the format." />
  <row Id="1719" PostHistoryTypeId="6" PostId="525" RevisionGUID="ae180855-55ce-4b29-81fe-3252af05260e" CreationDate="2017-06-08T08:47:24.203" UserId="96" Comment="tag normalization" Text="&lt;bam&gt;&lt;file-formats&gt;&lt;format-conversion&gt;" />
  <row Id="1720" PostHistoryTypeId="24" PostId="525" RevisionGUID="ae180855-55ce-4b29-81fe-3252af05260e" CreationDate="2017-06-08T08:47:24.203" Comment="Proposed by 96 approved by 77, 73 edit id of 141" />
  <row Id="1721" PostHistoryTypeId="6" PostId="27" RevisionGUID="8d792875-8f5b-4b51-be20-83e01c9816d4" CreationDate="2017-06-08T08:47:30.533" UserId="96" Comment="tag normalization" Text="&lt;bam&gt;&lt;bed&gt;&lt;reference&gt;&lt;file-formats&gt;&lt;format-conversion&gt;" />
  <row Id="1722" PostHistoryTypeId="24" PostId="27" RevisionGUID="8d792875-8f5b-4b51-be20-83e01c9816d4" CreationDate="2017-06-08T08:47:30.533" Comment="Proposed by 96 approved by 77, 73 edit id of 145" />
  <row Id="1723" PostHistoryTypeId="6" PostId="344" RevisionGUID="112382ac-65cd-4c65-b9a1-fab1a5a6b345" CreationDate="2017-06-08T08:47:34.387" UserId="96" Comment="tag normalization" Text="&lt;vcf&gt;&lt;htslib&gt;&lt;file-formats&gt;" />
  <row Id="1724" PostHistoryTypeId="24" PostId="344" RevisionGUID="112382ac-65cd-4c65-b9a1-fab1a5a6b345" CreationDate="2017-06-08T08:47:34.387" Comment="Proposed by 96 approved by 77, 73 edit id of 143" />
  <row Id="1725" PostHistoryTypeId="5" PostId="568" RevisionGUID="5ff2ae6a-fb89-4334-83be-c7c93ddbca08" CreationDate="2017-06-08T08:48:35.860" UserId="48" Comment="Adding annotation tag, centering the question about how to build the file" Text="This question is based on [a question][1] on BioStars  posted &gt;2 years ago by user jack.&#xD;&#xA;&#xD;&#xA;It describes a very frequent problem of generating GO annotations for non-model organisms. While it is based on some specific format and single application (Ontologizer), it would be useful to have a general description of the pathway to getting to a GAF file. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I want to do the Gene enrichment using Ontologizer without a predefined association file(it's not model organism). &#xD;&#xA;&#xD;&#xA;I have parsed a file with two columns for that organism like this : &#xD;&#xA;&#xD;&#xA;    geneA  GO:0006950,GO:0005737&#xD;&#xA;    geneB  GO:0016020,GO:0005524,GO:0006468,GO:0005737,GO:0004674,GO:0006914,GO:0016021,GO:0015031&#xD;&#xA;    geneC  GO:0003779,GO:0006941,GO:0005524,GO:0003774,GO:0005516,GO:0005737,GO:0005863&#xD;&#xA;    geneD  GO:0005634,GO:0003677,GO:0030154,GO:0006350,GO:0006355,GO:0007275,GO:0030528&#xD;&#xA;     &#xD;&#xA;&#xD;&#xA;I have downloaded the .ob file from Gene ontology file which contain this information (from [here][2]) : &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    !&#xD;&#xA;    ! GO IDs (primary only) and name text strings&#xD;&#xA;    ! GO:0000000 [tab] text string [tab] F|P|C&#xD;&#xA;    ! where F = molecular function, P = biological process, C = cellular component&#xD;&#xA;    !&#xD;&#xA;    GO:0000001  mitochondrion inheritance   P&#xD;&#xA;    GO:0000002  mitochondrial genome maintenance    P&#xD;&#xA;    GO:0000003  reproduction    P&#xD;&#xA;    GO:0000005  ribosomal chaperone activity    F&#xD;&#xA;    GO:0000006  high affinity zinc uptake transmembrane transporter activity    F&#xD;&#xA;    GO:0000007  low-affinity zinc ion transmembrane transporter activity    F&#xD;&#xA;    GO:0000008  thioredoxin F&#xD;&#xA;    GO:0000009  alpha-1,6-mannosyltransferase activity  F&#xD;&#xA;    GO:0000010  trans-hexaprenyltranstransferase activity   F&#xD;&#xA;    GO:0000011  vacuole inheritance P&#xD;&#xA;     &#xD;&#xA;&#xD;&#xA;What I need as output is .gaf file in the following format (in the format of the files [here][3]):&#xD;&#xA;&#xD;&#xA;    !gaf-version: 2.0&#xD;&#xA;    &#xD;&#xA;    !Project_name: Leishmania major GeneDB&#xD;&#xA;    &#xD;&#xA;    !URL: http://www.genedb.org/leish&#xD;&#xA;    &#xD;&#xA;    !Contact Email: mb4@sanger.ac.uk&#xD;&#xA;    &#xD;&#xA;    GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0003723    PMID:22396527    ISO    GeneDB:Tb927.10.10130    F    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;    &#xD;&#xA;    GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0044429    PMID:20660476    ISS        C    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20100803    GeneDB_Lmajor       &#xD;&#xA;    &#xD;&#xA;    GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0016554    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;    &#xD;&#xA;    GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0048255    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor &#xD;&#xA;&#xD;&#xA;How to create your own GO association file (gaf)?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/129057/&#xD;&#xA;  [2]: http://www.geneontology.org/doc/GO.terms_and_ids&#xD;&#xA;  [3]: http://geneontology.org/page/download-annotations" />
  <row Id="1726" PostHistoryTypeId="4" PostId="568" RevisionGUID="5ff2ae6a-fb89-4334-83be-c7c93ddbca08" CreationDate="2017-06-08T08:48:35.860" UserId="48" Comment="Adding annotation tag, centering the question about how to build the file" Text="How to create your own GO association file (gaf)?" />
  <row Id="1727" PostHistoryTypeId="6" PostId="568" RevisionGUID="5ff2ae6a-fb89-4334-83be-c7c93ddbca08" CreationDate="2017-06-08T08:48:35.860" UserId="48" Comment="Adding annotation tag, centering the question about how to build the file" Text="&lt;annotation&gt;&lt;gene-ontology&gt;&lt;gaf&gt;" />
  <row Id="1728" PostHistoryTypeId="24" PostId="568" RevisionGUID="5ff2ae6a-fb89-4334-83be-c7c93ddbca08" CreationDate="2017-06-08T08:48:35.860" Comment="Proposed by 48 approved by 77, 73 edit id of 148" />
  <row Id="1729" PostHistoryTypeId="5" PostId="569" RevisionGUID="951cd280-cea5-4612-abc1-94923a234f86" CreationDate="2017-06-08T08:49:31.847" UserId="191" Comment="language" Text="There are multiple ways to do this, and multiple protein interaction databases besides the ones you mentioned, such as [BioGRID][1] or [IntAct][2]. Interaction databases are different in how interactions are defined, sometimes it can be experimental evidence of interaction, sometimes coexpression, orthology-based predictions, etc. &#xD;&#xA;&#xD;&#xA;There is no single solution to your problem. For String-DB you can use their R package [STRINGdb][3].&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    # string-db&#xD;&#xA;    library(STRINGdb)&#xD;&#xA;    &#xD;&#xA;    # retrieve full graph for H. sapiens&#xD;&#xA;    string_db &lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xD;&#xA;                              score_threshold=400, input_directory=&quot;&quot; )&#xD;&#xA;   &#xD;&#xA;    # define genes of interest&#xD;&#xA;    genes.of.interest &lt;- ...&#xD;&#xA;    &#xD;&#xA;    # get their neighbors&#xD;&#xA;    neighbors &lt;- string_db$get_neighbors(genes.of.interest)&#xD;&#xA;    &#xD;&#xA;    # get the subgraph of interest (your genes + their neighbors)&#xD;&#xA;    my.subgraph &lt;- string_db$get_subnetwork(c(genes.of.interest, neighbors))&#xD;&#xA;&#xD;&#xA;    # look how many genes and interactions you have&#xD;&#xA;    my.subgraph&#xD;&#xA;&#xD;&#xA;If you prefer GUI, you might consider using [Cytoscape][4]. They have [multiple addons for interaction databases][5], e.g. [for STRINGdb][6]. There you can just provide your gene list and get a network in a few clicks.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://thebiogrid.org/&#xD;&#xA;  [2]: http://www.ebi.ac.uk/intact/&#xD;&#xA;  [3]: http://bioconductor.org/packages/release/bioc/html/STRINGdb.html&#xD;&#xA;  [4]: http://www.cytoscape.org/&#xD;&#xA;  [5]: http://apps.cytoscape.org/apps/with_tag/interactiondatabase&#xD;&#xA;  [6]: http://apps.cytoscape.org/apps/stringapp" />
  <row Id="1730" PostHistoryTypeId="5" PostId="566" RevisionGUID="928f9c53-1a6a-49cb-95e6-94c1b043af5b" CreationDate="2017-06-08T08:50:10.537" UserId="77" Comment="typo" Text="Presuming we consider &quot;human readable&quot;, &quot;easily parsable&quot;, and &quot;quickly queryable&quot; to be objectively good qualities (and if not, I worry for the future):&#xD;&#xA;&#xD;&#xA;1. Text-based: It's absurdly common to want to use `grep` or `awk` on annotations. Sure, one could make variants of these that are binary-format aware, but why reinvent the wheel. Of course text files don't innately allow region-based queries of their contents, so on to point 2...&#xD;&#xA;  * This should further explicitly be line-based. Fields would need to be tab-separated (ideally it'd use the ascii record separator character instead, but I fear that ship has long ago sailed).&#xD;&#xA;2. A strictly defined line order: This is one of my personal pet-peeves about BED/GFF/GTF files. If you're going to make a text-based annotation format, everyone would end up ahead if said format made explicit that it should be sorted. This then allows things like tabix to be used so the &quot;query a region&quot; problem is then solved. But I would go further than that. The issue with things like GFF is that there are multiple inter-dependent lines and there's no strict rule about whether a parent line absolutely must come before a child line. This just makes implementing things a nightmare and more often than not a randomly ordered file will just break tools. Since GTF or GFF-style interdependent lines likely be how any annotation format works, this ordering between lines with the same start position should be made explicit in the format." />
  <row Id="1731" PostHistoryTypeId="2" PostId="570" RevisionGUID="126ca716-8ffd-4e11-9e98-a62342af79ca" CreationDate="2017-06-08T09:00:53.407" UserId="682" Text="I have many alignments from Rfam Database, and I would like to edit them. &#xD;&#xA;I saw that many tools are used for Protein sequence alignments, but there is something specific to edit RNA alignments ? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1732" PostHistoryTypeId="1" PostId="570" RevisionGUID="126ca716-8ffd-4e11-9e98-a62342af79ca" CreationDate="2017-06-08T09:00:53.407" UserId="682" Text="What to use to edit RNA alignments?" />
  <row Id="1733" PostHistoryTypeId="3" PostId="570" RevisionGUID="126ca716-8ffd-4e11-9e98-a62342af79ca" CreationDate="2017-06-08T09:00:53.407" UserId="682" Text="&lt;rna&gt;" />
  <row Id="1734" PostHistoryTypeId="5" PostId="570" RevisionGUID="841b961d-d15e-4c3d-a001-baf987ff4c72" CreationDate="2017-06-08T09:06:54.373" UserId="682" Comment="added 1221 characters in body" Text="I have many alignments from Rfam Database, and I would like to edit them. &#xD;&#xA;I saw that many tools are used for Protein sequence alignments, but there is something specific to edit RNA alignments ? &#xD;&#xA;&#xD;&#xA;e.g. &#xD;&#xA;&#xD;&#xA;`&gt;CR543861.1/1111804-1111620&#xD;&#xA;UUGUCUCUGGG-AUGCUCGCCAGCGAGUCAGAA----UCCCC-UGA-GCCGAUACUAUCA&#xD;&#xA;CUUUA-----GGAUAGCGUUCUG---CAUAU-UUAGAGU-GU-AUGCCUAC---------&#xD;&#xA;------CCAGA---------GUAGGAA---AC---CCAGCAAG-U-AUG--CGUCGCU--&#xD;&#xA;UCCACCUUG-----AAC----UUAA---GGGU--UCAAG--GG-------UAAUGACAGG&#xD;&#xA;CAGCGGCAU--CUUCGGAGGCAUU`&#xD;&#xA;&#xD;&#xA;`&gt;AB036820.1/118-302&#xD;&#xA;GAGAUUAUGUA-CUCCACUCCGAUUACCUA--------GUUU-UGA--CCGAACAUUUUU&#xD;&#xA;UUAUAC----GGGAGCUUGCCCU--UUCCUU-UCUGCAU-AA-AUGCCCUC---------&#xD;&#xA;-------AUUA---------GAGGCAC---UU---AUAAAGUC-A-GGA--AGAAAGC-A&#xD;&#xA;CCCACCUGC----CAUG---GUGCG---GGGU--UCA-A--AAC------GAAGGAAAGG&#xD;&#xA;A-GGGGAAGAAAUAAGAAAGAUAU`&#xD;&#xA;&#xD;&#xA;`&gt;AL591979.1/136531-136336&#xD;&#xA;GAAACCCUAAU-GUAUUCGAUAUAGUUCCUGAU----AUUUU-UGA-ACCGAACAAUUUU&#xD;&#xA;UUAUACC-UAGGGAGCUUGGAGU--UCCGGC-GCGGCGC-AC-AUGCCUU----------&#xD;&#xA;------CACAC----------GAGGAA---GU---GCAAACCG-U-UAG--ACAGAGC-A&#xD;&#xA;CCCACCUGCU--UUAAUUGAGAGCG----GGU--UCA-A--AGG-----AAGGGAAUCCU&#xD;&#xA;AAACGGUAC--GAUUGGGGUUUCU`&#xD;&#xA;&#xD;&#xA;`&gt;BX842646.1/183093-182914&#xD;&#xA;AGAAGGGUUCG-ACGG-CGUAAAGCUCUAUC------GAAAA-UAA-ACCUUACAAGCAU&#xD;&#xA;AUUUUU----GGGGACUGUCCC----UGACA-AUGGUGA-GC-AAGCUCAG---------&#xD;&#xA;------CUCGU--------ACUGAGAA---GC---CUAAAGUU-G-UUA--UCAUGGUUA&#xD;&#xA;CCCACCUUAG---CUAA----AACA----GGU--UUACU-UUUU-------------CAG&#xD;&#xA;AGUGGUCUU---CGAGCCUUUUUU`&#xD;&#xA;" />
  <row Id="1735" PostHistoryTypeId="2" PostId="571" RevisionGUID="790b5015-d8ce-4302-aa93-e7733e652f32" CreationDate="2017-06-08T09:08:04.750" UserId="298" Text="Here's a Perl script that can do this:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env perl &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    ## Change this to whatever taxon you are working with&#xD;&#xA;    my $taxon = 'taxon:1000';&#xD;&#xA;    chomp(my $date = `date +%Y%M%d`);&#xD;&#xA;    &#xD;&#xA;    my (%aspect, %gos);&#xD;&#xA;    ## Read the GO.terms_and_ids file to get the aspect (sub ontology)&#xD;&#xA;    ## of each GO term. &#xD;&#xA;    open(my $fh, $ARGV[0]) or die &quot;Need a GO.terms_and_ids file as 1st arg: $!\n&quot;;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;    	next if /^!/;&#xD;&#xA;    	chomp;&#xD;&#xA;    	my @fields = split(/\t/);&#xD;&#xA;    	## $aspect{GO:0000001} = 'P'&#xD;&#xA;    	$aspect{$fields[0]} = $fields[2];&#xD;&#xA;    }&#xD;&#xA;    close($fh);&#xD;&#xA;    &#xD;&#xA;    ## Read the list of gene annotations&#xD;&#xA;    open($fh, $ARGV[1]) or die &quot;Need a list of gene annotattions as 2nd arg: $!\n&quot;;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;    	chomp;&#xD;&#xA;    	my ($gene, @terms) = split(/[\s,]+/);&#xD;&#xA;    	## $gos{geneA} = (go1, go2 ... goN)&#xD;&#xA;    	$gos{$gene} = [ @terms ];&#xD;&#xA;    }&#xD;&#xA;    close($fh);&#xD;&#xA;    &#xD;&#xA;    foreach my $gene (keys(%gos)) {&#xD;&#xA;    	foreach my $term (@{$gos{$gene}}) {&#xD;&#xA;    		## Warn and skip if there is no aspect for this term&#xD;&#xA;    		if (!$aspect{$term}) {&#xD;&#xA;    			print STDERR &quot;Unknown GO term ($term) for gene $gene\n&quot;;&#xD;&#xA;    			next;&#xD;&#xA;    		}&#xD;&#xA;    		## Build a pseudo GAF line &#xD;&#xA;    		my @out = ('DB', $gene, $gene, ' ', $term, 'PMID:foo', 'TAS', ' ', $aspect{$term},&#xD;&#xA;    							 $gene, ' ', 'protein', $taxon, $date, 'DB', ' ', ' ');&#xD;&#xA;    		print join(&quot;\t&quot;, @out). &quot;\n&quot;;&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Make it executable and run it with the `GO.terms_and_ids` file as the 1st argument and the list of gene annotations as the second. Using the current `GO.terms_and_ids` and the example annotations in the question, I get:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    $ foo.pl GO.terms_and_ids file.gos &#xD;&#xA;    DB	geneD	geneD	 	GO:0005634	PMID:foo	TAS	 	C	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneD	geneD	 	GO:0003677	PMID:foo	TAS	 	F	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneD	geneD	 	GO:0030154	PMID:foo	TAS	 	P	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    Unknown GO term (GO:0006350) for gene geneD&#xD;&#xA;    DB	geneD	geneD	 	GO:0006355	PMID:foo	TAS	 	P	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneD	geneD	 	GO:0007275	PMID:foo	TAS	 	P	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneD	geneD	 	GO:0030528	PMID:foo	TAS	 	F	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0016020	PMID:foo	TAS	 	C	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0005524	PMID:foo	TAS	 	F	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0006468	PMID:foo	TAS	 	P	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0005737	PMID:foo	TAS	 	C	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0004674	PMID:foo	TAS	 	F	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0006914	PMID:foo	TAS	 	P	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0016021	PMID:foo	TAS	 	C	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0015031	PMID:foo	TAS	 	P	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneA	geneA	 	GO:0006950	PMID:foo	TAS	 	P	geneA	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneA	geneA	 	GO:0005737	PMID:foo	TAS	 	C	geneA	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0003779	PMID:foo	TAS	 	F	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0006941	PMID:foo	TAS	 	P	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0005524	PMID:foo	TAS	 	F	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0003774	PMID:foo	TAS	 	F	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0005516	PMID:foo	TAS	 	F	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0005737	PMID:foo	TAS	 	C	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0005863	PMID:foo	TAS	 	C	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Note that this is very much a pseudo-GAF file since most of the fields apart from the gene name, GO term and sub-ontology are fake. It should still work for what you need, however." />
  <row Id="1736" PostHistoryTypeId="5" PostId="421" RevisionGUID="d69b3116-6802-4975-8b30-bb4a4e42b829" CreationDate="2017-06-08T09:10:32.403" UserId="134" Comment="added 9 characters in body" Text="The SAM format originally had only **M**, **I**, **D**, **N**, **S**, **H**, and **P** CIGAR operators.  See the [original SAM specification](https://github.com/samtools/hts-specs/tree/a9b16d4c54d8b2bf900b210b8319474163d7ce2a) (if you can view Apple Pages documents) and Table 1 in [_The Sequence Alignment/Map format and SAMtools_](https://www.ncbi.nlm.nih.gov/pubmed/19505943) (Li _et al_, 2009).  This was in line with previous tools using CIGAR strings, notably [_exonerate_](http://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate) which introduced them with just the **M**, **I**, and **D** operators.&#xD;&#xA;&#xD;&#xA;BWA-backtrack was written contemporaneously with the SAM format in 2008 and 2009 (and [published in May 2009](https://www.ncbi.nlm.nih.gov/pubmed/19451168)).  Its _ChangeLog_ shows that it was outputting mismatch information in an **MD** tag from January 2009 and that **MD** was defined in the SAM specification of the time (and that the tag value's syntax was somewhat in flux in February 2009).  The **MD** tag is also described in that early v1.0 Pages-formatted SAM specification.&#xD;&#xA;&#xD;&#xA;The **=** and **X** CIGAR operators were introduced later in [SAM v1.3](https://github.com/samtools/hts-specs/commit/07dc1c67a717a6c5cc9d65eeb8b3c99612744cde) as a result of [this lengthy samtools-devel mailing list thread](https://sourceforge.net/p/samtools/mailman/samtools-devel/thread/ee957a250907301016y22c96c5x2907a33c2e719da2%40mail.gmail.com/).  The characters used for the operators and the initial implementations were essentially in place by November 2009.&#xD;&#xA;&#xD;&#xA;Since then, the **=**/**X** operators have not really taken over from **M**, as you've seen.  There's probably a number of factors contributing to this:&#xD;&#xA;&#xD;&#xA;* Their later introduction, long after the basic **M**/**I**/**D** operators and **MD** tag were well-established;&#xD;&#xA;* Conceivably their being specific to SAM and unavailable in other CIGAR flavours;&#xD;&#xA;* The **MD** tag still provides more information — **X** doesn't tell you what the mismatched reference bases were." />
  <row Id="1737" PostHistoryTypeId="5" PostId="570" RevisionGUID="e4826f5e-883e-4c9d-ac23-236e3b66892d" CreationDate="2017-06-08T09:12:47.930" UserId="682" Comment="added 1221 characters in body" Text="I have many alignments from Rfam Database, and I would like to edit them. &#xD;&#xA;I saw that many tools are used for Protein sequence alignments, but there is something specific to edit RNA alignments ? &#xD;&#xA;&#xD;&#xA;e.g. Stockholm Alignment of Pistol (just few entries). &#xD;&#xA; &#xD;&#xA;&#xD;&#xA;`FP929053.1/1669026-1668956   AGUGGUCACAGCCACUAUAAACA-GGGCUU-UAAGCUGUG-AGCGUUGACCGUC----------ACAA-----CGGCGGUCAGGUAGUC&#xD;&#xA;AFOX01000025.1/1981-1912     ACUCGUCUGAGCGAGUAUAAACA-GGUCAU-UAAGCUCAG-AGCGUUCACCGGG----------AUCA------UUCGGUGAGGUUGGC&#xD;&#xA;HE577054.1/3246821-3246752   ACUCGUCUGAGCGAGUAUAAACA-GGUCAU-UAAGCUCAG-AGCGUUCACCGGG----------AUCA------UGCGGUGAGGUUGGC&#xD;&#xA;CP000154.1/3364237-3364168   GUUCGUCUGAGCGAACGCAAACA-GGCCAU-UAAGCUCAG-AGCGUUCACUGGA----------UUCG------UCCAGUGAGAUUGGC`&#xD;&#xA;`#=GC SS_cons                 &lt;&lt;&lt;&lt;__AAAAA_&gt;&gt;&gt;&gt;-------..&lt;&lt;&lt;&lt;-.----aaaaa.----&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;..........____....._&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;--&gt;&gt;&gt;&gt;`&#xD;&#xA;`#=GC RF                      acUCGUCuggGCGAguAUAAAuA..cgCaU.UAgGCccaG.AGCGUcccggcgg..........uUau.....uccgccgggGGUuGcg&#xD;&#xA;//`" />
  <row Id="1738" PostHistoryTypeId="5" PostId="568" RevisionGUID="42d7fb71-f09f-4fb7-ac6c-0825f12972b8" CreationDate="2017-06-08T09:17:13.747" UserId="298" Comment="Put the quoted text in a quote box" Text="This question is based on [a question][1] on BioStars  posted &gt;2 years ago by user jack.&#xD;&#xA;&#xD;&#xA;It describes a very frequent problem of generating GO annotations for non-model organisms. While it is based on some specific format and single application (Ontologizer), it would be useful to have a general description of the pathway to getting to a GAF file. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt; I want to do the Gene enrichment using Ontologizer without a&#xD;&#xA;&gt; predefined association file(it's not model organism). &#xD;&#xA;&gt; &#xD;&#xA;&gt; I have parsed a file with two columns for that organism like this : &#xD;&#xA;&gt; &#xD;&#xA;&gt;     geneA  GO:0006950,GO:0005737&#xD;&#xA;&gt;     geneB  GO:0016020,GO:0005524,GO:0006468,GO:0005737,GO:0004674,GO:0006914,GO:0016021,GO:0015031&#xD;&#xA;&gt;     geneC  GO:0003779,GO:0006941,GO:0005524,GO:0003774,GO:0005516,GO:0005737,GO:0005863&#xD;&#xA;&gt;     geneD  GO:0005634,GO:0003677,GO:0030154,GO:0006350,GO:0006355,GO:0007275,GO:0030528&#xD;&#xA;&gt;      &#xD;&#xA;&gt; &#xD;&#xA;&gt; I have downloaded the .ob file from Gene ontology file which contain&#xD;&#xA;&gt; this information (from [here][2]) : &#xD;&#xA;&gt; &#xD;&#xA;&gt; &#xD;&#xA;&gt; &#xD;&#xA;&gt;     !&#xD;&#xA;&gt;     ! GO IDs (primary only) and name text strings&#xD;&#xA;&gt;     ! GO:0000000 [tab] text string [tab] F|P|C&#xD;&#xA;&gt;     ! where F = molecular function, P = biological process, C = cellular component&#xD;&#xA;&gt;     !&#xD;&#xA;&gt;     GO:0000001  mitochondrion inheritance   P&#xD;&#xA;&gt;     GO:0000002  mitochondrial genome maintenance    P&#xD;&#xA;&gt;     GO:0000003  reproduction    P&#xD;&#xA;&gt;     GO:0000005  ribosomal chaperone activity    F&#xD;&#xA;&gt;     GO:0000006  high affinity zinc uptake transmembrane transporter activity    F&#xD;&#xA;&gt;     GO:0000007  low-affinity zinc ion transmembrane transporter activity    F&#xD;&#xA;&gt;     GO:0000008  thioredoxin F&#xD;&#xA;&gt;     GO:0000009  alpha-1,6-mannosyltransferase activity  F&#xD;&#xA;&gt;     GO:0000010  trans-hexaprenyltranstransferase activity   F&#xD;&#xA;&gt;     GO:0000011  vacuole inheritance P&#xD;&#xA;&gt;      &#xD;&#xA;&gt; &#xD;&#xA;&gt; What I need as output is .gaf file in the following format (in the&#xD;&#xA;&gt; format of the files [here][3]):&#xD;&#xA;&gt; &#xD;&#xA;&gt;     !gaf-version: 2.0&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     !Project_name: Leishmania major GeneDB&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     !URL: http://www.genedb.org/leish&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     !Contact Email: mb4@sanger.ac.uk&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0003723    PMID:22396527    ISO    GeneDB:Tb927.10.10130    F    mitochondrial&#xD;&#xA;&gt; RNA binding complex 1 subunit, putative    LmjF36.4770    gene   &#xD;&#xA;&gt; taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;&gt;     &#xD;&#xA;&gt;     GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0044429    PMID:20660476    ISS        C    mitochondrial RNA binding complex 1&#xD;&#xA;&gt; subunit, putative    LmjF36.4770    gene    taxon:347515    20100803  &#xD;&#xA;&gt; GeneDB_Lmajor       &#xD;&#xA;&gt;     &#xD;&#xA;&gt;     GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0016554    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial&#xD;&#xA;&gt; RNA binding complex 1 subunit, putative    LmjF36.4770    gene   &#xD;&#xA;&gt; taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;&gt;     &#xD;&#xA;&gt;     GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0048255    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial&#xD;&#xA;&gt; RNA binding complex 1 subunit, putative    LmjF36.4770    gene   &#xD;&#xA;&gt; taxon:347515    20120910    GeneDB_Lmajor &#xD;&#xA;&gt; &#xD;&#xA;&gt; How to create your own GO association file (gaf)?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/129057/&#xD;&#xA;  [2]: http://www.geneontology.org/doc/GO.terms_and_ids&#xD;&#xA;  [3]: http://geneontology.org/page/download-annotations" />
  <row Id="1739" PostHistoryTypeId="24" PostId="568" RevisionGUID="42d7fb71-f09f-4fb7-ac6c-0825f12972b8" CreationDate="2017-06-08T09:17:13.747" Comment="Proposed by 298 approved by 311 edit id of 150" />
  <row Id="1740" PostHistoryTypeId="2" PostId="572" RevisionGUID="5cd30b70-04f7-4f94-9333-781878ed4685" CreationDate="2017-06-08T09:18:07.773" UserId="693" Text="For miRNA precursor predictions in species in Ensembl but not in miRBase, you can have a go at MapMi (http://www.ebi.ac.uk/enright-srv/MapMi/) . &#xD;&#xA;&#xD;&#xA;You can easily run it locally for other species or if you cave smallRNA sequencing done on something new. &#xD;&#xA;&#xD;&#xA;Let me know if you run into difficulties or have ideas to improve it. &#xD;&#xA;&#xD;&#xA;Ensembl itself has its own miRNA predictions. " />
  <row Id="1741" PostHistoryTypeId="5" PostId="568" RevisionGUID="490784d9-ceb1-49cb-ae17-6098ec55530a" CreationDate="2017-06-08T09:18:59.677" UserId="298" Comment="Put the quoted text in a quote box" Text="This question is based on [a question][1] on BioStars  posted &gt;2 years ago by user jack.&#xD;&#xA;&#xD;&#xA;It describes a very frequent problem of generating GO annotations for non-model organisms. While it is based on some specific format and single application (Ontologizer), it would be useful to have a general description of the pathway to getting to a GAF file. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt; I want to do the Gene enrichment using Ontologizer without a&#xD;&#xA;&gt; predefined association file(it's not model organism). &#xD;&#xA;&gt; &#xD;&#xA;&gt; I have parsed a file with two columns for that organism like this : &#xD;&#xA;&gt; &#xD;&#xA;&gt;     geneA  GO:0006950,GO:0005737&#xD;&#xA;&gt;     geneB  GO:0016020,GO:0005524,GO:0006468,GO:0005737,GO:0004674,GO:0006914,GO:0016021,GO:0015031&#xD;&#xA;&gt;     geneC  GO:0003779,GO:0006941,GO:0005524,GO:0003774,GO:0005516,GO:0005737,GO:0005863&#xD;&#xA;&gt;     geneD  GO:0005634,GO:0003677,GO:0030154,GO:0006350,GO:0006355,GO:0007275,GO:0030528&#xD;&#xA;&gt;      &#xD;&#xA;&gt; &#xD;&#xA;&gt; I have downloaded the .ob file from Gene ontology file which contain&#xD;&#xA;&gt; this information (from [here][2]) : &#xD;&#xA;&gt; &#xD;&#xA;&gt; &#xD;&#xA;&gt; &#xD;&#xA;&gt;     !&#xD;&#xA;&gt;     ! GO IDs (primary only) and name text strings&#xD;&#xA;&gt;     ! GO:0000000 [tab] text string [tab] F|P|C&#xD;&#xA;&gt;     ! where F = molecular function, P = biological process, C = cellular component&#xD;&#xA;&gt;     !&#xD;&#xA;&gt;     GO:0000001  mitochondrion inheritance   P&#xD;&#xA;&gt;     GO:0000002  mitochondrial genome maintenance    P&#xD;&#xA;&gt;     GO:0000003  reproduction    P&#xD;&#xA;&gt;     GO:0000005  ribosomal chaperone activity    F&#xD;&#xA;&gt;     GO:0000006  high affinity zinc uptake transmembrane transporter activity    F&#xD;&#xA;&gt;     GO:0000007  low-affinity zinc ion transmembrane transporter activity    F&#xD;&#xA;&gt;     GO:0000008  thioredoxin F&#xD;&#xA;&gt;     GO:0000009  alpha-1,6-mannosyltransferase activity  F&#xD;&#xA;&gt;     GO:0000010  trans-hexaprenyltranstransferase activity   F&#xD;&#xA;&gt;     GO:0000011  vacuole inheritance P&#xD;&#xA;&gt;      &#xD;&#xA;&gt; &#xD;&#xA;&gt; What I need as output is .gaf file in the following format (in the&#xD;&#xA;&gt; format of the files [here][3]):&#xD;&#xA;&gt; &#xD;&#xA;&gt;     !gaf-version: 2.0&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     !Project_name: Leishmania major GeneDB&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     !URL: http://www.genedb.org/leish&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     !Contact Email: mb4@sanger.ac.uk&#xD;&#xA;&gt;     &#xD;&#xA;&gt;      GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0003723    PMID:22396527    ISO    GeneDB:Tb927.10.10130    F    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;&gt;      GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0044429    PMID:20660476    ISS        C    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20100803 GeneDB_Lmajor             GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0016554    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene   taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;&gt;      GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0048255    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor  &#xD;&#xA;&#xD;&#xA;&gt; How to create your own GO association file (gaf)?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/129057/&#xD;&#xA;  [2]: http://www.geneontology.org/doc/GO.terms_and_ids&#xD;&#xA;  [3]: http://geneontology.org/page/download-annotations" />
  <row Id="1742" PostHistoryTypeId="24" PostId="568" RevisionGUID="490784d9-ceb1-49cb-ae17-6098ec55530a" CreationDate="2017-06-08T09:18:59.677" Comment="Proposed by 298 approved by 311 edit id of 152" />
  <row Id="1743" PostHistoryTypeId="2" PostId="573" RevisionGUID="04f343df-a0a7-4275-adcd-a2307d4d00cc" CreationDate="2017-06-08T09:22:17.537" UserId="298" Text="There's nothing really special about RNA alignments, you can use any alignment editor, including whichever one you use for protein. That said, a classic and very useful tool for this sort of thing is [JalView](http://www.jalview.org/). It can be installed locally or run as a Java webapp from your browser.&#xD;&#xA;&#xD;&#xA;&gt; Jalview has built in DNA, RNA and protein sequence and structure visualisation and analysis capabilities. It uses Jmol to view 3D structures, and VARNA to display RNA secondary structure. The Jalview Desktop can also connect with databases and analysis services, and provides a graphical interface to the alignment and analysis services provided by the JavA Bioinformatics Analysis Web Services framework.&#xD;&#xA;&#xD;&#xA;[![jalview image][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/I76Yq.png" />
  <row Id="1744" PostHistoryTypeId="2" PostId="574" RevisionGUID="50fe7eb2-208c-4f9a-8875-b20a95238eb4" CreationDate="2017-06-08T09:23:18.300" UserId="640" Text="I would suggest use RALEE—RNALignment Editor in Emacs. It can get for you the consensus secondary structure, you can move left/right sequences and their secondary structures (you can't do it in JalView!), and more.&#xD;&#xA; &#xD;&#xA;Sam Griffiths-Jones&#xD;&#xA;&#xD;&#xA;Bioinformatics (2005) 21 (2): 257-259. DOI: https://doi.org/10.1093/bioinformatics/bth489[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;Fig. You can move left/right sequences and their secondary structures (you can't do it in JalView!)&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/XOWVV.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/UPcwI.png" />
  <row Id="1745" PostHistoryTypeId="6" PostId="418" RevisionGUID="ef996b0b-fef8-4ab8-a671-803ad5ae6d9f" CreationDate="2017-06-08T09:26:37.033" UserId="298" Comment="edited tags" Text="&lt;ngs&gt;&lt;bwa&gt;&lt;read-alignment&gt;" />
  <row Id="1746" PostHistoryTypeId="5" PostId="574" RevisionGUID="f46720c7-2f4a-4782-9531-2cf1aaeec369" CreationDate="2017-06-08T09:29:44.137" UserId="640" Comment="added 279 characters in body" Text="I would suggest use RALEE—RNALignment Editor in Emacs. It can get for you the consensus secondary structure, you can move left/right sequences and their secondary structures (you can't do it in JalView!), and more.&#xD;&#xA;&#xD;&#xA;It's an Emacs mode, so could be a bit hard to start off, but just try, you don't have to use all Emacs features to edit your alignments! &#xD;&#xA;&#xD;&#xA;&gt; The RALEE (RNA ALignment Editor in Emacs) tool provides a simple&#xD;&#xA;&gt; environment for RNA multiple sequence alignment editing, including&#xD;&#xA;&gt; structure-specific colour schemes, utilizing helper applications for&#xD;&#xA;&gt; structure prediction and many more conventional editing functions.&#xD;&#xA; &#xD;&#xA;Sam Griffiths-Jones Bioinformatics (2005) 21 (2): 257-259. DOI: https://doi.org/10.1093/bioinformatics/bth489[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;Fig. You can move left/right sequences and their secondary structures (you can't do it in JalView!)&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/XOWVV.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/UPcwI.png" />
  <row Id="1747" PostHistoryTypeId="5" PostId="568" RevisionGUID="e6de7320-7421-49ff-9514-ada7095d5bee" CreationDate="2017-06-08T09:30:15.353" UserId="311" Comment="added 228 characters in body" Text="This question is based on [a question][1] on BioStars  posted &gt;2 years ago by user jack.&#xD;&#xA;&#xD;&#xA;It describes a very frequent problem of generating GO annotations for non-model organisms. While it is based on some specific format and single application (Ontologizer), it would be useful to have a general description of the pathway to getting to a GAF file. &#xD;&#xA;&#xD;&#xA;Note, that the input format is lacking a bit of essential information, like how it was obtained. Therefore, it is har to assign evidence code. Therefore, lets assume that the assignments of GO terms were done automagically. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt; I want to do the Gene enrichment using Ontologizer without a&#xD;&#xA;&gt; predefined association file(it's not model organism). &#xD;&#xA;&gt; &#xD;&#xA;&gt; I have parsed a file with two columns for that organism like this : &#xD;&#xA;&gt; &#xD;&#xA;&gt;     geneA  GO:0006950,GO:0005737&#xD;&#xA;&gt;     geneB  GO:0016020,GO:0005524,GO:0006468,GO:0005737,GO:0004674,GO:0006914,GO:0016021,GO:0015031&#xD;&#xA;&gt;     geneC  GO:0003779,GO:0006941,GO:0005524,GO:0003774,GO:0005516,GO:0005737,GO:0005863&#xD;&#xA;&gt;     geneD  GO:0005634,GO:0003677,GO:0030154,GO:0006350,GO:0006355,GO:0007275,GO:0030528&#xD;&#xA;&gt;      &#xD;&#xA;&gt; &#xD;&#xA;&gt; I have downloaded the .ob file from Gene ontology file which contain&#xD;&#xA;&gt; this information (from [here][2]) : &#xD;&#xA;&gt; &#xD;&#xA;&gt; &#xD;&#xA;&gt; &#xD;&#xA;&gt;     !&#xD;&#xA;&gt;     ! GO IDs (primary only) and name text strings&#xD;&#xA;&gt;     ! GO:0000000 [tab] text string [tab] F|P|C&#xD;&#xA;&gt;     ! where F = molecular function, P = biological process, C = cellular component&#xD;&#xA;&gt;     !&#xD;&#xA;&gt;     GO:0000001  mitochondrion inheritance   P&#xD;&#xA;&gt;     GO:0000002  mitochondrial genome maintenance    P&#xD;&#xA;&gt;     GO:0000003  reproduction    P&#xD;&#xA;&gt;     GO:0000005  ribosomal chaperone activity    F&#xD;&#xA;&gt;     GO:0000006  high affinity zinc uptake transmembrane transporter activity    F&#xD;&#xA;&gt;     GO:0000007  low-affinity zinc ion transmembrane transporter activity    F&#xD;&#xA;&gt;     GO:0000008  thioredoxin F&#xD;&#xA;&gt;     GO:0000009  alpha-1,6-mannosyltransferase activity  F&#xD;&#xA;&gt;     GO:0000010  trans-hexaprenyltranstransferase activity   F&#xD;&#xA;&gt;     GO:0000011  vacuole inheritance P&#xD;&#xA;&gt;      &#xD;&#xA;&gt; &#xD;&#xA;&gt; What I need as output is .gaf file in the following format (in the&#xD;&#xA;&gt; format of the files [here][3]):&#xD;&#xA;&gt; &#xD;&#xA;&gt;     !gaf-version: 2.0&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     !Project_name: Leishmania major GeneDB&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     !URL: http://www.genedb.org/leish&#xD;&#xA;&gt;     &#xD;&#xA;&gt;     !Contact Email: mb4@sanger.ac.uk&#xD;&#xA;&gt;     &#xD;&#xA;&gt;      GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0003723    PMID:22396527    ISO    GeneDB:Tb927.10.10130    F    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;&gt;      GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0044429    PMID:20660476    ISS        C    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20100803 GeneDB_Lmajor             GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0016554    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene   taxon:347515    20120910    GeneDB_Lmajor       &#xD;&#xA;&gt;      GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0048255    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor  &#xD;&#xA;&#xD;&#xA;&gt; How to create your own GO association file (gaf)?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/129057/&#xD;&#xA;  [2]: http://www.geneontology.org/doc/GO.terms_and_ids&#xD;&#xA;  [3]: http://geneontology.org/page/download-annotations" />
  <row Id="1748" PostHistoryTypeId="2" PostId="575" RevisionGUID="2e03a1d2-dc63-468d-bb7e-093383a445a3" CreationDate="2017-06-08T09:35:17.007" UserId="-1" Text="" />
  <row Id="1749" PostHistoryTypeId="1" PostId="575" RevisionGUID="2e03a1d2-dc63-468d-bb7e-093383a445a3" CreationDate="2017-06-08T09:35:17.007" UserId="-1" />
  <row Id="1750" PostHistoryTypeId="2" PostId="576" RevisionGUID="86d0634b-5771-4109-9487-75654eff5476" CreationDate="2017-06-08T09:35:17.007" UserId="-1" Text="" />
  <row Id="1751" PostHistoryTypeId="1" PostId="576" RevisionGUID="86d0634b-5771-4109-9487-75654eff5476" CreationDate="2017-06-08T09:35:17.007" UserId="-1" />
  <row Id="1752" PostHistoryTypeId="5" PostId="572" RevisionGUID="5c077430-b500-4746-ab89-f50c2059c1b1" CreationDate="2017-06-08T10:23:38.590" UserId="77" Comment="Fixed some small typos" Text="For miRNA precursor predictions in species in Ensembl but not in miRBase, you can have a go to MapMi (http://www.ebi.ac.uk/enright-srv/MapMi/) . &#xD;&#xA;&#xD;&#xA;You can easily run it locally for other species or if you have smallRNA sequencing done on something new. &#xD;&#xA;&#xD;&#xA;Let me know if you run into difficulties or have ideas to improve it. &#xD;&#xA;&#xD;&#xA;Ensembl itself has its own miRNA predictions. " />
  <row Id="1753" PostHistoryTypeId="5" PostId="567" RevisionGUID="8a302911-706e-4b45-9d47-a0b7ca7e9ecd" CreationDate="2017-06-08T10:27:52.733" UserId="734" Comment="added 6 characters in body" Text="Assume I have found the top 0.01% most frequent genes from a [gene expression file][1]. &#xD;&#xA;&#xD;&#xA;Let's say, these are 10 genes and I want to study the [protein protein interactions][2], the protein network and pathway.&#xD;&#xA; &#xD;&#xA;I thought to use [string-db][3] and [interactome][4], but I am not sure what would be a plausible way to approach this problem and to build the protein network etc. Are there other more suitable databases?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/559/how-to-read-and-interpret-a-gene-expression-quantification-file&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Protein%E2%80%93protein_interaction&#xD;&#xA;  [3]: https://string-db.org&#xD;&#xA;  [4]: http://interactome.dfci.harvard.edu" />
  <row Id="1754" PostHistoryTypeId="6" PostId="559" RevisionGUID="6c2b9a51-17c5-4127-801c-5cc8b266e0f9" CreationDate="2017-06-08T10:28:06.803" UserId="292" Comment="Added a normalization tag, since the answer has to do with normalization" Text="&lt;gene&gt;&lt;normalization&gt;&lt;identifiers&gt;&lt;quantification&gt;" />
  <row Id="1755" PostHistoryTypeId="24" PostId="559" RevisionGUID="6c2b9a51-17c5-4127-801c-5cc8b266e0f9" CreationDate="2017-06-08T10:28:06.803" Comment="Proposed by 292 approved by 77, 734 edit id of 149" />
  <row Id="1756" PostHistoryTypeId="2" PostId="577" RevisionGUID="bc4fb2e5-5374-4fbe-870a-0a0d03865391" CreationDate="2017-06-08T10:31:33.980" UserId="298" Text="Many interaction databases now works with [PSI format][1] files. Most of the main databases can do this and the EBI has set up [PSICQUIC View][2], a very useful page where you can query multiple databases at once. &#xD;&#xA;&#xD;&#xA;Note that it is very important to limit the results according to the detection method. There is a lot of noise in protein interaction databases. Depending on what you want to do you could limit to only experimentally verified interactions or to only direct, binary interactions &#xD;&#xA;(so exclude the results of, for example, ChIP analyses which can also find complexes) etc.&#xD;&#xA;&#xD;&#xA;That said, here's a simple example script that will query the APID database using its PSICQUIC service:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    use LWP::Simple;&#xD;&#xA;    &#xD;&#xA;    my @proteins;&#xD;&#xA;    ## Read a list of target proteins, one per line (this expects UniProt names)&#xD;&#xA;    open(my $fh, &quot;$ARGV[0]&quot;) or die &quot;Need a list of proteins as the 1st argument: $!\n&quot;;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;    	chomp;&#xD;&#xA;    	push @proteins, $_;&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    ## Get human interactions only&#xD;&#xA;    my $species=&quot;9606&quot;;&#xD;&#xA;    &#xD;&#xA;    ## Get the interactions for each target protein&#xD;&#xA;    foreach my $protein (@proteins) {&#xD;&#xA;    	my $queryUrl= &quot;http://cicblade.dep.usal.es/psicquic-ws/webservices/current/search/query/$protein&quot;;&#xD;&#xA;    	$queryUrl  .= &quot;?taxidA:$species%20ANDtaxidB:$species&quot;;&#xD;&#xA;    &#xD;&#xA;    	my $tries=1;&#xD;&#xA;    	my $content = get $queryUrl;&#xD;&#xA;    	while ($tries&lt;=10) {&#xD;&#xA;    		if (defined($content)) {&#xD;&#xA;    			$tries=11;&#xD;&#xA;    		} else {&#xD;&#xA;    			print STDERR &quot;Could not retrieve $queryUrl, retrying($tries)...\n&quot;;&#xD;&#xA;    			$content = get $queryUrl;&#xD;&#xA;    		}&#xD;&#xA;    		$tries++;&#xD;&#xA;    	}&#xD;&#xA;    &#xD;&#xA;    	# Now list all interactions&#xD;&#xA;    	my @lines = split(/\n/, $content);&#xD;&#xA;    	my $LINES= @lines;&#xD;&#xA;    	my $count = 0;&#xD;&#xA;    	for my $line (@lines) {&#xD;&#xA;    		$count++;&#xD;&#xA;    		my @flds = split(/\t/, $line); # split tab delimited lines      &#xD;&#xA;    		# split fields of a PSIMITAB 2.5 line&#xD;&#xA;    		my ($idA, $idB, $altIdA, $altIdB, $aliasA, $aliasB, $detMethod, $author, $pub, $orgA, $orgB, $intType, $sourceDb, $intID, $conf) = @flds;&#xD;&#xA;    		## Here you can add logic to limit the interactions by specific detection method codes ($detMethod)&#xD;&#xA;    		## or database of origin etc. &#xD;&#xA;    		&#xD;&#xA;    		## Print&#xD;&#xA;    		print &quot;$line\n&quot;&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;I tested it by using a file with the UniProt ID for human TP53 (P04637) and it returns a list of 3988 interactions (showing 10 randomly selected results below):&#xD;&#xA;&#xD;&#xA;    $ foo.pl names.txt | shuf -n 10&#xD;&#xA;    uniprotkb:Q9BWC9	uniprotkb:P04637	-	-	uniprotkb:CCDC106(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0007&quot;(anti tag coimmunoprecipitation)	Zhou, J. et al.(2010)	pubmed:20159018	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-7812926	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:O15126	-	-	uniprotkb:TP53(gene_name)	uniprotkb:SCAMP1(gene_name)	psi-mi:&quot;MI:0018&quot;(two hybrid)	Lim, J. et al.(2006)	pubmed:16713569	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:720622	-&#xD;&#xA;    uniprotkb:P63165	uniprotkb:P04637	-	-	uniprotkb:SUMO1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0018&quot;(two hybrid)	Minty, A. et al.(2000)	pubmed:10961991	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:262339	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:P31350	-	-	uniprotkb:TP53(gene_name)	uniprotkb:RRM2(gene_name)	psi-mi:&quot;MI:0416&quot;(fluorescence microscopy)	Xue, L. et al.(2003)	pubmed:12615712	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0465&quot;(dip)	dip:DIP-40167E	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q00987	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MDM2(gene_name)	psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)	Dai, MS. et al.(2004)	pubmed:15308643	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:478073	-&#xD;&#xA;    uniprotkb:Q99576	uniprotkb:P04637	-	-	uniprotkb:TSC22D3(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0428&quot;(imaging technique)	Ayroldi, E. et al.(2015)	pubmed:25168242	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:1255896	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q00987	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MDM2(gene_name)	psi-mi:&quot;MI:0415&quot;(enzymatic study)	Lui, K. et al.(2013)	pubmed:23572512	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:859223	-&#xD;&#xA;    uniprotkb:P25685	uniprotkb:P04637	-	-	uniprotkb:DNAJB1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)	Qi, M. et al.(2014)	pubmed:24361594	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:938952	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q8IW41	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MAPKAPK5(gene_name)	psi-mi:&quot;MI:0424&quot;(protein kinase assay)	Sun, P. et al.(2007)	pubmed:17254968	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-1202077	-&#xD;&#xA;    uniprotkb:Q13526	uniprotkb:P04637	-	-	uniprotkb:PIN1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0096&quot;(pull down)	Mantovani, F. et al.(2007)	pubmed:17906639	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-6112688	-&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;  [1]: http://psidev.sourceforge.net/molecular_interactions/xml/doc/user/&#xD;&#xA;  [2]: http://www.ebi.ac.uk/Tools/webservices/psicquic/view/main.xhtml" />
  <row Id="1757" PostHistoryTypeId="2" PostId="578" RevisionGUID="b448dbd5-33fc-4301-a38c-e4345b6bf455" CreationDate="2017-06-08T10:38:42.810" UserId="691" Text="I’m trying to find common names from a list of latin ones (not all will have them though). I was attempting to use taxize in R but it aborts if it doesn’t find an entry in EOL and I don’t know a way around this other than manually editing the list- in which case I might as well check all this way. Does anyone have a better way of doing this?" />
  <row Id="1758" PostHistoryTypeId="1" PostId="578" RevisionGUID="b448dbd5-33fc-4301-a38c-e4345b6bf455" CreationDate="2017-06-08T10:38:42.810" UserId="691" Text="common names from latin ones?" />
  <row Id="1759" PostHistoryTypeId="3" PostId="578" RevisionGUID="b448dbd5-33fc-4301-a38c-e4345b6bf455" CreationDate="2017-06-08T10:38:42.810" UserId="691" Text="&lt;r&gt;&lt;database&gt;" />
  <row Id="1760" PostHistoryTypeId="2" PostId="579" RevisionGUID="c487969f-361c-4038-ba3d-1754499d0a4c" CreationDate="2017-06-08T10:44:47.550" UserId="71" Text="use the NCBI taxon dump under ftp://ftp.ncbi.nih.gov/pub/taxonomy&#xD;&#xA;&#xD;&#xA;in taxdmp.zip you'll find all the names for a given NCBI taxon&#xD;&#xA;&#xD;&#xA;    $ grep -w ^9606 names.dmp &#xD;&#xA;    9606	|	Homo sapiens	|		|	scientific name	|&#xD;&#xA;    9606	|	Homo sapiens Linnaeus, 1758	|		|	authority	|&#xD;&#xA;    9606	|	human	|		|	genbank common name	|&#xD;&#xA;    9606	|	man	|		|	common name	|&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1761" PostHistoryTypeId="2" PostId="580" RevisionGUID="39a223d5-82da-49a7-9226-1eb41128a11d" CreationDate="2017-06-08T10:51:40.737" UserId="235" Text="miRbase is a database of miRNA sequences, their genomic locations and the evidence for their existence. It is not a database of their targets, so I am assuming that this is the information you are asking for. &#xD;&#xA;&#xD;&#xA;Despite not having been updated since 2014, as far as I am aware miRBase is still the go to place for this data. For example, [courses at the European Bioinformatics Institute][1] were still teaching the use of miRBase in April this year (2017). &#xD;&#xA;&#xD;&#xA;[RNACentral](http://rnacentral.org/) is repository for all non-coding RNAs, but as it is a synthesis of other databases, including miRBase, I suspect data their will still be largely the same as miRBase. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://blog.rnacentral.org/2015/04/new-training-course-online-resources.html" />
  <row Id="1762" PostHistoryTypeId="2" PostId="581" RevisionGUID="306280d5-8735-4102-97e5-4e6dc6876fd5" CreationDate="2017-06-08T10:57:53.833" UserId="298" Text="As Pierre suggested, you can get a dump of such names from [NCBI][1]. Then, to query it for the common name for a species using its Latin name, you can do:&#xD;&#xA;&#xD;&#xA;    $ awk -F'\t' -vname=&quot;Mus musculus&quot; '($7==&quot;scientific name&quot; &amp;&amp; $3==name){&#xD;&#xA;                                          a[$1]=$3&#xD;&#xA;                                        } &#xD;&#xA;                                        ($1 in a &amp;&amp; /genbank common name/){  &#xD;&#xA;                                          print $3&#xD;&#xA;                                        }' names.dmp&#xD;&#xA;    house mouse&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;  [1]: ftp://ftp.ncbi.nih.gov/pub/taxonomy" />
  <row Id="1763" PostHistoryTypeId="5" PostId="581" RevisionGUID="5aa871f3-f606-42a9-ab3a-a37251e34933" CreationDate="2017-06-08T11:02:41.800" UserId="298" Comment="added 66 characters in body" Text="As Pierre [suggested][1], you can get a dump of such names from [NCBI][2]. Then, to query it for the common name for a species using its Latin name, you can do:&#xD;&#xA;&#xD;&#xA;    $ awk -F'\t' -vname=&quot;Mus musculus&quot; '($7==&quot;scientific name&quot; &amp;&amp; $3==name){&#xD;&#xA;                                          a[$1]=$3&#xD;&#xA;                                        } &#xD;&#xA;                                        ($1 in a &amp;&amp; /genbank common name/){  &#xD;&#xA;                                          print $3&#xD;&#xA;                                        }' names.dmp&#xD;&#xA;    house mouse&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/579/298&#xD;&#xA;  [2]: ftp://ftp.ncbi.nih.gov/pub/taxonomy" />
  <row Id="1764" PostHistoryTypeId="2" PostId="582" RevisionGUID="a6017a91-edde-48a9-a0c1-6a6d31e3b2a0" CreationDate="2017-06-08T11:03:51.673" UserId="191" Text="As [Pierre mentions][1], NCBI is a good source for this kind of transformation.&#xD;&#xA;&#xD;&#xA;You can still use `taxize` to perform the conversion.&#xD;&#xA;&#xD;&#xA;    library(&quot;taxize&quot;)&#xD;&#xA;    &#xD;&#xA;    sci2comm(c('Helianthus annuus', 'Homo sapiens', 'Rattus rattus', 'Mus musculus'), db = 'ncbi')&#xD;&#xA;&#xD;&#xA;    ## $`Helianthus annuus`&#xD;&#xA;    ## [1] &quot;common sunflower&quot;&#xD;&#xA;    ##&#xD;&#xA;    ## $`Homo sapiens`&#xD;&#xA;    ## [1] &quot;human&quot;&#xD;&#xA;    ## &#xD;&#xA;    ## $`Rattus rattus`&#xD;&#xA;    ## [1] &quot;black rat&quot;&#xD;&#xA;    ## &#xD;&#xA;    ## $`Mus musculus`&#xD;&#xA;    ## [1] &quot;house mouse&quot;&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;You can also try a different database, e.g. `db='itis'`.&#xD;&#xA;&#xD;&#xA;The advantage of this approach is that you do not have to parse a file yourself. The downside is that it can be slow for large list, since for every species a database request is performed.&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/579/191&#xD;&#xA;" />
  <row Id="1765" PostHistoryTypeId="5" PostId="582" RevisionGUID="1118df06-3dca-4e04-9c12-5e0c8754523e" CreationDate="2017-06-08T11:10:46.423" UserId="191" Comment="add example of missing entry" Text="As [Pierre mentions][1], NCBI is a good source for this kind of transformation.&#xD;&#xA;&#xD;&#xA;You can still use `taxize` to perform the conversion.&#xD;&#xA;&#xD;&#xA;    library(&quot;taxize&quot;)&#xD;&#xA;    &#xD;&#xA;    sci2comm(c('Helianthus annuus', 'Mycobacterium bovis', 'Homo sapiens', 'Rattus rattus', 'Mus musculus'), db = 'ncbi')&#xD;&#xA;&#xD;&#xA;    ## $`Helianthus annuus`&#xD;&#xA;    ## [1] &quot;common sunflower&quot;&#xD;&#xA;    ##&#xD;&#xA;    ## $`Mycobacterium bovis`&#xD;&#xA;    ## character(0)&#xD;&#xA;    ##&#xD;&#xA;    ## $`Homo sapiens`&#xD;&#xA;    ## [1] &quot;human&quot;&#xD;&#xA;    ## &#xD;&#xA;    ## $`Rattus rattus`&#xD;&#xA;    ## [1] &quot;black rat&quot;&#xD;&#xA;    ## &#xD;&#xA;    ## $`Mus musculus`&#xD;&#xA;    ## [1] &quot;house mouse&quot;&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;This works correctly if no common name is available. You can also try a different database, e.g. `db='itis'`.&#xD;&#xA;&#xD;&#xA;The advantage of this approach is that you do not have to parse a file yourself. The downside is that it can be slow for large list, since for every species a database request is performed.&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/579/191&#xD;&#xA;" />
  <row Id="1766" PostHistoryTypeId="4" PostId="568" RevisionGUID="60311638-ae75-481c-afbf-52eee69a846c" CreationDate="2017-06-08T11:20:04.643" UserId="298" Comment="How to is a declaration not a question" Text="How can I create my own GO association file (gaf)?" />
  <row Id="1767" PostHistoryTypeId="24" PostId="568" RevisionGUID="60311638-ae75-481c-afbf-52eee69a846c" CreationDate="2017-06-08T11:20:04.643" Comment="Proposed by 298 approved by 77, 311 edit id of 154" />
  <row Id="1768" PostHistoryTypeId="5" PostId="582" RevisionGUID="91bb861c-fe6a-4a84-b2b0-6c102b4f5eb1" CreationDate="2017-06-08T11:23:12.190" UserId="191" Comment="now working with incorrect species names" Text="As [Pierre mentions][1], NCBI is a good source for this kind of transformation.&#xD;&#xA;&#xD;&#xA;You can still use `taxize` to perform the conversion.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt; &#xD;&#xA;&#xD;&#xA;    library(&quot;taxize&quot;)&#xD;&#xA;    &#xD;&#xA;    species &lt;- c('Helianthus annuus', 'Mycobacterium bovis', 'Rattus rattus', 'XX', 'Mus musculus')&#xD;&#xA;    &#xD;&#xA;    uids &lt;- get_uid(species)&#xD;&#xA;    &#xD;&#xA;    # keep only uids which you have in the database&#xD;&#xA;    uids.found &lt;- as.uid(uids[!is.na(uids)])&#xD;&#xA;    # keep only species names  corresponding to your ids&#xD;&#xA;    species.found &lt;- species[!is.na(uids)]&#xD;&#xA;    &#xD;&#xA;    common.names &lt;- sci2comm(uids.found, db = 'ncbi')&#xD;&#xA;    names(common.names) &lt;- species.found&#xD;&#xA;&#xD;&#xA;    common.names&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;This works correctly if no common name is available, or no species in the database. You can also try a different database, e.g. `db='itis'`.&#xD;&#xA;&#xD;&#xA;The advantage of this approach is that you do not have to parse a file yourself. The downside is that it can be slow for large list, since for every species a database request is performed.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/579/191&#xD;&#xA;" />
  <row Id="1769" PostHistoryTypeId="5" PostId="582" RevisionGUID="78fa11b9-62d6-451f-8001-8f2e14b6bc50" CreationDate="2017-06-08T11:29:43.307" UserId="191" Comment="show output" Text="As [Pierre mentions][1], NCBI is a good source for this kind of transformation.&#xD;&#xA;&#xD;&#xA;You can still use `taxize` to perform the conversion.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt; &#xD;&#xA;&#xD;&#xA;    library(&quot;taxize&quot;)&#xD;&#xA;    &#xD;&#xA;    species &lt;- c('Helianthus annuus', 'Mycobacterium bovis', 'Rattus rattus', 'XX', 'Mus musculus')&#xD;&#xA;    &#xD;&#xA;    uids &lt;- get_uid(species)&#xD;&#xA;    &#xD;&#xA;    # keep only uids which you have in the database&#xD;&#xA;    uids.found &lt;- as.uid(uids[!is.na(uids)])&#xD;&#xA;    # keep only species names  corresponding to your ids&#xD;&#xA;    species.found &lt;- species[!is.na(uids)]&#xD;&#xA;    &#xD;&#xA;    common.names &lt;- sci2comm(uids.found, db = 'ncbi')&#xD;&#xA;    names(common.names) &lt;- species.found&#xD;&#xA;&#xD;&#xA;    common.names&#xD;&#xA;&#xD;&#xA;    ## output:&#xD;&#xA;    ## $`Helianthus annuus`&#xD;&#xA;    ## [1] &quot;common sunflower&quot;&#xD;&#xA;    ## &#xD;&#xA;    ## $`Mycobacterium bovis`&#xD;&#xA;    ## character(0)&#xD;&#xA;    ## &#xD;&#xA;    ## $`Rattus rattus`&#xD;&#xA;    ## [1] &quot;black rat&quot;&#xD;&#xA;    ## &#xD;&#xA;    ## $`Mus musculus`&#xD;&#xA;    ## [1] &quot;house mouse&quot;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;This works correctly if no common name is available, or no species in the database. You can also try a different database, e.g. `db='itis'`.&#xD;&#xA;&#xD;&#xA;The advantage of this approach is that you do not have to parse a file yourself. The downside is that it can be slow for large list, since for every species a database request is performed.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/579/191&#xD;&#xA;" />
  <row Id="1770" PostHistoryTypeId="5" PostId="578" RevisionGUID="c704480e-882b-4c9b-8c68-d2738a62eb56" CreationDate="2017-06-08T11:33:58.550" UserId="191" Comment="mostly formatting &amp; language" Text="I’m trying to find common names from a list of scientific names (not all will have them though).&#xD;&#xA;&#xD;&#xA;I was attempting to use `taxize` in R but it aborts if it doesn’t find an entry in EOL and I don’t know a way around this other than manually editing the list - in which case I might as well check all this way.&#xD;&#xA;&#xD;&#xA;Does anyone have a better way of doing this?" />
  <row Id="1771" PostHistoryTypeId="4" PostId="578" RevisionGUID="c704480e-882b-4c9b-8c68-d2738a62eb56" CreationDate="2017-06-08T11:33:58.550" UserId="191" Comment="mostly formatting &amp; language" Text="How to convert species names into common names?" />
  <row Id="1772" PostHistoryTypeId="6" PostId="578" RevisionGUID="c704480e-882b-4c9b-8c68-d2738a62eb56" CreationDate="2017-06-08T11:33:58.550" UserId="191" Comment="mostly formatting &amp; language" Text="&lt;r&gt;&lt;database&gt;&lt;taxonomy&gt;" />
  <row Id="1773" PostHistoryTypeId="24" PostId="578" RevisionGUID="c704480e-882b-4c9b-8c68-d2738a62eb56" CreationDate="2017-06-08T11:33:58.550" Comment="Proposed by 191 approved by 77, 73 edit id of 155" />
  <row Id="1774" PostHistoryTypeId="5" PostId="570" RevisionGUID="a190a6b0-f61e-446e-9676-107e5234f8e0" CreationDate="2017-06-08T11:34:12.590" UserId="668" Comment="used code block for formatting" Text="I have many alignments from Rfam Database, and I would like to edit them. &#xD;&#xA;I saw that many tools are used for Protein sequence alignments, but there is something specific to edit RNA alignments ? &#xD;&#xA;&#xD;&#xA;e.g. Stockholm Alignment of Pistol (just few entries). &#xD;&#xA; &#xD;&#xA;&#xD;&#xA;    FP929053.1/1669026-1668956   AGUGGUCACAGCCACUAUAAACA-GGGCUU-UAAGCUGUG-AGCGUUGACCGUC----------ACAA-----CGGCGGUCAGGUAGUC&#xD;&#xA;    AFOX01000025.1/1981-1912     ACUCGUCUGAGCGAGUAUAAACA-GGUCAU-UAAGCUCAG-AGCGUUCACCGGG----------AUCA------UUCGGUGAGGUUGGC&#xD;&#xA;    HE577054.1/3246821-3246752   ACUCGUCUGAGCGAGUAUAAACA-GGUCAU-UAAGCUCAG-AGCGUUCACCGGG----------AUCA------UGCGGUGAGGUUGGC&#xD;&#xA;    CP000154.1/3364237-3364168   GUUCGUCUGAGCGAACGCAAACA-GGCCAU-UAAGCUCAG-AGCGUUCACUGGA----------UUCG------UCCAGUGAGAUUGGC`&#xD;&#xA;    `#=GC SS_cons                 &lt;&lt;&lt;&lt;__AAAAA_&gt;&gt;&gt;&gt;-------..&lt;&lt;&lt;&lt;-.----aaaaa.----&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;..........____....._&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;--&gt;&gt;&gt;&gt;`&#xD;&#xA;    `#=GC RF                      acUCGUCuggGCGAguAUAAAuA..cgCaU.UAgGCccaG.AGCGUcccggcgg..........uUau.....uccgccgggGGUuGcg&#xD;&#xA;    //" />
  <row Id="1775" PostHistoryTypeId="24" PostId="570" RevisionGUID="a190a6b0-f61e-446e-9676-107e5234f8e0" CreationDate="2017-06-08T11:34:12.590" Comment="Proposed by 668 approved by 77, 73 edit id of 151" />
  <row Id="1777" PostHistoryTypeId="5" PostId="539" RevisionGUID="eb21562d-3c43-47a2-9b64-1ee80bf38722" CreationDate="2017-06-08T11:39:46.183" UserId="73" Comment="added 85 characters in body" Text="questions involving microarray analysis (e.g. Illumina BeadChip, Affymetrix GeneChip)" />
  <row Id="1778" PostHistoryTypeId="24" PostId="539" RevisionGUID="eb21562d-3c43-47a2-9b64-1ee80bf38722" CreationDate="2017-06-08T11:39:46.183" Comment="Proposed by 73 approved by 77, 55 edit id of 130" />
  <row Id="1779" PostHistoryTypeId="5" PostId="432" RevisionGUID="e1098243-68f5-48dd-be2f-1d27f5d50888" CreationDate="2017-06-08T11:39:57.483" UserId="57" Comment="excerpt changed to describe type of question associated with it " Text="Questions associated with the generation or analysis of data from sequencer MinION. For questions about long reads, use tag \long-reads instead." />
  <row Id="1780" PostHistoryTypeId="24" PostId="432" RevisionGUID="e1098243-68f5-48dd-be2f-1d27f5d50888" CreationDate="2017-06-08T11:39:57.483" Comment="Proposed by 57 approved by 77, 55 edit id of 115" />
  <row Id="1781" PostHistoryTypeId="6" PostId="508" RevisionGUID="ae3f0bc7-de36-4b7a-b9c0-d1a877957480" CreationDate="2017-06-08T11:40:57.807" UserId="55" Comment="edited tags" Text="&lt;bam&gt;&lt;alignment&gt;&lt;bwa&gt;" />
  <row Id="1782" PostHistoryTypeId="6" PostId="202" RevisionGUID="4743e62e-1cf1-4aad-9b08-e394a9162cef" CreationDate="2017-06-08T11:41:29.360" UserId="163" Comment="standardise nanopore tag" Text="&lt;nanopore&gt;&lt;genome-sequencing&gt;&lt;3rd-gen-sequencing&gt;&lt;software-recommendation&gt;" />
  <row Id="1783" PostHistoryTypeId="24" PostId="202" RevisionGUID="4743e62e-1cf1-4aad-9b08-e394a9162cef" CreationDate="2017-06-08T11:41:29.360" Comment="Proposed by 163 approved by 77, 55 edit id of 138" />
  <row Id="1784" PostHistoryTypeId="5" PostId="473" RevisionGUID="c01756b0-85a0-42ad-bacb-ba550d406f2f" CreationDate="2017-06-08T11:42:01.353" UserId="57" Comment="changing bit meaning of tag to cover all nanopore sequencers, defining similar tags" Text="Questions specific to nanopore sequencing. For general question about long reads, use tag long-reads instead and for questions about specific sequencer use a specific sequencer tag (i.e. minion, gridion)" />
  <row Id="1785" PostHistoryTypeId="24" PostId="473" RevisionGUID="c01756b0-85a0-42ad-bacb-ba550d406f2f" CreationDate="2017-06-08T11:42:01.353" Comment="Proposed by 57 approved by 77, 55 edit id of 114" />
  <row Id="1786" PostHistoryTypeId="6" PostId="488" RevisionGUID="1ddbc7ef-3a43-426c-a300-926d44923eec" CreationDate="2017-06-08T11:43:09.197" UserId="55" Comment="edited tags" Text="&lt;database&gt;&lt;gene&gt;&lt;conversion&gt;" />
  <row Id="1787" PostHistoryTypeId="6" PostId="486" RevisionGUID="08e4a074-0499-4e4a-8960-1e8e2e8c40b6" CreationDate="2017-06-08T11:43:24.667" UserId="55" Comment="edited tags" Text="&lt;read-mapping&gt;&lt;metagenome&gt;" />
  <row Id="1788" PostHistoryTypeId="6" PostId="481" RevisionGUID="b757c166-30c3-43f9-9465-61fbb1ade483" CreationDate="2017-06-08T11:43:42.683" UserId="55" Comment="edited tags" Text="&lt;vcf&gt;&lt;ancestry&gt;&lt;personal-genomics&gt;" />
  <row Id="1789" PostHistoryTypeId="5" PostId="380" RevisionGUID="73e543c4-85ec-4c30-ae48-2ca867c62487" CreationDate="2017-06-08T11:49:09.380" UserId="292" Comment="Added a pyGATB based solution" Text="With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (including gzipped versions), so the above command should also handle fastq format robustly.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters (got it from &lt;https://unix.stackexchange.com/a/169575/55127&gt;). I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)&#xD;&#xA;&#xD;&#xA;### Edit&#xD;&#xA;&#xD;&#xA;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;https://bioinformatics.stackexchange.com/a/365/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &lt; /Genomes/C_elegans/Caenorhabditis_elegans/Ensembl/WBcel235/Sequence/WholeGenomeFasta/genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.976s&#xD;&#xA;    user	0m0.876s&#xD;&#xA;    sys 	0m0.100s&#xD;&#xA;&#xD;&#xA;Comparing with something adapted from this answer: &lt;https://bioinformatics.stackexchange.com/a/363/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&gt;')))&quot; &lt; /Genomes/C_elegans/Caenorhabditis_elegans/Ensembl/WBcel235/Sequence/WholeGenomeFasta/genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.143s&#xD;&#xA;    user	0m1.100s&#xD;&#xA;    sys 	0m0.040s&#xD;&#xA;&#xD;&#xA;(It is slower with python 3.6 than with python 2.7 on my computer)&#xD;&#xA;&#xD;&#xA;### New edit (08/06/2017)&#xD;&#xA;&#xD;&#xA;I just learned that [GATB](http://gatb.inria.fr/) includes a fasta/fastq parser and has recently released a python API. I tried to use it yesterday to test another answer to the present question and [found a bug](https://github.com/GATB/pyGATB/issues/2). This bug is now fixed, so here is a pyGATB-based anwser:&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; bank = Bank(\&quot;/Genomes/C_elegans/Caenorhabditis_elegans/Ensembl/WBcel235/Sequence/WholeGenomeFasta/genome.fa\&quot;); print(sum((seq.sequence.decode(\&quot;utf-8\&quot;).count(\&quot;A\&quot;) for seq in bank)))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.687s&#xD;&#xA;    user	0m0.596s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;This is almost as fast as the `grep`, `tr`, `wc` pipeline." />
  <row Id="1790" PostHistoryTypeId="5" PostId="481" RevisionGUID="bc2959b6-d31c-4817-8738-6a405a8296e4" CreationDate="2017-06-08T11:50:57.103" UserId="73" Comment="quoted" Text="*This is a question from [/u/beneficii9](https://www.reddit.com/user/beneficii9) on reddit. The original post can be found [here](https://www.reddit.com/r/bioinformatics/comments/6f040g/how_do_you_do_admixture_testing_with_a_whole/).*&#xD;&#xA;&#xD;&#xA;&gt; Through the Personal Genome Project, I have had my whole genome sequenced by Veritas, and have it in the form of a single VCF file for the whole genome and one BAS file for each chromosome. The reference genome associated with the VCF file is hg19. It has been helpful in health data; for example, I discovered I'm homozygous for the non-functional variant CYP-2D6 gene ([rs3892097](https://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=3892097)), which can render several common medications useless, and helps explain why some medicines didn't really work for me. My doctor has found this information very helpful.&#xD;&#xA;&#xD;&#xA;&gt; Unfortunately, I can't find any way of looking at admixture or ancestry. I've tried setting everything up using a combination of VCFTools, Plink1.9, and ADMIXTURE, but I can't get it to work. I think for ADMIXTURE you have to have a bunch of genomes sorted by geographical origin to compare your genome against, but I'm not sure how to do that, and what's online isn't very clear to me. So scratch that one off.&#xD;&#xA;&#xD;&#xA;&gt; I've tried converting the file to 23andme format (and at this [/u/psychosomaticism](https://www.reddit.com/u/psychosomaticism) has been very helpful). I did that (though it seems there were problems because of the way the VCF file was set up). But the websites that take the data want you to point them to your 23andme account, and that doesn't really work if you only have the file. 23andme doesn't provide for people who had their whole genomes sequenced. They want you to give them a saliva sample like everyone else.&#xD;&#xA;&#xD;&#xA;&gt; So, what can I do?" />
  <row Id="1791" PostHistoryTypeId="6" PostId="202" RevisionGUID="135e88a6-d24a-4398-b44a-92c77ff798f5" CreationDate="2017-06-08T11:52:35.820" UserId="73" Comment="removed 3GS tag -- this doesn't apply to PacBio" Text="&lt;nanopore&gt;&lt;genome-sequencing&gt;&lt;software-recommendation&gt;" />
  <row Id="1792" PostHistoryTypeId="4" PostId="567" RevisionGUID="712648b8-4e00-4c3d-8734-8f3aab52b137" CreationDate="2017-06-08T11:55:26.413" UserId="298" Comment="How to is a declaration not a question" Text="How can I build a protein network pathway from a gene expression quantification file?" />
  <row Id="1793" PostHistoryTypeId="24" PostId="567" RevisionGUID="712648b8-4e00-4c3d-8734-8f3aab52b137" CreationDate="2017-06-08T11:55:26.413" Comment="Proposed by 298 approved by 73, 77 edit id of 156" />
  <row Id="1794" PostHistoryTypeId="2" PostId="583" RevisionGUID="70701cb6-4314-4762-bac1-ad6b2560c6a7" CreationDate="2017-06-08T12:07:20.897" UserId="752" Text="I calculated an AMOVA from a genind object, with one hierarchical factor. In the table I obtain there are SSD values (for my grouping factor,&quot;Error&quot; and total) and sigma2 values (for my grouping factor and &quot;Error&quot;)&#xD;&#xA;&#xD;&#xA;I have two questions:&#xD;&#xA;&#xD;&#xA;- What does &quot;error&quot; stand for?&#xD;&#xA;- How do I calculate Fst? Which values do I have to use?" />
  <row Id="1795" PostHistoryTypeId="1" PostId="583" RevisionGUID="70701cb6-4314-4762-bac1-ad6b2560c6a7" CreationDate="2017-06-08T12:07:20.897" UserId="752" Text="How to calculate Fst from AMOVA" />
  <row Id="1796" PostHistoryTypeId="3" PostId="583" RevisionGUID="70701cb6-4314-4762-bac1-ad6b2560c6a7" CreationDate="2017-06-08T12:07:20.897" UserId="752" Text="&lt;r&gt;" />
  <row Id="1797" PostHistoryTypeId="2" PostId="584" RevisionGUID="48ce6071-f2c1-4b14-bbea-7d38e8d9d82d" CreationDate="2017-06-08T12:07:22.530" UserId="208" Text="I have a list of ~500 differentially expressed genes between two conditions (cell types), together with fold changes, expression means etc., produced with DESeq2 on bulk RNA-seq data.&#xD;&#xA;&#xD;&#xA;What are the best practices to process this list?&#xD;&#xA;&#xD;&#xA;I've found PPI analysis on [NetworkAnalyst][1], and  GO and pathway analysis on [InnateDB][2] useful.&#xD;&#xA;&#xD;&#xA;What other options are available?&#xD;&#xA;&#xD;&#xA;I'm particularly interested in approaches using `R`.&#xD;&#xA;&#xD;&#xA;  [1]: http://networkanalyst.ca&#xD;&#xA;  [2]: http://innatedb.com &quot;InnateDB&quot;" />
  <row Id="1798" PostHistoryTypeId="1" PostId="584" RevisionGUID="48ce6071-f2c1-4b14-bbea-7d38e8d9d82d" CreationDate="2017-06-08T12:07:22.530" UserId="208" Text="What are the ways to process a list of differentially expressed genes?" />
  <row Id="1799" PostHistoryTypeId="3" PostId="584" RevisionGUID="48ce6071-f2c1-4b14-bbea-7d38e8d9d82d" CreationDate="2017-06-08T12:07:22.530" UserId="208" Text="&lt;rna-seq&gt;&lt;r&gt;&lt;deseq2&gt;&lt;networks&gt;" />
  <row Id="1800" PostHistoryTypeId="6" PostId="583" RevisionGUID="93ef8f8a-90ca-4937-b9f2-57bcd2bb2290" CreationDate="2017-06-08T12:17:05.633" UserId="298" Comment="Added the new &quot;amova&quot; tag" Text="&lt;r&gt;&lt;amova&gt;" />
  <row Id="1801" PostHistoryTypeId="24" PostId="583" RevisionGUID="93ef8f8a-90ca-4937-b9f2-57bcd2bb2290" CreationDate="2017-06-08T12:17:05.633" Comment="Proposed by 298 approved by 752 edit id of 157" />
  <row Id="1802" PostHistoryTypeId="2" PostId="585" RevisionGUID="7bd001b1-1cc9-4ed6-8777-3cd185ecbc91" CreationDate="2017-06-08T12:38:57.067" UserId="73" Text="You've asked a very broad question, so I'll try to demonstrate why this is such a hard question to answer. I've done two fairly large differential analysis studies (and a few smaller ones) covering very different areas of research, and the approaches that other researchers used subsequent to my differential expression calculations were unsurprisingly also very different.&#xD;&#xA;&#xD;&#xA;In the [first study](www.sciencedirect.com/science/article/pii/S221112471401047X), I did a *de-novo* transcriptome assembly of *Schmidtea mediterranea (Smed)* from Illumina RNASeq reads, then carried out a DE analysis on a *β-catenin* knockout mutant population compared to wildtype. The other researchers had some very impressive tools available at their disposal, allowing them to subsequently visualise the positional expression of differentially-expressed genes in the knockout and wildtype populations after amputation. They found a few candidate genes that had graded expression along the anterior-posterior axis in the wildtype population, and this expression disappeared in the knockout mutants. One of these genes was a *teashirt (tsh)* family member, and a good candidate for further discovery based on the researchers' existing knowledge of the gene pathways associated with cellular regeneration. To cut a very long story short, they experimentally validated the *teashirt* link through further RNAi studies, generating a double-headed *Smed* as a result. This was followed up in Zebrafish *(Danio rerio)*, because there happened to be another building about 100m away that specialised in Zebrafish development.&#xD;&#xA;&#xD;&#xA;*[to be updated with the [second study](https://www.growkudos.com/publications/10.1084%252Fjem.20160470#!) shortly]*" />
  <row Id="1803" PostHistoryTypeId="5" PostId="584" RevisionGUID="e400e87a-1c30-4081-b31b-ac8255c0a27c" CreationDate="2017-06-08T12:53:17.433" UserId="208" Comment="Explained experiment" Text="We are studying six different human macrophage/dendritic cell types isolated from healthy skin. They all differ from each other in a few cell surface markers.&#xD;&#xA;&#xD;&#xA;We are interested in the characteristics of each cell type (&quot;marker&quot; genes or biological processes), the differences between them (especially cell surface proteins), and their relations (e.g. &quot;which are the closest relatives?&quot;) as can be inferred from the transcriptome, from an immunological point of view. The wider context is HIV-infection, thus HIV infection-related differences (and similarities) are of particular interest.&#xD;&#xA;&#xD;&#xA;One naive approach is to contrast two of the most similar cell types (as inferred from previous knowledge, the sorting strategy and the PCA plot), and produce a list of differentially expressed (DE) genes.&#xD;&#xA;I have now a list of ~500 DE genes between two cell types, together with fold changes, expression means etc., produced with DESeq2 on bulk RNA-seq data.&#xD;&#xA;&#xD;&#xA;What are the best practices to process this list to answer some of the above?&#xD;&#xA;&#xD;&#xA;I've found PPI analysis on [NetworkAnalyst][1], and  GO and pathway analysis on [InnateDB][2] useful.&#xD;&#xA;&#xD;&#xA;What other options are available?&#xD;&#xA;&#xD;&#xA;I'm particularly interested in approaches using `R`.&#xD;&#xA;&#xD;&#xA;  [1]: http://networkanalyst.ca&#xD;&#xA;  [2]: http://innatedb.com &quot;InnateDB&quot;" />
  <row Id="1804" PostHistoryTypeId="5" PostId="583" RevisionGUID="ddb2cd69-85b8-4454-b9d0-bb93f7931b40" CreationDate="2017-06-08T12:59:17.373" UserId="752" Comment="added 833 characters in body" Text="I calculated an AMOVA from a genind object, with one hierarchical factor. In the table I obtain there are SSD values (for my grouping factor,&quot;Error&quot; and total) and sigma2 values (for my grouping factor and &quot;Error&quot;)&#xD;&#xA;&#xD;&#xA;I have two questions:&#xD;&#xA;&#xD;&#xA;- What does &quot;error&quot; stand for?&#xD;&#xA;- How do I calculate Fst? Which values do I have to use?&#xD;&#xA;&#xD;&#xA;A genind object in R is an object which contains allelic information about a set of individuals.  A genind object in R is an object which contains allelic information about a set of individuals.&#xD;&#xA;This is the table that I get:  &lt;br/&gt;&#xD;&#xA;A genind object in R is an object which contains allelic information about a set of individuals.  &lt;br/&gt;&#xD;&#xA;This is the table that I get:  &lt;br/&gt;&#xD;&#xA;	Analysis of Molecular Variance  &lt;br/&gt;&#xD;&#xA;&#xD;&#xA;Call: amova(formula = plantula.dist ~ g, nperm = 100)  &lt;br/&gt;&#xD;&#xA;&#xD;&#xA;           SSD         MSD  df  &lt;br/&gt;&#xD;&#xA;g     354992.9 354992.9297   1  &lt;br/&gt;&#xD;&#xA;Error 310814.9    693.7834 448  &lt;br/&gt;&#xD;&#xA;Total 665807.9   1482.8683 449  &lt;br/&gt;&#xD;&#xA;&#xD;&#xA;Variance components:  &lt;br/&gt;&#xD;&#xA;       sigma2 P.value  &lt;br/&gt;&#xD;&#xA;g     1574.69       0  &lt;br/&gt;&#xD;&#xA;Error  693.78          &lt;br/&gt;&#xD;&#xA;&#xD;&#xA;Variance coefficients:  &lt;br/&gt;&#xD;&#xA;       a   &lt;br/&gt;&#xD;&#xA;224.9956   &lt;br/&gt;&#xD;&#xA;" />
  <row Id="1805" PostHistoryTypeId="5" PostId="380" RevisionGUID="a411602a-62c5-40d9-8686-5ed17c3f61e6" CreationDate="2017-06-08T13:37:58.073" UserId="292" Comment="Tried to improve readability, emphasised that the pyGATB solution is tested with python3" Text="## Using bioawk&#xD;&#xA;&#xD;&#xA;With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (including gzipped versions), so the above command should also handle fastq format robustly.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters (got it from &lt;https://unix.stackexchange.com/a/169575/55127&gt;). I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)&#xD;&#xA;&#xD;&#xA;## Using python&#xD;&#xA;&#xD;&#xA;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;https://bioinformatics.stackexchange.com/a/365/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.976s&#xD;&#xA;    user	0m0.876s&#xD;&#xA;    sys 	0m0.100s&#xD;&#xA;&#xD;&#xA;Comparing with something adapted from this answer: &lt;https://bioinformatics.stackexchange.com/a/363/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&gt;')))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.143s&#xD;&#xA;    user	0m1.100s&#xD;&#xA;    sys 	0m0.040s&#xD;&#xA;&#xD;&#xA;(It is slower with python 3.6 than with python 2.7 on my computer)&#xD;&#xA;&#xD;&#xA;### Edit (08/06/2017)&#xD;&#xA;&#xD;&#xA;I just learned that [GATB](http://gatb.inria.fr/) includes a fasta/fastq parser and has recently released a python API. I tried to use it yesterday to test another answer to the present question and [found a bug](https://github.com/GATB/pyGATB/issues/2). This bug is now fixed, so here is a [pyGATB](https://pypi.python.org/pypi/pyGATB/0.1.2)-based answer:&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.decode(\&quot;utf-8\&quot;).count(\&quot;A\&quot;) for seq in Bank(\&quot;genome.fa\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.688s&#xD;&#xA;    user	0m0.604s&#xD;&#xA;    sys 	0m0.080s&#xD;&#xA;&#xD;&#xA;Although I used python3.6 here (pyGATB seems python3-only), this is faster than the other two python approaches (for which the reported timings are obtained with python 2.7). This is even almost as fast as the `grep`, `tr`, `wc` pipeline." />
  <row Id="1806" PostHistoryTypeId="5" PostId="380" RevisionGUID="95ad5a5e-fc89-4613-9653-1bff98feb7fa" CreationDate="2017-06-08T13:44:05.763" UserId="292" Comment="Tried to improve readability, emphasised that the pyGATB solution is tested with python3, added Biopython solution" Text="## Using bioawk&#xD;&#xA;&#xD;&#xA;With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (**including gzipped versions**), so the above command **should also handle fastq format robustly**.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters (got it from &lt;https://unix.stackexchange.com/a/169575/55127&gt;). I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)&#xD;&#xA;&#xD;&#xA;## Using python&#xD;&#xA;&#xD;&#xA;### readfq&#xD;&#xA;&#xD;&#xA;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;https://bioinformatics.stackexchange.com/a/365/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.976s&#xD;&#xA;    user	0m0.876s&#xD;&#xA;    sys 	0m0.100s&#xD;&#xA;&#xD;&#xA;### &quot;pure python&quot;&#xD;&#xA;&#xD;&#xA;Comparing with something adapted from this answer: &lt;https://bioinformatics.stackexchange.com/a/363/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&gt;')))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.143s&#xD;&#xA;    user	0m1.100s&#xD;&#xA;    sys 	0m0.040s&#xD;&#xA;&#xD;&#xA;(It is slower with python 3.6 than with python 2.7 on my computer)&#xD;&#xA;&#xD;&#xA;### pyGATB&#xD;&#xA;&#xD;&#xA;I just learned (08/06/2017) that [GATB](http://gatb.inria.fr/) includes a fasta/fastq parser and has recently released a python API. I tried to use it yesterday to test another answer to the present question and [found a bug](https://github.com/GATB/pyGATB/issues/2). This bug is now fixed, so here is a [pyGATB](https://pypi.python.org/pypi/pyGATB/0.1.2)-based answer:&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.decode(\&quot;utf-8\&quot;).count(\&quot;A\&quot;) for seq in Bank(\&quot;genome.fa\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.688s&#xD;&#xA;    user	0m0.604s&#xD;&#xA;    sys 	0m0.080s&#xD;&#xA;&#xD;&#xA;Although I used python3.6 here (pyGATB seems python3-only), this is faster than the other two python approaches (for which the reported timings are obtained with python 2.7). This is even almost as fast as the `grep`, `tr`, `wc` pipeline.&#xD;&#xA;&#xD;&#xA;### Biopython&#xD;&#xA;&#xD;&#xA;And, to have even more comparisons, here is a solution using `SeqIO.parse` from Biopython (with python2.7):&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from Bio import SeqIO; print(sum((rec.seq.count(\&quot;A\&quot;) for rec in SeqIO.parse(\&quot;genome.fa\&quot;, \&quot;fasta\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.632s&#xD;&#xA;    user	0m1.532s&#xD;&#xA;    sys 	0m0.096s&#xD;&#xA;&#xD;&#xA;This is a bit slower than the &quot;pure python&quot; solution, but perhaps more robust." />
  <row Id="1808" PostHistoryTypeId="5" PostId="583" RevisionGUID="e8776292-b90f-4976-ac72-26ad4dfc2f79" CreationDate="2017-06-08T14:11:10.127" UserId="48" Comment="Improved formatting" Text="I calculated an AMOVA from a genind object, with one hierarchical factor. In the table I obtain there are SSD values (for my grouping factor,&quot;Error&quot; and total) and sigma2 values (for my grouping factor and &quot;Error&quot;)&#xD;&#xA;&#xD;&#xA;I have two questions:&#xD;&#xA;&#xD;&#xA;- What does &quot;error&quot; stand for?&#xD;&#xA;- How do I calculate Fst? Which values do I have to use?&#xD;&#xA;&#xD;&#xA;A genind object in R is an object which contains allelic information about a set of individuals.  A genind object in R is an object which contains allelic information about a set of individuals.&#xD;&#xA;This is the table that I get: &#xD;&#xA;&#xD;&#xA;	Analysis of Molecular Variance&#xD;&#xA;&#xD;&#xA;    Call: amova(formula = plantula.dist ~ g, nperm = 100)  &#xD;&#xA;    &#xD;&#xA;               SSD         MSD  df  &#xD;&#xA;    g     354992.9 354992.9297   1  &#xD;&#xA;    Error 310814.9    693.7834 448 &#xD;&#xA;    Total 665807.9   1482.8683 449 &#xD;&#xA;    &#xD;&#xA;    Variance components:  &#xD;&#xA;           sigma2 P.value  &#xD;&#xA;    g     1574.69       0  &#xD;&#xA;    Error  693.78         &#xD;&#xA;    &#xD;&#xA;    Variance coefficients:  &#xD;&#xA;           a   &#xD;&#xA;    224.9956   &#xD;&#xA;&#xD;&#xA;" />
  <row Id="1809" PostHistoryTypeId="24" PostId="583" RevisionGUID="e8776292-b90f-4976-ac72-26ad4dfc2f79" CreationDate="2017-06-08T14:11:10.127" Comment="Proposed by 48 approved by 77, 73 edit id of 158" />
  <row Id="1810" PostHistoryTypeId="2" PostId="586" RevisionGUID="328a09b8-47e5-4487-924e-96b2aa69df06" CreationDate="2017-06-08T14:32:07.633" UserId="29" Text="The problem is that GFF, fundamentally, is a [relational](https://en.wikipedia.org/wiki/Relational_model) format: it provides tags that relate individual rows via one-to-many relationships (e.g. gene–exon). This indirectly highlights the second complication: individual rows have different types, and therefore store different attributes in the 9th column.&#xD;&#xA;&#xD;&#xA;Over the last few decades (!), we have accumulated a wealth of theory and tools to work with this kind of data. And the usual solution is to create a database schema for a relational database, and to use database drivers and database query languages (e.g. SQL, but increasingly also data-relational mappers such as LINQ and dplyr).&#xD;&#xA;&#xD;&#xA;Using a text-based format is attractive for many reasons that Devon mentions, but it is fundamentally at odds with a lot of the theory and tools for relational data. This creates an [impedance mismatch](https://en.wikipedia.org/wiki/Object-relational_impedance_mismatch).&#xD;&#xA;&#xD;&#xA;I’m convinced that, in the long run, the solution is going to be to revert to using relational databases for complex annotations. I say “revert” even though these databases already exist, they are simply often ignored in bioinformaticians’ day-to-day work (*I* never use them). Because this is objectively the best technical solution, and we have the research to back this up." />
  <row Id="1811" PostHistoryTypeId="5" PostId="585" RevisionGUID="7d504556-a121-4d44-8b1f-d2f547af48e6" CreationDate="2017-06-08T14:37:56.240" UserId="73" Comment="added summary of our second study" Text="You originally had asked a very broad question, so I'll try to demonstrate why that is such a hard question to answer. I've done two fairly large differential analysis studies (and a few smaller ones) covering very different areas of research, and the approaches that other researchers used subsequent to my differential expression calculations were unsurprisingly also very different.&#xD;&#xA;&#xD;&#xA;In the [first study](http://www.sciencedirect.com/science/article/pii/S221112471401047X), I did a *de-novo* transcriptome assembly of *Schmidtea mediterranea (Smed)* from Illumina RNASeq reads, then carried out a DE analysis (DESeq) on a *β-catenin* knockout mutant population compared to wildtype. The other researchers had some very impressive tools available at their disposal, allowing them to subsequently visualise the positional expression of differentially-expressed genes in the knockout and wildtype populations after amputation. They found a few candidate genes that had graded expression along the anterior-posterior axis in the wildtype population, and this expression disappeared in the knockout mutants. One of these genes was a *teashirt (tsh)* family member, and a good candidate for further discovery based on the researchers' existing knowledge of the gene pathways associated with cellular regeneration. To cut a very long story short, they experimentally validated the *teashirt* link through further RNAi studies, generating a double-headed *Smed* as a result. This was followed up in Zebrafish *(Danio rerio)*, because there happened to be another building about 100m away that specialised in Zebrafish development.&#xD;&#xA;&#xD;&#xA;The [second study](https://www.growkudos.com/publications/10.1084%252Fjem.20160470) was/is a bit more similar to your current situation. We looked at sorted dendritic cell expression in mice after the application of two Th2-inducing treatment conditions (injected *Nippostrongylus brasiliensis*; applied DBP+FITC) and one control condition (injected PBS). I did a differential expression analysis with [DESeq2](http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html), and developed a [Shiny App](http://shiny.rstudio.com/gallery/) for result exploration after I got tired of getting asked to do a whole bunch of mechanical follow-up tasks (mostly generating graphs, lists, and heatmaps for different gene subsets). We ended up with thousands of differentially expressed genes, so it wasn't feasible to look individually at every gene for research evidence and pathways -- one scientist spent a couple of weeks looking at one interesting-looking candidate gene. The differentially-expressed genes were fed into the oh-so-expensive [IPA](https://www.qiagenbioinformatics.com/products/ingenuity-pathway-analysis/) (which gives results, but not necessarily good ones) and [DAVID](https://david.ncifcrf.gov/), among a few others. These helped a little bit, and led to what was the central idea of our paper (i.e. that there were at least two different Th2 responses), but we still had the problem of too many &quot;significant&quot; pathways, and lots of differentially-expressed genes that were members of a whole bunch of other pathways. We tried [WGCNA](https://labs.genetics.ucla.edu/horvath/htdocs/CoexpressionNetwork/Rpackages/WGCNA/); we did a bit of [GSEA](http://software.broadinstitute.org/gsea/index.jsp); and we compared [Limma](https://bioconductor.org/packages/release/bioc/html/limma.html) and DESeq2, but still couldn't find anything concrete or obvious to demonstrate what was triggering the Th2 response. There were experimental studies as well: blocking and mutant studies to investigate the two types of Th2 response (and experimentally confirm the differential expression). Our experiments and exploration are still ongoing, but we felt it was necessary to get one paper out at the intermediate stage, so just ended up taking the top 25 differentially-expressed genes in each experiment (by fold change), and finding some way of grouping some of them together (via expert knowledge and literature searches). It kind-of-sort-of worked, but we're left with a bit of frustration in that we (and the tools we used) were basically stumped by having too many biologically-relevant results." />
  <row Id="1812" PostHistoryTypeId="6" PostId="565" RevisionGUID="94ed4cbd-c2c6-4325-b3b7-f2977c0ab89a" CreationDate="2017-06-08T14:46:51.810" UserId="73" Comment="clarified tag: annotation -&gt; sequence-annotation" Text="&lt;file-formats&gt;&lt;bed&gt;&lt;gff3&gt;&lt;sequence-annotation&gt;" />
  <row Id="1813" PostHistoryTypeId="2" PostId="587" RevisionGUID="7521d387-d536-42af-85cf-409725245be1" CreationDate="2017-06-08T14:54:57.103" UserId="110" Text="We're considering switching our storage format from BAM to CRAM. We work with human cancer samples, which may have very low prevalence variants (i.e. not diploid frequency).&#xD;&#xA;&#xD;&#xA;If we use lossy CRAM to save more space, how much will variants called from those CRAM files change? Which compression strategy has the lowest impact? &#xD;&#xA;&#xD;&#xA;Are there any other impacts on downstream tools that we're not considering?" />
  <row Id="1814" PostHistoryTypeId="1" PostId="587" RevisionGUID="7521d387-d536-42af-85cf-409725245be1" CreationDate="2017-06-08T14:54:57.103" UserId="110" Text="Do variant calls change when you call from CRAM?" />
  <row Id="1815" PostHistoryTypeId="3" PostId="587" RevisionGUID="7521d387-d536-42af-85cf-409725245be1" CreationDate="2017-06-08T14:54:57.103" UserId="110" Text="&lt;bam&gt;&lt;file-formats&gt;&lt;variant-calling&gt;&lt;storage&gt;" />
  <row Id="1816" PostHistoryTypeId="2" PostId="588" RevisionGUID="a8844131-5d8e-477c-bf83-5cea925907d8" CreationDate="2017-06-08T15:00:23.893" UserId="73" Text="I would expect that without any further insights into protein folding, a quantum method would look fairly similar to existing methods, but might have a bit more capability of exploring multiple states at the same time (without multitasking).&#xD;&#xA;&#xD;&#xA;Here's the approach used by [Rosetta](http://boinc.bakerlab.org/rosetta/rah_graphics.php), which is used in both the [Rosetta@home](http://boinc.bakerlab.org/) distributed computing project, and the [foldit](http://fold.it/portal/) distributed gaming project:&#xD;&#xA;&#xD;&#xA;&gt; Rosetta's strategy for finding low energy shapes looks like this:&#xD;&#xA;&#xD;&#xA;&gt; 1. Start with a fully unfolded chain (like a metal chain with its ends pulled).&#xD;&#xA;&gt; 2. Move a part of the chain to create a new shape.&#xD;&#xA;&gt; 3. Calculate the energy of the new shape.&#xD;&#xA;&gt; 4. Accept or reject the move depending on the change in energy.&#xD;&#xA;&gt; 5. Repeat 2 through 4 until every part of the chain has been moved a lot of times. &#xD;&#xA;&#xD;&#xA;&gt; We call this a trajectory. The end result of a trajectory is a predicted structure." />
  <row Id="1817" PostHistoryTypeId="5" PostId="380" RevisionGUID="38b23482-8f64-46e3-a459-dc037c792bd0" CreationDate="2017-06-08T15:26:53.440" UserId="292" Comment="Applied suggestion to avoid decode" Text="## Using bioawk&#xD;&#xA;&#xD;&#xA;With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (**including gzipped versions**), so the above command **should also handle fastq format robustly**.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters (got it from &lt;https://unix.stackexchange.com/a/169575/55127&gt;). I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)&#xD;&#xA;&#xD;&#xA;## Using python&#xD;&#xA;&#xD;&#xA;### readfq&#xD;&#xA;&#xD;&#xA;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;https://bioinformatics.stackexchange.com/a/365/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.976s&#xD;&#xA;    user	0m0.876s&#xD;&#xA;    sys 	0m0.100s&#xD;&#xA;&#xD;&#xA;### &quot;pure python&quot;&#xD;&#xA;&#xD;&#xA;Comparing with something adapted from this answer: &lt;https://bioinformatics.stackexchange.com/a/363/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&gt;')))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.143s&#xD;&#xA;    user	0m1.100s&#xD;&#xA;    sys 	0m0.040s&#xD;&#xA;&#xD;&#xA;(It is slower with python 3.6 than with python 2.7 on my computer)&#xD;&#xA;&#xD;&#xA;### pyGATB&#xD;&#xA;&#xD;&#xA;I just learned (08/06/2017) that [GATB](http://gatb.inria.fr/) includes a fasta/fastq parser and has recently released a python API. I tried to use it yesterday to test another answer to the present question and [found a bug](https://github.com/GATB/pyGATB/issues/2). This bug is now fixed, so here is a [pyGATB](https://pypi.python.org/pypi/pyGATB/0.1.2)-based answer:&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.count(b\&quot;A\&quot;) for seq in Bank(\&quot;genome.fa\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.663s&#xD;&#xA;    user	0m0.568s&#xD;&#xA;    sys 	0m0.092s&#xD;&#xA;&#xD;&#xA;(You can also do `sequence.decode(&quot;utf-8&quot;).count(&quot;A&quot;)` but this seems [a little slower](https://github.com/GATB/pyGATB/issues/2#issuecomment-307131176).)&#xD;&#xA;&#xD;&#xA;Although I used python3.6 here (pyGATB seems python3-only), this is faster than the other two python approaches (for which the reported timings are obtained with python 2.7). This is even almost as fast as the `grep`, `tr`, `wc` pipeline.&#xD;&#xA;&#xD;&#xA;### Biopython&#xD;&#xA;&#xD;&#xA;And, to have even more comparisons, here is a solution using `SeqIO.parse` from Biopython (with python2.7):&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from Bio import SeqIO; print(sum((rec.seq.count(\&quot;A\&quot;) for rec in SeqIO.parse(\&quot;genome.fa\&quot;, \&quot;fasta\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.632s&#xD;&#xA;    user	0m1.532s&#xD;&#xA;    sys 	0m0.096s&#xD;&#xA;&#xD;&#xA;This is a bit slower than the &quot;pure python&quot; solution, but perhaps more robust." />
  <row Id="1818" PostHistoryTypeId="2" PostId="589" RevisionGUID="890bdb2e-0cc2-4bba-bd1a-eac8e830795a" CreationDate="2017-06-08T15:27:39.173" UserId="73" Text="I would be concerned about the input data that produced these numbers. This dataset has a really odd shape with a dip at the start:&#xD;&#xA;&#xD;&#xA;    data.df &lt;- read.table(&quot;soft_threshold.txt&quot;, header=TRUE);&#xD;&#xA;    png(&quot;scale-free-194.png&quot;);&#xD;&#xA;    plot(data.df$SFT.R.sq);&#xD;&#xA;    dummy &lt;- dev.off();&#xD;&#xA;&#xD;&#xA;[![scale-free power with an odd inflection][1]][1]&#xD;&#xA;&#xD;&#xA;Here's what I expect this to look like (from the supplementary information in [this](http://www.pnas.org/content/103/47/17973) paper):&#xD;&#xA;&#xD;&#xA;[![normal scale-free topology][2]][2]&#xD;&#xA;&#xD;&#xA;It's easier to select the power/threshold when the plot looks like the second situation. With the data as presented, it doesn't look like any value would be suitable. but if I were forced to pick, then I'd choose 12 because it is the first number above the magic threshold (ignoring the weirdness at the start). However, I'll admit that I don't understand the concepts behind the power, and am just following the recommended instructions, so can't explain why it is what it is.&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/F1MT9.png&#xD;&#xA;  [2]: https://www.pnas.org/content/suppl/2006/11/03/0605938103.DC1/Image439.gif&#xD;&#xA;" />
  <row Id="1819" PostHistoryTypeId="2" PostId="590" RevisionGUID="8886890c-819d-429f-87a6-e03e9e1b81f9" CreationDate="2017-06-08T15:46:03.133" UserId="762" Text="I want to calculate thermodynamic paremeter, but I don't know what software can do this work. Please tell me, thank you very much !" />
  <row Id="1820" PostHistoryTypeId="1" PostId="590" RevisionGUID="8886890c-819d-429f-87a6-e03e9e1b81f9" CreationDate="2017-06-08T15:46:03.133" UserId="762" Text="Is there a software or web server to calculate thermodynamic parameter ^G ^H ^S?" />
  <row Id="1821" PostHistoryTypeId="3" PostId="590" RevisionGUID="8886890c-819d-429f-87a6-e03e9e1b81f9" CreationDate="2017-06-08T15:46:03.133" UserId="762" Text="&lt;database&gt;&lt;annotation&gt;&lt;bwa&gt;&lt;samtools&gt;&lt;structural-variation&gt;" />
  <row Id="1822" PostHistoryTypeId="2" PostId="591" RevisionGUID="53d4171c-56d4-41fa-9801-036b5d19263a" CreationDate="2017-06-08T15:49:51.480" UserId="37" Text="I quite like BED and GFF3 (I don't like GTF/GFF2, though). As text-based formats, I don't think they leave us much room for improvement. Anyway, if you want a new format, here is one. The following is a hybrid between GFF3 and BED. It is a TAB-delimited text-based format with the following fields:&#xD;&#xA;&#xD;&#xA;1. chr, required&#xD;&#xA;2. start (0-based), required&#xD;&#xA;3. end, required&#xD;&#xA;4. strand&#xD;&#xA;5. ID&#xD;&#xA;6. type (see below)&#xD;&#xA;&#xD;&#xA;Like BED, only the first 3 columns are required, the rest are optional. What is different starts with the &quot;type&quot; field as in GFF. This &quot;type&quot;, when present, defines the columns following it. For example, if type==coding, we could have cdsStart, cdsEnd, blockCount, blockSizes and blockStarts like BED; if type==exon, column 7 could be a &quot;phase&quot;, indicating the phase of the first base. This way, &quot;type&quot; makes this format highly extensible while still relatively easy to parse in comparison to using optional tags all the way. In addition, we may have semi-colon-separated &quot;key&quot;=&quot;value&quot; pairs at the end of each line as in GFF3. Example:&#xD;&#xA;&#xD;&#xA;    chr1   10000  50000  -  x1 coding   10100   40800  2  1000,2000   10000,48000&#xD;&#xA;    chr1   10000  11000  -  *  exon     *       foo1=bar1;foo2=bar2&#xD;&#xA;    chr1   48000  50000  -  *  exon     2&#xD;&#xA;    chr2   10000  50000&#xD;&#xA;&#xD;&#xA;The above only gives a bare bone of the format. There are subtle questions such as: 1) the uniqueness/scope of ID; 2) whether to have parent ID as a fixed field or a type-specific field; 3) where to put display name; 4) sorting order. These are also important in practice.&#xD;&#xA;&#xD;&#xA;PS: I like SQL a lot and think it should get used more often in bioinformatics, but I don't think it replaces text formats completely. Formats are useful for serialization and data transfer. They require less skill to work with and less software/hardware resource to deploy. Carefully engineered binary representations of formats can be much more efficient in a lot of use cases." />
  <row Id="1823" PostHistoryTypeId="5" PostId="590" RevisionGUID="9e9263d9-598b-4172-a204-aeaa39390bb9" CreationDate="2017-06-08T15:56:17.633" UserId="762" Comment="added 8 characters in body; edited tags; edited title" Text="I want to calculate thermodynamic paremeter enthalpy，but I don't know what software can do this work. Please tell me, thank you very much !" />
  <row Id="1824" PostHistoryTypeId="4" PostId="590" RevisionGUID="9e9263d9-598b-4172-a204-aeaa39390bb9" CreationDate="2017-06-08T15:56:17.633" UserId="762" Comment="added 8 characters in body; edited tags; edited title" Text="Is there a software or web server to calculate thermodynamic parameter enthalpy ？" />
  <row Id="1825" PostHistoryTypeId="6" PostId="590" RevisionGUID="9e9263d9-598b-4172-a204-aeaa39390bb9" CreationDate="2017-06-08T15:56:17.633" UserId="762" Comment="added 8 characters in body; edited tags; edited title" Text="&lt;structural-variation&gt;" />
  <row Id="1826" PostHistoryTypeId="5" PostId="590" RevisionGUID="1f2b82e7-87c7-4565-8469-4278e8eaa6f0" CreationDate="2017-06-08T16:16:07.847" UserId="762" Comment="added 21 characters in body; edited title" Text="I want to calculate thermodynamic paremeter enthalpy change of cellulase, but I don't know what software can do this work. Please tell me, thank you very much !" />
  <row Id="1827" PostHistoryTypeId="4" PostId="590" RevisionGUID="1f2b82e7-87c7-4565-8469-4278e8eaa6f0" CreationDate="2017-06-08T16:16:07.847" UserId="762" Comment="added 21 characters in body; edited title" Text="Is there a software or web server to calculate thermodynamic parameter enthalpy change of cellulase?" />
  <row Id="1828" PostHistoryTypeId="2" PostId="592" RevisionGUID="f81920ce-ebc9-4f65-acb3-a771e748daf6" CreationDate="2017-06-08T16:20:00.480" UserId="37" Text="By default, a CRAM you create with samtools is lossless. It typically halves the input BAM in terms of file size. If you want to compress more, you can let samtools convert most read names to integers. You won't be able to tell optical duplicates from read names, but this is a minor concern. You can also drop useless tags depending on your mapper and the downstream caller in use. For cancer data, I wouldn't reduce the resolution of base quality without comprehensive benchmarks. Unfortunately, base quality takes most of space in CRAM. Discarding the original read names and some tags probably won't save you much space." />
  <row Id="1829" PostHistoryTypeId="2" PostId="593" RevisionGUID="a1b432a2-69ac-49ff-bcac-463701bdeedd" CreationDate="2017-06-08T17:54:28.040" UserId="191" Text="Protein folding problem can be viewed as a minimization problem. One can use [quantum annealing][1] to perform the minimization. Running this on quantum computers would improve the performance since they can perform the tunneling directly.&#xD;&#xA;&#xD;&#xA;In fact, quantum annealing was [used][2] ([blog post][3]) for lattice protein folding on the the D-Wave quantum computer (128 qubits). But this was an extremely simplified model, with only 40 discrete states.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Quantum_annealing&#xD;&#xA;  [2]: https://dx.doi.org/10.1038/srep00571&#xD;&#xA;  [3]: http://blogs.nature.com/news/2012/08/d-wave-quantum-computer-solves-protein-folding-problem.html" />
  <row Id="1830" PostHistoryTypeId="6" PostId="528" RevisionGUID="af725f0d-2cda-452c-ae12-c5e8780d0e1f" CreationDate="2017-06-08T18:31:15.580" UserId="640" Comment="add rna-structure tag" Text="&lt;public-databases&gt;&lt;pdb&gt;&lt;rna&gt;&lt;rna-structure&gt;" />
  <row Id="1831" PostHistoryTypeId="24" PostId="528" RevisionGUID="af725f0d-2cda-452c-ae12-c5e8780d0e1f" CreationDate="2017-06-08T18:31:15.580" Comment="Proposed by 640 approved by 37, 77 edit id of 160" />
  <row Id="1832" PostHistoryTypeId="6" PostId="590" RevisionGUID="faafb753-f533-48cc-9dfd-afdbf1c9d058" CreationDate="2017-06-08T20:07:15.413" UserId="96" Comment="structural variation has to do with the order of nucleotides in the genome, not the 3d structure of enzymes" Text="&lt;protein-structure&gt;&lt;thermodynamics&gt;&lt;biophysics&gt;" />
  <row Id="1833" PostHistoryTypeId="24" PostId="590" RevisionGUID="faafb753-f533-48cc-9dfd-afdbf1c9d058" CreationDate="2017-06-08T20:07:15.413" Comment="Proposed by 96 approved by 37, 73 edit id of 161" />
  <row Id="1834" PostHistoryTypeId="6" PostId="570" RevisionGUID="6a652960-0c51-4cdd-8404-64436ab219bc" CreationDate="2017-06-08T20:08:04.130" UserId="640" Comment="add a tag rna-alignment" Text="&lt;rna&gt;&lt;rna-alignment&gt;" />
  <row Id="1835" PostHistoryTypeId="24" PostId="570" RevisionGUID="6a652960-0c51-4cdd-8404-64436ab219bc" CreationDate="2017-06-08T20:08:04.130" Comment="Proposed by 640 approved by 37, 73 edit id of 159" />
  <row Id="1836" PostHistoryTypeId="6" PostId="294" RevisionGUID="00add0db-39a9-4d59-96fe-cd90fdb886d0" CreationDate="2017-06-08T20:21:12.793" UserId="73" Comment="This is minion-specific; adding/changing tag back to minion" Text="&lt;sequencing&gt;&lt;minion&gt;" />
  <row Id="1837" PostHistoryTypeId="2" PostId="594" RevisionGUID="0808c621-ed87-44d9-97b4-8fd19f14f97f" CreationDate="2017-06-08T20:21:18.697" UserId="146" Text="I have around ~3,000 short sequences of approximately ~10Kb long. What are the best ways to find the motifs among all of these sequences? Is there a certain software/method recommended?&#xD;&#xA;&#xD;&#xA;Finding the individual motifs of each 10Kb sequence isn't quite robust enough. One really should take into account all 3000 sequences together. " />
  <row Id="1838" PostHistoryTypeId="1" PostId="594" RevisionGUID="0808c621-ed87-44d9-97b4-8fd19f14f97f" CreationDate="2017-06-08T20:21:18.697" UserId="146" Text="What motif finding software is available for multiple sequences ~10Kb?" />
  <row Id="1839" PostHistoryTypeId="3" PostId="594" RevisionGUID="0808c621-ed87-44d9-97b4-8fd19f14f97f" CreationDate="2017-06-08T20:21:18.697" UserId="146" Text="&lt;motifs&gt;&lt;software-recommendation&gt;" />
  <row Id="1840" PostHistoryTypeId="6" PostId="296" RevisionGUID="48f53dd6-f970-467b-ab1e-ef735baf4aba" CreationDate="2017-06-08T20:23:34.517" UserId="73" Comment="removed redundant tag nanopore (MinION already included, and question is specific to MinION)" Text="&lt;fastq&gt;&lt;minion&gt;&lt;yield&gt;" />
  <row Id="1841" PostHistoryTypeId="5" PostId="571" RevisionGUID="40379452-1bc6-4aac-8eb7-85573c901938" CreationDate="2017-06-08T20:43:01.263" UserId="298" Comment="added 32 characters in body" Text="Here's a Perl script that can do this:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-perl --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env perl &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    ## Change this to whatever taxon you are working with&#xD;&#xA;    my $taxon = 'taxon:1000';&#xD;&#xA;    chomp(my $date = `date +%Y%M%d`);&#xD;&#xA;    &#xD;&#xA;    my (%aspect, %gos);&#xD;&#xA;    ## Read the GO.terms_and_ids file to get the aspect (sub ontology)&#xD;&#xA;    ## of each GO term. &#xD;&#xA;    open(my $fh, $ARGV[0]) or die &quot;Need a GO.terms_and_ids file as 1st arg: $!\n&quot;;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;    	next if /^!/;&#xD;&#xA;    	chomp;&#xD;&#xA;    	my @fields = split(/\t/);&#xD;&#xA;    	## $aspect{GO:0000001} = 'P'&#xD;&#xA;    	$aspect{$fields[0]} = $fields[2];&#xD;&#xA;    }&#xD;&#xA;    close($fh);&#xD;&#xA;    &#xD;&#xA;    ## Read the list of gene annotations&#xD;&#xA;    open($fh, $ARGV[1]) or die &quot;Need a list of gene annotattions as 2nd arg: $!\n&quot;;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;    	chomp;&#xD;&#xA;    	my ($gene, @terms) = split(/[\s,]+/);&#xD;&#xA;    	## $gos{geneA} = (go1, go2 ... goN)&#xD;&#xA;    	$gos{$gene} = [ @terms ];&#xD;&#xA;    }&#xD;&#xA;    close($fh);&#xD;&#xA;    &#xD;&#xA;    foreach my $gene (keys(%gos)) {&#xD;&#xA;    	foreach my $term (@{$gos{$gene}}) {&#xD;&#xA;    		## Warn and skip if there is no aspect for this term&#xD;&#xA;    		if (!$aspect{$term}) {&#xD;&#xA;    			print STDERR &quot;Unknown GO term ($term) for gene $gene\n&quot;;&#xD;&#xA;    			next;&#xD;&#xA;    		}&#xD;&#xA;    		## Build a pseudo GAF line &#xD;&#xA;    		my @out = ('DB', $gene, $gene, ' ', $term, 'PMID:foo', 'TAS', ' ', $aspect{$term},&#xD;&#xA;    							 $gene, ' ', 'protein', $taxon, $date, 'DB', ' ', ' ');&#xD;&#xA;    		print join(&quot;\t&quot;, @out). &quot;\n&quot;;&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Make it executable and run it with the `GO.terms_and_ids` file as the 1st argument and the list of gene annotations as the second. Using the current `GO.terms_and_ids` and the example annotations in the question, I get:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    $ foo.pl GO.terms_and_ids file.gos &#xD;&#xA;    DB	geneD	geneD	 	GO:0005634	PMID:foo	TAS	 	C	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneD	geneD	 	GO:0003677	PMID:foo	TAS	 	F	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneD	geneD	 	GO:0030154	PMID:foo	TAS	 	P	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    Unknown GO term (GO:0006350) for gene geneD&#xD;&#xA;    DB	geneD	geneD	 	GO:0006355	PMID:foo	TAS	 	P	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneD	geneD	 	GO:0007275	PMID:foo	TAS	 	P	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneD	geneD	 	GO:0030528	PMID:foo	TAS	 	F	geneD	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0016020	PMID:foo	TAS	 	C	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0005524	PMID:foo	TAS	 	F	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0006468	PMID:foo	TAS	 	P	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0005737	PMID:foo	TAS	 	C	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0004674	PMID:foo	TAS	 	F	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0006914	PMID:foo	TAS	 	P	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0016021	PMID:foo	TAS	 	C	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneB	geneB	 	GO:0015031	PMID:foo	TAS	 	P	geneB	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneA	geneA	 	GO:0006950	PMID:foo	TAS	 	P	geneA	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneA	geneA	 	GO:0005737	PMID:foo	TAS	 	C	geneA	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0003779	PMID:foo	TAS	 	F	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0006941	PMID:foo	TAS	 	P	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0005524	PMID:foo	TAS	 	F	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0003774	PMID:foo	TAS	 	F	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0005516	PMID:foo	TAS	 	F	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0005737	PMID:foo	TAS	 	C	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    DB	geneC	geneC	 	GO:0005863	PMID:foo	TAS	 	C	geneC	 	protein	taxon:1000	20170308	DB	 	 &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Note that this is very much a pseudo-GAF file since most of the fields apart from the gene name, GO term and sub-ontology are fake. It should still work for what you need, however." />
  <row Id="1843" PostHistoryTypeId="5" PostId="581" RevisionGUID="f907c395-ec64-418e-ac2a-1ca9575b5d13" CreationDate="2017-06-08T21:12:29.263" UserId="298" Comment="added 29 characters in body" Text="As Pierre [suggested][1], you can get a dump of such names from [NCBI][2]. Then, to query it for the common name for a species using its Latin name, you can do:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;    $ awk -F'\t' -vname=&quot;Mus musculus&quot; '($7==&quot;scientific name&quot; &amp;&amp; $3==name){&#xD;&#xA;                                          a[$1]=$3&#xD;&#xA;                                        } &#xD;&#xA;                                        ($1 in a &amp;&amp; /genbank common name/){  &#xD;&#xA;                                          print $3&#xD;&#xA;                                        }' names.dmp&#xD;&#xA;    house mouse&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/579/298&#xD;&#xA;  [2]: ftp://ftp.ncbi.nih.gov/pub/taxonomy" />
  <row Id="1844" PostHistoryTypeId="5" PostId="577" RevisionGUID="f6e77b9c-b972-4163-a1d4-0c1cd4c50f42" CreationDate="2017-06-08T21:13:47.617" UserId="298" Comment="added 32 characters in body" Text="Many interaction databases now works with [PSI format][1] files. Most of the main databases can do this and the EBI has set up [PSICQUIC View][2], a very useful page where you can query multiple databases at once. &#xD;&#xA;&#xD;&#xA;Note that it is very important to limit the results according to the detection method. There is a lot of noise in protein interaction databases. Depending on what you want to do you could limit to only experimentally verified interactions or to only direct, binary interactions &#xD;&#xA;(so exclude the results of, for example, ChIP analyses which can also find complexes) etc.&#xD;&#xA;&#xD;&#xA;That said, here's a simple example script that will query the APID database using its PSICQUIC service:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-perl --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    use LWP::Simple;&#xD;&#xA;    &#xD;&#xA;    my @proteins;&#xD;&#xA;    ## Read a list of target proteins, one per line (this expects UniProt names)&#xD;&#xA;    open(my $fh, &quot;$ARGV[0]&quot;) or die &quot;Need a list of proteins as the 1st argument: $!\n&quot;;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;    	chomp;&#xD;&#xA;    	push @proteins, $_;&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    ## Get human interactions only&#xD;&#xA;    my $species=&quot;9606&quot;;&#xD;&#xA;    &#xD;&#xA;    ## Get the interactions for each target protein&#xD;&#xA;    foreach my $protein (@proteins) {&#xD;&#xA;    	my $queryUrl= &quot;http://cicblade.dep.usal.es/psicquic-ws/webservices/current/search/query/$protein&quot;;&#xD;&#xA;    	$queryUrl  .= &quot;?taxidA:$species%20ANDtaxidB:$species&quot;;&#xD;&#xA;    &#xD;&#xA;    	my $tries=1;&#xD;&#xA;    	my $content = get $queryUrl;&#xD;&#xA;    	while ($tries&lt;=10) {&#xD;&#xA;    		if (defined($content)) {&#xD;&#xA;    			$tries=11;&#xD;&#xA;    		} else {&#xD;&#xA;    			print STDERR &quot;Could not retrieve $queryUrl, retrying($tries)...\n&quot;;&#xD;&#xA;    			$content = get $queryUrl;&#xD;&#xA;    		}&#xD;&#xA;    		$tries++;&#xD;&#xA;    	}&#xD;&#xA;    &#xD;&#xA;    	# Now list all interactions&#xD;&#xA;    	my @lines = split(/\n/, $content);&#xD;&#xA;    	my $LINES= @lines;&#xD;&#xA;    	my $count = 0;&#xD;&#xA;    	for my $line (@lines) {&#xD;&#xA;    		$count++;&#xD;&#xA;    		my @flds = split(/\t/, $line); # split tab delimited lines      &#xD;&#xA;    		# split fields of a PSIMITAB 2.5 line&#xD;&#xA;    		my ($idA, $idB, $altIdA, $altIdB, $aliasA, $aliasB, $detMethod, $author, $pub, $orgA, $orgB, $intType, $sourceDb, $intID, $conf) = @flds;&#xD;&#xA;    		## Here you can add logic to limit the interactions by specific detection method codes ($detMethod)&#xD;&#xA;    		## or database of origin etc. &#xD;&#xA;    		&#xD;&#xA;    		## Print&#xD;&#xA;    		print &quot;$line\n&quot;&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;I tested it by using a file with the UniProt ID for human TP53 (P04637) and it returns a list of 3988 interactions (showing 10 randomly selected results below):&#xD;&#xA;&#xD;&#xA;    $ foo.pl names.txt | shuf -n 10&#xD;&#xA;    uniprotkb:Q9BWC9	uniprotkb:P04637	-	-	uniprotkb:CCDC106(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0007&quot;(anti tag coimmunoprecipitation)	Zhou, J. et al.(2010)	pubmed:20159018	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-7812926	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:O15126	-	-	uniprotkb:TP53(gene_name)	uniprotkb:SCAMP1(gene_name)	psi-mi:&quot;MI:0018&quot;(two hybrid)	Lim, J. et al.(2006)	pubmed:16713569	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:720622	-&#xD;&#xA;    uniprotkb:P63165	uniprotkb:P04637	-	-	uniprotkb:SUMO1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0018&quot;(two hybrid)	Minty, A. et al.(2000)	pubmed:10961991	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:262339	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:P31350	-	-	uniprotkb:TP53(gene_name)	uniprotkb:RRM2(gene_name)	psi-mi:&quot;MI:0416&quot;(fluorescence microscopy)	Xue, L. et al.(2003)	pubmed:12615712	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0465&quot;(dip)	dip:DIP-40167E	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q00987	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MDM2(gene_name)	psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)	Dai, MS. et al.(2004)	pubmed:15308643	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:478073	-&#xD;&#xA;    uniprotkb:Q99576	uniprotkb:P04637	-	-	uniprotkb:TSC22D3(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0428&quot;(imaging technique)	Ayroldi, E. et al.(2015)	pubmed:25168242	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:1255896	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q00987	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MDM2(gene_name)	psi-mi:&quot;MI:0415&quot;(enzymatic study)	Lui, K. et al.(2013)	pubmed:23572512	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:859223	-&#xD;&#xA;    uniprotkb:P25685	uniprotkb:P04637	-	-	uniprotkb:DNAJB1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)	Qi, M. et al.(2014)	pubmed:24361594	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:938952	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q8IW41	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MAPKAPK5(gene_name)	psi-mi:&quot;MI:0424&quot;(protein kinase assay)	Sun, P. et al.(2007)	pubmed:17254968	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-1202077	-&#xD;&#xA;    uniprotkb:Q13526	uniprotkb:P04637	-	-	uniprotkb:PIN1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0096&quot;(pull down)	Mantovani, F. et al.(2007)	pubmed:17906639	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-6112688	-&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;  [1]: http://psidev.sourceforge.net/molecular_interactions/xml/doc/user/&#xD;&#xA;  [2]: http://www.ebi.ac.uk/Tools/webservices/psicquic/view/main.xhtml" />
  <row Id="1845" PostHistoryTypeId="5" PostId="583" RevisionGUID="df62204b-573e-42e7-aef6-859eef7d8ab3" CreationDate="2017-06-08T21:33:04.827" UserId="775" Comment="removed duplicate statement" Text="I calculated an AMOVA from a genind object, with one hierarchical factor. In the table I obtain there are SSD values (for my grouping factor,&quot;Error&quot; and total) and sigma2 values (for my grouping factor and &quot;Error&quot;)&#xD;&#xA;&#xD;&#xA;I have two questions:&#xD;&#xA;&#xD;&#xA;- What does &quot;error&quot; stand for?&#xD;&#xA;- How do I calculate Fst? Which values do I have to use?&#xD;&#xA;&#xD;&#xA;A genind object in R is an object which contains allelic information about a set of individuals.  &#xD;&#xA;This is the table that I get: &#xD;&#xA;&#xD;&#xA;	Analysis of Molecular Variance&#xD;&#xA;&#xD;&#xA;    Call: amova(formula = plantula.dist ~ g, nperm = 100)  &#xD;&#xA;    &#xD;&#xA;               SSD         MSD  df  &#xD;&#xA;    g     354992.9 354992.9297   1  &#xD;&#xA;    Error 310814.9    693.7834 448 &#xD;&#xA;    Total 665807.9   1482.8683 449 &#xD;&#xA;    &#xD;&#xA;    Variance components:  &#xD;&#xA;           sigma2 P.value  &#xD;&#xA;    g     1574.69       0  &#xD;&#xA;    Error  693.78         &#xD;&#xA;    &#xD;&#xA;    Variance coefficients:  &#xD;&#xA;           a   &#xD;&#xA;    224.9956   &#xD;&#xA;&#xD;&#xA;" />
  <row Id="1846" PostHistoryTypeId="24" PostId="583" RevisionGUID="df62204b-573e-42e7-aef6-859eef7d8ab3" CreationDate="2017-06-08T21:33:04.827" Comment="Proposed by 775 approved by 73, 77 edit id of 164" />
  <row Id="1847" PostHistoryTypeId="5" PostId="594" RevisionGUID="2557686c-97ea-492d-9e01-929e0d9e73af" CreationDate="2017-06-08T21:34:24.920" UserId="146" Comment="added 263 characters in body" Text="I have around ~3,000 short sequences of approximately ~10Kb long. What are the best ways to find the motifs among all of these sequences? Is there a certain software/method recommended?&#xD;&#xA;&#xD;&#xA;There are several ways to do this. My goal would be to:&#xD;&#xA;&#xD;&#xA;(1) Check for motifs repeated within individual sequences&#xD;&#xA;&#xD;&#xA;(2) Check for motifs shared among all sequences&#xD;&#xA;&#xD;&#xA;(3) Check for the presence of &quot;expected&quot; or known motifs&#xD;&#xA;&#xD;&#xA;With respect to #3, I'm also curious if I find e.g. trinucleotide sequences, how does one check the context around these regions?&#xD;&#xA;&#xD;&#xA;Thank you for the recommendations/help!&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1848" PostHistoryTypeId="2" PostId="595" RevisionGUID="066e014f-78e6-4385-b5d9-3841cad745fa" CreationDate="2017-06-08T21:34:33.547" UserId="779" Text="Is there a convenient way to extract the longest isoforms from a .fasta transcriptome file? I had found some scripts on biostars but none are functional and I'm having difficulty getting them to work.&#xD;&#xA;&#xD;&#xA;I'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes." />
  <row Id="1849" PostHistoryTypeId="1" PostId="595" RevisionGUID="066e014f-78e6-4385-b5d9-3841cad745fa" CreationDate="2017-06-08T21:34:33.547" UserId="779" Text="Extract longest isoforms from .fasta file" />
  <row Id="1850" PostHistoryTypeId="3" PostId="595" RevisionGUID="066e014f-78e6-4385-b5d9-3841cad745fa" CreationDate="2017-06-08T21:34:33.547" UserId="779" Text="&lt;fasta&gt;" />
  <row Id="1853" PostHistoryTypeId="5" PostId="582" RevisionGUID="79526ecf-80c8-4da7-ae90-199387ab30b7" CreationDate="2017-06-08T21:44:35.360" UserId="298" Comment="For some reason, I needed to add an extra blank line to activate the syntax highlighting; made a few more changes to meet the minimum character threshold" Text="As [Pierre mentioned][1], NCBI is a good resource for this kind of transformation.&#xD;&#xA;&#xD;&#xA;You can still use `taxize` to perform the conversion:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    library(&quot;taxize&quot;)&#xD;&#xA;    &#xD;&#xA;    species &lt;- c('Helianthus annuus', 'Mycobacterium bovis', 'Rattus rattus', 'XX', 'Mus musculus')&#xD;&#xA;    &#xD;&#xA;    uids &lt;- get_uid(species)&#xD;&#xA;    &#xD;&#xA;    # keep only uids which you have in the database&#xD;&#xA;    uids.found &lt;- as.uid(uids[!is.na(uids)])&#xD;&#xA;    # keep only species names  corresponding to your ids&#xD;&#xA;    species.found &lt;- species[!is.na(uids)]&#xD;&#xA;    &#xD;&#xA;    common.names &lt;- sci2comm(uids.found, db = 'ncbi')&#xD;&#xA;    names(common.names) &lt;- species.found&#xD;&#xA;&#xD;&#xA;    common.names&#xD;&#xA;&#xD;&#xA;    ## output:&#xD;&#xA;    ## $`Helianthus annuus`&#xD;&#xA;    ## [1] &quot;common sunflower&quot;&#xD;&#xA;    ## &#xD;&#xA;    ## $`Mycobacterium bovis`&#xD;&#xA;    ## character(0)&#xD;&#xA;    ## &#xD;&#xA;    ## $`Rattus rattus`&#xD;&#xA;    ## [1] &quot;black rat&quot;&#xD;&#xA;    ## &#xD;&#xA;    ## $`Mus musculus`&#xD;&#xA;    ## [1] &quot;house mouse&quot;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;This works correctly if no common name is available, or if there is no species in the database. You can also try a different database, e.g. `db='itis'`.&#xD;&#xA;&#xD;&#xA;The advantage of this approach is that you do not have to parse a file yourself. The downside is that it can be slow for large list, since for every species a database request is performed.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/579/191&#xD;&#xA;" />
  <row Id="1854" PostHistoryTypeId="24" PostId="582" RevisionGUID="79526ecf-80c8-4da7-ae90-199387ab30b7" CreationDate="2017-06-08T21:44:35.360" Comment="Proposed by 298 approved by 191 edit id of 165" />
  <row Id="1855" PostHistoryTypeId="5" PostId="595" RevisionGUID="d709b37c-c51c-47eb-9044-7877909eaa15" CreationDate="2017-06-08T21:48:49.633" UserId="779" Comment="added 740 characters in body" Text="Is there a convenient way to extract the longest isoforms from a .fasta transcriptome file? I had found some scripts on biostars (https://www.biostars.org/p/107759/) but none are functional and I'm having difficulty getting them to work.&#xD;&#xA;&#xD;&#xA;I'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes.&#xD;&#xA;&#xD;&#xA;Here is what the fasta file looks like currently (sequence shortened to save space)&#xD;&#xA;&#xD;&#xA;&gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&#xD;&#xA;atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;ttaatataaccaaaaacctagaaattctagccttcgatgatgttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;&#xD;&#xA;&gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t2&#xD;&#xA;atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;&#xD;&#xA;The format is as such:&#xD;&#xA;&#xD;&#xA;Gene 1 isoform 1&#xD;&#xA;Gene 1 isoform 2&#xD;&#xA;Gene 2 isoform 1&#xD;&#xA;Gene 2 isoform 2 &#xD;&#xA;&#xD;&#xA;and so forth. There are several genes that have more than one pair of isoforms (up to 3 or 4). There are roughly 80,000 total transcripts, probably 25,000 genes." />
  <row Id="1856" PostHistoryTypeId="5" PostId="595" RevisionGUID="2a837186-438e-4fa9-8a8f-c97535e35ece" CreationDate="2017-06-08T21:49:50.857" UserId="77" Comment="Fix formatting" Text="Is there a convenient way to extract the longest isoforms from a .fasta transcriptome file? I had found some scripts on biostars (https://www.biostars.org/p/107759/) but none are functional and I'm having difficulty getting them to work.&#xD;&#xA;&#xD;&#xA;I'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes.&#xD;&#xA;&#xD;&#xA;Here is what the fasta file looks like currently (sequence shortened to save space)&#xD;&#xA;&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;    ttaatataaccaaaaacctagaaattctagccttcgatgatgttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t2&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;&#xD;&#xA;The format is as such:&#xD;&#xA;&#xD;&#xA;    Gene 1 isoform 1&#xD;&#xA;    Gene 1 isoform 2&#xD;&#xA;    Gene 2 isoform 1&#xD;&#xA;    Gene 2 isoform 2 &#xD;&#xA;&#xD;&#xA;and so forth. There are several genes that have more than one pair of isoforms (up to 3 or 4). There are roughly 80,000 total transcripts, probably 25,000 genes." />
  <row Id="1857" PostHistoryTypeId="5" PostId="595" RevisionGUID="34ff496c-8c0d-451b-983d-7b305a08e70a" CreationDate="2017-06-08T21:50:44.780" UserId="779" Comment="added 4 characters in body" Text="Is there a convenient way to extract the longest isoforms from a .fasta transcriptome file? I had found some scripts on biostars (https://www.biostars.org/p/107759/) but none are functional and I'm having difficulty getting them to work.&#xD;&#xA;&#xD;&#xA;I'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes.&#xD;&#xA;&#xD;&#xA;The fasta was generated via Augustus. Here is what the fasta file looks like currently (sequence shortened to save space)&#xD;&#xA;&#xD;&#xA;&gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&#xD;&#xA;atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;ttaatataaccaaaaacctagaaattctagccttcgatgatgttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;&#xD;&#xA;&gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t2&#xD;&#xA;atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;&#xD;&#xA;The format looks like this:&#xD;&#xA;&#xD;&#xA;&gt; Gene 1 isoform 1 &#xD;&#xA;&gt; Gene 1 isoform 2 &#xD;&#xA;&gt; Gene 2 isoform 1 &#xD;&#xA;&gt; Gene 2 isoform 2&#xD;&#xA;&#xD;&#xA;and so forth. There are several genes that have more than one pair of isoforms (up to 3 or 4). There are roughly 80,000 total transcripts, probably 25,000 genes." />
  <row Id="1858" PostHistoryTypeId="5" PostId="595" RevisionGUID="ff376e6d-bec0-447a-ab42-904abec146bc" CreationDate="2017-06-08T21:52:35.227" UserId="77" Comment="Fix once again" Text="Is there a convenient way to extract the longest isoforms from a .fasta transcriptome file? I had found some scripts on biostars (https://www.biostars.org/p/107759/) but none are functional and I'm having difficulty getting them to work.&#xD;&#xA;&#xD;&#xA;I'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes.&#xD;&#xA;&#xD;&#xA;The fasta was generated via Augustus. Here is what the fasta file looks like currently (sequence shortened to save space)&#xD;&#xA;&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;    ttaatataaccaaaaacctagaaattctagccttcgatgatgttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t2&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;&#xD;&#xA;The format is as such:&#xD;&#xA;&#xD;&#xA;    Gene 1 isoform 1&#xD;&#xA;    Gene 1 isoform 2&#xD;&#xA;    Gene 2 isoform 1&#xD;&#xA;    Gene 2 isoform 2 &#xD;&#xA;&#xD;&#xA;and so forth. There are several genes that have more than one pair of isoforms (up to 3 or 4). There are roughly 80,000 total transcripts, probably 25,000 genes." />
  <row Id="1859" PostHistoryTypeId="5" PostId="595" RevisionGUID="37e2c94d-4c76-49d5-8dd8-b346100ab8df" CreationDate="2017-06-08T21:53:06.507" UserId="779" Comment="added 8 characters in body" Text="Is there a convenient way to extract the longest isoforms from a .fasta transcriptome file? I had found some scripts on biostars (https://www.biostars.org/p/107759/) but none are functional and I'm having difficulty getting them to work.&#xD;&#xA;&#xD;&#xA;I'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes.&#xD;&#xA;&#xD;&#xA;The fasta was generated via Augustus. Here is what the fasta file looks like currently (sequence shortened to save space)&#xD;&#xA;&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;    ttaatataaccaaaaacctagaaattctagccttcgatgatgttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t2&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;&#xD;&#xA;The format is as such:&#xD;&#xA;&#xD;&#xA;    Gene 1 isoform 1  &#xD;&#xA;    Gene 1 isoform 2  &#xD;&#xA;    Gene 2 isoform 1  &#xD;&#xA;    Gene 2 isoform 2   &#xD;&#xA;&#xD;&#xA;and so forth. There are several genes that have more than one pair of isoforms (up to 3 or 4). There are roughly 80,000 total transcripts, probably 25,000 genes." />
  <row Id="1860" PostHistoryTypeId="2" PostId="597" RevisionGUID="197ae649-cbca-42da-847c-e95dbec44af7" CreationDate="2017-06-08T22:03:44.497" UserId="298" Text="A ex-coworker (Josep Avril) has written a couple of very useful little scripts that convert fasta to tbl (seqID&lt;TAB&gt;Sequence) and back again. These are extremely handy for this sort of thing (and are included at the end of this answer). Using them, you can convert your fasta to a one sequence per line format, keep the longest sequence with a simple awk script and convert back to fasta.&#xD;&#xA;&#xD;&#xA;I am assuming that the part after the last `.` in the ID line should be removed (so that `Doug_NoIndex_L005_R1_001_contig_2.g7.t1` and `Doug_NoIndex_L005_R1_001_contig_2.g7.t2` both map to `Doug_NoIndex_L005_R1_001_contig_2.g7`). If that is a correct assumption, this should work for you:&#xD;&#xA;&#xD;&#xA;    $ FastaToTbl file.fa | sed 's/\.[^.]*\t/\t/' | &#xD;&#xA;        awk -F&quot;\t&quot; '{&#xD;&#xA;                        if(length($2)&gt;length(a[$1])){&#xD;&#xA;                            a[$1]=$0&#xD;&#xA;                        }&#xD;&#xA;                    }&#xD;&#xA;                    END{&#xD;&#xA;                        for(i in a){&#xD;&#xA;                            print a[i]&#xD;&#xA;                        }&#xD;&#xA;                    }' | TblToFasta&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7 &#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgat&#xD;&#xA;    tcgtatgatcaactcatcattaatataaccaaaaacctagaaattctagccttcgatgat&#xD;&#xA;    gttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;    &#xD;&#xA;------------&#xD;&#xA;&#xD;&#xA;* FastaToTbl&#xD;&#xA;&#xD;&#xA;    &lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;        #!/usr/bin/awk -f&#xD;&#xA;        {&#xD;&#xA;                if (substr($1,1,1)==&quot;&gt;&quot;)&#xD;&#xA;        		if (NR&gt;1)&#xD;&#xA;                        	printf &quot;\n%s\t&quot;, substr($0,2,length($0)-1)&#xD;&#xA;        		else &#xD;&#xA;        			printf &quot;%s\t&quot;, substr($0,2,length($0)-1)&#xD;&#xA;                else &#xD;&#xA;                        printf &quot;%s&quot;, $0&#xD;&#xA;        }END{printf &quot;\n&quot;}&#xD;&#xA;        &#xD;&#xA;* TblToFasta&#xD;&#xA;&#xD;&#xA;    &lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;        #! /usr/bin/awk -f&#xD;&#xA;        {&#xD;&#xA;          sequence=$NF&#xD;&#xA;        &#xD;&#xA;          ls = length(sequence)&#xD;&#xA;          is = 1&#xD;&#xA;          fld  = 1&#xD;&#xA;          while (fld &lt; NF)&#xD;&#xA;          {&#xD;&#xA;             if (fld == 1){printf &quot;&gt;&quot;}&#xD;&#xA;             printf &quot;%s &quot; , $fld&#xD;&#xA;            &#xD;&#xA;             if (fld == NF-1)&#xD;&#xA;              {&#xD;&#xA;                printf &quot;\n&quot;&#xD;&#xA;              }&#xD;&#xA;              fld = fld+1&#xD;&#xA;          }&#xD;&#xA;          while (is &lt;= ls)&#xD;&#xA;          {&#xD;&#xA;            printf &quot;%s\n&quot;, substr(sequence,is,60)&#xD;&#xA;            is=is+60&#xD;&#xA;          }&#xD;&#xA;        }&#xD;&#xA;        " />
  <row Id="1861" PostHistoryTypeId="2" PostId="598" RevisionGUID="682bd133-a2b5-4f7d-87d2-96b18935a0de" CreationDate="2017-06-08T22:04:16.040" UserId="776" Text="I wrote a command-line k-mer counter called `kmer-counter` that will output results in a form that your Python script can consume: https://github.com/alexpreynolds/kmer-counter&#xD;&#xA;&#xD;&#xA;You might use it in Python like so:&#xD;&#xA;&#xD;&#xA;    result = {}&#xD;&#xA;    kmerCmd = 'kmer-counter --fasta --k=%d %s' % (k, fastaFile)&#xD;&#xA;    try:&#xD;&#xA;        output = subprocess.check_output(kmerCmd, shell=True)&#xD;&#xA;        for line in output.splitlines():&#xD;&#xA;            (header, counts) = line.strip().split('\t')&#xD;&#xA;            header = header[1:]&#xD;&#xA;            kmers = dict((k,int(v)) for (k,v) in [d.split(':') for d in counts.split(' ')])&#xD;&#xA;            result[header] = kmers&#xD;&#xA;    except subprocess.CalledProcessError as error:&#xD;&#xA;        output = error&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Given example FASTA like this:&#xD;&#xA;&#xD;&#xA;    &gt;foo&#xD;&#xA;    TTAACG&#xD;&#xA;    &gt;bar&#xD;&#xA;    GTGGAAGTTCTTAGGGCATGGCAAAGAGTCAGAATTTGAC&#xD;&#xA;&#xD;&#xA;For k=6, you would get a iterable Python dictionary like this:&#xD;&#xA;&#xD;&#xA;    {'foo': {'TTAACG': 1, 'CGTTAA': 1}, 'bar': {'GTTCTT': 1, 'AGAACT': 1, 'GAGTCA': 1, 'ATGGCA': 1, 'GAACTT': 1, 'ATTCTG': 1, 'CTAAGA': 1, 'CTTCCA': 1, 'ATTTGA': 1, 'GGAAGT': 1, 'AGGGCA': 1, 'CCTAAG': 1, 'CTCTTT': 1, 'AATTTG': 1, 'TCTGAC': 1, 'TTTGCC': 1, 'CTTAGG': 1, 'TTTGAC': 1, 'GAAGTT': 1, 'CCCTAA': 1, 'AGAATT': 1, 'AGTCAG': 1, 'CTGACT': 1, 'TCTTAG': 1, 'CGTTAA': 1, 'GTGGAA': 1, 'TGCCAT': 1, 'ACTCTT': 1, 'GGGCAT': 1, 'TTAGGG': 1, 'CTTTGC': 1, 'TGGAAG': 1, 'GACTCT': 1, 'CATGCC': 1, 'GCAAAG': 1, 'AAATTC': 1, 'GTCAAA': 1, 'TGACTC': 1, 'TAGGGC': 1, 'AAGTTC': 1, 'ATGCCC': 1, 'TCAAAT': 1, 'CAAAGA': 1, 'AACTTC': 1, 'GTCAGA': 1, 'CAAATT': 1, 'TAAGAA': 1, 'CATGGC': 1, 'AAGAAC': 1, 'AAGAGT': 1, 'TCTTTG': 1, 'TTCCAC': 1, 'TGGCAA': 1, 'GGCAAA': 1, 'AGTTCT': 1, 'AGAGTC': 1, 'TCAGAA': 1, 'GAATTT': 1, 'AAAGAG': 1, 'TGCCCT': 1, 'CCATGC': 1, 'GGCATG': 1, 'TTGCCA': 1, 'CAGAAT': 1, 'AATTCT': 1, 'GCATGG': 1, 'ACTTCC': 1, 'TTCTTA': 1, 'GCCATG': 1, 'GCCCTA': 1, 'TTCTGA': 1}}&#xD;&#xA;&#xD;&#xA;See: https://github.com/alexpreynolds/kmer-counter/blob/master/test/kmer-test.py" />
  <row Id="1862" PostHistoryTypeId="2" PostId="599" RevisionGUID="e6ac6e40-cc48-4107-a962-ac3fb3625025" CreationDate="2017-06-08T22:06:35.127" UserId="96" Text="This solution is will work for your example and probably all Augustus-derived Fastas, but mileage will vary beyond that.&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    def parse_fasta(data):&#xD;&#xA;        &quot;&quot;&quot;Stolen shamelessly from http://stackoverflow.com/a/7655072/459780.&quot;&quot;&quot;&#xD;&#xA;        name, seq = None, []&#xD;&#xA;        for line in data:&#xD;&#xA;            line = line.rstrip()&#xD;&#xA;            if line.startswith('&gt;'):&#xD;&#xA;                if name:&#xD;&#xA;                    yield (name, ''.join(seq))&#xD;&#xA;                name, seq = line, []&#xD;&#xA;            else:&#xD;&#xA;                seq.append(line)&#xD;&#xA;        if name:&#xD;&#xA;            yield (name, ''.join(seq))&#xD;&#xA;    &#xD;&#xA;    isoforms = dict()&#xD;&#xA;    for defline, sequence in parse_fasta(sys.stdin):&#xD;&#xA;        geneid = '.'.join(defline[1:].split('.')[:-1])&#xD;&#xA;        if geneid in isoforms:&#xD;&#xA;            otherdefline, othersequence = isoforms[geneid]&#xD;&#xA;            if len(sequence) &gt; len(othersequence):&#xD;&#xA;                isoforms[geneid] = (defline, sequence)&#xD;&#xA;        else:&#xD;&#xA;            isoforms[geneid] = (defline, sequence)&#xD;&#xA;    &#xD;&#xA;    for defline, sequence in isoforms.values():&#xD;&#xA;        print(defline, sequence, sep='\n')&#xD;&#xA;&#xD;&#xA;Save as `longest.py`, and invoke on the command line like so.&#xD;&#xA;&#xD;&#xA;    python longest.py &lt; intput.fasta &gt; output.fasta&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;The line `geneid = '.'.join(defline[1:].split('.')[:-1])` is key. Let me break it down.&#xD;&#xA;&#xD;&#xA;- `defline[1:]`: ignore the first character of the defline (the `&gt;` symbol)&#xD;&#xA;- `defline[1:].split('.')`: split the string on `.` symbol&#xD;&#xA;- `defline[1:].split('.')[:-1]`: ignore the last value after the split (the isoform name)&#xD;&#xA;- `'.'.join(defline[1:].split('.')[:-1])`: join the split values again&#xD;&#xA;&#xD;&#xA;This value will be the same for all isoforms for the same gene, so we use it in the script to keep track of the longest sequence associated with each gene ID." />
  <row Id="1863" PostHistoryTypeId="2" PostId="600" RevisionGUID="6673ad1a-62e4-434e-bb12-b7f3e734b6ac" CreationDate="2017-06-08T22:09:13.897" UserId="672" Text="Just throwing this out there, as all the best stuff has already been said, but why does one need to specify a strict specification for a bioinformatic data format? As you say, what typically happens is all the action ends up in the optional fields.&#xD;&#xA;&#xD;&#xA;There's a lot to be said for the libertarian values of just letting people figure it out on their own. Take optional fields in the BAM tag spec. for example. Here you can have your own tags, but they must start with an &quot;X&quot;, a &quot;Y&quot; or a &quot;Z&quot; and be followed by &quot;[A-Za-z0-9]&quot;. Why? Why can't people use their own tag names like &quot;read is unique&quot; or &quot;edit distance to genome&quot; or whatever they want. Are we not to be trusted with the power of naming things and recalling things arbitrarily? The result is one must look up the tag name's actual meaning in the publication of the tool that produced it, or ask on certain forums, etc. And this is assuming i know the tool that produced the reads - if i don't then who knows wtf 'XT' stands for.&#xD;&#xA;&#xD;&#xA;Essentially, downstream readers of optional fields already trust that 'XT' is what it thinks it is. So if we're happy to use optional fields in any capacity, why stop at fields?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Extend this logic to the columns of the data format. Let users determine column names. You may once in a blue moon come across a file with the chromosome column as &quot;chr&quot; rather than &quot;chromosome&quot;, but most of the time you'll be looking for &quot;read is quantum-radioactive&quot;, and detecting and fixing naming-errors is not going to be as much of a problem as using a dataformat that does not store what it is you want to store. Or worse, it can store it, but in a really illogical way that results in everyone who uses the dataformat to have to ask at least 3 questions here or on other forums before they understand whats really going on.&#xD;&#xA;&#xD;&#xA;So you end up with a general solution to the problem of storing tabular information in a compatible way across many different types of software, also known as SQL (or any other kind of general-purpose database such as REDIS, Neo4J, Numpy, etc). In fact, does it even matter what the datastore is, so long as it's tabular and has the &quot;chromosome&quot; and &quot;position&quot; values for each item?&#xD;&#xA;&#xD;&#xA;TL;DR - We will not think of tomorrow's best data format today, because the nature of tomorrow's data is not yet known. Less policing in this area would most likely result in more robust software, where nothing is taken for granted and no assumptions about the data can be made until the schema has been parsed.&#xD;&#xA;&#xD;&#xA;In truth, the only reason we think we need to specify our dataformats is because we know if we didn't, other people will do something we didn't think of - and we, i think falsely, assume that will be bad." />
  <row Id="1864" PostHistoryTypeId="5" PostId="597" RevisionGUID="48151791-74e2-4ab5-b290-c4eb559ff805" CreationDate="2017-06-08T22:15:02.950" UserId="298" Comment="added 832 characters in body" Text="An ex-coworker (Josep Avril) has written a couple of very useful little scripts that convert fasta to tbl (seqID&lt;TAB&gt;Sequence) and back again. These are extremely handy for this sort of thing (and are included at the end of this answer). Using them, you can convert your fasta to a one sequence per line format, keep the longest sequence with a simple awk script and convert back to fasta.&#xD;&#xA;&#xD;&#xA;I am assuming that the part after the last `.` in the ID line should be removed (so that `Doug_NoIndex_L005_R1_001_contig_2.g7.t1` and `Doug_NoIndex_L005_R1_001_contig_2.g7.t2` both map to `Doug_NoIndex_L005_R1_001_contig_2.g7`). If that is a correct assumption, this should work for you:&#xD;&#xA;&#xD;&#xA;    $ FastaToTbl file.fa | sed 's/\.[^.]*\t/\t/' | &#xD;&#xA;        awk -F&quot;\t&quot; '{&#xD;&#xA;                        if(length($2)&gt;length(a[$1])){&#xD;&#xA;                            a[$1]=$0&#xD;&#xA;                        }&#xD;&#xA;                    }&#xD;&#xA;                    END{&#xD;&#xA;                        for(i in a){&#xD;&#xA;                            print a[i]&#xD;&#xA;                        }&#xD;&#xA;                    }' | TblToFasta&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7 &#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgat&#xD;&#xA;    tcgtatgatcaactcatcattaatataaccaaaaacctagaaattctagccttcgatgat&#xD;&#xA;    gttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;One possible issue with that approach is that it needs to keep one sequence per ID in memory. If you have a huge fasta file and not much memory, that might be a problem. If that's the case, you can first sort the file to ensure that similar IDs always appear together and then print each line as soon as you find the next ID:&#xD;&#xA;&#xD;&#xA;    FastaToTbl file.fa | sed 's/\.[^.]*\t/\t/' | LC_ALL=C sort -t$'\t' -k1,1 |  &#xD;&#xA;        awk -F&quot;\t&quot; '{&#xD;&#xA;                        if(NR&gt;1 &amp;&amp; prev!=$1){&#xD;&#xA;                            print a[$1]; &#xD;&#xA;                            prev=$1&#xD;&#xA;                        } &#xD;&#xA;                        if(length($2)&gt;length(a[$1])){&#xD;&#xA;                            a[$1]=$0&#xD;&#xA;                        }&#xD;&#xA;                    }&#xD;&#xA;                    END{&#xD;&#xA;                        print a[$1]&#xD;&#xA;                    }' | TblToFasta&#xD;&#xA;&#xD;&#xA;------------&#xD;&#xA;&#xD;&#xA;* FastaToTbl&#xD;&#xA;&#xD;&#xA;    &lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;        #!/usr/bin/awk -f&#xD;&#xA;        {&#xD;&#xA;                if (substr($1,1,1)==&quot;&gt;&quot;)&#xD;&#xA;        		if (NR&gt;1)&#xD;&#xA;                        	printf &quot;\n%s\t&quot;, substr($0,2,length($0)-1)&#xD;&#xA;        		else &#xD;&#xA;        			printf &quot;%s\t&quot;, substr($0,2,length($0)-1)&#xD;&#xA;                else &#xD;&#xA;                        printf &quot;%s&quot;, $0&#xD;&#xA;        }END{printf &quot;\n&quot;}&#xD;&#xA;        &#xD;&#xA;* TblToFasta&#xD;&#xA;&#xD;&#xA;    &lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;        #! /usr/bin/awk -f&#xD;&#xA;        {&#xD;&#xA;          sequence=$NF&#xD;&#xA;        &#xD;&#xA;          ls = length(sequence)&#xD;&#xA;          is = 1&#xD;&#xA;          fld  = 1&#xD;&#xA;          while (fld &lt; NF)&#xD;&#xA;          {&#xD;&#xA;             if (fld == 1){printf &quot;&gt;&quot;}&#xD;&#xA;             printf &quot;%s &quot; , $fld&#xD;&#xA;            &#xD;&#xA;             if (fld == NF-1)&#xD;&#xA;              {&#xD;&#xA;                printf &quot;\n&quot;&#xD;&#xA;              }&#xD;&#xA;              fld = fld+1&#xD;&#xA;          }&#xD;&#xA;          while (is &lt;= ls)&#xD;&#xA;          {&#xD;&#xA;            printf &quot;%s\n&quot;, substr(sequence,is,60)&#xD;&#xA;            is=is+60&#xD;&#xA;          }&#xD;&#xA;        }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1865" PostHistoryTypeId="5" PostId="599" RevisionGUID="6a166d9a-d570-49da-a171-74949e046951" CreationDate="2017-06-08T22:25:39.287" UserId="96" Comment="added 42 characters in body" Text="This solution is will work for your example and probably all Augustus-derived Fastas, but mileage will vary beyond that.&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xA;    from __future__ import print_function&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    def parse_fasta(data):&#xD;&#xA;        &quot;&quot;&quot;Stolen shamelessly from http://stackoverflow.com/a/7655072/459780.&quot;&quot;&quot;&#xD;&#xA;        name, seq = None, []&#xD;&#xA;        for line in data:&#xD;&#xA;            line = line.rstrip()&#xD;&#xA;            if line.startswith('&gt;'):&#xD;&#xA;                if name:&#xD;&#xA;                    yield (name, ''.join(seq))&#xD;&#xA;                name, seq = line, []&#xD;&#xA;            else:&#xD;&#xA;                seq.append(line)&#xD;&#xA;        if name:&#xD;&#xA;            yield (name, ''.join(seq))&#xD;&#xA;    &#xD;&#xA;    isoforms = dict()&#xD;&#xA;    for defline, sequence in parse_fasta(sys.stdin):&#xD;&#xA;        geneid = '.'.join(defline[1:].split('.')[:-1])&#xD;&#xA;        if geneid in isoforms:&#xD;&#xA;            otherdefline, othersequence = isoforms[geneid]&#xD;&#xA;            if len(sequence) &gt; len(othersequence):&#xD;&#xA;                isoforms[geneid] = (defline, sequence)&#xD;&#xA;        else:&#xD;&#xA;            isoforms[geneid] = (defline, sequence)&#xD;&#xA;    &#xD;&#xA;    for defline, sequence in isoforms.values():&#xD;&#xA;        print(defline, sequence, sep='\n')&#xD;&#xA;&#xD;&#xA;Save as `longest.py`, and invoke on the command line like so.&#xD;&#xA;&#xD;&#xA;    python longest.py &lt; intput.fasta &gt; output.fasta&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;The line `geneid = '.'.join(defline[1:].split('.')[:-1])` is key. Let me break it down.&#xD;&#xA;&#xD;&#xA;- `defline[1:]`: ignore the first character of the defline (the `&gt;` symbol)&#xD;&#xA;- `defline[1:].split('.')`: split the string on `.` symbol&#xD;&#xA;- `defline[1:].split('.')[:-1]`: ignore the last value after the split (the isoform name)&#xD;&#xA;- `'.'.join(defline[1:].split('.')[:-1])`: join the split values again&#xD;&#xA;&#xD;&#xA;This value will be the same for all isoforms for the same gene, so we use it in the script to keep track of the longest sequence associated with each gene ID." />
  <row Id="1866" PostHistoryTypeId="5" PostId="598" RevisionGUID="4586c786-8877-44b5-adc2-c2c6970ed4ca" CreationDate="2017-06-08T23:31:58.547" UserId="776" Comment="added 1 character in body" Text="I wrote a command-line k-mer counter called `kmer-counter` that will output results in a form that your Python script can consume: https://github.com/alexpreynolds/kmer-counter&#xD;&#xA;&#xD;&#xA;You can grab, build and install it like so:&#xD;&#xA;&#xD;&#xA;    $ git clone https://github.com/alexpreynolds/kmer-counter.git&#xD;&#xA;    $ cd kmer-counter&#xD;&#xA;    $ make&#xD;&#xA;    $ cp kmer-counter /usr/local/bin&#xD;&#xA;&#xD;&#xA;Once the binary is in your path, you might use it in Python like so:&#xD;&#xA;&#xD;&#xA;    result = {}&#xD;&#xA;    kmerCmd = 'kmer-counter --fasta --k=%d %s' % (k, fastaFile)&#xD;&#xA;    try:&#xD;&#xA;        output = subprocess.check_output(kmerCmd, shell=True)&#xD;&#xA;        for line in output.splitlines():&#xD;&#xA;            (header, counts) = line.strip().split('\t')&#xD;&#xA;            header = header[1:]&#xD;&#xA;            kmers = dict((k,int(v)) for (k,v) in [d.split(':') for d in counts.split(' ')])&#xD;&#xA;            result[header] = kmers&#xD;&#xA;    except subprocess.CalledProcessError as error:&#xD;&#xA;        output = error&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Given example FASTA like this:&#xD;&#xA;&#xD;&#xA;    &gt;foo&#xD;&#xA;    TTAACG&#xD;&#xA;    &gt;bar&#xD;&#xA;    GTGGAAGTTCTTAGGGCATGGCAAAGAGTCAGAATTTGAC&#xD;&#xA;&#xD;&#xA;For k=6, you would get an iterable Python dictionary like this:&#xD;&#xA;&#xD;&#xA;    {'foo': {'TTAACG': 1, 'CGTTAA': 1}, 'bar': {'GTTCTT': 1, 'AGAACT': 1, 'GAGTCA': 1, 'ATGGCA': 1, 'GAACTT': 1, 'ATTCTG': 1, 'CTAAGA': 1, 'CTTCCA': 1, 'ATTTGA': 1, 'GGAAGT': 1, 'AGGGCA': 1, 'CCTAAG': 1, 'CTCTTT': 1, 'AATTTG': 1, 'TCTGAC': 1, 'TTTGCC': 1, 'CTTAGG': 1, 'TTTGAC': 1, 'GAAGTT': 1, 'CCCTAA': 1, 'AGAATT': 1, 'AGTCAG': 1, 'CTGACT': 1, 'TCTTAG': 1, 'CGTTAA': 1, 'GTGGAA': 1, 'TGCCAT': 1, 'ACTCTT': 1, 'GGGCAT': 1, 'TTAGGG': 1, 'CTTTGC': 1, 'TGGAAG': 1, 'GACTCT': 1, 'CATGCC': 1, 'GCAAAG': 1, 'AAATTC': 1, 'GTCAAA': 1, 'TGACTC': 1, 'TAGGGC': 1, 'AAGTTC': 1, 'ATGCCC': 1, 'TCAAAT': 1, 'CAAAGA': 1, 'AACTTC': 1, 'GTCAGA': 1, 'CAAATT': 1, 'TAAGAA': 1, 'CATGGC': 1, 'AAGAAC': 1, 'AAGAGT': 1, 'TCTTTG': 1, 'TTCCAC': 1, 'TGGCAA': 1, 'GGCAAA': 1, 'AGTTCT': 1, 'AGAGTC': 1, 'TCAGAA': 1, 'GAATTT': 1, 'AAAGAG': 1, 'TGCCCT': 1, 'CCATGC': 1, 'GGCATG': 1, 'TTGCCA': 1, 'CAGAAT': 1, 'AATTCT': 1, 'GCATGG': 1, 'ACTTCC': 1, 'TTCTTA': 1, 'GCCATG': 1, 'GCCCTA': 1, 'TTCTGA': 1}}&#xD;&#xA;&#xD;&#xA;See: https://github.com/alexpreynolds/kmer-counter/blob/master/test/kmer-test.py" />
  <row Id="1867" PostHistoryTypeId="5" PostId="577" RevisionGUID="410e6cd7-7b0d-4f69-bca8-1de3b080b55b" CreationDate="2017-06-09T00:00:46.293" UserId="298" Comment="deleted 1 character in body" Text="Many interaction databases now work with [PSI format][1] files. Most of the main databases can do this and the EBI has set up [PSICQUIC View][2], a very useful page where you can query multiple databases at once. &#xD;&#xA;&#xD;&#xA;Note that it is very important to limit the results according to the detection method. There is a lot of noise in protein interaction databases. Depending on what you want to do you could limit to only experimentally verified interactions or to only direct, binary interactions &#xD;&#xA;(so exclude the results of, for example, ChIP analyses which can also find complexes) etc.&#xD;&#xA;&#xD;&#xA;That said, here's a simple example script that will query the APID database using its PSICQUIC service:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-perl --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    use LWP::Simple;&#xD;&#xA;    &#xD;&#xA;    my @proteins;&#xD;&#xA;    ## Read a list of target proteins, one per line (this expects UniProt names)&#xD;&#xA;    open(my $fh, &quot;$ARGV[0]&quot;) or die &quot;Need a list of proteins as the 1st argument: $!\n&quot;;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;    	chomp;&#xD;&#xA;    	push @proteins, $_;&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    ## Get human interactions only&#xD;&#xA;    my $species=&quot;9606&quot;;&#xD;&#xA;    &#xD;&#xA;    ## Get the interactions for each target protein&#xD;&#xA;    foreach my $protein (@proteins) {&#xD;&#xA;    	my $queryUrl= &quot;http://cicblade.dep.usal.es/psicquic-ws/webservices/current/search/query/$protein&quot;;&#xD;&#xA;    	$queryUrl  .= &quot;?taxidA:$species%20ANDtaxidB:$species&quot;;&#xD;&#xA;    &#xD;&#xA;    	my $tries=1;&#xD;&#xA;    	my $content = get $queryUrl;&#xD;&#xA;    	while ($tries&lt;=10) {&#xD;&#xA;    		if (defined($content)) {&#xD;&#xA;    			$tries=11;&#xD;&#xA;    		} else {&#xD;&#xA;    			print STDERR &quot;Could not retrieve $queryUrl, retrying($tries)...\n&quot;;&#xD;&#xA;    			$content = get $queryUrl;&#xD;&#xA;    		}&#xD;&#xA;    		$tries++;&#xD;&#xA;    	}&#xD;&#xA;    &#xD;&#xA;    	# Now list all interactions&#xD;&#xA;    	my @lines = split(/\n/, $content);&#xD;&#xA;    	my $LINES= @lines;&#xD;&#xA;    	my $count = 0;&#xD;&#xA;    	for my $line (@lines) {&#xD;&#xA;    		$count++;&#xD;&#xA;    		my @flds = split(/\t/, $line); # split tab delimited lines      &#xD;&#xA;    		# split fields of a PSIMITAB 2.5 line&#xD;&#xA;    		my ($idA, $idB, $altIdA, $altIdB, $aliasA, $aliasB, $detMethod, $author, $pub, $orgA, $orgB, $intType, $sourceDb, $intID, $conf) = @flds;&#xD;&#xA;    		## Here you can add logic to limit the interactions by specific detection method codes ($detMethod)&#xD;&#xA;    		## or database of origin etc. &#xD;&#xA;    		&#xD;&#xA;    		## Print&#xD;&#xA;    		print &quot;$line\n&quot;&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;I tested it by using a file with the UniProt ID for human TP53 (P04637) and it returns a list of 3988 interactions (showing 10 randomly selected results below):&#xD;&#xA;&#xD;&#xA;    $ foo.pl names.txt | shuf -n 10&#xD;&#xA;    uniprotkb:Q9BWC9	uniprotkb:P04637	-	-	uniprotkb:CCDC106(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0007&quot;(anti tag coimmunoprecipitation)	Zhou, J. et al.(2010)	pubmed:20159018	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-7812926	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:O15126	-	-	uniprotkb:TP53(gene_name)	uniprotkb:SCAMP1(gene_name)	psi-mi:&quot;MI:0018&quot;(two hybrid)	Lim, J. et al.(2006)	pubmed:16713569	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:720622	-&#xD;&#xA;    uniprotkb:P63165	uniprotkb:P04637	-	-	uniprotkb:SUMO1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0018&quot;(two hybrid)	Minty, A. et al.(2000)	pubmed:10961991	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:262339	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:P31350	-	-	uniprotkb:TP53(gene_name)	uniprotkb:RRM2(gene_name)	psi-mi:&quot;MI:0416&quot;(fluorescence microscopy)	Xue, L. et al.(2003)	pubmed:12615712	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0465&quot;(dip)	dip:DIP-40167E	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q00987	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MDM2(gene_name)	psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)	Dai, MS. et al.(2004)	pubmed:15308643	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:478073	-&#xD;&#xA;    uniprotkb:Q99576	uniprotkb:P04637	-	-	uniprotkb:TSC22D3(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0428&quot;(imaging technique)	Ayroldi, E. et al.(2015)	pubmed:25168242	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:1255896	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q00987	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MDM2(gene_name)	psi-mi:&quot;MI:0415&quot;(enzymatic study)	Lui, K. et al.(2013)	pubmed:23572512	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:859223	-&#xD;&#xA;    uniprotkb:P25685	uniprotkb:P04637	-	-	uniprotkb:DNAJB1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)	Qi, M. et al.(2014)	pubmed:24361594	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:938952	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q8IW41	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MAPKAPK5(gene_name)	psi-mi:&quot;MI:0424&quot;(protein kinase assay)	Sun, P. et al.(2007)	pubmed:17254968	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-1202077	-&#xD;&#xA;    uniprotkb:Q13526	uniprotkb:P04637	-	-	uniprotkb:PIN1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0096&quot;(pull down)	Mantovani, F. et al.(2007)	pubmed:17906639	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-6112688	-&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;  [1]: http://psidev.sourceforge.net/molecular_interactions/xml/doc/user/&#xD;&#xA;  [2]: http://www.ebi.ac.uk/Tools/webservices/psicquic/view/main.xhtml" />
  <row Id="1868" PostHistoryTypeId="5" PostId="577" RevisionGUID="af0021b0-fcbe-4f07-aaa0-8089ae6ce736" CreationDate="2017-06-09T00:07:33.717" UserId="298" Comment="deleted 1 character in body" Text="Many interaction databases now work with [PSI format][1] files. Most of the main databases can do this and the EBI has set up [PSICQUIC View][2], a very useful page where you can query multiple databases at once. &#xD;&#xA;&#xD;&#xA;Note that it is very important to limit the results according to the detection method. There is a lot of noise in protein interaction databases. Depending on what you want to do you could limit to only experimentally verified interactions or to only direct, binary interactions &#xD;&#xA;(so exclude the results of, for example, ChIP analyses which can also find complexes) etc.&#xD;&#xA;&#xD;&#xA;That said, here's a simple example script that will query the APID database using its PSICQUIC service:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-perl --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/perl&#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    use LWP::Simple;&#xD;&#xA;    &#xD;&#xA;    my @proteins;&#xD;&#xA;    ## Read a list of target proteins, one per line (this expects UniProt names)&#xD;&#xA;    open(my $fh, &quot;$ARGV[0]&quot;) or die &quot;Need a list of proteins as the 1st argument: $!\n&quot;;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;    	chomp;&#xD;&#xA;    	push @proteins, $_;&#xD;&#xA;    }&#xD;&#xA;    close($fh);&#xD;&#xA;    ## Get human interactions only&#xD;&#xA;    my $species=&quot;9606&quot;;&#xD;&#xA;    &#xD;&#xA;    ## Get the interactions for each target protein&#xD;&#xA;    foreach my $protein (@proteins) {&#xD;&#xA;    	my $queryUrl= &quot;http://cicblade.dep.usal.es/psicquic-ws/webservices/current/search/query/$protein&quot;;&#xD;&#xA;    	$queryUrl  .= &quot;?taxidA:$species%20ANDtaxidB:$species&quot;;&#xD;&#xA;    &#xD;&#xA;    	my $tries=1;&#xD;&#xA;    	my $content = get $queryUrl;&#xD;&#xA;    	while ($tries&lt;=10) {&#xD;&#xA;    		if (defined($content)) {&#xD;&#xA;    			$tries=11;&#xD;&#xA;    		} else {&#xD;&#xA;    			print STDERR &quot;Could not retrieve $queryUrl, retrying($tries)...\n&quot;;&#xD;&#xA;    			$content = get $queryUrl;&#xD;&#xA;    		}&#xD;&#xA;    		$tries++;&#xD;&#xA;    	}&#xD;&#xA;    &#xD;&#xA;    	# Now list all interactions&#xD;&#xA;    	my @lines = split(/\n/, $content);&#xD;&#xA;    	my $LINES= @lines;&#xD;&#xA;    	my $count = 0;&#xD;&#xA;    	for my $line (@lines) {&#xD;&#xA;    		$count++;&#xD;&#xA;    		my @flds = split(/\t/, $line); # split tab delimited lines      &#xD;&#xA;    		# split fields of a PSIMITAB 2.5 line&#xD;&#xA;    		my ($idA, $idB, $altIdA, $altIdB, $aliasA, $aliasB, $detMethod, $author, $pub, $orgA, $orgB, $intType, $sourceDb, $intID, $conf) = @flds;&#xD;&#xA;    		## Here you can add logic to limit the interactions by specific detection method codes ($detMethod)&#xD;&#xA;    		## or database of origin etc. &#xD;&#xA;    		&#xD;&#xA;    		## Print&#xD;&#xA;    		print &quot;$line\n&quot;&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;I tested it by using a file with the UniProt ID for human TP53 (P04637) and it returns a list of 3988 interactions (showing 10 randomly selected results below):&#xD;&#xA;&#xD;&#xA;    $ foo.pl names.txt | shuf -n 10&#xD;&#xA;    uniprotkb:Q9BWC9	uniprotkb:P04637	-	-	uniprotkb:CCDC106(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0007&quot;(anti tag coimmunoprecipitation)	Zhou, J. et al.(2010)	pubmed:20159018	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-7812926	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:O15126	-	-	uniprotkb:TP53(gene_name)	uniprotkb:SCAMP1(gene_name)	psi-mi:&quot;MI:0018&quot;(two hybrid)	Lim, J. et al.(2006)	pubmed:16713569	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:720622	-&#xD;&#xA;    uniprotkb:P63165	uniprotkb:P04637	-	-	uniprotkb:SUMO1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0018&quot;(two hybrid)	Minty, A. et al.(2000)	pubmed:10961991	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:262339	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:P31350	-	-	uniprotkb:TP53(gene_name)	uniprotkb:RRM2(gene_name)	psi-mi:&quot;MI:0416&quot;(fluorescence microscopy)	Xue, L. et al.(2003)	pubmed:12615712	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0465&quot;(dip)	dip:DIP-40167E	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q00987	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MDM2(gene_name)	psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)	Dai, MS. et al.(2004)	pubmed:15308643	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:478073	-&#xD;&#xA;    uniprotkb:Q99576	uniprotkb:P04637	-	-	uniprotkb:TSC22D3(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0428&quot;(imaging technique)	Ayroldi, E. et al.(2015)	pubmed:25168242	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:1255896	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q00987	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MDM2(gene_name)	psi-mi:&quot;MI:0415&quot;(enzymatic study)	Lui, K. et al.(2013)	pubmed:23572512	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:859223	-&#xD;&#xA;    uniprotkb:P25685	uniprotkb:P04637	-	-	uniprotkb:DNAJB1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)	Qi, M. et al.(2014)	pubmed:24361594	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0463&quot;(biogrid)	biogrid:938952	-&#xD;&#xA;    uniprotkb:P04637	uniprotkb:Q8IW41	-	-	uniprotkb:TP53(gene_name)	uniprotkb:MAPKAPK5(gene_name)	psi-mi:&quot;MI:0424&quot;(protein kinase assay)	Sun, P. et al.(2007)	pubmed:17254968	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-1202077	-&#xD;&#xA;    uniprotkb:Q13526	uniprotkb:P04637	-	-	uniprotkb:PIN1(gene_name)	uniprotkb:TP53(gene_name)	psi-mi:&quot;MI:0096&quot;(pull down)	Mantovani, F. et al.(2007)	pubmed:17906639	taxid:9606(Homo sapiens)	taxid:9606(Homo sapiens)	-	psi-mi:&quot;MI:0469&quot;(intact)	intact:EBI-6112688	-&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;  [1]: http://psidev.sourceforge.net/molecular_interactions/xml/doc/user/&#xD;&#xA;  [2]: http://www.ebi.ac.uk/Tools/webservices/psicquic/view/main.xhtml" />
  <row Id="1869" PostHistoryTypeId="5" PostId="369" RevisionGUID="d6971b33-4e43-40a7-8eb3-db2b0e4fc4cc" CreationDate="2017-06-09T01:02:55.120" UserId="37" Comment="Syntax highlighting; added -lz" Text="For FASTQ:&#xD;&#xA;&#xD;&#xA;    seqtk fqchk in.fq | head -2&#xD;&#xA;&#xD;&#xA;It gives you percentage of &quot;N&quot; bases, not the exact count, though.&#xD;&#xA;&#xD;&#xA;For FASTA:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-sh --&gt;&#xD;&#xA;&#xD;&#xA;    seqtk comp in.fa | awk '{x+=$9}END{print x}'&#xD;&#xA;&#xD;&#xA;This command line also works with FASTQ, but it will be slower as awk is slow.&#xD;&#xA;&#xD;&#xA;EDIT: ok, based on @BaCH's reminder, here we go (you need [kseq.h](https://github.com/lh3/readfq/blob/master/kseq.h) to compile):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-c --&gt;&#xD;&#xA;    &#xD;&#xA;    // to compile: gcc -O2 -o count-N this-prog.c -lz&#xD;&#xA;    #include &lt;zlib.h&gt;&#xD;&#xA;    #include &lt;stdio.h&gt;&#xD;&#xA;    #include &lt;stdint.h&gt;&#xD;&#xA;    #include &quot;kseq.h&quot;&#xD;&#xA;    KSEQ_INIT(gzFile, gzread)&#xD;&#xA;    &#xD;&#xA;    unsigned char dna5tbl[256] = {&#xD;&#xA;    	0, 1, 2, 3,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 5, 4, 4,&#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 0, 4, 1,  4, 4, 4, 2,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  3, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 0, 4, 1,  4, 4, 4, 2,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  3, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xD;&#xA;    	4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4&#xD;&#xA;    };&#xD;&#xA;    &#xD;&#xA;    int main(int argc, char *argv[]) {&#xD;&#xA;    	long i, n_n = 0, n_acgt = 0, n_gap = 0;&#xD;&#xA;    	gzFile fp;&#xD;&#xA;    	kseq_t *seq;&#xD;&#xA;    	if (argc == 1) {&#xD;&#xA;    		fprintf(stderr, &quot;Usage: count-N &lt;in.fa&gt;\n&quot;);&#xD;&#xA;    		return 1;&#xD;&#xA;    	}&#xD;&#xA;    	if ((fp = gzopen(argv[1], &quot;r&quot;)) == 0) {&#xD;&#xA;    		fprintf(stderr, &quot;ERROR: fail to open the input file\n&quot;);&#xD;&#xA;    		return 1;&#xD;&#xA;    	}&#xD;&#xA;    	seq = kseq_init(fp);&#xD;&#xA;    	while (kseq_read(seq) &gt;= 0) {&#xD;&#xA;    		for (i = 0; i &lt; seq-&gt;seq.l; ++i) {&#xD;&#xA;    			int c = dna5tbl[(unsigned char)seq-&gt;seq.s[i]];&#xD;&#xA;    			if (c &lt; 4) ++n_acgt;&#xD;&#xA;    			else if (c == 4) ++n_n;&#xD;&#xA;    			else ++n_gap;&#xD;&#xA;    		}&#xD;&#xA;    	}&#xD;&#xA;    	kseq_destroy(seq);&#xD;&#xA;    	gzclose(fp);&#xD;&#xA;    	printf(&quot;%ld\t%ld\t%ld\n&quot;, n_acgt, n_n, n_gap);&#xD;&#xA;    	return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;It works for both FASTA/Q and gzip'ed FASTA/Q. The following uses SeqAn:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-cpp --&gt;&#xD;&#xA;&#xD;&#xA;    #include &lt;seqan/seq_io.h&gt;&#xD;&#xA;&#xD;&#xA;    using namespace seqan;&#xD;&#xA;&#xD;&#xA;    int main(int argc, char *argv[]) {&#xD;&#xA;        if (argc == 1) {&#xD;&#xA;            std::cerr &lt;&lt; &quot;Usage: count-N &lt;in.fastq&gt;&quot; &lt;&lt; std::endl;&#xD;&#xA;            return 1;&#xD;&#xA;        }&#xD;&#xA;        std::ios::sync_with_stdio(false);&#xD;&#xA;        CharString id;&#xD;&#xA;        Dna5String seq;&#xD;&#xA;        SeqFileIn seqFileIn(argv[1]);&#xD;&#xA;        long i, n_n = 0, n_acgt = 0;&#xD;&#xA;        while (!atEnd(seqFileIn)) {&#xD;&#xA;            readRecord(id, seq, seqFileIn);&#xD;&#xA;            for (i = beginPosition(seq); i &lt; endPosition(seq); ++i)&#xD;&#xA;                if (seq[i] &lt; 4) ++n_acgt;&#xD;&#xA;                else ++n_n;&#xD;&#xA;        }&#xD;&#xA;        std::cout &lt;&lt; n_acgt &lt;&lt; '\t' &lt;&lt; n_n &lt;&lt; std::endl;&#xD;&#xA;        return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;On a FASTQ with 4-million 150bp reads:&#xD;&#xA;&#xD;&#xA;* The C version: ~0.74 sec&#xD;&#xA;* The C++ version: ~2.15 sec&#xD;&#xA;* An older C version without a lookup table (see [the previous edit](https://bioinformatics.stackexchange.com/posts/369/revisions)): ~2.65 sec" />
  <row Id="1870" PostHistoryTypeId="5" PostId="598" RevisionGUID="51cd10a4-5194-4c72-b5b5-020f24598112" CreationDate="2017-06-09T01:20:27.493" UserId="776" Comment="added 156 characters in body" Text="I wrote a command-line k-mer counter called `kmer-counter` that will output results in a form that your Python script can consume: https://github.com/alexpreynolds/kmer-counter&#xD;&#xA;&#xD;&#xA;You can grab, build and install it like so:&#xD;&#xA;&#xD;&#xA;    $ git clone https://github.com/alexpreynolds/kmer-counter.git&#xD;&#xA;    $ cd kmer-counter&#xD;&#xA;    $ make&#xD;&#xA;    $ cp kmer-counter /usr/local/bin&#xD;&#xA;&#xD;&#xA;Once the binary is in your path, you might use it in Python like so:&#xD;&#xA;&#xD;&#xA;    k = 6&#xD;&#xA;    fastaFile = '/path/to/some/seqs.fa'&#xD;&#xA;    kmerCmd = 'kmer-counter --fasta --k=%d %s' % (k, fastaFile)&#xD;&#xA;    try:&#xD;&#xA;        output = subprocess.check_output(kmerCmd, shell=True)&#xD;&#xA;        result = {}&#xD;&#xA;        for line in output.splitlines():&#xD;&#xA;            (header, counts) = line.strip().split('\t')&#xD;&#xA;            header = header[1:]&#xD;&#xA;            kmers = dict((k,int(v)) for (k,v) in [d.split(':') for d in counts.split(' ')])&#xD;&#xA;            result[header] = kmers&#xD;&#xA;        sys.stdout.write(&quot;%s&quot; % (str(result)))&#xD;&#xA;    except subprocess.CalledProcessError as error:&#xD;&#xA;        sys.stderr.write(&quot;%s&quot; % (str(error)))&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Given example FASTA like this:&#xD;&#xA;&#xD;&#xA;    &gt;foo&#xD;&#xA;    TTAACG&#xD;&#xA;    &gt;bar&#xD;&#xA;    GTGGAAGTTCTTAGGGCATGGCAAAGAGTCAGAATTTGAC&#xD;&#xA;&#xD;&#xA;For k=6, you would get an iterable Python dictionary like this:&#xD;&#xA;&#xD;&#xA;    {'foo': {'TTAACG': 1, 'CGTTAA': 1}, 'bar': {'GTTCTT': 1, 'AGAACT': 1, 'GAGTCA': 1, 'ATGGCA': 1, 'GAACTT': 1, 'ATTCTG': 1, 'CTAAGA': 1, 'CTTCCA': 1, 'ATTTGA': 1, 'GGAAGT': 1, 'AGGGCA': 1, 'CCTAAG': 1, 'CTCTTT': 1, 'AATTTG': 1, 'TCTGAC': 1, 'TTTGCC': 1, 'CTTAGG': 1, 'TTTGAC': 1, 'GAAGTT': 1, 'CCCTAA': 1, 'AGAATT': 1, 'AGTCAG': 1, 'CTGACT': 1, 'TCTTAG': 1, 'CGTTAA': 1, 'GTGGAA': 1, 'TGCCAT': 1, 'ACTCTT': 1, 'GGGCAT': 1, 'TTAGGG': 1, 'CTTTGC': 1, 'TGGAAG': 1, 'GACTCT': 1, 'CATGCC': 1, 'GCAAAG': 1, 'AAATTC': 1, 'GTCAAA': 1, 'TGACTC': 1, 'TAGGGC': 1, 'AAGTTC': 1, 'ATGCCC': 1, 'TCAAAT': 1, 'CAAAGA': 1, 'AACTTC': 1, 'GTCAGA': 1, 'CAAATT': 1, 'TAAGAA': 1, 'CATGGC': 1, 'AAGAAC': 1, 'AAGAGT': 1, 'TCTTTG': 1, 'TTCCAC': 1, 'TGGCAA': 1, 'GGCAAA': 1, 'AGTTCT': 1, 'AGAGTC': 1, 'TCAGAA': 1, 'GAATTT': 1, 'AAAGAG': 1, 'TGCCCT': 1, 'CCATGC': 1, 'GGCATG': 1, 'TTGCCA': 1, 'CAGAAT': 1, 'AATTCT': 1, 'GCATGG': 1, 'ACTTCC': 1, 'TTCTTA': 1, 'GCCATG': 1, 'GCCCTA': 1, 'TTCTGA': 1}}&#xD;&#xA;&#xD;&#xA;For a fully-fleshed out demonstration, see: https://github.com/alexpreynolds/kmer-counter/blob/master/test/kmer-test.py" />
  <row Id="1871" PostHistoryTypeId="5" PostId="550" RevisionGUID="51b03eaf-a452-47d2-9dfa-ddde3945b300" CreationDate="2017-06-09T01:45:40.630" UserId="714" Comment="corrected grammar" Text="There area few different influenza virus database resources:&#xD;&#xA;&#xD;&#xA;- The [Influenza Research Database (IRD)](http://www.fludb.org) &#xD;&#xA;    - A NIAID Bioinformatics Resource Center or BRC which highly curates the data brought in and integrates it with numerous other relevant data types&#xD;&#xA;- The [NCBI Influenza Virus Resource](https://www.ncbi.nlm.nih.gov/genomes/FLU/Database/nph-select.cgi?go=database)&#xD;&#xA;    - A sub-project of the NCBI with data curated over and above the GenBank data that is part of the NCBI&#xD;&#xA;- The [GISAID EpiFlu Database](http://platform.gisaid.org/)&#xD;&#xA;    - A database of sequences from the Global Initiative on Sharing All Influenza Data. Has unique data from many countries but requires user agree to a data sharing policy.&#xD;&#xA;- The [OpenFluDB](http://openflu.vital-it.ch/browse.php)&#xD;&#xA;    - Former GISAID database that contains some sequence data that GenBank does not have.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;For those who also may be interested in other virus databases, there are:&#xD;&#xA;&#xD;&#xA;- [Virus Pathogen Resource (VIPR)](https://www.viprbrc.org)&#xD;&#xA;    - A companion portal to the IRD, which hosts curated and integrated data for most other NIAID A-C virus pathogens including (but not limited to) Ebola, Zika, Dengue, Enterovirus, and Hepatitis C&#xD;&#xA;- [LANL HIV database](https://www.hiv.lanl.gov/)&#xD;&#xA;   - Los Alamos National Laboratory HIV database with HIV data and many useful tools for all virus bioinformatics&#xD;&#xA;- [PaVE: Papilloma virus genome database](https://pave.niaid.nih.gov) (from quintik comment)&#xD;&#xA;   - NIAID developed and maintained Papilloma virus bioinformatics portal&#xD;&#xA;&#xD;&#xA;Disclaimer: I used to work for the IRD / VIPR and currently work for NIAID." />
  <row Id="1872" PostHistoryTypeId="5" PostId="454" RevisionGUID="65e41b4d-8ea1-4f06-8ff5-18b908066783" CreationDate="2017-06-09T01:58:30.053" UserId="298" Comment="For some reason, the syntax highlighting isn't activated despite the presence of the HTML comment. Any edit seems to fix it so I made a silly one to enable the pretty colors. " Text="Turns out, simply keeping track of the next candidate line (after sorting the sample line numbers) fixes the performance issue, and most of the remaining slowness seems to be due to the overhead of actually reading the file so there’s not very much to improve.&#xD;&#xA;&#xD;&#xA;Since I don’t know how how to do this in `sed`, and it’s not trivial in `awk` either, here’s a Perl script:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-perl --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env perl&#xD;&#xA;    &#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    &#xD;&#xA;    my $file = $ARGV[0];&#xD;&#xA;    my $lines_file = $ARGV[1];&#xD;&#xA;    &#xD;&#xA;    open my $lines_fh, '&lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xD;&#xA;    chomp (my @lines = &lt;$lines_fh&gt;);&#xD;&#xA;    close $lines_fh;&#xD;&#xA;    &#xD;&#xA;    @lines = sort {$a &lt;=&gt; $b} @lines;&#xD;&#xA;    &#xD;&#xA;    open my $fh, '&lt;', $file or die &quot;Cannot read file $file&quot;;&#xD;&#xA;    my $line = 1;&#xD;&#xA;    my $next_line = 0;&#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;        last if $next_line == scalar @lines;&#xD;&#xA;        if ($line++ == $lines[$next_line]) {&#xD;&#xA;            $next_line++;&#xD;&#xA;            print;&#xD;&#xA;        }&#xD;&#xA;    }&#xD;&#xA;    close $fh;&#xD;&#xA;&#xD;&#xA;I’ve implemented a [similar function in C++](https://gist.github.com/klmr/a3e88c0ce102be0d3b2ce52ae83825e5) for an [R package](https://github.com/HannahVMeyer/PhenotypeSimulator), that's only slightly longer than the Perl script. It is ~3 times faster than the Perl script on my test file." />
  <row Id="1873" PostHistoryTypeId="24" PostId="454" RevisionGUID="65e41b4d-8ea1-4f06-8ff5-18b908066783" CreationDate="2017-06-09T01:58:30.053" Comment="Proposed by 298 approved by 37, 73 edit id of 166" />
  <row Id="1874" PostHistoryTypeId="5" PostId="600" RevisionGUID="fe371587-012e-4a6f-998e-2f27d11ce1b1" CreationDate="2017-06-09T01:59:00.970" UserId="205" Comment="&quot;I&quot; to &quot;one&quot;" Text="Just throwing this out there, as all the best stuff has already been said, but why does one need to specify a strict specification for a bioinformatic data format? As you say, what typically happens is all the action ends up in the optional fields.&#xD;&#xA;&#xD;&#xA;There's a lot to be said for the libertarian values of just letting people figure it out on their own. Take optional fields in the BAM tag spec. for example. Here you can have your own tags, but they must start with an &quot;X&quot;, a &quot;Y&quot; or a &quot;Z&quot; and be followed by &quot;[A-Za-z0-9]&quot;. Why? Why can't people use their own tag names like &quot;read is unique&quot; or &quot;edit distance to genome&quot; or whatever they want. Are we not to be trusted with the power of naming things and recalling things arbitrarily? The result is one must look up the tag name's actual meaning in the publication of the tool that produced it, or ask on certain forums, etc. And this is assuming one knows the tool that produced the reads - if not then who knows wtf 'XT' stands for.&#xD;&#xA;&#xD;&#xA;Essentially, downstream readers of optional fields already trust that 'XT' is what it thinks it is. So if we're happy to use optional fields in any capacity, why stop at fields?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Extend this logic to the columns of the data format. Let users determine column names. You may once in a blue moon come across a file with the chromosome column as &quot;chr&quot; rather than &quot;chromosome&quot;, but most of the time you'll be looking for &quot;read is quantum-radioactive&quot;, and detecting and fixing naming-errors is not going to be as much of a problem as using a dataformat that does not store what it is you want to store. Or worse, it can store it, but in a really illogical way that results in everyone who uses the dataformat to have to ask at least 3 questions here or on other forums before they understand whats really going on.&#xD;&#xA;&#xD;&#xA;So you end up with a general solution to the problem of storing tabular information in a compatible way across many different types of software, also known as SQL (or any other kind of general-purpose database such as REDIS, Neo4J, Numpy, etc). In fact, does it even matter what the datastore is, so long as it's tabular and has the &quot;chromosome&quot; and &quot;position&quot; values for each item?&#xD;&#xA;&#xD;&#xA;TL;DR - We will not think of tomorrow's best data format today, because the nature of tomorrow's data is not yet known. Less policing in this area would most likely result in more robust software, where nothing is taken for granted and no assumptions about the data can be made until the schema has been parsed.&#xD;&#xA;&#xD;&#xA;In truth, the only reason we think we need to specify our dataformats is because we know if we didn't, other people will do something we didn't think of - and we, i think falsely, assume that will be bad." />
  <row Id="1875" PostHistoryTypeId="24" PostId="600" RevisionGUID="fe371587-012e-4a6f-998e-2f27d11ce1b1" CreationDate="2017-06-09T01:59:00.970" Comment="Proposed by 205 approved by 37, 73 edit id of 168" />
  <row Id="1876" PostHistoryTypeId="2" PostId="601" RevisionGUID="56b71be6-3331-4ffd-b2a5-d52b6991bc1b" CreationDate="2017-06-09T02:19:01.120" UserId="73" Text="Your sub-problem (2) seems similar to the problem I'm having with *Nippostrongylus brasiliensis* genome sequences, where I'd like to find regions of very high homology (length 500bp to 20kb or more, 95-99% similar) that are repeated throughout the genome. These sequences are killing the assembly.&#xD;&#xA;&#xD;&#xA;The main way I can find these regions is by looking at a coverage plot of long nanopore reads mapped to the assembled genome (using GraphMap or BWA). Any regions with substantially higher than median coverage are likely to be shared repeats.&#xD;&#xA;&#xD;&#xA;I've played around in the past with chopping up the reads to smaller sizes, which works better for hitting smaller repeated regions that are such a small proportion of most reads that they are never mapped to all the repeated locations. I wrote [my own script](https://github.com/gringer/bioinfscripts/blob/master/normalise_seqlengths.pl) a while back to chop up reads (for a different purpose), which produces a FASTA/FASTQ file where all reads are exactly the same length. For some unknown reason I took the time to document that script &quot;properly&quot; using POD, so here's a short summary:&#xD;&#xA;&#xD;&#xA;&gt;  Converts all sequences in the input FASTA file to the same length.&#xD;&#xA;   Sequences shorter than the target length are dropped, and sequences longer&#xD;&#xA;   than the target length are split into overlapping subsequences covering&#xD;&#xA;   the entire range. This prepares the sequences for use in an&#xD;&#xA;   overlap-consensus assembler requiring constant-length sequences (such as&#xD;&#xA;   edena).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;And here's the syntax:&#xD;&#xA;&#xD;&#xA;    $ ./normalise_seqlengths.pl -h&#xD;&#xA;    Usage:&#xD;&#xA;        ./normalise_seqlengths.pl &lt;reads.fa&gt; [options]&#xD;&#xA;    &#xD;&#xA;      Options:&#xD;&#xA;        -help&#xD;&#xA;          Only display this help message&#xD;&#xA;    &#xD;&#xA;        -fraglength&#xD;&#xA;          Target fragment length (in base-pairs, default 2000)&#xD;&#xA;    &#xD;&#xA;        -overlap&#xD;&#xA;          Minimum overlap length (in base-pairs, default 200)&#xD;&#xA;    &#xD;&#xA;        -short&#xD;&#xA;          Keep short sequences (shorter than fraglength)" />
  <row Id="1877" PostHistoryTypeId="5" PostId="601" RevisionGUID="f7e6d591-2100-4a55-a60c-75ef61f04e23" CreationDate="2017-06-09T04:14:39.550" UserId="73" Comment="added 594 characters in body" Text="For (3), [this page](http://www.geneinfinity.org/sp/sp_motif.html) has a lot of links to pattern/motif finding tools. Following through the [YMF](http://bio.cs.washington.edu/YMF/YMFWeb/YMFInput.pl) link on that page, I came across the University of Washington [Motif Discovery](http://bio.cs.washington.edu/software/motif_discovery#Motif%20Discovery) section. Of these [projection](http://www1.cse.wustl.edu/~jbuhler/pgt/) seemed to be the only downloadable tool. I find it interesting how old all these tools are; maybe the introduction of microarrays and NGS has made them all redundant.&#xD;&#xA;&#xD;&#xA;Your sub-problem (2) seems similar to the problem I'm having with *Nippostrongylus brasiliensis* genome sequences, where I'd like to find regions of very high homology (length 500bp to 20kb or more, 95-99% similar) that are repeated throughout the genome. These sequences are killing the assembly.&#xD;&#xA;&#xD;&#xA;The main way I can find these regions is by looking at a coverage plot of long nanopore reads mapped to the assembled genome (using GraphMap or BWA). Any regions with substantially higher than median coverage are likely to be shared repeats.&#xD;&#xA;&#xD;&#xA;I've played around in the past with chopping up the reads to smaller sizes, which works better for hitting smaller repeated regions that are such a small proportion of most reads that they are never mapped to all the repeated locations. I wrote [my own script](https://github.com/gringer/bioinfscripts/blob/master/normalise_seqlengths.pl) a while back to chop up reads (for a different purpose), which produces a FASTA/FASTQ file where all reads are exactly the same length. For some unknown reason I took the time to document that script &quot;properly&quot; using POD, so here's a short summary:&#xD;&#xA;&#xD;&#xA;&gt;  Converts all sequences in the input FASTA file to the same length.&#xD;&#xA;   Sequences shorter than the target length are dropped, and sequences longer&#xD;&#xA;   than the target length are split into overlapping subsequences covering&#xD;&#xA;   the entire range. This prepares the sequences for use in an&#xD;&#xA;   overlap-consensus assembler requiring constant-length sequences (such as&#xD;&#xA;   edena).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;And here's the syntax:&#xD;&#xA;&#xD;&#xA;    $ ./normalise_seqlengths.pl -h&#xD;&#xA;    Usage:&#xD;&#xA;        ./normalise_seqlengths.pl &lt;reads.fa&gt; [options]&#xD;&#xA;    &#xD;&#xA;      Options:&#xD;&#xA;        -help&#xD;&#xA;          Only display this help message&#xD;&#xA;    &#xD;&#xA;        -fraglength&#xD;&#xA;          Target fragment length (in base-pairs, default 2000)&#xD;&#xA;    &#xD;&#xA;        -overlap&#xD;&#xA;          Minimum overlap length (in base-pairs, default 200)&#xD;&#xA;    &#xD;&#xA;        -short&#xD;&#xA;          Keep short sequences (shorter than fraglength)" />
  <row Id="1878" PostHistoryTypeId="2" PostId="602" RevisionGUID="ce85c15e-812c-47c7-b484-8c5b4624fd8b" CreationDate="2017-06-09T07:07:13.327" UserId="48" Text="Error is the estimation that can't be fitted by your parameters (in this case g). &#xD;&#xA;The amova function in pegas package does the following: &#xD;&#xA;&#xD;&#xA;&gt; This function performs a hierarchical analysis of molecular variance as described in Excoffier et al.&#xD;&#xA;(1992). This implementation accepts any number of hierarchical levels&#xD;&#xA;&#xD;&#xA;And as per the definition of Fixation index, you need the variance in the frequency of the allele between different subpopulations, weighted by the sizes of the subpopulations, the variance of the allelic state in the total population. Which is not provided by amova. Thus you cannot compute the fixation index from amova results. " />
  <row Id="1879" PostHistoryTypeId="2" PostId="603" RevisionGUID="0200d1da-b63e-44af-b80a-c779a4b8ca38" CreationDate="2017-06-09T07:17:47.383" UserId="77" Text="While the solution from https://bioinformatics.stackexchange.com/users/96/daniel-standage should work (after adjusting for possible python3 incompatibility), the following is a shorter and less memory demanding method that uses biopython:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    import sys&#xD;&#xA;&#xD;&#xA;    lastGene = None&#xD;&#xA;    longest = (None, None)&#xD;&#xA;    for rec in SeqIO.parse(sys.argv[1], &quot;fasta&quot;):&#xD;&#xA;        gene = &quot;.&quot;.join(rec.id.split(&quot;.&quot;)[:-1])&#xD;&#xA;        l = len(rec)&#xD;&#xA;        if lastGene is not None:&#xD;&#xA;            if gene == lastGene:&#xD;&#xA;                if longest[0] &lt; l:&#xD;&#xA;                    longest = (l, rec)&#xD;&#xA;            else:&#xD;&#xA;                lastGene = gene&#xD;&#xA;                SeqIO.write(longest[1], sys.stdout, &quot;fasta&quot;)&#xD;&#xA;                longest = (l, rec)&#xD;&#xA;        else:&#xD;&#xA;            lastGene = gene&#xD;&#xA;            longest = (l, rec)&#xD;&#xA;    SeqIO.write(longest[1], sys.stdout, &quot;fasta&quot;)&#xD;&#xA;&#xD;&#xA;If you saved this as `filter.py`, then `filter.py original.fa &gt; subset.fa` would be the command to use." />
  <row Id="1880" PostHistoryTypeId="5" PostId="150" RevisionGUID="cf79f3b6-c824-4e6c-ab4e-be93cf502d95" CreationDate="2017-06-09T08:05:55.257" UserId="200" Comment="added 23 characters in body" Text="I have an [obitools script](http://datadryad.org/bitstream/handle/10255/dryad.122664/obitools_script.txt?sequence=1) (de Barba et al. 2016) that I would like to run faster. How would you run it in parallel to cut down on time?&#xD;&#xA;&#xD;&#xA;    illuminapairedend -r rawdata_scandinavia_R2.fastq rawdata_scandinavia_R1.fastq | tee rawdata_scandinavia.fastq | obiannotate -S goodAli:'&quot;Alignement&quot; if score&gt;40.00 else &quot;Bad&quot;' | obisplit -t goodAli -p rawdata_scandinavia.&#xD;&#xA;    touch rawdata_scandinavia.Bad.fastq&#xD;&#xA;    touch rawdata_scandinavia.Alignement.fastq&#xD;&#xA;    touch rawdata_scandinavia.fastq&#xD;&#xA;    ngsfilter -t rawdata_scandinavia.ngsfilter -u rawdata_scandinavia.unidentified.fastq rawdata_scandinavia.Alignement.fastq &gt; rawdata_scandinavia.filtered.fastq&#xD;&#xA;    obisplit -p MICROSAT.PCR_ -t experiment rawdata_scandinavia.filtered.fastq&#xD;&#xA;&#xD;&#xA;Can someone describe what tools and steps one needs to take to run this job in parallel, given that the functions are not designed to be run in parallel out of the box?&#xD;&#xA;&#xD;&#xA;De Barba, M., Miquel, C., Lobréaux, S., Quenette, P. Y., Swenson, J. E., &amp; Taberlet, P. (2016). High-throughput microsatellite genotyping in ecology: improved accuracy, efficiency, standardization and success with low-quantity and degraded DNA. Molecular Ecology Resources, 1–16. https://doi.org/10.1111/1755-0998.12594" />
  <row Id="1881" PostHistoryTypeId="5" PostId="603" RevisionGUID="46d9acf8-fbce-48f4-ab17-7f3bc8bd6160" CreationDate="2017-06-09T08:06:41.350" UserId="191" Comment="add syntax highlighting" Text="While the solution from https://bioinformatics.stackexchange.com/users/96/daniel-standage should work (after adjusting for possible python3 incompatibility), the following is a shorter and less memory demanding method that uses biopython:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    import sys&#xD;&#xA;&#xD;&#xA;    lastGene = None&#xD;&#xA;    longest = (None, None)&#xD;&#xA;    for rec in SeqIO.parse(sys.argv[1], &quot;fasta&quot;):&#xD;&#xA;        gene = &quot;.&quot;.join(rec.id.split(&quot;.&quot;)[:-1])&#xD;&#xA;        l = len(rec)&#xD;&#xA;        if lastGene is not None:&#xD;&#xA;            if gene == lastGene:&#xD;&#xA;                if longest[0] &lt; l:&#xD;&#xA;                    longest = (l, rec)&#xD;&#xA;            else:&#xD;&#xA;                lastGene = gene&#xD;&#xA;                SeqIO.write(longest[1], sys.stdout, &quot;fasta&quot;)&#xD;&#xA;                longest = (l, rec)&#xD;&#xA;        else:&#xD;&#xA;            lastGene = gene&#xD;&#xA;            longest = (l, rec)&#xD;&#xA;    SeqIO.write(longest[1], sys.stdout, &quot;fasta&quot;)&#xD;&#xA;&#xD;&#xA;If you saved this as `filter.py`, then `filter.py original.fa &gt; subset.fa` would be the command to use." />
  <row Id="1882" PostHistoryTypeId="24" PostId="603" RevisionGUID="46d9acf8-fbce-48f4-ab17-7f3bc8bd6160" CreationDate="2017-06-09T08:06:41.350" Comment="Proposed by 191 approved by 73, 77 edit id of 169" />
  <row Id="1883" PostHistoryTypeId="5" PostId="599" RevisionGUID="c297d3f5-a9d2-429b-95b7-1b17125b121f" CreationDate="2017-06-09T08:07:00.293" UserId="191" Comment="add syntax highlighting" Text="This solution is will work for your example and probably all Augustus-derived Fastas, but mileage will vary beyond that.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from __future__ import print_function&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    def parse_fasta(data):&#xD;&#xA;        &quot;&quot;&quot;Stolen shamelessly from http://stackoverflow.com/a/7655072/459780.&quot;&quot;&quot;&#xD;&#xA;        name, seq = None, []&#xD;&#xA;        for line in data:&#xD;&#xA;            line = line.rstrip()&#xD;&#xA;            if line.startswith('&gt;'):&#xD;&#xA;                if name:&#xD;&#xA;                    yield (name, ''.join(seq))&#xD;&#xA;                name, seq = line, []&#xD;&#xA;            else:&#xD;&#xA;                seq.append(line)&#xD;&#xA;        if name:&#xD;&#xA;            yield (name, ''.join(seq))&#xD;&#xA;    &#xD;&#xA;    isoforms = dict()&#xD;&#xA;    for defline, sequence in parse_fasta(sys.stdin):&#xD;&#xA;        geneid = '.'.join(defline[1:].split('.')[:-1])&#xD;&#xA;        if geneid in isoforms:&#xD;&#xA;            otherdefline, othersequence = isoforms[geneid]&#xD;&#xA;            if len(sequence) &gt; len(othersequence):&#xD;&#xA;                isoforms[geneid] = (defline, sequence)&#xD;&#xA;        else:&#xD;&#xA;            isoforms[geneid] = (defline, sequence)&#xD;&#xA;    &#xD;&#xA;    for defline, sequence in isoforms.values():&#xD;&#xA;        print(defline, sequence, sep='\n')&#xD;&#xA;&#xD;&#xA;Save as `longest.py`, and invoke on the command line like so.&#xD;&#xA;&#xD;&#xA;    python longest.py &lt; intput.fasta &gt; output.fasta&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;The line `geneid = '.'.join(defline[1:].split('.')[:-1])` is key. Let me break it down.&#xD;&#xA;&#xD;&#xA;- `defline[1:]`: ignore the first character of the defline (the `&gt;` symbol)&#xD;&#xA;- `defline[1:].split('.')`: split the string on `.` symbol&#xD;&#xA;- `defline[1:].split('.')[:-1]`: ignore the last value after the split (the isoform name)&#xD;&#xA;- `'.'.join(defline[1:].split('.')[:-1])`: join the split values again&#xD;&#xA;&#xD;&#xA;This value will be the same for all isoforms for the same gene, so we use it in the script to keep track of the longest sequence associated with each gene ID." />
  <row Id="1884" PostHistoryTypeId="24" PostId="599" RevisionGUID="c297d3f5-a9d2-429b-95b7-1b17125b121f" CreationDate="2017-06-09T08:07:00.293" Comment="Proposed by 191 approved by 73, 77 edit id of 170" />
  <row Id="1885" PostHistoryTypeId="2" PostId="604" RevisionGUID="bcba84fe-5e39-43b2-82d8-f1921ad8a29b" CreationDate="2017-06-09T09:15:26.540" UserId="263" Text="I got a customized GRCh38.79 .gtf file (modified to have no MT genes) and I need to create a reference genome out of it (for 10xGenomics CellRanger pipeline). I suspect that the .79 part is the Ensembl number, which according this [ensembl archive list][1] is paired with the GRCh38.p2 patch.&#xD;&#xA;&#xD;&#xA;Should I use this patch's fasta file, or it would be fine to use any of the GRCh38 patches?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ensembl.org/Help/ArchiveList" />
  <row Id="1886" PostHistoryTypeId="1" PostId="604" RevisionGUID="bcba84fe-5e39-43b2-82d8-f1921ad8a29b" CreationDate="2017-06-09T09:15:26.540" UserId="263" Text="Can a customized GRCh38 .gtf file be used with any of the GRCh38 released patches?" />
  <row Id="1887" PostHistoryTypeId="3" PostId="604" RevisionGUID="bcba84fe-5e39-43b2-82d8-f1921ad8a29b" CreationDate="2017-06-09T09:15:26.540" UserId="263" Text="&lt;human-genome&gt;&lt;reference-genome&gt;" />
  <row Id="1888" PostHistoryTypeId="2" PostId="605" RevisionGUID="9b096fdf-c150-4a16-b229-3c9dc2da80b9" CreationDate="2017-06-09T09:41:25.517" UserId="599" Text="Most tools I know of looks for enrichment of specific motifs - but that requires that you have a set of sequences which are of special interest and a background set to test against.&#xD;&#xA;&#xD;&#xA;Is that your case?" />
  <row Id="1889" PostHistoryTypeId="2" PostId="606" RevisionGUID="89ba0524-f2b6-493a-9271-59eaddc63a14" CreationDate="2017-06-09T10:20:12.173" UserId="787" Text="it is under development, but maybe [BaMMmotif!][1] is something for you?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/soedinglab/BaMMmotif" />
  <row Id="1890" PostHistoryTypeId="2" PostId="607" RevisionGUID="dba66809-7134-497d-a1a4-8a5750bffea4" CreationDate="2017-06-09T10:51:42.197" UserId="383" Text="Here is a solution in R. Could get really slow with big files. Works for the example you posted.&#xD;&#xA;&#xD;&#xA;    library(Biostrings)&#xD;&#xA;    &#xD;&#xA;    ## read your fasta in as Biostrings object&#xD;&#xA;    fasta.s &lt;- readDNAStringSet(&quot;sample.fa&quot;)&#xD;&#xA;    &#xD;&#xA;    ## get the read names (in your case it has the isoform info)&#xD;&#xA;    names.fasta &lt;- names(fasta.s)&#xD;&#xA;    &#xD;&#xA;    ## extract only the relevant gene and isoform id (split name by the period symbol)&#xD;&#xA;    gene.iso &lt;- sapply(names.fasta,function(j) cbind(unlist(strsplit(j,'\\.'))[2:3]))&#xD;&#xA;    &#xD;&#xA;    ## convert to good data.frame = transpose result from previous step and add relevant column names&#xD;&#xA;    gene.iso.df &lt;- data.frame(t(gene.iso))&#xD;&#xA;    colnames(gene.iso.df) &lt;- c('gene','isoform')&#xD;&#xA;    &#xD;&#xA;    ## and length of isoforms&#xD;&#xA;    gene.iso.df$width &lt;- width(fasta.s)&#xD;&#xA;    &#xD;&#xA;    ## split data.frame into list with entry for each gene&#xD;&#xA;    gene.iso.df.split &lt;- split(gene.iso.df,gene.iso.df$gene)&#xD;&#xA;    &#xD;&#xA;    ## optional to keep all the information but really you just need indices&#xD;&#xA;    ##gene.iso.df.split.best &lt;- lapply(gene.iso.df.split,function(x) x[order(x$width)[1],])&#xD;&#xA;    &#xD;&#xA;    ## pull out the longest isoform ID for each gene (in case of a tie just take the first one)&#xD;&#xA;    best.id &lt;- sapply(gene.iso.df.split,function(x) row.names(x)[order(x$width)[1]])&#xD;&#xA;    &#xD;&#xA;    ## subset your original reads with the subset&#xD;&#xA;    fasta.s.best &lt;- fasta.s[best.id]&#xD;&#xA;    &#xD;&#xA;    ## export new fastafile containing longest isoform per gene&#xD;&#xA;    writeXStringSet(fasta.s, filepath='sample_best_isoform.fasta')&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1891" PostHistoryTypeId="5" PostId="567" RevisionGUID="8ae144ab-e6e3-49d7-bd5a-779979ab093e" CreationDate="2017-06-09T10:54:13.770" UserId="734" Comment="deleted 1 character in body" Text="Assume I have found the top 0.01% most frequent genes from a [gene expression file][1]. &#xD;&#xA;&#xD;&#xA;Let's say, these are 10 genes and I want to study the [protein protein interactions][2], the protein network and pathway.&#xD;&#xA; &#xD;&#xA;I thought to use [string-db][3] or [interactome][4], but I am not sure what would be a plausible way to approach this problem and to build the protein network etc. Are there other more suitable databases?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/559/how-to-read-and-interpret-a-gene-expression-quantification-file&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Protein%E2%80%93protein_interaction&#xD;&#xA;  [3]: https://string-db.org&#xD;&#xA;  [4]: http://interactome.dfci.harvard.edu" />
  <row Id="1893" PostHistoryTypeId="5" PostId="380" RevisionGUID="61731abe-18a3-4a64-b55e-58c1e657e036" CreationDate="2017-06-09T12:49:56.420" UserId="292" Comment="Added solution using SimpleFastaParser" Text="## Using bioawk&#xD;&#xA;&#xD;&#xA;With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (**including gzipped versions**), so the above command **should also handle fastq format robustly**.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters (got it from &lt;https://unix.stackexchange.com/a/169575/55127&gt;). I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)&#xD;&#xA;&#xD;&#xA;## Using python&#xD;&#xA;&#xD;&#xA;### readfq&#xD;&#xA;&#xD;&#xA;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;https://bioinformatics.stackexchange.com/a/365/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.976s&#xD;&#xA;    user	0m0.876s&#xD;&#xA;    sys 	0m0.100s&#xD;&#xA;&#xD;&#xA;### &quot;pure python&quot;&#xD;&#xA;&#xD;&#xA;Comparing with something adapted from this answer: &lt;https://bioinformatics.stackexchange.com/a/363/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&gt;')))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.143s&#xD;&#xA;    user	0m1.100s&#xD;&#xA;    sys 	0m0.040s&#xD;&#xA;&#xD;&#xA;(It is slower with python 3.6 than with python 2.7 on my computer)&#xD;&#xA;&#xD;&#xA;### pyGATB&#xD;&#xA;&#xD;&#xA;I just learned (08/06/2017) that [GATB](http://gatb.inria.fr/) includes a fasta/fastq parser and has recently released a python API. I tried to use it yesterday to test another answer to the present question and [found a bug](https://github.com/GATB/pyGATB/issues/2). This bug is now fixed, so here is a [pyGATB](https://pypi.python.org/pypi/pyGATB/0.1.2)-based answer:&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.count(b\&quot;A\&quot;) for seq in Bank(\&quot;genome.fa\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.663s&#xD;&#xA;    user	0m0.568s&#xD;&#xA;    sys 	0m0.092s&#xD;&#xA;&#xD;&#xA;(You can also do `sequence.decode(&quot;utf-8&quot;).count(&quot;A&quot;)` but this seems [a little slower](https://github.com/GATB/pyGATB/issues/2#issuecomment-307131176).)&#xD;&#xA;&#xD;&#xA;Although I used python3.6 here (pyGATB seems python3-only), this is faster than the other two python approaches (for which the reported timings are obtained with python 2.7). This is even almost as fast as the `grep`, `tr`, `wc` pipeline.&#xD;&#xA;&#xD;&#xA;### Biopython&#xD;&#xA;&#xD;&#xA;And, to have even more comparisons, here is a solution using `SeqIO.parse` from Biopython (with python2.7):&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from Bio import SeqIO; print(sum((rec.seq.count(\&quot;A\&quot;) for rec in SeqIO.parse(\&quot;genome.fa\&quot;, \&quot;fasta\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.632s&#xD;&#xA;    user	0m1.532s&#xD;&#xA;    sys 	0m0.096s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;This is a bit slower than the &quot;pure python&quot; solution, but perhaps more robust.&#xD;&#xA;&#xD;&#xA;There seems to be a slight improvement with @peterjc's suggestion to use the lower level `SimpleFastaParser`:&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from Bio.SeqIO.FastaIO import SimpleFastaParser; print(sum(seq.count('A') for title, seq in SimpleFastaParser(open('genome.fa'))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.618s&#xD;&#xA;    user	0m1.500s&#xD;&#xA;    sys 	0m0.116s&#xD;&#xA;&#xD;&#xA;(I did a series of timings and tried to take one that seemed representative, but there's a lot of overlap with the higher-level parser's timings.)" />
  <row Id="1894" PostHistoryTypeId="5" PostId="567" RevisionGUID="03b96dc5-4e69-4443-8e33-ca41e718c9d9" CreationDate="2017-06-09T14:09:08.673" UserId="734" Comment="added 65 characters in body" Text="Assume I have found the top 0.01% most frequent genes from a [gene expression file][1]. &#xD;&#xA;&#xD;&#xA;Let's say, these are 10 genes and I want to study the [protein protein interactions][2], the protein network and pathway.&#xD;&#xA; &#xD;&#xA;I thought to use [string-db][3] or [interactome][4], but I am not sure what would be a plausible way to approach this problem and to build the protein network etc. Are there other more suitable databases?&#xA;&#xA;How can I build a mathematical graph or network for these data?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/559/how-to-read-and-interpret-a-gene-expression-quantification-file&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Protein%E2%80%93protein_interaction&#xD;&#xA;  [3]: https://string-db.org&#xD;&#xA;  [4]: http://interactome.dfci.harvard.edu" />
  <row Id="1895" PostHistoryTypeId="2" PostId="608" RevisionGUID="897f3be6-4802-496b-ba62-ff6affab2fd6" CreationDate="2017-06-09T15:08:40.710" UserId="292" Text="I start with a sorted and indexed bam file (&quot;mapped.bam&quot;) representing the mapping of small reads on a reference genome, and a bed file (&quot;genes.bed&quot;) containing the coordinates of a set of features of interest (let's say they are genes), for which I want to compute an average profile using programs from [deeptools](http://deeptools.readthedocs.io/en/latest/content/list_of_tools.html). **I would like to understand the steps involved to be sure of what the vertical axis of the final profile represents.**&#xD;&#xA;&#xD;&#xA;### First step: making a bigwig file&#xD;&#xA;&#xD;&#xA;I create a bigwig file (&quot;mapped.bw&quot;) from the bam file using `bamCoverage` as follows:&#xD;&#xA;&#xD;&#xA;    bamCoverage -b mapped.bam -bs 10 -of=bigwig -o mapped.bw&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The help of `bamCoverage` says:&#xD;&#xA;&#xD;&#xA;&gt; The coverage is calculated as the number of reads per bin, where bins are short consecutive counting windows of a defined size.&#xD;&#xA;&#xD;&#xA;In my case, the bins are 10 bp long. My reads are longer than that.&#xD;&#xA;&#xD;&#xA;For a given bin, a given read can:&#xD;&#xA;&#xD;&#xA;1. completely overlap the bin&#xD;&#xA;&#xD;&#xA;2. overlap the bin on n bp, n &lt; 10&#xD;&#xA;&#xD;&#xA;3. not overlap the bin at all&#xD;&#xA;&#xD;&#xA;Please correct me if I'm wrong: My guess it that the read is counted as 1 in cases 1. and 2., and 0 otherwise, and I also suppose that a read can be counted for several successive bins if it is long enough.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### Second step: averaging over genes and plotting&#xD;&#xA;&#xD;&#xA;I compute a &quot;meta profile matrix&quot; (&quot;mapped_on_genes.gz&quot;) using `computeMatrix scale-regions` as follows:&#xD;&#xA;&#xD;&#xA;    computeMatrix scale-regions \&#xD;&#xA;        -S mapped.bw \&#xD;&#xA;        -R genes.bed \&#xD;&#xA;        --upstream 300 \&#xD;&#xA;        --unscaled5prime 500 \&#xD;&#xA;        --regionBodyLength 2000 \&#xD;&#xA;        --unscaled3prime 500 \&#xD;&#xA;        --downstream 300 \&#xD;&#xA;        -out mapped_on_genes.gz&#xD;&#xA;&#xD;&#xA;(There is s `-bs` parameter which default value is 10 according to the help of the command.)&#xD;&#xA;&#xD;&#xA;I use this to plot a profile using `plotProfile`:&#xD;&#xA;&#xD;&#xA;    plotProfile -m mapped_on_genes.gz \&#xD;&#xA;        -out mapped_on_genes_meta_profile.pdf&#xD;&#xA;&#xD;&#xA;I obtain a profile in with values on the y axis. In what units are these values?&#xD;&#xA;&#xD;&#xA;My guess is the following:&#xD;&#xA;&#xD;&#xA;For the upstream (300 bp) and internal 5-prime (500 bp), since the bin size was the same in `bamCoverage` and `computeMatrix`, each point on the x axis probably represents a 10 bp window, and its y coordinate is the average over the regions present in the bed file of the corresponding bins in the bigwig file, so it is an average number of reads overlapping a 10 bp bin.&#xD;&#xA;&#xD;&#xA;Same thing at the 3-prime and downstream side.&#xD;&#xA;&#xD;&#xA;For the central 100 bp portion, before averaging over regions some shrinking or spreading of the bins must have been performed, I guess by averaging between neighbouring bins. So the final unit is still a **number of reads overlapping a 10 bp bin**.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;And if I use larger bins, I should end up with proportionally higher values.&#xD;&#xA;&#xD;&#xA;**Am I correct?**" />
  <row Id="1896" PostHistoryTypeId="1" PostId="608" RevisionGUID="897f3be6-4802-496b-ba62-ff6affab2fd6" CreationDate="2017-06-09T15:08:40.710" UserId="292" Text="What unit do I get on the y-axis of a metagene profile plot?" />
  <row Id="1897" PostHistoryTypeId="3" PostId="608" RevisionGUID="897f3be6-4802-496b-ba62-ff6affab2fd6" CreationDate="2017-06-09T15:08:40.710" UserId="292" Text="&lt;hts&gt;&lt;deeptools&gt;&lt;bigwig&gt;&lt;metagene&gt;" />
  <row Id="1898" PostHistoryTypeId="2" PostId="609" RevisionGUID="b8d96ca2-65b8-41a5-81d9-6cee8c7cd997" CreationDate="2017-06-09T15:09:55.103" UserId="796" Text="I have two different internal standards in a sample (1), and a sample (2) with these internal standards as well as analyte. From sample 1, I can get the true ratio between standard 1 (S1) and standard 2 (S2), and from sample 2 I can get the ratio between the analyte (L) to each of the standards:&#xD;&#xA;&#xD;&#xA;Sample 1: S1/S2&#xD;&#xA;&#xD;&#xA;Sample 2: L/S1, L/S2&#xD;&#xA;&#xD;&#xA;I can thus calculate the ratio S1/S2 from the measured ratios in sample 2. Each sample is run in 5 technical replicates, so S1/S2 ratios for one particular analyte could look like this (mock data):&#xD;&#xA;&#xD;&#xA;    measured	calculated&#xD;&#xA;    0.967		0.987&#xD;&#xA;    1.007		0.967&#xD;&#xA;    1.044		1.012&#xD;&#xA;    1.041		1.025&#xD;&#xA;    1.048		1.046&#xD;&#xA;&#xD;&#xA;I want to compare the measured ratios to the calculated, to see if there are any bias in my acquisition of data. As the calculated and the measured ratio are from different samples, the replicates aren't &quot;linked&quot; so I could not use for example Bland Altman analysis. Should I use some kind of ANOVA? Or do you have any other recommendation?&#xD;&#xA;&#xD;&#xA;Any input would be greatly appreciated.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;" />
  <row Id="1899" PostHistoryTypeId="1" PostId="609" RevisionGUID="b8d96ca2-65b8-41a5-81d9-6cee8c7cd997" CreationDate="2017-06-09T15:09:55.103" UserId="796" Text="How do I compare measured ratios to calculated ratios?" />
  <row Id="1900" PostHistoryTypeId="3" PostId="609" RevisionGUID="b8d96ca2-65b8-41a5-81d9-6cee8c7cd997" CreationDate="2017-06-09T15:09:55.103" UserId="796" Text="&lt;proteins&gt;" />
  <row Id="1901" PostHistoryTypeId="5" PostId="595" RevisionGUID="cd7f1042-d0f0-46b4-8247-85fd4fb46078" CreationDate="2017-06-09T15:15:22.383" UserId="191" Comment="use url syntax &amp; add tag" Text="Is there a convenient way to extract the longest isoforms from a .fasta transcriptome file? I had found some [scripts on biostars](https://www.biostars.org/p/107759/) but none are functional and I'm having difficulty getting them to work.&#xD;&#xA;&#xD;&#xA;I'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes.&#xD;&#xA;&#xD;&#xA;The fasta was generated via Augustus. Here is what the fasta file looks like currently (sequence shortened to save space)&#xD;&#xA;&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;    ttaatataaccaaaaacctagaaattctagccttcgatgatgttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t2&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;&#xD;&#xA;The format is as such:&#xD;&#xA;&#xD;&#xA;    Gene 1 isoform 1  &#xD;&#xA;    Gene 1 isoform 2  &#xD;&#xA;    Gene 2 isoform 1  &#xD;&#xA;    Gene 2 isoform 2   &#xD;&#xA;&#xD;&#xA;and so forth. There are several genes that have more than one pair of isoforms (up to 3 or 4). There are roughly 80,000 total transcripts, probably 25,000 genes." />
  <row Id="1902" PostHistoryTypeId="4" PostId="595" RevisionGUID="cd7f1042-d0f0-46b4-8247-85fd4fb46078" CreationDate="2017-06-09T15:15:22.383" UserId="191" Comment="use url syntax &amp; add tag" Text="How to extract longest isoforms from .fasta file?" />
  <row Id="1903" PostHistoryTypeId="6" PostId="595" RevisionGUID="cd7f1042-d0f0-46b4-8247-85fd4fb46078" CreationDate="2017-06-09T15:15:22.383" UserId="191" Comment="use url syntax &amp; add tag" Text="&lt;fasta&gt;&lt;isoform&gt;" />
  <row Id="1904" PostHistoryTypeId="24" PostId="595" RevisionGUID="cd7f1042-d0f0-46b4-8247-85fd4fb46078" CreationDate="2017-06-09T15:15:22.383" Comment="Proposed by 191 approved by 77, -1 edit id of 171" />
  <row Id="1905" PostHistoryTypeId="5" PostId="595" RevisionGUID="1a77a7a6-f256-4dbd-8589-1eb07d8acd4a" CreationDate="2017-06-09T15:15:22.383" UserId="73" Comment="use url syntax &amp; add tag; clarify question as per-gene" Text="Is there a convenient way to extract the longest isoforms from a transcriptome fasta file? I had found some [scripts on biostars](https://www.biostars.org/p/107759/) but none are functional and I'm having difficulty getting them to work.&#xD;&#xA;&#xD;&#xA;I'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes.&#xD;&#xA;&#xD;&#xA;The fasta was generated via Augustus. Here is what the fasta file looks like currently (sequence shortened to save space)&#xD;&#xA;&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;    ttaatataaccaaaaacctagaaattctagccttcgatgatgttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t2&#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xD;&#xA;&#xD;&#xA;The format is as such:&#xD;&#xA;&#xD;&#xA;    Gene 1 isoform 1  &#xD;&#xA;    Gene 1 isoform 2  &#xD;&#xA;    Gene 2 isoform 1  &#xD;&#xA;    Gene 2 isoform 2   &#xD;&#xA;&#xD;&#xA;and so forth. There are several genes that have more than one pair of isoforms (up to 3 or 4). There are roughly 80,000 total transcripts, probably 25,000 genes. I would like to extract the single longest isoform for each gene." />
  <row Id="1906" PostHistoryTypeId="4" PostId="595" RevisionGUID="1a77a7a6-f256-4dbd-8589-1eb07d8acd4a" CreationDate="2017-06-09T15:15:22.383" UserId="73" Comment="use url syntax &amp; add tag; clarify question as per-gene" Text="How can longest isoforms (per gene) be extracted from a FASTA file?" />
  <row Id="1907" PostHistoryTypeId="6" PostId="595" RevisionGUID="1a77a7a6-f256-4dbd-8589-1eb07d8acd4a" CreationDate="2017-06-09T15:15:22.383" UserId="73" Comment="use url syntax &amp; add tag; clarify question as per-gene" Text="&lt;fasta&gt;&lt;isoform&gt;&lt;filtering&gt;" />
  <row Id="1908" PostHistoryTypeId="5" PostId="608" RevisionGUID="4664265d-b65b-42ea-98fa-c7c166f83b42" CreationDate="2017-06-09T15:30:37.097" UserId="292" Comment="edited body" Text="I start with a sorted and indexed bam file (&quot;mapped.bam&quot;) representing the mapping of small reads on a reference genome, and a bed file (&quot;genes.bed&quot;) containing the coordinates of a set of features of interest (let's say they are genes), for which I want to compute an average profile using programs from [deeptools](http://deeptools.readthedocs.io/en/latest/content/list_of_tools.html). **I would like to understand the steps involved to be sure of what the vertical axis of the final profile represents.**&#xD;&#xA;&#xD;&#xA;### First step: making a bigwig file&#xD;&#xA;&#xD;&#xA;I create a bigwig file (&quot;mapped.bw&quot;) from the bam file using `bamCoverage` as follows:&#xD;&#xA;&#xD;&#xA;    bamCoverage -b mapped.bam -bs 10 -of=bigwig -o mapped.bw&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The help of `bamCoverage` says:&#xD;&#xA;&#xD;&#xA;&gt; The coverage is calculated as the number of reads per bin, where bins are short consecutive counting windows of a defined size.&#xD;&#xA;&#xD;&#xA;In my case, the bins are 10 bp long. My reads are longer than that.&#xD;&#xA;&#xD;&#xA;For a given bin, a given read can:&#xD;&#xA;&#xD;&#xA;1. completely overlap the bin&#xD;&#xA;&#xD;&#xA;2. overlap the bin on n bp, n &lt; 10&#xD;&#xA;&#xD;&#xA;3. not overlap the bin at all&#xD;&#xA;&#xD;&#xA;Please correct me if I'm wrong: My guess it that the read is counted as 1 in cases 1. and 2., and 0 otherwise, and I also suppose that a read can be counted for several successive bins if it is long enough.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### Second step: averaging over genes and plotting&#xD;&#xA;&#xD;&#xA;I compute a &quot;meta profile matrix&quot; (&quot;mapped_on_genes.gz&quot;) using `computeMatrix scale-regions` as follows:&#xD;&#xA;&#xD;&#xA;    computeMatrix scale-regions \&#xD;&#xA;        -S mapped.bw \&#xD;&#xA;        -R genes.bed \&#xD;&#xA;        --upstream 300 \&#xD;&#xA;        --unscaled5prime 500 \&#xD;&#xA;        --regionBodyLength 2000 \&#xD;&#xA;        --unscaled3prime 500 \&#xD;&#xA;        --downstream 300 \&#xD;&#xA;        -out mapped_on_genes.gz&#xD;&#xA;&#xD;&#xA;(There is a `-bs` parameter which default value is 10 according to the help of the command.)&#xD;&#xA;&#xD;&#xA;I use this to plot a profile using `plotProfile`:&#xD;&#xA;&#xD;&#xA;    plotProfile -m mapped_on_genes.gz \&#xD;&#xA;        -out mapped_on_genes_meta_profile.pdf&#xD;&#xA;&#xD;&#xA;I obtain a profile in with values on the y axis. In what units are these values?&#xD;&#xA;&#xD;&#xA;My guess is the following:&#xD;&#xA;&#xD;&#xA;For the upstream (300 bp) and internal 5-prime (500 bp), since the bin size was the same in `bamCoverage` and `computeMatrix`, each point on the x axis probably represents a 10 bp window, and its y coordinate is the average over the regions present in the bed file of the corresponding bins in the bigwig file, so it is an average number of reads overlapping a 10 bp bin.&#xD;&#xA;&#xD;&#xA;Same thing at the 3-prime and downstream side.&#xD;&#xA;&#xD;&#xA;For the central 100 bp portion, before averaging over regions some shrinking or spreading of the bins must have been performed, I guess by averaging between neighbouring bins. So the final unit is still a **number of reads overlapping a 10 bp bin**.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;And if I use larger bins, I should end up with proportionally higher values.&#xD;&#xA;&#xD;&#xA;**Am I correct?**" />
  <row Id="1909" PostHistoryTypeId="2" PostId="610" RevisionGUID="dcf5260b-e843-4c77-95e0-78e1c907030a" CreationDate="2017-06-09T15:37:49.977" UserId="506" Text="I'm thinking that this isn't actually possible, but I'd like to check before I write it off.  I have RNA-seq data from cells under three different conditions, there are no replicates for any of the conditions.  The data has been processed with RSEM, and log2 fold changes have been calculated for each control-test pairing using the normalized expected read counts using EBseq.&#xD;&#xA;&#xD;&#xA;If possible, I'd like to also calculate the p-value for each of these fold-changes, however, because there are no replicates I don't think that this is possible.  Is there any way to do this?" />
  <row Id="1910" PostHistoryTypeId="1" PostId="610" RevisionGUID="dcf5260b-e843-4c77-95e0-78e1c907030a" CreationDate="2017-06-09T15:37:49.977" UserId="506" Text="Is it possible to calculate p-values for fold changes of single replicate RNA-seq samples?" />
  <row Id="1911" PostHistoryTypeId="3" PostId="610" RevisionGUID="dcf5260b-e843-4c77-95e0-78e1c907030a" CreationDate="2017-06-09T15:37:49.977" UserId="506" Text="&lt;ebseq&gt;&lt;rsem&gt;" />
  <row Id="1912" PostHistoryTypeId="5" PostId="609" RevisionGUID="632cef1e-f270-4340-9de5-ca7f7609a727" CreationDate="2017-06-09T15:42:26.287" UserId="796" Comment="added 156 characters in body" Text="I have two different sets of internal standards in a sample (1), and a sample (2) with these internal standards as well as about 30 peptides. Each peptide have one of each of these kind of standards. From sample 1, I can get the true ratio between standard 1 (S1) and standard 2 (S2), and from sample 2 I can get the ratio between the analyte (L) to each of the standards:&#xD;&#xA;&#xD;&#xA;Sample 1: S1/S2&#xD;&#xA;&#xD;&#xA;Sample 2: L/S1, L/S2&#xD;&#xA;&#xD;&#xA;I can thus calculate the ratio S1/S2 from the measured ratios in sample 2. Each sample is run in 5 technical replicates (that is; 5 injections into the LC-mass spectrometer), so S1/S2 ratios for one particular analyte could look like this (mock data):&#xD;&#xA;&#xD;&#xA;    measured	calculated&#xD;&#xA;    0.967		0.987&#xD;&#xA;    1.007		0.967&#xD;&#xA;    1.044		1.012&#xD;&#xA;    1.041		1.025&#xD;&#xA;    1.048		1.046&#xD;&#xA;&#xD;&#xA;I want to compare the measured ratios to the calculated, to see if there are any bias in my acquisition of data. As the calculated and the measured ratio are from different samples, the replicates aren't &quot;linked&quot; so I could not use for example Bland Altman analysis. Should I use some kind of ANOVA? Or do you have any other recommendation?&#xD;&#xA;&#xD;&#xA;I would also like to make an overall assessment of the ratios of all 30 peptides to see if the bias is different for different peptides. How could I do that?&#xD;&#xA;&#xD;&#xA;Any input would be greatly appreciated.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;" />
  <row Id="1913" PostHistoryTypeId="4" PostId="609" RevisionGUID="632cef1e-f270-4340-9de5-ca7f7609a727" CreationDate="2017-06-09T15:42:26.287" UserId="796" Comment="added 156 characters in body" Text="How do I compare measured ratios to calculated ratios in peptide mass spectrometry?" />
  <row Id="1914" PostHistoryTypeId="5" PostId="609" RevisionGUID="f4455052-d1cb-4d9d-9014-97ea46bfca13" CreationDate="2017-06-09T15:49:03.807" UserId="796" Comment="deleted 1 character in body" Text="I have two different sets of internal standards in a sample (1), and a sample (2) with these internal standards as well as about 30 peptides. Each peptide has one of each of these kind of standards. From sample 1, I can get the true ratio between standard 1 (S1) and standard 2 (S2), and from sample 2 I can get the ratio between the analyte (L) to each of the standards:&#xD;&#xA;&#xD;&#xA;Sample 1: S1/S2&#xD;&#xA;&#xD;&#xA;Sample 2: L/S1, L/S2&#xD;&#xA;&#xD;&#xA;I can thus calculate the ratio S1/S2 from the measured ratios in sample 2. Each sample is run in 5 technical replicates (that is; 5 injections into the LC-mass spectrometer), so S1/S2 ratios for one particular analyte could look like this (mock data):&#xD;&#xA;&#xD;&#xA;    measured	calculated&#xD;&#xA;    0.967		0.987&#xD;&#xA;    1.007		0.967&#xD;&#xA;    1.044		1.012&#xD;&#xA;    1.041		1.025&#xD;&#xA;    1.048		1.046&#xD;&#xA;&#xD;&#xA;I want to compare the measured ratios to the calculated, to see if there are any bias in my acquisition of data. As the calculated and the measured ratio are from different samples, the replicates aren't &quot;linked&quot; so I could not use for example Bland Altman analysis. Should I use some kind of ANOVA? Or do you have any other recommendation?&#xD;&#xA;&#xD;&#xA;I would also like to make an overall assessment of the ratios of all 30 peptides to see if the bias is different for different peptides. How could I do that?&#xD;&#xA;&#xD;&#xA;Any input would be greatly appreciated.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;" />
  <row Id="1915" PostHistoryTypeId="2" PostId="611" RevisionGUID="344a9741-e83f-4b5b-93f6-cc4126fd6a2d" CreationDate="2017-06-09T15:51:24.883" UserId="272" Text="The [SAM specification][1] indicates that each read group must have a unique ID field, but does not mark any other field as required. &#xD;&#xA;&#xD;&#xA;I have also discovered that htsjdk throws exceptions if the sample (SM) field is empty, though there is no indication in the specification that this is required. &#xD;&#xA;&#xD;&#xA;Are there other read group fields that I should expect to be required by common tools? &#xD;&#xA;&#xD;&#xA;  [1]: https://samtools.github.io/hts-specs/SAMv1.pdf" />
  <row Id="1916" PostHistoryTypeId="1" PostId="611" RevisionGUID="344a9741-e83f-4b5b-93f6-cc4126fd6a2d" CreationDate="2017-06-09T15:51:24.883" UserId="272" Text="What are the de facto required fields in a SAM/BAM read group?" />
  <row Id="1917" PostHistoryTypeId="3" PostId="611" RevisionGUID="344a9741-e83f-4b5b-93f6-cc4126fd6a2d" CreationDate="2017-06-09T15:51:24.883" UserId="272" Text="&lt;bam&gt;&lt;sam&gt;" />
  <row Id="1918" PostHistoryTypeId="2" PostId="612" RevisionGUID="a377f16a-5619-4e66-87d4-8f75be25c39a" CreationDate="2017-06-09T15:53:02.983" UserId="73" Text="DESeq2 is able to adjust p-values for single-replicate samples by estimating shared dispersion across all samples. DESeq2 will give a warning, but try its best to carry out the analysis. More information on doing that can be found in the [DESeq2 vignette](https://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#can-i-use-deseq2-to-analyze-a-dataset-without-replicates) and the DESeq2 manual page.&#xD;&#xA;&#xD;&#xA;The results will not be anywhere near the sensitivity that can be achieved when replicate information is available, but it might be good enough if only very obvious large-scale changes are of importance." />
  <row Id="1919" PostHistoryTypeId="2" PostId="613" RevisionGUID="b7363e97-590d-4322-b721-7b0593450cb0" CreationDate="2017-06-09T16:01:18.067" UserId="73" Text="The read group identifier needs to be specified in both the header lines of the BAM/SAM file and the alignment line. No other fields are required, but note that because the additional information is only stored once (i.e. in the header), it won't add much to file sizes or processing time if additional fields are included. If any of the other optional fields cause problems (either by inclusion or exclusion), it would be helpful to [report an issue](https://github.com/samtools/htsjdk/issues) about it.&#xD;&#xA;&#xD;&#xA;Older tools required you to specify both a header read group ID and an alignment read group ID, but most tools of the recent tools I've used seem to be intelligent enough that they will add headers for any read groups without that information." />
  <row Id="1920" PostHistoryTypeId="2" PostId="614" RevisionGUID="a4dd1d8f-ad89-4334-952b-51228e4c4ad9" CreationDate="2017-06-09T16:08:03.673" UserId="-1" Text="" />
  <row Id="1921" PostHistoryTypeId="1" PostId="614" RevisionGUID="a4dd1d8f-ad89-4334-952b-51228e4c4ad9" CreationDate="2017-06-09T16:08:03.673" UserId="-1" />
  <row Id="1922" PostHistoryTypeId="2" PostId="615" RevisionGUID="1ee97c82-957c-4414-9b14-df2cdf70d0f3" CreationDate="2017-06-09T16:08:03.673" UserId="-1" Text="" />
  <row Id="1923" PostHistoryTypeId="1" PostId="615" RevisionGUID="1ee97c82-957c-4414-9b14-df2cdf70d0f3" CreationDate="2017-06-09T16:08:03.673" UserId="-1" />
  <row Id="1924" PostHistoryTypeId="6" PostId="604" RevisionGUID="994fb68a-46fe-4709-bcb4-6debce3bdc7c" CreationDate="2017-06-09T16:09:16.243" UserId="96" Comment="added tag for GTF" Text="&lt;human-genome&gt;&lt;reference-genome&gt;&lt;gtf&gt;" />
  <row Id="1925" PostHistoryTypeId="24" PostId="604" RevisionGUID="994fb68a-46fe-4709-bcb4-6debce3bdc7c" CreationDate="2017-06-09T16:09:16.243" Comment="Proposed by 96 approved by -1 edit id of 172" />
  <row Id="1926" PostHistoryTypeId="6" PostId="604" RevisionGUID="ce5987c5-d2ee-46b0-8b4d-04860a9a1d09" CreationDate="2017-06-09T16:09:16.243" UserId="37" Comment="added tag for GTF and 10x-genomics" Text="&lt;human-genome&gt;&lt;reference-genome&gt;&lt;gtf&gt;&lt;10x-genomics&gt;" />
  <row Id="1927" PostHistoryTypeId="2" PostId="616" RevisionGUID="8cf5ce43-419d-4d0f-8328-e138c64eb7a6" CreationDate="2017-06-09T16:16:17.263" UserId="-1" Text="" />
  <row Id="1928" PostHistoryTypeId="1" PostId="616" RevisionGUID="8cf5ce43-419d-4d0f-8328-e138c64eb7a6" CreationDate="2017-06-09T16:16:17.263" UserId="-1" />
  <row Id="1929" PostHistoryTypeId="2" PostId="617" RevisionGUID="cff01032-99c7-42ed-b7ba-3f90b180f400" CreationDate="2017-06-09T16:16:17.263" UserId="-1" Text="" />
  <row Id="1930" PostHistoryTypeId="1" PostId="617" RevisionGUID="cff01032-99c7-42ed-b7ba-3f90b180f400" CreationDate="2017-06-09T16:16:17.263" UserId="-1" />
  <row Id="1931" PostHistoryTypeId="2" PostId="618" RevisionGUID="1f608f4f-93db-404e-9a9a-23aaba4a0c39" CreationDate="2017-06-09T16:28:09.460" UserId="37" Text="The sample tag (i.e. SM) was a mandatory tag in the [initial SAM spec](https://sourceforge.net/p/samtools/code/HEAD/tree/trunk/sam-spec/) (see the `.pages` file; you need a mac to open it). When transitioned to Latex, this requirement was mysteriously dropped. Picard is conforming to the initial spec. Anyway, the sample tag is important to quite a few tools. I would encourage you to add it." />
  <row Id="1932" PostHistoryTypeId="2" PostId="619" RevisionGUID="a0f93bc5-9013-43ab-bcd4-82e7498d5019" CreationDate="2017-06-09T16:28:26.360" UserId="74" Text="The main concern has always been the &quot;binning&quot; of quality scores that occurs via CRAM compression (and is also standard on the HiSeqX, HiSeq4000, and NovaSeq platforms). Anecdotally, I can report very little difference between 4-bin quality scores and full quality scores on cancer samples, though I don't know if I've seen a direct head-to-head comparison. " />
  <row Id="1933" PostHistoryTypeId="2" PostId="620" RevisionGUID="6d95d5cf-e3fc-451f-a823-67321e8fd9b7" CreationDate="2017-06-09T16:40:08.390" UserId="800" Text="Is there a computational tool for measuring what fraction of an RNA is spliced in an RNAseq experiment?&#xD;&#xA;&#xD;&#xA;I'm not particularly interested in complicated analyzes that given ratios for all possible alternative splicing variations. I'd rather have a binary classification. For example: (RNA1 = 35% spliced, 65% unspliced)" />
  <row Id="1934" PostHistoryTypeId="1" PostId="620" RevisionGUID="6d95d5cf-e3fc-451f-a823-67321e8fd9b7" CreationDate="2017-06-09T16:40:08.390" UserId="800" Text="Spliced vs. unspliced ratios for transcripts in RNA-seq data" />
  <row Id="1935" PostHistoryTypeId="3" PostId="620" RevisionGUID="6d95d5cf-e3fc-451f-a823-67321e8fd9b7" CreationDate="2017-06-09T16:40:08.390" UserId="800" Text="&lt;rna-seq&gt;" />
  <row Id="1936" PostHistoryTypeId="5" PostId="620" RevisionGUID="68dadd27-7eea-4f49-a530-daf2838cc648" CreationDate="2017-06-09T16:43:06.830" UserId="298" Comment="Corrected spelling" Text="Is there a computational tool for measuring what percentage of RNA is spliced in an RNAseq experiment?&#xD;&#xA;&#xD;&#xA;I'm not particularly interested in complicated analyses that give ratios for all possible alternative splicing variations. I'd rather have a binary classification. For example: (RNA1 = 35% spliced, 65% unspliced)" />
  <row Id="1937" PostHistoryTypeId="24" PostId="620" RevisionGUID="68dadd27-7eea-4f49-a530-daf2838cc648" CreationDate="2017-06-09T16:43:06.830" Comment="Proposed by 298 approved by 800 edit id of 179" />
  <row Id="1938" PostHistoryTypeId="2" PostId="621" RevisionGUID="a91eebc4-0f1e-40b3-baf1-affae47726e3" CreationDate="2017-06-09T16:48:33.833" UserId="96" Text="Take this first comment with a grain of salt, since this isn't an area I've worked in much, but: is binary classification possible? If a gene has 3 introns, and 2 are spliced out but 1 is retained, is this &quot;spliced&quot; or &quot;unspliced&quot;. My first impression is that an analysis would be a bit more nuanced than a binary classification.&#xD;&#xA;&#xD;&#xA;That said, I'm aware of a tool called Keep Me Around for intron retention analysis ([preprint](https://arxiv.org/abs/1510.00696), [code](https://github.com/pachterlab/kma)). I've never used this software, but it looks like it's actively maintained and was created by a research group with a lot of experience and influence in this area, for whatever that's worth. :-)&#xD;&#xA;&#xD;&#xA;**UPDATE**: It looks like the original post was modified since I submitted my answer, but I'll leave the text about binary classification around for posterity. :-)" />
  <row Id="1939" PostHistoryTypeId="6" PostId="620" RevisionGUID="23d4634f-ef51-4247-bf4e-de08324f356e" CreationDate="2017-06-09T16:51:24.270" UserId="96" Comment="added tag for splicing" Text="&lt;rna-seq&gt;&lt;rna-splicing&gt;" />
  <row Id="1940" PostHistoryTypeId="24" PostId="620" RevisionGUID="23d4634f-ef51-4247-bf4e-de08324f356e" CreationDate="2017-06-09T16:51:24.270" Comment="Proposed by 96 approved by 800 edit id of 180" />
  <row Id="1941" PostHistoryTypeId="4" PostId="611" RevisionGUID="1a600b57-82b9-47bf-acc9-ec2af911ea79" CreationDate="2017-06-09T16:52:01.640" UserId="292" Comment="bam tag is not relevant here" Text="fileWhat are the de facto required fields in a SAM/BAM read group?" />
  <row Id="1942" PostHistoryTypeId="6" PostId="611" RevisionGUID="1a600b57-82b9-47bf-acc9-ec2af911ea79" CreationDate="2017-06-09T16:52:01.640" UserId="292" Comment="bam tag is not relevant here" Text="&lt;file-formats&gt;&lt;sam&gt;" />
  <row Id="1943" PostHistoryTypeId="24" PostId="611" RevisionGUID="1a600b57-82b9-47bf-acc9-ec2af911ea79" CreationDate="2017-06-09T16:52:01.640" Comment="Proposed by 292 approved by -1 edit id of 181" />
  <row Id="1944" PostHistoryTypeId="4" PostId="611" RevisionGUID="5fcd7889-54bf-4382-a2b9-f701816607fa" CreationDate="2017-06-09T16:52:01.640" UserId="73" Comment="bam tag is not relevant here" Text="What are the de facto required fields in a SAM/BAM read group?" />
  <row Id="1946" PostHistoryTypeId="2" PostId="622" RevisionGUID="1733985f-4974-4a56-95f1-a0f5a5a2af15" CreationDate="2017-06-09T17:04:19.710" UserId="77" Text="Feel free to @ me in deepTools questions, since I'm the primary developer.&#xD;&#xA;&#xD;&#xA;For a given bin, the count assigned to it is the number of reads overlapping it, regardless of whether they overlap by 1 or 10 bases. So a read overlapping only partially and one overlapping completely are treated the same.&#xD;&#xA;&#xD;&#xA;Since your bigWig file is in units of &quot;alignments&quot; (i.e., it's not 1x normalized), the resulting profile will also be in units of &quot;alignments&quot; (i.e., profiles and heatmaps are in whatever units the input files are in).&#xD;&#xA;&#xD;&#xA;Upstream/downstream regions and unscaled regions are also 10 base bins. Note that these are then the average of the per-base value, since the bins here may not perfectly correspond to the bins in the bigWig files. The line in the profile plot is indeed the average (by default, you can choose median, max, min, etc.) of the underlying regions for each bin.&#xD;&#xA;&#xD;&#xA;Regarding the scaled section in the middle, the the number of genomic bases per bin is changed such that the region will have `&quot;length&quot;/(regionBodyLength/binSize)` bases each. As above, the per-base value is then averaged (or whatever you specify) to derive the per-bin value. The `length` here is decreased if you have unscaled regions, since otherwise bases would get counted twice." />
  <row Id="1947" PostHistoryTypeId="5" PostId="614" RevisionGUID="59b0e3b3-a334-43d0-a2cc-429a62291cad" CreationDate="2017-06-09T17:20:16.950" UserId="96" Comment="spec url" Text="The GFF3 formal specification is available at https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md." />
  <row Id="1948" PostHistoryTypeId="24" PostId="614" RevisionGUID="59b0e3b3-a334-43d0-a2cc-429a62291cad" CreationDate="2017-06-09T17:20:16.950" Comment="Proposed by 96 approved by 73, 77 edit id of 174" />
  <row Id="1949" PostHistoryTypeId="5" PostId="184" RevisionGUID="4c968e7c-7f0c-4d8d-9b73-cf41684b88b2" CreationDate="2017-06-09T17:20:24.750" UserId="96" Comment="added 94 characters in body" Text="BED has also been adapted for a variety of other more generic genome annotation use cases.&#xD;&#xA;&#xD;&#xA;- [UCSC description](https://genome.ucsc.edu/FAQ/FAQformat.html)&#xD;&#xA;&#xD;&#xA;- [Ensembl description](http://www.ensembl.org/info/website/upload/bed.html)" />
  <row Id="1950" PostHistoryTypeId="24" PostId="184" RevisionGUID="4c968e7c-7f0c-4d8d-9b73-cf41684b88b2" CreationDate="2017-06-09T17:20:24.750" Comment="Proposed by 96 approved by 73, 77 edit id of 175" />
  <row Id="1951" PostHistoryTypeId="5" PostId="616" RevisionGUID="ba61bc4d-32e0-408e-b308-cc66d371e879" CreationDate="2017-06-09T17:20:38.977" UserId="96" Comment="added 120 characters in body" Text="- [Ensembl spec](http://www.ensembl.org/info/website/upload/gff.html)&#xD;&#xA;- [WUSTL spec](http://mblab.wustl.edu/GTF22.html)" />
  <row Id="1952" PostHistoryTypeId="24" PostId="616" RevisionGUID="ba61bc4d-32e0-408e-b308-cc66d371e879" CreationDate="2017-06-09T17:20:38.977" Comment="Proposed by 96 approved by 73, 77 edit id of 177" />
  <row Id="1953" PostHistoryTypeId="2" PostId="623" RevisionGUID="03d095db-5597-4b29-b8e2-64603a3f5aa2" CreationDate="2017-06-09T18:17:58.273" UserId="803" Text="I am using **PythonCyc** API in order to write a query for metabolites in **BioCyc**. The purpose of this API is to communicate with the database software of BioCyc- Pathway Tools. Pathway Tools is in lisp therefore, PythonCyc creates a bridge between python and common lisp. To use this this API one must first create a PGDB object with a specified organism ID (orgid). In the example below I create a PGDB with orgid &quot;meta&quot;. After this, I am able to call methods from PythonCyc with this object:&#xD;&#xA;&#xD;&#xA;    import sys&#xD;&#xA;    import pythoncyc&#xD;&#xA;    #this creates PGDB object associated with meta(MetaCyc)&#xD;&#xA;    meta = pythoncyc.select_organism('meta')&#xD;&#xA;    #lists pathways of specified compound&#xD;&#xA;    pathways = meta.pathways_of_compound('sucrose')&#xD;&#xA;    print pathways&#xD;&#xA;&#xD;&#xA;The query for the metabolite above, 'sucrose', provides a list a pathways:&#xD;&#xA;&#xD;&#xA;    [u'|PWY-7347|', u'|SUCSYN-PWY|', u'|PWY-7238|', u'|PWY-6527|', u'|PWY-5343|', u'|PWY-3801|', u'|PWY-7345|', u'|PWY-862|', u'|SUCUTIL-PWY|', u'|PWY66-373|', u'|PWY-621|', u'|PWY-822|', u'|SUCROSEUTIL2-PWY|', u'|PWY-6525|', u'|PWY-6524|', u'|PWY-5337|', u'|PWY-5384|']&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;However if I switch this metabolite name to a common amino acid name, such as 'valine':&#xD;&#xA;&#xD;&#xA;    import sys&#xD;&#xA;    import pythoncyc&#xD;&#xA;    #this creates PGDB object associated with meta(MetaCyc)&#xD;&#xA;    meta = pythoncyc.select_organism('meta')&#xD;&#xA;    #lists pathways of specified compound&#xD;&#xA;    pathways = meta.pathways_of_compound('valine')&#xD;&#xA;    print pathways&#xD;&#xA;&#xD;&#xA;The query provides an error message that states &quot;non coercible frame&quot;, which means it cannot find this ID. This is a common metabolite, just as sucrose, which should very well have pathway entries in database, however there are none found with this method. I have also tried synonyms of valine and other methods such as &quot;all_pathways(cpds)&quot; stated in API, which give me the same error message.&#xD;&#xA;What is the reason for this? Does BioCyc not list any pathways for amino acids? Or am I using an incorrect method to access amino acid information?&#xD;&#xA;&#xD;&#xA;[Link to PythonCyc API][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://pythoncyc.readthedocs.io/en/latest/" />
  <row Id="1954" PostHistoryTypeId="1" PostId="623" RevisionGUID="03d095db-5597-4b29-b8e2-64603a3f5aa2" CreationDate="2017-06-09T18:17:58.273" UserId="803" Text="What methods should I use from PythonCyc API to query metabolites in BioCyc database?" />
  <row Id="1955" PostHistoryTypeId="3" PostId="623" RevisionGUID="03d095db-5597-4b29-b8e2-64603a3f5aa2" CreationDate="2017-06-09T18:17:58.273" UserId="803" Text="&lt;biopython&gt;&lt;api&gt;" />
  <row Id="1956" PostHistoryTypeId="5" PostId="620" RevisionGUID="47f1233f-1acd-4fed-9ce8-b73c8400eb62" CreationDate="2017-06-09T18:33:13.550" UserId="800" Comment="added 646 characters in body" Text="Is there a computational tool for measuring what percentage of RNA is spliced in an RNAseq experiment?&#xD;&#xA;&#xD;&#xA;I'm not particularly interested in complicated analyses that give ratios for all possible alternative splicing variations. I'd rather have a binary classification. For example: (RNA1 = 35% spliced, 65% unspliced)&#xD;&#xA;&#xD;&#xA;**Edit: (Additional information)**&#xD;&#xA;&#xD;&#xA;We're very early in the exploratory stage, so a specific hypothesis is still forming. But we're interested in the sequence content of lncRNAs, many of which are poorly spliced. So we want to get a sense for how prevalent unspliced version of these transcript are. &#xD;&#xA;&#xD;&#xA;Because this is currently exploratory, estimations and noise are fairly acceptable. A very basic approach might be to calculate the ratio of reads that map to exon-to-exon junctions, verses those that map from exon-to-intro junctions. &#xD;&#xA;&#xD;&#xA;A more advanced approach I'm also considering is [Salmon](https://combine-lab.github.io/salmon/)." />
  <row Id="1957" PostHistoryTypeId="2" PostId="624" RevisionGUID="f05f2c80-4352-4112-969a-bf20f198f8c6" CreationDate="2017-06-09T20:27:11.420" UserId="776" Text="I wrote a command-line (C++14) tool called `subset` which is up on Github: https://github.com/alexpreynolds/subset&#xD;&#xA;&#xD;&#xA;This should be reasonably memory efficient and fast. The `subset` tool does not store input lines in a table, but instead streams through the file once, storing a 4 or 8k buffer chunk of the input file (depending on OS). &#xD;&#xA;&#xD;&#xA;It stores line numbers in an array, but eight bytes per integer * 100k is 800k, for that use case — not very much memory there.&#xD;&#xA;&#xD;&#xA;There's an O(nlogn) sort penalty on the line number array, but again this list will be much smaller than the query file, and integer sorting is fairly optimized, so the hit should be small. &#xD;&#xA;&#xD;&#xA;If your line number list is already sorted, I could add an option to skip sorting; let me know if that would be useful.&#xD;&#xA;&#xD;&#xA;The filtering step walks through the line number array and input file linearly, printing lines where there are index matches, and skipping over the rest.&#xD;&#xA;&#xD;&#xA;You can grab, build and install it like so:&#xD;&#xA;&#xD;&#xA;    $ git clone https://github.com/alexpreynolds/subset.git&#xD;&#xA;    $ cd subset&#xD;&#xA;    $ make&#xD;&#xA;    $ cp subset /usr/local/bin&#xD;&#xA;&#xD;&#xA;Once the binary is in your path, there are a couple ways to use it.&#xD;&#xA;&#xD;&#xA;For example, you can specify a start index and length value. The following grabs seven lines starting with the 33rd line (32 as a 0-indexed value):&#xD;&#xA;&#xD;&#xA;    $ subset --prefix-with-indices -s 32 -n 7 -i query.txt &gt; answer.txt&#xD;&#xA;&#xD;&#xA;Or you can specify a text file containing line numbers, each on a separate line. The following reads in a file called `line-numbers.txt` and uses that to filter `query.txt`:&#xD;&#xA;&#xD;&#xA;    $ subset --prefix-with-indices -l line-numbers.txt -i query.txt &gt; answer.txt&#xD;&#xA;&#xD;&#xA;The indices in `line-numbers.txt` should be positive, 0-indexed integers. The list of numbers does not need to be sorted, as `subset` will sort the list of numbers for you. This is so that an efficient single pass through the input/query file can be done.&#xD;&#xA;&#xD;&#xA;You can leave out `--prefix-with-indices` to leave out the debug prefix. This is there so that you can do a sanity check on the result.&#xD;&#xA;&#xD;&#xA;The `test/makefile` tests demonstrate options and usages for the two types of filtering." />
  <row Id="1958" PostHistoryTypeId="5" PostId="624" RevisionGUID="05413efd-28c2-495e-b851-2ec3ae8901d7" CreationDate="2017-06-09T20:36:02.113" UserId="776" Comment="added 197 characters in body" Text="I wrote a command-line (C++14) tool called `subset` which is up on Github: https://github.com/alexpreynolds/subset&#xD;&#xA;&#xD;&#xA;This should be reasonably memory efficient and fast. The `subset` tool does not store input lines in a table, but instead streams through the file once, storing a 4 or 8k buffer chunk of the input file (depending on OS). &#xD;&#xA;&#xD;&#xA;It stores line numbers in an array, but eight bytes per integer * 100k is 800k, for that use case — not very much memory there.&#xD;&#xA;&#xD;&#xA;There's an O(nlogn) sort penalty on the line number array, but again this list will be much smaller than the query file, and integer sorting is fairly optimized, so the hit should be small. &#xD;&#xA;&#xD;&#xA;If your line number list is already sorted, I could add an option to skip sorting; let me know if that would be useful.&#xD;&#xA;&#xD;&#xA;The filtering step walks through the line number array and input file linearly, printing lines where there are index matches, and skipping over the rest. &#xD;&#xA;&#xD;&#xA;Indeed, `subset` will quit early in parsing the input file if there are no more line numbers to query. So this feature is especially useful for speeding up filtering of very large query files.&#xD;&#xA;&#xD;&#xA;You can grab, build and install it like so:&#xD;&#xA;&#xD;&#xA;    $ git clone https://github.com/alexpreynolds/subset.git&#xD;&#xA;    $ cd subset&#xD;&#xA;    $ make&#xD;&#xA;    $ cp subset /usr/local/bin&#xD;&#xA;&#xD;&#xA;Once the binary is in your path, there are a couple ways to use it.&#xD;&#xA;&#xD;&#xA;For example, you can specify a start index and length value. The following grabs seven lines starting with the 33rd line (32 as a 0-indexed value):&#xD;&#xA;&#xD;&#xA;    $ subset --prefix-with-indices -s 32 -n 7 -i query.txt &gt; answer.txt&#xD;&#xA;&#xD;&#xA;Or you can specify a text file containing line numbers, each on a separate line. The following reads in a file called `line-numbers.txt` and uses that to filter `query.txt`:&#xD;&#xA;&#xD;&#xA;    $ subset --prefix-with-indices -l line-numbers.txt -i query.txt &gt; answer.txt&#xD;&#xA;&#xD;&#xA;The indices in `line-numbers.txt` should be positive, 0-indexed integers. The list of numbers does not need to be sorted, as `subset` will sort the list of numbers for you. This is so that an efficient single pass through the input/query file can be done.&#xD;&#xA;&#xD;&#xA;You can leave out `--prefix-with-indices` to leave out the debug prefix. This is there so that you can do a sanity check on the result.&#xD;&#xA;&#xD;&#xA;The `test/makefile` tests demonstrate options and usages for the two types of filtering." />
  <row Id="1959" PostHistoryTypeId="5" PostId="624" RevisionGUID="c42d98f5-c5b4-4a64-a0c7-8cff6f873a49" CreationDate="2017-06-09T21:38:04.380" UserId="776" Comment="added 1 character in body" Text="I wrote a command-line (C++14) tool called `subset` which is up on Github: https://github.com/alexpreynolds/subset&#xD;&#xA;&#xD;&#xA;This should be reasonably memory efficient and fast. The `subset` tool does not store input lines in a table, but instead streams through the file once, storing a 4 or 8k buffer chunk of the input file (depending on OS). &#xD;&#xA;&#xD;&#xA;It stores line numbers in an array, but eight bytes per integer * 100k is 800kB, for that use case — not very much memory there.&#xD;&#xA;&#xD;&#xA;There's an O(nlogn) sort penalty on the line number array, but again this list will be much smaller than the query file, and integer sorting is fairly optimized, so the hit should be small. &#xD;&#xA;&#xD;&#xA;If your line number list is already sorted, I could add an option to skip sorting; let me know if that would be useful.&#xD;&#xA;&#xD;&#xA;The filtering step walks through the line number array and input file linearly, printing lines where there are index matches, and skipping over the rest. &#xD;&#xA;&#xD;&#xA;Indeed, `subset` will quit early in parsing the input file if there are no more line numbers to query. So this feature is especially useful for speeding up filtering of very large query files.&#xD;&#xA;&#xD;&#xA;You can grab, build and install it like so:&#xD;&#xA;&#xD;&#xA;    $ git clone https://github.com/alexpreynolds/subset.git&#xD;&#xA;    $ cd subset&#xD;&#xA;    $ make&#xD;&#xA;    $ cp subset /usr/local/bin&#xD;&#xA;&#xD;&#xA;Once the binary is in your path, there are a couple ways to use it.&#xD;&#xA;&#xD;&#xA;For example, you can specify a start index and length value. The following grabs seven lines starting with the 33rd line (32 as a 0-indexed value):&#xD;&#xA;&#xD;&#xA;    $ subset --prefix-with-indices -s 32 -n 7 -i query.txt &gt; answer.txt&#xD;&#xA;&#xD;&#xA;Or you can specify a text file containing line numbers, each on a separate line. The following reads in a file called `line-numbers.txt` and uses that to filter `query.txt`:&#xD;&#xA;&#xD;&#xA;    $ subset --prefix-with-indices -l line-numbers.txt -i query.txt &gt; answer.txt&#xD;&#xA;&#xD;&#xA;The indices in `line-numbers.txt` should be positive, 0-indexed integers. The list of numbers does not need to be sorted, as `subset` will sort the list of numbers for you. This is so that an efficient single pass through the input/query file can be done.&#xD;&#xA;&#xD;&#xA;You can leave out `--prefix-with-indices` to leave out the debug prefix. This is there so that you can do a sanity check on the result.&#xD;&#xA;&#xD;&#xA;The `test/makefile` tests demonstrate options and usages for the two types of filtering." />
  <row Id="1960" PostHistoryTypeId="5" PostId="609" RevisionGUID="14103cee-58bd-4a2c-9c4e-faec695b41c3" CreationDate="2017-06-09T22:15:09.710" UserId="57" Comment="removed thanks, added mass-spectrometry tag" Text="I have two different sets of internal standards in a sample (1), and a sample (2) with these internal standards as well as about 30 peptides. Each peptide has one of each of these kind of standards. From sample 1, I can get the true ratio between standard 1 (S1) and standard 2 (S2), and from sample 2 I can get the ratio between the analyte (L) to each of the standards:&#xD;&#xA;&#xD;&#xA;Sample 1: S1/S2&#xD;&#xA;&#xD;&#xA;Sample 2: L/S1, L/S2&#xD;&#xA;&#xD;&#xA;I can thus calculate the ratio S1/S2 from the measured ratios in sample 2. Each sample is run in 5 technical replicates (that is; 5 injections into the LC-mass spectrometer), so S1/S2 ratios for one particular analyte could look like this (mock data):&#xD;&#xA;&#xD;&#xA;    measured	calculated&#xD;&#xA;    0.967		0.987&#xD;&#xA;    1.007		0.967&#xD;&#xA;    1.044		1.012&#xD;&#xA;    1.041		1.025&#xD;&#xA;    1.048		1.046&#xD;&#xA;&#xD;&#xA;I want to compare the measured ratios to the calculated, to see if there are any bias in my acquisition of data. As the calculated and the measured ratio are from different samples, the replicates aren't &quot;linked&quot; so I could not use for example Bland Altman analysis. Should I use some kind of ANOVA? Or do you have any other recommendation?&#xD;&#xA;&#xD;&#xA;I would also like to make an overall assessment of the ratios of all 30 peptides to see if the bias is different for different peptides. How could I do that?&#xD;&#xA;&#xD;&#xA;Any input would be greatly appreciated.&#xD;&#xA;" />
  <row Id="1961" PostHistoryTypeId="6" PostId="609" RevisionGUID="14103cee-58bd-4a2c-9c4e-faec695b41c3" CreationDate="2017-06-09T22:15:09.710" UserId="57" Comment="removed thanks, added mass-spectrometry tag" Text="&lt;proteins&gt;&lt;mass-spectrometry&gt;" />
  <row Id="1962" PostHistoryTypeId="24" PostId="609" RevisionGUID="14103cee-58bd-4a2c-9c4e-faec695b41c3" CreationDate="2017-06-09T22:15:09.710" Comment="Proposed by 57 approved by 73, 77 edit id of 182" />
  <row Id="1963" PostHistoryTypeId="5" PostId="624" RevisionGUID="be035bd3-84a8-4474-8655-39fd37b2deb3" CreationDate="2017-06-10T01:29:46.010" UserId="776" Comment="added 143 characters in body" Text="I wrote a command-line (C++14) tool called `subset` which is up on Github: https://github.com/alexpreynolds/subset&#xD;&#xA;&#xD;&#xA;This should be reasonably memory efficient and fast. The `subset` tool does not store input lines in a table, but instead streams through the file once, storing a 4 or 8k buffer chunk of the input file (depending on OS). &#xD;&#xA;&#xD;&#xA;It stores line numbers in an array, but eight bytes per integer * 100k is 800kB, for that use case — not very much memory there.&#xD;&#xA;&#xD;&#xA;There's an O(nlogn) sort penalty on the line number array, but again this list will be much smaller than the query file, and integer sorting is fairly optimized, so the hit should be small. &#xD;&#xA;&#xD;&#xA;If your line number list is already sorted, I could add an option to skip sorting; let me know if that would be useful.&#xD;&#xA;&#xD;&#xA;The filtering step walks through the line number array and input file linearly, printing lines where there are index matches, and skipping over the rest. &#xD;&#xA;&#xD;&#xA;Indeed, `subset` will quit early in parsing the input file if there are no more line numbers to query. So this feature is especially useful for speeding up filtering of very large query files. (If your query file has 1M rows, say, and your last line number of interest is 12345, there's no reason to read through the rest of the file.)&#xD;&#xA;&#xD;&#xA;You can grab, build and install it like so:&#xD;&#xA;&#xD;&#xA;    $ git clone https://github.com/alexpreynolds/subset.git&#xD;&#xA;    $ cd subset&#xD;&#xA;    $ make&#xD;&#xA;    $ cp subset /usr/local/bin&#xD;&#xA;&#xD;&#xA;Once the binary is in your path, there are a couple ways to use it.&#xD;&#xA;&#xD;&#xA;For example, you can specify a start index and length value. The following grabs seven lines starting with the 33rd line (32 as a 0-indexed value):&#xD;&#xA;&#xD;&#xA;    $ subset --prefix-with-indices -s 32 -n 7 -i query.txt &gt; answer.txt&#xD;&#xA;&#xD;&#xA;Or you can specify a text file containing line numbers, each on a separate line. The following reads in a file called `line-numbers.txt` and uses that to filter `query.txt`:&#xD;&#xA;&#xD;&#xA;    $ subset --prefix-with-indices -l line-numbers.txt -i query.txt &gt; answer.txt&#xD;&#xA;&#xD;&#xA;The indices in `line-numbers.txt` should be positive, 0-indexed integers. The list of numbers does not need to be sorted, as `subset` will sort the list of numbers for you. This is so that an efficient single pass through the input/query file can be done.&#xD;&#xA;&#xD;&#xA;You can leave out `--prefix-with-indices` to leave out the debug prefix. This is there so that you can do a sanity check on the result.&#xD;&#xA;&#xD;&#xA;The `test/makefile` tests demonstrate options and usages for the two types of filtering." />
  <row Id="1964" PostHistoryTypeId="2" PostId="625" RevisionGUID="d1accc63-10f8-4cae-a8a1-d788d0f0265e" CreationDate="2017-06-10T03:52:26.343" UserId="156" Text="My bet is that the nanopore folks have (of course!) done a ton of optimisation on two key things:&#xD;&#xA;&#xD;&#xA; 1. The DNA cleanup&#xD;&#xA; 2. The library preps&#xD;&#xA;&#xD;&#xA;I think that for most DNA extractions cleanups can vary a lot, but something like a Blue Pippen will do an exceptional job if you have access to one and can use it (they won't work for SUPER long DNA fragments which can't move through a gel of course). The Blue Pippen also does your size selection, so you don't burn out pores sequencing lots of little stuff. This should help yield too.&#xD;&#xA;&#xD;&#xA;Then it's all down to the library prep, and @gringer's answer has some good tips on that. " />
  <row Id="1965" PostHistoryTypeId="2" PostId="626" RevisionGUID="b3f337a6-9664-456b-a135-140bd63b5b58" CreationDate="2017-06-10T03:57:26.523" UserId="156" Text="I have a large phylogenomic alignment of &gt;1000 loci (each locus is ~1000bp), and &gt;100 species. I have relatively little missing data (&lt;10%). &#xD;&#xA;&#xD;&#xA;I want to estimate a maximum-likelihood phylogenetic tree from this data, with measures of statistical support on each node.&#xD;&#xA;&#xD;&#xA;There are many phylogenetics programs that claim to be able to analyse datasets like this (e.g. RAxML, ExaML, IQtree, FastTree, PhyML?, etc). Given that I have access to a big server (512GB RAM, 56 cores), what are the pros and cons of each program. Which is likely to give me the most accurate estimate of the ML tree for a dataset of this size?" />
  <row Id="1966" PostHistoryTypeId="1" PostId="626" RevisionGUID="b3f337a6-9664-456b-a135-140bd63b5b58" CreationDate="2017-06-10T03:57:26.523" UserId="156" Text="What is the best method to estimate a phylogenetic tree from a large dataset of &gt;1000 loci and &gt;100 species" />
  <row Id="1967" PostHistoryTypeId="3" PostId="626" RevisionGUID="b3f337a6-9664-456b-a135-140bd63b5b58" CreationDate="2017-06-10T03:57:26.523" UserId="156" Text="&lt;phylogenetics&gt;" />
  <row Id="1968" PostHistoryTypeId="5" PostId="625" RevisionGUID="cde959c0-85be-49b0-abac-359fdf5b9c6d" CreationDate="2017-06-10T05:01:59.620" UserId="73" Comment="corrected trademark spelling (Pippin)" Text="My bet is that the nanopore folks have (of course!) done a ton of optimisation on two key things:&#xD;&#xA;&#xD;&#xA; 1. The DNA cleanup&#xD;&#xA; 2. The library preps&#xD;&#xA;&#xD;&#xA;I think that for most DNA extractions cleanups can vary a lot, but something like a Blue Pippin will do an exceptional job if you have access to one and can use it (they won't work for SUPER long DNA fragments which can't move through a gel of course). The Blue Pippin also does your size selection, so you don't burn out pores sequencing lots of little stuff. This should help yield too.&#xD;&#xA;&#xD;&#xA;Then it's all down to the library prep, and @gringer's answer has some good tips on that. " />
  <row Id="1969" PostHistoryTypeId="6" PostId="567" RevisionGUID="95c7064c-f827-4074-9f4e-ad03b28172d6" CreationDate="2017-06-10T06:04:33.900" UserId="734" Comment="edited tags" Text="&lt;proteins&gt;&lt;networks&gt;&lt;gene&gt;&lt;pathway&gt;" />
  <row Id="1970" PostHistoryTypeId="5" PostId="597" RevisionGUID="d3bc981a-82c3-4ecb-935a-658fa3f0c3db" CreationDate="2017-06-10T07:35:23.737" UserId="298" Comment="added 2 characters in body" Text="An ex-coworker (Josep Avril) has written a couple of very useful little scripts that convert fasta to tbl (`seqID&lt;TAB&gt;Sequence`) and back again. These are extremely handy for this sort of thing (and are included at the end of this answer). Using them, you can convert your fasta to a one sequence per line format, keep the longest sequence with a simple awk script and convert back to fasta.&#xD;&#xA;&#xD;&#xA;I am assuming that the part after the last `.` in the ID line should be removed (so that `Doug_NoIndex_L005_R1_001_contig_2.g7.t1` and `Doug_NoIndex_L005_R1_001_contig_2.g7.t2` both map to `Doug_NoIndex_L005_R1_001_contig_2.g7`). If that is a correct assumption, this should work for you:&#xD;&#xA;&#xD;&#xA;    $ FastaToTbl file.fa | sed 's/\.[^.]*\t/\t/' | &#xD;&#xA;        awk -F&quot;\t&quot; '{&#xD;&#xA;                        if(length($2)&gt;length(a[$1])){&#xD;&#xA;                            a[$1]=$0&#xD;&#xA;                        }&#xD;&#xA;                    }&#xD;&#xA;                    END{&#xD;&#xA;                        for(i in a){&#xD;&#xA;                            print a[i]&#xD;&#xA;                        }&#xD;&#xA;                    }' | TblToFasta&#xD;&#xA;    &gt;Doug_NoIndex_L005_R1_001_contig_2.g7 &#xD;&#xA;    atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgat&#xD;&#xA;    tcgtatgatcaactcatcattaatataaccaaaaacctagaaattctagccttcgatgat&#xD;&#xA;    gttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;One possible issue with that approach is that it needs to keep one sequence per ID in memory. If you have a huge fasta file and not much memory, that might be a problem. If that's the case, you can first sort the file to ensure that similar IDs always appear together and then print each line as soon as you find the next ID:&#xD;&#xA;&#xD;&#xA;    FastaToTbl file.fa | sed 's/\.[^.]*\t/\t/' | LC_ALL=C sort -t$'\t' -k1,1 |  &#xD;&#xA;        awk -F&quot;\t&quot; '{&#xD;&#xA;                        if(NR&gt;1 &amp;&amp; prev!=$1){&#xD;&#xA;                            print a[$1]; &#xD;&#xA;                            prev=$1&#xD;&#xA;                        } &#xD;&#xA;                        if(length($2)&gt;length(a[$1])){&#xD;&#xA;                            a[$1]=$0&#xD;&#xA;                        }&#xD;&#xA;                    }&#xD;&#xA;                    END{&#xD;&#xA;                        print a[$1]&#xD;&#xA;                    }' | TblToFasta&#xD;&#xA;&#xD;&#xA;------------&#xD;&#xA;&#xD;&#xA;* FastaToTbl&#xD;&#xA;&#xD;&#xA;    &lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;        #!/usr/bin/awk -f&#xD;&#xA;        {&#xD;&#xA;                if (substr($1,1,1)==&quot;&gt;&quot;)&#xD;&#xA;        		if (NR&gt;1)&#xD;&#xA;                        	printf &quot;\n%s\t&quot;, substr($0,2,length($0)-1)&#xD;&#xA;        		else &#xD;&#xA;        			printf &quot;%s\t&quot;, substr($0,2,length($0)-1)&#xD;&#xA;                else &#xD;&#xA;                        printf &quot;%s&quot;, $0&#xD;&#xA;        }END{printf &quot;\n&quot;}&#xD;&#xA;        &#xD;&#xA;* TblToFasta&#xD;&#xA;&#xD;&#xA;    &lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;        #! /usr/bin/awk -f&#xD;&#xA;        {&#xD;&#xA;          sequence=$NF&#xD;&#xA;        &#xD;&#xA;          ls = length(sequence)&#xD;&#xA;          is = 1&#xD;&#xA;          fld  = 1&#xD;&#xA;          while (fld &lt; NF)&#xD;&#xA;          {&#xD;&#xA;             if (fld == 1){printf &quot;&gt;&quot;}&#xD;&#xA;             printf &quot;%s &quot; , $fld&#xD;&#xA;            &#xD;&#xA;             if (fld == NF-1)&#xD;&#xA;              {&#xD;&#xA;                printf &quot;\n&quot;&#xD;&#xA;              }&#xD;&#xA;              fld = fld+1&#xD;&#xA;          }&#xD;&#xA;          while (is &lt;= ls)&#xD;&#xA;          {&#xD;&#xA;            printf &quot;%s\n&quot;, substr(sequence,is,60)&#xD;&#xA;            is=is+60&#xD;&#xA;          }&#xD;&#xA;        }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1971" PostHistoryTypeId="2" PostId="627" RevisionGUID="381e9a2d-0f00-4fac-9738-ef829506dfd8" CreationDate="2017-06-10T10:41:51.353" UserId="73" Text="I've been processing data for a consortium project that uses the [rvtests](https://github.com/zhanxw/rvtests) toolkit. Our data analysis process uses a kinship matrix and the calculation of inverse-normalised transformed statistics after covariate adjustment (for 1kG-imputed data from a SNPchip). One of the operations we have been asked to do is to analyse males and females separately for a number of different phenotypes.&#xD;&#xA;&#xD;&#xA;Here's an example of the command line that I'm running:&#xD;&#xA;&#xD;&#xA;    rvtest --inVcf chr${ch}.imputed.poly.vcf.gz \&#xD;&#xA;     --pheno ../scripts/phenotypes.raw.2017-May-01.ped \&#xD;&#xA;     --pheno-name ${traitName} \&#xD;&#xA;     --out rv_results/${traitName}_FEMALE_chr${ch} \&#xD;&#xA;     --kinship kinship_matrix.kinship \&#xD;&#xA;     --meta score,cov[windowSize=500000] \&#xD;&#xA;     --peopleIncludeFile ../scripts/female.iids.txt \&#xD;&#xA;     --covar ../scripts/covar.2017-May-11.ped \&#xD;&#xA;     --covar-name ${covs} --useResidualAsPhenotype --dosage DS&#xD;&#xA;&#xD;&#xA;Unfortunately, the kinship matrix doesn't interact well with the `--peopleinclude` parameter. When I run this, the program complains about unexpected columns in the kinship matrix (starting at one column greater than the number of females in the dataset). I suspect what is happening is that rvtests is assuming that the kinship matrix applies to the sample set *after* filtering, rather than applying to the total sample set. This sets off a few warning bells in my head, because it means that rvtest is probably ignoring the column/row labels in the kinship matrix that it has generated.&#xD;&#xA;&#xD;&#xA;How can I work around this problem?" />
  <row Id="1972" PostHistoryTypeId="1" PostId="627" RevisionGUID="381e9a2d-0f00-4fac-9738-ef829506dfd8" CreationDate="2017-06-10T10:41:51.353" UserId="73" Text="How can I combine a kinship matrix with subset individuals when using rvtests?" />
  <row Id="1973" PostHistoryTypeId="3" PostId="627" RevisionGUID="381e9a2d-0f00-4fac-9738-ef829506dfd8" CreationDate="2017-06-10T10:41:51.353" UserId="73" Text="&lt;microarray&gt;&lt;rvtests&gt;&lt;covariate-analysis&gt;" />
  <row Id="1974" PostHistoryTypeId="2" PostId="628" RevisionGUID="d2246c6e-a344-4cc9-99e3-103e13cd5eba" CreationDate="2017-06-10T11:14:30.313" UserId="813" Text="I used a script in R language that uses nnet library to predict promoter bacteria and i would like to know how to extract rules from this neural network results.&#xD;&#xA;&#xD;&#xA;Which algorithms or softwares could i use?" />
  <row Id="1975" PostHistoryTypeId="1" PostId="628" RevisionGUID="d2246c6e-a344-4cc9-99e3-103e13cd5eba" CreationDate="2017-06-10T11:14:30.313" UserId="813" Text="Rule Extraction from nnet results" />
  <row Id="1976" PostHistoryTypeId="3" PostId="628" RevisionGUID="d2246c6e-a344-4cc9-99e3-103e13cd5eba" CreationDate="2017-06-10T11:14:30.313" UserId="813" Text="&lt;r&gt;&lt;fasta&gt;&lt;computation&gt;" />
  <row Id="1977" PostHistoryTypeId="5" PostId="628" RevisionGUID="af687228-9ea5-46ef-9737-97063557373f" CreationDate="2017-06-10T11:38:47.257" UserId="813" Comment="added 370 characters in body" Text="I used a script in R language that uses nnet library to predict promoter bacteria and i would like to know how to extract rules from this neural network results.&#xD;&#xA;&#xD;&#xA;As my input of the neural network i have n positive examples of promoter and n false examples. I use as input these examples and the FASTA file with the genome of the bacteria. As a result, i have a value of 0 to 1 for each network tested corresponding to its learning. I want to discover roles that can improve my network in future experiments in the same bacteria.&#xD;&#xA;&#xD;&#xA;Which algorithms or softwares could i use?" />
  <row Id="1978" PostHistoryTypeId="2" PostId="629" RevisionGUID="d7041c84-3fb4-4094-860a-2566bf633f69" CreationDate="2017-06-10T11:58:10.863" UserId="45" Text="[This paper claims][1] that FastTree is almost as accurate as RAxML, while being much faster. You just have be careful, however, that the support values output by FastTree are not bootstrap values, they are [based on the Shimodaira-Hasegawa test][2]. (Also, [see this comment][3] for the case you have very short branch lengths). &#xD;&#xA;&#xD;&#xA;From what I understand, you should use ExaML only if your data is too large to be handled by RAxML in a single node. ExaML should perform like RAxML but with some parallelization overhead. For all effects I treat them as the same. I don't know of relevant advantages of phyML over RAxML (for me, it's easier to use but I am very used to phyML).&#xD;&#xA;&#xD;&#xA;I am not familiar with IQ-tree, but [its authors claim][4] that even given the same time as RAxML or phyML, IQ-tree already finds better likelihoods more often than not (although by default it takes a bit longer to converge). [A recent comparison between all these programs][5] favoured RAxML for single-gene analysis and IQ-tree for concatenation (with RAxML very close). It may also estimate branch support through a SH-like test only, but I'm not sure.&#xD;&#xA;&#xD;&#xA;However, since you have few missing data, you might also want to try a single-locus tree inference followed by gene tree clustering (using [treescape][6] or [treeCL][7]) to see how spread your data is, or to see the effect of removal of outliers, or to use ideas similar to [statistical binning][8].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0027731&#xD;&#xA;  [2]: http://www.microbesonline.org/fasttree/&#xD;&#xA;  [3]: http://darlinglab.org/blog/2015/03/23/not-so-fast-fasttree.html&#xD;&#xA;  [4]: https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msu300&#xD;&#xA;  [5]: http://biorxiv.org/content/early/2017/05/25/142323&#xD;&#xA;  [6]: http://onlinelibrary.wiley.com/doi/10.1111/1755-0998.12676/abstract&#xD;&#xA;  [7]: https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msw038&#xD;&#xA;  [8]: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0129183" />
  <row Id="1979" PostHistoryTypeId="2" PostId="630" RevisionGUID="83547862-03a0-4fde-b660-7a77ce5aa538" CreationDate="2017-06-10T13:12:47.700" UserId="73" Text="Adam Locke (a collaborator of this project) suggests that removing covariate information for the unselected individuals (i.e. setting it to NA) works around this problem:&#xD;&#xA;&#xD;&#xA;&gt; I believe the problem is that he is using a pre-computed kinship matrix including both males and females, and when using the “—peopleIncludeFile” it can’t properly select the right people from the kinship matrix. I don’t know that others have tried to do analysis this way. An alternative, that I think has worked for others, is to generate phenotype variables that have values only for males or females (so, e.g., BMI, BMI_females, BMI_males). For whatever reason, this appears to correctly only use the and select the samples desired without the kinship matrix.&#xD;&#xA;&#xD;&#xA;Presumably a kinship matrix that is generated for just the subset individuals will work as well, although I don't like the idea that rvtests isn't able to do a label lookup on the kinship matrix to find the correct individuals." />
  <row Id="1981" PostHistoryTypeId="2" PostId="631" RevisionGUID="ee65a774-a501-495e-8be2-7dbfcf5e0718" CreationDate="2017-06-10T14:14:17.493" UserId="37" Text="R's nnet package only supports fully connected neural networks with one hidden layer. This is the most primitive type of network. I doubt it will work well for promotors finding. In addition, such network won't give you useful interpretations.&#xD;&#xA;&#xD;&#xA;If you want to explore neural networks, you should use a one-dimension convolution layer as is described in [this paper](http://www.nature.com/nbt/journal/v33/n8/full/nbt.3300.html). This layer effectively represents a positional weight matrix (PWM). You can know which motifs are used. With backtracking, you can also identify the coordinates of motifs. To deploy such a model, you need to be fairly familiar with deep learning and to learn one deep learning framework. For framework, you may start with [keras](https://keras.io). Once you get used to keras, you can implement your full promotor finder in less than 100 lines of python code. Alternatively, you may try [dragonn](http://kundajelab.github.io/dragonn/). It is supposed to simplify deploying models for DNA sequences. I have no experience with it, though.&#xD;&#xA;&#xD;&#xA;For promotor finding, it is also worth trying traditional methods. It is interesting that few/no neuralNet-based publications have evaluated traditional methods, probably because many ML people know little about classical motif finding. These methods could work well, too." />
  <row Id="1982" PostHistoryTypeId="5" PostId="553" RevisionGUID="17fd412c-49a1-4420-9bd4-0239417253c6" CreationDate="2017-06-10T14:39:04.273" UserId="45" Comment="replaced 'homology' statements by 'similarity'" Text="I'm a newcomer to the world of bioinformatics, and in need of help solving a problem.&#xD;&#xA;&#xD;&#xA;My goal is to take a list of human proteins, and identify segments (13-17aa in length) with a high degree of similarity to microbial sequences. Ideally, I would like to start with list of FASTA sequences, and have an easy way to generate an output of the corresponding high similarity segments of each protein.&#xD;&#xA;&#xD;&#xA;Are there existing tools or software that I should be aware of that will make my life easier?&#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="1983" PostHistoryTypeId="4" PostId="553" RevisionGUID="17fd412c-49a1-4420-9bd4-0239417253c6" CreationDate="2017-06-10T14:39:04.273" UserId="45" Comment="replaced 'homology' statements by 'similarity'" Text="Detecting portions of human proteins with high degree of microbial similarity" />
  <row Id="1984" PostHistoryTypeId="24" PostId="553" RevisionGUID="17fd412c-49a1-4420-9bd4-0239417253c6" CreationDate="2017-06-10T14:39:04.273" Comment="Proposed by 45 approved by 37, 77 edit id of 183" />
  <row Id="1985" PostHistoryTypeId="2" PostId="632" RevisionGUID="6a4a86ed-a25f-464d-9324-e780b1ebef95" CreationDate="2017-06-10T15:40:21.290" UserId="492" Text="From a statistical perspective ratios are icky because the variance of the sampling distribution of a ratio decreases with the size of the numerator and denominator being sampled to create the ratio. If you can get at the numerator and denominator separately, then you should absolutely do that and look at count based tests like the Chi-square test, Fisher's exact test, or Poisson regression. &#xD;&#xA;&#xD;&#xA;That said, a t-test would be appropriate in the two-sample case if its assumptions are met.  You need to check that both samples are normally distributed, ideally via a qq-plot. You also need to check the variance of both samples and choose a variant of the t-test that does or does not assume equal variances.&#xD;&#xA;&#xD;&#xA;If the samples are not normally distributed, then the next approach would be a [Mann-Whitney U](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) test." />
  <row Id="1987" PostHistoryTypeId="5" PostId="629" RevisionGUID="d0a87991-1e14-46f8-bd27-a73d645519c4" CreationDate="2017-06-10T17:19:47.500" UserId="45" Comment="fixed typo " Text="[This paper claims][1] that FastTree is almost as accurate as RAxML, while being much faster. You just have to be careful, however, that the support values output by FastTree are not bootstrap values, they are [based on the Shimodaira-Hasegawa test][2]. (Also, [see this comment][3] for the case you have very short branch lengths). &#xD;&#xA;&#xD;&#xA;From what I understand, you should use ExaML only if your data is too large to be handled by RAxML in a single node. ExaML should perform like RAxML but with some parallelization overhead. For all effects I treat them as the same. I don't know of relevant advantages of phyML over RAxML (for me, it's easier to use but I am very used to phyML).&#xD;&#xA;&#xD;&#xA;I am not familiar with IQ-tree, but [its authors claim][4] that even given the same time as RAxML or phyML, IQ-tree already finds better likelihoods more often than not (although by default it takes a bit longer to converge). [A recent comparison between all these programs][5] favoured RAxML for single-gene analysis and IQ-tree for concatenation (with RAxML very close). It may also estimate branch support through a SH-like test only, but I'm not sure.&#xD;&#xA;&#xD;&#xA;However, since you have few missing data, you might also want to try a single-locus tree inference followed by gene tree clustering (using [treescape][6] or [treeCL][7]) to see how spread your data is, or to see the effect of removal of outliers, or to use ideas similar to [statistical binning][8].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0027731&#xD;&#xA;  [2]: http://www.microbesonline.org/fasttree/&#xD;&#xA;  [3]: http://darlinglab.org/blog/2015/03/23/not-so-fast-fasttree.html&#xD;&#xA;  [4]: https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msu300&#xD;&#xA;  [5]: http://biorxiv.org/content/early/2017/05/25/142323&#xD;&#xA;  [6]: http://onlinelibrary.wiley.com/doi/10.1111/1755-0998.12676/abstract&#xD;&#xA;  [7]: https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msw038&#xD;&#xA;  [8]: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0129183" />
  <row Id="1988" PostHistoryTypeId="6" PostId="628" RevisionGUID="4ee9e9e9-dc0b-436e-8fd0-9cf86183a753" CreationDate="2017-06-10T19:20:57.900" UserId="96" Comment="Editing the tags" Text="&lt;r&gt;&lt;fasta&gt;&lt;machine-learning&gt;" />
  <row Id="1989" PostHistoryTypeId="24" PostId="628" RevisionGUID="4ee9e9e9-dc0b-436e-8fd0-9cf86183a753" CreationDate="2017-06-10T19:20:57.900" Comment="Proposed by 96 approved by 37, 77 edit id of 184" />
  <row Id="1990" PostHistoryTypeId="2" PostId="633" RevisionGUID="13d576f2-c769-449f-b1df-9f53226e2487" CreationDate="2017-06-10T20:24:19.480" UserId="734" Text="I have seen the [Cellminer tool][1]. I am not sure how do they calculate the cross correlation of the genes, what does it actually mean? Based on what databases? &#xD;&#xA;&#xD;&#xA;For example if I take their example input: &#xD;&#xA;	&#xD;&#xA;    abcb1&#xD;&#xA;    BRCA2&#xD;&#xA;    CNBP&#xD;&#xA;&#xD;&#xA;I get the following cross correlation matrix (based on what signals).&#xD;&#xA;&#xD;&#xA;    Identifier	    abcb1	BRCA2	CNBP&#xD;&#xA;    abcb1	        1	   -0.142	0.069&#xD;&#xA;    BRCA2	       -0.142	1	    0.176&#xD;&#xA;    CNBP	       0.069	0.176	1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://discover.nci.nih.gov/cellminer/" />
  <row Id="1991" PostHistoryTypeId="1" PostId="633" RevisionGUID="13d576f2-c769-449f-b1df-9f53226e2487" CreationDate="2017-06-10T20:24:19.480" UserId="734" Text="How does Cellminer's &quot;Cross-correlations of transcripts, drugs, and microRNAs&quot; work" />
  <row Id="1992" PostHistoryTypeId="3" PostId="633" RevisionGUID="13d576f2-c769-449f-b1df-9f53226e2487" CreationDate="2017-06-10T20:24:19.480" UserId="734" Text="&lt;statistics&gt;&lt;gene&gt;" />
  <row Id="1993" PostHistoryTypeId="5" PostId="633" RevisionGUID="b8e62ba4-02a8-4727-a49d-c66107df0ddc" CreationDate="2017-06-10T20:40:08.670" UserId="734" Comment="added 46 characters in body" Text="I have seen the [Cellminer tool][1]. I am not sure how do they calculate the cross correlation of the genes, what does it actually mean? Based on what databases? &#xD;&#xA;&#xD;&#xA;For example if I take their example input ([HUGO][2]): &#xD;&#xA;	&#xD;&#xA;    abcb1&#xD;&#xA;    BRCA2&#xD;&#xA;    CNBP&#xD;&#xA;&#xD;&#xA;I get the following cross correlation matrix (based on what signals).&#xD;&#xA;&#xD;&#xA;    Identifier	    abcb1	BRCA2	CNBP&#xD;&#xA;    abcb1	        1	   -0.142	0.069&#xD;&#xA;    BRCA2	       -0.142	1	    0.176&#xD;&#xA;    CNBP	       0.069	0.176	1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://discover.nci.nih.gov/cellminer/&#xD;&#xA;  [2]: http://www.genenames.org/" />
  <row Id="1994" PostHistoryTypeId="2" PostId="634" RevisionGUID="8a86d67f-6467-4612-a843-c23fc1b41f6f" CreationDate="2017-06-10T23:09:04.270" UserId="269" Text="You can see the exact assembly by looking in the README file in Ensembl's genome download page. ftp://ftp.ensembl.org/pub/release-89/fasta/homo_sapiens/dna/README&#xD;&#xA;&#xD;&#xA;As you can see the current assembly is GCA_000001405.25. Ensembl 79 uses version GCA_000001405.17. You can expect some differences between assembly versions. &#xD;&#xA;&#xD;&#xA;If you take a look at https://www.ncbi.nlm.nih.gov/assembly?term=GRCh38&amp;cmd=DetailsSearch &#xD;&#xA;&#xD;&#xA;You can see that 20,466,394 bp have been added since 17 -&gt; 25. This is only about 0.6% change so I would guess there is not that much difference between these versions. However, if I were you I would use the corresponding version of Ensembl. Even better would be using the latest version of Ensembl, as not only the genomes primary sequence but also the positions of genes can change between versions (https://doi.org/10.1093/bib/bbw017). &#xD;&#xA;&#xD;&#xA;" />
  <row Id="1999" PostHistoryTypeId="5" PostId="633" RevisionGUID="dc1b1edb-f607-46c2-b261-dfb977e22cb4" CreationDate="2017-06-10T23:23:11.857" UserId="734" Comment="added 4 characters in body" Text="I have seen one of [Cellminer tools][1]. I am not sure how do they calculate the cross correlation of the genes, what does it actually mean? Based on what databases? &#xD;&#xA;&#xD;&#xA;For example if I take their example input ([HUGO][2]): &#xD;&#xA;	&#xD;&#xA;    abcb1&#xD;&#xA;    BRCA2&#xD;&#xA;    CNBP&#xD;&#xA;&#xD;&#xA;I get the following cross correlation matrix (based on what signals).&#xD;&#xA;&#xD;&#xA;    Identifier	    abcb1	BRCA2	CNBP&#xD;&#xA;    abcb1	        1	   -0.142	0.069&#xD;&#xA;    BRCA2	       -0.142	1	    0.176&#xD;&#xA;    CNBP	       0.069	0.176	1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://discover.nci.nih.gov/cellminer/&#xD;&#xA;  [2]: http://www.genenames.org/" />
  <row Id="2000" PostHistoryTypeId="2" PostId="637" RevisionGUID="fc7f3d85-5e07-4acc-bf9f-4aa923630427" CreationDate="2017-06-10T23:27:13.180" UserId="734" Text="I read a lecture notes about [mutations][1], what kind of algorithms are there to detect mutation? How do people now if the gene is mutated or it's a sequencing error?&#xD;&#xA;&#xD;&#xA;I saw [this][2] which is related, but I am not sure how to work with the CIGAR is it 100% accurate? What's the underline mechanism to detect mutation? Is there a way to predict what the mutation will cause.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://mcb.berkeley.edu/courses/mcb142/lecture%20topics/Dernburg/Lecture6_Chapter8_screenviewing.pdf&#xD;&#xA;  [2]: https://bioinformatics.stackexchange.com/questions/98/how-to-quickly-determine-mutations-in-a-read-of-a-sam-file" />
  <row Id="2001" PostHistoryTypeId="1" PostId="637" RevisionGUID="fc7f3d85-5e07-4acc-bf9f-4aa923630427" CreationDate="2017-06-10T23:27:13.180" UserId="734" Text="How to detect a mutation and predict its consequences?" />
  <row Id="2002" PostHistoryTypeId="3" PostId="637" RevisionGUID="fc7f3d85-5e07-4acc-bf9f-4aa923630427" CreationDate="2017-06-10T23:27:13.180" UserId="734" Text="&lt;gene&gt;" />
  <row Id="2003" PostHistoryTypeId="2" PostId="638" RevisionGUID="22fb48c7-e374-4bd7-bef3-2329b97c58ac" CreationDate="2017-06-10T23:58:21.667" UserId="57" Text="The keywords you are searching for are &quot;variant calling&quot;. Basically you have to map sequencing reads to reference genome and then estimate for each position of genome if the observed difference is of mapped reads and the reference is more likely a sequencing error or a mutation (in genomic glossary - variant). &#xD;&#xA;&#xD;&#xA;Popular tools for variant calling are [GATK](https://software.broadinstitute.org/gatk/), [FreeBayes](https://github.com/ekg/freebayes) or [samtools](https://github.com/samtools/samtools)." />
  <row Id="2004" PostHistoryTypeId="5" PostId="638" RevisionGUID="c99996d3-6269-4b03-979c-f98dbd3be6c3" CreationDate="2017-06-11T00:09:55.077" UserId="57" Comment="added more details" Text="The keywords you are searching for are &quot;variant calling&quot;. Basically you have to map sequencing reads to reference genome (or gene) and then estimate for each position of genome if the observed difference is of mapped reads and the reference is more likely a sequencing error or a mutation (in genomic glossary - variant). &#xD;&#xA;&#xD;&#xA;Popular tools for variant calling are [GATK](https://software.broadinstitute.org/gatk/), [FreeBayes](https://github.com/ekg/freebayes) or [samtools](https://github.com/samtools/samtools).&#xD;&#xA;&#xD;&#xA;The thread discusses a quick alternatives of simple looking for variants. Indeed you can just visualise the mapped reads to the reference sequence and see if the variant is there or not. CIGAR is just a notation of read alignment used in sam files (files with mapped reads), you can find good explanation of CIGAR strings [here](http://genome.sph.umich.edu/wiki/SAM)." />
  <row Id="2005" PostHistoryTypeId="6" PostId="637" RevisionGUID="095a7ae8-d8cc-4558-9045-d34fd260d8b6" CreationDate="2017-06-11T00:39:58.810" UserId="57" Comment="added variant calling tag" Text="&lt;variant-calling&gt;&lt;gene&gt;" />
  <row Id="2006" PostHistoryTypeId="24" PostId="637" RevisionGUID="095a7ae8-d8cc-4558-9045-d34fd260d8b6" CreationDate="2017-06-11T00:39:58.810" Comment="Proposed by 57 approved by 734 edit id of 185" />
  <row Id="2007" PostHistoryTypeId="2" PostId="639" RevisionGUID="a9046a0d-8ffa-41bd-aa37-96a8a0d16fd2" CreationDate="2017-06-11T01:28:57.607" UserId="208" Text="What are the advantages of having Bioconductor, for the bioinformatics community?&#xD;&#xA;&#xD;&#xA;I've read the '[About][1]' section and skimmed the [paper][2], but still cannot really answer this.&#xD;&#xA;&#xD;&#xA;I understand Bioconductor is released twice a year (unlike `R`), but if I want to use the latest version of a package, I'll have to use the dev version anyway. A stamp of *approval* could be achieved much easier with a tag or something, so it sounds just like an extra (and unnecessary) layer to maintain.&#xD;&#xA;&#xD;&#xA;Related to this, what are the advantages as a developer to have your package accepted into Bioconductor?&#xD;&#xA;&#xD;&#xA;  [1]: https://bioconductor.org/about/&#xD;&#xA;  [2]: https://genomebiology.biomedcentral.com/articles/10.1186/gb-2004-5-10-r80" />
  <row Id="2008" PostHistoryTypeId="1" PostId="639" RevisionGUID="a9046a0d-8ffa-41bd-aa37-96a8a0d16fd2" CreationDate="2017-06-11T01:28:57.607" UserId="208" Text="Why Bioconductor?" />
  <row Id="2009" PostHistoryTypeId="3" PostId="639" RevisionGUID="a9046a0d-8ffa-41bd-aa37-96a8a0d16fd2" CreationDate="2017-06-11T01:28:57.607" UserId="208" Text="&lt;r&gt;&lt;bioconductor&gt;" />
  <row Id="2011" PostHistoryTypeId="5" PostId="633" RevisionGUID="e8c92c3c-42f2-4d17-9aa3-1301e434c1ae" CreationDate="2017-06-11T05:26:04.453" UserId="734" Comment="added 7 characters in body" Text="I have seen one of [Cellminer tools][1]. I am not sure how do they calculate the cross correlation of the genes, what does it actually mean? Based on what databases? &#xD;&#xA;&#xD;&#xA;For example if I take their example input ([HUGO][2] format): &#xD;&#xA;	&#xD;&#xA;    abcb1&#xD;&#xA;    BRCA2&#xD;&#xA;    CNBP&#xD;&#xA;&#xD;&#xA;I get the following cross correlation matrix (based on what signals).&#xD;&#xA;&#xD;&#xA;    Identifier	    abcb1	BRCA2	CNBP&#xD;&#xA;    abcb1	        1	   -0.142	0.069&#xD;&#xA;    BRCA2	       -0.142	1	    0.176&#xD;&#xA;    CNBP	       0.069	0.176	1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://discover.nci.nih.gov/cellminer/&#xD;&#xA;  [2]: http://www.genenames.org/" />
  <row Id="2012" PostHistoryTypeId="5" PostId="638" RevisionGUID="a7f33ef5-9612-4e4f-bfe5-1e90c3787081" CreationDate="2017-06-11T06:45:35.053" UserId="57" Comment="phrasing" Text="The keywords you are searching for are &quot;variant calling&quot;. Basically you have to map sequencing reads to reference genome (or gene) and then estimate for each position of genome if the observed difference is of mapped reads and the reference is more likely a sequencing error or a mutation (in genomic glossary - variant). &#xD;&#xA;&#xD;&#xA;Popular tools for variant calling are [GATK](https://software.broadinstitute.org/gatk/), [FreeBayes](https://github.com/ekg/freebayes) or [samtools](https://github.com/samtools/samtools).&#xD;&#xA;&#xD;&#xA;The question you linked asks for a quick alternatives of simple looking for variants. Indeed, you can just visualise the mapped reads to the reference sequence and see if the variant is there or not. CIGAR is just a notation of read alignment used in sam files (files with mapped reads), you can find good explanation of CIGAR strings [here](http://genome.sph.umich.edu/wiki/SAM)." />
  <row Id="2013" PostHistoryTypeId="2" PostId="641" RevisionGUID="7660f839-3536-482a-ac66-33337d5ea754" CreationDate="2017-06-11T07:13:30.900" UserId="57" Text="Having a central repository for packages is very useful. For couple of reasons:&#xD;&#xA;&#xD;&#xA;1. It makes very easy to resolve dependencies. Installing all the dependencies manually would be exhausting but also dangerous (point 2).&#xD;&#xA;&#xD;&#xA;2. Package compatibility! If I install package with dependencies, I would like to be sure that I install correct versions of all the dependencies.&#xD;&#xA;&#xD;&#xA;3. Reliability thanks to unified and integrated testing. `Bioconductor` is trying really hard to force developers to write good test, they also have people manually testing submitted packages. They also remove packages that are not maintained. Packages in `Bioconductor` are (reasonably) reliable.&#xD;&#xA;&#xD;&#xA;In the end, installing dev versions of R packages is in my opinion **very bad practise** for reproducible science. If developers delete GitHub repo, commit hash you have used won't be enough to get the code.&#xD;&#xA;&#xD;&#xA;However, I never fully understood why `Bioconductor` instead of using standard package repository for `R` : `CRAN`. We discussed it with my labmates couple of times and we came up only with : `Bioconductor` also provides a platform for the bioinformatics R community. `Bioconductor` uses more strict testing of packages (good for maintaining healthy repository). " />
  <row Id="2014" PostHistoryTypeId="2" PostId="642" RevisionGUID="f5f0db0e-2b94-4ce4-a623-fd09f578a418" CreationDate="2017-06-11T10:09:41.653" UserId="77" Text="Regarding what the advantage is to you as a developer of having a bioconductor package rather than using CRAN:&#xD;&#xA;&#xD;&#xA;1. There's a hierarchy of package quality, with bioconductor on the top (followed by CRAN and then &quot;random github repos&quot;). While there are many excellent CRAN packages, the average bioconductor package is better tested and documented. So if I as a user have two different packages that I can use and one is on bioconductor and the other CRAN, then I use the bioconductor package.&#xD;&#xA;2. Higher visibility. Since bioconductor packages are held in higher regard, they also become more visible. Further, the different &quot;views&quot; (e.g., &quot;Alternative Splicing&quot; and &quot;Transcription&quot;) make it convenient to find relevant packages. You can always search CRAN, but allowing tagging like this aids in discovery." />
  <row Id="2015" PostHistoryTypeId="2" PostId="643" RevisionGUID="fe6586f0-36f5-48b8-b9e8-0fdd74aa848e" CreationDate="2017-06-11T10:13:24.243" UserId="48" Text="Here is a list of the advantages of Bioconductor for the bioinformatic community compared to CRAN:&#xD;&#xA;&#xD;&#xA; - **Outreach**: You have a **repository for the field**, in that language. Some packages related to bioinformatics (in R) are distributed through personal repositories, CRAN, github, bitbucket, sourceforge, but they are less used and harder to find. There are such efforts in other languages too: Biopython, Bioperl, Biojava, ... Also is harder to find the repositories related to a subject in CRAN, you don't have the BiocViews, the equivalent is optional and not usually filled.&#xD;&#xA;&#xD;&#xA; - **Quality**: In Bioconductor each package is tested in Linux, Windows, and iOS, to make sure it works in all major operative systems (with all the dependencies). You are required to provide a vignette and examples in every exported element (and the vignette, examples and tests should pass). You are required to be able to install the package with stricter quality than CRAN, because there is a manual review (They pointed out a *comment* in one of my functions!). They also provide docker images of the base packages. You don't need to install the latest R version to use the Bioconductor! But developers do so (at least when checked by Bioconductor servers) to ensure that the package will keep working in next R release. &#xD;&#xA;&#xD;&#xA; - **Reusing**: Bioconductor provides the basic elements to a big number of applications. For example the summarizedExperiment class is provided so that any package that needs a similar object can (should) use it. Or GSEABase is the base package to deal with GSEA enrichment analysis, providing functions, methods and classes for gene sets, and collections, making easier for anyone to create their own GSE analysis. It is easier to build upon the work of others if you know you are following the same quality standards. &#xD;&#xA;&#xD;&#xA; -  **Support**: To support Q&amp;A is mandatory, the package maintainer must be registered in the [webpage][1]. While in CRAN usually the support is given by each package in its own way, in Bioconductor you can directly reach the maintainer and the users by posting in the same central place. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.bioconductor.org" />
  <row Id="2016" PostHistoryTypeId="2" PostId="644" RevisionGUID="1dea71cc-bb79-4401-8403-94b00de7cb90" CreationDate="2017-06-11T10:15:56.960" UserId="77" Text="Regarding predicting what the variant (&quot;mutation&quot; is a very loaded term) will do, there are a variety of tools. Chief among these are [annovar](http://annovar.openbioinformatics.org/en/latest/) and [VEP](http://www.ensembl.org/info/docs/tools/vep/index.html). The general idea behind these is to classify the variants according to their overlap with genes, which codons they change (if any), how big that change is (e.g., changes in charge are more likely detrimental) and so on. One could also consider conservation, since if a position is highly conserved then changes in it are more likely to be detrimental.&#xD;&#xA;&#xD;&#xA;If you really want to predict how a variant will change a protein's function then that usually requires prior knowledge about the proteins in question. Eventually someone will use machine learning to cull the literature and provide good predictions, but I haven't seen that yet." />
  <row Id="2017" PostHistoryTypeId="2" PostId="645" RevisionGUID="92b10169-82bc-469f-bfdb-7729be792bff" CreationDate="2017-06-11T10:18:55.697" UserId="283" Text="The most important reason is that Bioconductor has growing set of common data structures and base packages. If package X and package Y needs to work with the same type of data, having a common data structure in the core Bioconductor package Z makes out lives so much easier. I could do something in package X, take out the results and keep working on my data with package Y. This works because I'm using versions of package X and Y that are compatible with the data structure defined in package Z. The Bioconductor team makes sure that all packages use the common data structures and already existing packages where possible, so that we're all on the same page and so that people don't invent the wheel again and again.&#xD;&#xA;&#xD;&#xA;Also, there's a sophisticated process around [getting your package accepted to Bioconductor](https://www.bioconductor.org/developers/package-submission/). This ensures that packages use data structures and functions already available in other Bioconductor packages (where reasonable), it ensures that packages are well written, that they have good documentation, and that they are well tested." />
  <row Id="2018" PostHistoryTypeId="5" PostId="643" RevisionGUID="2bd027f8-0563-419f-a0be-d280887c479c" CreationDate="2017-06-11T10:25:31.730" UserId="48" Comment="Update about the availability on all platforms" Text="Here is a list of the advantages of Bioconductor for the bioinformatic community compared to CRAN:&#xD;&#xA;&#xD;&#xA; - **Outreach**: You have a **repository for the field**, in that language. &#xD;&#xA;&#xD;&#xA;Some packages related to bioinformatics (in R) are distributed through personal repositories, CRAN, github, bitbucket, sourceforge, but they are less used and harder to find.  &#xD;&#xA;There are such efforts in other languages too: Biopython, Bioperl, Biojava, ...  &#xD;&#xA;Also is harder to find the repositories related to a subject in CRAN, you don't have the BiocViews, the equivalent is optional and not usually filled.&#xD;&#xA;&#xD;&#xA; - **Quality**: In Bioconductor each package is tested in Linux, Windows, and iOS, to make sure it works in all major operative systems (with all the dependencies). &#xD;&#xA;&#xD;&#xA;In some rare cases like [this one][1], a packages is not supported for certain platform, but you can known it by checking the [build report][2].  &#xD;&#xA;You are required to provide a vignette and examples in every exported element (and the vignette, examples and tests should pass). You are required to be able to install the package with stricter quality than CRAN, because there is a manual review (They pointed out a *comment* in one of my functions!).  &#xD;&#xA;They also provide docker images of the base packages. You don't need to install the latest R version to use the Bioconductor! But developers do so (at least when checked by Bioconductor servers) to ensure that the package will keep working in next R release. &#xD;&#xA;&#xD;&#xA; - **Reusing**: Bioconductor provides the basic elements to a big number of applications. &#xD;&#xA;&#xD;&#xA;For example the summarizedExperiment class is provided so that any package that needs a similar object can (should) use it. Or GSEABase is the base package to deal with GSEA enrichment analysis, providing functions, methods and classes for gene sets, and collections, making easier for anyone to create their own GSE analysis.  &#xD;&#xA;It is easier to build upon the work of others if you know you are following the same quality standards. &#xD;&#xA;&#xD;&#xA; -  **Support**: To support Q&amp;A is mandatory, the package maintainer must be registered in the [webpage][3]. &#xD;&#xA;&#xD;&#xA;While in CRAN usually the support is given by each package in its own way, in Bioconductor you can directly reach the maintainer and the users by posting in the same central place. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioconductor.org/packages/release/bioc/html/Rsubread.html&#xD;&#xA;  [2]: http://bioconductor.org/checkResults/release/bioc-LATEST/Rsubread/&#xD;&#xA;  [3]: https://support.bioconductor.org" />
  <row Id="2019" PostHistoryTypeId="5" PostId="641" RevisionGUID="dce36db9-3711-46fa-91ba-22f571f4bc53" CreationDate="2017-06-11T12:06:10.213" UserId="57" Comment="fixed typo" Text="Having a central repository for packages is very useful. For couple of reasons:&#xD;&#xA;&#xD;&#xA;1. It makes very easy to resolve dependencies. Installing all the dependencies manually would be exhausting but also dangerous (point 2).&#xD;&#xA;2. Package compatibility! If I install package with dependencies, I would like to be sure that I install correct versions of all the dependencies.&#xD;&#xA;3. Reliability thanks to unified and integrated testing. `Bioconductor` is trying really hard to force developers to write good test, they also have people manually testing submitted packages. They also remove packages that are not maintained. Packages in `Bioconductor` are (reasonably) reliable.&#xD;&#xA;&#xD;&#xA;In the end, installing dev versions of R packages is in my opinion **very bad practise** for reproducible science. If developers delete GitHub repo, commit hash you have used won't be enough to get the code.&#xD;&#xA;&#xD;&#xA;However, I never fully understood why `Bioconductor` instead of using standard package repository for `R` : `CRAN`. We discussed it with my labmates couple of times and we came up only with : `Bioconductor` also provides a platform for the bioinformatics R community. `Bioconductor` uses more strict testing of packages (good for maintaining healthy repository). &#xD;&#xA;&#xD;&#xA;-- edit --&#xD;&#xA;&#xD;&#xA;I forgot about the advantages for you as developer to submit your package to `Bioconductor`:&#xD;&#xA;&#xD;&#xA;1. Your package will be more visible&#xD;&#xA;2. users will have a guarantee that your code was checked by third person&#xD;&#xA;3. Your package will be for users easier to install&#xD;&#xA;4. Your package will be forced to use standardized vignettes, version tags and tests -&gt; will be more accessible by community to build on your code&#xD;&#xA;" />
  <row Id="2020" PostHistoryTypeId="5" PostId="638" RevisionGUID="1eddef50-55c1-421a-8171-9dbef064eafe" CreationDate="2017-06-11T12:45:03.093" UserId="57" Comment="fixed typo" Text="The keywords you are searching for are &quot;variant calling&quot;. Basically you have to map sequencing reads to reference genome (or gene) and then estimate for each position of genome if the observed difference of mapped reads and the reference is more likely a sequencing error or a mutation (in genomic glossary - variant). &#xD;&#xA;&#xD;&#xA;Popular tools for variant calling are [GATK](https://software.broadinstitute.org/gatk/), [FreeBayes](https://github.com/ekg/freebayes) or [samtools](https://github.com/samtools/samtools).&#xD;&#xA;&#xD;&#xA;The question you linked asks for a quick alternatives of simple looking for variants. Indeed, you can just visualise the mapped reads to the reference sequence and see if the variant is there or not. CIGAR is just a notation of read alignment used in sam files (files with mapped reads), you can find good explanation of CIGAR strings [here](http://genome.sph.umich.edu/wiki/SAM)." />
  <row Id="2021" PostHistoryTypeId="5" PostId="638" RevisionGUID="e1b31576-2192-4c29-97b5-eb8f88b125e9" CreationDate="2017-06-11T12:53:56.983" UserId="298" Comment="Samtools has no variant caller; copy edit" Text="The keywords you are searching for are &quot;variant calling&quot;. Basically you have to map sequencing reads to a reference genome (or gene) and then estimate for each position of the genome if the observed difference of mapped reads and the reference is more likely a sequencing error or a mutation (in genomic glossary - variant). &#xD;&#xA;&#xD;&#xA;Popular tools for variant calling are [GATK](https://software.broadinstitute.org/gatk/), [FreeBayes](https://github.com/ekg/freebayes).&#xD;&#xA;&#xD;&#xA;The question you linked asks for quick alternatives to simply looking for variants. Indeed, you can just visualise the mapped reads to the reference sequence and see if the variant is there or not. CIGAR is just a notation of read alignment used in sam files (files with mapped reads), you can find good explanation of CIGAR strings [here](http://genome.sph.umich.edu/wiki/SAM)." />
  <row Id="2022" PostHistoryTypeId="24" PostId="638" RevisionGUID="e1b31576-2192-4c29-97b5-eb8f88b125e9" CreationDate="2017-06-11T12:53:56.983" Comment="Proposed by 298 approved by 57 edit id of 186" />
  <row Id="2023" PostHistoryTypeId="5" PostId="644" RevisionGUID="b1ebd4e6-8069-4696-bcd6-24883ce2f0c2" CreationDate="2017-06-11T13:12:17.440" UserId="77" Comment="added 108 characters in body" Text="I'll follow up to the great answer from https://bioinformatics.stackexchange.com/users/57/kamil-s-jaron:&#xD;&#xA;&#xD;&#xA;Regarding predicting what the variant (&quot;mutation&quot; is a very loaded term) will do, there are a variety of tools. Chief among these are [annovar](http://annovar.openbioinformatics.org/en/latest/) and [VEP](http://www.ensembl.org/info/docs/tools/vep/index.html). The general idea behind these is to classify the variants according to their overlap with genes, which codons they change (if any), how big that change is (e.g., changes in charge are more likely detrimental) and so on. One could also consider conservation, since if a position is highly conserved then changes in it are more likely to be detrimental.&#xD;&#xA;&#xD;&#xA;If you really want to predict how a variant will change a protein's function then that usually requires prior knowledge about the proteins in question. Eventually someone will use machine learning to cull the literature and provide good predictions, but I haven't seen that yet." />
  <row Id="2024" PostHistoryTypeId="2" PostId="646" RevisionGUID="cfbd0180-4ce6-475b-86cf-2a8af90b8192" CreationDate="2017-06-11T13:49:45.317" UserId="818" Text="I need to insert a group of PDB IDs (IDs related to protein-protein interactions) in PDB and get their structures and sequences and then find the refseq sequence related to each PDB ID. what should I do?&#xD;&#xA;" />
  <row Id="2025" PostHistoryTypeId="1" PostId="646" RevisionGUID="cfbd0180-4ce6-475b-86cf-2a8af90b8192" CreationDate="2017-06-11T13:49:45.317" UserId="818" Text="getting structure and sequence related to PDB IDs" />
  <row Id="2026" PostHistoryTypeId="3" PostId="646" RevisionGUID="cfbd0180-4ce6-475b-86cf-2a8af90b8192" CreationDate="2017-06-11T13:49:45.317" UserId="818" Text="&lt;protein-structure&gt;" />
  <row Id="2027" PostHistoryTypeId="5" PostId="638" RevisionGUID="cdb352f4-228a-44bb-a03c-0c01a8e79cb8" CreationDate="2017-06-11T14:10:18.750" UserId="57" Comment="added link to Devon's answer. Added back Samtools for variant calling..." Text="**Part 1 : how to detect mutations**&#xD;&#xA;&#xD;&#xA;The keywords you are searching for are &quot;variant calling&quot;. Basically you have to map sequencing reads to a reference genome (or gene) and then estimate for each position of the genome if the observed difference of mapped reads and the reference is more likely a sequencing error or a mutation (in genomic glossary - variant). &#xD;&#xA;&#xD;&#xA;Popular tools for variant calling are [GATK](https://software.broadinstitute.org/gatk/), [FreeBayes](https://github.com/ekg/freebayes) or [bcftools](https://samtools.github.io/bcftools/howtos/variant-calling.html) (previously part as Samtools package).&#xD;&#xA;&#xD;&#xA;The question you linked asks for quick alternatives to simply looking for variants. Indeed, you can just visualise the mapped reads to the reference sequence and see if the variant is there or not. CIGAR is just a notation of read alignment used in sam files (files with mapped reads), you can find good explanation of CIGAR strings [here](http://genome.sph.umich.edu/wiki/SAM).&#xD;&#xA;&#xD;&#xA;The follow-up about &quot;How to estimate an effect of mutation&quot; is in @DevonRyan 's awesome answer." />
  <row Id="2028" PostHistoryTypeId="2" PostId="647" RevisionGUID="00547527-c008-49ba-b9df-f6ecc476c5a3" CreationDate="2017-06-11T14:55:08.643" UserId="640" Text="You can download seqs and structures based on a list of PDB ids using http://www.rcsb.org/pdb/download/download.do#FASTA " />
  <row Id="2031" PostHistoryTypeId="5" PostId="646" RevisionGUID="b2308eaf-86bf-4c18-8f11-33282ffeeae6" CreationDate="2017-06-11T19:59:51.613" UserId="818" Comment="added 1923 characters in body" Text="I am going to describle the problem completely. First I should say that there are two kinds of interactions: transient and stable interactions. We are supposed to work on stable interaction like interactions between two monomers in a heterodimer. In a heterodimer there are two chains in which there are some residues between monomers which take part in the interaction of monomers and building the structure of heterodimer and also there are other residues interacting inside each monomer for building the structure of that monomer. There are some PDB IDs related to heterodimers in PDB. Now imagine that we are trying to show which residues are physically interacting between two monomers in a heterodimer and then we are trying two show the site of these physically interacting residues in the sequence related to that special structure. For that, we need to find the sequence related to that structure and we can download it from PDB. But if we want to make a multiple sequence alignment of that sequence, first we need to find the refseq sequence of that and then we must blast the sequence and find homologues of that sequence. Here there are some challenges: first, we must find residues interacting in the structure of heterodimer between two monomers. Second, we must find the sequences of monomer chains of the heterodimer and map the interacting residues in the structure to the sequence. Third, we know that when a structure is solved and its monomers are sequenced, may be they are not able to completely sequence the heterodimer and residue numbers are maybe different from the related refseq sequence. &#xD;&#xA;I would like to know how can we find the related refseq sequence (or sequences, in the case of heterodimer) and then how can we map the physically interacting residues in the structure of PDB ID to refseq sequence (finding the sites of these interacting residues in that refseq sequence) regarding three challenges mentioned above. &#xD;&#xA;Imagine that I have 100 PDB IDs of 100 nonredundant heterodimers and I would like to find and download the structures, sequence and finally refseq sequence. What should I do? " />
  <row Id="2034" PostHistoryTypeId="2" PostId="649" RevisionGUID="c7ab97f2-63b4-47e9-be70-73e6c6eb4124" CreationDate="2017-06-11T20:27:23.053" UserId="492" Text="Another next-gen read simulation tool is [gemsim](https://popmodels.cancercontrol.cancer.gov/gsr/packages/gemsim/).  I haven't tested it, but I would be interested if anyone has had any experience with it." />
  <row Id="2035" PostHistoryTypeId="2" PostId="650" RevisionGUID="0077219b-fcea-49f7-8559-1f72f7ee25ab" CreationDate="2017-06-11T20:33:33.147" UserId="818" Text="I am going to describle the problem completely. First I should say that there are two kinds of interactions: transient and stable interactions. We are supposed to work on stable interaction like interactions between two monomers in a heterodimer. In a heterodimer there are two chains in which there are some residues between monomers which take part in the interaction of monomers and building the structure of heterodimer and also there are other residues interacting inside each monomer for building the structure of that monomer. There are some PDB IDs related to heterodimers in PDB. Now imagine that we are trying to show which residues are physically interacting between two monomers in a heterodimer and then we are trying two show the site of these physically interacting residues in the sequence related to that special structure. For that, we need to find the sequence related to that structure and we can download it from PDB. But if we want to make a multiple sequence alignment of that sequence, first we need to find the refseq sequence of that and then we must blast the sequence and find homologues of that sequence. Here there are some challenges: first, we must find residues interacting in the structure of heterodimer between two monomers. Second, we must find the sequences of monomer chains of the heterodimer and map the interacting residues in the structure to the sequence. Third, we know that when a structure is solved and its monomers are sequenced, may be they are not able to completely sequence the heterodimer and residue numbers are maybe different from the related refseq sequence. I would like to know how can we find the related refseq sequence (or sequences, in the case of heterodimer) and then how can we map the physically interacting residues in the structure of PDB ID to refseq sequence (finding the sites of these interacting residues in that refseq sequence) regarding three challenges mentioned above. Imagine that I have 100 PDB IDs of 100 nonredundant heterodimers and I would like to find and download the structures, sequence and finally refseq sequence. What should I do?" />
  <row Id="2036" PostHistoryTypeId="1" PostId="650" RevisionGUID="0077219b-fcea-49f7-8559-1f72f7ee25ab" CreationDate="2017-06-11T20:33:33.147" UserId="818" Text="Finding residues physically interacting in a PDB structure and sequence related to that structure" />
  <row Id="2037" PostHistoryTypeId="3" PostId="650" RevisionGUID="0077219b-fcea-49f7-8559-1f72f7ee25ab" CreationDate="2017-06-11T20:33:33.147" UserId="818" Text="&lt;protein-structure&gt;" />
  <row Id="2040" PostHistoryTypeId="2" PostId="651" RevisionGUID="cda5d721-dc74-4a3a-aaa3-d8d2b918e5f2" CreationDate="2017-06-11T21:57:03.280" UserId="191" Text="I think there is no metabolite with such name in the database. I tried [their search](https://biocyc.org/ECOLI/search-query?type=COMPOUND&amp;name=Valine), the correct name seems to be `L-valine`. I expect that they use similar naming for other amino acids. If you cannot find something, you can use their compound search through the website. " />
  <row Id="2041" PostHistoryTypeId="5" PostId="643" RevisionGUID="9b857a66-ad5c-48d3-8b5e-1768ca931f17" CreationDate="2017-06-11T22:00:10.837" UserId="48" Comment="added 40 characters in body" Text="Here is a list of the advantages of having Bioconductor for the bioinformatic community:&#xD;&#xA;&#xD;&#xA; - **Outreach**: You have a **repository for the field**, in that language. &#xD;&#xA;&#xD;&#xA;Some packages related to bioinformatics (in R) are distributed through personal repositories, CRAN, github, bitbucket, sourceforge, but they are less used and harder to find.  &#xD;&#xA;There are such efforts in other languages too: Biopython, Bioperl, Biojava, ...  &#xD;&#xA;Also is harder to find the repositories related to a subject in CRAN, you don't have the BiocViews, the equivalent is optional and not usually filled,  which is quite useful when looking for a method.&#xD;&#xA;&#xD;&#xA; - **Quality**: In Bioconductor each package is tested in Linux, Windows, and iOS, to make sure it works in all major operative systems (with all the dependencies). &#xD;&#xA;&#xD;&#xA;In some rare cases like [this one][1], a packages is not supported for certain platform, but you can known it by checking the [build report][2].  &#xD;&#xA;You are required to provide a vignette and examples in every exported element (and the vignette, examples and tests should pass). You are required to be able to install the package with stricter quality than CRAN, because there is a manual review (They pointed out a *comment* in one of my functions!).  &#xD;&#xA;They also provide docker images of the base packages. You don't need to install the latest R version to use the Bioconductor! But developers do so (at least when checked by Bioconductor servers) to ensure that the package will keep working in next R release. &#xD;&#xA;&#xD;&#xA; - **Reusing**: Bioconductor provides the basic elements to a big number of applications. &#xD;&#xA;&#xD;&#xA;For example the summarizedExperiment class is provided so that any package that needs a similar object can (should) use it. Or GSEABase is the base package to deal with GSEA enrichment analysis, providing functions, methods and classes for gene sets, and collections, making easier for anyone to create their own GSE analysis.  &#xD;&#xA;It is easier to build upon the work of others if you know you are following the same quality standards. &#xD;&#xA;&#xD;&#xA; -  **Support**: To support Q&amp;A is mandatory, the package maintainer must be registered in the [webpage][3]. &#xD;&#xA;&#xD;&#xA;While in CRAN usually the support is given by each package in its own way, in Bioconductor you can directly reach the maintainer and the users by posting in the same central place. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioconductor.org/packages/release/bioc/html/Rsubread.html&#xD;&#xA;  [2]: http://bioconductor.org/checkResults/release/bioc-LATEST/Rsubread/&#xD;&#xA;  [3]: https://support.bioconductor.org" />
  <row Id="2046" PostHistoryTypeId="2" PostId="653" RevisionGUID="3d4eadc7-6902-4a9f-b945-c2c9a7a853d0" CreationDate="2017-06-12T06:08:10.227" UserId="734" Text="I would like to visualize [this][1] interaction list.  Is there an online/web based way to do it? Is there a way to analyze the data?&#xD;&#xA;&#xD;&#xA;Assuming I have exported the list into lines such as:&#xD;&#xA;&#xD;&#xA;`2	A2M	348	APOE`&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=shownetwork" />
  <row Id="2047" PostHistoryTypeId="1" PostId="653" RevisionGUID="3d4eadc7-6902-4a9f-b945-c2c9a7a853d0" CreationDate="2017-06-12T06:08:10.227" UserId="734" Text="What are the standard way to visualize protein-protein or gene-gene interactions" />
  <row Id="2048" PostHistoryTypeId="3" PostId="653" RevisionGUID="3d4eadc7-6902-4a9f-b945-c2c9a7a853d0" CreationDate="2017-06-12T06:08:10.227" UserId="734" Text="&lt;gene&gt;&lt;proteins&gt;&lt;interactions&gt;" />
  <row Id="2049" PostHistoryTypeId="5" PostId="606" RevisionGUID="d18fd34f-17e6-46a8-a3af-10e0e865abb4" CreationDate="2017-06-12T06:25:40.003" UserId="787" Comment="added 707 characters in body" Text="it is under development, but maybe [BaMMmotif!][1] is something for you? Its main selling point is that it can look for motifs enriched in a set of sequences of equal length *de novo*. If you can't/don't want to supply a negative set it learns one from the positive sequences. There is a wealth of options to choose from if you have more information about your sequences: there are different models for &quot;zero or one&quot;, &quot;one&quot; and &quot;multiple&quot; occurrences of the motif.&#xD;&#xA;&#xD;&#xA;You can also use it to look for known motifs, if you encode them as an XXmotif PWM. If you have a file with motifs (like binding sites) you can use that as initialization as well.&#xD;&#xA;&#xD;&#xA;While I have not used the software myself, the authors are very responsive on git and the installation instructions seem pretty straightforward.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/soedinglab/BaMMmotif" />
  <row Id="2050" PostHistoryTypeId="2" PostId="654" RevisionGUID="628bc992-2162-4b05-8df3-3ead19f525af" CreationDate="2017-06-12T07:49:43.730" UserId="73" Text="*This is a problem I have to solve frequently, and I'd be interested in knowing what other methods people use to solve the same problem.*&#xD;&#xA;&#xD;&#xA;About twice a year, I get asked to determine variants from Illumina reads, usually from either mouse or human. These are things that have good reference genomes (e.g. [human genome from ensembl](http://www.ensembl.org/Homo_sapiens/Info/Index), [mouse genome from ensembl](http://www.ensembl.org/Mus_musculus/Info/Index)), and are frequently tested out on various bioinformatics tools.&#xD;&#xA;&#xD;&#xA;I have at my disposal three computers, in order of usage preference: a laptop with 4 processing threads and 8GB of memory, a desktop with 12 processing threads and 64GB of memory, and a server with 24 processing threads and 256GB of memory. Occasionally I get access to better computers when I need to do lots of processing, or to do things quickly, but these comprise my basic bioinformatics team.&#xD;&#xA;&#xD;&#xA;I've had whole-genome projects that involved processing data from 4 individuals, up to projects with about 120 individuals, with each individual having about 10-40M paired-end reads (100-125bp each). Beyond this level, I suspect other researchers will typically have their own bioinformatics team and/or capabilities, which is why I don't get any of the larger-scale jobs that I hear about from salaried bioinformaticians.&#xD;&#xA;&#xD;&#xA;What is the standard approach that you would use to process these reads into reference-mapped variant data (i.e. gzipped VCF) for use in downstream analysis tools (e.g. [VEP](http://www.ensembl.org/Homo_sapiens/Info/Index))?" />
  <row Id="2051" PostHistoryTypeId="1" PostId="654" RevisionGUID="628bc992-2162-4b05-8df3-3ead19f525af" CreationDate="2017-06-12T07:49:43.730" UserId="73" Text="How do I generate a variant list (i.e. VCF file) using Illumina reads from a human genome?" />
  <row Id="2052" PostHistoryTypeId="3" PostId="654" RevisionGUID="628bc992-2162-4b05-8df3-3ead19f525af" CreationDate="2017-06-12T07:49:43.730" UserId="73" Text="&lt;variant-calling&gt;&lt;read-mapping&gt;&lt;human-genome&gt;" />
  <row Id="2053" PostHistoryTypeId="2" PostId="655" RevisionGUID="59c42d5f-9f09-43f4-a8fc-f05f50e276f3" CreationDate="2017-06-12T08:34:12.630" UserId="690" Text="since I am not yet allowed to comment, I will have to pots this as an answer.&#xD;&#xA;&#xD;&#xA;I guess Cytoscape (http://cytoscape.org/) would be a nice way to analyse such interaction lists/networks (still depends a bit on what you plan to do).&#xD;&#xA;&#xD;&#xA;However, for this you will have to preprocess the downloadable excel sheet a bit.&#xD;&#xA;&#xD;&#xA; 1. save as tab-separated file (tsv)&#xD;&#xA; 2. run the below python3 script to format into an easily importable format for cytoscape. You can run the script as `python3 script.py &lt;path-to-tsv-file&gt; &lt;delimeter for cytoscape&gt;` . The new format is written to std out on the console, therefore maybe you need to write the output to a new file. A sample call could look like `python3 testbench.py human_interactome_2017-06-12.tsv.csv &quot;;&quot;`. Make sure to add the delimeter in quotes.&#xD;&#xA; 3. you can then import the new file into cytoscape (import network as table) where the first column is your source, the third column your target and the middle column your interaction type.&#xD;&#xA; 4. Within cytoscape you can then adapt your edge style to vary the line type in a discrete mapping depending on the value of the interaction.&#xD;&#xA;&#xD;&#xA;Screenshot from cytoscape:&#xD;&#xA;[![Cytoscape screenshot][1]][1]&#xD;&#xA;&#xD;&#xA;Code below:&#xD;&#xA;&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    HIIIidx = 4&#xD;&#xA;    HII5idx = 5&#xD;&#xA;    venkatesanidx = 6&#xD;&#xA;    yuidx = 7&#xD;&#xA;    Litidx = 8&#xD;&#xA;    &#xD;&#xA;    if len(sys.argv) &lt;= 1:&#xD;&#xA;        print(&quot;Usage: script.py file delimeter&quot;)&#xD;&#xA;&#xD;&#xA;    delim = ';'&#xD;&#xA;    inFile = sys.argv[1]&#xD;&#xA;&#xD;&#xA;    if len(sys.argv) &gt; 2:&#xD;&#xA;        delim = sys.argv[2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    def printInteraction(syma, symb, inter):&#xD;&#xA;        print(syma + delim + inter + delim + symb)&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    lineCount = -1&#xD;&#xA;    for line in open( inFile , 'r'):&#xD;&#xA;    &#xD;&#xA;        lineCount += 1&#xD;&#xA;    &#xD;&#xA;        if lineCount == 0:&#xD;&#xA;            continue&#xD;&#xA;    &#xD;&#xA;        line = line.strip('\n')&#xD;&#xA;        aline = line.split('\t')&#xD;&#xA;    &#xD;&#xA;        if aline[HIIIidx] == '1':&#xD;&#xA;            printInteraction(aline[1], aline[3], 'HIII')&#xD;&#xA;    &#xD;&#xA;        if aline[HII5idx] == '1':&#xD;&#xA;            printInteraction(aline[1], aline[3], 'HII5')&#xD;&#xA;    &#xD;&#xA;        if aline[venkatesanidx] == '1':&#xD;&#xA;            printInteraction(aline[1], aline[3], 'VEN')&#xD;&#xA;    &#xD;&#xA;        if aline[yuidx] == '1':&#xD;&#xA;            printInteraction(aline[1], aline[3], 'YU')&#xD;&#xA;    &#xD;&#xA;        if aline[Litidx] == '1':&#xD;&#xA;            printInteraction(aline[1], aline[3], 'LIT')&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/FlSN5.png" />
  <row Id="2054" PostHistoryTypeId="2" PostId="656" RevisionGUID="9957ff63-f2b7-48cc-9b73-b022e041e208" CreationDate="2017-06-12T08:41:12.713" UserId="73" Text="1. Adapter Trimming&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;One of the first things I do after encountering a set of reads is to remove the adapter sequences from the start and end of reads. Most basecalling software includes some amount of built-in adapter trimming, but it is almost always the case that some adapter sequence will remain. Removing adapters is helpful for mapping because it reduces the effort the mapper needs to go through to make a match (which may help some borderline reads to be mapped properly).&#xD;&#xA;&#xD;&#xA;My preferred adapter trimmer is [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic). It has the ability to search for palindromic adapter read through, and includes an adaptive sliding window option. Trimmomatic is threaded, and seems to work reasonably fast when run through files one at a time in threaded mode. Here's an example script that I might run to trim reads from some samples. This puts trimmed FASTQ files into a `trimmed` subdirectory:&#xD;&#xA;&#xD;&#xA;    TRIMPATH=/data/all/programs/trimmomatic/Trimmomatic-0.36;&#xD;&#xA;    JARPATH=${TRIMPATH}/trimmomatic-0.36.jar;&#xD;&#xA;    ADAPTfile=${TRIMPATH}/adapters/TruSeq3-PE-2.fa;&#xD;&#xA;    CMDS=&quot; ILLUMINACLIP:${ADAPTfile}:2:30:10:5:true LEADING:3 TRAILING:3  SLIDINGWINDOW:10:20 MINLEN:40 &quot;;&#xD;&#xA;&#xD;&#xA;    mkdir -p trimmed;&#xD;&#xA;    &#xD;&#xA;    for INFILE1 in *_1.fastq.gz;&#xD;&#xA;      do base=$(basename INFILE1 _1.fastq.gz);&#xD;&#xA;      do echo ${base};&#xD;&#xA;      INFILE2=${base}_2.fastq.gz;&#xD;&#xA;      OUTFILE_P1=trimmed/${base}_P1.fastq.gz;&#xD;&#xA;      OUTFILE_P2=trimmed/${base}_P2.fastq.gz;&#xD;&#xA;      OUTFILE_U1=trimmed/${base}_U1.fastq.gz;&#xD;&#xA;      OUTFILE_U2=trimmed/${base}_U2.fastq.gz;&#xD;&#xA;      java -jar ${JARPATH} PE -threads 20 -phred33 \&#xD;&#xA;            ${INFILE1}  ${INFILE2} \&#xD;&#xA;            ${OUTFILE_P1}  ${OUTFILE_U1}  ${OUTFILE_P2}  ${OUTFILE_U2} \&#xD;&#xA;            ${CMDS};&#xD;&#xA;    done;&#xD;&#xA;&#xD;&#xA;Trimmomatic is somewhat sensitive to bad input data, and will loudly fail (i.e. stop running) if it sees quality strings of different length to sequence strings:&#xD;&#xA;&#xD;&#xA;    Exception in thread &quot;Thread-1&quot; java.lang.RuntimeException: Sequence and quality length don't match: 'TACATGGCCCTGAAATGACTTTCACCCAGGCAACCAGTGCCCCCTGTATAGACACATGCCTTGGGCGCTCCCCACCCTTCCTCGCGTGGCCACACCTCTGT' vs '-AAFFJFJJ7A-FJJJFJFJFJJFFA-FFAF-&lt;A-&lt;FFFJA-&lt;-A7-F&lt;&lt;FFJJAJJJJJJJJJ--&lt;--7-7AJ7&lt;AAJA--J7&lt;ACTGCTGTGGGGCACCCAGCCCCCCAGATAGCCTGGCAGAAGGATGGGGGCACAGACTTCCCAGCTGCACGGGAGAGAC'&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqRecord.&lt;init&gt;(FastqRecord.java:25)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:89)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)&#xD;&#xA;            at org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)&#xD;&#xA;            at java.lang.Thread.run(Thread.java:745)&#xD;&#xA;&#xD;&#xA;2. Read Mapping&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;Read mapping finds the most likely location for a read within a target genome.&#xD;&#xA;There are extremely fast approximate mappers available, but these don't yet work for variant calling, and an exact mapping approach is necessary. My current preferred mapper is [HISAT2](https://ccb.jhu.edu/software/hisat2/index.shtml). I use this instead of BWA due to the double-read issue, and because of the local variant-aware mapping. HISAT2 is made by the same computing group as Bowtie2 and Tophat2 (JHUCCB), and is their recommended tool for replacing those other programs.&#xD;&#xA;&#xD;&#xA;&gt; HISAT2 is a successor to both HISAT and TopHat2. We recommend that HISAT and TopHat2 users switch to HISAT2.&#xD;&#xA;&#xD;&#xA;The HISAT2 page includes a link to a [genomic index file](ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38_snp.tar.gz) for the human genome, including SNP variants, which I use when mapping human reads. HISAT2 is also threaded, so I run it on the files one at a time and pipe through samtools to create a sorted BAM file:&#xD;&#xA;&#xD;&#xA;    mkdir mapped&#xD;&#xA;    for r1 in trimmed/*_P1.fastq.gz | perl -pe 's/_P1.fastq.gz//');&#xD;&#xA;      do base=basename ${x} _P1.fastq.gz;&#xD;&#xA;      r2=trimmed/${base}_P2.fastq.gz;&#xD;&#xA;      echo ${base};&#xD;&#xA;      hisat2 -p 20 -t -x /data/all/genomes/hsap/hisat_grch38_snp/genome_snp -1 \&#xD;&#xA;        ${r1} -2 ${r2} 2&gt;mapped/hisat2_${y}_vs_grch38_stderr.txt | \&#xD;&#xA;        samtools sort &gt; mapped/hisat2_${y}_vs_grch38.bam;&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;3. Variant Calling&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;I currently use samtools/bcftools to do this, but would be interested in other opinions as we recently had a grant reviewer response that samtools was a dinosaur program and better approaches were available. Samtools doesn't currently work with threads for variant calling, so I save the commands to a text file and then run them through [GNU Parallel](https://www.gnu.org/software/parallel/). This requires the genome files to be first downloaded and indexed via `samtools faidx`. This produces a set of gzipped VCF files in the `variants` directory:&#xD;&#xA;&#xD;&#xA;    mkdir -p variants;&#xD;&#xA;    (for x in mapped/*.bam;&#xD;&#xA;       do echo samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa ${x} \| \&#xD;&#xA;       bcftools call -v -m -O z -o variants/$(basename ${x} .bam).vcf.gz; done) &gt; call_jobs.txt&#xD;&#xA;    cat call_jobs.txt | parallel -u;" />
  <row Id="2055" PostHistoryTypeId="5" PostId="605" RevisionGUID="81f068e5-6e13-497f-91d5-5071c8d59340" CreationDate="2017-06-12T08:41:32.520" UserId="599" Comment="Update after clarification that no background set is available." Text="Most tools I know of looks for enrichment of specific motifs - but that requires that you have a set of sequences which are of special interest and a background set to test against.&#xD;&#xA;&#xD;&#xA;Is that your case?&#xD;&#xA;&#xD;&#xA;Update after comments 12th June 2017.&#xD;&#xA;&#xD;&#xA;You could try the meme suite: http://meme-suite.org/ more specifically the motif finder http://meme-suite.org/tools/meme&#xD;&#xA;&#xD;&#xA;Hope this helps." />
  <row Id="2056" PostHistoryTypeId="5" PostId="629" RevisionGUID="5aba3691-11de-4956-a414-b501e926d451" CreationDate="2017-06-12T08:49:18.100" UserId="45" Comment="added info based on OP comments" Text="[This paper claims][1] that FastTree is almost as accurate as RAxML, while being much faster. You just have to be careful, however, that the support values output by FastTree are not bootstrap values, they are [based on the Shimodaira-Hasegawa test][2]. (Also, [see this comment][3] for the case you have very short branch lengths).   [**update:** However, according to [the recent comparison paper mentioned below][5] FastTree performed quite poorly in comparison to RAxML or IQ-tree.]&#xD;&#xA;&#xD;&#xA;From what I understand, you should use ExaML only if your data is too large to be handled by RAxML in a single node. ExaML should perform like RAxML but with some parallelization overhead. For all effects I treat them as the same. I don't know of relevant advantages of phyML over RAxML (for me, it's easier to use but I am very used to phyML).&#xD;&#xA;&#xD;&#xA;I am not familiar with IQ-tree, but [its authors claim][4] that even given the same time as RAxML or phyML, IQ-tree already finds better likelihoods more often than not (although by default it takes a bit longer to converge). [A recent comparison between all these programs][5] favoured RAxML for single-gene analysis and IQ-tree for concatenation (with RAxML very close). It may also estimate branch support through a SH-like test only, but I'm not sure.   [**update:** IQ-tree offers 3 measures of support, standard bootstrap, aLRT, and ultrafast bootstrap. See OP's comment below for details.]&#xD;&#xA;&#xD;&#xA;However, since you have few missing data, you might also want to try a single-locus tree inference followed by gene tree clustering (using [treescape][6] or [treeCL][7]) to see how spread your data is, or to see the effect of removal of outliers, or to use ideas similar to [statistical binning][8].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0027731&#xD;&#xA;  [2]: http://www.microbesonline.org/fasttree/&#xD;&#xA;  [3]: http://darlinglab.org/blog/2015/03/23/not-so-fast-fasttree.html&#xD;&#xA;  [4]: https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msu300&#xD;&#xA;  [5]: http://biorxiv.org/content/early/2017/05/25/142323&#xD;&#xA;  [6]: http://onlinelibrary.wiley.com/doi/10.1111/1755-0998.12676/abstract&#xD;&#xA;  [7]: https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msw038&#xD;&#xA;  [8]: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0129183" />
  <row Id="2057" PostHistoryTypeId="2" PostId="657" RevisionGUID="c901e92f-7324-4318-8265-f15652ff0f7f" CreationDate="2017-06-12T09:26:50.953" UserId="48" Text="Usually interactions are represented in a graph, with edges representing the interactions (colors, thickness or values on the edges represent different properties of those edges).&#xD;&#xA;&#xD;&#xA;As far as I know, there aren't on line programs to represent this kind of information. For an off line representation, you can use the [igraph][1] package in R, or [Cytoscape][2] program. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://cran.r-project.org/package=igraph&#xD;&#xA;  [2]: http://www.cytoscape.org/" />
  <row Id="2058" PostHistoryTypeId="5" PostId="606" RevisionGUID="8127efce-090b-478a-8aae-1f2bdff94a8d" CreationDate="2017-06-12T09:44:17.427" UserId="787" Comment="added 166 characters in body" Text="it is under development, but maybe [BaMMmotif!][1] is something for you? Its main selling point is that it can look for motifs enriched in a set of sequences of equal length *de novo*. If you can't/don't want to supply a negative set it learns one from the positive sequences. There is a wealth of options to choose from if you have more information about your sequences: there are different models for &quot;zero or one&quot;, &quot;one&quot; and &quot;multiple&quot; occurrences of the motif.&#xD;&#xA;&#xD;&#xA;You can also use it to look for known motifs, if you encode them as an XXmotif PWM. If you have a file with motifs (like binding sites) you can use that as initialization as well.&#xD;&#xA;&#xD;&#xA;While I have not used the software myself, the authors are very responsive on git and the installation instructions seem pretty straightforward.&#xD;&#xA;&#xD;&#xA;EDIT: Apparently the software was developed with ChIP experiments as the usual use case scenario, it might misbehave for larger sequences or take super long to run.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/soedinglab/BaMMmotif" />
  <row Id="2059" PostHistoryTypeId="5" PostId="645" RevisionGUID="595d31d9-4fad-4472-8cfa-9f58be062b04" CreationDate="2017-06-12T10:22:33.963" UserId="283" Comment="edited body" Text="The most important reason is that Bioconductor has growing set of common data structures and base packages. If package X and package Y needs to work with the same type of data, having a common data structure in the core Bioconductor package Z makes our lives so much easier. I could do something in package X, take out the results and keep working on my data with package Y. This works because I'm using versions of package X and Y that are compatible with the data structure defined in package Z. The Bioconductor team makes sure that all packages use the common data structures and already existing packages where possible, so that we're all on the same page and so that people don't reinvent the wheel again and again.&#xD;&#xA;&#xD;&#xA;Also, there's a sophisticated process around [getting your package accepted to Bioconductor](https://www.bioconductor.org/developers/package-submission/). This ensures that packages use data structures and functions already available in other Bioconductor packages (where reasonable), it ensures that packages are well written, that they have good documentation, and that they are well tested." />
  <row Id="2060" PostHistoryTypeId="5" PostId="656" RevisionGUID="35d66c4e-0b77-46d4-ac27-cf711a961af7" CreationDate="2017-06-12T10:59:18.250" UserId="73" Comment="added ploidy setting as recommended by most recent bcftools" Text="1. Adapter Trimming&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;One of the first things I do after encountering a set of reads is to remove the adapter sequences from the start and end of reads. Most basecalling software includes some amount of built-in adapter trimming, but it is almost always the case that some adapter sequence will remain. Removing adapters is helpful for mapping because it reduces the effort the mapper needs to go through to make a match (which may help some borderline reads to be mapped properly).&#xD;&#xA;&#xD;&#xA;My preferred adapter trimmer is [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic). It has the ability to search for palindromic adapter read through, and includes an adaptive sliding window option. Trimmomatic is threaded, and seems to work reasonably fast when run through files one at a time in threaded mode. Here's an example script that I might run to trim reads from some samples. This puts trimmed FASTQ files into a `trimmed` subdirectory:&#xD;&#xA;&#xD;&#xA;    TRIMPATH=/data/all/programs/trimmomatic/Trimmomatic-0.36;&#xD;&#xA;    JARPATH=${TRIMPATH}/trimmomatic-0.36.jar;&#xD;&#xA;    ADAPTfile=${TRIMPATH}/adapters/TruSeq3-PE-2.fa;&#xD;&#xA;    CMDS=&quot; ILLUMINACLIP:${ADAPTfile}:2:30:10:5:true LEADING:3 TRAILING:3  SLIDINGWINDOW:10:20 MINLEN:40 &quot;;&#xD;&#xA;&#xD;&#xA;    mkdir -p trimmed;&#xD;&#xA;    &#xD;&#xA;    for INFILE1 in *_1.fastq.gz;&#xD;&#xA;      do base=$(basename INFILE1 _1.fastq.gz);&#xD;&#xA;      do echo ${base};&#xD;&#xA;      INFILE2=${base}_2.fastq.gz;&#xD;&#xA;      OUTFILE_P1=trimmed/${base}_P1.fastq.gz;&#xD;&#xA;      OUTFILE_P2=trimmed/${base}_P2.fastq.gz;&#xD;&#xA;      OUTFILE_U1=trimmed/${base}_U1.fastq.gz;&#xD;&#xA;      OUTFILE_U2=trimmed/${base}_U2.fastq.gz;&#xD;&#xA;      java -jar ${JARPATH} PE -threads 20 -phred33 \&#xD;&#xA;            ${INFILE1}  ${INFILE2} \&#xD;&#xA;            ${OUTFILE_P1}  ${OUTFILE_U1}  ${OUTFILE_P2}  ${OUTFILE_U2} \&#xD;&#xA;            ${CMDS};&#xD;&#xA;    done;&#xD;&#xA;&#xD;&#xA;Trimmomatic is somewhat sensitive to bad input data, and will loudly fail (i.e. stop running) if it sees quality strings of different length to sequence strings:&#xD;&#xA;&#xD;&#xA;    Exception in thread &quot;Thread-1&quot; java.lang.RuntimeException: Sequence and quality length don't match: 'TACATGGCCCTGAAATGACTTTCACCCAGGCAACCAGTGCCCCCTGTATAGACACATGCCTTGGGCGCTCCCCACCCTTCCTCGCGTGGCCACACCTCTGT' vs '-AAFFJFJJ7A-FJJJFJFJFJJFFA-FFAF-&lt;A-&lt;FFFJA-&lt;-A7-F&lt;&lt;FFJJAJJJJJJJJJ--&lt;--7-7AJ7&lt;AAJA--J7&lt;ACTGCTGTGGGGCACCCAGCCCCCCAGATAGCCTGGCAGAAGGATGGGGGCACAGACTTCCCAGCTGCACGGGAGAGAC'&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqRecord.&lt;init&gt;(FastqRecord.java:25)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:89)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)&#xD;&#xA;            at org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)&#xD;&#xA;            at java.lang.Thread.run(Thread.java:745)&#xD;&#xA;&#xD;&#xA;2. Read Mapping&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;Read mapping finds the most likely location for a read within a target genome.&#xD;&#xA;There are extremely fast approximate mappers available, but these don't yet work for variant calling, and an exact mapping approach is necessary. My current preferred mapper is [HISAT2](https://ccb.jhu.edu/software/hisat2/index.shtml). I use this instead of BWA due to the double-read issue, and because of the local variant-aware mapping. HISAT2 is made by the same computing group as Bowtie2 and Tophat2 (JHUCCB), and is their recommended tool for replacing those other programs.&#xD;&#xA;&#xD;&#xA;&gt; HISAT2 is a successor to both HISAT and TopHat2. We recommend that HISAT and TopHat2 users switch to HISAT2.&#xD;&#xA;&#xD;&#xA;The HISAT2 page includes a link to a [genomic index file](ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38_snp.tar.gz) for the human genome, including SNP variants, which I use when mapping human reads. HISAT2 is also threaded, so I run it on the files one at a time and pipe through samtools to create a sorted BAM file:&#xD;&#xA;&#xD;&#xA;    mkdir mapped&#xD;&#xA;    for r1 in trimmed/*_P1.fastq.gz | perl -pe 's/_P1.fastq.gz//');&#xD;&#xA;      do base=basename ${x} _P1.fastq.gz;&#xD;&#xA;      r2=trimmed/${base}_P2.fastq.gz;&#xD;&#xA;      echo ${base};&#xD;&#xA;      hisat2 -p 20 -t -x /data/all/genomes/hsap/hisat_grch38_snp/genome_snp -1 \&#xD;&#xA;        ${r1} -2 ${r2} 2&gt;mapped/hisat2_${y}_vs_grch38_stderr.txt | \&#xD;&#xA;        samtools sort &gt; mapped/hisat2_${y}_vs_grch38.bam;&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;3. Variant Calling&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;I currently use samtools/bcftools to do this, but would be interested in other opinions as we recently had a grant reviewer response that samtools was a dinosaur program and better approaches were available. Samtools doesn't currently work with threads for variant calling, so I save the commands to a text file and then run them through [GNU Parallel](https://www.gnu.org/software/parallel/). This requires the genome files to be first downloaded and indexed via `samtools faidx`. This produces a set of gzipped VCF files in the `variants` directory:&#xD;&#xA;&#xD;&#xA;    mkdir -p variants;&#xD;&#xA;    (for x in mapped/*.bam;&#xD;&#xA;       do echo samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa ${x} \| \&#xD;&#xA;       bcftools call --ploidy GRCh38 -v -m -O z -o variants/$(basename ${x} .bam).vcf.gz; done) &gt; call_jobs.txt&#xD;&#xA;    cat call_jobs.txt | parallel -u;" />
  <row Id="2061" PostHistoryTypeId="5" PostId="644" RevisionGUID="ef314312-2ca1-4f50-9ba0-3a861a338c89" CreationDate="2017-06-12T11:24:40.193" UserId="292" Comment="Added link to the other answer" Text="I'll follow up to the [great answer](https://bioinformatics.stackexchange.com/a/638/292) from https://bioinformatics.stackexchange.com/users/57/kamil-s-jaron:&#xD;&#xA;&#xD;&#xA;Regarding predicting what the variant (&quot;mutation&quot; is a very loaded term) will do, there are a variety of tools. Chief among these are [annovar](http://annovar.openbioinformatics.org/en/latest/) and [VEP](http://www.ensembl.org/info/docs/tools/vep/index.html). The general idea behind these is to classify the variants according to their overlap with genes, which codons they change (if any), how big that change is (e.g., changes in charge are more likely detrimental) and so on. One could also consider conservation, since if a position is highly conserved then changes in it are more likely to be detrimental.&#xD;&#xA;&#xD;&#xA;If you really want to predict how a variant will change a protein's function then that usually requires prior knowledge about the proteins in question. Eventually someone will use machine learning to cull the literature and provide good predictions, but I haven't seen that yet." />
  <row Id="2062" PostHistoryTypeId="24" PostId="644" RevisionGUID="ef314312-2ca1-4f50-9ba0-3a861a338c89" CreationDate="2017-06-12T11:24:40.193" Comment="Proposed by 292 approved by 77 edit id of 188" />
  <row Id="2063" PostHistoryTypeId="5" PostId="638" RevisionGUID="06f30b7c-7ac3-44b6-af0a-6af6b71c3c6b" CreationDate="2017-06-12T11:36:07.287" UserId="292" Comment="Added link to the other answer" Text="**Part 1 : how to detect mutations**&#xD;&#xA;&#xD;&#xA;The keywords you are searching for are &quot;variant calling&quot;. Basically you have to map sequencing reads to a reference genome (or gene) and then estimate for each position of the genome if the observed difference of mapped reads and the reference is more likely a sequencing error or a mutation (in genomic glossary - variant). &#xD;&#xA;&#xD;&#xA;Popular tools for variant calling are [GATK](https://software.broadinstitute.org/gatk/), [FreeBayes](https://github.com/ekg/freebayes) or [bcftools](https://samtools.github.io/bcftools/howtos/variant-calling.html) (previously part as Samtools package).&#xD;&#xA;&#xD;&#xA;The question you linked asks for quick alternatives to simply looking for variants. Indeed, you can just visualise the mapped reads to the reference sequence and see if the variant is there or not. CIGAR is just a notation of read alignment used in sam files (files with mapped reads), you can find good explanation of CIGAR strings [here](http://genome.sph.umich.edu/wiki/SAM).&#xD;&#xA;&#xD;&#xA;The follow-up about &quot;How to estimate an effect of mutation&quot; is in [@DevonRyan 's awesome answer](https://bioinformatics.stackexchange.com/a/644/292)." />
  <row Id="2064" PostHistoryTypeId="24" PostId="638" RevisionGUID="06f30b7c-7ac3-44b6-af0a-6af6b71c3c6b" CreationDate="2017-06-12T11:36:07.287" Comment="Proposed by 292 approved by 77, 57 edit id of 187" />
  <row Id="2065" PostHistoryTypeId="2" PostId="658" RevisionGUID="5315cdeb-9a2f-49d8-a7a5-e71e150855fe" CreationDate="2017-06-12T11:51:35.007" UserId="208" Text="I'm looking for cloud computing services that can be used for doing bioinformatics. An example I found is [InsideDNA][1] and there is [Amazon][2] of course.&#xD;&#xA;A little description of these would be appreciated.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://insidedna.me&#xD;&#xA;  [2]: https://aws.amazon.com/health/genomics/" />
  <row Id="2066" PostHistoryTypeId="1" PostId="658" RevisionGUID="5315cdeb-9a2f-49d8-a7a5-e71e150855fe" CreationDate="2017-06-12T11:51:35.007" UserId="208" Text="What are the available cloud computing services for bioinformatics?" />
  <row Id="2067" PostHistoryTypeId="3" PostId="658" RevisionGUID="5315cdeb-9a2f-49d8-a7a5-e71e150855fe" CreationDate="2017-06-12T11:51:35.007" UserId="208" Text="&lt;linux&gt;" />
  <row Id="2068" PostHistoryTypeId="2" PostId="659" RevisionGUID="db051685-b344-4b41-b02e-8f6c7b88909a" CreationDate="2017-06-12T12:51:03.997" UserId="104" Text="I have trialed the free version of InsideDNA, and these were my notes:&#xD;&#xA;&#xD;&#xA; - Cost: $225 per month for a team of 5 with 50TB storage or $45 per month with 10TB storage for individuals (assuming 6 month package: https://insidedna.me/pricing).&#xD;&#xA; - Software installed: Around 600 bioinformatic tools available and standard command line tools; some popular tools missing (like [CD-HIT][1]), but should be possible to install on request.&#xD;&#xA; - Jobs: Maximum of 32 CPUs and 208 RAM per job submission. Test jobs generally worked, although a larger test job failed.&#xD;&#xA; - Other points: Command line was sometimes slow, `wget` queries were slow, and `scp` was blocked. However, these may be resolvable issues.&#xD;&#xA;&#xD;&#xA;Overall, I felt InsideDNA could be useful for groups without their own computational infrastructure and could be used for easily sharing resources between groups. The packages on offer seem not expensive, but I had a few issues, and I don't know how good their sys admin support would be.&#xD;&#xA;&#xD;&#xA;I have not used the Amazon service, so can't comment beyond the details on their website. Also there are a few alternative companies, such as [Genestack][2] and [DNAnexus][3], but I haven't directly tested them either.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://weizhongli-lab.org/cd-hit/&#xD;&#xA;  [2]: https://genestack.com/&#xD;&#xA;  [3]: https://www.dnanexus.com/" />
  <row Id="2069" PostHistoryTypeId="5" PostId="655" RevisionGUID="b6b710b9-cf7a-49cb-aeba-c8a2b558fd76" CreationDate="2017-06-12T12:59:17.563" UserId="690" Comment="improved code as per comments" Text="since I am not yet allowed to comment, I will have to pots this as an answer.&#xD;&#xA;&#xD;&#xA;I guess Cytoscape (http://cytoscape.org/) would be a nice way to analyse such interaction lists/networks (still depends a bit on what you plan to do).&#xD;&#xA;&#xD;&#xA;However, for this you will have to preprocess the downloadable excel sheet a bit.&#xD;&#xA;&#xD;&#xA; 1. save as tab-separated file (tsv)&#xD;&#xA; 2. run the below python3 script to format into an easily importable format for cytoscape. You can run the script as `python3 script.py &lt;path-to-tsv-file&gt; &lt;delimeter for cytoscape&gt;` . The new format is written to std out on the console, therefore maybe you need to write the output to a new file. A sample call could look like `python3 testbench.py human_interactome_2017-06-12.tsv.csv &quot;;&quot;`. Make sure to add the delimeter in quotes.&#xD;&#xA; 3. you can then import the new file into cytoscape (import network as table) where the first column is your source, the third column your target and the middle column your interaction type.&#xD;&#xA; 4. Within cytoscape you can then adapt your edge style to vary the line type in a discrete mapping depending on the value of the interaction.&#xD;&#xA;&#xD;&#xA;Screenshot from cytoscape:&#xD;&#xA;[![Cytoscape screenshot][1]][1]&#xD;&#xA;&#xD;&#xA;Code below:&#xD;&#xA;&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    HIIIidx = 4&#xD;&#xA;    HII5idx = 5&#xD;&#xA;    venkatesanidx = 6&#xD;&#xA;    yuidx = 7&#xD;&#xA;    Litidx = 8&#xD;&#xA;    &#xD;&#xA;    print(sys.argv)&#xD;&#xA;    &#xD;&#xA;    if len(sys.argv) &lt;= 1:&#xD;&#xA;        print(&quot;Usage: script.py file delimeter&quot;)&#xD;&#xA;    &#xD;&#xA;    delim = ';'&#xD;&#xA;    inFile = sys.argv[1]&#xD;&#xA;    &#xD;&#xA;    if len(sys.argv) &gt; 2:&#xD;&#xA;        delim = sys.argv[2]&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    def printInteraction(syma, symb, inter):&#xD;&#xA;        print(syma + delim + inter + delim + symb)&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    with open( inFile , 'r') as file:&#xD;&#xA;    &#xD;&#xA;        file.readline()&#xD;&#xA;    &#xD;&#xA;        for line in file:&#xD;&#xA;    &#xD;&#xA;            aline = line.strip('\n').split('\t')&#xD;&#xA;    &#xD;&#xA;            if aline[HIIIidx] == '1':&#xD;&#xA;                printInteraction(aline[1], aline[3], 'HIII')&#xD;&#xA;    &#xD;&#xA;            if aline[HII5idx] == '1':&#xD;&#xA;                printInteraction(aline[1], aline[3], 'HII5')&#xD;&#xA;    &#xD;&#xA;            if aline[venkatesanidx] == '1':&#xD;&#xA;                printInteraction(aline[1], aline[3], 'VEN')&#xD;&#xA;    &#xD;&#xA;            if aline[yuidx] == '1':&#xD;&#xA;                printInteraction(aline[1], aline[3], 'YU')&#xD;&#xA;    &#xD;&#xA;            if aline[Litidx] == '1':&#xD;&#xA;                printInteraction(aline[1], aline[3], 'LIT')&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/FlSN5.png" />
  <row Id="2070" PostHistoryTypeId="2" PostId="660" RevisionGUID="44dfdf60-2edb-43bb-a07a-6a0cd7b43a5b" CreationDate="2017-06-12T13:56:06.117" UserId="824" Text="Depending on your applications and uses, you might be interested in checking out [CyVerse][1]. It is an NSF funded initiative that provides you with data storage, high performance computing resources, and easy access to commonly used tools. As far as I know, it is free to use once you have an account. I also usually encounter it being used with plant and microbial genomics, so not sure how it will work with something like human genomics projects. But might be worth checking out at least. :)&#xD;&#xA;&#xD;&#xA;More information: http://www.cyverse.org/about&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cyverse.org" />
  <row Id="2071" PostHistoryTypeId="2" PostId="661" RevisionGUID="b87b82d0-5108-499f-9994-668cfaefc832" CreationDate="2017-06-12T15:30:36.277" UserId="292" Text="I was wondering if I could use the gff parsing capability of bioawk to facilitate the parsing of gtf files, and I looked at the following help message:&#xD;&#xA;&#xD;&#xA;    $ bioawk -c help&#xD;&#xA;    bed:&#xD;&#xA;    	1:chrom 2:start 3:end 4:name 5:score 6:strand 7:thickstart 8:thickend 9:rgb 10:blockcount 11:blocksizes 12:blockstarts &#xD;&#xA;    sam:&#xD;&#xA;    	1:qname 2:flag 3:rname 4:pos 5:mapq 6:cigar 7:rnext 8:pnext 9:tlen 10:seq 11:qual &#xD;&#xA;    vcf:&#xD;&#xA;    	1:chrom 2:pos 3:id 4:ref 5:alt 6:qual 7:filter 8:info &#xD;&#xA;    gff:&#xD;&#xA;    	1:seqname 2:source 3:feature 4:start 5:end 6:score 7:filter 8:strand 9:group 10:attribute &#xD;&#xA;    fastx:&#xD;&#xA;    	1:name 2:seq 3:qual 4:comment &#xD;&#xA;&#xD;&#xA;I see that there are 10 fields defined for gff parsing.&#xD;&#xA;&#xD;&#xA;However, when I look at the pages for gtf and gff in the ensembl website ([gff/gtf](http://www.ensembl.org/info/website/upload/gff.html) and [gff3](http://www.ensembl.org/info/website/upload/gff3.html)), all have 9 fields.&#xD;&#xA;&#xD;&#xA;I'm curious about these &quot;filter&quot; and &quot;group&quot; fields. What are they meant for? Are they extracted from some of the columns mentioned in the above pages?" />
  <row Id="2072" PostHistoryTypeId="1" PostId="661" RevisionGUID="b87b82d0-5108-499f-9994-668cfaefc832" CreationDate="2017-06-12T15:30:36.277" UserId="292" Text="What kind of &quot;gff&quot; format does bioawk parse?" />
  <row Id="2073" PostHistoryTypeId="3" PostId="661" RevisionGUID="b87b82d0-5108-499f-9994-668cfaefc832" CreationDate="2017-06-12T15:30:36.277" UserId="292" Text="&lt;file-formats&gt;&lt;gff3&gt;&lt;gtf&gt;&lt;bioawk&gt;" />
  <row Id="2074" PostHistoryTypeId="2" PostId="662" RevisionGUID="e26d0a51-8c32-46c5-8227-3b8c37821541" CreationDate="2017-06-12T16:02:21.920" UserId="33" Text="Check out [Homer][1]. &quot;Software for motif discovery and next generation sequencing analysis&quot;, it's what my lab uses currently for finding eRNA motifs.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://homer.ucsd.edu/homer/" />
  <row Id="2075" PostHistoryTypeId="5" PostId="662" RevisionGUID="b4ebdbe9-1734-4694-8128-7d99284df7d1" CreationDate="2017-06-12T16:48:00.760" UserId="33" Comment="added De Novo Motif to answer" Text="Check out [HOMER][1]. &quot;Software for motif discovery and next generation sequencing analysis&quot;, it's what my lab uses currently for finding eRNA motifs.&#xD;&#xA;&#xD;&#xA;Edit: For @ShanZhengYang &quot;HOMER was designed as a de novo motif discovery algorithm...&quot; &#xD;&#xA;[HOMER De Novo Motif][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://homer.ucsd.edu/homer/&#xD;&#xA;  [2]: http://homer.ucsd.edu/homer/introduction/basics.html" />
  <row Id="2077" PostHistoryTypeId="2" PostId="663" RevisionGUID="7df24aa2-e266-4427-814a-b0ff83d5f0cb" CreationDate="2017-06-12T17:06:26.053" UserId="643" Text="I have a set of differentially methylated/expressed/whatever entities with p-values attached (example below).&#xD;&#xA;&#xD;&#xA;    entity_name    p-value    magnitude&#xD;&#xA;    entity1        0.04459    0.68&#xD;&#xA;    entity2        0.02283    0.99&#xD;&#xA;    ...&#xD;&#xA;    entity_n       0.78       0.025&#xD;&#xA;&#xD;&#xA;Typically, I apply the p.adjust function in R with the &quot;fdr&quot; (Benjamini-Hochberg) approach to leave me with p-values adjusted to control the FDR.&#xD;&#xA;&#xD;&#xA;    adjusted&lt;-p.adjust(mydata,&quot;fdr&quot;)&#xD;&#xA;&#xD;&#xA;However, I am interested in showing a volcano plot with the unadjusted p-values, and two alpha levels: 0.05 and one that corresponds to the correction. What is the best way to get this alpha? Is it appropriate to set the &quot;corrected alpha&quot; to the lowest original p-value that doesn't pass FDR correction?&#xD;&#xA;&#xD;&#xA;Thank you!&#xD;&#xA;" />
  <row Id="2078" PostHistoryTypeId="1" PostId="663" RevisionGUID="7df24aa2-e266-4427-814a-b0ff83d5f0cb" CreationDate="2017-06-12T17:06:26.053" UserId="643" Text="How to correct alpha, and not p-values themselves, for visualization purposes" />
  <row Id="2079" PostHistoryTypeId="3" PostId="663" RevisionGUID="7df24aa2-e266-4427-814a-b0ff83d5f0cb" CreationDate="2017-06-12T17:06:26.053" UserId="643" Text="&lt;r&gt;" />
  <row Id="2080" PostHistoryTypeId="2" PostId="664" RevisionGUID="5073a082-8ab4-447e-8d8c-afd269a48d86" CreationDate="2017-06-12T17:45:52.033" UserId="506" Text="From an RNA-seq experiment I have about 17000 gene ids for 2 sample conditions arranged according to their log2 fold changes when compared to a control.  I need to annotate these, but I've never done annotation before and am wondering how to do this in R?  There seems to be multiple packages available, and I'm wondering if any of them stand out as being the best?" />
  <row Id="2081" PostHistoryTypeId="1" PostId="664" RevisionGUID="5073a082-8ab4-447e-8d8c-afd269a48d86" CreationDate="2017-06-12T17:45:52.033" UserId="506" Text="How to perform functional analysis on a gene list in R?" />
  <row Id="2082" PostHistoryTypeId="3" PostId="664" RevisionGUID="5073a082-8ab4-447e-8d8c-afd269a48d86" CreationDate="2017-06-12T17:45:52.033" UserId="506" Text="&lt;r&gt;&lt;annotation&gt;" />
  <row Id="2083" PostHistoryTypeId="2" PostId="665" RevisionGUID="86b2c2dc-ee6c-49f1-aa74-c35f3953b3d4" CreationDate="2017-06-12T18:00:21.667" UserId="77" Text="I would consider the description there a bug. The `filter` is actually the strand, `strand` is the frame, `group` is the attribute, and `attribute` does nothing. These are really meant to be the 9 columns.&#xD;&#xA;&#xD;&#xA;**Edit**: There's a [bug report](https://github.com/lh3/bioawk/issues/16) related to this." />
  <row Id="2084" PostHistoryTypeId="5" PostId="665" RevisionGUID="82069b9a-d539-441b-acc8-559b1ed4f8ad" CreationDate="2017-06-12T18:06:58.950" UserId="77" Comment="added 40 characters in body" Text="I would consider the description there a bug. The `filter` is actually the strand, `strand` is the frame, `group` is the attribute, and `attribute` does nothing. These are really meant to be the 9 columns.&#xD;&#xA;&#xD;&#xA;**Edit**: There's a [bug report](https://github.com/lh3/bioawk/issues/16) related to this.&#xD;&#xA;&#xD;&#xA;**Edit 2**: I'll make a pull request" />
  <row Id="2085" PostHistoryTypeId="5" PostId="665" RevisionGUID="3c722797-e749-4036-b39c-98f3ef7cf256" CreationDate="2017-06-12T18:12:41.167" UserId="77" Comment="added 96 characters in body" Text="I would consider the description there a bug. The `filter` is actually the strand, `strand` is the frame, `group` is the attribute, and `attribute` does nothing. These are really meant to be the 9 columns.&#xD;&#xA;&#xD;&#xA;**Edit**: There's a [bug report](https://github.com/lh3/bioawk/issues/16) related to this.&#xD;&#xA;&#xD;&#xA;**Edit 2**: I've made [a pull request](https://github.com/lh3/bioawk/pull/21) to clarify this and fix the aforementioned bug report." />
  <row Id="2086" PostHistoryTypeId="2" PostId="666" RevisionGUID="29217d55-f6f7-4fbe-a192-669abf4ab525" CreationDate="2017-06-12T18:22:42.213" UserId="77" Text="The only way to get the alpha levels is to determine what they will be with `p.adjust()`, since they will depend on the distribution of your unadjusted p values. The general steps you should follow will be:&#xD;&#xA;&#xD;&#xA;1. Add a column of adjusted p-values to your dataframe (`mydata$padj = p.adjust(mydata, method=&quot;BH&quot;)`, which is the same as FDR and saves a character).&#xD;&#xA;2. Use `which` and `max` to determine your two alpha threshold (e.g., `max(mydata$pvalue[mydata$padj &lt;= 0.05])`&#xD;&#xA;&#xD;&#xA;Then you can adjust your plots however you like (presumably with some horizontal lines at the various alphas). Whether you take the smallest non-significant value or the largest significant value is up to you, just describe what &quot;dots on the line&quot; represent." />
  <row Id="2087" PostHistoryTypeId="5" PostId="641" RevisionGUID="037ba617-c3a3-4f05-830f-638ebf11ff9c" CreationDate="2017-06-12T18:31:09.837" UserId="57" Comment="deleted holy war paragraph, made clear that I talk about central repos in general" Text="**Benefits of central repository for Community**&#xD;&#xA;&#xD;&#xA;Having a central repository for packages is very useful. For couple of reasons:&#xD;&#xA;&#xD;&#xA;- It makes very easy to resolve **dependencies**. Installing all the dependencies manually would be exhausting but also dangerous (point 2).&#xD;&#xA;- Package **compatibility**! If I install package with dependencies, I would like to be sure that I install correct versions of all the dependencies.&#xD;&#xA;- **Reliability** thanks to unified and integrated testing. `Bioconductor` is trying really hard to force developers to write good test, they also have people manually testing submitted packages. They also remove packages that are not maintained. Packages in `Bioconductor` are (reasonably) reliable.&#xD;&#xA;&#xD;&#xA;In the end, installing dev versions of R packages is in my opinion **very bad practise** for reproducible science. If developers delete GitHub repo, commit hash you have used won't be enough to get the code.&#xD;&#xA;&#xD;&#xA;**Benefits of central repository for developers**&#xD;&#xA;&#xD;&#xA;I forgot about the advantages for you as developer to submit your package to `Bioconductor`:&#xD;&#xA;&#xD;&#xA;1. Your package will be more visible&#xD;&#xA;2. users will have a guarantee that your code was checked by third person&#xD;&#xA;3. Your package will be for users easier to install&#xD;&#xA;4. Your package will be forced to use standardized vignettes, version tags and tests -&gt; will be more accessible by community to build on your code&#xD;&#xA;&#xD;&#xA;**Bioconductor specific advantages over CRAN**&#xD;&#xA;&#xD;&#xA;I see the big advantage in the [community support page][1], provided by `Bioconductor`. [@Llopis' comprehensive elaboration][2]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.bioconductor.org/&#xD;&#xA;  [2]: https://bioinformatics.stackexchange.com/a/643/57" />
  <row Id="2088" PostHistoryTypeId="5" PostId="665" RevisionGUID="d3c63b14-69a3-4a3f-a7c2-99c795242884" CreationDate="2017-06-12T18:46:37.480" UserId="77" Comment="added 315 characters in body" Text="I would consider the description there a bug. The `filter` is actually the strand, `strand` is the frame, `group` is the attribute, and `attribute` does nothing. These are really meant to be the 9 columns.&#xD;&#xA;&#xD;&#xA;**Edit**: There's a [bug report](https://github.com/lh3/bioawk/issues/16) related to this.&#xD;&#xA;&#xD;&#xA;**Edit 2**: I've made [a pull request](https://github.com/lh3/bioawk/pull/21) to clarify this and fix the aforementioned bug report.&#xD;&#xA;&#xD;&#xA;**Edit 3**: I realized that I never directly answered the title of your question (*mea culpa*). bioawk itself will work with gff, gff3, or gtf files. It really is just treating them as tab-separated files with named columns (this is surprisingly convenient, since it's a PITA to remember what column does what)." />
  <row Id="2089" PostHistoryTypeId="5" PostId="664" RevisionGUID="fd31c4c2-c48a-4f1b-9d52-91afc41dd936" CreationDate="2017-06-12T19:01:53.170" UserId="77" Comment="added 69 characters in body" Text="From an RNA-seq experiment I have about 17000 gene ids for 2 sample conditions arranged according to their log2 fold changes when compared to a control.  I need to annotate these, but I've never done annotation before and am wondering how to do this in R?  There seems to be multiple packages available, and I'm wondering if any of them stand out as being the best?&#xD;&#xA;&#xD;&#xA;I'm primarily interested in human samples and annotated pathways." />
  <row Id="2090" PostHistoryTypeId="2" PostId="667" RevisionGUID="33f65000-0a76-4a7b-af52-7926a5ed9f7a" CreationDate="2017-06-12T20:17:58.167" UserId="831" Text="###Google Genomics###&#xD;&#xA;Google has an API called [Google Genomics][1].&#xD;&#xA;&#xD;&#xA;###The CyDAS Project###&#xD;&#xA;And, there's the CyDAS project which has an API that can analyze ISCN formulae. Per their [web site][2]: their API &quot;lets you analyze a Karyotype for virtually all information which can be extracted from karyotypes and the rearrangements therein: gains and losses of chromosomal material, break points, junctions...&quot; It's a **free** service, but I don't know how up to date it is.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://cloud.google.com/genomics&#xD;&#xA;  [2]: http://www.cydas.org" />
  <row Id="2091" PostHistoryTypeId="2" PostId="668" RevisionGUID="f0ffc991-2b45-42d1-9d9a-39b3ae1e194a" CreationDate="2017-06-12T20:51:34.303" UserId="73" Text="When you're doing a p-value adjustment, the same unadjusted p-value in different genes can be given different adjusted p-values depending on other factors. That means that you can't directly draw a line associated with the FDR on a plot of unadjusted p-value.&#xD;&#xA;&#xD;&#xA;One possibility would be to take a range of values that are close to the FDR threshold (e.g. the 20 values closest to threshold), and draw a p-value greyzone within that region:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/Rscript&#xD;&#xA;    values &lt;- c(rnorm(10000),rnorm(100, mean=1.5));&#xD;&#xA;    val.mean &lt;- median(values);&#xD;&#xA;    val.diffs &lt;- abs(values - median(values));&#xD;&#xA;    val.reldiffs &lt;- (values - median(values));&#xD;&#xA;    &#xD;&#xA;    val.pval &lt;- pnorm(val.diffs, mean = mean(val.diffs),&#xD;&#xA;                       sd=sd(val.diffs), lower.tail=FALSE);&#xD;&#xA;    val.padj &lt;- p.adjust(val.pvals, method=&quot;BH&quot;);&#xD;&#xA;    fdr.threshold &lt;- 0.1;&#xD;&#xA;    close.bh &lt;- order(abs(val.padj - fdr.threshold))[1:20];&#xD;&#xA;    &#xD;&#xA;    png(&quot;SE.663.png&quot;);&#xD;&#xA;    plot(val.reldiffs, -log10(val.pval),&#xD;&#xA;         col=ifelse(1:10100 &lt;= 10000,&quot;darkblue&quot;,&quot;darkgreen&quot;));&#xD;&#xA;    abline(h=-log10(0.05), col=&quot;red&quot;);&#xD;&#xA;    text(0,-log10(0.05),&quot;p=0.05&quot;, pos=1);&#xD;&#xA;    abline(h=range(-log10(val.pval[close.bh])), col=&quot;#00000040&quot;, lty=&quot;dashed&quot;);&#xD;&#xA;    rect(xleft=min(val.reldiffs)*2, xright=max(val.reldiffs)*2,&#xD;&#xA;         ytop=max(-log10(val.pval[close.bh])),&#xD;&#xA;         ybottom=min(-log10(val.pval[close.bh])), col=&quot;#00000020&quot;, border=NA);&#xD;&#xA;    text(0,min(-log10(val.pval[close.bh])),&quot;FDR=0.1&quot;, pos=1);&#xD;&#xA;    dummy &lt;- dev.off();&#xD;&#xA;&#xD;&#xA;[![Volcano plot of p-values with FDR greyzone][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/l8Ldk.png" />
  <row Id="2092" PostHistoryTypeId="2" PostId="669" RevisionGUID="da33a1d2-f0f9-4b02-af51-643833bbeaa4" CreationDate="2017-06-12T21:52:41.197" UserId="77" Text="N.B., I'm avoiding discussion of &quot;best&quot;, since that's more or less impossible to answer.&#xD;&#xA;&#xD;&#xA;Your question can actually be divided into two:&#xD;&#xA;&#xD;&#xA;1. What's a good tool for pathways analysis (ideally in R)?&#xD;&#xA;2. What are good sources of pathway information?&#xD;&#xA;&#xD;&#xA;For (1), there are a few different possibilities, but I prefer either roast or camera from limma or goseq (a stand-alone bioconductor package). Of those, I expect what roast or goseq are doing are the most similar to what panther is doing in the sense that they're not competitive. However, since genes do correlate with each other, I prefer competitive tests like that offered by camera.&#xD;&#xA;&#xD;&#xA;For (2), your options are basically [KEGG](http://bioconductor.org/packages/release/data/annotation/html/KEGG.db.html) or [reactome](http://bioconductor.org/packages/release/data/annotation/html/KEGG.db.html). KEGG is problematic, since you need a license for anything remotely recent (there are some R packages to get around this, but I haven't a clue how kosher they are legally). Given that, reactome might be your best bet.&#xD;&#xA;&#xD;&#xA;Having written all of that, the best database out there is what [IPA](https://www.qiagenbioinformatics.com/products/ingenuity-pathway-analysis/) uses. This is a commercial product (I have no affiliation), but you can get a demo license that will work for a couple weeks at least. If you're doing analyses relatively often then it makes sense to spend a bit of cash on a license (maybe share with neighboring groups).&#xD;&#xA;&#xD;&#xA;Regardless of what you use, you might also want to use [pathview](https://bioconductor.org/packages/release/bioc/html/pathview.html), which can produce easily interpretable plots related to pathway enrichment." />
  <row Id="2093" PostHistoryTypeId="5" PostId="663" RevisionGUID="1fbf8eff-fdc4-4308-8789-b56b1c1b73ed" CreationDate="2017-06-12T22:05:16.573" UserId="57" Comment="code formating, deleted thank you" Text="I have a set of differentially methylated/expressed/whatever entities with p-values attached (example below).&#xD;&#xA;&#xD;&#xA;    entity_name    p-value    magnitude&#xD;&#xA;    entity1        0.04459    0.68&#xD;&#xA;    entity2        0.02283    0.99&#xD;&#xA;    ...&#xD;&#xA;    entity_n       0.78       0.025&#xD;&#xA;&#xD;&#xA;Typically, I apply the p.adjust function in R with the &quot;fdr&quot; (Benjamini-Hochberg) approach to leave me with p-values adjusted to control the FDR.&#xD;&#xA;&#xD;&#xA;    adjusted &lt;- p.adjust(mydata,&quot;fdr&quot;)&#xD;&#xA;&#xD;&#xA;However, I am interested in showing a volcano plot with the unadjusted p-values, and two alpha levels: 0.05 and one that corresponds to the correction. What is the best way to get this alpha? Is it appropriate to set the &quot;corrected alpha&quot; to the lowest original p-value that doesn't pass FDR correction?&#xD;&#xA;" />
  <row Id="2094" PostHistoryTypeId="24" PostId="663" RevisionGUID="1fbf8eff-fdc4-4308-8789-b56b1c1b73ed" CreationDate="2017-06-12T22:05:16.573" Comment="Proposed by 57 approved by 643 edit id of 189" />
  <row Id="2095" PostHistoryTypeId="4" PostId="653" RevisionGUID="ac7a18a1-6aa2-4822-ae7e-ef3239f479ba" CreationDate="2017-06-12T22:25:17.163" UserId="734" Comment="edited title" Text="What are the standard ways to visualize protein-protein or gene-gene interactions" />
  <row Id="2096" PostHistoryTypeId="2" PostId="670" RevisionGUID="ab92f2d0-750f-4e55-be27-049af80d47d5" CreationDate="2017-06-12T22:39:40.763" UserId="823" Text="My reputation isn't high enough to make a comment, but have you considered a modelling software such as PyMol or Chimera? You can execute a command line script with softwares such as these and these have the functionality to predict H-bonds. &#xD;&#xA;&#xD;&#xA;In the PDB files there is information regarding the GenBank and UNIPROT info of the protein, and you can find more information here: &#xD;&#xA;&#xD;&#xA;http://www.wwpdb.org/documentation/file-format-content/format33/sect3.html. &#xD;&#xA;&#xD;&#xA;If you want RefSeq ID, you can use GenBank info you extract and convert by piping into a biomaRt (Bioconducor) script, although maybe there is a more direct way.&#xD;&#xA;&#xD;&#xA;As for finding which residues in the sequence are interacting, I'm fairly certain that the software packages like PyMol or Chimera will give you information about the interacting residues. &#xD;&#xA;&#xD;&#xA;From what I understand, you will want to filter out only the residues that interact and are on different chains (e.g, Chain A vs. Chain B). This process will require you to work with the syntax of the different software packages when you are scripting. For example, Chimera has a basic primer here:&#xD;&#xA;&#xD;&#xA;https://www.cgl.ucsf.edu/chimera/current/docs/ProgrammersGuide/basicPrimer.html&#xD;&#xA;&#xD;&#xA;Note that I only gave a reference to Chimera because I am more comfortable with that then other packages and there is likely a more efficient way to do this. With Chimera, you can find the position within the chain of residues participating in the interaction and then you'd store these in a file to do whatever you want later.&#xD;&#xA;&#xD;&#xA;I hope this helps." />
  <row Id="2097" PostHistoryTypeId="2" PostId="671" RevisionGUID="fef9c821-bdd1-42ae-851f-24670e593bbb" CreationDate="2017-06-13T00:28:05.247" UserId="215" Text="Late to the party here, but I like to try to avoid writing scripts when some command line magic will do. It's good practice to index your `FASTA` so use it.&#xD;&#xA;&#xD;&#xA;    samtools faidx &lt;myfasta.fa&gt;&#xD;&#xA;&#xD;&#xA;A little bit of `awk` and `sort` can determine the largest isoform of each gene (and they don't even have to be sorted by name in the source `FASTA`).&#xD;&#xA;&#xD;&#xA;    awk -F'[\t.]' '{print $1,$2,$3,$4}' &lt;myfasta.fa&gt;.fai | sort -k4nr,4 | sort -uk1,2 | cut -f1,2,3 -d' '| tr ' ' '.' &gt; selection.ls&#xD;&#xA;&#xD;&#xA;* The `-F[\t.]` specifies both the `fai` tab delimiters, and the full stops in your contig names (`Doug_NoIndex_L005_R1_001_contig_2.g7.t1`) as delimiters for `awk`.&#xD;&#xA;* For each line we print four columns. `$1` to `$3` are the three components of your contig name (after splitting on the `.`), and column $4 is the column in the index for sequence length.&#xD;&#xA;* We use sort to organise the lines by `$4` (the contig length). Reverse sort with `-r` so the longest genes appear first.&#xD;&#xA;* We then subsort by columns `$1` and `$2` (the contig and gene name (`g1`...)). `-u` then drops any repeat of column `$1` and `$2` pairs (that is, all lines after the first pair -- the longest length).&#xD;&#xA;* Finally, we reassemble `$1` to `$3` with `cut` and `tr` to give you a list of sequence names.&#xD;&#xA;&#xD;&#xA;A loop with our old friend `samtools faidx` can put your chosen sequences out to a new `FASTA`.&#xD;&#xA;&#xD;&#xA;    while read contig;&#xD;&#xA;        do samtools faidx &lt;myfasta.fa&gt; $contig &gt;&gt; selection.fa;&#xD;&#xA;    done &lt; selection.ls&#xD;&#xA;&#xD;&#xA;Don't forget to index it, they are useful!&#xD;&#xA;&#xD;&#xA;    samtools faidx selection.fa" />
  <row Id="2098" PostHistoryTypeId="5" PostId="671" RevisionGUID="569767ea-d0a6-477b-99f8-623721d05513" CreationDate="2017-06-13T00:51:03.047" UserId="215" Comment="deleted 2 characters in body" Text="Late to the party here, but I like to try to avoid writing scripts when some command line magic will do. It's good practice to index your `FASTA` so use it.&#xD;&#xA;&#xD;&#xA;    samtools faidx &lt;myfasta.fa&gt;&#xD;&#xA;&#xD;&#xA;A little bit of `awk` and `sort` can determine the largest isoform of each gene (and they don't even have to be sorted by name in the source `FASTA`).&#xD;&#xA;&#xD;&#xA;    awk -F'[\t.]' '{print $1,$2,$3,$4}' &lt;myfasta.fa&gt;.fai | sort -k4nr,4 | sort -uk1,2 | cut -f1-3 -d' '| tr ' ' '.' &gt; selection.ls&#xD;&#xA;&#xD;&#xA;* The `-F[\t.]` specifies both the `fai` tab delimiters, and the full stops in your contig names (`Doug_NoIndex_L005_R1_001_contig_2.g7.t1`) as delimiters for `awk`.&#xD;&#xA;* For each line we print four columns. `$1` to `$3` are the three components of your contig name (after splitting on the `.`), and column $4 is the column in the index for sequence length.&#xD;&#xA;* We use sort to organise the lines by `$4` (the contig length). Reverse sort with `-r` so the longest genes appear first.&#xD;&#xA;* We then subsort by columns `$1` and `$2` (the contig and gene name (`g1`...)). `-u` then drops any repeat of column `$1` and `$2` pairs (that is, all lines after the first pair -- the longest length).&#xD;&#xA;* Finally, we reassemble `$1` to `$3` with `cut` and `tr` to give you a list of sequence names.&#xD;&#xA;&#xD;&#xA;A loop with our old friend `samtools faidx` can put your chosen sequences out to a new `FASTA`.&#xD;&#xA;&#xD;&#xA;    while read contig;&#xD;&#xA;        do samtools faidx &lt;myfasta.fa&gt; $contig &gt;&gt; selection.fa;&#xD;&#xA;    done &lt; selection.ls&#xD;&#xA;&#xD;&#xA;Don't forget to index it, they are useful!&#xD;&#xA;&#xD;&#xA;    samtools faidx selection.fa" />
  <row Id="2099" PostHistoryTypeId="2" PostId="672" RevisionGUID="41a2f6d4-07f1-4bd8-a0a0-2d0a3c8627e8" CreationDate="2017-06-13T01:33:49.053" UserId="832" Text="My data is a VCF file from exome sequencing variant call. I'm not very familiar with the sequencing process and variant calling process. I noticed that there are some missing genotypes, which is recorded as &quot;./.&quot; at the GT field. From googling I learned that they're not homozygous reference genotype (&quot;0/0&quot;), but missing calls due to some sequencing ignorance.&#xD;&#xA;&#xD;&#xA;In order to make my data &quot;cleaner&quot;, I thought it's better to filter out the loci with missing calls if there're not too many of them. I also checked the corresponding &quot;DP&quot; of the loci with and without missing calls. For example:&#xD;&#xA;&#xD;&#xA;    chr1:123 GT:DP 0/0:2 1/1:4 ./.&#xD;&#xA;    chr1:234 GT:DP 0/0:10 1/1:11 1/1:20&#xD;&#xA;    chr1:345 GT:DP 0/1:40 1/1:37 0/0:78&#xD;&#xA;    chr1:456 GT:DP 0/1:7 0/0:23 ./.&#xD;&#xA;    chr1:567 GT:DP 0/1:34 1/1:39 0/0:58&#xD;&#xA;&#xD;&#xA;In the above toy example, there're 3 people and 5 loci. I checked the mean DP of all 5 loci and I found that the mean DP of loci with missing calls (1st and 4th loci) to be significantly lower than the mean DP of loci without missing calls (2nd, 3rd and 5th loci). This happens to my real data with a lot more loci and samples than this toy example. Is that a coincidence? Or are there any specific reason that the loci with missing calls have lower coverage than the &quot;normal&quot; loci? Thanks!&#xD;&#xA;" />
  <row Id="2100" PostHistoryTypeId="1" PostId="672" RevisionGUID="41a2f6d4-07f1-4bd8-a0a0-2d0a3c8627e8" CreationDate="2017-06-13T01:33:49.053" UserId="832" Text="What are the exact reason causing missing calls in a VCF file from exome sequencing?" />
  <row Id="2101" PostHistoryTypeId="3" PostId="672" RevisionGUID="41a2f6d4-07f1-4bd8-a0a0-2d0a3c8627e8" CreationDate="2017-06-13T01:33:49.053" UserId="832" Text="&lt;variant-calling&gt;&lt;vcf&gt;&lt;sequencing&gt;&lt;exome&gt;&lt;reads&gt;" />
  <row Id="2102" PostHistoryTypeId="2" PostId="673" RevisionGUID="8c734f96-2fb9-4506-b768-6245ae79d7a0" CreationDate="2017-06-13T01:38:38.563" UserId="823" Text="I have a list of transcription factors and I am interested in finding out which which genes might be transcribed as a result of the formation of transcription factor complexes with that transcription factor.&#xD;&#xA;&#xD;&#xA;Any ideas on good databases? I've seen that ENCODE hosts data from CHIPseq experiments, but I'm not sure if I can find conclusion regarding interactions from this site. I'm looking for a database with known or putative transcription/gene interactions.&#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="2103" PostHistoryTypeId="1" PostId="673" RevisionGUID="8c734f96-2fb9-4506-b768-6245ae79d7a0" CreationDate="2017-06-13T01:38:38.563" UserId="823" Text="Given a transcription factor, what genes does it regulate?" />
  <row Id="2104" PostHistoryTypeId="3" PostId="673" RevisionGUID="8c734f96-2fb9-4506-b768-6245ae79d7a0" CreationDate="2017-06-13T01:38:38.563" UserId="823" Text="&lt;chip-seq&gt;&lt;encode&gt;&lt;multi-omics&gt;" />
  <row Id="2105" PostHistoryTypeId="2" PostId="674" RevisionGUID="aabff656-bf04-4a75-80f7-458788b73a53" CreationDate="2017-06-13T01:53:41.920" UserId="35" Text="[MSigDB][1] has a collection (C3:TFT) of gene sets corresponding to transcription factor targets.&#xD;&#xA;&#xD;&#xA;[Harmonizome][2] has functional terms for genes extracted from over a hundred publicly available resources.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://software.broadinstitute.org/gsea/msigdb/collections.jsp#C3&#xD;&#xA;  [2]: http://amp.pharm.mssm.edu/Harmonizome/" />
  <row Id="2106" PostHistoryTypeId="5" PostId="672" RevisionGUID="c49811e0-3cad-4486-b9c3-a04ccb0e97d6" CreationDate="2017-06-13T02:34:51.760" UserId="73" Comment="fixed grammar, clarified title question" Text="My data is a VCF file generated from an exome sequencing variant call pipeline. I'm not very familiar with the sequencing and variant calling process. I noticed that there are some missing genotypes, which are recorded as &quot;./.&quot; at the GT field. From googling I learned that they're not homozygous reference genotype (&quot;0/0&quot;), but missing calls due to some sequencing failure.&#xD;&#xA;&#xD;&#xA;In order to make my data &quot;cleaner&quot;, I thought it would be better to filter out the loci with missing calls if there're not too many of them. I also checked the corresponding &quot;DP&quot; of the loci with and without missing calls. For example:&#xD;&#xA;&#xD;&#xA;    chr1:123 GT:DP 0/0:2 1/1:4 ./.&#xD;&#xA;    chr1:234 GT:DP 0/0:10 1/1:11 1/1:20&#xD;&#xA;    chr1:345 GT:DP 0/1:40 1/1:37 0/0:78&#xD;&#xA;    chr1:456 GT:DP 0/1:7 0/0:23 ./.&#xD;&#xA;    chr1:567 GT:DP 0/1:34 1/1:39 0/0:58&#xD;&#xA;&#xD;&#xA;In the above toy example, there are 3 people and 5 loci. I checked the mean DP of all 5 loci and I found that the mean DP of loci with missing calls (1st and 4th loci) to be significantly lower than the mean DP of loci without missing calls (2nd, 3rd and 5th loci). This happens to my real data with a lot more loci and samples than this toy example. Is that a coincidence? Or are there any specific reason that the loci with missing calls have lower coverage than the &quot;normal&quot; loci? Thanks!" />
  <row Id="2107" PostHistoryTypeId="4" PostId="672" RevisionGUID="c49811e0-3cad-4486-b9c3-a04ccb0e97d6" CreationDate="2017-06-13T02:34:51.760" UserId="73" Comment="fixed grammar, clarified title question" Text="Why are there missing calls in a VCF file from exome sequencing?" />
  <row Id="2108" PostHistoryTypeId="2" PostId="675" RevisionGUID="df1a6863-1006-4b63-b6f6-f3b71a06d50c" CreationDate="2017-06-13T02:49:37.973" UserId="73" Text="I've just been generating data like this, so can tell you about why/how missing calls are created in my dataset. There are two main reasons:&#xD;&#xA;&#xD;&#xA;1. Sequencing failure&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;When reads don't map across the variant region, then it's impossible to accurately determine a genotype for that region. This will commonly happen at the borders of the selected regions for exome sequencing, but can also happen through natural random sampling of the genome, or through repeated sequences coupled with systematic error in either the assembled genome or the sequencer. In this case, it would be expected that called variant regions including missing data would be lower coverage. It may be helpful to plot the coverage within a particular region to work out if this is likely to be happening.&#xD;&#xA;&#xD;&#xA;2. Merged non-variant data&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;Variant calling can be processed in two different modes, one which takes into account *all* samples when doing calling, and another which does the calling one sample at a time, followed by subsequent merging of the results. Multi-sample variant calling is more accurate, but computational limits (or experimental design) might mean that it is more appropriate for samples to be called one-by-one. It's fairly common (for the purpose of preserving space and computational popwer) for calling algorithms to only report variant information so if a single sample is identical to the reference, and that sample is the only one being called, then the variant and coverage information for that sample will be lost. When samples are merged after being separately called, the declared call for the &quot;same as reference&quot; samples will be set to missing. In this case, it would be generally expected that the other called samples would have high-coverage within the variant region." />
  <row Id="2109" PostHistoryTypeId="2" PostId="676" RevisionGUID="b58d0dae-1818-4b54-a6e8-0faff07eed3e" CreationDate="2017-06-13T02:51:01.843" UserId="823" Text="Are there any advantages to learning Biopython instead of learning Bioperl? Ideally, we would learn both, but for someone starting out in bioinformatics or computational biology, which problems cater better to Biopython or Bioperl?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2110" PostHistoryTypeId="1" PostId="676" RevisionGUID="b58d0dae-1818-4b54-a6e8-0faff07eed3e" CreationDate="2017-06-13T02:51:01.843" UserId="823" Text="Biopython vs Bioperl" />
  <row Id="2111" PostHistoryTypeId="3" PostId="676" RevisionGUID="b58d0dae-1818-4b54-a6e8-0faff07eed3e" CreationDate="2017-06-13T02:51:01.843" UserId="823" Text="&lt;biopython&gt;&lt;language-perl&gt;" />
  <row Id="2112" PostHistoryTypeId="5" PostId="646" RevisionGUID="cef3aa00-9638-4ba3-a66f-353a33ecfd1a" CreationDate="2017-06-13T02:55:06.973" UserId="73" Comment="broke the wall of text up into paragraphs" Text="I have two kinds of interactions: transient and stable. We are supposed to work on stable interaction, like interactions between two monomers in a heterodimer.&#xD;&#xA;&#xD;&#xA;In a heterodimer there are two chains in which there are some residues between monomers which take part in the interaction of monomers and building the structure of heterodimer, and there are other residues interacting inside each monomer for building the structure of that monomer. There are some PDB IDs related to heterodimers in PDB.&#xD;&#xA;&#xD;&#xA;Now imagine that we are trying to show which residues are physically interacting between two monomers in a heterodimer, and then we are trying two show the site of these physically interacting residues in the sequence related to that special structure. For that, we need to find the sequence related to that structure and we can download it from PDB.&#xD;&#xA;&#xD;&#xA;But if we want to make a multiple sequence alignment of that sequence, first we need to find the refseq sequence of that and then we must blast the sequence and find homologues of that sequence.&#xD;&#xA;&#xD;&#xA;Here there are some challenges:&#xD;&#xA;&#xD;&#xA;1. we must find residues interacting in the structure of heterodimer between two monomers.&#xD;&#xA;2. we must find the sequences of monomer chains of the heterodimer and map the interacting residues in the structure to the sequence.&#xD;&#xA;3. we know that when a structure is solved and its monomers are sequenced, may be they are not able to completely sequence the heterodimer and residue numbers are maybe different from the related refseq sequence. &#xD;&#xA;&#xD;&#xA;I would like to know how can we find the related refseq sequence (or sequences, in the case of heterodimer) and then how can we map the physically interacting residues in the structure of PDB ID to refseq sequence (finding the sites of these interacting residues in that refseq sequence) regarding three challenges mentioned above.&#xD;&#xA;&#xD;&#xA;Imagine that I have 100 PDB IDs of 100 nonredundant heterodimers and I would like to find and download the structures, sequence and finally refseq sequence. What should I do? " />
  <row Id="2113" PostHistoryTypeId="4" PostId="646" RevisionGUID="cef3aa00-9638-4ba3-a66f-353a33ecfd1a" CreationDate="2017-06-13T02:55:06.973" UserId="73" Comment="broke the wall of text up into paragraphs" Text="Getting structure and sequence related to PDB IDs" />
  <row Id="2114" PostHistoryTypeId="2" PostId="677" RevisionGUID="be93c599-ef9f-42df-88df-59d862b90b9e" CreationDate="2017-06-13T03:03:52.503" UserId="833" Text="for my undergrad research I'm looking for a database that gives the bound form of a particular protein structure.Is there any database that provide us with such data? So far I've found following proteins related to my work Ascorbate peroxidase (APX), beta-glucosidase, calcineurin b-like protein-interacting protein kinase. I've searched in PDB and UNIPROT. " />
  <row Id="2115" PostHistoryTypeId="1" PostId="677" RevisionGUID="be93c599-ef9f-42df-88df-59d862b90b9e" CreationDate="2017-06-13T03:03:52.503" UserId="833" Text="how to find the bound form of an enzyme structure?" />
  <row Id="2116" PostHistoryTypeId="3" PostId="677" RevisionGUID="be93c599-ef9f-42df-88df-59d862b90b9e" CreationDate="2017-06-13T03:03:52.503" UserId="833" Text="&lt;database&gt;&lt;protein-structure&gt;" />
  <row Id="2117" PostHistoryTypeId="5" PostId="656" RevisionGUID="10a03084-3f1e-4c38-8538-b97917512b0d" CreationDate="2017-06-13T03:05:36.313" UserId="73" Comment="added 560 characters in body" Text="1. Adapter Trimming&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;One of the first things I do after encountering a set of reads is to remove the adapter sequences from the start and end of reads. Most basecalling software includes some amount of built-in adapter trimming, but it is almost always the case that some adapter sequence will remain. Removing adapters is helpful for mapping because it reduces the effort the mapper needs to go through to make a match (which may help some borderline reads to be mapped properly).&#xD;&#xA;&#xD;&#xA;My preferred adapter trimmer is [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic). It has the ability to search for palindromic adapter read through, and includes an adaptive sliding window option. Trimmomatic is threaded, and seems to work reasonably fast when run through files one at a time in threaded mode. Here's an example script that I might run to trim reads from some samples. This puts trimmed FASTQ files into a `trimmed` subdirectory:&#xD;&#xA;&#xD;&#xA;    TRIMPATH=/data/all/programs/trimmomatic/Trimmomatic-0.36;&#xD;&#xA;    JARPATH=${TRIMPATH}/trimmomatic-0.36.jar;&#xD;&#xA;    ADAPTfile=${TRIMPATH}/adapters/TruSeq3-PE-2.fa;&#xD;&#xA;    CMDS=&quot; ILLUMINACLIP:${ADAPTfile}:2:30:10:5:true LEADING:3 TRAILING:3  SLIDINGWINDOW:10:20 MINLEN:40 &quot;;&#xD;&#xA;&#xD;&#xA;    mkdir -p trimmed;&#xD;&#xA;    &#xD;&#xA;    for INFILE1 in *_1.fastq.gz;&#xD;&#xA;      do base=$(basename INFILE1 _1.fastq.gz);&#xD;&#xA;      do echo ${base};&#xD;&#xA;      INFILE2=${base}_2.fastq.gz;&#xD;&#xA;      OUTFILE_P1=trimmed/${base}_P1.fastq.gz;&#xD;&#xA;      OUTFILE_P2=trimmed/${base}_P2.fastq.gz;&#xD;&#xA;      OUTFILE_U1=trimmed/${base}_U1.fastq.gz;&#xD;&#xA;      OUTFILE_U2=trimmed/${base}_U2.fastq.gz;&#xD;&#xA;      java -jar ${JARPATH} PE -threads 20 -phred33 \&#xD;&#xA;            ${INFILE1}  ${INFILE2} \&#xD;&#xA;            ${OUTFILE_P1}  ${OUTFILE_U1}  ${OUTFILE_P2}  ${OUTFILE_U2} \&#xD;&#xA;            ${CMDS};&#xD;&#xA;    done;&#xD;&#xA;&#xD;&#xA;Trimmomatic is somewhat sensitive to bad input data, and will loudly fail (i.e. stop running) if it sees quality strings of different length to sequence strings:&#xD;&#xA;&#xD;&#xA;    Exception in thread &quot;Thread-1&quot; java.lang.RuntimeException: Sequence and quality length don't match: 'TACATGGCCCTGAAATGACTTTCACCCAGGCAACCAGTGCCCCCTGTATAGACACATGCCTTGGGCGCTCCCCACCCTTCCTCGCGTGGCCACACCTCTGT' vs '-AAFFJFJJ7A-FJJJFJFJFJJFFA-FFAF-&lt;A-&lt;FFFJA-&lt;-A7-F&lt;&lt;FFJJAJJJJJJJJJ--&lt;--7-7AJ7&lt;AAJA--J7&lt;ACTGCTGTGGGGCACCCAGCCCCCCAGATAGCCTGGCAGAAGGATGGGGGCACAGACTTCCCAGCTGCACGGGAGAGAC'&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqRecord.&lt;init&gt;(FastqRecord.java:25)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:89)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)&#xD;&#xA;            at org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)&#xD;&#xA;            at java.lang.Thread.run(Thread.java:745)&#xD;&#xA;&#xD;&#xA;2. Read Mapping&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;Read mapping finds the most likely location for a read within a target genome.&#xD;&#xA;There are extremely fast approximate mappers available, but these don't yet work for variant calling, and an exact mapping approach is necessary. My current preferred mapper is [HISAT2](https://ccb.jhu.edu/software/hisat2/index.shtml). I use this instead of BWA due to the double-read issue, and because of the local variant-aware mapping. HISAT2 is made by the same computing group as Bowtie2 and Tophat2 (JHUCCB), and is their recommended tool for replacing those other programs.&#xD;&#xA;&#xD;&#xA;&gt; HISAT2 is a successor to both HISAT and TopHat2. We recommend that HISAT and TopHat2 users switch to HISAT2.&#xD;&#xA;&#xD;&#xA;The HISAT2 page includes a link to a [genomic index file](ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38_snp.tar.gz) for the human genome, including SNP variants, which I use when mapping human reads. HISAT2 is also threaded, so I run it on the files one at a time and pipe through samtools to create a sorted BAM file:&#xD;&#xA;&#xD;&#xA;    mkdir mapped&#xD;&#xA;    for r1 in trimmed/*_P1.fastq.gz | perl -pe 's/_P1.fastq.gz//');&#xD;&#xA;      do base=basename ${x} _P1.fastq.gz;&#xD;&#xA;      r2=trimmed/${base}_P2.fastq.gz;&#xD;&#xA;      echo ${base};&#xD;&#xA;      hisat2 -p 20 -t -x /data/all/genomes/hsap/hisat_grch38_snp/genome_snp -1 \&#xD;&#xA;        ${r1} -2 ${r2} 2&gt;mapped/hisat2_${y}_vs_grch38_stderr.txt | \&#xD;&#xA;        samtools sort &gt; mapped/hisat2_${y}_vs_grch38.bam;&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;3. Variant Calling&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;I currently use samtools/bcftools to do this, but would be interested in other opinions as we recently had a grant reviewer response that samtools was a dinosaur program and better approaches were available. Samtools doesn't currently work with threads for variant calling, so I save the commands to a text file and then run them through [GNU Parallel](https://www.gnu.org/software/parallel/). This requires the genome files to be first downloaded and indexed via `samtools faidx`. This produces a set of gzipped VCF files in the `variants` directory:&#xD;&#xA;&#xD;&#xA;    mkdir -p variants;&#xD;&#xA;    (for x in mapped/*.bam;&#xD;&#xA;       do echo samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa ${x} \| \&#xD;&#xA;       bcftools call --ploidy GRCh38 -v -m -O z -o variants/$(basename ${x} .bam).vcf.gz; done) &gt; call_jobs.txt&#xD;&#xA;    cat call_jobs.txt | parallel -u;&#xD;&#xA;&#xD;&#xA;As a slow, but more accurate alternative, variants can be called for all samples at the same time:&#xD;&#xA;&#xD;&#xA;    samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa mapped/*.bam | \&#xD;&#xA;      bcftools call --ploidy GRCh38 -v -m -O z -o variants/hisat2_allCalled_vs_grch38.vcf.gz&#xD;&#xA;&#xD;&#xA;If a faster processing is desired, then the variant calling can be parallelised across chromosomes using `parallel` and merged afterwards using `bcftools norm`. The implementation of this is left as an exercise to the reader.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2118" PostHistoryTypeId="2" PostId="678" RevisionGUID="72423c3d-a951-44f0-8d2e-26d9dd5a623c" CreationDate="2017-06-13T03:48:15.283" UserId="156" Text="I have a BAM file, and I have a read ID. What is the simplest way to get mapping statistics of that read in human-readable format?&#xD;&#xA;&#xD;&#xA;E.g. I might want: % identity of aligned bases; number of insertions and deletions; number of bases aligned to the reference; etc. &#xD;&#xA;&#xD;&#xA;Extra points for one-liners that give detailed summary statistics, and extra-extra points if you can extend the answer to output summaries for all reads longer than some predetermined length." />
  <row Id="2119" PostHistoryTypeId="1" PostId="678" RevisionGUID="72423c3d-a951-44f0-8d2e-26d9dd5a623c" CreationDate="2017-06-13T03:48:15.283" UserId="156" Text="Get the mapping statistics of a single read from a BAM file" />
  <row Id="2120" PostHistoryTypeId="3" PostId="678" RevisionGUID="72423c3d-a951-44f0-8d2e-26d9dd5a623c" CreationDate="2017-06-13T03:48:15.283" UserId="156" Text="&lt;bam&gt;" />
  <row Id="2121" PostHistoryTypeId="5" PostId="677" RevisionGUID="a6ac1a54-ec3a-4fba-80d9-5c8c2094026c" CreationDate="2017-06-13T04:08:37.613" UserId="175" Comment="Improved formatting and spelling" Text="For my undergraduate research I'm looking for a database that gives the bound form of a particular protein structure. Is there any database that provide us with such data? So far I've found following proteins related to my work &#xD;&#xA;&#xD;&#xA; - Ascorbate peroxidase (APX)&#xD;&#xA; - Beta-glucosidase&#xD;&#xA; - Calcineurin b-like protein-interacting protein kinase.&#xD;&#xA;&#xD;&#xA;I've searched in PDB and UNIPROT. " />
  <row Id="2122" PostHistoryTypeId="24" PostId="677" RevisionGUID="a6ac1a54-ec3a-4fba-80d9-5c8c2094026c" CreationDate="2017-06-13T04:08:37.613" Comment="Proposed by 175 approved by 37, 833 edit id of 190" />
  <row Id="2123" PostHistoryTypeId="2" PostId="679" RevisionGUID="ba736e5e-5290-45b1-b102-995b3924101f" CreationDate="2017-06-13T04:45:30.723" UserId="73" Text="Are you aware of [BRENDA](http://www.brenda-enzymes.org/index.php)? I was just introduced to it today for a completely separate reason (looking at carbohydrate enzyme families in the *Nippostrongylus brasiliensis* proteome), and it seems to be a fairly comprehensive database. There is at least a literature link there for the [ascorbate-complexed crystalisation of Ascorbate peroxidase](http://www.brenda-enzymes.org/enzyme.php?ecno=1.11.1.11#Crystallization/COMMENTARY) on that site. Following through the paper, I see that [the complex has been uploaded to PDB](http://www.rcsb.org/pdb/explore/explore.do?pdbId=1OAF)." />
  <row Id="2124" PostHistoryTypeId="2" PostId="680" RevisionGUID="36b65dff-9f7a-41be-9bfe-6422f8c3339f" CreationDate="2017-06-13T05:32:15.000" UserId="73" Text="`samtools stats` seems to be able to do most of this, excluding the CIGAR-string parsing stuff (i.e. INDELs):&#xD;&#xA;&#xD;&#xA;    $ samtools view -h mapped.bam | grep -e '^@' -e 'readName' | \&#xD;&#xA;        samtools stats | grep '^SN' | cut -f 2-&#xD;&#xA;    &#xD;&#xA;    raw total sequences:	2&#xD;&#xA;    filtered sequences:	0&#xD;&#xA;    sequences:	2&#xD;&#xA;    is sorted:	1&#xD;&#xA;    1st fragments:	2&#xD;&#xA;    last fragments:	0&#xD;&#xA;    reads mapped:	2&#xD;&#xA;    reads mapped and paired:	0	# paired-end technology bit set + both mates mapped&#xD;&#xA;    reads unmapped:	0&#xD;&#xA;    reads properly paired:	0	# proper-pair bit set&#xD;&#xA;    reads paired:	0	# paired-end technology bit set&#xD;&#xA;    reads duplicated:	0	# PCR or optical duplicate bit set&#xD;&#xA;    reads MQ0:	0	# mapped and MQ=0&#xD;&#xA;    reads QC failed:	0&#xD;&#xA;    non-primary alignments:	0&#xD;&#xA;    total length:	13632	# ignores clipping&#xD;&#xA;    bases mapped:	13632	# ignores clipping&#xD;&#xA;    bases mapped (cigar):	13502	# more accurate&#xD;&#xA;    bases trimmed:	0&#xD;&#xA;    bases duplicated:	0&#xD;&#xA;    mismatches:	3670	# from NM fields&#xD;&#xA;    error rate:	2.718116e-01	# mismatches / bases mapped (cigar)&#xD;&#xA;    average length:	6816&#xD;&#xA;    maximum length:	6816&#xD;&#xA;    average quality:	8.9&#xD;&#xA;    insert size average:	0.0&#xD;&#xA;    insert size standard deviation:	0.0&#xD;&#xA;    inward oriented pairs:	0&#xD;&#xA;    outward oriented pairs:	0&#xD;&#xA;    pairs with other orientation:	0&#xD;&#xA;    pairs on different chromosomes:	0&#xD;&#xA;&#xD;&#xA;For a length filter, an awk length check can be slotted in instead of the grep:&#xD;&#xA;&#xD;&#xA;    $ samtools view -h mapped.bam | \&#xD;&#xA;        awk -F'\t' '{if((/^@/) || (length($10)&gt;500)){print $0}}' | \&#xD;&#xA;        samtools stats | grep '^SN' | cut -f 2-&#xD;&#xA;&#xD;&#xA;    raw total sequences:	131&#xD;&#xA;    filtered sequences:	0&#xD;&#xA;    sequences:	131&#xD;&#xA;    is sorted:	1&#xD;&#xA;    1st fragments:	131&#xD;&#xA;    last fragments:	0&#xD;&#xA;    reads mapped:	131&#xD;&#xA;    reads mapped and paired:	0	# paired-end technology bit set + both mates mapped&#xD;&#xA;    reads unmapped:	0&#xD;&#xA;    reads properly paired:	0	# proper-pair bit set&#xD;&#xA;    reads paired:	0	# paired-end technology bit set&#xD;&#xA;    reads duplicated:	0	# PCR or optical duplicate bit set&#xD;&#xA;    reads MQ0:	0	# mapped and MQ=0&#xD;&#xA;    reads QC failed:	0&#xD;&#xA;    non-primary alignments:	0&#xD;&#xA;    total length:	569425	# ignores clipping&#xD;&#xA;    bases mapped:	569425	# ignores clipping&#xD;&#xA;    bases mapped (cigar):	453884	# more accurate&#xD;&#xA;    bases trimmed:	0&#xD;&#xA;    bases duplicated:	0&#xD;&#xA;    mismatches:	113755	# from NM fields&#xD;&#xA;    error rate:	2.506257e-01	# mismatches / bases mapped (cigar)&#xD;&#xA;    average length:	4346&#xD;&#xA;    maximum length:	16909&#xD;&#xA;    average quality:	9.1&#xD;&#xA;    insert size average:	0.0&#xD;&#xA;    insert size standard deviation:	0.0&#xD;&#xA;    inward oriented pairs:	0&#xD;&#xA;    outward oriented pairs:	0&#xD;&#xA;    pairs with other orientation:	0&#xD;&#xA;    pairs on different chromosomes:	0&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2125" PostHistoryTypeId="5" PostId="665" RevisionGUID="753da71f-0b0e-4be4-996a-4ed81264ac36" CreationDate="2017-06-13T07:02:39.247" UserId="77" Comment="added 105 characters in body" Text="I would consider the description there a bug. The `filter` is actually the strand, `strand` is the frame, `group` is the attribute, and `attribute` does nothing. These are really meant to be the 9 columns.&#xD;&#xA;&#xD;&#xA;**Edit**: There's a [bug report](https://github.com/lh3/bioawk/issues/16) related to this.&#xD;&#xA;&#xD;&#xA;**Edit 2**: I've made [a pull request](https://github.com/lh3/bioawk/pull/21) to clarify this and fix the aforementioned bug report.&#xD;&#xA;&#xD;&#xA;**Edit 3**: I realized that I never directly answered the title of your question (*mea culpa*). bioawk itself will work with gff, gff3, or gtf files. It really is just treating them as tab-separated files with named columns (this is surprisingly convenient, since it's a PITA to remember what column does what).&#xD;&#xA;&#xD;&#xA;**Edit 4**: The PR has been merged. If you install from github then you'll see corrected field names." />
  <row Id="2126" PostHistoryTypeId="2" PostId="681" RevisionGUID="4a81d74f-340e-42f6-a36a-135573997675" CreationDate="2017-06-13T08:01:30.593" UserId="601" Text="Missing variant calls due to lack of coverage shouldn't happen in the targeted capture region and I'd think most of these would come from off-target regions where some samples had reads mapped. I'd filter out the VCF to only include on-target loci before proceeding with further analyses.&#xD;&#xA;&#xD;&#xA;If you did your variant calling on samples separately and then merged them together, you'll have missing calls at loci called variant in one sample and homozygous reference in the other.&#xD;&#xA;&#xD;&#xA;You can get over this by asking the variant caller to emit all sites within the target region, which would lead to a large VCF or do multi-sample calling, which is more computationally intensive. &#xD;&#xA;&#xD;&#xA;One other way is to do variant calling on all samples separately, generating a BED file of all variant regions in the cohort and then asking the variant caller to genotype those loci across all samples before merging them together." />
  <row Id="2127" PostHistoryTypeId="2" PostId="682" RevisionGUID="c26d1899-ac6a-4a8b-b7ea-94e7cd85fff3" CreationDate="2017-06-13T08:12:58.063" UserId="668" Text="Regading the perl vs python discussion, there is no final answer which language is better. But I have some advise for you:&#xD;&#xA;&#xD;&#xA;**Learn the one your colleagues or your advisor use**. This way, you are able to discuss your code with them and also get help if you run into problems." />
  <row Id="2128" PostHistoryTypeId="2" PostId="683" RevisionGUID="63727ffb-6192-4082-8315-20fdf4178c93" CreationDate="2017-06-13T08:44:30.520" UserId="377" Text="This usually comes down to religious issues, so let me try and steer it back to more objective grounds:&#xD;&#xA;&#xD;&#xA; - What language do you know (better)? Use the library for that one.&#xD;&#xA; - If you know neither and will be learning a language to use the library, the majority opinion would be that Python is easier to learn.&#xD;&#xA; - However, some people say that they &quot;click&quot; with Perl better&#xD;&#xA; - Python is what the majority of bioinformaticians are using at the moment (about 60%). And there is a virtue and aid in using what everyone else is.&#xD;&#xA; - What are your colleagues / collaborators going to be using?&#xD;&#xA; - Broadly, newer libraries tend to be written in Python.&#xD;&#xA; - Conversely, many sing the praises of regular expressions and string handling in Perl, citing speed and ease of use.&#xD;&#xA; - I think the numerics support in Python may be markedly better than that in Perl.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2129" PostHistoryTypeId="2" PostId="684" RevisionGUID="11da20de-b508-4330-9cf7-4b3c59790426" CreationDate="2017-06-13T09:06:00.697" UserId="194" Text="[iRegulon](http://iregulon.aertslab.org) takes a sequence-based approach to finding transcription factor targets. There's a Cytoscape app that you can use to find the regulators of a given gene list, or the targets of a particular transcription factor.&#xD;&#xA;&#xD;&#xA;Transcription factor binding sites are predicted using a collection of position weight matrices (PWMs) from [a number of sources](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003731#pcbi-1003731-t001), including Jaspar and TRANSFAC.&#xD;&#xA;&#xD;&#xA;iRegulon: From a Gene List to a Gene Regulatory Network Using Large Motif and Track Collections PLoS Comput Biol. 2014 Jul 24;10(7):e1003731. doi: [10.1371/journal.pcbi.1003731](http://dx,doi.org/10.1371/journal.pcbi.1003731). eCollection 2014 Jul." />
  <row Id="2130" PostHistoryTypeId="2" PostId="685" RevisionGUID="b206b430-7698-4f5e-ae05-f3e78e3936e9" CreationDate="2017-06-13T09:25:06.267" UserId="69" Text="In general, there is two options to identify targets for transcription factors: experimental (ChIP-seq) and sequence-based predictions.&#xD;&#xA;&#xD;&#xA;### TF binding from experimental data&#xD;&#xA;&#xD;&#xA;The are multiple projects that produce binding data of transcription factors and quantify their peaks across the genome. The advantage here is that you know that binding actually occurs, as opposed to motif predictions. But you need the corresponding experiments.&#xD;&#xA;&#xD;&#xA;You already mentioned [ENCODE](https://www.encodeproject.org/), which is probably the biggest producer of such data. NIH's [Roadmap Epigenomics](https://www.ncbi.nlm.nih.gov/pubmed/20709693) is another one, but that focusses less on TFs.&#xD;&#xA;&#xD;&#xA;If you have a gene list and would like to know which transcription factor was likely involved, you can do an enrichment test of your gene set in known TF targets. The [ChEA (ChIP Enrichment Analysis) database](https://www.ncbi.nlm.nih.gov/pubmed/20709693) does this.&#xD;&#xA;&#xD;&#xA;### Prediction using motifs&#xD;&#xA;&#xD;&#xA;Another possibility is to look at [binding motifs](https://en.wikipedia.org/wiki/Sequence_motif) and see whether your gene list is enriched in those. These will, however, be inactive in a given tissue or cell type if the chromatin is packed or the methylation state of the promotor is unfavorable.&#xD;&#xA;&#xD;&#xA;Examples for those are the [JASPAR](http://jaspar.genereg.net/) and [TRANSFAC](https://en.wikipedia.org/wiki/TRANSFAC) databases, or the [MSigDB motif gene set](http://software.broadinstitute.org/gsea/msigdb/collections.jsp#C3) (as burger mentioned). You can also query those features using the Ensembl genome database ([BioMart](http://www.ensembl.org/biomart/martview/c48f212aea63b05694341e6ead06d756), [REST](https://rest.ensembl.org/)).&#xD;&#xA;&#xD;&#xA;### Calculating enrichment in gene sets&#xD;&#xA;&#xD;&#xA;Most likely, an analysis you will want to perform is to calculate the enrichment in your gene list, e.g. that you got from differential expression in two conditions.&#xD;&#xA;&#xD;&#xA;The most convenient way is to use the [Enrichr platform](http://amp.pharm.mssm.edu/Enrichr/), which is a web page that accepts a gene list and will compute enrichment in ChEA, JASPAR, TRANSFAC, etc. You can also [download their gene sets](http://amp.pharm.mssm.edu/Enrichr/#stats)." />
  <row Id="2131" PostHistoryTypeId="2" PostId="686" RevisionGUID="1df1fd4c-95c3-4b94-a229-befe28ead781" CreationDate="2017-06-13T09:49:44.683" UserId="383" Text="Currently you could use either but a major question is which platform will others be using in the future. AFAIK Perl is only superior to Python for regex.&#xD;&#xA;&#xD;&#xA;Based on the trend I see for new programmers and new software being released: Perl is on the way out and Python is still growing.&#xD;&#xA;&#xD;&#xA;https://trends.google.com/trends/explore?date=all&amp;q=bioperl,biopython&#xD;&#xA;" />
  <row Id="2132" PostHistoryTypeId="5" PostId="656" RevisionGUID="01a02784-dad4-4596-8952-a060f114ef7c" CreationDate="2017-06-13T10:01:33.920" UserId="298" Comment="Fixed syntax errors" Text="1. Adapter Trimming&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;One of the first things I do after encountering a set of reads is to remove the adapter sequences from the start and end of reads. Most basecalling software includes some amount of built-in adapter trimming, but it is almost always the case that some adapter sequence will remain. Removing adapters is helpful for mapping because it reduces the effort the mapper needs to go through to make a match (which may help some borderline reads to be mapped properly).&#xD;&#xA;&#xD;&#xA;My preferred adapter trimmer is [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic). It has the ability to search for palindromic adapter read through, and includes an adaptive sliding window option. Trimmomatic is threaded, and seems to work reasonably fast when run through files one at a time in threaded mode. Here's an example script that I might run to trim reads from some samples. This puts trimmed FASTQ files into a `trimmed` subdirectory:&#xD;&#xA;&#xD;&#xA;    TRIMPATH=/data/all/programs/trimmomatic/Trimmomatic-0.36;&#xD;&#xA;    JARPATH=${TRIMPATH}/trimmomatic-0.36.jar;&#xD;&#xA;    ADAPTfile=${TRIMPATH}/adapters/TruSeq3-PE-2.fa;&#xD;&#xA;    CMDS=&quot; ILLUMINACLIP:${ADAPTfile}:2:30:10:5:true LEADING:3 TRAILING:3  SLIDINGWINDOW:10:20 MINLEN:40 &quot;;&#xD;&#xA;&#xD;&#xA;    mkdir -p trimmed;&#xD;&#xA;    &#xD;&#xA;    for INFILE1 in *_1.fastq.gz; do&#xD;&#xA;      base=$(basename &quot;$INFILE1&quot; _1.fastq.gz);&#xD;&#xA;      echo ${base};&#xD;&#xA;      INFILE2=&quot;${base}&quot;_2.fastq.gz;&#xD;&#xA;      OUTFILE_P1=trimmed/&quot;${base}&quot;_P1.fastq.gz;&#xD;&#xA;      OUTFILE_P2=trimmed/&quot;${base}&quot;_P2.fastq.gz;&#xD;&#xA;      OUTFILE_U1=trimmed/&quot;${base}&quot;_U1.fastq.gz;&#xD;&#xA;      OUTFILE_U2=trimmed/&quot;${base}&quot;_U2.fastq.gz;&#xD;&#xA;      java -jar &quot;${JARPATH}&quot; PE -threads 20 -phred33 \&#xD;&#xA;            &quot;${INFILE1}&quot;  &quot;${INFILE2}&quot; \&#xD;&#xA;            &quot;${OUTFILE_P1}&quot; &quot;${OUTFILE_U1}&quot; &quot;${OUTFILE_P2}&quot;  &quot;${OUTFILE_U2}&quot; \&#xD;&#xA;            &quot;${CMDS}&quot;;&#xD;&#xA;    done;&#xD;&#xA;&#xD;&#xA;Trimmomatic is somewhat sensitive to bad input data, and will loudly fail (i.e. stop running) if it sees quality strings of different length to sequence strings:&#xD;&#xA;&#xD;&#xA;    Exception in thread &quot;Thread-1&quot; java.lang.RuntimeException: Sequence and quality length don't match: 'TACATGGCCCTGAAATGACTTTCACCCAGGCAACCAGTGCCCCCTGTATAGACACATGCCTTGGGCGCTCCCCACCCTTCCTCGCGTGGCCACACCTCTGT' vs '-AAFFJFJJ7A-FJJJFJFJFJJFFA-FFAF-&lt;A-&lt;FFFJA-&lt;-A7-F&lt;&lt;FFJJAJJJJJJJJJ--&lt;--7-7AJ7&lt;AAJA--J7&lt;ACTGCTGTGGGGCACCCAGCCCCCCAGATAGCCTGGCAGAAGGATGGGGGCACAGACTTCCCAGCTGCACGGGAGAGAC'&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqRecord.&lt;init&gt;(FastqRecord.java:25)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:89)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)&#xD;&#xA;            at org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)&#xD;&#xA;            at java.lang.Thread.run(Thread.java:745)&#xD;&#xA;&#xD;&#xA;2. Read Mapping&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;Read mapping finds the most likely location for a read within a target genome.&#xD;&#xA;There are extremely fast approximate mappers available, but these don't yet work for variant calling, and an exact mapping approach is necessary. My current preferred mapper is [HISAT2](https://ccb.jhu.edu/software/hisat2/index.shtml). I use this instead of BWA due to the double-read issue, and because of the local variant-aware mapping. HISAT2 is made by the same computing group as Bowtie2 and Tophat2 (JHUCCB), and is their recommended tool for replacing those other programs.&#xD;&#xA;&#xD;&#xA;&gt; HISAT2 is a successor to both HISAT and TopHat2. We recommend that HISAT and TopHat2 users switch to HISAT2.&#xD;&#xA;&#xD;&#xA;The HISAT2 page includes a link to a [genomic index file](ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38_snp.tar.gz) for the human genome, including SNP variants, which I use when mapping human reads. HISAT2 is also threaded, so I run it on the files one at a time and pipe through samtools to create a sorted BAM file:&#xD;&#xA;&#xD;&#xA;    mkdir mapped&#xD;&#xA;    for r1 in trimmed/*_P1.fastq.gz | perl -pe 's/_P1.fastq.gz//');&#xD;&#xA;      do base=basename ${x} _P1.fastq.gz;&#xD;&#xA;      r2=trimmed/${base}_P2.fastq.gz;&#xD;&#xA;      echo ${base};&#xD;&#xA;      hisat2 -p 20 -t -x /data/all/genomes/hsap/hisat_grch38_snp/genome_snp -1 \&#xD;&#xA;        ${r1} -2 ${r2} 2&gt;mapped/hisat2_${y}_vs_grch38_stderr.txt | \&#xD;&#xA;        samtools sort &gt; mapped/hisat2_${y}_vs_grch38.bam;&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;3. Variant Calling&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;I currently use samtools/bcftools to do this, but would be interested in other opinions as we recently had a grant reviewer response that samtools was a dinosaur program and better approaches were available. Samtools doesn't currently work with threads for variant calling, so I save the commands to a text file and then run them through [GNU Parallel](https://www.gnu.org/software/parallel/). This requires the genome files to be first downloaded and indexed via `samtools faidx`. This produces a set of gzipped VCF files in the `variants` directory:&#xD;&#xA;&#xD;&#xA;    mkdir -p variants;&#xD;&#xA;    (for x in mapped/*.bam;&#xD;&#xA;       do echo samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa ${x} \| \&#xD;&#xA;       bcftools call --ploidy GRCh38 -v -m -O z -o variants/$(basename ${x} .bam).vcf.gz; done) &gt; call_jobs.txt&#xD;&#xA;    cat call_jobs.txt | parallel -u;&#xD;&#xA;&#xD;&#xA;As a slow, but more accurate alternative, variants can be called for all samples at the same time:&#xD;&#xA;&#xD;&#xA;    samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa mapped/*.bam | \&#xD;&#xA;      bcftools call --ploidy GRCh38 -v -m -O z -o variants/hisat2_allCalled_vs_grch38.vcf.gz&#xD;&#xA;&#xD;&#xA;If a faster processing is desired, then the variant calling can be parallelised across chromosomes using `parallel` and merged afterwards using `bcftools norm`. The implementation of this is left as an exercise to the reader.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2133" PostHistoryTypeId="24" PostId="656" RevisionGUID="01a02784-dad4-4596-8952-a060f114ef7c" CreationDate="2017-06-13T10:01:33.920" Comment="Proposed by 298 approved by 77, -1 edit id of 191" />
  <row Id="2134" PostHistoryTypeId="5" PostId="656" RevisionGUID="5107e4da-f69d-4b44-8d98-be29a7ceb464" CreationDate="2017-06-13T10:01:33.920" UserId="73" Comment="Fixed syntax errors" Text="1. Adapter Trimming&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;One of the first things I do after encountering a set of reads is to remove the adapter sequences from the start and end of reads. Most basecalling software includes some amount of built-in adapter trimming, but it is almost always the case that some adapter sequence will remain. Removing adapters is helpful for mapping because it reduces the effort the mapper needs to go through to make a match (which may help some borderline reads to be mapped properly).&#xD;&#xA;&#xD;&#xA;My preferred adapter trimmer is [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic). It has the ability to search for palindromic adapter read through, and includes an adaptive sliding window option. Trimmomatic is threaded, and seems to work reasonably fast when run through files one at a time in threaded mode. Here's an example script that I might run to trim reads from some samples. This puts trimmed FASTQ files into a `trimmed` subdirectory:&#xD;&#xA;&#xD;&#xA;    TRIMPATH=/data/all/programs/trimmomatic/Trimmomatic-0.36;&#xD;&#xA;    JARPATH=&quot;${TRIMPATH}/trimmomatic-0.36.jar&quot;;&#xD;&#xA;    ADAPTfile=&quot;${TRIMPATH}/adapters/TruSeq3-PE-2.fa&quot;;&#xD;&#xA;    CMDS=&quot; ILLUMINACLIP:${ADAPTfile}:2:30:10:5:true LEADING:3 TRAILING:3  SLIDINGWINDOW:10:20 MINLEN:40 &quot;;&#xD;&#xA;&#xD;&#xA;    mkdir -p trimmed;&#xD;&#xA;    &#xD;&#xA;    for INFILE1 in *_1.fastq.gz; do&#xD;&#xA;      base=$(basename &quot;$INFILE1&quot; _1.fastq.gz);&#xD;&#xA;      echo ${base};&#xD;&#xA;      INFILE2=&quot;${base}_2.fastq.gz&quot;;&#xD;&#xA;      OUTFILE_P1=&quot;trimmed/${base}_P1.fastq.gz&quot;;&#xD;&#xA;      OUTFILE_P2=&quot;trimmed/${base}_P2.fastq.gz&quot;;&#xD;&#xA;      OUTFILE_U1=&quot;trimmed/${base}_U1.fastq.gz&quot;;&#xD;&#xA;      OUTFILE_U2=&quot;trimmed/${base}_U2.fastq.gz&quot;;&#xD;&#xA;      java -jar &quot;${JARPATH}&quot; PE -threads 20 -phred33 \&#xD;&#xA;            &quot;${INFILE1}&quot;  &quot;${INFILE2}&quot; \&#xD;&#xA;            &quot;${OUTFILE_P1}&quot; &quot;${OUTFILE_U1}&quot; &quot;${OUTFILE_P2}&quot;  &quot;${OUTFILE_U2}&quot; \&#xD;&#xA;            &quot;${CMDS}&quot;;&#xD;&#xA;    done;&#xD;&#xA;&#xD;&#xA;Trimmomatic is somewhat sensitive to bad input data, and will loudly fail (i.e. stop running) if it sees quality strings of different length to sequence strings:&#xD;&#xA;&#xD;&#xA;    Exception in thread &quot;Thread-1&quot; java.lang.RuntimeException: Sequence and quality length don't match: 'TACATGGCCCTGAAATGACTTTCACCCAGGCAACCAGTGCCCCCTGTATAGACACATGCCTTGGGCGCTCCCCACCCTTCCTCGCGTGGCCACACCTCTGT' vs '-AAFFJFJJ7A-FJJJFJFJFJJFFA-FFAF-&lt;A-&lt;FFFJA-&lt;-A7-F&lt;&lt;FFJJAJJJJJJJJJ--&lt;--7-7AJ7&lt;AAJA--J7&lt;ACTGCTGTGGGGCACCCAGCCCCCCAGATAGCCTGGCAGAAGGATGGGGGCACAGACTTCCCAGCTGCACGGGAGAGAC'&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqRecord.&lt;init&gt;(FastqRecord.java:25)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:89)&#xD;&#xA;            at org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)&#xD;&#xA;            at org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)&#xD;&#xA;            at java.lang.Thread.run(Thread.java:745)&#xD;&#xA;&#xD;&#xA;2. Read Mapping&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;Read mapping finds the most likely location for a read within a target genome.&#xD;&#xA;There are extremely fast approximate mappers available, but these don't yet work for variant calling, and an exact mapping approach is necessary. My current preferred mapper is [HISAT2](https://ccb.jhu.edu/software/hisat2/index.shtml). I use this instead of BWA due to the double-read issue, and because of the local variant-aware mapping. HISAT2 is made by the same computing group as Bowtie2 and Tophat2 (JHUCCB), and is their recommended tool for replacing those other programs.&#xD;&#xA;&#xD;&#xA;&gt; HISAT2 is a successor to both HISAT and TopHat2. We recommend that HISAT and TopHat2 users switch to HISAT2.&#xD;&#xA;&#xD;&#xA;The HISAT2 page includes a link to a [genomic index file](ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38_snp.tar.gz) for the human genome, including SNP variants, which I use when mapping human reads. HISAT2 is also threaded, so I run it on the files one at a time and pipe through samtools to create a sorted BAM file:&#xD;&#xA;&#xD;&#xA;    mkdir mapped&#xD;&#xA;    for r1 in trimmed/*_P1.fastq.gz;&#xD;&#xA;      do base=$(basename &quot;${x}&quot; _P1.fastq.gz);&#xD;&#xA;      r2=&quot;trimmed/${base}_P2.fastq.gz&quot;;&#xD;&#xA;      echo ${base};&#xD;&#xA;      hisat2 -p 20 -t -x /data/all/genomes/hsap/hisat_grch38_snp/genome_snp -1 \&#xD;&#xA;        &quot;${r1}&quot; -2 &quot;${r2}&quot; 2&gt;&quot;mapped/hisat2_${y}_vs_grch38_stderr.txt&quot; | \&#xD;&#xA;        samtools sort &gt; &quot;mapped/hisat2_${y}_vs_grch38.bam&quot;;&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;3. Variant Calling&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;I currently use samtools/bcftools to do this, but would be interested in other opinions as we recently had a grant reviewer response that samtools was a dinosaur program and better approaches were available. Samtools doesn't currently work with threads for variant calling, so I save the commands to a text file and then run them through [GNU Parallel](https://www.gnu.org/software/parallel/). This requires the genome files to be first downloaded and indexed via `samtools faidx`. This produces a set of gzipped VCF files in the `variants` directory:&#xD;&#xA;&#xD;&#xA;    mkdir -p variants;&#xD;&#xA;    (for x in mapped/*.bam;&#xD;&#xA;       do echo samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa &quot;${x}&quot; \| \&#xD;&#xA;       bcftools call --ploidy GRCh38 -v -m -O z -o variants/$(basename &quot;${x}&quot; .bam).vcf.gz; done) &gt; call_jobs.txt&#xD;&#xA;    cat call_jobs.txt | parallel -u;&#xD;&#xA;&#xD;&#xA;As a slow, but more accurate alternative, variants can be called for all samples at the same time:&#xD;&#xA;&#xD;&#xA;    samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa mapped/*.bam | \&#xD;&#xA;      bcftools call --ploidy GRCh38 -v -m -O z -o variants/hisat2_allCalled_vs_grch38.vcf.gz&#xD;&#xA;&#xD;&#xA;If a faster processing is desired, then the variant calling can be parallelised across chromosomes using `parallel` and merged afterwards using `bcftools norm`. The implementation of this is left as an exercise to the reader." />
  <row Id="2135" PostHistoryTypeId="5" PostId="668" RevisionGUID="fc1929ad-4210-49bb-b415-59ccf6f4deb8" CreationDate="2017-06-13T10:21:37.260" UserId="73" Comment="added 2 characters in body" Text="When you're doing a p-value adjustment, the same unadjusted p-value in different genes can be given different adjusted p-values depending on other factors. That means that you can't directly draw a line associated with the FDR on a plot of unadjusted p-value.&#xD;&#xA;&#xD;&#xA;One possibility would be to take a range of values that are close to the FDR threshold (e.g. the 20 values closest to threshold), and draw a p-value greyzone within that region:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/Rscript&#xD;&#xA;    values &lt;- c(rnorm(10000),rnorm(100, mean=1.5));&#xD;&#xA;    val.mean &lt;- median(values);&#xD;&#xA;    val.diffs &lt;- abs(values - median(values));&#xD;&#xA;    val.reldiffs &lt;- (values - median(values));&#xD;&#xA;    &#xD;&#xA;    val.pval &lt;- pnorm(val.diffs, mean = mean(val.diffs),&#xD;&#xA;                       sd=sd(val.diffs), lower.tail=FALSE);&#xD;&#xA;    val.padj &lt;- p.adjust(val.pvals, method=&quot;BH&quot;);&#xD;&#xA;    fdr.threshold &lt;- 0.1;&#xD;&#xA;    close.bh &lt;- order(abs(val.padj - fdr.threshold))[1:20];&#xD;&#xA;    &#xD;&#xA;    png(&quot;SE.663.png&quot;);&#xD;&#xA;    plot(val.reldiffs, -log10(val.pval),&#xD;&#xA;         col=ifelse(1:10100 &lt;= 10000,&quot;darkblue&quot;,&quot;darkgreen&quot;));&#xD;&#xA;    abline(h=-log10(0.05), col=&quot;red&quot;);&#xD;&#xA;    text(0,-log10(0.05),&quot;p=0.05&quot;, pos=1);&#xD;&#xA;    abline(h=range(-log10(val.pval[close.bh])), col=&quot;#00000040&quot;, lty=&quot;dashed&quot;);&#xD;&#xA;    rect(xleft=min(val.reldiffs)*2, xright=max(val.reldiffs)*2,&#xD;&#xA;         ytop=max(-log10(val.pval[close.bh])),&#xD;&#xA;         ybottom=min(-log10(val.pval[close.bh])), col=&quot;#00000020&quot;, border=NA);&#xD;&#xA;    text(0,min(-log10(val.pval[close.bh])),&quot;FDR=0.1&quot;, pos=1);&#xD;&#xA;    invisible(dev.off());&#xD;&#xA;&#xD;&#xA;[![Volcano plot of p-values with FDR greyzone][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/l8Ldk.png" />
  <row Id="2136" PostHistoryTypeId="2" PostId="687" RevisionGUID="375a4406-dfbf-443c-aa71-6e6966c4e5f7" CreationDate="2017-06-13T10:23:17.233" UserId="208" Text="I have a gene expression count matrix produced from bulk RNA-seq data. I'd like to find genes that were **not** expressed in a group of samples and were expressed in another group.&#xD;&#xA;&#xD;&#xA;The problem of course is that not all *effectively* non-expressed genes will have 0 counts due to sequencing errors, or because they were expressed in a small subset of cells.&#xD;&#xA;&#xD;&#xA;I'm interested in solutions using `R`." />
  <row Id="2137" PostHistoryTypeId="1" PostId="687" RevisionGUID="375a4406-dfbf-443c-aa71-6e6966c4e5f7" CreationDate="2017-06-13T10:23:17.233" UserId="208" Text="What methods are available to find a cutoff value for non-expressed genes in RNA-seq?" />
  <row Id="2138" PostHistoryTypeId="3" PostId="687" RevisionGUID="375a4406-dfbf-443c-aa71-6e6966c4e5f7" CreationDate="2017-06-13T10:23:17.233" UserId="208" Text="&lt;rna-seq&gt;&lt;r&gt;" />
  <row Id="2139" PostHistoryTypeId="2" PostId="688" RevisionGUID="dbec9aad-34a4-4893-9873-5e4ad92ec911" CreationDate="2017-06-13T10:32:44.133" UserId="298" Text="Your list has two identifiers for the same node per line. In order to use it, you will need to change that. If you want to use the gene name (2nd and 4th fields, in your example), just run:&#xD;&#xA;&#xD;&#xA;    awk 'print $2,$4' netw.txt &gt; netw.gr&#xD;&#xA;&#xD;&#xA;If you want to use the Entrez geneIDs instead, run:&#xD;&#xA;&#xD;&#xA;    awk 'print $1,$3' netw.txt &gt; netw.gr&#xD;&#xA;&#xD;&#xA;Then, as others already mentioned, install [`Cytoscape`](http://cytoscape.org/), launch it and import your gene list:&#xD;&#xA;&#xD;&#xA;1. In the welcome screen, select &quot;From Network File...&quot;:&#xD;&#xA;&#xD;&#xA;  [![Cytoscape welcome screen][1]][1]&#xD;&#xA;&#xD;&#xA;2. Then, select your network file (`netw.gr` in the example above) and choose &quot;Advanced Options&quot;:&#xD;&#xA;&#xD;&#xA; [![Advanced options][2]][2]&#xD;&#xA;&#xD;&#xA;3. Set the delimiter to SPACE and uncheck &quot;Use first line as column names&quot;:&#xD;&#xA;&#xD;&#xA; [![select the delimiter][3]][3]&#xD;&#xA;&#xD;&#xA;4. Finally, click on the header of each column and set the first to &quot;source node&quot; and the second to &quot;target node&quot;:&#xD;&#xA;&#xD;&#xA; [![source node][4]][4]  &#xD;&#xA;&#xD;&#xA; [![target node][5]][5]&#xD;&#xA;&#xD;&#xA;Now, click &quot;OK&quot; and you will have successfully imported your network into Cytoscape. &#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/EeYPU.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/nDM2t.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/YOnzG.png&#xD;&#xA;  [4]: https://i.stack.imgur.com/pUBYI.png&#xD;&#xA;  [5]: https://i.stack.imgur.com/LbSm3.png" />
  <row Id="2140" PostHistoryTypeId="2" PostId="689" RevisionGUID="f6555690-fb05-4c9e-93a0-34d0d4b202f8" CreationDate="2017-06-13T11:09:16.817" UserId="77" Text="A common method is to use [zFPKMs](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3870982/), which you can find implemented in R [here](https://github.com/severinEvo/gene_expression/blob/master/zFPKM.R).&#xD;&#xA;&#xD;&#xA;Having said that, there's an inherent problem in declaring a difference between two things on either side of a given threshold. Given that, I would encourage you to use more than &quot;expressed in one and not in another&quot;, likely adding at least a &quot;with a minimal difference of X between them&quot; metric. You may also find the tau metric useful, which is implemented in R [here](https://github.com/severinEvo/gene_expression/blob/master/tau.R). This is meant to measure tissue-specificity, which is more akin to what you're probably interested in doing." />
  <row Id="2141" PostHistoryTypeId="5" PostId="666" RevisionGUID="fb5b76bb-cda9-477e-b1d3-5f56ce349bf1" CreationDate="2017-06-13T11:10:13.613" UserId="77" Comment="deleted 1 character in body" Text="The only way to get the alpha levels is to determine what they will be with `p.adjust()`, since they will depend on the distribution of your unadjusted p values. The general steps you should follow will be:&#xD;&#xA;&#xD;&#xA;1. Add a column of adjusted p-values to your dataframe (`mydata$padj = p.adjust(mydata, method=&quot;BH&quot;)`, which is the same as FDR and saves a character).&#xD;&#xA;2. Use `which` and `max` to determine your two alpha threshold (e.g., `max(mydata$pvalue[mydata$padj &lt; 0.05])`&#xD;&#xA;&#xD;&#xA;Then you can adjust your plots however you like (presumably with some horizontal lines at the various alphas). Whether you take the smallest non-significant value or the largest significant value is up to you, just describe what &quot;dots on the line&quot; represent." />
  <row Id="2142" PostHistoryTypeId="2" PostId="690" RevisionGUID="6718ce7c-df3e-4358-a234-d311abacca24" CreationDate="2017-06-13T11:13:08.053" UserId="29" Text="&gt; I'd like to find genes that were not expressed in a group of samples and were expressed in another group.&#xD;&#xA;&#xD;&#xA;This is, fundamentally, a differential expression analysis, with a twist. To solve this, you’d first use a differential expression library of your choice (e.g. DESeq2) and perform a one-tailed test of differential expression.&#xD;&#xA;&#xD;&#xA;Briefly, you’d perform the normal setup and then use&#xD;&#xA;&#xD;&#xA;    results(dds, altHypothesis = 'greater')&#xD;&#xA;&#xD;&#xA;To perform a one-tailed test. This will give you only those genes that are significantly upregulated in one group. Check chapter 3.9 of the vignette for details.&#xD;&#xA;&#xD;&#xA;Of course this won’t tell you that the genes are *unexpressed* in the other group. Unfortunately I don’t know of a good value to threshold the results; I would start by plotting a histogram of the (regularised log-transformed) expression values in your first group, and then visually choose an expression threshold that cleanly separates genes that are clearly expressed from zeros:&#xD;&#xA;&#xD;&#xA;    rlog_counts = rlog(dds)&#xD;&#xA;    dens = density(rlog_counts[, replicates])&#xD;&#xA;    plot(dens)&#xD;&#xA;&#xD;&#xA;(This merges the replicates in the group, which should be fine.)&#xD;&#xA;&#xD;&#xA;Counts usually follow a bimodal distribution, with one mode for unexpressed and one for expressed genes. Values between these provide good thresholds:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;Here I used `identify(dens)` to identify the threshold interactively but you could also use an analytical method.&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/rQ4c2.png" />
  <row Id="2143" PostHistoryTypeId="8" PostId="666" RevisionGUID="28128d12-f336-460a-ba1e-b4c850ea632f" CreationDate="2017-06-13T11:13:16.753" UserId="77" Comment="Rollback to [29217d55-f6f7-4fbe-a192-669abf4ab525]" Text="The only way to get the alpha levels is to determine what they will be with `p.adjust()`, since they will depend on the distribution of your unadjusted p values. The general steps you should follow will be:&#xD;&#xA;&#xD;&#xA;1. Add a column of adjusted p-values to your dataframe (`mydata$padj = p.adjust(mydata, method=&quot;BH&quot;)`, which is the same as FDR and saves a character).&#xD;&#xA;2. Use `which` and `max` to determine your two alpha threshold (e.g., `max(mydata$pvalue[mydata$padj &lt;= 0.05])`&#xD;&#xA;&#xD;&#xA;Then you can adjust your plots however you like (presumably with some horizontal lines at the various alphas). Whether you take the smallest non-significant value or the largest significant value is up to you, just describe what &quot;dots on the line&quot; represent." />
  <row Id="2144" PostHistoryTypeId="8" PostId="666" RevisionGUID="4e978a66-0448-4164-8a29-286c73ef1d1b" CreationDate="2017-06-13T11:14:15.507" UserId="77" Comment="Rollback to [fb5b76bb-cda9-477e-b1d3-5f56ce349bf1]" Text="The only way to get the alpha levels is to determine what they will be with `p.adjust()`, since they will depend on the distribution of your unadjusted p values. The general steps you should follow will be:&#xD;&#xA;&#xD;&#xA;1. Add a column of adjusted p-values to your dataframe (`mydata$padj = p.adjust(mydata, method=&quot;BH&quot;)`, which is the same as FDR and saves a character).&#xD;&#xA;2. Use `which` and `max` to determine your two alpha threshold (e.g., `max(mydata$pvalue[mydata$padj &lt; 0.05])`&#xD;&#xA;&#xD;&#xA;Then you can adjust your plots however you like (presumably with some horizontal lines at the various alphas). Whether you take the smallest non-significant value or the largest significant value is up to you, just describe what &quot;dots on the line&quot; represent." />
  <row Id="2145" PostHistoryTypeId="2" PostId="691" RevisionGUID="20b26b67-3fec-4fd0-b8ab-9a10abe97dfb" CreationDate="2017-06-13T11:30:59.540" UserId="599" Text="Unfortunatly that is not doable - there is now way of distinguishing between features not expressed and features not expressed for technical reasons. Furthermore a lot of genes might seem lowly expressed to to technical noise.&#xD;&#xA;&#xD;&#xA;This also means that you will have to choose a cutoff and simply use that. For choosing a cutoff looking at the replicate distribution of log10(counts) and log10(FPKM) are probably the best options." />
  <row Id="2146" PostHistoryTypeId="2" PostId="692" RevisionGUID="a6a2c39d-ac17-45eb-b862-761a0350d1c0" CreationDate="2017-06-13T11:51:32.633" UserId="450" Text="I have a large collection of sequence trace files which are in [SCF file format][1] (binary files that additionally to the string of DNA bases also contain the electropherogram of the sample and quality information about the base calls).&#xD;&#xA;&#xD;&#xA;I have an existing BioPython pipeline that accepts files in [AB1 format](http://www6.appliedbiosystems.com/support/software_community/ABIF_File_Format.pdf) (pdf), so I need to convert my SCF files to AB1 files.&#xD;&#xA;&#xD;&#xA;I found a question on Biostars that asks for the opposite: [converting from SCF to AB1][2].&#xD;&#xA;&#xD;&#xA;The accepted answer there was to use [`convert_trace`][3] from the Staden package.&#xD;&#xA;&#xD;&#xA;    ./convert_trace -out_format scf &lt; trace.ab1 &gt; trace.scf&#xD;&#xA;&#xD;&#xA;So I tried the opposite:&#xD;&#xA;&#xD;&#xA;    ./convert_trace -out_format abi &lt; trace.scf &gt; trace.ab1&#xD;&#xA;&#xD;&#xA;This did indeed produce a file with `*.ab1` extension, but upon importing it to my BioPython pipeline, I got the error&#xD;&#xA;&#xD;&#xA;    File should start ABIF, not '\xaeZTR'&#xD;&#xA;&#xD;&#xA;So apparently, `convert_trace` converted my files to [ZTR format][4], even altough I specified `-out_format abi`.&#xD;&#xA;&#xD;&#xA;Closer reading of the manual states that `[convert_exe] can write CTF, EXP, PLN, SCF and ZTR formats.` (so indeed no ABI), but it confuses me why `ABI` is still a valid output format.&#xD;&#xA;&#xD;&#xA;So, my question is, how do I convert SCF files into ABI files?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://staden.sourceforge.net/manual/formats_unix_2.html&#xD;&#xA;  [2]: https://www.biostars.org/p/622/&#xD;&#xA;  [3]: http://staden.sourceforge.net/manual/manpages_unix_1.html&#xD;&#xA;  [4]: http://staden.sourceforge.net/manual/formats_unix_12.html" />
  <row Id="2147" PostHistoryTypeId="1" PostId="692" RevisionGUID="a6a2c39d-ac17-45eb-b862-761a0350d1c0" CreationDate="2017-06-13T11:51:32.633" UserId="450" Text="How to convert SCF trace files to ABI files?" />
  <row Id="2148" PostHistoryTypeId="3" PostId="692" RevisionGUID="a6a2c39d-ac17-45eb-b862-761a0350d1c0" CreationDate="2017-06-13T11:51:32.633" UserId="450" Text="&lt;scf&gt;&lt;abi&gt;&lt;staden&gt;" />
  <row Id="2149" PostHistoryTypeId="5" PostId="680" RevisionGUID="6bf68053-e1dd-45c8-b29c-306feeb60480" CreationDate="2017-06-13T12:16:07.970" UserId="73" Comment="deleted 6 characters in body" Text="`samtools stats` seems to be able to do most of this, excluding the CIGAR-string parsing stuff (i.e. INDELs):&#xD;&#xA;&#xD;&#xA;    $ samtools view -h mapped.bam | grep -e '^@' -e 'readName' |&#xD;&#xA;        samtools stats | grep '^SN' | cut -f 2-&#xD;&#xA;    &#xD;&#xA;    raw total sequences:	2&#xD;&#xA;    filtered sequences:	0&#xD;&#xA;    sequences:	2&#xD;&#xA;    is sorted:	1&#xD;&#xA;    1st fragments:	2&#xD;&#xA;    last fragments:	0&#xD;&#xA;    reads mapped:	2&#xD;&#xA;    reads mapped and paired:	0	# paired-end technology bit set + both mates mapped&#xD;&#xA;    reads unmapped:	0&#xD;&#xA;    reads properly paired:	0	# proper-pair bit set&#xD;&#xA;    reads paired:	0	# paired-end technology bit set&#xD;&#xA;    reads duplicated:	0	# PCR or optical duplicate bit set&#xD;&#xA;    reads MQ0:	0	# mapped and MQ=0&#xD;&#xA;    reads QC failed:	0&#xD;&#xA;    non-primary alignments:	0&#xD;&#xA;    total length:	13632	# ignores clipping&#xD;&#xA;    bases mapped:	13632	# ignores clipping&#xD;&#xA;    bases mapped (cigar):	13502	# more accurate&#xD;&#xA;    bases trimmed:	0&#xD;&#xA;    bases duplicated:	0&#xD;&#xA;    mismatches:	3670	# from NM fields&#xD;&#xA;    error rate:	2.718116e-01	# mismatches / bases mapped (cigar)&#xD;&#xA;    average length:	6816&#xD;&#xA;    maximum length:	6816&#xD;&#xA;    average quality:	8.9&#xD;&#xA;    insert size average:	0.0&#xD;&#xA;    insert size standard deviation:	0.0&#xD;&#xA;    inward oriented pairs:	0&#xD;&#xA;    outward oriented pairs:	0&#xD;&#xA;    pairs with other orientation:	0&#xD;&#xA;    pairs on different chromosomes:	0&#xD;&#xA;&#xD;&#xA;For a length filter, an awk length check can be slotted in instead of the grep:&#xD;&#xA;&#xD;&#xA;    $ samtools view -h mapped.bam |&#xD;&#xA;        awk -F'\t' '{if((/^@/) || (length($10)&gt;500)){print $0}}' |&#xD;&#xA;        samtools stats | grep '^SN' | cut -f 2-&#xD;&#xA;&#xD;&#xA;    raw total sequences:	131&#xD;&#xA;    filtered sequences:	0&#xD;&#xA;    sequences:	131&#xD;&#xA;    is sorted:	1&#xD;&#xA;    1st fragments:	131&#xD;&#xA;    last fragments:	0&#xD;&#xA;    reads mapped:	131&#xD;&#xA;    reads mapped and paired:	0	# paired-end technology bit set + both mates mapped&#xD;&#xA;    reads unmapped:	0&#xD;&#xA;    reads properly paired:	0	# proper-pair bit set&#xD;&#xA;    reads paired:	0	# paired-end technology bit set&#xD;&#xA;    reads duplicated:	0	# PCR or optical duplicate bit set&#xD;&#xA;    reads MQ0:	0	# mapped and MQ=0&#xD;&#xA;    reads QC failed:	0&#xD;&#xA;    non-primary alignments:	0&#xD;&#xA;    total length:	569425	# ignores clipping&#xD;&#xA;    bases mapped:	569425	# ignores clipping&#xD;&#xA;    bases mapped (cigar):	453884	# more accurate&#xD;&#xA;    bases trimmed:	0&#xD;&#xA;    bases duplicated:	0&#xD;&#xA;    mismatches:	113755	# from NM fields&#xD;&#xA;    error rate:	2.506257e-01	# mismatches / bases mapped (cigar)&#xD;&#xA;    average length:	4346&#xD;&#xA;    maximum length:	16909&#xD;&#xA;    average quality:	9.1&#xD;&#xA;    insert size average:	0.0&#xD;&#xA;    insert size standard deviation:	0.0&#xD;&#xA;    inward oriented pairs:	0&#xD;&#xA;    outward oriented pairs:	0&#xD;&#xA;    pairs with other orientation:	0&#xD;&#xA;    pairs on different chromosomes:	0&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2150" PostHistoryTypeId="10" PostId="650" RevisionGUID="9c55376d-3635-489a-a569-22b48f62af8c" CreationDate="2017-06-13T12:36:21.717" UserId="-1" Comment="101" Text="{&quot;OriginalQuestionIds&quot;:[646],&quot;Voters&quot;:[{&quot;Id&quot;:292,&quot;DisplayName&quot;:&quot;bli&quot;},{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;},{&quot;Id&quot;:57,&quot;DisplayName&quot;:&quot;Kamil S Jaron&quot;},{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;}]}" />
  <row Id="2151" PostHistoryTypeId="5" PostId="692" RevisionGUID="d8e93589-e7ea-46a2-b520-6c30c7a52499" CreationDate="2017-06-13T12:41:26.687" UserId="298" Comment="How to is a declaration not a question" Text="I have a large collection of sequence trace files which are in [SCF file format][1] (binary files that in addition to the string of DNA bases also contain the electropherogram of the sample and quality information about the base calls).&#xD;&#xA;&#xD;&#xA;I have an existing BioPython pipeline that accepts files in [AB1 format](http://www6.appliedbiosystems.com/support/software_community/ABIF_File_Format.pdf) (pdf), so I need to convert my SCF files to AB1 files.&#xD;&#xA;&#xD;&#xA;I found a question on Biostars that asks for the opposite: [converting from SCF to AB1][2].&#xD;&#xA;&#xD;&#xA;The accepted answer there was to use [`convert_trace`][3] from the Staden package.&#xD;&#xA;&#xD;&#xA;    ./convert_trace -out_format scf &lt; trace.ab1 &gt; trace.scf&#xD;&#xA;&#xD;&#xA;So I tried the opposite:&#xD;&#xA;&#xD;&#xA;    ./convert_trace -out_format abi &lt; trace.scf &gt; trace.ab1&#xD;&#xA;&#xD;&#xA;This did indeed produce a file with `*.ab1` extension, but upon importing it to my BioPython pipeline, I got the error&#xD;&#xA;&#xD;&#xA;    File should start ABIF, not '\xaeZTR'&#xD;&#xA;&#xD;&#xA;So apparently, `convert_trace` converted my files to [ZTR format][4], even altough I specified `-out_format abi`.&#xD;&#xA;&#xD;&#xA;Closer reading of the manual states that `[convert_exe] can write CTF, EXP, PLN, SCF and ZTR formats.` (so indeed no ABI), but it confuses me why `ABI` is still a valid output format.&#xD;&#xA;&#xD;&#xA;So, my question is, how do I convert SCF files into ABI files?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://staden.sourceforge.net/manual/formats_unix_2.html&#xD;&#xA;  [2]: https://www.biostars.org/p/622/&#xD;&#xA;  [3]: http://staden.sourceforge.net/manual/manpages_unix_1.html&#xD;&#xA;  [4]: http://staden.sourceforge.net/manual/formats_unix_12.html" />
  <row Id="2152" PostHistoryTypeId="4" PostId="692" RevisionGUID="d8e93589-e7ea-46a2-b520-6c30c7a52499" CreationDate="2017-06-13T12:41:26.687" UserId="298" Comment="How to is a declaration not a question" Text="How can I convert SCF trace files to ABI files?" />
  <row Id="2153" PostHistoryTypeId="24" PostId="692" RevisionGUID="d8e93589-e7ea-46a2-b520-6c30c7a52499" CreationDate="2017-06-13T12:41:26.687" Comment="Proposed by 298 approved by 77, 450 edit id of 193" />
  <row Id="2154" PostHistoryTypeId="2" PostId="693" RevisionGUID="dfe893a9-0e4a-4117-94af-3eedf74bd2a8" CreationDate="2017-06-13T13:03:09.127" UserId="208" Text="In single-cell RNA-seq data we have an inflated number of 0 (or near-zero) counts due to low mRNA capture rate and other inefficiencies.&#xD;&#xA;&#xD;&#xA;How can we decide which genes are 0 due to gene dropout (lack of measurement sensitivity), and which are genuinely not expressed in the cell?&#xD;&#xA;&#xD;&#xA;Deeper sequencing does not solve this problem as shown on the below saturation curve of 10x Chromium data:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/cBXmF.png" />
  <row Id="2155" PostHistoryTypeId="1" PostId="693" RevisionGUID="dfe893a9-0e4a-4117-94af-3eedf74bd2a8" CreationDate="2017-06-13T13:03:09.127" UserId="208" Text="How can we distinguish between true zero and dropout-zero counts in single-cell RNA-seq?" />
  <row Id="2156" PostHistoryTypeId="3" PostId="693" RevisionGUID="dfe893a9-0e4a-4117-94af-3eedf74bd2a8" CreationDate="2017-06-13T13:03:09.127" UserId="208" Text="&lt;rna-seq&gt;&lt;sequencing&gt;" />
  <row Id="2157" PostHistoryTypeId="5" PostId="682" RevisionGUID="b4754405-f3a5-446f-9d5f-29ebe61c0e26" CreationDate="2017-06-13T13:09:37.473" UserId="73" Comment="correct spelling / grammar" Text="Regading the perl vs python discussion, there is no final answer which language is better, but I have some advice for you:&#xD;&#xA;&#xD;&#xA;**Learn the language your colleagues or your advisor use**. This way you are able to discuss your code with them and also get help if you run into problems." />
  <row Id="2160" PostHistoryTypeId="2" PostId="694" RevisionGUID="5b8be8de-dfb8-457e-9485-c7bdc475c2ac" CreationDate="2017-06-13T13:29:05.017" UserId="266" Text="I'd like to test [MMTF](http://mmtf.rcsb.org), a new format for storing biomolecular structures which is promoted by RCSB as a more compact alternative to mmCIF and PDB.&#xD;&#xA;&#xD;&#xA;From [MMTF FAQ](http://mmtf.rcsb.org/faq.html):&#xD;&#xA;&#xD;&#xA;&gt; * How do I convert a PDBx/mmCIF file to an MMTF file?&#xD;&#xA;&gt;&#xD;&#xA;&gt;  The BioJava library contains methods to read and write PDBx/mmCIF files and MMTF files.&#xD;&#xA;&#xD;&#xA;Can I do such a conversion, ideally from command line, but without writing my own Java program?" />
  <row Id="2161" PostHistoryTypeId="1" PostId="694" RevisionGUID="5b8be8de-dfb8-457e-9485-c7bdc475c2ac" CreationDate="2017-06-13T13:29:05.017" UserId="266" Text="Converter between PDB or mmCIF and MMTF" />
  <row Id="2162" PostHistoryTypeId="3" PostId="694" RevisionGUID="5b8be8de-dfb8-457e-9485-c7bdc475c2ac" CreationDate="2017-06-13T13:29:05.017" UserId="266" Text="&lt;file-formats&gt;&lt;pdb&gt;&lt;mmcif&gt;&lt;3d-structure&gt;" />
  <row Id="2163" PostHistoryTypeId="2" PostId="695" RevisionGUID="8f460ee6-c320-49cb-91fd-9c8307b15a45" CreationDate="2017-06-13T13:43:41.210" UserId="248" Text="I have a set of genomic ranges that are potentially overlapping. I want to count the amount of ranges at certain positions. &#xD;&#xA;&#xD;&#xA;Pretty sure there are good solutions, but i seem to be unable to find them. &#xD;&#xA;&#xD;&#xA;Solutions like [cut](https://stackoverflow.com/questions/26140151/binning-two-vectors-of-different-ranges-using-r?rq=1) or [findIntervals](https://stackoverflow.com/questions/11963508/generate-bins-from-a-data-frame) don't achieve what i want as they only count on one vector or accumulate by `all values &lt;= break`&#xD;&#xA;&#xD;&#xA;Also `countMatches {GenomicRanges}` doesn't seem to cover it.&#xD;&#xA;&#xD;&#xA;Probably one could use Bedtools, but i don't want to leave R.&#xD;&#xA;&#xD;&#xA;I could only come up with a hilariously slow solution&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    # generate test data&#xD;&#xA;    testdata &lt;- data.frame(chrom = rep(seq(1,10),10),&#xD;&#xA;                           starts = abs(rnorm(100, mean = 1, sd = 1)) * 1000,&#xD;&#xA;                           ends = abs(rnorm(100, mean = 2, sd = 1)) * 2000)&#xD;&#xA;    &#xD;&#xA;    # make sure that all end coordinates are bigger than start&#xD;&#xA;    # this is a requirement of the original data&#xD;&#xA;    testdata &lt;- testdata[testdata$ends - testdata$starts &gt; 0,]&#xD;&#xA;    &#xD;&#xA;    # count overlapping ranges on certain positions&#xD;&#xA;    count.data &lt;- lapply(unique(testdata$chrom), function(chromosome){&#xD;&#xA;        tmp.inner &lt;- lapply(seq(1,10000, by = 120), function(i){&#xD;&#xA;            sum(testdata$chrom == chromosome &amp; testdata$starts &lt;= i &amp; testdata$ends &gt;= i)&#xD;&#xA;        })&#xD;&#xA;        return(unlist(tmp.inner))&#xD;&#xA;    })&#xD;&#xA;    &#xD;&#xA;    # generate a data.frame containing all data&#xD;&#xA;    df.count.data &lt;- ldply(count.data, rbind)&#xD;&#xA;    &#xD;&#xA;    # ideally the chromosome will be columns and not rows&#xD;&#xA;    t(df.count.data)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2164" PostHistoryTypeId="1" PostId="695" RevisionGUID="8f460ee6-c320-49cb-91fd-9c8307b15a45" CreationDate="2017-06-13T13:43:41.210" UserId="248" Text="Count genomic ranges" />
  <row Id="2165" PostHistoryTypeId="3" PostId="695" RevisionGUID="8f460ee6-c320-49cb-91fd-9c8307b15a45" CreationDate="2017-06-13T13:43:41.210" UserId="248" Text="&lt;r&gt;" />
  <row Id="2166" PostHistoryTypeId="2" PostId="696" RevisionGUID="2b29bf15-a512-4a71-adac-874792d11173" CreationDate="2017-06-13T14:01:53.477" UserId="37" Text="Use [htsbox](https://github.com/lh3/htsbox):&#xD;&#xA;&#xD;&#xA;    htsbox samview -p in.bam | less -S&#xD;&#xA;    htsbox samview -pS in.sam | less -S&#xD;&#xA;&#xD;&#xA;It outputs mapping positions in the [PAF](https://github.com/lh3/miniasm/blob/master/PAF.md) format, which looks something like:&#xD;&#xA;&#xD;&#xA;    read1  4983  774  4982  +  chr18  80373285  26911072  26915544  3835  4631  60 \&#xD;&#xA;           mm:i:214  io:i:119  in:i:159  do:i:339  dn:i:423&#xD;&#xA;&#xD;&#xA;This is one long line in terminal. I folded the line for display purposes. For the first 12 fixed fields, see the table in [PAF page](https://github.com/lh3/miniasm/blob/master/PAF.md). They give mapping positions and identity. The optional fields tell you #mismatches (mm), #insOpens (io), #insertions (in), #delOpens (do) and #deletions (dn). Note that the alignment MUST have an &quot;NM&quot; tag; otherwise the number of matching bases (col. 11) is overestimated and &quot;mm&quot; will always be zero.&#xD;&#xA;&#xD;&#xA;You can easily compute summary statistics from PAF if you are familiar with command lines. For example:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-shell --&gt;&#xD;&#xA;&#xD;&#xA;    htsbox samview -p in.bam | awk '{x+=$10;y+=$11}END{print x/y}'  # identity&#xD;&#xA;    htsbox samview -p in.bam | awk '$2&gt;1000{x+=$10;y+=$11}END{print x/y}'  # identity for &gt;1000bp&#xD;&#xA;    htsbox samview -p in.bam \&#xD;&#xA;      | perl -ane '{$g+=$1 while /[id]n:i:(\d+)/g;$y+=$F[10]}END{print $g/$y,&quot;\n&quot;}' # gap rate&#xD;&#xA;&#xD;&#xA;If you don't like long one-liners, feel free to write a proper script to collect all statistics in one go." />
  <row Id="2167" PostHistoryTypeId="2" PostId="697" RevisionGUID="cd24cacb-5a68-4e8e-b54a-77c83ae9b516" CreationDate="2017-06-13T14:04:02.240" UserId="29" Text="[`GenomicRanges::countOverlaps`](https://www.rdocumentation.org/packages/GenomicRanges/versions/1.24.1/topics/findOverlaps-methods) seems to be what you’re after:&#xD;&#xA;&#xD;&#xA;    position_range = GRanges(position$chrom, IRanges(position, position, width = 1))&#xD;&#xA;    ranges_at_position = countOverlaps(position_ranges, granges)" />
  <row Id="2168" PostHistoryTypeId="2" PostId="698" RevisionGUID="6fc5770c-6c74-4949-8b49-95f1c5868eaa" CreationDate="2017-06-13T14:06:16.443" UserId="44" Text="Apparently `convert_trace` does not do a good parameter checking and silently sets ZTR as default if it does not recognise a valid output format.&#xD;&#xA;&#xD;&#xA;I don't know of any freely available command-line converters in that specific direction. Even at the height of Sanger sequencing (late 90s, early 2000s), pipelines usually tried to reduce complexity and footprint by converting AB1 to anything else, often SCF (ZTR came too late and never got enough traction).&#xD;&#xA;&#xD;&#xA;That being said, I dug out an SCF I had from 1997, loaded it into [Geneious][1], and re-exported it as .ab1. I was a bit astonished to see that this worked. So Geneious (or similar packages from other companies) may be your best bet for a quick, one-off thing.&#xD;&#xA;&#xD;&#xA;Depending on your needs, you might want to think about other ways: either expand BioPython to read SCFs, or maybe to expand the Staden io_lib to be able to write .ab1 (James still maintains that package and is extremely helpful).&#xD;&#xA;&#xD;&#xA;Good luck.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.geneious.com/" />
  <row Id="2169" PostHistoryTypeId="5" PostId="695" RevisionGUID="e2fc68b4-af30-4e64-a6dc-96919c6fa655" CreationDate="2017-06-13T14:14:59.873" UserId="298" Comment="Copy edited" Text="I have a set of genomic ranges that are potentially overlapping. I want to count the amount of ranges at certain positions using R. &#xD;&#xA;&#xD;&#xA;I'm Pretty sure there are good solutions, but I seem to be unable to find them. &#xD;&#xA;&#xD;&#xA;Solutions like [cut](https://stackoverflow.com/questions/26140151/binning-two-vectors-of-different-ranges-using-r?rq=1) or [findIntervals](https://stackoverflow.com/questions/11963508/generate-bins-from-a-data-frame) don't achieve what I want as they only count on one vector or accumulate by `all values &lt;= break`.&#xD;&#xA;&#xD;&#xA;Also `countMatches {GenomicRanges}` doesn't seem to cover it.&#xD;&#xA;&#xD;&#xA;Probably one could use Bedtools, but I don't want to leave R.&#xD;&#xA;&#xD;&#xA;I could only come up with a hilariously slow solution&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    # generate test data&#xD;&#xA;    testdata &lt;- data.frame(chrom = rep(seq(1,10),10),&#xD;&#xA;                           starts = abs(rnorm(100, mean = 1, sd = 1)) * 1000,&#xD;&#xA;                           ends = abs(rnorm(100, mean = 2, sd = 1)) * 2000)&#xD;&#xA;    &#xD;&#xA;    # make sure that all end coordinates are bigger than start&#xD;&#xA;    # this is a requirement of the original data&#xD;&#xA;    testdata &lt;- testdata[testdata$ends - testdata$starts &gt; 0,]&#xD;&#xA;    &#xD;&#xA;    # count overlapping ranges on certain positions&#xD;&#xA;    count.data &lt;- lapply(unique(testdata$chrom), function(chromosome){&#xD;&#xA;        tmp.inner &lt;- lapply(seq(1,10000, by = 120), function(i){&#xD;&#xA;            sum(testdata$chrom == chromosome &amp; testdata$starts &lt;= i &amp; testdata$ends &gt;= i)&#xD;&#xA;        })&#xD;&#xA;        return(unlist(tmp.inner))&#xD;&#xA;    })&#xD;&#xA;    &#xD;&#xA;    # generate a data.frame containing all data&#xD;&#xA;    df.count.data &lt;- ldply(count.data, rbind)&#xD;&#xA;    &#xD;&#xA;    # ideally the chromosome will be columns and not rows&#xD;&#xA;    t(df.count.data)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2170" PostHistoryTypeId="24" PostId="695" RevisionGUID="e2fc68b4-af30-4e64-a6dc-96919c6fa655" CreationDate="2017-06-13T14:14:59.873" Comment="Proposed by 298 approved by 77, 29 edit id of 194" />
  <row Id="2171" PostHistoryTypeId="2" PostId="699" RevisionGUID="f86ca967-6a16-4f9b-a058-29af7d995868" CreationDate="2017-06-13T14:59:54.690" UserId="712" Text="I agree with @story and @agapow here. As it stands right now BioPerl is still much more mature than BioPython. If there's a tool or method that only exists in the BioPerl library then you should probably use BioPerl to accomplish what you need to accomplish. But it's undeniable that BioPython is progressing very quickly and will soon overtake BioPerl. That being said there are still a lot of plus side regarding Perl in the Bioinformatics realm and it will definitely help to be somewhat competent in it. &#xD;&#xA;&#xD;&#xA;So in short my answer is to use both! I find myself mostly using Python and BioPython but if I need to accomplish something using a tool that isn't available in the BioPython library or can be accomplished more quickly using Perl I will look towards BioPerl/Perl. " />
  <row Id="2173" PostHistoryTypeId="5" PostId="377" RevisionGUID="8962eb38-f2f0-4988-8573-a93324f9e208" CreationDate="2017-06-13T15:08:56.573" UserId="434" Comment="added 14 characters in body" Text="[The Open Group Base Specifications Issue 7&#xD;&#xA;IEEE Std 1003.1™-2008, 2016 Edition](http://pubs.opengroup.org/onlinepubs/9699919799.2016edition/), or &quot;The POSIX Standard&quot; for short, is the standard that defines the interfaces and utilities provided by a Unix system.  Among these is the command line shell language and tools (see &quot;Shell &amp; Utilities&quot; in the main index on the page linked above).&#xD;&#xA;&#xD;&#xA;As far as I know, there is no shell that implement _exactly_ what's specified by the standard, but both `bash` and `ksh93` does a pretty good job of adhering to the standard along with their own, sometimes conflicting, extensions. The `ksh93` shell in particular has had a big impact on the past development of the POSIX shell specification, but future POSIX specifications _may_ borrow more from `bash` due to its wide use on Linux.&#xD;&#xA;&#xD;&#xA;The `bash` shell is pretty much ubiquitous on Linux systems, and may be installed on all other Unices too. `ksh93` is also available for most Unices but is usually not installed by default on Linux. `ksh93` is available by default on at least macOS (as `ksh`) and Solaris.&#xD;&#xA;&#xD;&#xA;If you are concerned about portability when writing a shell script (which is IMHO a good thing to be concerned about), you should make sure that you use only the POSIX utilities and their POSIX command line flags, as well as only use POSIX shell syntax. You should then ensure that you script is executed by `/bin/sh` which is supposed to be a shell that understands the POSIX specification.  `/bin/sh` is often implemented by `bash` running in &quot;POSIX mode&quot;, but it may also be `dash`, `ash` or `pdksh` (or something else) depending on what Unix you are using.&#xD;&#xA;&#xD;&#xA;For a Linux user, the most difficult bit in writing a portable script is often not the shell per se, but the multitude of non-standard command line flags provided by the GNU implementation of the many shell utilities.  The GNU coreutils (basic shell utilities) may, like `bash`, be installed on all Unices though." />
  <row Id="2174" PostHistoryTypeId="2" PostId="700" RevisionGUID="e0cc80dc-0a8f-45e7-8d41-82ec0951a5d1" CreationDate="2017-06-13T15:21:16.947" UserId="35" Text="Some people use imputation to differentiate between true zeros and dropout in single-cell data. Some approaches you can look into:&#xD;&#xA;&#xD;&#xA; - [CIDR][1]&#xD;&#xA; - [scImpute][2]&#xD;&#xA; - Seurat ([addImputedScore][3])&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/VCCRI/CIDR&#xD;&#xA;  [2]: https://github.com/Vivianstats/scImpute&#xD;&#xA;  [3]: http://satijalab.org/seurat/seurat_spatial_tutorial_part2.html" />
  <row Id="2175" PostHistoryTypeId="2" PostId="701" RevisionGUID="7b332ce3-370d-4749-9688-eceb736c91b5" CreationDate="2017-06-13T15:23:02.040" UserId="48" Text="You requested a tool similar to [PANTHER][1] but in R. First, PANTHER does a The PANTHER (Protein ANalysis THrough Evolutionary Relationships) tool does a classification  based on evolutionarily related proteins, gene ontologies ( molecular function, and biological process) and pathways.&#xD;&#xA;&#xD;&#xA;AFAIK there isn't a tool in R that integrates all these into one, but several packages do pieces of it. However, my observations after a couple of usage is that is based a lot in gene ontologies. Gene Ontologies are not pathways, but the best tool I found for it is [topGO][2] (A bit hard to use but useful to get insights on the biology) which test over representations of a term in a given list. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://pantherdb.org/&#xD;&#xA;  [2]: http://bioconductor.org/packages/topGO" />
  <row Id="2176" PostHistoryTypeId="10" PostId="676" RevisionGUID="df60486a-3477-453a-9e80-3db42f792631" CreationDate="2017-06-13T15:34:12.113" UserId="-1" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;},{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;},{&quot;Id&quot;:191,&quot;DisplayName&quot;:&quot;Iakov Davydov&quot;},{&quot;Id&quot;:57,&quot;DisplayName&quot;:&quot;Kamil S Jaron&quot;}]}" />
  <row Id="2177" PostHistoryTypeId="2" PostId="702" RevisionGUID="0864ba56-abf0-4129-82c9-c5255a66b77c" CreationDate="2017-06-13T15:38:05.440" UserId="842" Text="I made this post in regular stack overflow but I was told about this awesome feature by @nbryans.&#xD;&#xA;&#xD;&#xA;I am a researcher (my programming knowledge is small) conducting analysis on a set of antibiotic (methicillin) resistant and a set of antibiotic (methicillin) susceptible. The sole difference between these two sets is assumed to be the resistance (R) vs susceptibility (S). I want to find if a genetic element in the resistant or susceptible genomes is correlated to resistance (positively or negatively). &#xD;&#xA;&#xD;&#xA;I have done a complete fisher exact test already and will detail how I did so below. However I would like to figure out how to prepare my data for a more complex learning algorithm such as those offered by sci-kit. &#xD;&#xA;&#xD;&#xA;(many of my techniques were learned from this [article][1])&#xD;&#xA;&#xD;&#xA;First I used [KMC][2] to split each fasta file for both R and S genomes into k-mers (I don't have enough &quot;reputation&quot; so I can't use &gt;2 links but k-mers are essentially genetic sequences of k length). The output for KMC was a dump of k-mers present in a genome.&#xD;&#xA;&#xD;&#xA;Example: kmc dump&#xD;&#xA;&#xD;&#xA;    AAAAAAAAAACGAATGTACACAATCGAA	1&#xD;&#xA;    AAAAAAAAACGAATGTACACAATCGAAC	1&#xD;&#xA;    AAAAAAAAATCAAATCCTGACTATTTAG	1&#xD;&#xA;    AAAAAAAAATCGGTCAATTCATTAAAAG	1&#xD;&#xA;    AAAAAAAACAAACATGAAGACCTTGTTA	1&#xD;&#xA;    AAAAAAAACGAATGTACACAATCGAACA	1&#xD;&#xA;    AAAAAAAACGTGTTAAAGTGAATCACAC	1&#xD;&#xA;&#xD;&#xA;I then merged the data into one binary matrix which had columns as genomes and rows as k-mers. &#xD;&#xA;&#xD;&#xA;Example: 1 resistant genome and 1 susceptible genome&#xD;&#xA;&#xD;&#xA;    kmer    R_1    S_1&#xD;&#xA;    AAAAAAAAAACGAATGTACACAATCGAA	1	0&#xD;&#xA;    AAAAAAAAACGAATGTACACAATCGAAC	1	0&#xD;&#xA;    AAAAAAAAATCAAATCCTGACTATTTAG	1	0&#xD;&#xA;    AAAAAAAAATCGGTCAATTCATTAAAAG	1	0&#xD;&#xA;    AAAAAAAAATTCCCTTCTAATCTTGAAT	0	1&#xD;&#xA;    AAAAAAAACAAAAATTATATAAAGCGAA	0	1&#xD;&#xA;    AAAAAAAACAAACATGAAGACCTTGTTA	1	1&#xD;&#xA;    AAAAAAAACAACCACCCATACATTGAGT	0	1&#xD;&#xA;    AAAAAAAACCCTTACAACAAATATGTAA	0	1&#xD;&#xA;&#xD;&#xA;This is the data I have been working with and would like to analyze further with sci kit or other classification algorithms. Essentially each k-mer is a feature whose correlation with resistance is based on its presence in R vs S genomes. This is where I would like help transitioning this data into a dataset format suitable for sci kit. &#xD;&#xA;&#xD;&#xA;I conducted a Fisher Exact test on each k-mer using R. For each k-mer a 2x2 matrix was created with the first column as resistant and the second column as susceptible. The two rows were present or not. So four numbers (# times k-mer was present in R, # in susceptible, # not in R, # not in S).&#xD;&#xA;&#xD;&#xA;          R    S&#xD;&#xA;    yes   #    #&#xD;&#xA;    no    #    #&#xD;&#xA;&#xD;&#xA;I used the following code in R:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    phenotype = as.numeric(grepl('^R_', colnames(raw_kmer_table)[2:ncol(raw_kmer_table)]))&#xD;&#xA;    fe_results = apply(kmer_pres_abs_matrix, 1, &#xD;&#xA;                   FUN = function(row) {&#xD;&#xA;                     fe_mat = matrix(0, ncol = 2, nrow = 2)&#xD;&#xA;                     fe_mat[1,1] = sum(row == 1 &amp; phenotype == 1)&#xD;&#xA;                     fe_mat[1,2] = sum(row == 1 &amp; phenotype == 0)&#xD;&#xA;                     fe_mat[2,1] = sum(row == 0 &amp; phenotype == 1)&#xD;&#xA;                     fe_mat[2,2] = sum(row == 0 &amp; phenotype == 0)&#xD;&#xA;                     &#xD;&#xA;                     fe = fisher.test(fe_mat)&#xD;&#xA;                     return(fe$p.value)&#xD;&#xA;                   }&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;Now that you have read how my data is currently formatted I would like to hear suggestions on how I can fit this data into a more complex test using sci kit or another resource. Algorithms I am interested in using are any you see fit and previously field-tested algorithms such as adaboost and forest learning algorithms. Also please ask me any clarifications or things I may have left out of this post!&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/pubmed/27297683&#xD;&#xA;  [2]: http://sun.aei.polsl.pl/REFRESH/index.php?page=projects&amp;project=kmc&amp;subpage=about&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/K-mer" />
  <row Id="2178" PostHistoryTypeId="1" PostId="702" RevisionGUID="0864ba56-abf0-4129-82c9-c5255a66b77c" CreationDate="2017-06-13T15:38:05.440" UserId="842" Text="Preparing binary matrix data for Scikit classification algorithms" />
  <row Id="2179" PostHistoryTypeId="3" PostId="702" RevisionGUID="0864ba56-abf0-4129-82c9-c5255a66b77c" CreationDate="2017-06-13T15:38:05.440" UserId="842" Text="&lt;k-mer&gt;&lt;biopython&gt;&lt;machine-learning&gt;" />
  <row Id="2180" PostHistoryTypeId="2" PostId="703" RevisionGUID="8704fdff-b557-418c-8e49-3e1c0994d18c" CreationDate="2017-06-13T16:02:47.477" UserId="208" Text="We sort different populations of blood cells using a number of fluorescent flow cytometry markers and then sequence RNA. We want to see what the transcriptome tells us about the similarity and relation between these cells. In my experience on bulk RNA-seq data, there is a very good agreement between flow cytometry and mRNA expression for the markers. It's good to remember we sort using a few markers only, while there are hundreds if not thousands of cell surface proteins.&#xD;&#xA;&#xD;&#xA;Should we exclude genes of these sorting (CD) marker proteins when we perform PCA or other types of clustering?&#xD;&#xA;&#xD;&#xA;The argument is that after sorting, these genes may dominate clustering results, even if the rest of the transcriptome would tell otherwise, and thus falsely confirm similarity relations inferred from sorting.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Should we at least check the influence of these genes on clustering results?&#xD;&#xA;" />
  <row Id="2181" PostHistoryTypeId="1" PostId="703" RevisionGUID="8704fdff-b557-418c-8e49-3e1c0994d18c" CreationDate="2017-06-13T16:02:47.477" UserId="208" Text="Should the cell sorting marker genes be excluded during clustering?" />
  <row Id="2182" PostHistoryTypeId="3" PostId="703" RevisionGUID="8704fdff-b557-418c-8e49-3e1c0994d18c" CreationDate="2017-06-13T16:02:47.477" UserId="208" Text="&lt;rna-seq&gt;&lt;cell-line&gt;&lt;clustering&gt;" />
  <row Id="2183" PostHistoryTypeId="2" PostId="704" RevisionGUID="ae9284b2-87c7-4bd5-b973-80fa976a6391" CreationDate="2017-06-13T16:03:16.817" UserId="787" Text="Actually this is one of the main problems you have when analyzing scRNA-seq data, and there is no established method for dealing with this. Different (dedicated) algorithms deal with it in different ways, but mostly you rely on how good the error modelling of your software is (a great read is the [review][1] by Wagner, Regev &amp; Yosef, esp. the section on &quot;False negatives and overamplification&quot;). There are a couple of options:&#xD;&#xA;&#xD;&#xA; - You can **impute values**, i.e. fill in the gaps on technical zeros. [CIDR][2] and [scImpute][3] do it directly. [MAGIC][4] and [ZIFA][5] project cells into a lower-dimensional space and use their similarity there to decide how to fill in the blanks.&#xD;&#xA; - Some people straight up **exclude genes** that are expressed in very low numbers. I can't give you citations off the top of my head, but many trajectory inference algorithms like [monocle2][6] and [SLICER][7] have heuristics to choose informative genes for their analysis.&#xD;&#xA; - If the method you use for analysis doesn't model gene expression explicitly but **uses some other distance method** to quantify similarity between cells (like cosine distance, euclidean distance, correlation), then the noise introduced by dropout can be covered by the signal of genes that are highly expressed. Note that this is dangerous, as genes that are highly expressed are not necessarily informative.&#xD;&#xA; - **ERCC spike** ins can help you reduce technical noise, but I am not familiar with the Chromium protocol so maybe it doesn't apply there (?)&#xD;&#xA;&#xD;&#xA;since we are speaking about noise, you might consider using a protocol with unique molecular identifiers. They remove the amplification errors almost completely, at least for the transcripts that you capture...&#xD;&#xA;&#xD;&#xA;EDIT: Also, I would highly recommend using something more advanced than PCA to do the analysis. Software like the above-mentioned Monocle or [destiny][8] is easy to operate and increases the power of your analysis considerably.&#xD;&#xA;&#xD;&#xA;  [1]: http://dx.doi.org/10.1038/nbt.3711&#xD;&#xA;  [2]: https://github.com/VCCRI/CIDR&#xD;&#xA;  [3]: https://github.com/Vivianstats/scImpute&#xD;&#xA;  [4]: http://biorxiv.org/content/early/2017/02/25/111591&#xD;&#xA;  [5]: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0805-z&#xD;&#xA;  [6]: https://bioconductor.org/packages/release/bioc/html/monocle.html&#xD;&#xA;  [7]: https://github.com/jw156605/SLICER&#xD;&#xA;  [8]: https://bioconductor.org/packages/release/bioc/html/destiny.html" />
  <row Id="2185" PostHistoryTypeId="5" PostId="605" RevisionGUID="2fb33ee0-17f6-4c42-9a9b-20d4f9270177" CreationDate="2017-06-13T16:21:27.300" UserId="57" Comment="deleted hopes, added links as hyperlinks to text" Text="Most tools I know of looks for enrichment of specific motifs - but that requires that you have a set of sequences which are of special interest and a background set to test against.&#xD;&#xA;&#xD;&#xA;Is that your case?&#xD;&#xA;&#xD;&#xA;Update after comments 12th June 2017.&#xD;&#xA;&#xD;&#xA;You could try the [meme suite][1] more specifically [the motif finder][2] &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://meme-suite.org/&#xD;&#xA;  [2]: http://meme-suite.org/tools/meme" />
  <row Id="2186" PostHistoryTypeId="2" PostId="705" RevisionGUID="061cc96b-6689-4072-aa3a-4ca78193c8d2" CreationDate="2017-06-13T16:24:28.293" UserId="272" Text="I am trying to understand how the MD:Z tag is used. The following is from the [SAM Optional Fields Specification][1], which gives an example but is not thorough. &#xD;&#xA;&#xD;&#xA;&gt; The MD field aims to achieve SNP/indel calling without looking at the&#xD;&#xA;&gt; reference. For example, a string ‘10A5^AC6’ means from the leftmost&#xD;&#xA;&gt; reference base in the alignment, there are 10 matches followed by an A&#xD;&#xA;&gt; on the reference which is different from the aligned read base; the&#xD;&#xA;&gt; next 5 reference bases are matches followed by a 2bp deletion from the&#xD;&#xA;&gt; reference; the deleted sequence is AC; the last 6 bases are matches.&#xD;&#xA;&gt; The MD field ought to match the CIGAR string.&#xD;&#xA;&#xD;&#xA;Suppose I have a read that I want to soft clip at both ends. If the read starts with CIGAR `100M` and MD `50G49`, and I want to change the CIGAR to `7S86M7S`, what should the MD field become? &#xD;&#xA;&#xD;&#xA;Is there a more complete explanation of what appears in the MD tag field?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://samtools.github.io/hts-specs/SAMtags.pdf" />
  <row Id="2187" PostHistoryTypeId="1" PostId="705" RevisionGUID="061cc96b-6689-4072-aa3a-4ca78193c8d2" CreationDate="2017-06-13T16:24:28.293" UserId="272" Text="How should the SAM MD tag match the CIGAR string?" />
  <row Id="2188" PostHistoryTypeId="3" PostId="705" RevisionGUID="061cc96b-6689-4072-aa3a-4ca78193c8d2" CreationDate="2017-06-13T16:24:28.293" UserId="272" Text="&lt;file-formats&gt;&lt;sam&gt;&lt;cigar&gt;" />
  <row Id="2191" PostHistoryTypeId="2" PostId="707" RevisionGUID="4ca6590b-11a4-49cc-bbc8-e7459e231aed" CreationDate="2017-06-13T16:50:57.050" UserId="77" Text="The MD string doesn't apply to soft or hard clipped regions. So you example read becomes `43G42`. " />
  <row Id="2192" PostHistoryTypeId="5" PostId="707" RevisionGUID="9da4cae6-cdab-4bfc-90b1-8ad88b7e92b0" CreationDate="2017-06-13T17:01:26.013" UserId="77" Comment="added 282 characters in body" Text="The MD string doesn't apply to soft or hard clipped regions,. so your example read becomes `43G42`. Since variant calling using something simplistic like this is only ever going to use the aligned portion, I guess there was never much reason to bother with adding in soft-clipping information. This also means that you need to parse the CIGAR string if you're using the MD string." />
  <row Id="2193" PostHistoryTypeId="2" PostId="708" RevisionGUID="0eb42447-dc4a-405c-b06f-bd3de8b7a3bb" CreationDate="2017-06-13T17:07:48.073" UserId="292" Text="I generate bigWig files using `bamCoverage` from deeptools, in part for my colleagues to visualize their mapped libraries in the IGV viewer.&#xD;&#xA;&#xD;&#xA;A problem is that the displayed track name is apparently the file name, which is not convenient because some files for different libraries have the same name. The reason is that the files are organized with one directory per library. I would prefer to avoid changing the way I name and organize the files.&#xD;&#xA;&#xD;&#xA;**Is there a way to edit my bigWig files in order to have control of the track name independently from the file name?**&#xD;&#xA;&#xD;&#xA;As far as I understand, bigWig format derives from wiggle format, which have a track definition line that can contain a `name` and a `definition` attribute: &lt;https://genome.ucsc.edu/goldenpath/help/customTrack.html#TRACK&gt;.&#xD;&#xA;&#xD;&#xA;Maybe there is a way to convert a bigWig file back to wiggle, edit the track definition, and then rebuild an updated bigWig file." />
  <row Id="2194" PostHistoryTypeId="1" PostId="708" RevisionGUID="0eb42447-dc4a-405c-b06f-bd3de8b7a3bb" CreationDate="2017-06-13T17:07:48.073" UserId="292" Text="Customizing bigWig file" />
  <row Id="2195" PostHistoryTypeId="3" PostId="708" RevisionGUID="0eb42447-dc4a-405c-b06f-bd3de8b7a3bb" CreationDate="2017-06-13T17:07:48.073" UserId="292" Text="&lt;file-formats&gt;&lt;format-conversion&gt;&lt;bigwig&gt;&lt;visualization&gt;&lt;genome-browser&gt;" />
  <row Id="2196" PostHistoryTypeId="2" PostId="709" RevisionGUID="dfd633d1-77e4-46df-8786-8fbd3bfc2347" CreationDate="2017-06-13T17:15:34.113" UserId="842" Text="When I use blastn and prokka (I will detail exactly how I did so below) on a 2.8 million bp fasta file I get output start/end numbers that do not seem to cover the entire genome. &#xD;&#xA;&#xD;&#xA;Starting with a .fna genome such as genome.fna I ...&#xD;&#xA;&#xD;&#xA;1 - blastn&#xD;&#xA;&#xD;&#xA;Searched the genome for 3135 different 28-mers using BLASTn.&#xD;&#xA;&#xD;&#xA;    makeblastdb -in genome.fna -dbtype nucl -parse_seqids -out ./output/genome&#xD;&#xA;*next command in python*&#xD;&#xA;&#xD;&#xA;    blast_tsv = NcbiblastnCommandline(query=Q, db=DB, perc_identity=100, outfmt=6, out=(OUT))&#xD;&#xA;        stdout, stderr = blast_tsv()&#xD;&#xA;*Q is this list of k-mers. DB is the database created*&#xD;&#xA;&#xD;&#xA;   This outputs a list of search results but the important thing is that none of the start/end (columns 7 and 8) integers were greater than 100,000 yet the entire genome is 2.8 million base pairs long. I can provide the link to this file in the comments. Does this have to do with blastn stopping searching after finding one match? And if so how can I tell blastn to search for every match for each k-mer? (I'm open to using other blastn programs other than biopython)&#xD;&#xA;&#xD;&#xA;2 - prokka&#xD;&#xA;&#xD;&#xA;I used prokka to create gff files for the genome. &#xD;&#xA;&#xD;&#xA;    prokka-1.12/bin/prokka —setupdb&#xD;&#xA;    prokka-1.12/bin/prokka -kingdom Bacteria -rfam -outdir ./prokka_database/genome -force -prefix &quot;${genome/.fna/}&quot; genome.fna&#xD;&#xA;&#xD;&#xA;Th gff file produced by this command only described genes with start and end positions less than 200k. I can provide the link to this file as well. " />
  <row Id="2197" PostHistoryTypeId="1" PostId="709" RevisionGUID="dfd633d1-77e4-46df-8786-8fbd3bfc2347" CreationDate="2017-06-13T17:15:34.113" UserId="842" Text="Why do BLASTn and prokka not seem to be searching the whole fasta file?" />
  <row Id="2198" PostHistoryTypeId="3" PostId="709" RevisionGUID="dfd633d1-77e4-46df-8786-8fbd3bfc2347" CreationDate="2017-06-13T17:15:34.113" UserId="842" Text="&lt;annotation&gt;&lt;genome&gt;&lt;k-mer&gt;&lt;biopython&gt;&lt;blast&gt;" />
  <row Id="2199" PostHistoryTypeId="2" PostId="710" RevisionGUID="d4a82283-58fc-4a5f-8304-cb2259a17334" CreationDate="2017-06-13T17:41:31.517" UserId="77" Text="There's no equivalent to the wiggle header in bigWig (or bigBed) files, which is why UCSC uses the file name. This is actually the reason for the track line stuff that you linked to, since you can then specify a name and just point to where the bigWig (or other format) file is on the internet.&#xD;&#xA;&#xD;&#xA;BTW, you can certainly convert your bigWig to wiggle, add the track line, and convert it back with UCSC tools (e.g., `bigWigToWig` and `wigToBigWig`). It shouldn't change the track name when you display it in UCSC (though if it does, then let me know, since I'll modify pyBigWig and libBigWig to support that then)." />
  <row Id="2200" PostHistoryTypeId="2" PostId="711" RevisionGUID="6277bdb9-8662-4b3c-b39a-01a1293bac18" CreationDate="2017-06-13T18:22:13.263" UserId="848" Text="With a k-mer size of 28 it shouldn't be finding that many matches. And the prokka results are suspicious as well. Maybe you have multiple contigs (none larger than 100kb) in that file? What is the result of &#xD;&#xA;&#xD;&#xA;&lt;pre&gt;&#xD;&#xA;grep ^'&gt;' fasta_file | wc -l&#xD;&#xA;&lt;/pre&gt;&#xD;&#xA;? This would show how many contigs you have in the file.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2201" PostHistoryTypeId="2" PostId="712" RevisionGUID="568e7adc-e9ec-4838-b3b3-2560b1fdfb68" CreationDate="2017-06-13T18:46:27.060" UserId="235" Text="Depending on how much effort you wish to put into this, here is one suggestion I have see used before (uses more than just R)&#xD;&#xA;&#xD;&#xA; 1. Take your gene models and collapse the introns down to maybe 100bp&#xD;&#xA;&#xD;&#xA; 2. Shift each gene into the nearest genomic space at least 1 kp from a genomic annotation.&#xD;&#xA;&#xD;&#xA; 3. Calculate the FPKM distribution of this shifted null set&#xD;&#xA;&#xD;&#xA; 4. From these two distributions it should be possible to calculate a [local FDR][1] for expression.&#xD;&#xA;&#xD;&#xA; 5. Carry out differential expression analysis as described in [Konrad's answer][2]&#xD;&#xA;&#xD;&#xA; 6. Subset the differentially expressed genes to only consider those that are not expressed in the control condition&#xD;&#xA;&#xD;&#xA; 7. You might want to also recalculate the FDRs for differential expression as you are only considering a subset of the tests you might have. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://cran.r-project.org/web/packages/locfdr/vignettes/locfdr-example.pdf&#xD;&#xA;  [2]: https://bioinformatics.stackexchange.com/a/690/235" />
  <row Id="2202" PostHistoryTypeId="2" PostId="713" RevisionGUID="bdf5c91b-e213-4165-9ffa-4e511b9bddeb" CreationDate="2017-06-13T18:55:36.833" UserId="640" Text="You looked at this https://github.com/rcsb/mmtf-python &#xD;&#xA;&#xD;&#xA;&quot;The python implementation of the MMTF API, decoder and encoder.&quot;" />
  <row Id="2204" PostHistoryTypeId="5" PostId="667" RevisionGUID="eadc37d1-a592-4498-a5fb-26067cda1f8a" CreationDate="2017-06-13T19:07:49.597" UserId="831" Comment="Added a few more web sites" Text="###Google Genomics###&#xD;&#xA;Google has an API called [Google Genomics][1].&#xD;&#xA;&#xD;&#xA;###SNPedia###&#xD;&#xA;&quot;SNPedia is a wiki investigating human genetics.&quot;&#xD;&#xA;snpedia.com&#xD;&#xA;&#xD;&#xA;###Promethease###&#xD;&#xA;&quot;Promethease is a literature retrieval system that builds a personal DNA report&quot;&#xD;&#xA;promethease.com&#xD;&#xA;&#xD;&#xA;###DNA Land###&#xD;&#xA;&quot;Compare DNA with reference data from different populations&quot;&#xD;&#xA;dna.land&#xD;&#xA;&#xD;&#xA;###The CyDAS Project###&#xD;&#xA;And, there's the CyDAS project which has an API that can analyze ISCN formulae. Per their [web site][2]: their API &quot;lets you analyze a Karyotype for virtually all information which can be extracted from karyotypes and the rearrangements therein: gains and losses of chromosomal material, break points, junctions...&quot; It's a **free** service, but I don't know how up to date it is.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://cloud.google.com/genomics&#xD;&#xA;  [2]: http://www.cydas.org" />
  <row Id="2205" PostHistoryTypeId="2" PostId="714" RevisionGUID="cbced8b9-3aef-4017-8eb0-2bc320ba180d" CreationDate="2017-06-13T19:14:44.307" UserId="235" Text="I don't know about tools, but i've used the following python code to calculate the ratio of reads that overlap the 5' or 3' ends of introns or that are spliced. We sum these across all introns in a gene set (we actaully use this for iCLIP analysis to see if RNA binding proteins bind pre-mRNA or spliced RNA).&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA; &#xD;&#xA; &#xD;&#xA;    import pysam&#xD;&#xA;    from collections import Counter&#xD;&#xA;    from CGAT import GTF, IOTools&#xD;&#xA;&#xD;&#xA;    def calculateSplicingIndex(bamfile, gtffile, outfile):&#xD;&#xA;    &#xD;&#xA;        bamfile = pysam.AlignmentFile(bamfile)&#xD;&#xA;    &#xD;&#xA;        counts = Counter()&#xD;&#xA;    &#xD;&#xA;        for transcript in GTF.transcript_iterator(&#xD;&#xA;                GTF.iterator(IOTools.openFile(gtffile))):&#xD;&#xA;    &#xD;&#xA;            introns = GTF.toIntronIntervals(transcript)&#xD;&#xA;            &#xD;&#xA;            for intron in introns:&#xD;&#xA;                reads = bamfile.fetch(&#xD;&#xA;                    reference=transcript[0].contig,&#xD;&#xA;                    start=intron[0], end=intron[1])&#xD;&#xA;                &#xD;&#xA;                for read in reads:&#xD;&#xA;                    if 'N' in read.cigarstring:&#xD;&#xA;                        blocks = read.get_blocks()&#xD;&#xA;                        starts, ends = zip(*blocks)&#xD;&#xA;                        if intron[0] in ends and intron[1] in starts:&#xD;&#xA;                            counts[&quot;Exon_Exon&quot;] += 1&#xD;&#xA;                        else:&#xD;&#xA;                            counts[&quot;spliced_uncounted&quot;] += 1&#xD;&#xA;                    elif (read.reference_start &lt;= intron[0] - 3&#xD;&#xA;                          and read.reference_end &gt;= intron[0] + 3):&#xD;&#xA;                        if transcript[0].strand == &quot;+&quot;:&#xD;&#xA;                            counts[&quot;Exon_Intron&quot;] += 1&#xD;&#xA;                        else:&#xD;&#xA;                            counts[&quot;Intron_Exon&quot;] += 1&#xD;&#xA;                    elif (read.reference_start &lt;= intron[1] - 3&#xD;&#xA;                          and read.reference_end &gt;= intron[1] + 3):&#xD;&#xA;                        if transcript[0].strand == &quot;+&quot;:&#xD;&#xA;                            counts[&quot;Intron_Exon&quot;] += 1&#xD;&#xA;                        else:&#xD;&#xA;                            counts[&quot;Exon_Intron&quot;] += 1&#xD;&#xA;                    else:&#xD;&#xA;                        counts[&quot;unspliced_uncounted&quot;] += 1&#xD;&#xA;&#xD;&#xA;        header = [&quot;Exon_Exon&quot;,&#xD;&#xA;                  &quot;Exon_Intron&quot;,&#xD;&#xA;                  &quot;Intron_Exon&quot;,&#xD;&#xA;                  &quot;spliced_uncounted&quot;,&#xD;&#xA;                  &quot;unspliced_uncounted&quot;]&#xD;&#xA;    &#xD;&#xA;        with IOTools.openFile(outfile, &quot;w&quot;) as outf:&#xD;&#xA;    &#xD;&#xA;            outf.write(&quot;\t&quot;.join(header)+&quot;\n&quot;)&#xD;&#xA;            outf.write(&quot;\t&quot;.join(map(str, [counts[col] for col in header]))&#xD;&#xA;                       + &quot;\n&quot;)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Unfortunately this uses a bunch of libraries you may or may not have, including CGAT (for the GTF parser and IOTools package) and pysam.&#xD;&#xA;&#xD;&#xA;Once you've got these statistics you can calculate the &quot;splicing index&quot; as &#xD;&#xA;the log2 ratio of 2 times the number of spliced reads divided by the number of reads overlapping the 3' and 5' ends of introns. &#xD;&#xA;" />
  <row Id="2206" PostHistoryTypeId="5" PostId="675" RevisionGUID="6b55a129-b0a0-44ca-9698-48b3b7c43c7e" CreationDate="2017-06-13T19:21:01.190" UserId="73" Comment="clarify sequencing failure mode" Text="I've just been generating data like this, so can tell you about why/how missing calls are created in my dataset. There are two main reasons:&#xD;&#xA;&#xD;&#xA;1. Sequencing failure&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;When reads don't map across the variant region, then it's impossible to accurately determine a genotype for that region. This will commonly happen just outside the borders of the selected regions for exome sequencing, but can also happen through natural random sampling of the genome, or through repeated sequences coupled with systematic error in either the assembled genome or the sequencer. In this case, it would be expected that called variant regions including missing data would be lower coverage. It may be helpful to plot the coverage within a particular region to work out if this is likely to be happening.&#xD;&#xA;&#xD;&#xA;2. Merged non-variant data&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;Variant calling can be processed in two different modes, one which takes into account *all* samples when doing calling, and another which does the calling one sample at a time, followed by subsequent merging of the results. Multi-sample variant calling is more accurate, but computational limits (or experimental design) might mean that it is more appropriate for samples to be called one-by-one. It's fairly common (for the purpose of preserving space and computational power) for calling algorithms to only report variant information so if a single sample is identical to the reference, and that sample is the only one being called, then the variant and coverage information for that sample will be lost. When samples are merged after being separately called, the declared call for the &quot;same as reference&quot; samples will be set to missing. In this case, it would be generally expected that the other called samples would have high-coverage within the variant region." />
  <row Id="2207" PostHistoryTypeId="2" PostId="715" RevisionGUID="f4aa426f-51e6-4b7a-9582-da152346b8b3" CreationDate="2017-06-13T19:28:19.813" UserId="313" Text="The [MEME Suite][1] web site contains a collection of tools for motif analysis (I'm one of the maintainers). You can also download and build command line tools for a local installation.&#xD;&#xA; &#xD;&#xA;For your first goal you could use [MEME][2] and select the &quot;Any number of repetitions model&quot; (ANR). For your second goal, you'd use MEME with the &quot;Zero or One Occurrences Per Sequence&quot; (ZOOPS) model. For your third goal you could use FIMO (Find Individual Motif Occurrences), and one or more of the motif databases provided on the [software and database download page][3].&#xD;&#xA;&#xD;&#xA;For short motifs (trinculeotides), you may want to use [DREME][4] rather than MEME. DREME is stronger than MEME at identifying short motifs.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://meme-suite.org&#xD;&#xA;  [2]: http://meme-suite.org/tools/meme&#xD;&#xA;  [3]: http://meme-suite.org/doc/download.html?man_type=web&#xD;&#xA;  [4]: http://meme-suite.org/tools/dreme" />
  <row Id="2208" PostHistoryTypeId="2" PostId="716" RevisionGUID="cf9dcbd3-18ab-4c02-9c60-746884d0722d" CreationDate="2017-06-13T19:39:13.280" UserId="235" Text="I am trying to calculate the mappability adjusted length of introns as described by [Boutz et al][1].&#xD;&#xA;&#xD;&#xA;Briefly for each intron I wish to calculate the length minus the number of bases that are non-uniquely mappable. Mappability tracks can be downloaded as BigWigs from [here][2]. The the score at each base is 1/number of mapping positions, so 1 indicates a read can be uniquely mapped at this location. &#xD;&#xA;&#xD;&#xA;I am struggling to do this computation in either a reasonable amount of time, or a reasonable amount of memory.&#xD;&#xA;&#xD;&#xA;First I tried importing the whole bigwig:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: r --&gt;&#xD;&#xA;    &#xD;&#xA;    library(rtracklayer)&#xD;&#xA;    mappability &lt;- import(BigWigFile(&quot;wgEncodeCrgMapabilityAlign50mer.bigWig&quot;),&#xD;&#xA;                          as=&quot;NumericList&quot;,&#xD;&#xA;                          selection=BigWigSelection(intron_ranges))&#xD;&#xA;&#xD;&#xA;where `intron_ranges` is a `GRanges` object with the introns (about 800,000 of them).&#xD;&#xA;&#xD;&#xA;This used **way** too much memory, and soon caused by machine to fall over.&#xD;&#xA;&#xD;&#xA;Second I tried processing one intron at a time:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    mappability_file = BigWigFile(&quot;wgEncodeCrgMapabilityAlign50mer.bigWig&quot;)&#xD;&#xA;    effective_length &lt;- function (gr) {&#xD;&#xA;      intron_selection &lt;- BigWigSelection(gr)&#xD;&#xA;      scores &lt;- import(mappability_file, as=&quot;NumericList&quot;, selection=intron_selection)&#xD;&#xA;      non_unique &lt;- sum(scores[[1]] &lt; 1.0)&#xD;&#xA;      eff_len = width(gr)[1] - non_unique&#xD;&#xA;      return(eff_len)&#xD;&#xA;    }    &#xD;&#xA;&#xD;&#xA;    mappability &lt;- sapply(intron_ranges, effective_length)&#xD;&#xA;&#xD;&#xA;This has been running for hours and shows no sign of finishing. &#xD;&#xA;&#xD;&#xA;Is there a way to do this that uses less than, say 4GB of RAM, but finishes in say less than 30 minutes? It feels like there should be. &#xD;&#xA;&#xD;&#xA;  [1]: http://genesdev.cshlp.org/content/29/1/63.abstract&#xD;&#xA;  [2]: http://genome.ucsc.edu/cgi-bin/hgFileUi?db=hg19&amp;g=wgEncodeMapability" />
  <row Id="2209" PostHistoryTypeId="1" PostId="716" RevisionGUID="cf9dcbd3-18ab-4c02-9c60-746884d0722d" CreationDate="2017-06-13T19:39:13.280" UserId="235" Text="Time and memory effiicent processing of many intervals for a bigwig file" />
  <row Id="2210" PostHistoryTypeId="3" PostId="716" RevisionGUID="cf9dcbd3-18ab-4c02-9c60-746884d0722d" CreationDate="2017-06-13T19:39:13.280" UserId="235" Text="&lt;r&gt;&lt;bigwig&gt;" />
  <row Id="2211" PostHistoryTypeId="5" PostId="716" RevisionGUID="91778964-ef01-4f42-ac47-5a0cb910b4f8" CreationDate="2017-06-13T19:52:15.517" UserId="235" Comment="Removed R tag and explained I'm happy with solutions using a range of tools" Text="I am trying to calculate the mappability adjusted length of introns as described by [Boutz et al][1].&#xD;&#xA;&#xD;&#xA;Briefly for each intron I wish to calculate the length minus the number of bases that are non-uniquely mappable. Mappability tracks can be downloaded as BigWigs from [here][2]. The the score at each base is 1/number of mapping positions, so 1 indicates a read can be uniquely mapped at this location. &#xD;&#xA;&#xD;&#xA;I am struggling to do this computation in either a reasonable amount of time, or a reasonable amount of memory.&#xD;&#xA;&#xD;&#xA;First I tried importing the whole bigwig:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: r --&gt;&#xD;&#xA;    &#xD;&#xA;    library(rtracklayer)&#xD;&#xA;    mappability &lt;- import(BigWigFile(&quot;wgEncodeCrgMapabilityAlign50mer.bigWig&quot;),&#xD;&#xA;                          as=&quot;NumericList&quot;,&#xD;&#xA;                          selection=BigWigSelection(intron_ranges))&#xD;&#xA;&#xD;&#xA;where `intron_ranges` is a `GRanges` object with the introns (about 800,000 of them).&#xD;&#xA;&#xD;&#xA;This used **way** too much memory, and soon caused by machine to fall over.&#xD;&#xA;&#xD;&#xA;Second I tried processing one intron at a time:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    mappability_file = BigWigFile(&quot;wgEncodeCrgMapabilityAlign50mer.bigWig&quot;)&#xD;&#xA;    effective_length &lt;- function (gr) {&#xD;&#xA;      intron_selection &lt;- BigWigSelection(gr)&#xD;&#xA;      scores &lt;- import(mappability_file, as=&quot;NumericList&quot;, selection=intron_selection)&#xD;&#xA;      non_unique &lt;- sum(scores[[1]] &lt; 1.0)&#xD;&#xA;      eff_len = width(gr)[1] - non_unique&#xD;&#xA;      return(eff_len)&#xD;&#xA;    }    &#xD;&#xA;&#xD;&#xA;    mappability &lt;- sapply(intron_ranges, effective_length)&#xD;&#xA;&#xD;&#xA;This has been running for hours and shows no sign of finishing. &#xD;&#xA;&#xD;&#xA;Is there a way to do this that uses less than, say 4GB of RAM, but finishes in say less than 30 minutes? It feels like there should be. &#xD;&#xA;&#xD;&#xA;I'm not tied to R, just what I've tried so far. Happy with answers using binary packages, common shell tools, python or R. &#xD;&#xA;&#xD;&#xA;  [1]: http://genesdev.cshlp.org/content/29/1/63.abstract&#xD;&#xA;  [2]: http://genome.ucsc.edu/cgi-bin/hgFileUi?db=hg19&amp;g=wgEncodeMapability" />
  <row Id="2212" PostHistoryTypeId="6" PostId="716" RevisionGUID="91778964-ef01-4f42-ac47-5a0cb910b4f8" CreationDate="2017-06-13T19:52:15.517" UserId="235" Comment="Removed R tag and explained I'm happy with solutions using a range of tools" Text="&lt;bigwig&gt;" />
  <row Id="2213" PostHistoryTypeId="2" PostId="717" RevisionGUID="4dbf30f7-655d-4434-a6bb-77fec1acdeb2" CreationDate="2017-06-13T20:43:29.640" UserId="77" Text="Assuming you can make a BED file of introns, you can then use the `pyBigWig` module in python:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: python --&gt;&#xD;&#xA;&#xD;&#xA;    import pyBigWig&#xD;&#xA;    import numpy&#xD;&#xA;&#xD;&#xA;    bw = pyBigWig.open(&quot;some file.bw&quot;)&#xD;&#xA;    bed = open(&quot;introns.bed&quot;)&#xD;&#xA;    for line in bed:&#xD;&#xA;        cols = line.strip().split(&quot;\t&quot;)&#xD;&#xA;        vals = bw.values(cols[0], int(cols[1]), int(cols[2]), numpy=True)&#xD;&#xA;        effLen = cols[2] - cols[1] - (vals &lt; 1.0).sum()&#xD;&#xA;        # Do something with this.&#xD;&#xA;&#xD;&#xA;One can make that parallel (with the deeptools API) should it prove too slow.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2214" PostHistoryTypeId="2" PostId="718" RevisionGUID="1f972dea-1d63-4873-acbb-c81f2b8697fc" CreationDate="2017-06-13T20:46:40.447" UserId="47" Text="It really depends on what you are trying to do, but here are a few services that I know of.&#xD;&#xA;&#xD;&#xA;- [GATK on Google Genomics Cloud](https://cloud.google.com/genomics/v1alpha2/gatk): Google and the Broad offer a cloud instance tailored to GATK pipelines.&#xD;&#xA;- [Genomics on Amazon Web Services](https://aws.amazon.com/health/genomics/): I don't think there is anything that makes this unique, but Amazon offers some resources to help get started with genomics/life sciences-centered cloud solutions.&#xD;&#xA;- [Illumina Bioinformatics](https://www.illumina.com/informatics.html): Illumina is working on a whole suite of bioinformatics software for the cloud.&#xD;&#xA;- [Cancer Genomics Cloud](http://www.cancergenomicscloud.org/): This is specific to cancer genomics, but I believe Seven Bridges allows you to push all sorts of data into the tool and analyze it.&#xD;&#xA; " />
  <row Id="2215" PostHistoryTypeId="5" PostId="717" RevisionGUID="85108657-ad7f-44c2-85ad-5a5c269078ad" CreationDate="2017-06-13T20:52:36.063" UserId="77" Comment="Expand on an alternate method" Text="Assuming you can make a BED file of introns, you can then use the `pyBigWig` module in python:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: python --&gt;&#xD;&#xA;&#xD;&#xA;    import pyBigWig&#xD;&#xA;    import numpy&#xD;&#xA;&#xD;&#xA;    bw = pyBigWig.open(&quot;some file.bw&quot;)&#xD;&#xA;    bed = open(&quot;introns.bed&quot;)&#xD;&#xA;    for line in bed:&#xD;&#xA;        cols = line.strip().split(&quot;\t&quot;)&#xD;&#xA;        vals = bw.values(cols[0], int(cols[1]), int(cols[2]), numpy=True)&#xD;&#xA;        effLen = cols[2] - cols[1] - (vals &lt; 1.0).sum()&#xD;&#xA;        # Do something with this.&#xD;&#xA;&#xD;&#xA;One can make that parallel (with the deeptools API) should it prove too slow. This will only use an appreciable amount of memory for very large introns, since there you need to store the value at each base. One could make this much more memory efficient by using `bw.intervals()` instead, but that'd require quite a bit more code (you'd need to determine the number of bases overlap per interval and then assume that either each base you want to query has an overlapping interval or, if not, keep track of that fact).&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2217" PostHistoryTypeId="5" PostId="715" RevisionGUID="33f12e22-ae96-436e-b5fb-1546ae0cfa0c" CreationDate="2017-06-13T22:20:11.820" UserId="313" Comment="added 577 characters in body" Text="The [MEME Suite][1] web site contains a collection of tools for motif analysis (I'm one of the maintainers). It contains two de novo motif discovery tools: [MEME][2] and [DREME][4]. Public web applications are provided, but you can also download and build command line tools for a local installation.&#xD;&#xA; &#xD;&#xA;For your first goal you could use [MEME][2] and select the &quot;Any number of repetitions model&quot; (ANR). For your second goal, you'd use MEME with the &quot;Zero or One Occurrences Per Sequence&quot; (ZOOPS) model. For your third goal you could use FIMO (Find Individual Motif Occurrences), and one or more of the motif databases provided on the [software and database download page][3].&#xD;&#xA;&#xD;&#xA;It sounds like your sequence data is about 30Mb. The MEME web application is limited to 60kb of sequence data, so you'd have to install a local copy of the MEME Suite. MEME would take a long time to analyze a 30Mb sequence database unless you have MPI configured, and lots of cores available. You might want to consider analyzing a randomly selected subset of your sequences. The running time of MEME grows as the cube of the number of sequences. &#xD;&#xA;&#xD;&#xA;For short motifs, you may want to use [DREME][4] rather than MEME. DREME is better than MEME at identifying short motifs, but is limited to motifs &lt;= 8 positions wide.&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;  [1]: http://meme-suite.org&#xD;&#xA;  [2]: http://meme-suite.org/tools/meme&#xD;&#xA;  [3]: http://meme-suite.org/doc/download.html?man_type=web&#xD;&#xA;  [4]: http://meme-suite.org/tools/dreme" />
  <row Id="2218" PostHistoryTypeId="2" PostId="719" RevisionGUID="798e6aa0-de1d-4c8c-a024-6d7be4e13391" CreationDate="2017-06-13T22:20:23.237" UserId="272" Text="From [SAM Optional Fields Specification][1] the NM field is &#xD;&#xA;&gt; Edit distance to the reference, including ambiguous bases but excluding clipping&#xD;&#xA;&#xD;&#xA;Assuming both the MD and CIGAR are present, is the edit distance simply the number of characters `[A-Z]` appearing in the MD field plus the number of bases inserted (`xI`, if any) from the CIGAR string? Are there any other complications? &#xD;&#xA;&#xD;&#xA;  [1]: https://samtools.github.io/hts-specs/SAMtags.pdf" />
  <row Id="2219" PostHistoryTypeId="1" PostId="719" RevisionGUID="798e6aa0-de1d-4c8c-a024-6d7be4e13391" CreationDate="2017-06-13T22:20:23.237" UserId="272" Text="Is the optional SAM NM field strictly computable from the MD and CIGAR?" />
  <row Id="2220" PostHistoryTypeId="3" PostId="719" RevisionGUID="798e6aa0-de1d-4c8c-a024-6d7be4e13391" CreationDate="2017-06-13T22:20:23.237" UserId="272" Text="&lt;file-formats&gt;&lt;sam&gt;&lt;cigar&gt;" />
  <row Id="2224" PostHistoryTypeId="5" PostId="673" RevisionGUID="9cb5ce62-f2bb-4f4a-97ad-1b25b284be9b" CreationDate="2017-06-13T23:23:39.460" UserId="823" Comment="added 1 character in body" Text="I have a list of transcription factors and I am interested in finding out which which genes might be transcribed as a result of the formation of transcription factor complexes with that transcription factor.&#xD;&#xA;&#xD;&#xA;Any ideas on good databases? I've seen that ENCODE hosts data from CHIPseq experiments, but I'm not sure if I can find conclusions regarding interactions from this site. I'm looking for a database with known or putative transcription/gene interactions.&#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="2225" PostHistoryTypeId="2" PostId="721" RevisionGUID="6ebbcaf4-4186-4997-98e3-1d2a6464b113" CreationDate="2017-06-14T01:38:01.883" UserId="853" Text="While better methods of evaluating your clusters would be to use an external dataset or a dataset with known truth, there are a variety of internal validation metrics that can be used to compare clustering solutions without another dataset.&#xD;&#xA;&#xD;&#xA;Here are a few metrics:&#xD;&#xA;&#xD;&#xA; - Davies-Bouldin Index&#xD;&#xA; - Calinski-Harabasz Index&#xD;&#xA; - Root-Mean-Square Standard Deviation&#xD;&#xA;&#xD;&#xA;Many more can be found in this clustering review: http://stke.sciencemag.org/content/9/432/re6&#xD;&#xA;&#xD;&#xA;These internal validation metrics grade your clustering solution based on three measures: compactness, connectedness, and separation. When using these metrics to compare clustering solutions, be sure to consider which metric is appropriate for your results as some algorithms work by optimizing certain measures.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2226" PostHistoryTypeId="2" PostId="722" RevisionGUID="65efa005-4e02-4f51-835c-365abe197fab" CreationDate="2017-06-14T03:16:07.773" UserId="163" Text="I have a dataset of Oxford Nanopore cDNA reads. Many of my reads are full-length or close to full-length transcripts, and I and am interested in examining alternative splicing. For this, I would like to begin by visualising my reads and comparing variants qualitatively.&#xD;&#xA;&#xD;&#xA;I have tried visualising my data in both IGV and SeqMonk, but neither has given a satisfying result.&#xD;&#xA;&#xD;&#xA;**IGV**&#xD;&#xA;&#xD;&#xA;[![IGV Visualisation of NOTCH2][1]][1]&#xD;&#xA;&#xD;&#xA;IGV shows some links between aligned sections, shown as fine blue lines and thick black lines. I cannot work out what the difference between these lines is, and even more confusingly, we have numerous alignments where two aligned exons come from the same sequence but are not joined by any line. This means that, to confirm or deny a spliced variant, I need to manually examine the read ID of each exon.&#xD;&#xA;&#xD;&#xA;**SeqMonk**&#xD;&#xA;&#xD;&#xA;[![SeqMonk Visualisation of NOTCH2][2]][2]&#xD;&#xA;&#xD;&#xA;SeqMonk gives a much cleaner visualisation, but unfortunately shows no links between aligned exons, and worse, I cannot find the read IDs. This means there is functionally (as far as I can see) no way to tell which exons come from the same read.&#xD;&#xA;&#xD;&#xA;Is there a visualisation tool which allows simple examination of splicing of full-length transcripts?&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/zM3Yn.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/ugYDT.png" />
  <row Id="2227" PostHistoryTypeId="1" PostId="722" RevisionGUID="65efa005-4e02-4f51-835c-365abe197fab" CreationDate="2017-06-14T03:16:07.773" UserId="163" Text="Visualisation of long read RNA-Seq splicing" />
  <row Id="2228" PostHistoryTypeId="3" PostId="722" RevisionGUID="65efa005-4e02-4f51-835c-365abe197fab" CreationDate="2017-06-14T03:16:07.773" UserId="163" Text="&lt;rna-seq&gt;&lt;nanopore&gt;&lt;read-mapping&gt;&lt;visualization&gt;" />
  <row Id="2229" PostHistoryTypeId="5" PostId="637" RevisionGUID="1878ea26-9cdc-4bdf-8ba8-34d87303a69c" CreationDate="2017-06-14T03:22:39.327" UserId="734" Comment="added 7 characters in body" Text="I read a lecture notes about [mutations][1], what kind of algorithms are there to detect mutations? How do people know if the gene is mutated or whether it's a sequencing error?&#xD;&#xA;&#xD;&#xA;I saw [this][2] which is related, but I am not sure how to work with CIGAR, is it 100% accurate? What's the underline mechanism to detect mutation? Is there a way to predict what the mutation will cause.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://mcb.berkeley.edu/courses/mcb142/lecture%20topics/Dernburg/Lecture6_Chapter8_screenviewing.pdf&#xD;&#xA;  [2]: https://bioinformatics.stackexchange.com/questions/98/how-to-quickly-determine-mutations-in-a-read-of-a-sam-file" />
  <row Id="2230" PostHistoryTypeId="2" PostId="723" RevisionGUID="960667b7-f8af-4afa-b21c-dbadd95847a7" CreationDate="2017-06-14T03:28:48.103" UserId="64" Text="The [Sashmi Plot](https://software.broadinstitute.org/software/igv/Sashimi) feature built into IGV. It gives a nice summary of the spliced transcripts and the coverage of each exon.&#xD;&#xA;&#xD;&#xA;For example:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/6MMjw.png" />
  <row Id="2231" PostHistoryTypeId="2" PostId="724" RevisionGUID="c3c07a12-5854-4b9a-b75d-c594fa376e11" CreationDate="2017-06-14T04:12:13.013" UserId="409" Text="Hi I'm working on a project where I have to assemble sequences generated by RADseq.When I was looking for a tool or pipeline I got to know about Stacks pipeline.I would like to know if there are any other tools or pipelines that I can use for assembling my data" />
  <row Id="2232" PostHistoryTypeId="1" PostId="724" RevisionGUID="c3c07a12-5854-4b9a-b75d-c594fa376e11" CreationDate="2017-06-14T04:12:13.013" UserId="409" Text="Assembling sequence data generated by RADseq" />
  <row Id="2233" PostHistoryTypeId="3" PostId="724" RevisionGUID="c3c07a12-5854-4b9a-b75d-c594fa376e11" CreationDate="2017-06-14T04:12:13.013" UserId="409" Text="&lt;ngs&gt;&lt;assembly&gt;" />
  <row Id="2234" PostHistoryTypeId="5" PostId="722" RevisionGUID="4f48d356-6ed4-40c5-ba4b-a6ca74487104" CreationDate="2017-06-14T04:38:50.317" UserId="163" Comment="add sashimi plot" Text="I have a dataset of Oxford Nanopore cDNA reads. Many of my reads are full-length or close to full-length transcripts, and I and am interested in examining alternative splicing. For this, I would like to begin by visualising my reads and comparing variants qualitatively.&#xD;&#xA;&#xD;&#xA;I have tried visualising my data in both IGV and SeqMonk, but neither has given a satisfying result.&#xD;&#xA;&#xD;&#xA;**IGV**&#xD;&#xA;&#xD;&#xA;[![IGV Visualisation of NOTCH2][1]][1]&#xD;&#xA;&#xD;&#xA;IGV shows some links between aligned sections, shown as fine blue lines and thick black lines. I cannot work out what the difference between these lines is, and even more confusingly, we have numerous alignments where two aligned exons come from the same sequence but are not joined by any line. This means that, to confirm or deny a spliced variant, I need to manually examine the read ID of each exon.&#xD;&#xA;&#xD;&#xA;**SeqMonk**&#xD;&#xA;&#xD;&#xA;[![SeqMonk Visualisation of NOTCH2][2]][2]&#xD;&#xA;&#xD;&#xA;SeqMonk gives a much cleaner visualisation, but unfortunately shows no links between aligned exons, and worse, I cannot find the read IDs. This means there is functionally (as far as I can see) no way to tell which exons come from the same read.&#xD;&#xA;&#xD;&#xA;**EDIT: Sashimi Plot**&#xD;&#xA;&#xD;&#xA;As recommended in [this answer][3], I have tried using IGV's Sashimi Plot. Unfortunately, I believe the high error rate and relatively low coverage is causing this to create somewhat confusing output. The Sashimi Plot shows mostly nearly every junction to have just a single read supporting it, and the first, fourth and fifth variants shown below the plot don't seem to have any supporting reads in the standard IGV view above (as far as I can see.) [![IGV Sashimi Plot of NOTCH2][4]][4]&#xD;&#xA;&#xD;&#xA;Is there a visualisation tool which allows simple examination of splicing of full-length transcripts?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/zM3Yn.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/ugYDT.png&#xD;&#xA;  [3]: https://bioinformatics.stackexchange.com/a/723/163&#xD;&#xA;  [4]: https://i.stack.imgur.com/5ah7z.png" />
  <row Id="2235" PostHistoryTypeId="2" PostId="725" RevisionGUID="90568881-73c9-4871-9e6c-e36b8bdb6e61" CreationDate="2017-06-14T05:23:42.993" UserId="138" Text="DNASTAR's software is for purchase, but high quality. GenVision Pro does genomic visualization, including Sashimi plots." />
  <row Id="2237" PostHistoryTypeId="2" PostId="726" RevisionGUID="d5ea3cdc-b5af-468e-8d4f-dbd55e175623" CreationDate="2017-06-14T06:43:46.623" UserId="77" Text="Assuming both the MD and CIGAR are present **and correct**, then yes, you can parse both to get the edit distance (`NM` auxiliary tag). One big caveat to this is that there's a reason that the `samtools calmd` command exists, since it's historically been the case that not all aligners have output correct MD strings. It's rare for the CIGAR string to be wrong and that'd be more of a catastrophic error on the part of an aligner. For what it's worth, if the `NM` auxiliary is absent on a given alignment but present on others produced by the same aligner then it's fair to assume `NM:i:0` for a given alignment by default (many aligners only produce `NM:i:XXX` if the edit distance is at least 1)." />
  <row Id="2238" PostHistoryTypeId="5" PostId="724" RevisionGUID="dab21e81-91a7-4ed2-b18b-5b8b32981e11" CreationDate="2017-06-14T07:51:50.080" UserId="57" Comment="deleted 1 character in body; edited tags" Text="I'm working on a project where I have to assemble sequences generated by RADseq. When I was looking for a tool or pipeline I got to know about Stacks pipeline. I would like to know if there are any other tools or pipelines that I can use for assembling my data" />
  <row Id="2239" PostHistoryTypeId="6" PostId="724" RevisionGUID="dab21e81-91a7-4ed2-b18b-5b8b32981e11" CreationDate="2017-06-14T07:51:50.080" UserId="57" Comment="deleted 1 character in body; edited tags" Text="&lt;ngs&gt;&lt;assembly&gt;&lt;radseq&gt;" />
  <row Id="2240" PostHistoryTypeId="2" PostId="727" RevisionGUID="5485660c-25c2-4711-becd-4d91d01545ac" CreationDate="2017-06-14T09:40:29.803" UserId="-1" Text="" />
  <row Id="2241" PostHistoryTypeId="1" PostId="727" RevisionGUID="5485660c-25c2-4711-becd-4d91d01545ac" CreationDate="2017-06-14T09:40:29.803" UserId="-1" />
  <row Id="2242" PostHistoryTypeId="2" PostId="728" RevisionGUID="e8bc7976-9fb1-4175-ab47-df9afa571813" CreationDate="2017-06-14T09:40:29.803" UserId="-1" Text="" />
  <row Id="2243" PostHistoryTypeId="1" PostId="728" RevisionGUID="e8bc7976-9fb1-4175-ab47-df9afa571813" CreationDate="2017-06-14T09:40:29.803" UserId="-1" />
  <row Id="2244" PostHistoryTypeId="5" PostId="724" RevisionGUID="4b435b5e-5a6d-4fa2-a686-f85facdb5f88" CreationDate="2017-06-14T09:49:13.553" UserId="57" Comment="edit to get closer to the real problem" Text="I'm working on a project where I have to assemble sequences generated by RADseq. At the end I hope to compare two species of woodpeckers in Sri Lanka by using SNPs. &#xD;&#xA;&#xD;&#xA;I tried to assemble it using Stacks pipeline, but it crushed on the way. &#xD;&#xA;&#xD;&#xA;Are there are any other tools or pipelines that I can use for assembling my data instead?" />
  <row Id="2245" PostHistoryTypeId="2" PostId="729" RevisionGUID="64f9b119-6897-489e-bc12-d52725477b16" CreationDate="2017-06-14T09:50:01.067" UserId="161" Text="[TRUSST](https://www.ncbi.nlm.nih.gov/pubmed/26066708) is a manually curated database with around 790 TFs and their target genes. [TRUSST Website](http://www.grnpedia.org/trrust/)&#xD;&#xA;" />
  <row Id="2251" PostHistoryTypeId="5" PostId="728" RevisionGUID="d8926631-d262-4e4b-b16e-191ab5e87960" CreationDate="2017-06-14T10:58:45.880" UserId="48" Comment="added 58 characters in body" Text="Use this tag for questions related to single-cell RNA-seq." />
  <row Id="2252" PostHistoryTypeId="24" PostId="728" RevisionGUID="d8926631-d262-4e4b-b16e-191ab5e87960" CreationDate="2017-06-14T10:58:45.880" Comment="Proposed by 48 approved by 77, 73 edit id of 195" />
  <row Id="2253" PostHistoryTypeId="2" PostId="731" RevisionGUID="e4f2e029-a843-4eff-94c7-4523b9159fa3" CreationDate="2017-06-14T11:10:24.310" UserId="678" Text="I have a list of 100 genes that are called as hits in a genetic screening. I want to have a network of the interactions between the proteins of these 100 genes. I am using both [STRINGdb web][1] and its [R API][2]. &#xD;&#xA;&#xD;&#xA;In both situations you can select the threshold of confidence of interaction (default 0.4) for displaying the network. Changing this parameter drastically changes the network that I am generating. I have read the FAQ section of STRINGdb and they recommend to choose some arbitrary number based on the number of interactions you need for you analysis. If I use default threshold I have few interactions and I don't know if lowering the threshold would be correct. &#xD;&#xA;&#xD;&#xA;Therefore, my question is, is there any established threshold based on habitual user's experience used for that, beyond the recommendation of set an arbitrary one? &#xD;&#xA;&#xD;&#xA;  [1]: https://string-db.org/&#xD;&#xA;  [2]: http://bioconductor.org/packages/release/bioc/html/STRINGdb.html&#xD;&#xA;" />
  <row Id="2254" PostHistoryTypeId="1" PostId="731" RevisionGUID="e4f2e029-a843-4eff-94c7-4523b9159fa3" CreationDate="2017-06-14T11:10:24.310" UserId="678" Text="How to select a cutoff for interaction condifence in STRINGdb?" />
  <row Id="2255" PostHistoryTypeId="3" PostId="731" RevisionGUID="e4f2e029-a843-4eff-94c7-4523b9159fa3" CreationDate="2017-06-14T11:10:24.310" UserId="678" Text="&lt;proteins&gt;&lt;public-databases&gt;&lt;networks&gt;" />
  <row Id="2256" PostHistoryTypeId="2" PostId="732" RevisionGUID="a73abda1-6bac-427f-817d-89ff3799f507" CreationDate="2017-06-14T11:26:35.647" UserId="73" Text="I've been asked to evaluate whether the MinION will be sufficient to distinguish between 20bp CRISPR guide RNAs from [the GeCKO v2 set](https://www.addgene.org/pooled-library/zhang-mouse-gecko-v2/). I know that this set has some [exactly identical sequences](https://groups.google.com/forum/#!topic/crispr/qxEi_jL7jls), which wouldn't be distinguishable regardless of the sequencing method used, so I would like to know whether any two non-indentical sequences differ by less than 15% of the 20bp (i.e. only 3 bases different).&#xD;&#xA;&#xD;&#xA;There are around 130k 20bp sequences in the dataset, and I'm pretty sure that an all-vs-all approach is not feasible, but perhaps there's some trick that I'm missing.&#xD;&#xA;&#xD;&#xA;What I thought of doing was looking at dimer and trimer/codon counts to hunt down sequences that were most similar. I found about 100 sequences that shared the same trimer count signature (but were not identical), and within this group found [only] one pair with 85% identity; all the remainder were 70% or less.&#xD;&#xA;&#xD;&#xA;... but I'm having trouble ignoring the doubt I have that these two sequences are the most similar. It might be possible that there are sequences in the dataset that don't have identical trimer count signatures, but differ by less than three bases. I'll give a very obvious example (which isn't in the dataset):&#xD;&#xA;&#xD;&#xA;    1: AAAAA AAAAA AAAAA AAAAA&#xD;&#xA;    2: AAAAC AAAAA AAAAA AAAAA&#xD;&#xA;&#xD;&#xA;The trimer counts for these two sequences are different (Sequence 1 -- AAA: 18; Sequence 2 -- AAA: 15; AAC: 1; ACA: 1; CAA: 1), and yet the sequences are only 1 base different (which might be a problem for the MinION). How can I work out if any such sequence pairs exist in the dataset?" />
  <row Id="2257" PostHistoryTypeId="1" PostId="732" RevisionGUID="a73abda1-6bac-427f-817d-89ff3799f507" CreationDate="2017-06-14T11:26:35.647" UserId="73" Text="How do I find the most similar sequences from a large set of short sequences?" />
  <row Id="2258" PostHistoryTypeId="3" PostId="732" RevisionGUID="a73abda1-6bac-427f-817d-89ff3799f507" CreationDate="2017-06-14T11:26:35.647" UserId="73" Text="&lt;minion&gt;&lt;crispr&gt;" />
  <row Id="2259" PostHistoryTypeId="5" PostId="716" RevisionGUID="408d2ee5-2548-428a-bdb5-fc5a2e958932" CreationDate="2017-06-14T11:44:08.643" UserId="292" Comment="typo in the title, added a &quot;performance&quot; tag" Text="I am trying to calculate the mappability adjusted length of introns as described by [Boutz et al][1].&#xD;&#xA;&#xD;&#xA;Briefly, for each intron I wish to calculate the length minus the number of bases that are non-uniquely mappable. Mappability tracks can be downloaded as bigWigs from [here][2]. The the score at each base is 1/number of mapping positions, so 1 indicates a read can be uniquely mapped at this location. &#xD;&#xA;&#xD;&#xA;I am struggling to do this computation in either a reasonable amount of time, or a reasonable amount of memory.&#xD;&#xA;&#xD;&#xA;First I tried importing the whole bigwig:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: r --&gt;&#xD;&#xA;    &#xD;&#xA;    library(rtracklayer)&#xD;&#xA;    mappability &lt;- import(BigWigFile(&quot;wgEncodeCrgMapabilityAlign50mer.bigWig&quot;),&#xD;&#xA;                          as=&quot;NumericList&quot;,&#xD;&#xA;                          selection=BigWigSelection(intron_ranges))&#xD;&#xA;&#xD;&#xA;where `intron_ranges` is a `GRanges` object with the introns (about 800,000 of them).&#xD;&#xA;&#xD;&#xA;This used **way** too much memory, and soon caused by machine to fall over.&#xD;&#xA;&#xD;&#xA;Second I tried processing one intron at a time:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    mappability_file = BigWigFile(&quot;wgEncodeCrgMapabilityAlign50mer.bigWig&quot;)&#xD;&#xA;    effective_length &lt;- function (gr) {&#xD;&#xA;      intron_selection &lt;- BigWigSelection(gr)&#xD;&#xA;      scores &lt;- import(mappability_file, as=&quot;NumericList&quot;, selection=intron_selection)&#xD;&#xA;      non_unique &lt;- sum(scores[[1]] &lt; 1.0)&#xD;&#xA;      eff_len = width(gr)[1] - non_unique&#xD;&#xA;      return(eff_len)&#xD;&#xA;    }    &#xD;&#xA;&#xD;&#xA;    mappability &lt;- sapply(intron_ranges, effective_length)&#xD;&#xA;&#xD;&#xA;This has been running for hours and shows no sign of finishing. &#xD;&#xA;&#xD;&#xA;Is there a way to do this that uses less than, say 4GB of RAM, but finishes in say less than 30 minutes? It feels like there should be. &#xD;&#xA;&#xD;&#xA;I'm not tied to R, just what I've tried so far. Happy with answers using binary packages, common shell tools, python or R. &#xD;&#xA;&#xD;&#xA;  [1]: http://genesdev.cshlp.org/content/29/1/63.abstract&#xD;&#xA;  [2]: http://genome.ucsc.edu/cgi-bin/hgFileUi?db=hg19&amp;g=wgEncodeMapability" />
  <row Id="2260" PostHistoryTypeId="4" PostId="716" RevisionGUID="408d2ee5-2548-428a-bdb5-fc5a2e958932" CreationDate="2017-06-14T11:44:08.643" UserId="292" Comment="typo in the title, added a &quot;performance&quot; tag" Text="Time and memory efficient processing of many intervals for a bigWig file" />
  <row Id="2261" PostHistoryTypeId="6" PostId="716" RevisionGUID="408d2ee5-2548-428a-bdb5-fc5a2e958932" CreationDate="2017-06-14T11:44:08.643" UserId="292" Comment="typo in the title, added a &quot;performance&quot; tag" Text="&lt;bigwig&gt;&lt;performance&gt;" />
  <row Id="2262" PostHistoryTypeId="24" PostId="716" RevisionGUID="408d2ee5-2548-428a-bdb5-fc5a2e958932" CreationDate="2017-06-14T11:44:08.643" Comment="Proposed by 292 approved by 73, 37 edit id of 196" />
  <row Id="2263" PostHistoryTypeId="5" PostId="731" RevisionGUID="4a8fc2b7-0e1b-4460-8e16-6c242b1a7b49" CreationDate="2017-06-14T12:10:48.763" UserId="292" Comment="typo in the title" Text="I have a list of 100 genes that are called as hits in a genetic screening. I want to have a network of the interactions between the proteins of these 100 genes. I am using both [STRINGdb web][1] and its [R API][2]. &#xD;&#xA;&#xD;&#xA;In both situations you can select the threshold of confidence of interaction (default 0.4) for displaying the network. Changing this parameter drastically changes the network that I am generating. I have read the FAQ section of STRINGdb and they recommend to choose some arbitrary number based on the number of interactions you need for you analysis. If I use the default threshold, I have few interactions and I don't know if lowering the threshold would be correct. &#xD;&#xA;&#xD;&#xA;Therefore, my question is, is there any established threshold based on habitual user's experience used for that, beyond the recommendation of setting an arbitrary one? &#xD;&#xA;&#xD;&#xA;  [1]: https://string-db.org/&#xD;&#xA;  [2]: http://bioconductor.org/packages/release/bioc/html/STRINGdb.html&#xD;&#xA;" />
  <row Id="2264" PostHistoryTypeId="4" PostId="731" RevisionGUID="4a8fc2b7-0e1b-4460-8e16-6c242b1a7b49" CreationDate="2017-06-14T12:10:48.763" UserId="292" Comment="typo in the title" Text="How to select a cutoff for interaction confidence in STRINGdb?" />
  <row Id="2265" PostHistoryTypeId="24" PostId="731" RevisionGUID="4a8fc2b7-0e1b-4460-8e16-6c242b1a7b49" CreationDate="2017-06-14T12:10:48.763" Comment="Proposed by 292 approved by 37, 678 edit id of 197" />
  <row Id="2266" PostHistoryTypeId="2" PostId="733" RevisionGUID="09d2e369-f570-496a-b777-73a1c46db3bc" CreationDate="2017-06-14T12:20:49.173" UserId="73" Text="&gt; I'm pretty sure that an all-vs-all approach is not feasible, but perhaps there's some trick that I'm missing.&#xD;&#xA;&#xD;&#xA;The &quot;trick&quot; is that I hadn't actually tried this, and Bowtie2 happens to be quite good at doing this. When there are other similar matches, the MAPQ score that Bowtie2 produces are reduced, so all that is needed is to identify those sequences with less than a MAPQ of 42. I've found 2810 such sequences that share some similarity to others (2416 if I exclude reverse-complement matches):&#xD;&#xA;&#xD;&#xA;    $ bowtie2-build Mouse_GeCKOv2_Library.fasta Mouse_GeCKOv2_Library.fasta&#xD;&#xA;    $ bowtie2 -f -x Mouse_GeCKOv2_Library.fasta -U Mouse_GeCKOv2_Library.fasta | &#xD;&#xA;       samtools view | awk '{if($5 != 42){print &quot;&gt;&quot;$1&quot;\n&quot;$10}}' &gt; similar_seqs.fasta&#xD;&#xA;    125795 reads; of these:&#xD;&#xA;      125795 (100.00%) were unpaired; of these:&#xD;&#xA;        1 (0.00%) aligned 0 times&#xD;&#xA;        122982 (97.76%) aligned exactly 1 time&#xD;&#xA;        2812 (2.24%) aligned &gt;1 times&#xD;&#xA;    100.00% overall alignment rate" />
  <row Id="2267" PostHistoryTypeId="5" PostId="690" RevisionGUID="30cdb5dc-eb65-4416-8861-e381062628d1" CreationDate="2017-06-14T12:23:37.687" UserId="29" Comment="improve and correct example" Text="&gt; I'd like to find genes that were not expressed in a group of samples and were expressed in another group.&#xD;&#xA;&#xD;&#xA;This is, fundamentally, a differential expression analysis, with a twist. To solve this, you’d first use a differential expression library of your choice (e.g. DESeq2) and perform a one-tailed test of differential expression.&#xD;&#xA;&#xD;&#xA;Briefly, you’d perform the normal setup and then use&#xD;&#xA;&#xD;&#xA;    results(dds, altHypothesis = 'greater')&#xD;&#xA;&#xD;&#xA;To perform a one-tailed test. This will give you only those genes that are significantly upregulated in one group. Check chapter 3.9 of the vignette for details.&#xD;&#xA;&#xD;&#xA;Of course this won’t tell you that the genes are *unexpressed* in the other group. Unfortunately I don’t know of a good value to threshold the results; I would start by plotting a histogram of the (variance stabilised) expression values in your first group, and then visually choose an expression threshold that cleanly separates genes that are clearly expressed from zeros:&#xD;&#xA;&#xD;&#xA;    vst_counts = assay(vst(dds))&#xD;&#xA;    dens = density(vst_counts[, replicate])&#xD;&#xA;    plot(dens, log = 'y')&#xD;&#xA;&#xD;&#xA;(This merges the replicates in the group, which should be fine.)&#xD;&#xA;&#xD;&#xA;Counts follow a multimodal distribution, with one mode for unexpressed and one or more for expressed genes. The expression threshold can be set somewhere between the clearly unexpressed and expressed peaks:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;Here I used `identify(dens)` to identify the threshold interactively but you could also use an analytical method:&#xD;&#xA;&#xD;&#xA;    threshold = identify(dens)&#xD;&#xA;    quantile = sum(dens$x &lt; dens$x[threshold]) / length(dens$x)&#xD;&#xA;    &#xD;&#xA;    # Using just one replicate here; more robust would be to use a mean value.&#xD;&#xA;    nonzero_counts = counts(dds, normalized = TRUE)[, replicates[1]]&#xD;&#xA;    nonzero_counts = nonzero_counts[nonzero_counts &gt; 0]&#xD;&#xA;    &#xD;&#xA;    (expression_threshold = quantile(nonzero_counts, probs = quantile))&#xD;&#xA;&#xD;&#xA;&lt;!-- --&gt;&#xD;&#xA;&#xD;&#xA;    26.5625%&#xD;&#xA;    4.112033&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/MF0vy.png&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2268" PostHistoryTypeId="2" PostId="734" RevisionGUID="69582958-eeb1-44af-954e-4b9001f81898" CreationDate="2017-06-14T12:42:31.153" UserId="824" Text="For this application, you could probably also do something like calculate the Hamming distances between all of the strings in an all-vs-all approach (it should not take too long or too much overhead). You could use something like the Hamming distance tools in Julia. Here is an example of what I mean (using Julia):&#xD;&#xA;&#xD;&#xA;    using StringDistances&#xD;&#xA;    &#xD;&#xA;    k = [&quot;AATTGGCC&quot;, &quot;AATTGGCA&quot;, &quot;AATTCCGG&quot;]&#xD;&#xA;    &#xD;&#xA;    for s in k&#xD;&#xA;        for y in k&#xD;&#xA;            print(s, &quot; compared to &quot;, y, &quot;:  &quot;, compare(Hamming(), s, y), &quot;\n&quot;)&#xD;&#xA;        end&#xD;&#xA;    end&#xD;&#xA;&#xD;&#xA;The output looks like this:&#xD;&#xA;&#xD;&#xA;    AATTGGCC compared to AATTGGCC:  1.0&#xD;&#xA;    AATTGGCC compared to AATTGGCA:  0.875&#xD;&#xA;    AATTGGCC compared to AATTCCGG:  0.5&#xD;&#xA;    AATTGGCA compared to AATTGGCC:  0.875&#xD;&#xA;    AATTGGCA compared to AATTGGCA:  1.0&#xD;&#xA;    AATTGGCA compared to AATTCCGG:  0.5&#xD;&#xA;    AATTCCGG compared to AATTGGCC:  0.5&#xD;&#xA;    AATTCCGG compared to AATTGGCA:  0.5&#xD;&#xA;    AATTCCGG compared to AATTCCGG:  1.0&#xD;&#xA;&#xD;&#xA;This is just a simple small example but the core is there. Should not be too bad scaling it up. Hope this helps and provides another helpful option. :)" />
  <row Id="2269" PostHistoryTypeId="5" PostId="380" RevisionGUID="509bd7f1-8c5b-4e4f-b80f-259b9eebcfb4" CreationDate="2017-06-14T14:11:41.247" UserId="292" Comment="Added benchmarks for fastq.gz" Text="## Using bioawk&#xD;&#xA;&#xD;&#xA;With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (**including gzipped versions**), so the above command **should also handle fastq format robustly**.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters (got it from &lt;https://unix.stackexchange.com/a/169575/55127&gt;). I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)&#xD;&#xA;&#xD;&#xA;## Using python&#xD;&#xA;&#xD;&#xA;### readfq&#xD;&#xA;&#xD;&#xA;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;https://bioinformatics.stackexchange.com/a/365/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.976s&#xD;&#xA;    user	0m0.876s&#xD;&#xA;    sys 	0m0.100s&#xD;&#xA;&#xD;&#xA;### &quot;pure python&quot;&#xD;&#xA;&#xD;&#xA;Comparing with something adapted from this answer: &lt;https://bioinformatics.stackexchange.com/a/363/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&gt;')))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.143s&#xD;&#xA;    user	0m1.100s&#xD;&#xA;    sys 	0m0.040s&#xD;&#xA;&#xD;&#xA;(It is slower with python 3.6 than with python 2.7 on my computer)&#xD;&#xA;&#xD;&#xA;### pyGATB&#xD;&#xA;&#xD;&#xA;I just learned (08/06/2017) that [GATB](http://gatb.inria.fr/) includes a fasta/fastq parser and has recently released a python API. I tried to use it yesterday to test another answer to the present question and [found a bug](https://github.com/GATB/pyGATB/issues/2). This bug is now fixed, so here is a [pyGATB](https://pypi.python.org/pypi/pyGATB/0.1.2)-based answer:&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.count(b\&quot;A\&quot;) for seq in Bank(\&quot;genome.fa\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.663s&#xD;&#xA;    user	0m0.568s&#xD;&#xA;    sys 	0m0.092s&#xD;&#xA;&#xD;&#xA;(You can also do `sequence.decode(&quot;utf-8&quot;).count(&quot;A&quot;)` but this seems [a little slower](https://github.com/GATB/pyGATB/issues/2#issuecomment-307131176).)&#xD;&#xA;&#xD;&#xA;Although I used python3.6 here (pyGATB seems python3-only), this is faster than the other two python approaches (for which the reported timings are obtained with python 2.7). This is even almost as fast as the `grep`, `tr`, `wc` pipeline.&#xD;&#xA;&#xD;&#xA;### Biopython&#xD;&#xA;&#xD;&#xA;And, to have even more comparisons, here is a solution using `SeqIO.parse` from Biopython (with python2.7):&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from Bio import SeqIO; print(sum((rec.seq.count(\&quot;A\&quot;) for rec in SeqIO.parse(\&quot;genome.fa\&quot;, \&quot;fasta\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.632s&#xD;&#xA;    user	0m1.532s&#xD;&#xA;    sys 	0m0.096s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;This is a bit slower than the &quot;pure python&quot; solution, but perhaps more robust.&#xD;&#xA;&#xD;&#xA;There seems to be a slight improvement with @peterjc's suggestion to use the lower level `SimpleFastaParser`:&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from Bio.SeqIO.FastaIO import SimpleFastaParser; print(sum(seq.count('A') for title, seq in SimpleFastaParser(open('genome.fa'))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.618s&#xD;&#xA;    user	0m1.500s&#xD;&#xA;    sys 	0m0.116s&#xD;&#xA;&#xD;&#xA;(I did a series of timings and tried to take one that seemed representative, but there's a lot of overlap with the higher-level parser's timings.)&#xD;&#xA;&#xD;&#xA;## Benchmarks on a fastq.gz file&#xD;&#xA;&#xD;&#xA;I tested some of the above solutions (or adaptations thereof), counting &quot;N&quot; in the same file that was used here: &lt;https://bioinformatics.stackexchange.com/a/400/292&gt;&#xD;&#xA;&#xD;&#xA;### bioawk&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/N/, &quot;&quot;, $seq)} END {print n}' SRR077487_2.filt.fastq.gz&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	1m9.686s&#xD;&#xA;    user	1m9.376s&#xD;&#xA;    sys 	0m0.304s&#xD;&#xA;&#xD;&#xA;### pigz + readfq python module&#xD;&#xA;&#xD;&#xA;readfq doesn't complain and is very fast when I pass directly the compressed fastq, but returns something wrong, so don't forget to manually take care of the decompression.&#xD;&#xA;&#xD;&#xA;Here I tried with `pigz`:&#xD;&#xA;&#xD;&#xA;    $ time pigz -dc SRR077487_2.filt.fastq.gz | python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('N') for _, seq, _ in readfq(stdin)))&quot;&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	0m52.347s&#xD;&#xA;    user	1m40.716s&#xD;&#xA;    sys 	0m8.604s&#xD;&#xA;&#xD;&#xA;The computer has 16 cores, but I suspect the limiting factor for `pigz` is reading from the disk: the processors are very far from running full speed.&#xD;&#xA;&#xD;&#xA;And with `gzip`:&#xD;&#xA;&#xD;&#xA;    $ time gzip -dc SRR077487_2.filt.fastq.gz | python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('N') for _, seq, _ in readfq(stdin)))&quot;&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	0m49.448s&#xD;&#xA;    user	1m31.984s&#xD;&#xA;    sys 	0m2.312s&#xD;&#xA;&#xD;&#xA;Here gzip and python both used a full processor. Resource usage balance was slightly better. I work on a desktop computer. I suppose a computing server would take advantage of `pigz` better.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### pyGATB&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.count(b\&quot;N\&quot;) for seq in Bank(\&quot;SRR077487_2.filt.fastq.gz\&quot;))))&quot;&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	0m46.784s&#xD;&#xA;    user	0m46.404s&#xD;&#xA;    sys 	0m0.296s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### biopython&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from gzip import open as gzopen; from Bio.SeqIO.QualityIO import FastqGeneralIterator; print(sum(seq.count('N') for (_, seq, _) in FastqGeneralIterator(gzopen('SRR077487_2.filt.fastq.gz'))))&quot;&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	3m18.103s&#xD;&#xA;    user	3m17.676s&#xD;&#xA;    sys 	0m0.428s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;## Conclusion&#xD;&#xA;&#xD;&#xA;pyGATB and bioawk both handle transparently compression (gzipped or not) and format differences (fasta or fastq). pyGATB is quite a new tool, but seems more efficient compared to the other python modules I tested.&#xD;&#xA;" />
  <row Id="2270" PostHistoryTypeId="2" PostId="735" RevisionGUID="4a26868f-2929-4bb0-a03a-0e67110b6fab" CreationDate="2017-06-14T14:14:33.443" UserId="37" Text="130k * 20bp is a small data set. At this scale, SSE2 Smith-Waterman may work well:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-shell --&gt;&#xD;&#xA;&#xD;&#xA;    git clone https://github.com/attractivechaos/klib&#xD;&#xA;    cd klib &amp;&amp; gcc -O2 -D_KSW_MAIN ksw.c -o ksw -lz&#xD;&#xA;    ./ksw -a1 -b1 -q1 -r1 -t14 20bp.fa 20bp.fa &gt; out.tsv&#xD;&#xA;&#xD;&#xA;I simulated 130k 20bp reads from E. coli K-12. The last command line takes about ~2 CPU hours based on the partial output.&#xD;&#xA;&#xD;&#xA;It is possible to make this much faster by using 5bp seeds. You can also try ssearch and fasta from the [fasta aligner package](http://fasta.bioch.virginia.edu/fasta_www2/fasta_list2.shtml). They are slower, probably because they are not optimized for such input. Computing Hamming distance with SSE2 will be much faster, but is probably of little use as it does not allow gaps. There are also Gene Myers' edit-distance based algorithms, which can be faster than SSE2-SW.&#xD;&#xA;&#xD;&#xA;Bowtie2 (as well as most short-read mappers) won't work well at the default setting. Bowtie2 uses long exact seeds. It will miss many 1-mismatch hits, let alone 3-mismatch ones. The number you get from bowtie2 is an underestimate. You might be able to tune bowtie2, but to find 3-mismatch hits, you have to make it a lot slower." />
  <row Id="2271" PostHistoryTypeId="2" PostId="736" RevisionGUID="78601aa5-ab66-4482-a604-082a0b76c735" CreationDate="2017-06-14T14:22:55.960" UserId="377" Text="I'm faced with having to align many (some 100s) bacterial genomes, where the genome length is in the millions. Obviously, this is beyond normal alignment techniques and it's unclear to me what the best practice is for such circumstances:&#xD;&#xA;&#xD;&#xA; - conventional alignment on a very powerful computer with lots of memory&#xD;&#xA; - break up the genome into smaller fragments and align them individually&#xD;&#xA; - some exotic different procedure&#xD;&#xA;&#xD;&#xA;What possible avenues of attack are there?&#xD;&#xA;&#xD;&#xA;(I've attempted to use Mafft and Clustal with little success)&#xD;&#xA;" />
  <row Id="2272" PostHistoryTypeId="1" PostId="736" RevisionGUID="78601aa5-ab66-4482-a604-082a0b76c735" CreationDate="2017-06-14T14:22:55.960" UserId="377" Text="Aligning many long sequences" />
  <row Id="2273" PostHistoryTypeId="3" PostId="736" RevisionGUID="78601aa5-ab66-4482-a604-082a0b76c735" CreationDate="2017-06-14T14:22:55.960" UserId="377" Text="&lt;alignment&gt;&lt;genome&gt;" />
  <row Id="2274" PostHistoryTypeId="5" PostId="734" RevisionGUID="6653f7bb-ca04-4b43-a46a-8d52ef0731b4" CreationDate="2017-06-14T15:00:55.240" UserId="298" Comment="I'd never heard of Julia before so I added a link for other ignorant folks. " Text="For this application, you could probably also do something like calculate the Hamming distances between all of the strings in an all-vs-all approach (it should not take too long or too much overhead). You could use something like the Hamming distance tools in [Julia][1]. Here is an example of what I mean (using Julia):&#xD;&#xA;&#xD;&#xA;    using StringDistances&#xD;&#xA;    &#xD;&#xA;    k = [&quot;AATTGGCC&quot;, &quot;AATTGGCA&quot;, &quot;AATTCCGG&quot;]&#xD;&#xA;    &#xD;&#xA;    for s in k&#xD;&#xA;        for y in k&#xD;&#xA;            print(s, &quot; compared to &quot;, y, &quot;:  &quot;, compare(Hamming(), s, y), &quot;\n&quot;)&#xD;&#xA;        end&#xD;&#xA;    end&#xD;&#xA;&#xD;&#xA;The output looks like this:&#xD;&#xA;&#xD;&#xA;    AATTGGCC compared to AATTGGCC:  1.0&#xD;&#xA;    AATTGGCC compared to AATTGGCA:  0.875&#xD;&#xA;    AATTGGCC compared to AATTCCGG:  0.5&#xD;&#xA;    AATTGGCA compared to AATTGGCC:  0.875&#xD;&#xA;    AATTGGCA compared to AATTGGCA:  1.0&#xD;&#xA;    AATTGGCA compared to AATTCCGG:  0.5&#xD;&#xA;    AATTCCGG compared to AATTGGCC:  0.5&#xD;&#xA;    AATTCCGG compared to AATTGGCA:  0.5&#xD;&#xA;    AATTCCGG compared to AATTCCGG:  1.0&#xD;&#xA;&#xD;&#xA;This is just a simple small example but the core is there. Should not be too bad scaling it up. Hope this helps and provides another helpful option. :)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://julialang.org/" />
  <row Id="2275" PostHistoryTypeId="24" PostId="734" RevisionGUID="6653f7bb-ca04-4b43-a46a-8d52ef0731b4" CreationDate="2017-06-14T15:00:55.240" Comment="Proposed by 298 approved by 77, 57 edit id of 198" />
  <row Id="2276" PostHistoryTypeId="2" PostId="737" RevisionGUID="205a0d8c-c5ba-452b-b5ba-1c33accbff1b" CreationDate="2017-06-14T15:12:37.680" UserId="57" Text="Whole genome aliment can be done using [Progressive Mauve][1] or [Mummer][2].&#xD;&#xA;&#xD;&#xA;If you are interested in a rough idea of the shared genome regions, you can use [bevel][3]. Bevel is not really an aligner, it is more like a dot-plot, but it is super fast (even for mammalian sized genomes), therefore I would recommend to start with it.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://darlinglab.org/mauve/user-guide/progressivemauve.html&#xD;&#xA;  [2]: https://github.com/mummer4/mummer&#xD;&#xA;  [3]: https://github.com/zeeev/bevel" />
  <row Id="2277" PostHistoryTypeId="2" PostId="738" RevisionGUID="43d276bd-3d49-4982-b1f5-b58a103615d3" CreationDate="2017-06-14T15:33:46.443" UserId="35" Text="Since you are using R, you probably don't want to use scikit-learn, which is for Python. However, there is a similar R library [mlr][1] (&quot;R package to make machine learning in R easy&quot;) that provides a unified interface to all popular machine learning methods. Check their [tutorial][2] on how to get started.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://mlr-org.github.io/&#xD;&#xA;  [2]: https://mlr-org.github.io/mlr-tutorial/devel/html/" />
  <row Id="2278" PostHistoryTypeId="5" PostId="737" RevisionGUID="fc22b8a9-0c5b-450c-80f5-a2d7efee8a40" CreationDate="2017-06-14T16:08:47.127" UserId="57" Comment="added LAST" Text="Whole genome aliment can be done using [Progressive Mauve][1], [LAST][2] or [Mummer][3]. For bacteria I used Mauve since it has also very nice visualisation engine.&#xD;&#xA;&#xD;&#xA;If you are interested in a rough idea of the shared genome regions, you can use [bevel][4]. Bevel is not really an aligner, it is more like a dot-plot, but it is super fast (even for mammalian sized genomes), therefore I would recommend to start with it.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://darlinglab.org/mauve/user-guide/progressivemauve.html&#xD;&#xA;  [2]: http://last.cbrc.jp/&#xD;&#xA;  [3]: https://github.com/mummer4/mummer&#xD;&#xA;  [4]: https://github.com/zeeev/bevel" />
  <row Id="2279" PostHistoryTypeId="2" PostId="739" RevisionGUID="d25f2bba-7310-4868-bbe3-832cc1df1ea9" CreationDate="2017-06-14T16:34:25.797" UserId="375" Text="I do not think there is a simple &quot;yes&quot; or &quot;no&quot; answer here. &#xD;&#xA;&#xD;&#xA;A good starting point would be, as you suggest, use all the genes and assess the results in the light of the marker genes and expected results. This could both serve as as good quality control as well as give you overview of all the processes happening in the cells. &#xD;&#xA;&#xD;&#xA;Depending on the marker genes effect and biological question you may then want to remove the marker genes with potential ordering effect, or even other genes, e.g. by GO terms or pathways. &#xD;&#xA;&#xD;&#xA;You most likely want to account for the cell cycle phases as well. And check for other technical factors. I recommend Bioconductor pipeline to get inspired https://www.bioconductor.org/help/workflows/simpleSingleCell/ when it comes to scRNA-seq analyses. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2281" PostHistoryTypeId="2" PostId="740" RevisionGUID="80e339fa-8206-4db1-bfef-8fb648208576" CreationDate="2017-06-14T17:09:49.490" UserId="298" Text="This depends on what you are trying to do and whether you value specificity over sensitivity. We can't tell you since it is entirely dependent on the biological question you want to answer. &#xD;&#xA;&#xD;&#xA;However, I would recommend two things:&#xD;&#xA;&#xD;&#xA;1. Don't use STRING. The creators of STRING made the choice to value sensitivity over all else, so they include *any* interaction they can get their hands on. That is sometimes fine, depending on what you want to do, but is more often a problem. Personally, I tend to avoid STRING as much as possible and instead use more curated databases like [APID][1] or [IntAct][2]. You can find a list of many PPI databases in the [EBI's Psiqcuick View][3] page. You might also be interested in my answer [here][4] which gives an example script using APID which can easily be modified to query other DBs. &#xD;&#xA;&#xD;&#xA;2. Whatever you use, I recommend you filter by detection method. There are many different interaction detection methods ranging from those that only identify direct, binary interactions between two proteins (e.g. [Yeast two-hybrid][5]), those that also find proteins that are in the same complex even if there is no *direct* interaction between them (e.g. [ChIP][6]), to various non-experimental methods which are used to *infer* interactions. &#xD;&#xA;&#xD;&#xA; Luckily, all of these have been collected into a controlled vocabulary which you can see in the [Interaction Detection Method][7] Ontology Lookup Service page. Go there, collect the interaction detection methods you consider good enough for whatever you want to do and filter your interactions using those. &#xD;&#xA;&#xD;&#xA;For example, for a high quality human interactome I built for a project I used to work on, I was only interested in direct binary interactions, so I only kept interactions detected by the following methods:&#xD;&#xA;&#xD;&#xA;	    &quot;MI:0008&quot; =&gt; &quot;array technology&quot;,&#xD;&#xA;	    &quot;MI:0009&quot; =&gt; &quot;bacterial display&quot;,&#xD;&#xA;	    &quot;MI:0010&quot; =&gt; &quot;beta galactosidase complementation&quot;,&#xD;&#xA;	    &quot;MI:0011&quot; =&gt; &quot;beta lactamase complementation&quot;,&#xD;&#xA;	    &quot;MI:0012&quot; =&gt; &quot;bioluminescence resonance energy transfer&quot;,&#xD;&#xA;	    &quot;MI:0013&quot; =&gt; &quot;biophysical&quot;,&#xD;&#xA;	    &quot;MI:0014&quot; =&gt; &quot;adenylate cyclase complementation&quot;,&#xD;&#xA;	    &quot;MI:0016&quot; =&gt; &quot;circular dichroism&quot;,&#xD;&#xA;	    &quot;MI:0017&quot; =&gt; &quot;classical fluorescence spectroscopy&quot;,&#xD;&#xA;	    &quot;MI:0018&quot; =&gt; &quot;two hybrid&quot;,&#xD;&#xA;	    &quot;MI:0020&quot; =&gt; &quot;transmission electron microscopy&quot;,&#xD;&#xA;	    &quot;MI:0030&quot; =&gt; &quot;cross-linking study&quot;,&#xD;&#xA;	    &quot;MI:0031&quot; =&gt; &quot;protein cross-linking with a bifunctional reagent&quot;,&#xD;&#xA;	    &quot;MI:0034&quot; =&gt; &quot;display technology&quot;,&#xD;&#xA;	    &quot;MI:0040&quot; =&gt; &quot;electron microscopy&quot;,&#xD;&#xA;	    &quot;MI:0041&quot; =&gt; &quot;electron nuclear double resonance&quot;,&#xD;&#xA;	    &quot;MI:0042&quot; =&gt; &quot;electron paramagnetic resonance&quot;,&#xD;&#xA;	    &quot;MI:0043&quot; =&gt; &quot;electron resonance&quot;,&#xD;&#xA;	    &quot;MI:0047&quot; =&gt; &quot;far western blotting&quot;,&#xD;&#xA;	    &quot;MI:0048&quot; =&gt; &quot;filamentous phage display&quot;,&#xD;&#xA;	    &quot;MI:0049&quot; =&gt; &quot;filter binding&quot;,&#xD;&#xA;	    &quot;MI:0051&quot; =&gt; &quot;fluorescence technology&quot;,&#xD;&#xA;	    &quot;MI:0052&quot; =&gt; &quot;fluorescence correlation spectroscopy&quot;,&#xD;&#xA;	    &quot;MI:0053&quot; =&gt; &quot;fluorescence polarization spectroscopy&quot;,&#xD;&#xA;	    &quot;MI:0055&quot; =&gt; &quot;fluorescent resonance energy transfer&quot;,&#xD;&#xA;	    &quot;MI:0065&quot; =&gt; &quot;isothermal titration calorimetry&quot;,&#xD;&#xA;	    &quot;MI:0066&quot; =&gt; &quot;lambda phage display&quot;,&#xD;&#xA;	    &quot;MI:0073&quot; =&gt; &quot;mrna display&quot;,&#xD;&#xA;	    &quot;MI:0081&quot; =&gt; &quot;peptide array&quot;,&#xD;&#xA;	    &quot;MI:0084&quot; =&gt; &quot;phage display&quot;,&#xD;&#xA;	    &quot;MI:0089&quot; =&gt; &quot;protein array&quot;,&#xD;&#xA;	    &quot;MI:0090&quot; =&gt; &quot;protein complementation assay&quot;,&#xD;&#xA;	    &quot;MI:0091&quot; =&gt; &quot;chromatography technology&quot;,&#xD;&#xA;	    &quot;MI:0092&quot; =&gt; &quot;protein in situ array&quot;,&#xD;&#xA;	    &quot;MI:0095&quot; =&gt; &quot;proteinchip(r) on a surface-enhanced laser desorption/ionization&quot;,&#xD;&#xA;	    &quot;MI:0097&quot; =&gt; &quot;reverse ras recruitment system&quot;,&#xD;&#xA;	    &quot;MI:0098&quot; =&gt; &quot;ribosome display&quot;,&#xD;&#xA;	    &quot;MI:0099&quot; =&gt; &quot;scintillation proximity assay&quot;,&#xD;&#xA;	    &quot;MI:0107&quot; =&gt; &quot;surface plasmon resonance&quot;,&#xD;&#xA;	    &quot;MI:0108&quot; =&gt; &quot;t7 phage display&quot;,&#xD;&#xA;	    &quot;MI:0111&quot; =&gt; &quot;dihydrofolate reductase reconstruction&quot;,&#xD;&#xA;	    &quot;MI:0112&quot; =&gt; &quot;ubiquitin reconstruction&quot;,&#xD;&#xA;	    &quot;MI:0114&quot; =&gt; &quot;x-ray crystallography&quot;,&#xD;&#xA;	    &quot;MI:0115&quot; =&gt; &quot;yeast display&quot;,&#xD;&#xA;	    &quot;MI:0226&quot; =&gt; &quot;ion exchange chromatography&quot;,&#xD;&#xA;	    &quot;MI:0227&quot; =&gt; &quot;reverse phase chromatography&quot;,&#xD;&#xA;	    &quot;MI:0231&quot; =&gt; &quot;mammalian protein protein interaction trap&quot;,&#xD;&#xA;	    &quot;MI:0232&quot; =&gt; &quot;transcriptional complementation assay&quot;,&#xD;&#xA;	    &quot;MI:0255&quot; =&gt; &quot;post transcriptional interference&quot;,&#xD;&#xA;	    &quot;MI:0369&quot; =&gt; &quot;lex-a dimerization assay&quot;,&#xD;&#xA;	    &quot;MI:0370&quot; =&gt; &quot;tox-r dimerization assay&quot;,&#xD;&#xA;	    &quot;MI:0397&quot; =&gt; &quot;two hybrid array&quot;,&#xD;&#xA;	    &quot;MI:0398&quot; =&gt; &quot;two hybrid pooling approach&quot;,&#xD;&#xA;	    &quot;MI:0399&quot; =&gt; &quot;two hybrid fragment pooling approach&quot;,&#xD;&#xA;	    &quot;MI:0400&quot; =&gt; &quot;affinity technology&quot;,&#xD;&#xA;	    &quot;MI:0401&quot; =&gt; &quot;biochemical&quot;,&#xD;&#xA;	    &quot;MI:0405&quot; =&gt; &quot;competition binding&quot;,&#xD;&#xA;	    &quot;MI:0406&quot; =&gt; &quot;deacetylase assay&quot;,&#xD;&#xA;	    &quot;MI:0410&quot; =&gt; &quot;electron tomography&quot;,&#xD;&#xA;	    &quot;MI:0411&quot; =&gt; &quot;enzyme linked immunosorbent assay&quot;,&#xD;&#xA;	    &quot;MI:0415&quot; =&gt; &quot;enzymatic study&quot;,&#xD;&#xA;	    &quot;MI:0416&quot; =&gt; &quot;fluorescence microscopy&quot;,&#xD;&#xA;	    &quot;MI:0419&quot; =&gt; &quot;gtpase assay&quot;,&#xD;&#xA;	    &quot;MI:0420&quot; =&gt; &quot;kinase homogeneous time resolved fluorescence&quot;,&#xD;&#xA;	    &quot;MI:0423&quot; =&gt; &quot;in-gel kinase assay&quot;,&#xD;&#xA;	    &quot;MI:0424&quot; =&gt; &quot;protein kinase assay&quot;,&#xD;&#xA;	    &quot;MI:0425&quot; =&gt; &quot;kinase scintillation proximity assay&quot;,&#xD;&#xA;	    &quot;MI:0426&quot; =&gt; &quot;light microscopy&quot;,&#xD;&#xA;	    &quot;MI:0428&quot; =&gt; &quot;imaging technique&quot;,&#xD;&#xA;	    &quot;MI:0432&quot; =&gt; &quot;one hybrid&quot;,&#xD;&#xA;	    &quot;MI:0434&quot; =&gt; &quot;phosphatase assay&quot;,&#xD;&#xA;	    &quot;MI:0435&quot; =&gt; &quot;protease assay&quot;,&#xD;&#xA;	    &quot;MI:0437&quot; =&gt; &quot;protein three hybrid&quot;,&#xD;&#xA;	    &quot;MI:0440&quot; =&gt; &quot;saturation binding&quot;,&#xD;&#xA;	    &quot;MI:0508&quot; =&gt; &quot;deacetylase radiometric assay&quot;,&#xD;&#xA;	    &quot;MI:0509&quot; =&gt; &quot;phosphatase homogeneous time resolved fluorescence&quot;,&#xD;&#xA;	    &quot;MI:0510&quot; =&gt; &quot;homogeneous time resolved fluorescence&quot;,&#xD;&#xA;	    &quot;MI:0511&quot; =&gt; &quot;protease homogeneous time resolved fluorescence&quot;,&#xD;&#xA;	    &quot;MI:0512&quot; =&gt; &quot;zymography&quot;,&#xD;&#xA;	    &quot;MI:0513&quot; =&gt; &quot;collagen film assay&quot;,&#xD;&#xA;	    &quot;MI:0514&quot; =&gt; &quot;in gel phosphatase assay&quot;,&#xD;&#xA;	    &quot;MI:0515&quot; =&gt; &quot;methyltransferase assay&quot;,&#xD;&#xA;	    &quot;MI:0516&quot; =&gt; &quot;methyltransferase radiometric assay&quot;,&#xD;&#xA;	    &quot;MI:0655&quot; =&gt; &quot;lambda repressor two hybrid&quot;,&#xD;&#xA;	    &quot;MI:0657&quot; =&gt; &quot;systematic evolution of ligands by exponential enrichment&quot;,&#xD;&#xA;	    &quot;MI:0678&quot; =&gt; &quot;antibody array&quot;,&#xD;&#xA;	    &quot;MI:0695&quot; =&gt; &quot;sandwich immunoassay&quot;,&#xD;&#xA;	    &quot;MI:0696&quot; =&gt; &quot;polymerase assay&quot;,&#xD;&#xA;	    &quot;MI:0726&quot; =&gt; &quot;reverse two hybrid&quot;,&#xD;&#xA;	    &quot;MI:0727&quot; =&gt; &quot;lexa b52 complementation&quot;,&#xD;&#xA;	    &quot;MI:0728&quot; =&gt; &quot;gal4 vp16 complementation&quot;,&#xD;&#xA;	    &quot;MI:0809&quot; =&gt; &quot;bimolecular fluorescence complementation&quot;,&#xD;&#xA;	    &quot;MI:0813&quot; =&gt; &quot;proximity enzyme linked immunosorbent assay&quot;,&#xD;&#xA;	    &quot;MI:0824&quot; =&gt; &quot;x-ray powder diffraction&quot;,&#xD;&#xA;	    &quot;MI:0825&quot; =&gt; &quot;x-ray fiber diffraction&quot;,&#xD;&#xA;	    &quot;MI:0827&quot; =&gt; &quot;x-ray tomography&quot;,&#xD;&#xA;	    &quot;MI:0841&quot; =&gt; &quot;phosphotransferase assay&quot;,&#xD;&#xA;	    &quot;MI:0870&quot; =&gt; &quot;demethylase assay&quot;,&#xD;&#xA;	    &quot;MI:0872&quot; =&gt; &quot;atomic force microscopy&quot;,&#xD;&#xA;	    &quot;MI:0879&quot; =&gt; &quot;nucleoside triphosphatase assay&quot;,&#xD;&#xA;	    &quot;MI:0880&quot; =&gt; &quot;atpase assay&quot;,&#xD;&#xA;	    &quot;MI:0887&quot; =&gt; &quot;histone acetylase assay&quot;,&#xD;&#xA;	    &quot;MI:0889&quot; =&gt; &quot;acetylase assay&quot;,&#xD;&#xA;	    &quot;MI:0892&quot; =&gt; &quot;solid phase assay&quot;,&#xD;&#xA;	    &quot;MI:0894&quot; =&gt; &quot;electron diffraction&quot;,&#xD;&#xA;	    &quot;MI:0895&quot; =&gt; &quot;protein kinase A complementation&quot;,&#xD;&#xA;	    &quot;MI:0899&quot; =&gt; &quot;p3 filamentous phage display&quot;,&#xD;&#xA;	    &quot;MI:0900&quot; =&gt; &quot;p8 filamentous phage display&quot;,&#xD;&#xA;	    &quot;MI:0905&quot; =&gt; &quot;amplified luminescent proximity homogeneous assay&quot;,&#xD;&#xA;	    &quot;MI:0916&quot; =&gt; &quot;lexa vp16 complementation&quot;,&#xD;&#xA;	    &quot;MI:0920&quot; =&gt; &quot;ribonuclease assay&quot;,&#xD;&#xA;	    &quot;MI:0921&quot; =&gt; &quot;surface plasmon resonance array&quot;,&#xD;&#xA;	    &quot;MI:0946&quot; =&gt; &quot;ping&quot;,&#xD;&#xA;	    &quot;MI:0947&quot; =&gt; &quot;bead aggregation assay&quot;,&#xD;&#xA;	    &quot;MI:0949&quot; =&gt; &quot;gdp/gtp exchange assay&quot;,&#xD;&#xA;	    &quot;MI:0953&quot; =&gt; &quot;polymerization&quot;,&#xD;&#xA;	    &quot;MI:0968&quot; =&gt; &quot;biosensor&quot;,&#xD;&#xA;	    &quot;MI:0969&quot; =&gt; &quot;bio-layer interferometry&quot;,&#xD;&#xA;	    &quot;MI:0972&quot; =&gt; &quot;phosphopantetheinylase assay&quot;,&#xD;&#xA;	    &quot;MI:0976&quot; =&gt; &quot;total internal reflection fluorescence spectroscopy&quot;,&#xD;&#xA;	    &quot;MI:0979&quot; =&gt; &quot;oxidoreductase assay&quot;,&#xD;&#xA;	    &quot;MI:0984&quot; =&gt; &quot;deaminase assay&quot;,&#xD;&#xA;	    &quot;MI:0989&quot; =&gt; &quot;amidase assay&quot;,&#xD;&#xA;	    &quot;MI:0990&quot; =&gt; &quot;cleavage assay&quot;,&#xD;&#xA;	    &quot;MI:0991&quot; =&gt; &quot;lipid cleavage assay&quot;,&#xD;&#xA;	    &quot;MI:0992&quot; =&gt; &quot;defarnesylase assay&quot;,&#xD;&#xA;	    &quot;MI:0993&quot; =&gt; &quot;degeranylase assay&quot;,&#xD;&#xA;	    &quot;MI:0994&quot; =&gt; &quot;demyristoylase assay&quot;,&#xD;&#xA;	    &quot;MI:0995&quot; =&gt; &quot;depalmitoylase assay&quot;,&#xD;&#xA;	    &quot;MI:0996&quot; =&gt; &quot;deformylase assay&quot;,&#xD;&#xA;	    &quot;MI:0997&quot; =&gt; &quot;ubiquitinase assay&quot;,&#xD;&#xA;	    &quot;MI:0998&quot; =&gt; &quot;deubiquitinase assay&quot;,&#xD;&#xA;	    &quot;MI:0999&quot; =&gt; &quot;formylase assay&quot;,&#xD;&#xA;	    &quot;MI:1000&quot; =&gt; &quot;hydroxylase assay&quot;,&#xD;&#xA;	    &quot;MI:1001&quot; =&gt; &quot;lipidase assay&quot;,&#xD;&#xA;	    &quot;MI:1002&quot; =&gt; &quot;myristoylase assay&quot;,&#xD;&#xA;	    &quot;MI:1003&quot; =&gt; &quot;geranylgeranylase assay&quot;,&#xD;&#xA;	    &quot;MI:1004&quot; =&gt; &quot;palmitoylase assay&quot;,&#xD;&#xA;	    &quot;MI:1005&quot; =&gt; &quot;adp ribosylase assay&quot;,&#xD;&#xA;	    &quot;MI:1006&quot; =&gt; &quot;deglycosylase assay&quot;,&#xD;&#xA;	    &quot;MI:1007&quot; =&gt; &quot;glycosylase assay&quot;,&#xD;&#xA;	    &quot;MI:1008&quot; =&gt; &quot;sumoylase assay&quot;,&#xD;&#xA;	    &quot;MI:1009&quot; =&gt; &quot;desumoylase assay&quot;,&#xD;&#xA;	    &quot;MI:1010&quot; =&gt; &quot;neddylase assay&quot;,&#xD;&#xA;	    &quot;MI:1011&quot; =&gt; &quot;deneddylase assay&quot;,&#xD;&#xA;	    &quot;MI:1016&quot; =&gt; &quot;fluorescence recovery after photobleaching&quot;,&#xD;&#xA;	    &quot;MI:1019&quot; =&gt; &quot;protein phosphatase assay&quot;,&#xD;&#xA;	    &quot;MI:1024&quot; =&gt; &quot;scanning electron microscopy&quot;,&#xD;&#xA;	    &quot;MI:1026&quot; =&gt; &quot;diphtamidase assay&quot;,&#xD;&#xA;	    &quot;MI:1030&quot; =&gt; &quot;excimer fluorescence&quot;,&#xD;&#xA;	    &quot;MI:1031&quot; =&gt; &quot;protein folding/unfolding&quot;,&#xD;&#xA;	    &quot;MI:1036&quot; =&gt; &quot;nucleotide exchange assay&quot;,&#xD;&#xA;	    &quot;MI:1037&quot; =&gt; &quot;Split renilla luciferase complementation&quot;,&#xD;&#xA;	    &quot;MI:1038&quot; =&gt; &quot;silicon nanowire field-effect transistor&quot;,&#xD;&#xA;	    &quot;MI:1087&quot; =&gt; &quot;monoclonal antibody blockade&quot;,&#xD;&#xA;	    &quot;MI:1088&quot; =&gt; &quot;phenotype-based detection assay&quot;,&#xD;&#xA;	    &quot;MI:1089&quot; =&gt; &quot;nuclear translocation assay&quot;,&#xD;&#xA;	    &quot;MI:1111&quot; =&gt; &quot;two hybrid bait or prey pooling approach&quot;,&#xD;&#xA;	    &quot;MI:1112&quot; =&gt; &quot;two hybrid prey pooling approach&quot;,&#xD;&#xA;	    &quot;MI:1113&quot; =&gt; &quot;two hybrid bait and prey pooling approach&quot;,&#xD;&#xA;	    &quot;MI:1137&quot; =&gt; &quot;carboxylation assay&quot;,&#xD;&#xA;	    &quot;MI:1138&quot; =&gt; &quot;decarboxylation assay&quot;,&#xD;&#xA;	    &quot;MI:1142&quot; =&gt; &quot;aminoacylation assay&quot;,&#xD;&#xA;	    &quot;MI:1145&quot; =&gt; &quot;phospholipase assay&quot;,&#xD;&#xA;	    &quot;MI:1147&quot; =&gt; &quot;ampylation assay&quot;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If you also want those that might not be direct binary but simply in the same complex, include:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;	      &quot;MI:0019&quot; =&gt; &quot;coimmunoprecipitation&quot;,&#xD;&#xA;	      &quot;MI:0006&quot; =&gt; &quot;anti bait coimmunoprecipitation&quot;,&#xD;&#xA;	      &quot;MI:0007&quot; =&gt; &quot;anti tag coimmunoprecipitation&quot;,&#xD;&#xA;	      &quot;MI:0858&quot; =&gt; &quot;immunodepleted coimmunoprecipitation&quot;,&#xD;&#xA;	      &quot;MI:0096&quot; =&gt; &quot;pull down&quot;,&#xD;&#xA;	      &quot;MI:0963&quot; =&gt; &quot;interactome parallel affinity capture&quot;,&#xD;&#xA;	      &quot;MI:0676&quot; =&gt; &quot;tandem affinity purification&quot;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cicblade.dep.usal.es:8080/APID/init.action&#xD;&#xA;  [2]: https://www.ebi.ac.uk/intact/&#xD;&#xA;  [3]: http://www.ebi.ac.uk/Tools/webservices/psicquic/view/main.xhtml&#xD;&#xA;  [4]: https://bioinformatics.stackexchange.com/a/577/298&#xD;&#xA;  [5]: https://en.wikipedia.org/wiki/Two-hybrid_screening&#xD;&#xA;  [6]: https://en.wikipedia.org/wiki/Chromatin_immunoprecipitation&#xD;&#xA;  [7]: http://www.ebi.ac.uk/ols/ontologies/MI/terms?obo_id=MI:0001" />
  <row Id="2282" PostHistoryTypeId="5" PostId="725" RevisionGUID="39df26d5-9e96-4bea-b9db-4a911cb2abf1" CreationDate="2017-06-14T20:57:57.380" UserId="138" Comment="added 184 characters in body" Text="DNASTAR's software is for purchase, but high quality. GenVision Pro does genomic visualization, including Sashimi plots.&#xD;&#xA;&#xD;&#xA;Edit: not sure why this answer is being downvoted, unless it's because the software isn't free. OP has tried IGV and SeqMonk, I mentioned an alternative he might not have heard of." />
  <row Id="2283" PostHistoryTypeId="2" PostId="741" RevisionGUID="f1ad8ae9-c50c-42c3-aba5-e037039e584c" CreationDate="2017-06-14T21:02:35.707" UserId="131" Text="Google searching for **NM_002084** gives below result:&#xD;&#xA;&#xD;&#xA;[NM_002084.4](https://www.ncbi.nlm.nih.gov/nuccore/NM_002084)&#xD;&#xA;This, I assume, is the latest version v4, hence `.4` suffix.&#xD;&#xA;&#xD;&#xA;Searching for preivious versions I get below, with note saying it was updated or removed.&#xD;&#xA;[NM_002084.3](https://www.ncbi.nlm.nih.gov/nuccore/89903006)&#xD;&#xA;&#xD;&#xA;&gt; This sequence has been updated. See current version.&#xD;&#xA;&#xD;&#xA;[NM_002084.2](https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?val=NM_002084.2)&#xD;&#xA;&#xD;&#xA;[NM_002084.1](https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?val=NM_002084.1)&#xD;&#xA;&#xD;&#xA;&gt; Record removed. This record was replaced or removed.&#xD;&#xA;&#xD;&#xA;Using *biomaRt* I can get the latest(**?**) version as below:&#xD;&#xA;&#xD;&#xA;    library(&quot;biomaRt&quot;)&#xD;&#xA;    &#xD;&#xA;    # define db&#xD;&#xA;    ensembl &lt;- useMart(&quot;ensembl&quot;, dataset = &quot;hsapiens_gene_ensembl&quot;)&#xD;&#xA;    &#xD;&#xA;    # get refseqs&#xD;&#xA;    getBM(attributes = c('refseq_mrna',&#xD;&#xA;                         'chromosome_name',&#xD;&#xA;                         'transcript_start',&#xD;&#xA;                         'transcript_end',&#xD;&#xA;                         'strand'),&#xD;&#xA;          filters = c('refseq_mrna'),&#xD;&#xA;          values = list(refseq_mrna = &quot;NM_002084&quot;),&#xD;&#xA;          mart = ensembl)&#xD;&#xA;    #  refseq_mrna chromosome_name transcript_start transcript_end strand&#xD;&#xA;    #1   NM_002084               5        151020438      151028992      1&#xD;&#xA;&#xD;&#xA;But querying for sepcific versions gives nothing:&#xD;&#xA;&#xD;&#xA;    getBM(attributes = c('refseq_mrna',&#xD;&#xA;                         'chromosome_name',&#xD;&#xA;                         'transcript_start',&#xD;&#xA;                         'transcript_end',&#xD;&#xA;                         'strand'),&#xD;&#xA;          filters = c('refseq_mrna'),&#xD;&#xA;          values = list(refseq_mrna = c(&quot;NM_002084.1&quot;, &quot;NM_002084.2&quot;, &quot;NM_002084.3&quot;, &quot;NM_002084.4&quot;)),&#xD;&#xA;          mart = ensembl)&#xD;&#xA;    # [1] refseq_mrna      chromosome_name  transcript_start transcript_end   strand          &#xD;&#xA;    # &lt;0 rows&gt; (or 0-length row.names)&#xD;&#xA;&#xD;&#xA;**Question:** How can I get all versions (preferably using R)?&#xD;&#xA;" />
  <row Id="2284" PostHistoryTypeId="1" PostId="741" RevisionGUID="f1ad8ae9-c50c-42c3-aba5-e037039e584c" CreationDate="2017-06-14T21:02:35.707" UserId="131" Text="Get RefSeq accession numbers with versions" />
  <row Id="2285" PostHistoryTypeId="3" PostId="741" RevisionGUID="f1ad8ae9-c50c-42c3-aba5-e037039e584c" CreationDate="2017-06-14T21:02:35.707" UserId="131" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;biomart&gt;&lt;refseq&gt;" />
  <row Id="2286" PostHistoryTypeId="2" PostId="742" RevisionGUID="28d0eadb-5615-443e-9e7b-eedce2d711a7" CreationDate="2017-06-14T22:42:30.370" UserId="138" Text="I wrote a very quick and dirty script to handle conversion between file types using BioJava.&#xD;&#xA;&#xD;&#xA;https://github.com/eedlund/Utils/tree/master/BioUtils&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Download the jar file here:&#xD;&#xA;https://github.com/eedlund/Utils/raw/master/BioUtils/BioUtils.jar&#xD;&#xA;&#xD;&#xA;To run:&#xD;&#xA;java -jar BioUtils.jar $FILE $TYPE&#xD;&#xA;&#xD;&#xA;where $FILE is a PDB or mmCIF file you'd like to convert and $TYPE is the format of the output file [PDB, CIF, MMTF]." />
  <row Id="2287" PostHistoryTypeId="5" PostId="725" RevisionGUID="e4690a06-bae3-47b2-aa80-2584e4809fed" CreationDate="2017-06-14T22:46:43.843" UserId="138" Comment="added 142 characters in body" Text="[![GenVision Pro Sashimi Plot][1]][1]DNASTAR's software is for purchase, but high quality. GenVision Pro does genomic visualization, including Sashimi plots.&#xD;&#xA;&#xD;&#xA;Edit: not sure why this answer is being downvoted, unless it's because the software isn't free. OP has tried IGV and SeqMonk, I mentioned an alternative he might not have heard of.&#xD;&#xA;&#xD;&#xA;Here is a video demonstrating the use of Sashimi plots in GenVision Pro:&#xD;&#xA;&#xD;&#xA;http://www.dnastar.com/t-support-videos.aspx?video=YJvcERoSIsg&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/xS23U.png" />
  <row Id="2289" PostHistoryTypeId="2" PostId="743" RevisionGUID="e94551ec-f27b-48a5-a939-a3241568bbb5" CreationDate="2017-06-14T22:56:46.020" UserId="138" Text="If you choose to perform your own culling of the PDB, resolution is probably the first thing you'll want to look at, which as Davidmh mentions is the main selection criteria for PISCES. High quality structures will also have better R-factor values. You can also give preference based on experimental technique, in descending order of quality:&#xD;&#xA;&#xD;&#xA;Neutron diffraction, X-ray diffraction, solution/solid state NMR, electron microscopy/crystallography, fiber diffraction, solution scattering." />
  <row Id="2290" PostHistoryTypeId="2" PostId="744" RevisionGUID="d3394a3a-3712-4bc6-83b8-797955139bea" CreationDate="2017-06-15T05:40:30.137" UserId="150" Text="I don't believe this is possible using `biomaRt`, nor using `AnnotationHub`.&#xD;&#xA;&#xD;&#xA;I have two suggestions, neither of them very satisfactory. First, you can specify an Ensembl archive for `biomaRt`, for example:&#xD;&#xA;&#xD;&#xA;    mart72.hs &lt;- useMart(&quot;ENSEMBL_MART_ENSEMBL&quot;, &quot;hsapiens_gene_ensembl&quot;, &#xD;&#xA;                          host = &quot;jun2013.archive.ensembl.org&quot;)&#xD;&#xA;&#xD;&#xA;Of course, that requires that you have some idea of the date for each accession version and that the archives span that date - so not especially useful.&#xD;&#xA;&#xD;&#xA;The other option is to access EUtils using *e.g.* `rentrez`, which does allow search by version number:&#xD;&#xA;&#xD;&#xA;    library(rentrez)&#xD;&#xA;    es &lt;- entrez_search(&quot;nuccore&quot;, &quot;NM_002084.1 NM_002084.2&quot;)&#xD;&#xA;    es$ids&#xD;&#xA;&#xD;&#xA;    [1] &quot;4504104&quot; &quot;6006000&quot;&#xD;&#xA;&#xD;&#xA;So knowing the accession, you could simply append 1, 2, 3... to it, run the search and see if UIDs come back, then get them using `entrez_fetch`." />
  <row Id="2291" PostHistoryTypeId="2" PostId="745" RevisionGUID="be57b792-4699-4508-9f7b-4870f1171bbf" CreationDate="2017-06-15T06:29:13.993" UserId="156" Text="The latest version of Albacore from Oxford Nanopore Technologies calls bases from raw fast5 files. A useful piece of output is the `sequence_summary.txt`, which is a big tab-delimited file with information on each read. &#xD;&#xA;&#xD;&#xA;One of the columns in there is the `channel`, which references the channel number from 1 to 512 on the flowcell. &#xD;&#xA;&#xD;&#xA;These channels obviously have a physical layout on the flowcell. What is the layout? I.e. how many channels are there per row and per column, and how do the channel numbers map to the rows and columns?&#xD;&#xA;&#xD;&#xA;My best guess is that it's a 16*24 array, but I don't know for sure, and even if I did I wouldn't know whether the `channels` map across the shorter or the longer axis in order (or maybe they are not in order at all, but that would be silly).&#xD;&#xA;" />
  <row Id="2292" PostHistoryTypeId="1" PostId="745" RevisionGUID="be57b792-4699-4508-9f7b-4870f1171bbf" CreationDate="2017-06-15T06:29:13.993" UserId="156" Text="Minion channel ID's from Albacore" />
  <row Id="2293" PostHistoryTypeId="3" PostId="745" RevisionGUID="be57b792-4699-4508-9f7b-4870f1171bbf" CreationDate="2017-06-15T06:29:13.993" UserId="156" Text="&lt;nanopore&gt;&lt;minion&gt;&lt;albacore&gt;" />
  <row Id="2294" PostHistoryTypeId="5" PostId="745" RevisionGUID="eea67f7e-b9d9-4dad-b4db-e5c19cf25420" CreationDate="2017-06-15T06:34:33.327" UserId="156" Comment="I can't count" Text="The latest version of Albacore from Oxford Nanopore Technologies calls bases from raw fast5 files. A useful piece of output is the `sequence_summary.txt`, which is a big tab-delimited file with information on each read. &#xD;&#xA;&#xD;&#xA;One of the columns in there is the `channel`, which references the channel number from 1 to 512 on the flowcell. &#xD;&#xA;&#xD;&#xA;These channels obviously have a physical layout on the flowcell. What is the layout? I.e. how many channels are there per row and per column, and how do the channel numbers map to the rows and columns?&#xD;&#xA;&#xD;&#xA;My best guess is that it's a 16*32 array, but I don't know for sure, and even if I did I wouldn't know whether the `channels` map across the shorter or the longer axis in order (or maybe they are not in order at all, but that would be silly).&#xD;&#xA;" />
  <row Id="2295" PostHistoryTypeId="5" PostId="744" RevisionGUID="54629781-e5d5-4699-8e33-119631038a6d" CreationDate="2017-06-15T07:16:26.393" UserId="150" Comment="added 675 characters in body" Text="I don't believe this is possible using `biomaRt`, nor using `AnnotationHub`.&#xD;&#xA;&#xD;&#xA;I have two suggestions, neither of them very satisfactory. First, you can specify an Ensembl archive for `biomaRt`, for example:&#xD;&#xA;&#xD;&#xA;    mart72.hs &lt;- useMart(&quot;ENSEMBL_MART_ENSEMBL&quot;, &quot;hsapiens_gene_ensembl&quot;, &#xD;&#xA;                          host = &quot;jun2013.archive.ensembl.org&quot;)&#xD;&#xA;&#xD;&#xA;Of course, that requires that you have some idea of the date for each accession version and that the archives span that date - so not especially useful.&#xD;&#xA;&#xD;&#xA;The other option is to access EUtils using *e.g.* `rentrez`, which does allow search by version number:&#xD;&#xA;&#xD;&#xA;    library(rentrez)&#xD;&#xA;    es &lt;- entrez_search(&quot;nuccore&quot;, &quot;NM_002084.1 NM_002084.2&quot;)&#xD;&#xA;    es$ids&#xD;&#xA;&#xD;&#xA;    [1] &quot;4504104&quot; &quot;6006000&quot;&#xD;&#xA;&#xD;&#xA;So knowing the accession, you could simply append 1, 2, 3... to it, run the search and see if UIDs come back, then get them using `entrez_fetch`.&#xD;&#xA;&#xD;&#xA;EDIT: here's a quick and dirty function which takes an accession as query, appends version = 1, fetches the ID, then increments the version and repeats until no more results are returned. It is not well-tested!&#xD;&#xA;&#xD;&#xA;    getVersions &lt;- function(accession) {&#xD;&#xA;      require(rentrez)&#xD;&#xA;      ids &lt;- character()&#xD;&#xA;      version &lt;- 1&#xD;&#xA;      repeat({&#xD;&#xA;        es &lt;- entrez_search(&quot;nuccore&quot;, paste0(accession, &quot;.&quot;, version))&#xD;&#xA;        if(length(es$ids) == 0) {&#xD;&#xA;          break&#xD;&#xA;        }&#xD;&#xA;        ids[version] &lt;- es$ids&#xD;&#xA;        version &lt;- version + 1&#xD;&#xA;      })&#xD;&#xA;      ids&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Example:&#xD;&#xA;&#xD;&#xA;    getVersions(&quot;NM_002804&quot;)&#xD;&#xA;    [1] &quot;4506210&quot;   &quot;21361143&quot;  &quot;24430153&quot;  &quot;133987588&quot;&#xD;&#xA;" />
  <row Id="2296" PostHistoryTypeId="2" PostId="746" RevisionGUID="eaaf9c74-b9eb-4ee4-b173-a009b9b9f620" CreationDate="2017-06-15T08:09:40.980" UserId="681" Text="[poRe][1] has a show.layout() function which shows you the 32*16 grid on which the channels are arranged.&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]&#xD;&#xA; [1,]  125  121  117  113  109  105  101   97   93    89    85    81    77&#xD;&#xA; [2,]  126  122  118  114  110  106  102   98   94    90    86    82    78&#xD;&#xA; [3,]  127  123  119  115  111  107  103   99   95    91    87    83    79&#xD;&#xA; [4,]  128  124  120  116  112  108  104  100   96    92    88    84    80&#xD;&#xA; [5,]  253  249  245  241  237  233  229  225  221   217   213   209   205&#xD;&#xA; [6,]  254  250  246  242  238  234  230  226  222   218   214   210   206&#xD;&#xA; [7,]  255  251  247  243  239  235  231  227  223   219   215   211   207&#xD;&#xA; [8,]  256  252  248  244  240  236  232  228  224   220   216   212   208&#xD;&#xA; [9,]  381  377  373  369  365  361  357  353  349   345   341   337   333&#xD;&#xA;[10,]  382  378  374  370  366  362  358  354  350   346   342   338   334&#xD;&#xA;[11,]  383  379  375  371  367  363  359  355  351   347   343   339   335&#xD;&#xA;[12,]  384  380  376  372  368  364  360  356  352   348   344   340   336&#xD;&#xA;[13,]  509  505  501  497  493  489  485  481  477   473   469   465   461&#xD;&#xA;[14,]  510  506  502  498  494  490  486  482  478   474   470   466   462&#xD;&#xA;[15,]  511  507  503  499  495  491  487  483  479   475   471   467   463&#xD;&#xA;[16,]  512  508  504  500  496  492  488  484  480   476   472   468   464&#xD;&#xA;      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]&#xD;&#xA; [1,]    73    69    65    61    57    53    49    45    41    37    33    29&#xD;&#xA; [2,]    74    70    66    62    58    54    50    46    42    38    34    30&#xD;&#xA; [3,]    75    71    67    63    59    55    51    47    43    39    35    31&#xD;&#xA; [4,]    76    72    68    64    60    56    52    48    44    40    36    32&#xD;&#xA; [5,]   201   197   193   189   185   181   177   173   169   165   161   157&#xD;&#xA; [6,]   202   198   194   190   186   182   178   174   170   166   162   158&#xD;&#xA; [7,]   203   199   195   191   187   183   179   175   171   167   163   159&#xD;&#xA; [8,]   204   200   196   192   188   184   180   176   172   168   164   160&#xD;&#xA; [9,]   329   325   321   317   313   309   305   301   297   293   289   285&#xD;&#xA;[10,]   330   326   322   318   314   310   306   302   298   294   290   286&#xD;&#xA;[11,]   331   327   323   319   315   311   307   303   299   295   291   287&#xD;&#xA;[12,]   332   328   324   320   316   312   308   304   300   296   292   288&#xD;&#xA;[13,]   457   453   449   445   441   437   433   429   425   421   417   413&#xD;&#xA;[14,]   458   454   450   446   442   438   434   430   426   422   418   414&#xD;&#xA;[15,]   459   455   451   447   443   439   435   431   427   423   419   415&#xD;&#xA;[16,]   460   456   452   448   444   440   436   432   428   424   420   416&#xD;&#xA;      [,26] [,27] [,28] [,29] [,30] [,31] [,32]&#xD;&#xA; [1,]    25    21    17    13     9     5     1&#xD;&#xA; [2,]    26    22    18    14    10     6     2&#xD;&#xA; [3,]    27    23    19    15    11     7     3&#xD;&#xA; [4,]    28    24    20    16    12     8     4&#xD;&#xA; [5,]   153   149   145   141   137   133   129&#xD;&#xA; [6,]   154   150   146   142   138   134   130&#xD;&#xA; [7,]   155   151   147   143   139   135   131&#xD;&#xA; [8,]   156   152   148   144   140   136   132&#xD;&#xA; [9,]   281   277   273   269   265   261   257&#xD;&#xA;[10,]   282   278   274   270   266   262   258&#xD;&#xA;[11,]   283   279   275   271   267   263   259&#xD;&#xA;[12,]   284   280   276   272   268   264   260&#xD;&#xA;[13,]   409   405   401   397   393   389   385&#xD;&#xA;[14,]   410   406   402   398   394   390   386&#xD;&#xA;[15,]   411   407   403   399   395   391   387&#xD;&#xA;[16,]   412   408   404   400   396   392   388&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/mw55309/poRe_docs" />
  <row Id="2298" PostHistoryTypeId="2" PostId="747" RevisionGUID="6619d8d6-b245-45d3-8016-9b03763e531d" CreationDate="2017-06-15T08:49:32.977" UserId="868" Text="Since I have seen NCBI gene names were called &quot;Entrez ID&quot; for the first time, I am wondering where that comes from. Such a weird name!&#xD;&#xA;&#xD;&#xA;Does anybody know where that originates?&#xD;&#xA;&#xD;&#xA;My hypothesis is: in French, &quot;entrez&quot; can be translated to &quot;please enter&quot; (it is `to enter` in the imperative mood), so I'm guessing that might come from a past website/database in French where `Entrez ID` as in &quot;Please enter the ID&quot; was written in the search bar...&#xD;&#xA;&#xD;&#xA;But I could not find any reference... I cannot be the only one to ask himself this!&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2299" PostHistoryTypeId="1" PostId="747" RevisionGUID="6619d8d6-b245-45d3-8016-9b03763e531d" CreationDate="2017-06-15T08:49:32.977" UserId="868" Text="What is the etymology of &quot;Entrez ID&quot;?" />
  <row Id="2300" PostHistoryTypeId="3" PostId="747" RevisionGUID="6619d8d6-b245-45d3-8016-9b03763e531d" CreationDate="2017-06-15T08:49:32.977" UserId="868" Text="&lt;gene&gt;" />
  <row Id="2301" PostHistoryTypeId="2" PostId="748" RevisionGUID="9d1caf98-ba45-46d8-9034-3358c7e7bbd0" CreationDate="2017-06-15T09:03:44.387" UserId="191" Text="The first version of Entrez database was [distributed][1] by NCBI in 1991 (on CD-ROM). At that time, it consisted of nucleotide sequences, protein sequences and associated citations and abstracts from MEDLINE. &#xD;&#xA;&#xD;&#xA;So even at the early stage it contained objects (entries) of very different types (sequences and abstracts). While I don't know the correct answer, my guess is that Entrez is an intentionally misspelled word &quot;entries&quot;. As Google comes from misspelled &quot;googol&quot; and Flickr is a misspelled version &quot;flicker&quot;.&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/books/NBK3837/" />
  <row Id="2302" PostHistoryTypeId="5" PostId="748" RevisionGUID="116755db-94aa-49aa-852b-229fa7413edb" CreationDate="2017-06-15T09:09:42.760" UserId="191" Comment="extend the answer" Text="The first version of Entrez database was [distributed][1] by NCBI in 1991 (on CD-ROM). At that time, it consisted of nucleotide sequences, protein sequences and associated citations and abstracts from MEDLINE. &#xD;&#xA;&#xD;&#xA;So even at the early stage it contained objects (entries) of very different types (sequences and abstracts). It is hard to find a word which will accurately and briefly describe all the types of data.&#xD;&#xA;&#xD;&#xA;While I don't know the correct answer and this is purely a speculation, my guess is that Entrez is an intentionally misspelled word &quot;entries&quot;; which accurately describes all the datatypes in the database.&#xD;&#xA;&#xD;&#xA;Using a misspelled word as a name is not uncommon, e.g. Google comes from misspelled &quot;googol&quot; and Flickr is a misspelled version of &quot;flicker&quot;.&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/books/NBK3837/" />
  <row Id="2303" PostHistoryTypeId="5" PostId="676" RevisionGUID="c98dbc2c-a014-4b62-9b64-95b9278727a2" CreationDate="2017-06-15T09:35:18.510" UserId="292" Comment="Made title reflect more the content of the question, tried to make the constructive aspect of the question more apparent" Text="Are there any advantages to learning Biopython instead of learning Bioperl?&#xD;&#xA;&#xD;&#xA;Ideally, we would learn both, but someone starting out in bioinformatics may have to chose what to learn first depending on the kind of problems actually encountered.&#xD;&#xA;&#xD;&#xA;**Are there problems for which Biopython is better than Bioperl (or vice-versa)?**" />
  <row Id="2304" PostHistoryTypeId="4" PostId="676" RevisionGUID="c98dbc2c-a014-4b62-9b64-95b9278727a2" CreationDate="2017-06-15T09:35:18.510" UserId="292" Comment="Made title reflect more the content of the question, tried to make the constructive aspect of the question more apparent" Text="For what bioinformatics tasks is Biopython more adapted than Bioperl?" />
  <row Id="2305" PostHistoryTypeId="24" PostId="676" RevisionGUID="c98dbc2c-a014-4b62-9b64-95b9278727a2" CreationDate="2017-06-15T09:35:18.510" Comment="Proposed by 292 approved by 57, -1 edit id of 201" />
  <row Id="2306" PostHistoryTypeId="5" PostId="676" RevisionGUID="577bdaa2-abc9-4c74-b389-f9f69b8fb287" CreationDate="2017-06-15T09:35:18.510" UserId="73" Comment="Made title reflect more the content of the question, tried to make the constructive aspect of the question more apparent" Text="Are there any advantages to learning Biopython instead of learning Bioperl?&#xD;&#xA;&#xD;&#xA;Ideally, we would learn both, but someone starting out in bioinformatics may have to choose what to learn first depending on the kind of problems actually encountered.&#xD;&#xA;&#xD;&#xA;**Are there problems for which Biopython is better than Bioperl (or vice-versa)?**" />
  <row Id="2307" PostHistoryTypeId="5" PostId="741" RevisionGUID="e7b98d9c-fbbb-486f-93f9-7f8462e0d3c0" CreationDate="2017-06-15T09:40:25.250" UserId="292" Comment="typos and language (not 100% confident about languages edit, though)" Text="Google searching for **NM_002084** gives the following result:&#xD;&#xA;&#xD;&#xA;[NM_002084.4](https://www.ncbi.nlm.nih.gov/nuccore/NM_002084)&#xD;&#xA;&#xD;&#xA;This, I assume, is the latest version v4, hence the `.4` suffix.&#xD;&#xA;&#xD;&#xA;Searching for previous versions I get the following results, along with notes saying it was updated or removed.&#xD;&#xA;&#xD;&#xA;[NM_002084.3](https://www.ncbi.nlm.nih.gov/nuccore/89903006)&#xD;&#xA;&#xD;&#xA;&gt; This sequence has been updated. See current version.&#xD;&#xA;&#xD;&#xA;[NM_002084.2](https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?val=NM_002084.2)&#xD;&#xA;&#xD;&#xA;[NM_002084.1](https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?val=NM_002084.1)&#xD;&#xA;&#xD;&#xA;&gt; Record removed. This record was replaced or removed.&#xD;&#xA;&#xD;&#xA;Using *biomaRt* I can get the latest(**?**) version as follows:&#xD;&#xA;&#xD;&#xA;    library(&quot;biomaRt&quot;)&#xD;&#xA;    &#xD;&#xA;    # define db&#xD;&#xA;    ensembl &lt;- useMart(&quot;ensembl&quot;, dataset = &quot;hsapiens_gene_ensembl&quot;)&#xD;&#xA;    &#xD;&#xA;    # get refseqs&#xD;&#xA;    getBM(attributes = c('refseq_mrna',&#xD;&#xA;                         'chromosome_name',&#xD;&#xA;                         'transcript_start',&#xD;&#xA;                         'transcript_end',&#xD;&#xA;                         'strand'),&#xD;&#xA;          filters = c('refseq_mrna'),&#xD;&#xA;          values = list(refseq_mrna = &quot;NM_002084&quot;),&#xD;&#xA;          mart = ensembl)&#xD;&#xA;    #  refseq_mrna chromosome_name transcript_start transcript_end strand&#xD;&#xA;    #1   NM_002084               5        151020438      151028992      1&#xD;&#xA;&#xD;&#xA;But querying for specific versions gives nothing:&#xD;&#xA;&#xD;&#xA;    getBM(attributes = c('refseq_mrna',&#xD;&#xA;                         'chromosome_name',&#xD;&#xA;                         'transcript_start',&#xD;&#xA;                         'transcript_end',&#xD;&#xA;                         'strand'),&#xD;&#xA;          filters = c('refseq_mrna'),&#xD;&#xA;          values = list(refseq_mrna = c(&quot;NM_002084.1&quot;, &quot;NM_002084.2&quot;, &quot;NM_002084.3&quot;, &quot;NM_002084.4&quot;)),&#xD;&#xA;          mart = ensembl)&#xD;&#xA;    # [1] refseq_mrna      chromosome_name  transcript_start transcript_end   strand          &#xD;&#xA;    # &lt;0 rows&gt; (or 0-length row.names)&#xD;&#xA;&#xD;&#xA;**Question:** How can I get all versions (preferably using R)?&#xD;&#xA;" />
  <row Id="2308" PostHistoryTypeId="24" PostId="741" RevisionGUID="e7b98d9c-fbbb-486f-93f9-7f8462e0d3c0" CreationDate="2017-06-15T09:40:25.250" Comment="Proposed by 292 approved by 73, 131 edit id of 200" />
  <row Id="2309" PostHistoryTypeId="5" PostId="712" RevisionGUID="779de6c6-4b2f-4d41-a9ab-96858f2e31b0" CreationDate="2017-06-15T09:53:50.680" UserId="235" Comment="Added detail on a couple of steps and reference to the orignal use of the proceedure." Text="Depending on how much effort you wish to put into this, here is one suggestion I have see used before (uses more than just R). Steps 1-4 come from [this paper][1] (see supplimentary material section &quot;Calculation of per gene Local FDR&quot;).&#xD;&#xA;&#xD;&#xA; 1. *Take your gene models and collapse the introns down to maybe 100bp*  &#xD;&#xA;  Thus if we have a gene with two exons and a 1kb intron eg. exons (1000, 1100) and (2000,2100), we reduce the intron size so that the exons are (1000,1100) and (1200,1300). This is because we need to find sufficient gene free space to fit the null gene in.    &#xD;&#xA;&#xD;&#xA; 2. *Shift each gene into the nearest genomic space at least 5 kp from a genomic annotation and free of ESTs.*&#xD;&#xA;&#xD;&#xA; 3. *Calculate the FPKM distribution of this shifted null set*  &#xD;&#xA;  &#xD;&#xA; 4. *From these two distributions it should be possible to calculate a [local FDR][3] for expression.*  &#xD;&#xA;    You will need to use a &quot;mixing proportion&quot; to do this. The above reference used [Qvality][2] to do this.&#xD;&#xA;&#xD;&#xA; 6. *Carry out differential expression analysis as described in [Konrad's answer][4]*&#xD;&#xA;&#xD;&#xA; 7. *Subset the differentially expressed genes to only consider those that are not expressed in the control condition*&#xD;&#xA;&#xD;&#xA; 8. *You might want to also recalculate the FDRs for differential expression as you are only considering a subset of the tests you might have.* &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://genome.cshlp.org/content/24/12/1918.long#ref-55&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2660870/&#xD;&#xA;  [3]: https://cran.r-project.org/web/packages/locfdr/vignettes/locfdr-example.pdf&#xD;&#xA;  [4]: https://bioinformatics.stackexchange.com/a/690/235" />
  <row Id="2310" PostHistoryTypeId="2" PostId="749" RevisionGUID="1c80e892-b39c-4735-8b3c-e67c99ddc46a" CreationDate="2017-06-15T10:08:22.437" UserId="73" Text="The results from poRe are quite different from the layout that I am familiar with. I suppose it's possible that it has changed with the most recent flow cells; I should probably look at the MinKNOW code sometime to work out if that is the case... or ask ONT directly.&#xD;&#xA;&#xD;&#xA;There are actually 2048 usable sequencing wells, hexagonally packed with four wells connected to the same sequencing sensor/channel via a multiplex (mux) selector. The combination of the mux and the channel number determines the physical location of the well. Unfortunately, the association between channel number and physical location is not obvious, and within each channel the muxes are also not in an obvious order, i.e. `[3,4,1,2,2,1,4,3]` for two adjacent channels.&#xD;&#xA;&#xD;&#xA;Here's what the layout was according to ONT a few years ago (for R7 flow cells):&#xD;&#xA;&#xD;&#xA; - Channels 1-64 occur at the top of the chip (the other low channel numbers are at the bottom)&#xD;&#xA; - Channels order down the chip: 1-64, 449-512, 385-448, 321-384, 257-320, 193-256, 129-192, 65-128&#xD;&#xA; - Muxes run from left to right in the order: 3, 4, 1, 2, 2, 1, 4, 3&#xD;&#xA;&#xD;&#xA;[![MinION Flow Cell layout][1]][1]&#xD;&#xA;&#xD;&#xA;Here's the backend of the code that I used to produce that image:&#xD;&#xA;&#xD;&#xA;    channel2poreXY &lt;- function(poreNum=rep(1:512, each=4),&#xD;&#xA;                               mux=NULL){&#xD;&#xA;        if(is.null(mux)){&#xD;&#xA;            mux &lt;- rep(1:4,length(poreNum))[1:length(poreNum)];&#xD;&#xA;        }&#xD;&#xA;        ## convert pore number to base 0 (makes arithmetic easier)&#xD;&#xA;        p0 &lt;- poreNum-1;&#xD;&#xA;        frame &lt;- floor(p0 / 32); # 0..15&#xD;&#xA;        side &lt;- (frame %% 2); # 0: left, 1: right&#xD;&#xA;        rightSide &lt;- side == 1;&#xD;&#xA;        yframe &lt;- (floor(frame / 2) + 7) %% 8; # 0..7 [bottom..top]&#xD;&#xA;        fPos &lt;- p0 %% 32;&#xD;&#xA;        framex &lt;- fPos %% 8;&#xD;&#xA;        framex[rightSide] &lt;- 7 - (fPos[rightSide] %% 8);&#xD;&#xA;        framey &lt;- (3 - floor(fPos / 8));&#xD;&#xA;        ## convert MUX to 0..3 for easier arithmetic&#xD;&#xA;        m0 &lt;- (mux+1) %% 4;&#xD;&#xA;        muxRev &lt;- (fPos %% 2) == 1;&#xD;&#xA;        m0[muxRev] &lt;- 3-m0[muxRev];&#xD;&#xA;        ## generate X,Y locations (with origin bottom left)&#xD;&#xA;        cx &lt;- (side*8 + framex) * 4 + m0;&#xD;&#xA;        cy &lt;- yframe * 4 + framey;&#xD;&#xA;        ## adjust for central gap&#xD;&#xA;        print(cx);&#xD;&#xA;        cx[cx&gt;31] &lt;- cx[cx&gt;31]+4;&#xD;&#xA;        ## adjust for hexagonal grid&#xD;&#xA;        cx &lt;- cx + (cy %% 2) * sqrt(3)/4;&#xD;&#xA;        return(cbind(cx,cy));&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/twBhP.png&#xD;&#xA;" />
  <row Id="2311" PostHistoryTypeId="5" PostId="744" RevisionGUID="07948787-e8ed-4190-93cd-8cb6d466952e" CreationDate="2017-06-15T10:17:12.697" UserId="150" Comment="added 4 characters in body" Text="I don't believe this is possible using `biomaRt`, nor using `AnnotationHub`.&#xD;&#xA;&#xD;&#xA;I have two suggestions, neither of them very satisfactory. First, you can specify an Ensembl archive for `biomaRt`, for example:&#xD;&#xA;&#xD;&#xA;    mart72.hs &lt;- useMart(&quot;ENSEMBL_MART_ENSEMBL&quot;, &quot;hsapiens_gene_ensembl&quot;, &#xD;&#xA;                          host = &quot;jun2013.archive.ensembl.org&quot;)&#xD;&#xA;&#xD;&#xA;Of course, that requires that you have some idea of the date for each accession version and that the archives span that date - so not especially useful.&#xD;&#xA;&#xD;&#xA;The other option is to access EUtils using *e.g.* `rentrez`, which does allow search by version number:&#xD;&#xA;&#xD;&#xA;    library(rentrez)&#xD;&#xA;    es &lt;- entrez_search(&quot;nuccore&quot;, &quot;NM_002084.1 NM_002084.2&quot;)&#xD;&#xA;    es$ids&#xD;&#xA;&#xD;&#xA;    [1] &quot;4504104&quot; &quot;6006000&quot;&#xD;&#xA;&#xD;&#xA;So knowing the accession, you could simply append 1, 2, 3... to it, run the search and see if UIDs come back, then get them using `entrez_fetch`.&#xD;&#xA;&#xD;&#xA;EDIT: here's a quick and dirty function which takes an accession as query, appends version = 1, fetches the ID, then increments the version and repeats until no more results are returned. It is not well-tested!&#xD;&#xA;&#xD;&#xA;    getVersions &lt;- function(accession) {&#xD;&#xA;      require(rentrez)&#xD;&#xA;      ids &lt;- character()&#xD;&#xA;      version &lt;- 1&#xD;&#xA;      repeat({&#xD;&#xA;        es &lt;- entrez_search(&quot;nuccore&quot;, paste0(accession, &quot;.&quot;, version))&#xD;&#xA;        if(length(es$ids) == 0) {&#xD;&#xA;          break&#xD;&#xA;        }&#xD;&#xA;        ids[version] &lt;- es$ids&#xD;&#xA;        version &lt;- version + 1&#xD;&#xA;      })&#xD;&#xA;      ids&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Example:&#xD;&#xA;&#xD;&#xA;    getVersions(&quot;NM_002084&quot;)&#xD;&#xA;    [1] &quot;4504104&quot;    &quot;6006000&quot;    &quot;89903006&quot;   &quot;1048339180&quot;&#xD;&#xA;" />
  <row Id="2312" PostHistoryTypeId="2" PostId="750" RevisionGUID="08a684cd-0878-4e87-a339-4cc1f142305b" CreationDate="2017-06-15T10:31:01.647" UserId="734" Text="I saw [this nature news][1], first I am not sure what's the meaning of &quot;tumor cell lines&quot;. &#xA;&#xA;Second, does it mean that [Cellminer][2] is obsolete? &#xA;What are the new tools to analyze the &quot;new cell lines&quot;?&#xA;&#xA;&#xA;  [1]: http://www.nature.com/news/us-cancer-institute-to-overhaul-tumour-cell-lines-1.19364&#xA;  [2]: https://discover.nci.nih.gov/cellminer/" />
  <row Id="2313" PostHistoryTypeId="1" PostId="750" RevisionGUID="08a684cd-0878-4e87-a339-4cc1f142305b" CreationDate="2017-06-15T10:31:01.647" UserId="734" Text="The &quot;new NCI-60&quot; tumor cell lines" />
  <row Id="2314" PostHistoryTypeId="3" PostId="750" RevisionGUID="08a684cd-0878-4e87-a339-4cc1f142305b" CreationDate="2017-06-15T10:31:01.647" UserId="734" Text="&lt;cancer&gt;" />
  <row Id="2315" PostHistoryTypeId="5" PostId="746" RevisionGUID="7c3b7c65-3e07-41e0-9768-b245b80ca707" CreationDate="2017-06-15T10:37:36.353" UserId="29" Comment="fix formatting" Text="[poRe][1] has a `show.layout()` function which shows you the 32*16 grid on which the channels are arranged.&#xD;&#xA;&#xD;&#xA;         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]&#xD;&#xA;     [1,]  125  121  117  113  109  105  101   97   93    89    85    81    77&#xD;&#xA;     [2,]  126  122  118  114  110  106  102   98   94    90    86    82    78&#xD;&#xA;     [3,]  127  123  119  115  111  107  103   99   95    91    87    83    79&#xD;&#xA;     [4,]  128  124  120  116  112  108  104  100   96    92    88    84    80&#xD;&#xA;     [5,]  253  249  245  241  237  233  229  225  221   217   213   209   205&#xD;&#xA;     [6,]  254  250  246  242  238  234  230  226  222   218   214   210   206&#xD;&#xA;     [7,]  255  251  247  243  239  235  231  227  223   219   215   211   207&#xD;&#xA;     [8,]  256  252  248  244  240  236  232  228  224   220   216   212   208&#xD;&#xA;     [9,]  381  377  373  369  365  361  357  353  349   345   341   337   333&#xD;&#xA;    [10,]  382  378  374  370  366  362  358  354  350   346   342   338   334&#xD;&#xA;    [11,]  383  379  375  371  367  363  359  355  351   347   343   339   335&#xD;&#xA;    [12,]  384  380  376  372  368  364  360  356  352   348   344   340   336&#xD;&#xA;    [13,]  509  505  501  497  493  489  485  481  477   473   469   465   461&#xD;&#xA;    [14,]  510  506  502  498  494  490  486  482  478   474   470   466   462&#xD;&#xA;    [15,]  511  507  503  499  495  491  487  483  479   475   471   467   463&#xD;&#xA;    [16,]  512  508  504  500  496  492  488  484  480   476   472   468   464&#xD;&#xA;          [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]&#xD;&#xA;     [1,]    73    69    65    61    57    53    49    45    41    37    33    29&#xD;&#xA;     [2,]    74    70    66    62    58    54    50    46    42    38    34    30&#xD;&#xA;     [3,]    75    71    67    63    59    55    51    47    43    39    35    31&#xD;&#xA;     [4,]    76    72    68    64    60    56    52    48    44    40    36    32&#xD;&#xA;     [5,]   201   197   193   189   185   181   177   173   169   165   161   157&#xD;&#xA;     [6,]   202   198   194   190   186   182   178   174   170   166   162   158&#xD;&#xA;     [7,]   203   199   195   191   187   183   179   175   171   167   163   159&#xD;&#xA;     [8,]   204   200   196   192   188   184   180   176   172   168   164   160&#xD;&#xA;     [9,]   329   325   321   317   313   309   305   301   297   293   289   285&#xD;&#xA;    [10,]   330   326   322   318   314   310   306   302   298   294   290   286&#xD;&#xA;    [11,]   331   327   323   319   315   311   307   303   299   295   291   287&#xD;&#xA;    [12,]   332   328   324   320   316   312   308   304   300   296   292   288&#xD;&#xA;    [13,]   457   453   449   445   441   437   433   429   425   421   417   413&#xD;&#xA;    [14,]   458   454   450   446   442   438   434   430   426   422   418   414&#xD;&#xA;    [15,]   459   455   451   447   443   439   435   431   427   423   419   415&#xD;&#xA;    [16,]   460   456   452   448   444   440   436   432   428   424   420   416&#xD;&#xA;          [,26] [,27] [,28] [,29] [,30] [,31] [,32]&#xD;&#xA;     [1,]    25    21    17    13     9     5     1&#xD;&#xA;     [2,]    26    22    18    14    10     6     2&#xD;&#xA;     [3,]    27    23    19    15    11     7     3&#xD;&#xA;     [4,]    28    24    20    16    12     8     4&#xD;&#xA;     [5,]   153   149   145   141   137   133   129&#xD;&#xA;     [6,]   154   150   146   142   138   134   130&#xD;&#xA;     [7,]   155   151   147   143   139   135   131&#xD;&#xA;     [8,]   156   152   148   144   140   136   132&#xD;&#xA;     [9,]   281   277   273   269   265   261   257&#xD;&#xA;    [10,]   282   278   274   270   266   262   258&#xD;&#xA;    [11,]   283   279   275   271   267   263   259&#xD;&#xA;    [12,]   284   280   276   272   268   264   260&#xD;&#xA;    [13,]   409   405   401   397   393   389   385&#xD;&#xA;    [14,]   410   406   402   398   394   390   386&#xD;&#xA;    [15,]   411   407   403   399   395   391   387&#xD;&#xA;    [16,]   412   408   404   400   396   392   388&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/mw55309/poRe_docs" />
  <row Id="2316" PostHistoryTypeId="5" PostId="750" RevisionGUID="b560f057-c12b-492e-b530-3485ecdda9ab" CreationDate="2017-06-15T10:38:38.543" UserId="734" Comment="added 28 characters in body; added 53 characters in body" Text="I saw [this nature news][1], first I am not sure what's the meaning of &quot;tumor cell lines&quot;. &#xA;&#xA;Second, does it mean that [Cellminer][2] is obsolete? &#xA;What are the new tools to analyze the &quot;new cell lines&quot;? Where's the new repository?&#xA;&#xA;What about the EuroPDX that the article refers to? &#xA;&#xA;&#xA;  [1]: http://www.nature.com/news/us-cancer-institute-to-overhaul-tumour-cell-lines-1.19364&#xA;  [2]: https://discover.nci.nih.gov/cellminer/" />
  <row Id="2317" PostHistoryTypeId="5" PostId="750" RevisionGUID="57762fc9-1304-4a29-a5aa-a2570b02487f" CreationDate="2017-06-15T11:11:44.183" UserId="734" Comment="added 11 characters in body" Text="I saw [this nature news][1], first I am not sure what's the meaning of &quot;tumor cell lines&quot;. &#xA;&#xA;Second, does it mean that [Cellminer][2] is obsolete? &#xA;What are the new tools to analyze the &quot;new cell lines&quot;? Where's the new repository?&#xA;&#xA;What about the European initative that the article refers to? &#xA;&#xA;&#xA;  [1]: http://www.nature.com/news/us-cancer-institute-to-overhaul-tumour-cell-lines-1.19364&#xA;  [2]: https://discover.nci.nih.gov/cellminer/" />
  <row Id="2318" PostHistoryTypeId="2" PostId="751" RevisionGUID="6af1f45a-d637-45ca-8146-e64ba1bfa69c" CreationDate="2017-06-15T11:13:40.643" UserId="208" Text="I'm going through an RNA-seq pipeline in `R/Bioconductor` and want to try multiple parameters at subsequent steps, for example, running clustering with different settings, running RegressOut or not on unwanted effects etc. That's a lot of &quot;versions&quot;, even if I don't do combinations of these steps.&#xD;&#xA;&#xD;&#xA;How can I keep track of this, and my conclusions?&#xD;&#xA;Not necessarily want to save the results.&#xD;&#xA;&#xD;&#xA;* Save the different scripts with git (seems overkill)&#xD;&#xA;* Make notes in the script itself&#xD;&#xA;" />
  <row Id="2319" PostHistoryTypeId="1" PostId="751" RevisionGUID="6af1f45a-d637-45ca-8146-e64ba1bfa69c" CreationDate="2017-06-15T11:13:40.643" UserId="208" Text="What are the ways to keep track of branches in the analysis?" />
  <row Id="2320" PostHistoryTypeId="3" PostId="751" RevisionGUID="6af1f45a-d637-45ca-8146-e64ba1bfa69c" CreationDate="2017-06-15T11:13:40.643" UserId="208" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;git&gt;" />
  <row Id="2321" PostHistoryTypeId="5" PostId="749" RevisionGUID="89fe18b2-e57d-4c7b-8c5b-89345fc8d0d5" CreationDate="2017-06-15T11:37:36.447" UserId="73" Comment="Added current layout" Text="There are actually 2048 usable sequencing wells, hexagonally packed with four wells connected to the same sequencing sensor/channel via a multiplex (mux) selector. The combination of the mux and the channel number determines the physical location of the well. Unfortunately, the association between channel number and physical location is not obvious, and within each channel the muxes are also not in an obvious order, i.e. `[3,4,1,2,2,1,4,3]` for two adjacent channels.&#xD;&#xA;&#xD;&#xA;Here's a text description of the layout as it was for R7. It doesn't seem to have changed for the most recent flow cells (R9.5):&#xD;&#xA;&#xD;&#xA; - Channels 1-64 occur at the top of the chip (the other low channel numbers are at the bottom)&#xD;&#xA; - Channels order down the chip: 1-64, 449-512, 385-448, 321-384, 257-320, 193-256, 129-192, 65-128&#xD;&#xA; - Muxes run from left to right in the order: 3, 4, 1, 2, 2, 1, 4, 3&#xD;&#xA;&#xD;&#xA;[![MinION Flow Cell layout][1]][1]&#xD;&#xA;&#xD;&#xA;Here's the backend of the code that I used to produce that image:&#xD;&#xA;&#xD;&#xA;    channel2poreXY &lt;- function(poreNum=rep(1:512, each=4),&#xD;&#xA;                               mux=NULL){&#xD;&#xA;        if(is.null(mux)){&#xD;&#xA;            mux &lt;- rep(1:4,length(poreNum))[1:length(poreNum)];&#xD;&#xA;        }&#xD;&#xA;        ## convert pore number to base 0 (makes arithmetic easier)&#xD;&#xA;        p0 &lt;- poreNum-1;&#xD;&#xA;        frame &lt;- floor(p0 / 32); # 0..15&#xD;&#xA;        side &lt;- (frame %% 2); # 0: left, 1: right&#xD;&#xA;        rightSide &lt;- side == 1;&#xD;&#xA;        yframe &lt;- (floor(frame / 2) + 7) %% 8; # 0..7 [bottom..top]&#xD;&#xA;        fPos &lt;- p0 %% 32;&#xD;&#xA;        framex &lt;- fPos %% 8;&#xD;&#xA;        framex[rightSide] &lt;- 7 - (fPos[rightSide] %% 8);&#xD;&#xA;        framey &lt;- (3 - floor(fPos / 8));&#xD;&#xA;        ## convert MUX to 0..3 for easier arithmetic&#xD;&#xA;        m0 &lt;- (mux+1) %% 4;&#xD;&#xA;        muxRev &lt;- (fPos %% 2) == 1;&#xD;&#xA;        m0[muxRev] &lt;- 3-m0[muxRev];&#xD;&#xA;        ## generate X,Y locations (with origin bottom left)&#xD;&#xA;        cx &lt;- (side*8 + framex) * 4 + m0;&#xD;&#xA;        cy &lt;- yframe * 4 + framey;&#xD;&#xA;        ## adjust for central gap&#xD;&#xA;        print(cx);&#xD;&#xA;        cx[cx&gt;31] &lt;- cx[cx&gt;31]+4;&#xD;&#xA;        ## adjust for hexagonal grid&#xD;&#xA;        cx &lt;- cx + (cy %% 2) * sqrt(3)/4;&#xD;&#xA;        return(cbind(cx,cy));&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;To confirm this layout, I pulled the channel/position lookup table definitions out of MinKNOW, and it seems to be the same as this (although the X locations are flipped):&#xD;&#xA;&#xD;&#xA;[![Current MinION channel layout][2]][2]&#xD;&#xA;&#xD;&#xA;Here's the code to produce this image (I haven't adjusted it to be hexagonal):&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/Rscript&#xD;&#xA;    library(rjson);&#xD;&#xA;    &#xD;&#xA;    layout.fname &lt;- &quot;/opt/ONT/MinKNOW/Client/resources/app/resources/channels.json&quot;;&#xD;&#xA;    &#xD;&#xA;    data.list &lt;- fromJSON(file=layout.fname);&#xD;&#xA;    &#xD;&#xA;    png(&quot;MinION_FC_Layout.png&quot;, width=3072, height=1536, pointsize=24);&#xD;&#xA;    par(mar=c(4,4,0.5,0.5), lwd=3);&#xD;&#xA;    data.mat &lt;- matrix(unlist(data.list), ncol=2, byrow=TRUE);&#xD;&#xA;    plot(data.mat, pch=21,&#xD;&#xA;         bg=(colorRampPalette(c(&quot;white&quot;,&quot;grey&quot;))(2048))[1:2048],&#xD;&#xA;         col=c(&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;,&quot;blue&quot;),&#xD;&#xA;         xlab=&quot;locX&quot;, ylab=&quot;locY&quot;, cex=2,&#xD;&#xA;         xlim=c(7,76), ylim=c(0,33));&#xD;&#xA;    text(x=data.mat, labels=rep(1:512, each=4), cex=0.4);&#xD;&#xA;    legend(&quot;top&quot;,legend=c(1,2,3,4), fill=c(&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;,&quot;blue&quot;),&#xD;&#xA;           inset=0.05, title=&quot;MUX&quot;);&#xD;&#xA;    text(x=6, y=data.mat[data.mat[,1] == 8,2],&#xD;&#xA;         labels = ceiling(which(data.mat[,1] == 8) / 4));&#xD;&#xA;    text(x=77, y=data.mat[data.mat[,1] == 75,2],&#xD;&#xA;         labels = ceiling(which(data.mat[,1] == 75) / 4));&#xD;&#xA;    arrows(x0=c(8,75), x1=c(39,44), y0=33, length=0.2);&#xD;&#xA;    invisible(dev.off());&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/twBhP.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/AGcU3.jpg&#xD;&#xA;" />
  <row Id="2322" PostHistoryTypeId="2" PostId="752" RevisionGUID="49f5752d-2044-4253-8c47-e7a6e5b3a38e" CreationDate="2017-06-15T11:56:51.837" UserId="180" Text="Is there a vcf file on the GRCh38 assembly with common cancer mutations I can download somewhere? Maybe from one of the big international cancer genomics consortia?&#xD;&#xA;&#xD;&#xA;By common, I mean whichever mutations have been found recurrent in different types of cancer. " />
  <row Id="2323" PostHistoryTypeId="1" PostId="752" RevisionGUID="49f5752d-2044-4253-8c47-e7a6e5b3a38e" CreationDate="2017-06-15T11:56:51.837" UserId="180" Text="GRCh38 vcf file with common cancer mutations" />
  <row Id="2324" PostHistoryTypeId="3" PostId="752" RevisionGUID="49f5752d-2044-4253-8c47-e7a6e5b3a38e" CreationDate="2017-06-15T11:56:51.837" UserId="180" Text="&lt;vcf&gt;&lt;cancer&gt;" />
  <row Id="2325" PostHistoryTypeId="2" PostId="753" RevisionGUID="c38bb7b3-3888-48fd-b845-41604ca81a2d" CreationDate="2017-06-15T12:26:26.903" UserId="191" Text="The main purpose of git is to version code, which usually means sequential improvement of the codebase. While it is possible to use branches for multiple variants of the software, permanent branches are traditionally used for gradual integration of new features (i.e. dev/testing/master branches). Supporting multiple independent branches requires some investment, i.e. distributing common changes among branches via merge or cherry-pick. This is hard to manage when you have more than two-three branches.&#xD;&#xA;&#xD;&#xA;If you compare different methods of analysis, you probably want to compare the results between methods. Having them on separate branches makes it hard.&#xD;&#xA;&#xD;&#xA;In my opinion you should integrate all methods of analysis into the master branch. To avoid copy &amp; paste, it is better to put common code in a library or an independent script. You can also specify a method as a run-time parameter of your pipeline, and create a meta-script which will execute all methods of interests.&#xD;&#xA;&#xD;&#xA;Once you performed benchmarking, you shouldn't remove unused methods from you master branch. Having them is important for reproducible research, and your scripts could be used in the future for new datasets." />
  <row Id="2326" PostHistoryTypeId="2" PostId="754" RevisionGUID="a1f8bbdd-c1fe-4701-8eef-c26170ac9724" CreationDate="2017-06-15T12:47:47.027" UserId="29" Text="&gt; Save the different scripts with git (seems overkill)&#xD;&#xA;&#xD;&#xA;**What????!**&#xD;&#xA;&#xD;&#xA;No. Version controlling your scripts (using Git or something similar) is the absolute minimum, and should become completely automatic. For every new project I begin, one of the very first steps is to issue the `git init` command, and to set up a remote repository (on Github).&#xD;&#xA;&#xD;&#xA;To keep track of *different* analyses, I use a combination of the following approaches:&#xD;&#xA;&#xD;&#xA;1. Write reusable functions/scripts and parametrise. The parameters are kept either inside the script itself (that then calls the relevant function repeatedly), or in a Makefile (I recommend [Snakemake](https://snakemake.readthedocs.io/en/stable/)).&#xD;&#xA;2. Document the alternative analysis approaches; once again, this could be a Makefile with different rules for alternative analyses, or a set of notebooks (via [R Markdown](http://rmarkdown.rstudio.com/)).&#xD;&#xA;3. Have different Git branches for mutually exclusive approaches. At the end of the analysis one of these branches gets merged into `master`, and published. If I want to publish several analysis approaches, I merge all these branches into `master`, and use approaches (1) or (2) enable them simultaneously.&#xD;&#xA;&#xD;&#xA;In fact, I recommend creating a Makefile for *every* analysis; I have found that this is the most practical, self-documenting way to run an analysis. It most closely resembles a wet-lab lab notebook. The advantage over a single R Markdown document is that rerunning just parts of the analysis can be completely automated, and dependencies in the workflow are apparent from the dependencies of the Makefile rules. This is much harder in R Markdown.&#xD;&#xA;&#xD;&#xA;Some time ago I create an [example analysis workflow](https://github.com/klmr/example-r-analysis) to show how this can be structured. Nowadays I would use Snakemake instead of GNU make." />
  <row Id="2327" PostHistoryTypeId="2" PostId="755" RevisionGUID="a8c3e9d1-65ba-4dbc-bbfd-a2ec9a3f0a0c" CreationDate="2017-06-15T12:55:47.280" UserId="298" Text="Your question doesn't give enough information for a specific answer but this should do for a start.&#xD;&#xA;&#xD;&#xA;1. Get the VCF file describing all variants in Clinvar from NCBI:&#xD;&#xA;&#xD;&#xA;        wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz&#xD;&#xA;&#xD;&#xA;2. Extract any variants whose VCF line contains the word &quot;cancer&quot;:&#xD;&#xA;&#xD;&#xA;        zgrep -iE '^#|CLNDBN=[^;]*cancer' clinvar.vcf.gz &gt; cancer.vcf&#xD;&#xA;&#xD;&#xA;Now, you should probably filter that further based on variant frequency, type of cancer, etc etc but that should be a good start. " />
  <row Id="2328" PostHistoryTypeId="2" PostId="756" RevisionGUID="56300087-bc69-4b37-be90-46a84533f1de" CreationDate="2017-06-15T13:09:16.177" UserId="872" Text="I want to find all experiments in GEO associated with a drug (for example tolvaptan), is there any quick way to to this?&#xD;&#xA;&#xD;&#xA;I tried to use bioDBnet to map Drug Bank ID to data sets, but I don't know what output format should I select." />
  <row Id="2329" PostHistoryTypeId="1" PostId="756" RevisionGUID="56300087-bc69-4b37-be90-46a84533f1de" CreationDate="2017-06-15T13:09:16.177" UserId="872" Text="How to found GEO data sets using Drug Bank ID and bioDBnet" />
  <row Id="2330" PostHistoryTypeId="3" PostId="756" RevisionGUID="56300087-bc69-4b37-be90-46a84533f1de" CreationDate="2017-06-15T13:09:16.177" UserId="872" Text="&lt;database&gt;&lt;gene&gt;&lt;drugs&gt;" />
  <row Id="2331" PostHistoryTypeId="2" PostId="757" RevisionGUID="55eae10a-4fc2-4538-8de2-c7966a3d821f" CreationDate="2017-06-15T13:22:38.700" UserId="73" Text="The trouble with looking for cancer-associated variants is that it can be difficult to tease out spurious effects (e.g. ethnicity) from causative variants. If you're interested in what *genes* are implicated in most types of cancer, the annotations for the NanoString panels are quite good:&#xD;&#xA;&#xD;&#xA; - [cancer pathways](https://www.nanostring.com/products/gene-expression-panels/hallmarks-cancer-gene-expression-panel-collection/pancancer-pathways-panel)&#xD;&#xA; - [cancer progression](https://www.nanostring.com/products/gene-expression-panels/hallmarks-cancer-gene-expression-panel-collection/pancancer-progression-panel)&#xD;&#xA;&#xD;&#xA;The &quot;Support documents&quot; section of those pages has a link to an Excel file with gene lists and probe sequences. While these lists don't give information about variants that disrupt function, I'm struggling a bit to think through why it would be useful in advance to know that, unless the aim is to induce cancer through CRISPR or something like that." />
  <row Id="2332" PostHistoryTypeId="5" PostId="756" RevisionGUID="02b6df38-f5c7-4e4e-8006-931a70d5a551" CreationDate="2017-06-15T13:28:59.873" UserId="73" Comment="correct spelling / grammar" Text="I want to find all experiments in GEO that are associated with a drug (for example tolvaptan). Is there any quick way to to this?&#xD;&#xA;&#xD;&#xA;I tried to use bioDBnet to map Drug Bank ID to look up data sets, but I don't know what output format should I select." />
  <row Id="2333" PostHistoryTypeId="4" PostId="756" RevisionGUID="02b6df38-f5c7-4e4e-8006-931a70d5a551" CreationDate="2017-06-15T13:28:59.873" UserId="73" Comment="correct spelling / grammar" Text="How to find GEO data sets using Drug Bank ID and bioDBnet" />
  <row Id="2334" PostHistoryTypeId="2" PostId="758" RevisionGUID="fc89f457-caf8-4858-9a31-f4bfce60a53c" CreationDate="2017-06-15T13:45:38.370" UserId="47" Text="I have used STRING pretty heavily, and have compared it to various other databases of protein interactions and signaling pathways.  I do feel like it has a lot of quality interaction annotations, but you have to sift through a lot of noise to get to them.  The simplest method I have found for doing this is to look at the individual scores for each interaction, and accept it if it passes one of the following tests:&#xD;&#xA;&#xD;&#xA;- Experiment Score &gt; 0.4&#xD;&#xA;- Database Score &gt; 0.9&#xD;&#xA;&#xD;&#xA;Anything that passes one of these thresholds we consider at least an interaction of acceptable quality.  Those interactions with Experiment Scores &gt; 0.9 are high-quality, and have been experimentally validated." />
  <row Id="2339" PostHistoryTypeId="5" PostId="185" RevisionGUID="0461c9af-6578-4251-b927-ba5d969a15ea" CreationDate="2017-06-15T13:53:14.837" UserId="96" Comment="added 115 characters in body" Text="Browser Extensible Data, a collection of related plain text formats for describing genome features for visualization with genome browsers." />
  <row Id="2340" PostHistoryTypeId="24" PostId="185" RevisionGUID="0461c9af-6578-4251-b927-ba5d969a15ea" CreationDate="2017-06-15T13:53:14.837" Comment="Proposed by 96 approved by 77, 55 edit id of 176" />
  <row Id="2341" PostHistoryTypeId="5" PostId="431" RevisionGUID="1035e65c-d279-4fce-8b9f-a5d2afb6083a" CreationDate="2017-06-15T13:53:23.610" UserId="73" Comment="distinction between minion and nanopore tag" Text="```minion``` -- The Oxford Nanopore MinION&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;A small, portable sequencer that uses electrical current flowing through small molecules (e.g. DNA nucleotides) to determine the underlying sequence.&#xD;&#xA;&#xD;&#xA;What questions should have this tag?&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;Anything that is associated with the generation or analysis of data from the MinION, e.g. long-read FASTQ files, raw signal data, nanopore sample prep QC. If questions are not specific to the *MinION* (e.g. also applicable to PromethION), then the `nanopore` tag should be used.&#xD;&#xA;&#xD;&#xA;Brief Introduction&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;The MinION's flow cell is comprised of 2048 wells containing a membrane perforated by nanopores. Ligated with a molecular motor, a single stranded DNA molecule passes through the pore, altering the recorded current. After the electronic sequencing is carried out, a software basecalling algorithm transforms the current trace into a modelled DNA sequence.&#xD;&#xA;&#xD;&#xA;The advantages of the MinION are rapid library preparation, portability ([Walter *et al.*, 2016][1]; [Castro-Wallace *et al.*, 2016][2]), long molecule sequencing ([Urban *et al.*, 2015][3]), and sequencing of non-model modifications of the DNA strand ([Simpson *et al.*, 2017][4]). With the recent improvement in the chemistry of the MinION, Oxford Nanopore has overcome the majority of issues associated with low yield and high error rates that have limited the range of its application. The MinION sequencing device has now been successfully applied to sequence genomes of a wide range of sizes, from bacterial and viral genomes ([Deschamps *et al.*, 2016][5]; [Quick *et al.*, 2017][6]), amplicon sequencing like bacterial 16S rRNA sequencing ([Benitez-paez *et al.*, 2016][7]), and more recently a human genome ([Jain *et al.*, 2017][8]). The MinION has also been used for cDNA sequencing ([Hargreaves *et al.*, 2015][9]), for detecting DNA methylation patterns without chemical treatment ([Simpson *et al.*, 2017][4]; [Rand *et al.*, 2017][10]), and for direct RNA sequencing with detection of modified 16S rRNA nucleotides ([Smith *et al.*, 2017][11]).&#xD;&#xA;&#xD;&#xA;[CC-by-4.0 text source by White *et al.*, 2017 [here][12]]&#xD;&#xA;&#xD;&#xA;Important links for learning more&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA; - [Summary of nanopore DNA sequencing][13] (official ONT website)&#xD;&#xA; - [F1000 Research Nanopore Analysis gateway][14]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://dx.doi.org/10.1016/j.jbiotec.2016.12.006&#xD;&#xA;  [2]: http://biorxiv.org/content/early/2016/09/27/077651&#xD;&#xA;  [3]: http://dx.doi.org/10.1101/019281&#xD;&#xA;  [4]: http://dx.doi.org/10.1038/nmeth.4184&#xD;&#xA;  [5]: http://dx.doi.org/10.1038/srep28625&#xD;&#xA;  [6]: http://dx.doi.org/10.1101/098913&#xD;&#xA;  [7]: http://dx.doi.org/10.1186/s13742-016-0111-z&#xD;&#xA;  [8]: http://dx.doi.org/10.1101/128835&#xD;&#xA;  [9]: http://dx.doi.org/10.7717/peerj.1441&#xD;&#xA;  [10]: http://dx.doi.org/10.1038/nmeth.4189&#xD;&#xA;  [11]: http://dx.doi.org/10.1101/132274&#xD;&#xA;  [12]: https://f1000research.com/articles/6-631/v1&#xD;&#xA;  [13]: https://nanoporetech.com/how-it-works&#xD;&#xA;  [14]: https://f1000research.com/gateways/nanoporeanalysis" />
  <row Id="2342" PostHistoryTypeId="24" PostId="431" RevisionGUID="1035e65c-d279-4fce-8b9f-a5d2afb6083a" CreationDate="2017-06-15T13:53:23.610" Comment="Proposed by 73 approved by 77, 55 edit id of 162" />
  <row Id="2343" PostHistoryTypeId="5" PostId="754" RevisionGUID="70b0004e-6e48-4069-bff6-e1b36542a64a" CreationDate="2017-06-15T13:53:31.883" UserId="131" Comment="&quot;What????!&quot; sounds a bit loud, hope you don't mind." Text="&gt; Save the different scripts with git (seems overkill)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**No.** Version controlling your scripts (using Git or something similar) is the absolute minimum, and should become completely automatic. For every new project I begin, one of the very first steps is to issue the `git init` command, and to set up a remote repository (on Github).&#xD;&#xA;&#xD;&#xA;To keep track of *different* analyses, I use a combination of the following approaches:&#xD;&#xA;&#xD;&#xA;1. Write reusable functions/scripts and parametrise. The parameters are kept either inside the script itself (that then calls the relevant function repeatedly), or in a Makefile (I recommend [Snakemake](https://snakemake.readthedocs.io/en/stable/)).&#xD;&#xA;2. Document the alternative analysis approaches; once again, this could be a Makefile with different rules for alternative analyses, or a set of notebooks (via [R Markdown](http://rmarkdown.rstudio.com/)).&#xD;&#xA;3. Have different Git branches for mutually exclusive approaches. At the end of the analysis one of these branches gets merged into `master`, and published. If I want to publish several analysis approaches, I merge all these branches into `master`, and use approaches (1) or (2) enable them simultaneously.&#xD;&#xA;&#xD;&#xA;In fact, I recommend creating a Makefile for *every* analysis; I have found that this is the most practical, self-documenting way to run an analysis. It most closely resembles a wet-lab lab notebook. The advantage over a single R Markdown document is that rerunning just parts of the analysis can be completely automated, and dependencies in the workflow are apparent from the dependencies of the Makefile rules. This is much harder in R Markdown.&#xD;&#xA;&#xD;&#xA;Some time ago I create an [example analysis workflow](https://github.com/klmr/example-r-analysis) to show how this can be structured. Nowadays I would use Snakemake instead of GNU make." />
  <row Id="2344" PostHistoryTypeId="24" PostId="754" RevisionGUID="70b0004e-6e48-4069-bff6-e1b36542a64a" CreationDate="2017-06-15T13:53:31.883" Comment="Proposed by 131 approved by 73, 55 edit id of 202" />
  <row Id="2345" PostHistoryTypeId="5" PostId="617" RevisionGUID="484c1571-ce32-467a-adea-3da3749168be" CreationDate="2017-06-15T13:53:49.963" UserId="96" Comment="added 109 characters in body" Text="Gene Transfer Format, a derivative of GFF, a tab-delimited plain-text format for describing gene annotations." />
  <row Id="2346" PostHistoryTypeId="24" PostId="617" RevisionGUID="484c1571-ce32-467a-adea-3da3749168be" CreationDate="2017-06-15T13:53:49.963" Comment="Proposed by 96 approved by 77, 55 edit id of 178" />
  <row Id="2347" PostHistoryTypeId="5" PostId="615" RevisionGUID="b48aa3fb-def3-4ba4-b11f-46a267680729" CreationDate="2017-06-15T13:53:54.950" UserId="96" Comment="spec url" Text="Generic Feature Format version 3, a common tab-delimited plain-text format for annotating genes and other genomic features." />
  <row Id="2348" PostHistoryTypeId="24" PostId="615" RevisionGUID="b48aa3fb-def3-4ba4-b11f-46a267680729" CreationDate="2017-06-15T13:53:54.950" Comment="Proposed by 96 approved by 77, 55 edit id of 173" />
  <row Id="2349" PostHistoryTypeId="5" PostId="432" RevisionGUID="9fde341a-357c-4bc8-82c5-4bf50878abec" CreationDate="2017-06-15T13:54:11.900" UserId="73" Comment="distinction between minion and nanopore tag" Text="Questions associated with the generation or analysis of data from sequencer MinION. For questions about long reads (e.g. including PacBio), use `long-reads` instead. For questions that are more generally about nanopore devices (e.g. including PromethION), use `nanopore` instead." />
  <row Id="2350" PostHistoryTypeId="24" PostId="432" RevisionGUID="9fde341a-357c-4bc8-82c5-4bf50878abec" CreationDate="2017-06-15T13:54:11.900" Comment="Proposed by 73 approved by 77, 55 edit id of 163" />
  <row Id="2351" PostHistoryTypeId="5" PostId="758" RevisionGUID="f17c4fe2-da04-4f86-bd94-1d18905bb410" CreationDate="2017-06-15T13:55:11.447" UserId="47" Comment="added 257 characters in body" Text="I have used STRING pretty heavily, and have compared it to various other databases of protein interactions and signaling pathways.  I do feel like it has a lot of quality interaction annotations, but you have to sift through a lot of noise to get to them.  The simplest method I have found for doing this is to look at the individual scores for each interaction, and accept it if it passes one of the following tests:&#xD;&#xA;&#xD;&#xA;- Experiment Score &gt; 0.4&#xD;&#xA;- Database Score &gt; 0.9&#xD;&#xA;&#xD;&#xA;Anything that passes one of these thresholds we consider at least an interaction of acceptable quality.  Those interactions with Experiment Scores &gt; 0.9 are high-quality, and have been experimentally validated.  The other scores represent inaccurate methods for determining signaling events, and should be ignored.&#xD;&#xA;&#xD;&#xA;You are not going to catch every actual protein signaling event this way, but you will at least be spared a lot of false positives.  The best way to construct an actual network of signaling events is to combine interaction records from multiple databases." />
  <row Id="2352" PostHistoryTypeId="5" PostId="748" RevisionGUID="bf28182c-6eac-4763-a4f9-d1df06d8734b" CreationDate="2017-06-15T14:40:09.540" UserId="191" Comment="add pronunciation " Text="## Update&#xD;&#xA;[As pointed out by neilfws][2]:&#xD;&#xA;&#xD;&#xA;According to [Wikipedia][3] and Practical bioinformatics by Michael Agostino: The name &quot;Entrez&quot; means &quot;Come in!&quot; in French (pronounced “on-tray”); it was chosen to reflect the spirit of welcoming the public to search the content.&#xD;&#xA;&#xD;&#xA;## Old (probably incorrect) theory&#xD;&#xA;The first version of Entrez database was [distributed][1] by NCBI in 1991 (on CD-ROM). At that time, it consisted of nucleotide sequences, protein sequences and associated citations and abstracts from MEDLINE. &#xD;&#xA;&#xD;&#xA;So even at the early stage it contained objects (entries) of very different types (sequences and abstracts). It is hard to find a word which will accurately and briefly describe all the types of data.&#xD;&#xA;&#xD;&#xA;While I don't know the correct answer and this is purely a speculation, my guess is that Entrez is an intentionally misspelled word &quot;entries&quot;; which accurately describes all the datatypes in the database.&#xD;&#xA;&#xD;&#xA;Using a misspelled word as a name is not uncommon, e.g. Google comes from misspelled &quot;googol&quot; and Flickr is a misspelled version of &quot;flicker&quot;.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/books/NBK3837/&#xD;&#xA;  [2]: https://bioinformatics.stackexchange.com/questions/747/what-is-the-etymology-of-entrez-id/748#comment1340_747&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/Entrez" />
  <row Id="2353" PostHistoryTypeId="2" PostId="759" RevisionGUID="b3df0007-dcd2-4dfa-9f2b-2a1cf494784c" CreationDate="2017-06-15T15:01:31.443" UserId="292" Text="I quite agree with [this answer](https://bioinformatics.stackexchange.com/a/754/292) by @Konrad Rudolph.&#xD;&#xA;&#xD;&#xA;I would like to emphasis that using parametrization for your scripts is what will help you avoid multiplying branches in git. So yes, use git, but you don't necessarily have to go &quot;overkill&quot; creating lots of branches.&#xD;&#xA;&#xD;&#xA;Then, I would command these scripts from a workflow managment tool that will somehow take care of generating the branches of your analyses. If you use Snakemake, the various options taken along the path from your data to your results will be represented by the wildcards system, and this will be visible in the structure of your folders and file names, due to the fact that Snakemake works by inferring what should be done to produce a file based on its name.&#xD;&#xA;&#xD;&#xA;This of course is not an excuse for not using other documentation approaches: Comments in the Snakefile and in the scripts, README files explaining how the workflow was run, using what configuration file. Put your scripts, the Snakefile, its configuration files and the README files under version control and document again using commit messages." />
  <row Id="2354" PostHistoryTypeId="6" PostId="751" RevisionGUID="8f421c7b-b732-437f-98c6-17b8c4ca343c" CreationDate="2017-06-15T15:06:38.017" UserId="292" Comment="Added tags" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;git&gt;&lt;workflow-management&gt;&lt;best-practice&gt;" />
  <row Id="2355" PostHistoryTypeId="24" PostId="751" RevisionGUID="8f421c7b-b732-437f-98c6-17b8c4ca343c" CreationDate="2017-06-15T15:06:38.017" Comment="Proposed by 292 approved by 208 edit id of 203" />
  <row Id="2356" PostHistoryTypeId="8" PostId="754" RevisionGUID="bd141f28-8058-43d2-ad50-09d27ebb1bd7" CreationDate="2017-06-15T15:41:47.873" UserId="29" Comment="Rollback to [a1f8bbdd-c1fe-4701-8eef-c26170ac9724] - Edit approval overridden by post owner or moderator" Text="&gt; Save the different scripts with git (seems overkill)&#xD;&#xA;&#xD;&#xA;**What????!**&#xD;&#xA;&#xD;&#xA;No. Version controlling your scripts (using Git or something similar) is the absolute minimum, and should become completely automatic. For every new project I begin, one of the very first steps is to issue the `git init` command, and to set up a remote repository (on Github).&#xD;&#xA;&#xD;&#xA;To keep track of *different* analyses, I use a combination of the following approaches:&#xD;&#xA;&#xD;&#xA;1. Write reusable functions/scripts and parametrise. The parameters are kept either inside the script itself (that then calls the relevant function repeatedly), or in a Makefile (I recommend [Snakemake](https://snakemake.readthedocs.io/en/stable/)).&#xD;&#xA;2. Document the alternative analysis approaches; once again, this could be a Makefile with different rules for alternative analyses, or a set of notebooks (via [R Markdown](http://rmarkdown.rstudio.com/)).&#xD;&#xA;3. Have different Git branches for mutually exclusive approaches. At the end of the analysis one of these branches gets merged into `master`, and published. If I want to publish several analysis approaches, I merge all these branches into `master`, and use approaches (1) or (2) enable them simultaneously.&#xD;&#xA;&#xD;&#xA;In fact, I recommend creating a Makefile for *every* analysis; I have found that this is the most practical, self-documenting way to run an analysis. It most closely resembles a wet-lab lab notebook. The advantage over a single R Markdown document is that rerunning just parts of the analysis can be completely automated, and dependencies in the workflow are apparent from the dependencies of the Makefile rules. This is much harder in R Markdown.&#xD;&#xA;&#xD;&#xA;Some time ago I create an [example analysis workflow](https://github.com/klmr/example-r-analysis) to show how this can be structured. Nowadays I would use Snakemake instead of GNU make." />
  <row Id="2357" PostHistoryTypeId="2" PostId="760" RevisionGUID="1ada6884-e196-4546-86c8-99b03da49dc3" CreationDate="2017-06-15T15:47:20.327" UserId="267" Text="For FASTA files, I've implemented a relatively efficient method in [pyfaidx](https://github.com/mdshw5/pyfaidx) v0.4.9.1. This post made me realize that my previous code was quite slow and easy to replace:&#xD;&#xA;&#xD;&#xA;    $ pip install pyfaidx&#xD;&#xA;    $ time faidx -i nucleotide ~/Downloads/hg38.fa&#xD;&#xA;    name	start	end	A	T	C	G	N	others&#xD;&#xA;    chr1	1	248956422	67070277	67244164	48055043	48111528	18475410	&#xD;&#xA;    chr10	1	133797422	38875926	39027555	27639505	27719976	534460	&#xD;&#xA;    chr11	1	135086622	39286730	39361954	27903257	27981801	552880	&#xD;&#xA;    chr11_KI270721v1_random	1	100316	18375	19887	31042	31012	0	&#xD;&#xA;    ...&#xD;&#xA;    chrX	1	156040895	46754807	46916701	30523780	30697741	1147866	&#xD;&#xA;    chrY	1	57227415	7886192	7956168	5285789	5286894	30812372	&#xD;&#xA;    chrY_KI270740v1_random	1	37240	8274	12978	7232	8756	0	&#xD;&#xA;&#xD;&#xA;    real	2m28.251s&#xD;&#xA;    user	2m15.548s&#xD;&#xA;    sys	0m9.951s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2358" PostHistoryTypeId="5" PostId="754" RevisionGUID="a10a4491-d708-4aa9-8be5-c101608d6c6a" CreationDate="2017-06-15T15:47:26.007" UserId="29" Comment="soften the phrasing. Slightly." Text="&gt; Save the different scripts with git (seems overkill)&#xD;&#xA;&#xD;&#xA;Whoa. I did an actual double take when reading this:&lt;sup&gt;1&lt;/sup&gt; it’s the *opposite* of overkill.&#xD;&#xA;&#xD;&#xA;Version controlling your scripts (using Git or something similar) is the absolute minimum, and should become completely automatic. For every new project I begin, one of the very first steps is to issue the `git init` command, and to set up a remote repository (on Github).&#xD;&#xA;&#xD;&#xA;To keep track of *different* analyses, I use a combination of the following approaches:&#xD;&#xA;&#xD;&#xA;1. Write reusable functions/scripts and parametrise. The parameters are kept either inside the script itself (that then calls the relevant function repeatedly), or in a Makefile (I recommend [Snakemake](https://snakemake.readthedocs.io/en/stable/)).&#xD;&#xA;2. Document the alternative analysis approaches; once again, this could be a Makefile with different rules for alternative analyses, or a set of notebooks (via [R Markdown](http://rmarkdown.rstudio.com/)).&#xD;&#xA;3. Have different Git branches for mutually exclusive approaches. At the end of the analysis one of these branches gets merged into `master`, and published. If I want to publish several analysis approaches, I merge all these branches into `master`, and use approaches (1) or (2) enable them simultaneously.&#xD;&#xA;&#xD;&#xA;In fact, I recommend creating a Makefile for *every* analysis; I have found that this is the most practical, self-documenting way to run an analysis. It most closely resembles a wet-lab lab notebook. The advantage over a single R Markdown document is that rerunning just parts of the analysis can be completely automated, and dependencies in the workflow are apparent from the dependencies of the Makefile rules. This is much harder in R Markdown.&#xD;&#xA;&#xD;&#xA;Some time ago I create an [example analysis workflow](https://github.com/klmr/example-r-analysis) to show how this can be structured. Nowadays I would use Snakemake instead of GNU make.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;&lt;sup&gt;1&lt;/sup&gt; To emphasise: take a look at the [edit history](https://bioinformatics.stackexchange.com/revisions/754/1) of this answer." />
  <row Id="2359" PostHistoryTypeId="5" PostId="750" RevisionGUID="461d1086-8da5-41f6-82db-efcbd690e629" CreationDate="2017-06-15T15:52:31.827" UserId="734" Comment="deleted 67 characters in body; edited title" Text="I saw [this nature news][1], it sounds that [Cellminer][2] is obsolete? &#xD;&#xA;What are the new tools to analyze the &quot;new cell lines&quot;? Where's the new repository?&#xD;&#xA;&#xD;&#xA;What about the European initiative that the article refers to? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.nature.com/news/us-cancer-institute-to-overhaul-tumour-cell-lines-1.19364&#xD;&#xA;  [2]: https://discover.nci.nih.gov/cellminer/" />
  <row Id="2360" PostHistoryTypeId="4" PostId="750" RevisionGUID="461d1086-8da5-41f6-82db-efcbd690e629" CreationDate="2017-06-15T15:52:31.827" UserId="734" Comment="deleted 67 characters in body; edited title" Text="How to access the patient-derived xenografts (PDXs), &quot;the new NCI-60&quot; tumor cell lines?" />
  <row Id="2361" PostHistoryTypeId="5" PostId="754" RevisionGUID="13e8c4ad-50cc-40a9-ab6c-1f533cf2640c" CreationDate="2017-06-15T15:57:32.463" UserId="29" Comment="soften the phrasing. Slightly." Text="&gt; Save the different scripts with git (seems overkill)&#xD;&#xA;&#xD;&#xA;Whoa. I did an actual double take when reading this:&lt;sup&gt;1&lt;/sup&gt; it’s the *opposite* of overkill.&#xD;&#xA;&#xD;&#xA;Version controlling your scripts (using Git or something similar) is the absolute minimum, and should become completely automatic. For every new project I begin, one of the very first steps is to issue the `git init` command, and to set up a remote repository (on Github).&#xD;&#xA;&#xD;&#xA;To keep track of *different* analyses, I use a combination of the following approaches:&#xD;&#xA;&#xD;&#xA;1. Write reusable functions/scripts and parametrise. The parameters are kept either inside the script itself (that then calls the relevant function repeatedly), or in a Makefile (I recommend [Snakemake](https://snakemake.readthedocs.io/en/stable/)).&#xD;&#xA;2. Document the alternative analysis approaches; once again, this could be a Makefile with different rules for alternative analyses, or a set of notebooks (via [R Markdown](http://rmarkdown.rstudio.com/)).&#xD;&#xA;3. Have different Git branches for mutually exclusive approaches. At the end of the analysis one of these branches gets merged into `master`, and published. If I want to publish several analysis approaches, I merge all these branches into `master`, and use approaches (1) or (2) enable them simultaneously.&#xD;&#xA;&#xD;&#xA;In fact, I recommend creating a Makefile for *every* analysis; I have found that this is the most practical, self-documenting way to run an analysis. It most closely resembles a wet-lab lab notebook. The advantage over a single R Markdown document is that rerunning just parts of the analysis can be completely automated, and dependencies in the workflow are apparent from the dependencies of the Makefile rules. This is much harder in R Markdown.&#xD;&#xA;&#xD;&#xA;Some time ago I create an [example analysis workflow](https://github.com/klmr/example-r-analysis) to show how this can be structured. Nowadays I would use Snakemake instead of GNU make.&#xD;&#xA;&#xD;&#xA;Regarding your other point:&#xD;&#xA;&#xD;&#xA;&gt; Make notes in the script itself&#xD;&#xA;&#xD;&#xA;“Notes” are a dangerous beasts: documentation is important, but experience shows that it’s sometimes very hard to keep documentation synchronised with the code. There is no mechanism that ensures that documentation and code actually agree. Differences between presumed analysis and actually executed analysis can become very problematic.&#xD;&#xA;&#xD;&#xA;People therefore prefer using self-documenting code as much as possible; that is: writing code so that its meaning becomes immediately clear, without comments, even to somebody who hasn’t worked on the code before. Doing this well is hard and takes practice, but improves overall code quality. Once again, using a Makefile helps here because the dependencies between the rules are self-documenting the kind of analysis that was performed.&#xD;&#xA;&#xD;&#xA;Jeff Atwood has written two seminal essays on this subject:&#xD;&#xA;&#xD;&#xA;* [Coding without comments](https://blog.codinghorror.com/coding-without-comments/), and&#xD;&#xA;* [Code tells you how, comments tell you why](https://blog.codinghorror.com/code-tells-you-how-comments-tell-you-why/).&#xD;&#xA;&#xD;&#xA;They are two of the best pieces of advice on programming that I could give.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;&lt;sup&gt;1&lt;/sup&gt; To emphasise: take a look at the [edit history](https://bioinformatics.stackexchange.com/revisions/754/1) of this answer." />
  <row Id="2362" PostHistoryTypeId="5" PostId="750" RevisionGUID="6971cd2b-67c5-4a98-a7f6-0125e301f054" CreationDate="2017-06-15T16:03:51.060" UserId="734" Comment="added 108 characters in body" Text="I saw [this nature news][1], it sounds that [Cellminer][2] is obsolete? &#xD;&#xA;What are the new tools to analyze the &quot;[new cell lines][3]&quot;? Where's the new repository?&#xD;&#xA;&#xD;&#xA;What about the European initiative that the article refers to? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xA;&#xA;  [1]: http://www.nature.com/news/us-cancer-institute-to-overhaul-tumour-cell-lines-1.19364&#xA;  [2]: https://discover.nci.nih.gov/cellminer/&#xA;  [3]: https://www.jax.org/jax-mice-and-services/in-vivo-pharmacology/oncology-services/pdx-tumors#" />
  <row Id="2363" PostHistoryTypeId="5" PostId="756" RevisionGUID="3d195be2-acfd-4612-9ddf-20117da3ce09" CreationDate="2017-06-15T17:48:38.077" UserId="57" Comment="added requirement for salability (100 &gt; drugs)" Text="I want to find all experiments in GEO that are associated with a drug (for example tolvaptan). Is there any quick and scalable way to to this? I want to query more than 100 drugs.&#xD;&#xA;&#xD;&#xA;I tried to use bioDBnet to map Drug Bank ID to look up data sets, but I don't know what output format should I select." />
  <row Id="2364" PostHistoryTypeId="2" PostId="761" RevisionGUID="177afc4b-3a05-44a3-ac83-6521b74e21d4" CreationDate="2017-06-15T17:51:10.583" UserId="712" Text="I am looking at the presence of viral genotypes within individual samples within an assay. Often times there is a sample whose read counts are firing off the charts and this sample tends to &quot;bleed through&quot; to the other samples. I have recently tried confronting this problem with chi square analysis of conditional probability. For instance let's say I observe with 10 sample viral genotypes A and B. I then calculate the expected number of samples having both genotypes A and B from the number of A positive samples and number of B positive samples. &#xD;&#xA;&#xD;&#xA;So far this is all I have. I do not know if anyone else has any ideas on how to determine and quantify bleed through in viral genotyping. " />
  <row Id="2365" PostHistoryTypeId="1" PostId="761" RevisionGUID="177afc4b-3a05-44a3-ac83-6521b74e21d4" CreationDate="2017-06-15T17:51:10.583" UserId="712" Text="Techniques for analyzing and quantifying sample bleed through in genotyping with Illumina" />
  <row Id="2366" PostHistoryTypeId="3" PostId="761" RevisionGUID="177afc4b-3a05-44a3-ac83-6521b74e21d4" CreationDate="2017-06-15T17:51:10.583" UserId="712" Text="&lt;genotyping&gt;" />
  <row Id="2367" PostHistoryTypeId="5" PostId="287" RevisionGUID="37930345-2e18-4d73-a392-d6354cc69cec" CreationDate="2017-06-15T18:16:11.983" UserId="57" Comment="added links as hyperlinks to text" Text="It is one of my favorite stories. &#xD;&#xA;&#xD;&#xA;Drop a glance to [StereoGene software](http://stereogene.bioinf.fbb.msu.ru/), it for genomic track correlation, it described in a [preprint](http://biorxiv.org/content/early/2017/05/25/059584).&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;You also can run MACS or another peak caller and estimate the correlation of two interval sets using the [GenomtriCorr](http://genometricorr.sourceforge.net/) package.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2368" PostHistoryTypeId="5" PostId="761" RevisionGUID="0b50ba77-0ab7-44ab-b86e-4e218de92a3d" CreationDate="2017-06-15T18:20:54.337" UserId="57" Comment="separated paragraph of explanatino of problem and description potential solution" Text="I am looking at the presence of viral genotypes within individual samples within an assay. Often times there is a sample whose read counts are firing off the charts and this sample tends to &quot;bleed through&quot; to the other samples. &#xD;&#xA;&#xD;&#xA;I have recently tried confronting this problem with chi square analysis of conditional probability. For instance let's say I observe with 10 sample viral genotypes A and B. I then calculate the expected number of samples having both genotypes A and B from the number of A positive samples and number of B positive samples. &#xD;&#xA;&#xD;&#xA;So far this is all I have. I do not know if anyone else has any ideas on how to determine and quantify bleed through in viral genotyping. " />
  <row Id="2369" PostHistoryTypeId="2" PostId="762" RevisionGUID="2f724e7b-1461-48de-9362-1def39a8bb2d" CreationDate="2017-06-15T19:10:23.020" UserId="823" Text="One of the problems I've found with GEO and even ArrayExpress is that it seems like there are tons of false positives that come up in the search. Additionally, there may be a ton of results that you miss because you didn't include the synonym of the drug. &#xD;&#xA;&#xD;&#xA;In order to make sure I have all the information relating to name, etc, you can write a script which downloads the result of a search in PubChem (this can be done with drugbank as well I think) and then parse the search result, extracting important information such as synonyms of drug name.&#xD;&#xA;&#xD;&#xA;Finally, take those synonyms and use them in an Entrez (EUtils) search for the experiments. Then try to filter the results to keep only what you're actually looking for. One bad way to do this is just by making sure the experiment summary contains the term your looking for. Another way to potentially do this, for example if there is a large experiment which tests serveral different factors is to programmatically search through associated files to ensure they contain a desired keyword. &#xD;&#xA;&#xD;&#xA;You'll get different experiment types and so the files associated with the results will depend on the type of experiment in the search result.&#xD;&#xA;&#xD;&#xA;When you get have a list of IDs of the experiments you want, you can feed them through to GEOquery in Bioconductor." />
  <row Id="2370" PostHistoryTypeId="2" PostId="763" RevisionGUID="7963a3fc-d024-4ba8-b4c4-0967f621d666" CreationDate="2017-06-15T19:21:31.780" UserId="823" Text="If you run this Rscript using the gene name as an argument, you'll get a file with the pathway written to the directory.&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/Rscript&#xD;&#xA;    args = commandArgs(trailingOnly=TRUE)&#xD;&#xA;    library(paxtoolsr)&#xD;&#xA;    id &lt;- args[1]&#xD;&#xA;    write.table(graphPc(source=id,kind='neighborhood',&#xD;&#xA;                format='BINARY_SIF',verbose=TRUE), &#xD;&#xA;                file=paste(id,'pathway',sep='_'), sep='\t',quote=FALSE)" />
  <row Id="2371" PostHistoryTypeId="2" PostId="764" RevisionGUID="09868563-787b-4f09-ac1c-e70403d9df25" CreationDate="2017-06-15T20:17:45.203" UserId="73" Text="Are you using dual-index barcodes with different barcodes at each end? There is a known phenomena of &quot;index switching&quot; that occurs in Illumina reads. One way to control for this is to add in unamplified water samples that have unused barcode combinations where each of the indexes is shared with another sample." />
  <row Id="2372" PostHistoryTypeId="5" PostId="764" RevisionGUID="de63f54a-1f5d-4f06-918d-0b723a7cb40c" CreationDate="2017-06-15T23:54:16.543" UserId="73" Comment="included additional verbosity about index switching" Text="Are you using dual-index barcodes with different barcodes at each end? There is a known phenomena of &quot;index switching&quot; that occurs in Illumina reads. One way to control for this is to add in unamplified water samples that have unused barcode combinations where each of the indexes is shared with another sample.&#xD;&#xA;&#xD;&#xA;More information can be found on [this SeqAnswers thread](http://seqanswers.com/forums/showthread.php?p=205928). Here are some useful associated links:&#xD;&#xA;&#xD;&#xA;- [Signal-spreading preprint](http://www.biorxiv.org/content/early/2017/04/09/125724) -- detailed investigation of the phenomenom, including methods, graphs, and associated data&#xD;&#xA;- [Crossblock](http://seqanswers.com/forums/showthread.php?t=73736) -- tool by Brian Bushnell that attempts to remove cross contamination&#xD;&#xA;- [Summary by James Hadfield](http://seqanswers.com/forums/showthread.php?p=205928) -- mentions a few fixes suggested by Illumina, many centred around not using barcode combinations that could conflict&#xD;&#xA;- [Illumina's quick-fire response](https://www.illumina.com/content/dam/illumina-marketing/documents/products/whitepapers/index-hopping-white-paper-770-2017-004.pdf?linkId=36607862) -- they've known about it for a long time, and point out that it shouldn't be a problem for most purposes&#xD;&#xA;&#xD;&#xA;I have suspicions that at least some of the index switching is being caused by ligation of two separate barcoded fragments during the sample prep (which definitely happens [during sample prep for nanopore](https://f1000research.com/articles/6-631/v1)), but I haven't done any experimentation to work out if this is also the case for Illumina (due to money/time/goal constraints). The experimental design would be to take a prepared Illumina library just prior to sequencing, ligate on ONT adapters, then sequence on both the ONT and Illumina machines to see if chimeric reads discovered via nanopore sufficiently matches the index switching observed via short-read sequencing." />
  <row Id="2373" PostHistoryTypeId="2" PostId="765" RevisionGUID="d75d005c-4abe-40dd-9f3e-70a375f2863a" CreationDate="2017-06-16T02:59:51.210" UserId="881" Text="At which sites on the Drosophila melanogaster CG2316 mRNA do the RNAi`s: 12170/FBti0089992, 12168/FBtp0030589, 107343/FBtp0042163 and 41984/FBst0034349 (stock numbers/flybase ID) cleave? &#xD;&#xA;&#xD;&#xA;And how did you work this out?&#xD;&#xA;&#xD;&#xA;This answer will be greatly appreciated. " />
  <row Id="2374" PostHistoryTypeId="1" PostId="765" RevisionGUID="d75d005c-4abe-40dd-9f3e-70a375f2863a" CreationDate="2017-06-16T02:59:51.210" UserId="881" Text="Can you tell me where these RNAi’s cleave on the Drosophila melanogaster CG2316 mRNA?" />
  <row Id="2375" PostHistoryTypeId="3" PostId="765" RevisionGUID="d75d005c-4abe-40dd-9f3e-70a375f2863a" CreationDate="2017-06-16T02:59:51.210" UserId="881" Text="&lt;fasta&gt;" />
  <row Id="2376" PostHistoryTypeId="5" PostId="749" RevisionGUID="68d76a98-0dac-43c5-b82a-a8c7572742c0" CreationDate="2017-06-16T05:49:50.170" UserId="73" Comment="removed historical information/code that I produced in 2014/2015" Text="There are actually 2048 usable sequencing wells, hexagonally packed with four wells connected to the same sequencing sensor/channel via a multiplex (mux) selector. The combination of the mux and the channel number determines the physical location of the well. Unfortunately, the association between channel number and physical location is not obvious, and within each channel the muxes are also not in an obvious order, i.e. `[3,4,1,2,2,1,4,3]` for two adjacent channels.&#xD;&#xA;&#xD;&#xA;Here's an &quot;official&quot; text description of the layout as it was for R7; it hasn't changed for the most recent flow cells (R9.5):&#xD;&#xA;&#xD;&#xA; - Channels 1-64 occur at the top of the chip (the other low channel numbers are at the bottom)&#xD;&#xA; - Channels order down the chip: 1-64, 449-512, 385-448, 321-384, 257-320, 193-256, 129-192, 65-128&#xD;&#xA; - Muxes run from left to right in the order: 3, 4, 1, 2, 2, 1, 4, 3&#xD;&#xA;&#xD;&#xA;To confirm this layout, I pulled the channel/position lookup table definitions out of MinKNOW, and it seems to be the same as this:&#xD;&#xA;&#xD;&#xA;[![Current MinION channel layout][2]][2]&#xD;&#xA;&#xD;&#xA;Here's the code to produce this image (I haven't adjusted it to have a hexagonal structure):&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/Rscript&#xD;&#xA;    library(rjson);&#xD;&#xA;    &#xD;&#xA;    layout.fname &lt;- &quot;/opt/ONT/MinKNOW/Client/resources/app/resources/channels.json&quot;;&#xD;&#xA;    &#xD;&#xA;    data.list &lt;- fromJSON(file=layout.fname);&#xD;&#xA;    &#xD;&#xA;    png(&quot;MinION_FC_Layout.png&quot;, width=3072, height=1536, pointsize=24);&#xD;&#xA;    par(mar=c(4,4,0.5,0.5), lwd=3);&#xD;&#xA;    data.mat &lt;- matrix(unlist(data.list), ncol=2, byrow=TRUE);&#xD;&#xA;    plot(data.mat, pch=21,&#xD;&#xA;         bg=(colorRampPalette(c(&quot;white&quot;,&quot;grey&quot;))(2048))[1:2048],&#xD;&#xA;         col=c(&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;,&quot;blue&quot;),&#xD;&#xA;         xlab=&quot;locX&quot;, ylab=&quot;locY&quot;, cex=2,&#xD;&#xA;         xlim=c(7,76), ylim=c(0,33));&#xD;&#xA;    text(x=data.mat, labels=rep(1:512, each=4), cex=0.4);&#xD;&#xA;    legend(&quot;top&quot;,legend=c(1,2,3,4), fill=c(&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;,&quot;blue&quot;),&#xD;&#xA;           inset=0.05, title=&quot;MUX&quot;);&#xD;&#xA;    text(x=6, y=data.mat[data.mat[,1] == 8,2],&#xD;&#xA;         labels = ceiling(which(data.mat[,1] == 8) / 4));&#xD;&#xA;    text(x=77, y=data.mat[data.mat[,1] == 75,2],&#xD;&#xA;         labels = ceiling(which(data.mat[,1] == 75) / 4));&#xD;&#xA;    arrows(x0=c(8,75), x1=c(39,44), y0=33, length=0.2);&#xD;&#xA;    invisible(dev.off());&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/twBhP.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/AGcU3.jpg&#xD;&#xA;" />
  <row Id="2378" PostHistoryTypeId="5" PostId="765" RevisionGUID="34f6ee28-7f46-4a39-a6f5-09f56f03399e" CreationDate="2017-06-16T08:47:12.393" UserId="77" Comment="added 782 characters in body" Text="At which sites on the Drosophila melanogaster CG2316 mRNA do the RNAi`s: 12170/FBti0089992, 12168/FBtp0030589, 107343/FBtp0042163 and 41984/FBst0034349 (stock numbers/flybase ID) cleave? &#xD;&#xA;&#xD;&#xA;And how did you work this out?&#xD;&#xA;&#xD;&#xA;This answer will be greatly appreciated.&#xD;&#xA;&#xD;&#xA;**Update**: &#xD;&#xA;&#xD;&#xA;This is not a homework question but simply a matter of interest. I am considering designing an experiment that requires capabilities I am yet to master. I have a decent understanding of cell, genetic and neurobiology but can't deny my weaknesses in bioinformatics, computer programming and statistics. I could not think of a more efficient way to learn it though than by watching and copying more proficient people.&#xD;&#xA;&#xD;&#xA;I attempted the question some time ago but with limited success. I found the Fasta sequence of CG2316 and searched for the two complementary sites of the RNAi hairpin using the word doc find function. I feel this rudimentary method was flawed on a few levels, not least because I only got a single hit. As such I have posted the problem here.&#xD;&#xA;" />
  <row Id="2379" PostHistoryTypeId="5" PostId="750" RevisionGUID="8e4a3c76-94a6-41c9-b032-8b1e8e544c66" CreationDate="2017-06-16T10:07:49.977" UserId="734" Comment="added 13 characters in body; edited title" Text="I saw [this nature news][1], it sounds that [Cellminer][2] is obsolete, is it right?&#xA;&#xD;&#xA;What are the new tools to analyze the &quot;[new cell lines][3]&quot;? Where's the PDX repository?&#xD;&#xA;&#xD;&#xA;What about the European initiative that the article refers to? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xA;&#xA;  [1]: http://www.nature.com/news/us-cancer-institute-to-overhaul-tumour-cell-lines-1.19364&#xA;  [2]: https://discover.nci.nih.gov/cellminer/&#xA;  [3]: https://www.jax.org/jax-mice-and-services/in-vivo-pharmacology/oncology-services/pdx-tumors#" />
  <row Id="2380" PostHistoryTypeId="4" PostId="750" RevisionGUID="8e4a3c76-94a6-41c9-b032-8b1e8e544c66" CreationDate="2017-06-16T10:07:49.977" UserId="734" Comment="added 13 characters in body; edited title" Text="How to access the patient-derived xenografts (PDXs) repository?" />
  <row Id="2381" PostHistoryTypeId="2" PostId="767" RevisionGUID="6cd7130e-4962-4fdc-952d-06e300c27b1c" CreationDate="2017-06-16T10:12:05.340" UserId="451" Text="I already see for links when I try to search. Take a look at them below. Also if you can be precise about kind of data are you trying to find, it would be better&#xD;&#xA;&#xD;&#xA;https://pdmr.cancer.gov/&#xD;&#xA;&#xD;&#xA;http://cdt.northwestern.edu/news/patient-derived-xenograft-repository-now-available-researchers&#xD;&#xA;&#xD;&#xA;http://www.epo-berlin.com/epo-tumor-models-xenografts.html&#xD;&#xA;&#xD;&#xA;http://www.proxe.org/&#xD;&#xA;&#xD;&#xA;Hope one of these 4 will serve the purpose for you." />
  <row Id="2382" PostHistoryTypeId="5" PostId="750" RevisionGUID="395217c3-7300-410a-b1fc-7d694089adcf" CreationDate="2017-06-16T10:14:25.340" UserId="734" Comment="added 115 characters in body" Text="I saw [this nature news][1], it sounds that [Cellminer][2] is obsolete, is it right?&#xA;&#xD;&#xA;What are the new tools to analyze the &quot;[new cell lines][3]&quot;? Where's the PDX repository?&#xD;&#xA;&#xD;&#xA;What about the European initiative that the article refers to? &#xA;&#xA;I would be happy to see correlation between drugs and genes and similar capabilities as we can see on Cellminer. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xA;&#xA;  [1]: http://www.nature.com/news/us-cancer-institute-to-overhaul-tumour-cell-lines-1.19364&#xA;  [2]: https://discover.nci.nih.gov/cellminer/&#xA;  [3]: https://www.jax.org/jax-mice-and-services/in-vivo-pharmacology/oncology-services/pdx-tumors#" />
  <row Id="2383" PostHistoryTypeId="2" PostId="768" RevisionGUID="147169d6-c710-4e67-9b63-d4fec10eaf74" CreationDate="2017-06-16T10:50:45.063" UserId="599" Text="Why not just use string-db's [online tool][1]? There you can adjust all parameters of interest such as interaction confidence as well as pull in extra protein interactions.&#xD;&#xA;&#xD;&#xA;If you make a user you can also upload a custom background gene-set set which means the analysis of go-terms and interconnectedness that sting provides is usable.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://string-db.org" />
  <row Id="2384" PostHistoryTypeId="2" PostId="769" RevisionGUID="e4f9c71e-ea59-43df-b54c-7a9dc2e52647" CreationDate="2017-06-16T10:53:49.660" UserId="61" Text="I'm looking to subset a VCF file which only includes insertions (i.e. not indels). &#xD;&#xA;&#xD;&#xA;I can get part of the way there with:&#xD;&#xA;&#xD;&#xA;    bcftools view -v indels &lt;vcf&gt; | awk '{if(length($4) == 1) print}'&#xD;&#xA;&#xD;&#xA;However this wouldn't catch an insertion that was part of a multi-allelic record with an insertion and deletion, where the reference length would be greater than 1 bp. Potentially then one way to go is a chain of decomposition, normalisation and then the same reference length filtering — surely there's a better way in one of the many VCF manipulation utils?" />
  <row Id="2385" PostHistoryTypeId="1" PostId="769" RevisionGUID="e4f9c71e-ea59-43df-b54c-7a9dc2e52647" CreationDate="2017-06-16T10:53:49.660" UserId="61" Text="How can I extract only insertions from a VCF file?" />
  <row Id="2386" PostHistoryTypeId="3" PostId="769" RevisionGUID="e4f9c71e-ea59-43df-b54c-7a9dc2e52647" CreationDate="2017-06-16T10:53:49.660" UserId="61" Text="&lt;vcf&gt;" />
  <row Id="2387" PostHistoryTypeId="2" PostId="770" RevisionGUID="1cfbf270-c54b-41f8-aaae-5c9ab15f5664" CreationDate="2017-06-16T11:01:15.400" UserId="599" Text="For the effective length part please see to Devons answer. I just have a small addition: Kallisto/Salmon/RSEM incorporate all bias estimates into the effective length meaning the effective length not only represent the length bias if you take the values from those tools (given that they were run with the bias algorithms enabled naturally).&#xD;&#xA;&#xD;&#xA;With regards to getting gene level estimates you should not choose a specific transcript. Instead you should extract/calculate the RPKM/FPKM/TxPM (transcript per million that Kallisto/Salmon/RSEM outputs) for each transcript and sum them up to get the gene level estimate." />
  <row Id="2388" PostHistoryTypeId="2" PostId="771" RevisionGUID="2a132042-560a-4ef5-a269-9a14a0ed0c89" CreationDate="2017-06-16T11:22:34.610" UserId="208" Text="Provide an overview of 10x data analysis packages.&#xD;&#xA;&#xD;&#xA;10x provides Cell Ranger which prepares a count matrix from the bcl sequencer output files and other files (see bottom of page [https://support.10xgenomics.com/docs/license][1] for the programs it uses).&#xD;&#xA;&#xD;&#xA;What can we do with the output files?&#xD;&#xA;&#xD;&#xA;  [1]: https://support.10xgenomics.com/docs/license" />
  <row Id="2389" PostHistoryTypeId="1" PostId="771" RevisionGUID="2a132042-560a-4ef5-a269-9a14a0ed0c89" CreationDate="2017-06-16T11:22:34.610" UserId="208" Text="10x Genomics Chromium single-cell RNA-seq data analysis options?" />
  <row Id="2390" PostHistoryTypeId="3" PostId="771" RevisionGUID="2a132042-560a-4ef5-a269-9a14a0ed0c89" CreationDate="2017-06-16T11:22:34.610" UserId="208" Text="&lt;rna-seq&gt;&lt;r&gt;&lt;bioconductor&gt;&lt;scrnaseq&gt;&lt;10x-genomics&gt;" />
  <row Id="2391" PostHistoryTypeId="2" PostId="772" RevisionGUID="89e19367-dd2e-4f5a-b537-38150468a745" CreationDate="2017-06-16T11:24:00.487" UserId="61" Text="One method is to decompose multi-allelic records so that they're represented as one-allele, one-record using [vt](https://github.com/atks/vt) or similar:&#xD;&#xA;&#xD;&#xA;    bcftools view -v indels &lt;vcf&gt; |&#xD;&#xA;      vt decompose - |&#xD;&#xA;      awk '{if(length($5)&gt;length($4)) print}'&#xD;&#xA;&#xD;&#xA;Some decomposed alleles will come with excess reference padding. To left-shift and trim these, add a `normalize` step (NB matching these back to your input VCF becomes non-trivial):&#xD;&#xA;&#xD;&#xA;    bcftools view -v indels &lt;vcf&gt; |&#xD;&#xA;      vt decompose - |&#xD;&#xA;      vt normalize -r &lt;reference.fasta - |&#xD;&#xA;      awk '{if(length($4)==1) print}'&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2392" PostHistoryTypeId="2" PostId="773" RevisionGUID="e855fb24-a3a1-42f7-8032-61db4434a2b0" CreationDate="2017-06-16T11:24:31.330" UserId="71" Text="Using [vcffilterjs][1]&#xD;&#xA;&#xD;&#xA; - get the length of the REF;&#xD;&#xA; - loop over the ALT, ignore the symbolic&#xD;&#xA; - accept the variant if it's an insertion , eq:  len(ALT)&gt;len(REF) &#xD;&#xA;&#xD;&#xA;.&#xD;&#xA;&#xD;&#xA;    java -jar dist/vcffilterjs.jar -e 'function accept(vc){var a=vc.getAlleles();var lenRef=a.get(0).length();for(i=1;i&lt;a.size();++i) {var alt=a.get(i);if(alt.isSymbolic()) continue;var lenAlt=alt.length(); if(lenRef&lt;lenAlt) return true; } return false; }accept(variant);' input.vcf&#xD;&#xA;&#xD;&#xA;.&#xD;&#xA;&#xD;&#xA;  [1]: http://lindenb.github.io/jvarkit/VCFFilterJS.html&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2393" PostHistoryTypeId="2" PostId="774" RevisionGUID="fd5895ca-aaaa-4d88-b305-05d7baa52b84" CreationDate="2017-06-16T11:31:55.983" UserId="599" Text="Q:&#xD;&#xA;&quot;Recent&quot; breakthrough in bioinformatics tools for quantification (e.g.&#xD;&#xA;[Cufflinks][1]/[Kallisto][2]/[Salmon][3] etc.) and tools which can identify differential transcript usage (DTU) (e.g. [DRIMSeq][4], [Cufflinks][1] etc.) means that from RNA-seq data we can now relatively easy obtain a genome wide analysis of transcripts that are differentially used betwen conditions (aka transcript switches / transcript switching).&#xD;&#xA;&#xD;&#xA;What can you use these results for? In other words what systematic analysis does this transcript level data enables?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cole-trapnell-lab.github.io/cufflinks/&#xD;&#xA;  [2]: http://pachterlab.github.io/kallisto/&#xD;&#xA;  [3]: https://github.com/COMBINE-lab/salmon&#xD;&#xA;  [4]: https://bioconductor.org/packages/release/bioc/html/DRIMSeq.html" />
  <row Id="2394" PostHistoryTypeId="1" PostId="774" RevisionGUID="fd5895ca-aaaa-4d88-b305-05d7baa52b84" CreationDate="2017-06-16T11:31:55.983" UserId="599" Text="Post analysis of differentially transcripts usage (DTU)" />
  <row Id="2395" PostHistoryTypeId="3" PostId="774" RevisionGUID="fd5895ca-aaaa-4d88-b305-05d7baa52b84" CreationDate="2017-06-16T11:31:55.983" UserId="599" Text="&lt;rna-seq&gt;&lt;transcriptome&gt;&lt;software-recommendation&gt;" />
  <row Id="2396" PostHistoryTypeId="2" PostId="775" RevisionGUID="8f86f5af-e72c-4fe7-8c98-4e60daa5766b" CreationDate="2017-06-16T11:35:51.537" UserId="208" Text="Data preparation&#xD;&#xA;================&#xD;&#xA;&#xD;&#xA;Cell Ranger uses the Illumina sequencing output (`.bcl`) files&#xD;&#xA;&#xD;&#xA; 1. [Make fastq files][1]:&#xD;&#xA;`cellranger mkfastq` ==&gt; `.fastq`&#xD;&#xA;&#xD;&#xA; 2. Prepare count matrix: `cellranger count` ==&gt; `matrix.mtx, web_summary.html, cloupe.cloupe`&#xD;&#xA; 3. Optional: combine multiple `matrix.mtx` files (libraries): `cellranger aggr`&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;Data analysis&#xD;&#xA;=============&#xD;&#xA;**[Loupe Cell Browser][2] visualization of** `cloupe.cloupe` **files**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Count table** `matrix.mtx` **analysis options:**&#xD;&#xA;&#xD;&#xA; - [`Python`][3]&#xD;&#xA;&#xD;&#xA; - `R` [Cell Ranger R Kit][4]: `cellrangerRkit::load_cellranger_matrix()` ==&gt; ExpressionSet&#xD;&#xA;&#xD;&#xA; - `R` [Scater][5]: `scater::read10XResults()` ==&gt; SCESet object&#xD;&#xA;&#xD;&#xA; - `R` [Seurat][6]: `seurat::Read10X()` ==&gt; Seurat object&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/mkfastq&#xD;&#xA;  [2]: https://support.10xgenomics.com/single-cell-gene-expression/software/visualization/latest/what-is-loupe-cell-browser&#xD;&#xA;  [3]: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/python&#xD;&#xA;  [4]: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/rkit&#xD;&#xA;  [5]: http://hemberg-lab.github.io/scRNA.seq.course/scater-package.html&#xD;&#xA;  [6]: http://satijalab.org/seurat/pbmc-tutorial.html" />
  <row Id="2397" PostHistoryTypeId="2" PostId="776" RevisionGUID="86014dd9-2023-4c67-b62c-2f735c4382ec" CreationDate="2017-06-16T11:38:59.103" UserId="599" Text="I will take the liberty of giving one possible answers to my own question – but I’m very interested in other answers.&#xD;&#xA;&#xD;&#xA;One analysis type that such data enables is the analysis of transcript switches with predicted potential consequences. &#xD;&#xA;&#xD;&#xA;I myself have recently developed such a tool called [IsoformSwitchAnalyzeR][1]. IsoformSwitchAnalyzeR enables statistical identification (via [DRIMSeq][2]) of isoform switches with predicted functional consequences. The consequences analyzed can be chosen from a long list which includes gain/loss of protein domains, signal peptides changes in NMD sensitivity etc. The R package also enables easy visualization of isoform switches along with their consequences and it directly supports the output of [Cufflinks/Cuffdiff][3], [RSEM][4], [Salmon][5] and [Kallisto][6].&#xD;&#xA;&#xD;&#xA;Apart from enabling identification of interesting examples switch identification also enables systematic analysis of what genes are affected. For inspiration I will recommend [one of my own articles][7] (describing results obtained with IsoformSwitchAnalyzeR) as well Hector et al’s recent [bioRxiv paper][8] (which is not using IsoformSwitchAnalyzeR). Of particular interest and finesse is Hector’s analysis of how isoform switches can disrupt protein-protein interactions.&#xD;&#xA;&#xD;&#xA;Looking forward to hear more ideas. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/IsoformSwitchAnalyzeR/&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/bioc/html/DRIMSeq.html&#xD;&#xA;  [3]: http://cole-trapnell-lab.github.io/cufflinks/&#xD;&#xA;  [4]: http://deweylab.github.io/RSEM/&#xD;&#xA;  [5]: https://github.com/COMBINE-lab/salmon&#xD;&#xA;  [6]: http://pachterlab.github.io/kallisto/&#xD;&#xA;  [7]: http://mcr.aacrjournals.org/content/early/2017/06/02/1541-7786.MCR-16-0459&#xD;&#xA;  [8]: http://biorxiv.org/content/early/2016/09/21/076653" />
  <row Id="2398" PostHistoryTypeId="5" PostId="774" RevisionGUID="8951d16b-ffcc-4a1e-8f46-33f274237e27" CreationDate="2017-06-16T11:39:46.143" UserId="599" Comment="Removed a sentence that narrowed the question to much, fixed spelling mistake" Text="Q:&#xD;&#xA;&quot;Recent&quot; breakthrough in bioinformatics tools for quantification (e.g.&#xD;&#xA;[Cufflinks][1]/[Kallisto][2]/[Salmon][3] etc.) and tools which can identify differential transcript usage (DTU) (e.g. [DRIMSeq][4], [Cufflinks][1] etc.) means that from RNA-seq data we can now relatively easy obtain a genome wide analysis of transcripts that are differentially used betwen conditions.&#xD;&#xA;&#xD;&#xA;What can you use these results for? In other words what systematic analysis does this transcript level data enables?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cole-trapnell-lab.github.io/cufflinks/&#xD;&#xA;  [2]: http://pachterlab.github.io/kallisto/&#xD;&#xA;  [3]: https://github.com/COMBINE-lab/salmon&#xD;&#xA;  [4]: https://bioconductor.org/packages/release/bioc/html/DRIMSeq.html" />
  <row Id="2399" PostHistoryTypeId="4" PostId="774" RevisionGUID="8951d16b-ffcc-4a1e-8f46-33f274237e27" CreationDate="2017-06-16T11:39:46.143" UserId="599" Comment="Removed a sentence that narrowed the question to much, fixed spelling mistake" Text="Post analysis of differentially transcript usage (DTU)" />
  <row Id="2400" PostHistoryTypeId="2" PostId="777" RevisionGUID="7b4a356c-d66c-4aa7-b729-bb27824a3dc6" CreationDate="2017-06-16T11:42:56.740" UserId="885" Text="I have two cancer cell lines (OCI-Ly18 &amp; riva) that I want to find gene expression data for, but I'm not aware of many gene expression databases that allow searching by cell-line without searching by gene.&#xD;&#xA;&#xD;&#xA;I tried Genevestigator on the recommendation of this thread: https://www.researchgate.net/post/Is_there_a_database_to_find_out_which_genes_are_expressed_in_which_cell_lines&#xD;&#xA;but found no data on either cell-line.&#xD;&#xA;&#xD;&#xA;What would be the best way to find gene expression data for these cell lines?" />
  <row Id="2401" PostHistoryTypeId="1" PostId="777" RevisionGUID="7b4a356c-d66c-4aa7-b729-bb27824a3dc6" CreationDate="2017-06-16T11:42:56.740" UserId="885" Text="Searching for gene expression data by cell line" />
  <row Id="2402" PostHistoryTypeId="3" PostId="777" RevisionGUID="7b4a356c-d66c-4aa7-b729-bb27824a3dc6" CreationDate="2017-06-16T11:42:56.740" UserId="885" Text="&lt;database&gt;&lt;cancer&gt;&lt;differential-expression&gt;&lt;cell-line&gt;" />
  <row Id="2403" PostHistoryTypeId="5" PostId="776" RevisionGUID="6e636ac2-dd87-4cab-afc2-3c1a034a9c85" CreationDate="2017-06-16T11:55:50.510" UserId="599" Comment="removed &quot;bioRxiv&quot; since it could sound condescending." Text="I will take the liberty of giving one possible answers to my own question – but I’m very interested in other answers.&#xD;&#xA;&#xD;&#xA;One analysis type that such data enables is the analysis of transcript switches with predicted potential consequences. &#xD;&#xA;&#xD;&#xA;I myself have recently developed such a tool called [IsoformSwitchAnalyzeR][1]. IsoformSwitchAnalyzeR enables statistical identification (via [DRIMSeq][2]) of isoform switches with predicted functional consequences. The consequences analyzed can be chosen from a long list which includes gain/loss of protein domains, signal peptides changes in NMD sensitivity etc. The R package also enables easy visualization of isoform switches along with their consequences and it directly supports the output of [Cufflinks/Cuffdiff][3], [RSEM][4], [Salmon][5] and [Kallisto][6].&#xD;&#xA;&#xD;&#xA;Apart from enabling identification of interesting examples switch identification also enables systematic analysis of what genes are affected. For inspiration I will recommend [one of my own articles][7] (describing results obtained with IsoformSwitchAnalyzeR) as well Hector et al’s [recent paper][8] (which is not using IsoformSwitchAnalyzeR). Of particular interest and finesse is Hector’s analysis of how isoform switches can disrupt protein-protein interactions.&#xD;&#xA;&#xD;&#xA;Looking forward to hear more ideas. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/IsoformSwitchAnalyzeR/&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/bioc/html/DRIMSeq.html&#xD;&#xA;  [3]: http://cole-trapnell-lab.github.io/cufflinks/&#xD;&#xA;  [4]: http://deweylab.github.io/RSEM/&#xD;&#xA;  [5]: https://github.com/COMBINE-lab/salmon&#xD;&#xA;  [6]: http://pachterlab.github.io/kallisto/&#xD;&#xA;  [7]: http://mcr.aacrjournals.org/content/early/2017/06/02/1541-7786.MCR-16-0459&#xD;&#xA;  [8]: http://biorxiv.org/content/early/2016/09/21/076653" />
  <row Id="2404" PostHistoryTypeId="2" PostId="778" RevisionGUID="6453b085-4a48-423b-ade4-54f5c9c3ac6f" CreationDate="2017-06-16T11:59:37.793" UserId="599" Text="Try the [Gene Expression Omnibus][1] - it looks like they have some datasets.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/geo/" />
  <row Id="2405" PostHistoryTypeId="2" PostId="779" RevisionGUID="8fd54139-b00e-4798-abde-25ed1fdcfea5" CreationDate="2017-06-16T12:17:34.883" UserId="208" Text="`cellranger aggr` can combine multiple libraries (samples), and appends each barcode with an integer (e.g. AGACCATTGAGACTTA-1). The sample identity is not recorded in the combined `matrix.mtx` file.&#xD;&#xA;&#xD;&#xA;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate#gem_groups&#xD;&#xA;&#xD;&#xA;How can we keep and assign sample information to each cell after loading the data into `R`? (e.g. `seurat::Read10X()`)&#xD;&#xA;" />
  <row Id="2406" PostHistoryTypeId="1" PostId="779" RevisionGUID="8fd54139-b00e-4798-abde-25ed1fdcfea5" CreationDate="2017-06-16T12:17:34.883" UserId="208" Text="Handling sample identity in aggregated 10x libraries?" />
  <row Id="2407" PostHistoryTypeId="3" PostId="779" RevisionGUID="8fd54139-b00e-4798-abde-25ed1fdcfea5" CreationDate="2017-06-16T12:17:34.883" UserId="208" Text="&lt;rna-seq&gt;&lt;r&gt;&lt;scrnaseq&gt;&lt;10x-genomics&gt;" />
  <row Id="2408" PostHistoryTypeId="5" PostId="767" RevisionGUID="6b47d392-d9a6-482d-9683-981c49248ea9" CreationDate="2017-06-16T12:18:46.110" UserId="451" Comment="added 109 characters in body" Text="I already see for links when I try to search. Take a look at them below. Also if you can be precise about kind of data are you trying to find, it would be better&#xD;&#xA;&#xD;&#xA;https://pdmr.cancer.gov/&#xD;&#xA;&#xD;&#xA;http://cdt.northwestern.edu/news/patient-derived-xenograft-repository-now-available-researchers&#xD;&#xA;&#xD;&#xA;http://www.epo-berlin.com/epo-tumor-models-xenografts.html&#xD;&#xA;&#xD;&#xA;http://www.proxe.org/&#xD;&#xA;&#xD;&#xA;http://data-analysis.charite.de/care/&#xD;&#xA;&#xD;&#xA;http://www.oasis-genomics.org/&#xD;&#xA;&#xD;&#xA;http://www.cbioportal.org/&#xD;&#xA;&#xD;&#xA;Hope one of these above will serve the purpose for you.&#xD;&#xA;&#xD;&#xA;Update: added some more links that might serve. As for the Cellminer the last update was 2012." />
  <row Id="2409" PostHistoryTypeId="5" PostId="769" RevisionGUID="ca0390fb-dfb8-4b6e-8bff-6b081c840d8a" CreationDate="2017-06-16T12:28:03.227" UserId="61" Comment="clarify" Text="I'm looking to subset a standard VCF file to generate one which only includes insertions (i.e. not indels). &#xD;&#xA;&#xD;&#xA;I can get part of the way there with:&#xD;&#xA;&#xD;&#xA;    bcftools view -v indels &lt;vcf&gt; | awk '{if(length($4) == 1) print}'&#xD;&#xA;&#xD;&#xA;However this wouldn't catch an insertion that was part of a multi-allelic record with an insertion and deletion, where the reference length would be greater than 1 bp. Potentially then one way to go is a chain of decomposition, normalisation and then the same reference length filtering — surely there's a better way in one of the many VCF manipulation utils?" />
  <row Id="2410" PostHistoryTypeId="2" PostId="780" RevisionGUID="e714d0d5-f696-4c29-9d99-898eabd3eb93" CreationDate="2017-06-16T12:30:32.550" UserId="678" Text="Try [in this section][1] of [Cancer Cell Line Encyclopedia][2]. It has expression data for a lot of cancer cell lines. I have tried few times and found all cell lines I wanted, except HeLa. You just have to select mRNA expression data and download the file you prefer (raw data, gene-centric, etc.)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://portals.broadinstitute.org/ccle/data/browseData?conversationPropagation=begin&#xD;&#xA;  [2]: https://portals.broadinstitute.org/ccle/home" />
  <row Id="2411" PostHistoryTypeId="5" PostId="777" RevisionGUID="14804214-760c-48f7-9bb5-c07c6d045b95" CreationDate="2017-06-16T12:55:32.073" UserId="57" Comment="added 2 characters in body" Text="I have two cancer cell lines (OCI-Ly18 &amp; riva) that I want to find gene expression data for, but I'm not aware of many gene expression databases that allow searching by cell-line without searching by gene.&#xD;&#xA;&#xD;&#xA;I tried Genevestigator on the recommendation of this [thread](https://www.researchgate.net/post/Is_there_a_database_to_find_out_which_genes_are_expressed_in_which_cell_lines)&#xD;&#xA;but found no data on either cell-line.&#xD;&#xA;&#xD;&#xA;What would be the best way to find gene expression data for these cell lines?" />
  <row Id="2412" PostHistoryTypeId="2" PostId="781" RevisionGUID="1ae3a047-1f76-46ab-933b-c7ec8484e2d1" CreationDate="2017-06-16T13:17:22.843" UserId="64" Text="You have to keep track of the file order you used for `aggr`.  The suffix number represents which represents the aggregated sample. You could store this information in a text file and load it into R independently of the `Read10X` function and combine it with the cell names to get their sample names.&#xD;&#xA;&#xD;&#xA;&gt; Gem Groups&#xD;&#xA;&gt; &#xD;&#xA;&gt; &#xD;&#xA;&gt; This is an integer that is appended to each barcode in the&#xD;&#xA;&gt; gene-barcode matrix. For example, AGACCATTGAGACTTA-1 and&#xD;&#xA;&gt; AGACCATTGAGACTTA-2 are distinct cell barcodes from different&#xD;&#xA;&gt; libraries, despite having the same nucleotide sequence.&#xD;&#xA;&gt; &#xD;&#xA;&gt; The numbering of the GEM groups will reflect the order that the&#xD;&#xA;&gt; libraries were provided in the Aggregation CSV." />
  <row Id="2413" PostHistoryTypeId="2" PostId="782" RevisionGUID="1c3bcb65-d88a-4511-829f-d8587bf60903" CreationDate="2017-06-16T13:39:16.387" UserId="123" Text="I am analysing 142 samples belonging to 6 batches. Additionally, those samples belong to 72 strains, which means that for most of the strains there are two samples.&#xD;&#xA;I could fit simple models (for strain and batches for instance), but when I get to the &quot;full&quot; model (~batch+strain), I get the following error:&#xD;&#xA;&#xD;&#xA;    so &lt;- sleuth_fit(so, ~strain+batch, 'full')&#xD;&#xA;    Error in solve.default(t(X) %*% X) :&#xD;&#xA;      system is computationally singular: reciprocal condition number = 5.2412e-19&#xD;&#xA;&#xD;&#xA;I should point out that of the 72 strains, only 15 have samples in distinct batches.&#xD;&#xA;&#xD;&#xA;Is the error due to an unknown bug or rather to the experimental design? Does it mean that the information on batches cannot be used?&#xD;&#xA;&#xD;&#xA;Thanks" />
  <row Id="2414" PostHistoryTypeId="1" PostId="782" RevisionGUID="1c3bcb65-d88a-4511-829f-d8587bf60903" CreationDate="2017-06-16T13:39:16.387" UserId="123" Text="Getting a &quot;system is computationally singular&quot; error in sleuth" />
  <row Id="2415" PostHistoryTypeId="3" PostId="782" RevisionGUID="1c3bcb65-d88a-4511-829f-d8587bf60903" CreationDate="2017-06-16T13:39:16.387" UserId="123" Text="&lt;rna-seq&gt;&lt;r&gt;&lt;differential-expression&gt;" />
  <row Id="2416" PostHistoryTypeId="5" PostId="781" RevisionGUID="c60c8b47-6abf-4942-b680-aa63c4766c1a" CreationDate="2017-06-16T13:44:38.527" UserId="48" Comment="Add link, of the quoted text, add link from the quote, and formatting the sequences as italics" Text="You have to keep track of the file order you used for `aggr`.  The suffix number represents which represents the aggregated sample. You could store this information in a text file and load it into R independently of the `Read10X` function and combine it with the cell names to get their sample names. From the page you [link][1]:&#xD;&#xA;&#xD;&#xA;&gt; Gem Groups&#xD;&#xA;&gt; &#xD;&#xA;&gt; &#xD;&#xA;&gt; This is an integer that is appended to each barcode in the&#xD;&#xA;&gt; gene-barcode matrix. For example, *AGACCATTGAGACTTA-1* and&#xD;&#xA;&gt; *AGACCATTGAGACTTA-2* are distinct cell barcodes from different&#xD;&#xA;&gt; libraries, despite having the same nucleotide sequence.&#xD;&#xA;&gt; &#xD;&#xA;&gt; The numbering of the GEM groups will reflect the order that the&#xD;&#xA;&gt; libraries were provided in the [Aggregation CSV][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate#gem_groups&#xD;&#xA;  [2]: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate#csv_setup" />
  <row Id="2417" PostHistoryTypeId="24" PostId="781" RevisionGUID="c60c8b47-6abf-4942-b680-aa63c4766c1a" CreationDate="2017-06-16T13:44:38.527" Comment="Proposed by 48 approved by 64 edit id of 204" />
  <row Id="2418" PostHistoryTypeId="5" PostId="772" RevisionGUID="63128fe1-1646-44ed-826d-ee1efc16717f" CreationDate="2017-06-16T13:54:03.000" UserId="61" Comment="rm header" Text="One method is to decompose multi-allelic records so that they're represented as one-allele, one-record using [vt](https://github.com/atks/vt) or similar:&#xD;&#xA;&#xD;&#xA;    bcftools view -v indels &lt;vcf&gt; |&#xD;&#xA;      vt decompose - |&#xD;&#xA;      bcftools view -H |&#xD;&#xA;      awk '{if(length($5)&gt;length($4)) print}'&#xD;&#xA;&#xD;&#xA;Some decomposed alleles will come with excess reference padding. To left-shift and trim these, add a `normalize` step (NB matching these back to your input VCF becomes non-trivial):&#xD;&#xA;&#xD;&#xA;    bcftools view -v indels &lt;vcf&gt; |&#xD;&#xA;      vt decompose - |&#xD;&#xA;      vt normalize -r &lt;reference.fasta - |&#xD;&#xA;      awk '{if(length($4)==1) print}'&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2419" PostHistoryTypeId="2" PostId="783" RevisionGUID="9ba2997d-cbf5-4995-8d9c-1c125de99971" CreationDate="2017-06-16T13:56:08.720" UserId="818" Text="If we have a PDB structrure, how can we find residues physically interacting with each other in space? I know that we must find the distance between residues and if the distance is less than 5-6 Angstrom, we say that residues are physically interacting. But how can we find the distance between all residues and how can we finally determine the distances between all residues? Is there a software or webserver for that?" />
  <row Id="2420" PostHistoryTypeId="1" PostId="783" RevisionGUID="9ba2997d-cbf5-4995-8d9c-1c125de99971" CreationDate="2017-06-16T13:56:08.720" UserId="818" Text="How can we find the distance between all residues in a PDB file?" />
  <row Id="2421" PostHistoryTypeId="3" PostId="783" RevisionGUID="9ba2997d-cbf5-4995-8d9c-1c125de99971" CreationDate="2017-06-16T13:56:08.720" UserId="818" Text="&lt;protein-structure&gt;&lt;pdb&gt;" />
  <row Id="2422" PostHistoryTypeId="5" PostId="772" RevisionGUID="ac4dd90b-2b06-4439-bd84-c93ec71d0291" CreationDate="2017-06-16T14:21:01.260" UserId="61" Comment="add suggested method" Text="One method is to decompose multi-allelic records so that they're represented as one-allele, one-record using [vt](https://github.com/atks/vt) or similar:&#xD;&#xA;&#xD;&#xA;    bcftools view -v indels &lt;vcf&gt; |&#xD;&#xA;      vt decompose - |&#xD;&#xA;      bcftools view -H |&#xD;&#xA;      awk '{if(length($5)&gt;length($4)) print}'&#xD;&#xA;&#xD;&#xA;Some decomposed alleles will come with excess reference padding. To left-shift and trim these, add a `normalize` step (NB matching these back to your input VCF becomes non-trivial):&#xD;&#xA;&#xD;&#xA;    bcftools view -v indels &lt;vcf&gt; |&#xD;&#xA;      vt decompose - |&#xD;&#xA;      vt normalize -r &lt;reference.fasta - |&#xD;&#xA;      awk '{if(length($4)==1) print}'&#xD;&#xA;&#xD;&#xA;**edit:** As gringer suggests, this can also be done without vt:&#xD;&#xA;&#xD;&#xA;    bcftools view -Ou -v indels &lt;vcf&gt; |&#xD;&#xA;      bcftools norm -Ou -Nm - |&#xD;&#xA;      bcftools view -H |&#xD;&#xA;      awk '{if(length($5)&gt;length($4)) print}'&#xD;&#xA;&#xD;&#xA;To also include complex alleles, use `view -V snps` (!snvs) instead of `-v indels`" />
  <row Id="2423" PostHistoryTypeId="2" PostId="784" RevisionGUID="592d52fa-69b6-48f0-aaf3-95e8aceeeb8b" CreationDate="2017-06-16T14:22:17.763" UserId="191" Text="If you need to process multiple files, you could use [Biopython][1] to parse a PDB structure.&#xD;&#xA;&#xD;&#xA;&lt;!-- language-all: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    from Bio.PDB import PDBParser&#xD;&#xA;    &#xD;&#xA;    # create parser&#xD;&#xA;    parser = PDBParser()&#xD;&#xA;&#xD;&#xA;    # read structure from file&#xD;&#xA;    structure = parser.get_structure('PHA-L', '1fat.pdb')&#xD;&#xA;    &#xD;&#xA;    model = structure[0]&#xD;&#xA;    chain = model['A']&#xD;&#xA;    &#xD;&#xA;    # this example uses only the first residue of a single chain.&#xD;&#xA;    # it is easy to extend this to multiple chains and residues.&#xD;&#xA;    for residue1 in chain:&#xD;&#xA;        for residue2 in chain:&#xD;&#xA;            if residue1 != residue2:&#xD;&#xA;                # compute distance between CA atoms&#xD;&#xA;                try:&#xD;&#xA;                    distance = residue1['CA'] - residue2['CA']&#xD;&#xA;                except KeyError:&#xD;&#xA;                    ## no CA atom, e.g. for H_NAG&#xD;&#xA;                    continue&#xD;&#xA;                if distance &lt; 6:&#xD;&#xA;                    print(residue1, residue2, distance)&#xD;&#xA;            # stop after first residue&#xD;&#xA;            break&#xD;&#xA;&#xD;&#xA;If you need to look at one structure, using a viewer perhaps would be easier. You could try [PyMOL][2]: ([how to measure distance][3]). There are other PDB viewers, some of which can work even through a browser.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biopython.org/wiki/The_Biopython_Structural_Bioinformatics_FAQ&#xD;&#xA;  [2]: https://www.pymol.org/&#xD;&#xA;  [3]: https://pymolwiki.org/index.php/Distance" />
  <row Id="2424" PostHistoryTypeId="5" PostId="775" RevisionGUID="cf57362a-bce9-42b5-a63a-6e048abc425a" CreationDate="2017-06-16T14:26:03.450" UserId="208" Comment="important typo" Text="Data preparation&#xD;&#xA;================&#xD;&#xA;&#xD;&#xA;Cell Ranger uses the Illumina sequencing output (`.bcl`) files&#xD;&#xA;&#xD;&#xA; 1. [Make fastq files][1]:&#xD;&#xA;`cellranger mkfastq` ==&gt; `.fastq`&#xD;&#xA;&#xD;&#xA; 2. Prepare count matrix: `cellranger count` ==&gt; `matrix.mtx, web_summary.html, cloupe.cloupe`&#xD;&#xA; 3. Optional: combine multiple `matrix.mtx` files (libraries): `cellranger aggr`&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;Data analysis&#xD;&#xA;=============&#xD;&#xA;**[Loupe Cell Browser][2] visualization of** `cloupe.cloupe` **files**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Count table** `matrix.mtx` **analysis options:**&#xD;&#xA;&#xD;&#xA; - [`Python`][3]&#xD;&#xA;&#xD;&#xA; - `R` [Cell Ranger R Kit][4]: `cellrangerRkit::load_cellranger_matrix()` ==&gt; ExpressionSet&#xD;&#xA;&#xD;&#xA; - `R` [Scater][5]: `scater::read10XResults()` ==&gt; SCESet object&#xD;&#xA;&#xD;&#xA; - `R` [Seurat][6]: `Seurat::Read10X()` ==&gt; Seurat object&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/mkfastq&#xD;&#xA;  [2]: https://support.10xgenomics.com/single-cell-gene-expression/software/visualization/latest/what-is-loupe-cell-browser&#xD;&#xA;  [3]: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/python&#xD;&#xA;  [4]: https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/rkit&#xD;&#xA;  [5]: http://hemberg-lab.github.io/scRNA.seq.course/scater-package.html&#xD;&#xA;  [6]: http://satijalab.org/seurat/pbmc-tutorial.html" />
  <row Id="2425" PostHistoryTypeId="2" PostId="785" RevisionGUID="c96540c0-dbf2-4fda-831e-4fe9011ccc65" CreationDate="2017-06-16T14:32:43.610" UserId="888" Text="I am looking for the positions of annotated regulatory sequences in the human genome.&#xD;&#xA;&#xD;&#xA;I looked at [Ensembl regulatory Build][1] and [PAZAR][2] but I am not used to look for datasets and I failed to find what I was looking for.&#xD;&#xA;&#xD;&#xA;**Can you help to find annotated regulatory sequences (start and end positions) in the human genome?**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ensembl.org/info/genome/funcgen/regulatory_build.html&#xD;&#xA;  [2]: http://www.pazar.info/" />
  <row Id="2426" PostHistoryTypeId="1" PostId="785" RevisionGUID="c96540c0-dbf2-4fda-831e-4fe9011ccc65" CreationDate="2017-06-16T14:32:43.610" UserId="888" Text="Dataset: Locations of regulatory sequences in the human genome?" />
  <row Id="2427" PostHistoryTypeId="3" PostId="785" RevisionGUID="c96540c0-dbf2-4fda-831e-4fe9011ccc65" CreationDate="2017-06-16T14:32:43.610" UserId="888" Text="&lt;database&gt;&lt;human-genome&gt;" />
  <row Id="2428" PostHistoryTypeId="5" PostId="785" RevisionGUID="22a41920-c655-4047-a34e-c205b0352e9f" CreationDate="2017-06-16T14:42:31.880" UserId="888" Comment="added 39 characters in body" Text="I am looking for the positions of annotated regulatory sequences (promoters, enhancers and suppressors) in the human genome.&#xD;&#xA;&#xD;&#xA;I looked at [Ensembl regulatory Build][1] and [PAZAR][2] but I am not used to look for datasets and I failed to find what I was looking for.&#xD;&#xA;&#xD;&#xA;**Can you help to find annotated regulatory sequences (start and end positions) in the human genome?**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ensembl.org/info/genome/funcgen/regulatory_build.html&#xD;&#xA;  [2]: http://www.pazar.info/" />
  <row Id="2429" PostHistoryTypeId="2" PostId="786" RevisionGUID="b91f7d37-2f8e-4efd-8b81-aaeea15a657a" CreationDate="2017-06-16T14:45:22.267" UserId="37" Text="A one-liner:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    zcat my.vcf.gz |&#xD;&#xA;      perl -ane '$x=0;for $y (split(&quot;,&quot;,$F[4])){$x=1 if length($y)&gt;length($F[3])}print if /^#/||$x'&#xD;&#xA;&#xD;&#xA;or equivalently&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    zcat my.vcf.gz |&#xD;&#xA;      perl -ane '$x=0;map{$x=1 if length&gt;length($F[3])}split(&quot;,&quot;,$F[4]);print if /^#/||$x'&#xD;&#xA;&#xD;&#xA;For simple VCF operations, I generally recommend to write a script. This may be faster than those using heavy libraries. With a script, you only parse fields you care about; most libraries unnecessarily parse every field.&#xD;&#xA;&#xD;&#xA;&lt;hr&gt;&#xD;&#xA;&#xD;&#xA;On a related note, I recommend **not** to decompose multi-allelic sites unless necessary. Decomposing is tricky, makes VCF harder to parse and to understand and may be a potential source of errors. Here is an example:&#xD;&#xA;&#xD;&#xA;    #CHROM POS ID REF  ALT               QUAL FILTER  INFO  FORMAT  S1  S2  S3  S4&#xD;&#xA;    11     101 .  GCGT G,GCGA,GTGA,CCGT  199  PASS    .     GT      0/1 1/2 2/3 2/4&#xD;&#xA;&#xD;&#xA;vt decompose+normalize produces the following VCF:&#xD;&#xA;&#xD;&#xA;    #CHROM POS ID REF  ALT QUAL FILTER INFO FORMAT  S1   S2   S3   S4&#xD;&#xA;    11     101 .  GCGT G   199  PASS   .    GT      0/1  1/.  ./.  ./.&#xD;&#xA;    11     101 .  G    C   199  PASS   .    GT      0/.  ./.  ./.  ./1&#xD;&#xA;    11     102 .  CGT  TGA 199  PASS   .    GT      0/.  ./.  ./1  ./.&#xD;&#xA;    11     104 .  T    A   199  PASS   .    GT      0/.  ./1  1/.  1/.&#xD;&#xA;&#xD;&#xA;In theory, you can reconstruct the original VCF from this output. However, it is very challenging for a program to do that. When you compute allele frequency line-by-line, this VCF will give you wrong results. `bcftools norm -m-` replaces &quot;.&quot; with &quot;0&quot;. You can get a correct ALT allele frequency from the bcftools output, but a wrong REF allele frequency. Furthermore, vt is also imperfect in that &quot;CGT=&gt;TGA&quot; is not decomposed.&#xD;&#xA;&#xD;&#xA;My preferred output is:&#xD;&#xA;&#xD;&#xA;    #CHROM POS    ID     REF    ALT    QUAL   FILTER INFO   FORMAT S1     S2     S3     S4&#xD;&#xA;    11     101    .      GCGT   G,&lt;M&gt;  0      .      .      GT     0/1    1/2    2/2    2/2&#xD;&#xA;    11     101    .      G      C,&lt;M&gt;  0      .      .      GT     0/2    2/0    0/0    0/1&#xD;&#xA;    11     102    .      C      T,&lt;M&gt;  0      .      .      GT     0/2    2/0    0/1    0/0&#xD;&#xA;    11     104    .      T      A,&lt;M&gt;  0      .      .      GT     0/2    2/1    1/1    1/0&#xD;&#xA;&#xD;&#xA;Here we use a symbolic allele `&lt;M&gt;` to represent &quot;another ALT allele&quot;. You can calculate the allele frequency by looking at one line, and won't confuse other ALT alleles with REF. bgt can produce such a VCF indirectly. However, it discards all INFO, so is not a practical solution, either.&#xD;&#xA;&#xD;&#xA;In summary, it is very difficult to decompose multi-allelic sites. When you get decomposing wrong, your downstream analyses may be inaccurate. Decomposition should be used with caution." />
  <row Id="2430" PostHistoryTypeId="2" PostId="787" RevisionGUID="dcfdd83d-b291-4c13-af43-9c97c29b044b" CreationDate="2017-06-16T14:48:10.087" UserId="252" Text="Could you use CCP4's NCONT program? There's a GUI and a command line interface, whatever suits. You can specify which chains you want to target and interact with and set a cut off for distance. The bonus here is once you're in you have a nice suite of other structural tools to use.&#xD;&#xA;&#xD;&#xA;If you're just doing it once, the GUI is friendly enough to work things out, if you're doing a batch then you can run it across several files via the command line." />
  <row Id="2431" PostHistoryTypeId="5" PostId="777" RevisionGUID="9742d119-9fe8-44ae-9798-980ad6543055" CreationDate="2017-06-16T14:49:44.300" UserId="180" Comment="added links to https://genevestigator.com" Text="I have two cancer cell lines (OCI-Ly18 &amp; riva) that I want to find gene expression data for, but I'm not aware of many gene expression databases that allow searching by cell-line without searching by gene.&#xD;&#xA;&#xD;&#xA;I tried [Genevestigator][1] on the recommendation of this [thread](https://www.researchgate.net/post/Is_there_a_database_to_find_out_which_genes_are_expressed_in_which_cell_lines)&#xD;&#xA;but found no data on either cell line.&#xD;&#xA;&#xD;&#xA;What would be the best way to find gene expression data for these cell lines?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://genevestigator.com" />
  <row Id="2432" PostHistoryTypeId="24" PostId="777" RevisionGUID="9742d119-9fe8-44ae-9798-980ad6543055" CreationDate="2017-06-16T14:49:44.300" Comment="Proposed by 180 approved by 191, 37 edit id of 205" />
  <row Id="2433" PostHistoryTypeId="2" PostId="788" RevisionGUID="e6fdaded-75da-4ec4-b7e8-a5e71d766ff5" CreationDate="2017-06-16T14:53:13.360" UserId="298" Text="This can be done quite easily using Ensebl's [BioMart][1].&#xD;&#xA;&#xD;&#xA;1. Choose the Ensembl Regulation database:&#xD;&#xA;&#xD;&#xA; [![Ensembl screenshot][2]][2] &#xD;&#xA;&#xD;&#xA;2. Select the &quot;Human Regulatory Features&quot; dataset:&#xD;&#xA;&#xD;&#xA; [![human regulatory features][3]][3]&#xD;&#xA;&#xD;&#xA;3. That's basically it right there, just click on &quot;Results&quot;:&#xD;&#xA;&#xD;&#xA; [![results link][4]][4]&#xD;&#xA;&#xD;&#xA;4. Export to file and click &quot;Go&quot;:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; [![GO link][5]][5]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;This will download a file called `mart_export.txt` which looks like this (I chose TSV for tab separated values):&#xD;&#xA;&#xD;&#xA;    $ head mart_export.txt &#xD;&#xA;    Chromosome/scaffold name	Start (bp)	End (bp)	Feature type&#xD;&#xA;    18	76429380	76430144	Open chromatin&#xD;&#xA;    8	66405962	66406502	Open chromatin&#xD;&#xA;    4	61184401	61184600	CTCF Binding Site&#xD;&#xA;    X	40733600	40737000	Promoter&#xD;&#xA;    5	97407001	97407200	CTCF Binding Site&#xD;&#xA;    X	73946201	73946600	Promoter Flanking Region&#xD;&#xA;    15	19948201	19949200	CTCF Binding Site&#xD;&#xA;    5	11302601	11303143	Open chromatin&#xD;&#xA;    2	208407801	208408000	CTCF Binding Site&#xD;&#xA;    &#xD;&#xA;As you can see the format is pretty straightforward. The first column has the chromosome or scaffold name (it should always be a chromosome for human), the second and third are the star and end positions and the last field is the type of region. &#xD;&#xA;&#xD;&#xA;You can also choose to limit by region type by choosing a filter (click the &quot;Filters&quot; link) in BioMart before downloading or by simply parsing the file once you've downloaded it. &#xD;&#xA;&#xD;&#xA;  [1]: http://www.ensembl.org/biomart/martview&#xD;&#xA;  [2]:  https://i.stack.imgur.com/tm8MK.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/6zTm8.png&#xD;&#xA;  [4]: https://i.stack.imgur.com/oPctm.png&#xD;&#xA;  [5]: https://i.stack.imgur.com/SYI1k.png" />
  <row Id="2434" PostHistoryTypeId="5" PostId="787" RevisionGUID="469f1715-9bd7-42b9-9390-bbb4893ef711" CreationDate="2017-06-16T14:57:04.067" UserId="252" Comment="added 63 characters in body" Text="Could you use CCP4's NCONT program? There's a GUI and a command line interface, whatever suits. You can specify which chains you want to target and interact with and set a cut off for distance. The bonus here is once you're in you have a nice suite of other structural tools to use.&#xD;&#xA;&#xD;&#xA;If you're just doing it once, the GUI is friendly enough to work things out, if you're doing a batch then you can run it across several files via the command line.&#xD;&#xA;&#xD;&#xA;[Download][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ccp4.ac.uk/download/" />
  <row Id="2435" PostHistoryTypeId="5" PostId="782" RevisionGUID="004f5a1a-9742-4097-b4cc-a587e68c822b" CreationDate="2017-06-16T15:07:34.600" UserId="123" Comment="Clarified the experimental design, plus style improvements" Text="I am analysing 142 samples belonging to 6 batches. Additionally, those samples belong to 72 strains, which means that for most of the strains there are two samples.&#xD;&#xA;&#xD;&#xA;I could fit simple models (for strain and batches for instance), but when I get to the &quot;full&quot; model (~batch+strain), I get the following error:&#xD;&#xA;&#xD;&#xA;    so &lt;- sleuth_fit(so, ~strain+batch, 'full')&#xD;&#xA;    Error in solve.default(t(X) %*% X) :&#xD;&#xA;      system is computationally singular: reciprocal condition number = 5.2412e-19&#xD;&#xA;&#xD;&#xA;I should point out that of the 72 strains, only 15 have samples in distinct batches. This means that most strains (57) have both samples in the same batch.&#xD;&#xA;&#xD;&#xA;Is the error due to an unknown bug or rather to the experimental design? Does it mean that the information on batches cannot be used?&#xD;&#xA;&#xD;&#xA;Thanks" />
  <row Id="2436" PostHistoryTypeId="2" PostId="789" RevisionGUID="71b150ea-76d8-4bcf-a9a3-700cde7d9f51" CreationDate="2017-06-16T15:10:27.567" UserId="48" Text="This happen when the variables (strain +batch) create a design matrix like this:&#xD;&#xA;&#xD;&#xA;    batch strain&#xD;&#xA;    1 1 #&#xD;&#xA;    1 1 #&#xD;&#xA;    1 2&#xD;&#xA;    2 2&#xD;&#xA;    3 3&#xD;&#xA;    4 3&#xD;&#xA;    ...&#xD;&#xA;    16 72&#xD;&#xA;&#xD;&#xA;Which means that some of the covariates are not linearly independent (ie batch 1 and strain 1), all the strain 1 is in batch 1. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&lt;sub&gt;There are a lot of related question in Bioconductor [support forum][1], from where I expanded an [answer][2].&lt;/sub&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.google.es/search?q=site%3Asupport.bioconductor.org%20%22system%20is%20computationally%20singular%22&amp;rlz=1C1WPZB_enES727ES727&amp;oq=site%3Asupport.bioconductor.org%20%22system%20is%20computationally%20singular%22&amp;ie=UTF-8&#xD;&#xA;  [2]: https://support.bioconductor.org/p/62058/#62059" />
  <row Id="2437" PostHistoryTypeId="2" PostId="790" RevisionGUID="df334347-c98d-4aa9-a6f9-ffc518765396" CreationDate="2017-06-16T15:33:36.017" UserId="252" Text="You could use [PISA][1] assuming you have a PDB file of cellulase? Sorry it's a short answer and should be a comment, I don't have the reputation, however.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ccp4.ac.uk/MG/ccp4mg_help/pisa.html" />
  <row Id="2438" PostHistoryTypeId="5" PostId="761" RevisionGUID="4dc81eb5-e6d0-4da8-8194-c73a361f53b4" CreationDate="2017-06-16T16:00:47.170" UserId="712" Comment="added 430 characters in body" Text="I am looking at the presence of viral genotypes within individual samples within an assay. Often times there is a sample whose read counts are firing off the charts and this sample tends to &quot;bleed through&quot; to the other samples. &#xD;&#xA;&#xD;&#xA;I have recently tried confronting this problem with chi square analysis of conditional probability. For instance let's say I observe with 10 sample viral genotypes A and B. I then calculate the expected number of samples having both genotypes A and B from the number of A positive samples and number of B positive samples. &#xD;&#xA;&#xD;&#xA;So far this is all I have. I do not know if anyone else has any ideas on how to determine and quantify bleed through in viral genotyping. &#xD;&#xA;&#xD;&#xA;Slightly more information about this situation: these samples were run on Illumina HiSeq, we are planning on doing another HiSeq run with even more samples shortly so would like to take proper precautions with that, additionally the current pipeline utilizes Novobarcode as a demultiplexer so if anyone has suggestions on what I can implement during processing to minimize the effect of cross contamination that'd be helpful. " />
  <row Id="2439" PostHistoryTypeId="2" PostId="791" RevisionGUID="57c2cbb6-7ffd-41b6-98ee-ca2210d995e6" CreationDate="2017-06-16T19:40:35.053" UserId="894" Text="Promoters and enhancers based on CAGE-seq from FANTOM5:&#xD;&#xA;http://fantom.gsc.riken.jp/5/data/&#xD;&#xA;&#xD;&#xA;Just choose the category and download bed-file, e.g. CAGE-peaks would be promoters. There are READMEs in each folder that explain the data. &#xD;&#xA; &#xD;&#xA;" />
  <row Id="2440" PostHistoryTypeId="2" PostId="792" RevisionGUID="f9b3fa01-0d4c-450b-b0ed-e940ce336485" CreationDate="2017-06-16T20:09:38.157" UserId="895" Text="You can use [MDtraj][1]. The package is easy to install using Anaconda.&#xD;&#xA;&#xD;&#xA;You can get the interacting residues using the following snippet (taken from http://mdtraj.org/1.6.2/examples/native-contact.html)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    heavy_pairs = np.array(&#xD;&#xA;        [(i,j) for (i,j) in combinations(heavy, 2)&#xD;&#xA;            if abs(native.topology.atom(i).residue.index - \&#xD;&#xA;                   native.topology.atom(j).residue.index) &gt; 3])&#xD;&#xA;    &#xD;&#xA;    # compute the distances between these pairs in the native state&#xD;&#xA;    heavy_pairs_distances = md.compute_distances(native[0], heavy_pairs)[0]&#xD;&#xA;    # and get the pairs s.t. the distance is less than NATIVE_CUTOFF&#xD;&#xA;    native_contacts = heavy_pairs[heavy_pairs_distances &lt; NATIVE_CUTOFF]&#xD;&#xA;    print(&quot;Number of native contacts&quot;, len(native_contacts))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://mdtraj.org/1.8.0/" />
  <row Id="2442" PostHistoryTypeId="2" PostId="793" RevisionGUID="0121f419-b924-4811-855f-6959c4210377" CreationDate="2017-06-16T23:03:39.673" UserId="776" Text="If you're doing set operations, you could use [`vcf2bed`][1]:&#xD;&#xA;&#xD;&#xA;    $ vcf2bed --insertions &lt; in.vcf &gt; out.bed&#xD;&#xA;&#xD;&#xA;Based on the VCF v4.2 specification, `--snvs`, `--insertions`, and `--deletions` are options available to filter input. In each case, the length of the reference and alternate alleles is used to determine which type of variant is being handled.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/vcf2bed.html" />
  <row Id="2443" PostHistoryTypeId="6" PostId="750" RevisionGUID="6e9611f7-d7ea-4531-a3b2-c596b5fb8955" CreationDate="2017-06-17T01:10:29.230" UserId="734" Comment="edited tags" Text="&lt;cancer&gt;&lt;cell-line&gt;" />
  <row Id="2446" PostHistoryTypeId="2" PostId="794" RevisionGUID="796a3e30-c6af-4a6b-9740-febc1858ea10" CreationDate="2017-06-18T17:04:43.097" UserId="35" Text="If you are looking for cancer mutations, the primary resource is COSMIC and they provide GRCh38 VCFs. The download page is here: http://cancer.sanger.ac.uk/cosmic/download&#xD;&#xA;&#xD;&#xA;It'll be up to you how you define &quot;common&quot;, but the VCF includes a lot of information you can use." />
  <row Id="2447" PostHistoryTypeId="2" PostId="795" RevisionGUID="8a9118cd-5940-494f-b070-acc53768e7a3" CreationDate="2017-06-18T18:04:09.980" UserId="491" Text="I have a [blog post][1] that describes the effective length (as well as these different relative abundance units).  The short explanation is that what people refer to as the &quot;effective length&quot; is actually the _expected_ effective length (i.e., the expectation, in a statistical sense, of the effective length).  The notion of effective length is actually a property of a transcript, fragment pair, and is equal to the number of potential starting locations for a fragment of this length on the given transcript.  If you take the average, over all fragments mapping to a transcript (potentially weighted by the conditional probability of this mapping), this quantity is the expected effective length of the transcript.  This is often approximated as simply l_i - \mu, or l_i - \mu_{l_i} --- where \mu_{l_i} is the mean of the _conditional_ fragment length distribution (conditioned on the fragment length being &lt; l_i to account for exactly the issue that you raise).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://robpatro.com/blog/?p=235" />
  <row Id="2448" PostHistoryTypeId="2" PostId="796" RevisionGUID="9dff471d-f36e-4bcb-bcad-68eb2d3b8feb" CreationDate="2017-06-19T00:24:14.877" UserId="156" Text="**The setup**&#xD;&#xA;&#xD;&#xA;Imagine that I work on an organism without a reference genome, and that the closest reference genome I can get is quite diverged. E.g. ~10% diverged in terms of SNVs when measured with short reads, and also has a lot of structural variants too. &#xD;&#xA;&#xD;&#xA;Now imagine I get a 1 million base-pair long-read (e.g. from Nanopore data) for my organism. The question is this:&#xD;&#xA;&#xD;&#xA;How can I estimate the proportion of the read that is meaningful sequence vs garbage? &#xD;&#xA;&#xD;&#xA;**Some things that probably won't work**&#xD;&#xA;&#xD;&#xA;Most standard approaches won't work here. E.g. I could try mapping the read to the reference, but *even if the read was perfectly good* I wouldn't expect most of it to map thanks to true structural variations between the read and the reference. The same goes for standard alignment or BLAST. &#xD;&#xA;&#xD;&#xA;**Some things that might work**&#xD;&#xA;&#xD;&#xA;The best naive method here seems to be to cut the read up into smaller pieces (either overlapping or not) and use standard approaches to map/align each of these. &#xD;&#xA;&#xD;&#xA;So, what have people tried for this? And what tools have you used and why?" />
  <row Id="2449" PostHistoryTypeId="1" PostId="796" RevisionGUID="9dff471d-f36e-4bcb-bcad-68eb2d3b8feb" CreationDate="2017-06-19T00:24:14.877" UserId="156" Text="How to estimate whether a long-read is meaningful sequence?" />
  <row Id="2450" PostHistoryTypeId="3" PostId="796" RevisionGUID="9dff471d-f36e-4bcb-bcad-68eb2d3b8feb" CreationDate="2017-06-19T00:24:14.877" UserId="156" Text="&lt;alignment&gt;&lt;read-mapping&gt;&lt;long-reads&gt;" />
  <row Id="2451" PostHistoryTypeId="2" PostId="797" RevisionGUID="344e5cc1-1cd3-45e2-8561-f778de522325" CreationDate="2017-06-19T00:34:27.393" UserId="163" Text="As a first pass, you could check if the read is chimeric. [Porechop][1] searches for known nanopore adaptors both on the ends and through the middle of the read. This won't resolve issues around blocked or empty pores, but it will at least check if you have found two long-ish reads lumped into the same file.&#xD;&#xA;&#xD;&#xA;By default, Porechop splits chimeric reads into two (or more, I suppose!) non-chimeric reads, but the option `--discard-middle` would be a quick and easy way to check - run it on a fasta file containing only your long read, and if the output is empty, the read is chimeric.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/rrwick/Porechop" />
  <row Id="2452" PostHistoryTypeId="2" PostId="798" RevisionGUID="7460bacb-6818-47b0-af5d-f978d586082f" CreationDate="2017-06-19T00:50:23.480" UserId="818" Text="I have a refseq ID of a protein from E.coli and I want to find homologs of this protein. I ran Blast against refseq database but I got a lot of sequences most of which were from Ecoli again. I decided to run PSI-Blast to get more divergent species, but I do not exactly know if my result are real homologs or false positives. what is your idea for finding homologous protein sequences from more divergent species? And what can I do for selecting the real (but not false positive) hits?" />
  <row Id="2453" PostHistoryTypeId="1" PostId="798" RevisionGUID="7460bacb-6818-47b0-af5d-f978d586082f" CreationDate="2017-06-19T00:50:23.480" UserId="818" Text="Finding homologs of a protein sequence" />
  <row Id="2454" PostHistoryTypeId="3" PostId="798" RevisionGUID="7460bacb-6818-47b0-af5d-f978d586082f" CreationDate="2017-06-19T00:50:23.480" UserId="818" Text="&lt;sequence-homology&gt;" />
  <row Id="2455" PostHistoryTypeId="6" PostId="796" RevisionGUID="83852285-ef0c-4a4a-acc4-ca152d01cf7b" CreationDate="2017-06-19T03:42:46.103" UserId="73" Comment="edited tags" Text="&lt;alignment&gt;&lt;nanopore&gt;&lt;read-mapping&gt;" />
  <row Id="2456" PostHistoryTypeId="2" PostId="799" RevisionGUID="f6a29bdf-004c-402c-89b1-baefd92623dd" CreationDate="2017-06-19T04:32:07.353" UserId="73" Text="If you're looking at a single organism, in the absence of a reference genome you can map other reads to suspect reads and look at coverage. Looking at the actual sequence can also be useful: real DNA usually doesn't have an abundance of two different bases.&#xD;&#xA;&#xD;&#xA;Nanopore reads also give another way to see if the read looks weird by having a look at the raw signal. Our [chimeric reads paper](https://f1000research.com/articles/6-631/v1) gives a few examples of what DNA sequence should look like under normal circumstances. Here's the first raw signal figure from that paper:&#xD;&#xA;&#xD;&#xA;[![good raw signal][1]][1]&#xD;&#xA;&#xD;&#xA;If there's lots of contiguous signal with very similar current levels (i.e. it looks like the &quot;stall&quot; region, then it's not a good read and any basecalls should be ignored.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/JOCQu.gif" />
  <row Id="2457" PostHistoryTypeId="2" PostId="800" RevisionGUID="af401592-ad50-4bbf-9b94-036299789c1e" CreationDate="2017-06-19T04:39:10.010" UserId="73" Text="It sounds like [Smart BLAST](https://blast.ncbi.nlm.nih.gov/smartblast/) might do what you want. Here's the description of it's goal:&#xD;&#xA;&#xD;&#xA;&gt; SmartBLAST is a new and experimental NCBI tool that makes it easier to&#xD;&#xA;&gt; complete common sequence analysis tasks, such as finding a candidate&#xD;&#xA;&gt; protein name for a sequence, locating regions of high sequence&#xD;&#xA;&gt; conservation, or identifying regions covered by database sequences but&#xD;&#xA;&gt; missing from the query.&#xD;&#xA;&gt; &#xD;&#xA;&gt; To do this, SmartBLAST performs the following tasks in much less time&#xD;&#xA;&gt; than it takes to run a typical BLASTp search:&#xD;&#xA;&gt; &#xD;&#xA;&gt;    - a BLASTp comparison of the query with the closest matching sequences available;&#xD;&#xA;&gt;    - a parallel BLASTp search to find the closest matches to high quality sequences from model organisms;&#xD;&#xA;&gt;    - a multiple alignment between the query and five of the closest matching sequences (usually including two high quality sequences);&#xD;&#xA;&gt;    - an analysis that produces a phylogenetic tree from the multiple sequence alignment.&#xD;&#xA;&#xD;&#xA;[from [NCBI Insights](https://ncbiinsights.ncbi.nlm.nih.gov/2015/07/29/smartblast/)]" />
  <row Id="2463" PostHistoryTypeId="5" PostId="746" RevisionGUID="dc0f0130-19bc-446d-9393-b03ca9609340" CreationDate="2017-06-19T09:06:55.287" UserId="681" Comment="added 58 characters in body" Text="[poRe][1] has a `show.layout()` function which shows you the 32*16 grid on which the channels are arranged. EDIT: THIS APPEARS NOT TO BE THE CORRECT PHYSICAL LAYOUT.&#xD;&#xA;&#xD;&#xA;         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]&#xD;&#xA;     [1,]  125  121  117  113  109  105  101   97   93    89    85    81    77&#xD;&#xA;     [2,]  126  122  118  114  110  106  102   98   94    90    86    82    78&#xD;&#xA;     [3,]  127  123  119  115  111  107  103   99   95    91    87    83    79&#xD;&#xA;     [4,]  128  124  120  116  112  108  104  100   96    92    88    84    80&#xD;&#xA;     [5,]  253  249  245  241  237  233  229  225  221   217   213   209   205&#xD;&#xA;     [6,]  254  250  246  242  238  234  230  226  222   218   214   210   206&#xD;&#xA;     [7,]  255  251  247  243  239  235  231  227  223   219   215   211   207&#xD;&#xA;     [8,]  256  252  248  244  240  236  232  228  224   220   216   212   208&#xD;&#xA;     [9,]  381  377  373  369  365  361  357  353  349   345   341   337   333&#xD;&#xA;    [10,]  382  378  374  370  366  362  358  354  350   346   342   338   334&#xD;&#xA;    [11,]  383  379  375  371  367  363  359  355  351   347   343   339   335&#xD;&#xA;    [12,]  384  380  376  372  368  364  360  356  352   348   344   340   336&#xD;&#xA;    [13,]  509  505  501  497  493  489  485  481  477   473   469   465   461&#xD;&#xA;    [14,]  510  506  502  498  494  490  486  482  478   474   470   466   462&#xD;&#xA;    [15,]  511  507  503  499  495  491  487  483  479   475   471   467   463&#xD;&#xA;    [16,]  512  508  504  500  496  492  488  484  480   476   472   468   464&#xD;&#xA;          [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]&#xD;&#xA;     [1,]    73    69    65    61    57    53    49    45    41    37    33    29&#xD;&#xA;     [2,]    74    70    66    62    58    54    50    46    42    38    34    30&#xD;&#xA;     [3,]    75    71    67    63    59    55    51    47    43    39    35    31&#xD;&#xA;     [4,]    76    72    68    64    60    56    52    48    44    40    36    32&#xD;&#xA;     [5,]   201   197   193   189   185   181   177   173   169   165   161   157&#xD;&#xA;     [6,]   202   198   194   190   186   182   178   174   170   166   162   158&#xD;&#xA;     [7,]   203   199   195   191   187   183   179   175   171   167   163   159&#xD;&#xA;     [8,]   204   200   196   192   188   184   180   176   172   168   164   160&#xD;&#xA;     [9,]   329   325   321   317   313   309   305   301   297   293   289   285&#xD;&#xA;    [10,]   330   326   322   318   314   310   306   302   298   294   290   286&#xD;&#xA;    [11,]   331   327   323   319   315   311   307   303   299   295   291   287&#xD;&#xA;    [12,]   332   328   324   320   316   312   308   304   300   296   292   288&#xD;&#xA;    [13,]   457   453   449   445   441   437   433   429   425   421   417   413&#xD;&#xA;    [14,]   458   454   450   446   442   438   434   430   426   422   418   414&#xD;&#xA;    [15,]   459   455   451   447   443   439   435   431   427   423   419   415&#xD;&#xA;    [16,]   460   456   452   448   444   440   436   432   428   424   420   416&#xD;&#xA;          [,26] [,27] [,28] [,29] [,30] [,31] [,32]&#xD;&#xA;     [1,]    25    21    17    13     9     5     1&#xD;&#xA;     [2,]    26    22    18    14    10     6     2&#xD;&#xA;     [3,]    27    23    19    15    11     7     3&#xD;&#xA;     [4,]    28    24    20    16    12     8     4&#xD;&#xA;     [5,]   153   149   145   141   137   133   129&#xD;&#xA;     [6,]   154   150   146   142   138   134   130&#xD;&#xA;     [7,]   155   151   147   143   139   135   131&#xD;&#xA;     [8,]   156   152   148   144   140   136   132&#xD;&#xA;     [9,]   281   277   273   269   265   261   257&#xD;&#xA;    [10,]   282   278   274   270   266   262   258&#xD;&#xA;    [11,]   283   279   275   271   267   263   259&#xD;&#xA;    [12,]   284   280   276   272   268   264   260&#xD;&#xA;    [13,]   409   405   401   397   393   389   385&#xD;&#xA;    [14,]   410   406   402   398   394   390   386&#xD;&#xA;    [15,]   411   407   403   399   395   391   387&#xD;&#xA;    [16,]   412   408   404   400   396   392   388&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/mw55309/poRe_docs" />
  <row Id="2464" PostHistoryTypeId="2" PostId="802" RevisionGUID="97f180e5-65c1-42b3-9df2-aec9b40d8194" CreationDate="2017-06-19T10:19:44.403" UserId="302" Text="I'm looking for the exact invocation used to generate the 16SMicrobial database that you can download from here:&#xD;&#xA;&#xD;&#xA;https://ftp.ncbi.nlm.nih.gov/blast/db/&#xD;&#xA;&#xD;&#xA;I'm hoping to create the same type of blastdb with the same properties with custom sequences." />
  <row Id="2465" PostHistoryTypeId="1" PostId="802" RevisionGUID="97f180e5-65c1-42b3-9df2-aec9b40d8194" CreationDate="2017-06-19T10:19:44.403" UserId="302" Text="What command/invocation is used to generate NCBI 16SMicrobial blastdb" />
  <row Id="2466" PostHistoryTypeId="3" PostId="802" RevisionGUID="97f180e5-65c1-42b3-9df2-aec9b40d8194" CreationDate="2017-06-19T10:19:44.403" UserId="302" Text="&lt;blast&gt;" />
  <row Id="2467" PostHistoryTypeId="2" PostId="803" RevisionGUID="0c1b9606-4667-4519-8a2b-2a6112966064" CreationDate="2017-06-19T11:18:01.990" UserId="48" Text="As part of a project me and some teammates did a [script][1] that outputs visual maps of distances between residues. It uses Biopython. &#xD;&#xA;&#xD;&#xA;The module contact_map.py does what you are looking for. As an example, if you want to find the residues whose CA are below 5 you can run the following command:&#xD;&#xA;&#xD;&#xA;    python3 contact_map.py pdb1cd8.ent -a CA -CA 5 &#xD;&#xA;&#xD;&#xA;This will produce three files:&#xD;&#xA;&#xD;&#xA;    distance_map_pdb1cd8_CA.png # Heatmap of the distance between the residues&#xD;&#xA;    contact_map_pdb1cd8_CA.png # Black/White heatmap: If it is at that min distance&#xD;&#xA;    contact_map.log  # The actions taken&#xD;&#xA;&#xD;&#xA;If you don't have downloaded already the pdb structue you can use the main module:&#xD;&#xA;&#xD;&#xA;    python3 cozmic.py real 1cd8 -a CA -CA 5 &#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/llrs/PYT-SBI" />
  <row Id="2469" PostHistoryTypeId="2" PostId="805" RevisionGUID="39d5caed-1698-47c3-81f8-1b6f869b8be4" CreationDate="2017-06-19T12:17:45.603" UserId="298" Text="I don't know what specific properties you are considering. As far as I know, that's just a normal blast database, like any other. These are produced using the `formatdb` command which is part of the `ncbi-blast` software. You haven't specified what system you are using so I can't help you find and install it, but the command will be something like:&#xD;&#xA;&#xD;&#xA;    formatdb -p F -i yourSeqs.fasta -n yourDBName&#xD;&#xA;&#xD;&#xA;* `formatdb` : the command&#xD;&#xA;* `-p F` : protein: false. This is needed to make databases of nucleotide sequences. &#xD;&#xA;* `-i yourSeqs.fasta` : your input file with all your sequences in fasta format. &#xD;&#xA;* `-n yourDBName` : the name (choose whatever you like) for your database.&#xD;&#xA;&#xD;&#xA;You can then use `yourDBName` as a database to blast against. The above assumes yuou are " />
  <row Id="2473" PostHistoryTypeId="2" PostId="807" RevisionGUID="9f647253-d44d-4843-ba39-241abcf6687b" CreationDate="2017-06-19T13:15:22.910" UserId="492" Text="I would like to select a random record (that is description, sequence, and quality scores) from a large set of `n` unaligned sequencing reads in `log(n)` time complexity or better.  The records do not fit in RAM and would need to be stored on disk.  Ideally, I would like to store the reads in a compressed format.&#xD;&#xA;&#xD;&#xA;I imagine that I could align the reads to a reference genome and then use the BAM format to do my lookups, but I would prefer a solution that does not require a reference genome.&#xD;&#xA;&#xD;&#xA;The title of this question mentions a FASTQ only because FASTQ is a common format for storing unaligned reads on disk.  I am happy with answers that require a single limited transformation of the data to another file format in time complexity order `n`." />
  <row Id="2474" PostHistoryTypeId="1" PostId="807" RevisionGUID="9f647253-d44d-4843-ba39-241abcf6687b" CreationDate="2017-06-19T13:15:22.910" UserId="492" Text="How to perform random access on FASTQ file?" />
  <row Id="2475" PostHistoryTypeId="3" PostId="807" RevisionGUID="9f647253-d44d-4843-ba39-241abcf6687b" CreationDate="2017-06-19T13:15:22.910" UserId="492" Text="&lt;bam&gt;&lt;fastq&gt;&lt;reads&gt;" />
  <row Id="2476" PostHistoryTypeId="2" PostId="808" RevisionGUID="d47c0370-dc18-444b-b662-6c80d9bde2b6" CreationDate="2017-06-19T13:15:22.910" UserId="492" Text="You could shuffle the FASTQ once and then read sequences off the top of the file as you need them:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | shuf | tr '\t' '\n'| gzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;I would recommend [pigz](https://zlib.net/pigz/) as a replacement for gzip in the compression step if you have it available.&#xD;&#xA;&#xD;&#xA;The downside of this approach is that you only get `n` reads before you need to run the shuffle again." />
  <row Id="2480" PostHistoryTypeId="5" PostId="782" RevisionGUID="a2ffa24a-1e9a-4deb-bf08-2126f51e93c7" CreationDate="2017-06-19T13:44:25.840" UserId="123" Comment="Added link to experimental design" Text="I am analysing 142 samples belonging to 6 batches. Additionally, those samples belong to 72 strains, which means that for most of the strains there are two samples.&#xD;&#xA;&#xD;&#xA;I could fit simple models (for strain and batches for instance), but when I get to the &quot;full&quot; model (~batch+strain), I get the following error:&#xD;&#xA;&#xD;&#xA;    so &lt;- sleuth_fit(so, ~strain+batch, 'full')&#xD;&#xA;    Error in solve.default(t(X) %*% X) :&#xD;&#xA;      system is computationally singular: reciprocal condition number = 5.2412e-19&#xD;&#xA;&#xD;&#xA;I should point out that of the 72 strains, only 15 have samples in distinct batches. This means that most strains (57) have both samples in the same batch.&#xD;&#xA;&#xD;&#xA;Is the error due to an unknown bug or rather to the experimental design? Does it mean that the information on batches cannot be used?&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;EDIT I've posted the experimental design in a [gist][1]&#xD;&#xA;&#xD;&#xA;    batch 	strain 	replica&#xD;&#xA;    batch_1 	strain_41 	1&#xD;&#xA;    batch_4 	strain_41 	2&#xD;&#xA;    batch_1 	strain_28 	1&#xD;&#xA;    batch_4 	strain_28 	2&#xD;&#xA;    batch_1 	strain_26 	1&#xD;&#xA;    [...]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://gist.github.com/mgalardini/7553e14ead89a7f020f2b0a610086805" />
  <row Id="2481" PostHistoryTypeId="2" PostId="809" RevisionGUID="4601d195-03e6-47e5-94c5-025c0912e3c1" CreationDate="2017-06-19T13:44:42.180" UserId="822" Text="One possibility is to:&#xD;&#xA;&#xD;&#xA;1. reformat the data such that each record is a single line containing the read description, bases, and quality scores&#xD;&#xA;2. pad out each record to a maximum length in each field such that every record in the file is the same number of bytes&#xD;&#xA;3. the total number of records can now be calculated as file size / record size&#xD;&#xA;4. choose a random record number between 0 and the total number of records&#xD;&#xA;5. binary search over the reformatted file until you obtain your read&#xD;&#xA;&#xD;&#xA;This would get you the log(n) lookup time you want.&#xD;&#xA;&#xD;&#xA;Of course, the data wouldn't be compressed.  You could 2-bit encode the bases and quantize the quals to save some space, but that'd be lossy and is perhaps not what you're looking for.  Alternatively, you could block-gzip the reformatted data and keep a record of how many blocks are in the file and how many reads are in each block (since the filesize will no longer reflect the number of records in the file).  Then to obtain a specific read, you'd calculate the block number it'll appear in, decompress the block, and return the appropriate read." />
  <row Id="2482" PostHistoryTypeId="2" PostId="810" RevisionGUID="13e02028-7eb0-41ba-85c7-60a675074ffe" CreationDate="2017-06-19T13:51:48.503" UserId="77" Text="I suppose I can provide code for this if needed, but keep in mind that it'd probably have to be done in C.&#xD;&#xA;&#xD;&#xA;One could make this easier by using a bgzf compressed fastq file. Yes, BAM uses that already, but since Illumina's software now defaults to producing that there should be a reasonable amount of it already available.&#xD;&#xA;&#xD;&#xA;1. Count the number of blocks in the file.&#xD;&#xA;2. Randomly select one of these blocks.&#xD;&#xA;3. Perform reservoir selection on the entries in that block.&#xD;&#xA;&#xD;&#xA;That won't be strictly O(log N), but it's lower on memory and IO requirements than parsing through the whole file and doing shuffling or otherwise completely munging the data into an otherwise not-terribly-useful format. This also has the benefit of allowing selecting more than one random entry a bit faster, since you can memoize things." />
  <row Id="2483" PostHistoryTypeId="2" PostId="811" RevisionGUID="9ff15538-d376-47ac-8077-0ffa42a9cd68" CreationDate="2017-06-19T13:54:59.920" UserId="298" Text="It depends on what you're looking for. If you're just looking for sequence homology, then you can simply pick the best hits from a blast search. If, however, you are referring to *functional* homology, if you are looking for the protein which has the same functions as your query, then it's more complicated. &#xD;&#xA;&#xD;&#xA;Sequence homology is not enough to infer functional homology. For example, you can have cases of gene duplication and subsequent functional divergence. Such *paralogs* are still *homologs* ([paralogs are a subset of homologs][1]), but they don't necessarily have the same function. It is also often the case that the homolog (be it orthologous or paralogous) of a protein in species B has a completely different function than its homolog in species A despite a high level of sequence similarity. This is usually very hard to determine *in silico*.&#xD;&#xA;&#xD;&#xA;To find the functionally homologous protein(s), you would ideally need to identify the essential residues that allow your protein to perform its function. This could be done using something like [PFam][2] which will identify protein domains. You can then check whether the homologs you find also have this domain. &#xD;&#xA;&#xD;&#xA;This is essentially what PSI-blast does. Although it doesn't take domains into account, each successive iteration is used to build a model of your proptein. The model is built under the reasonable assumption that highly conserved residues are important. So it will consider more diverged sequences as homologous if those residues are conserved. &#xD;&#xA;&#xD;&#xA;If you know how your protein works and what residues are important, you can use that knowledge to refine the results of your PSI-blast. If you don't, you'll have to use only &quot;good&quot; hits to make the model. One way to do this, for well studied proteins, is to only add proteins that are already annotated as homologs of what you are looking for to build your model, then use that model to search in un-annotated species. &#xD;&#xA;&#xD;&#xA;If you don't know, you could try looking for recognizable protein domains in your query protein (use PFam) and then use the HMM (hidden markov model) of the domain to identify important residues. For example, [this is](http://pfam.xfam.org/family/PF01754#tabview=tab4) the HMM logo for the zf-A20 zinc finger domain:&#xD;&#xA;&#xD;&#xA;[![zinc finger sequence logo][3]][3]&#xD;&#xA;&#xD;&#xA;The huge cysteine (C) residues are shown that size because they are very conserved across proteins carrying this domain and, presumably, are functionally important for the domain. So, if you pass your protein through PFam and identify domains, find the important residues and make sure all your homologs have those conserved. If using PSI-blast, only include sequences where those residues are conserved in the results you keep.&#xD;&#xA;&#xD;&#xA;Finally, another useful tool that works in the same way is [HMMER][4]. This takes a protein alignment as input, like PSI-blast builds an HMM model from it and then can use this model to query a protein database for more hits. Methods like HMMER and PSI-blast are far better than simple sequence similarity approaches when looking for homologs.  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://biology.stackexchange.com/a/4964/1306&#xD;&#xA;  [2]: http://pfam.xfam.org/&#xD;&#xA;  [3]: https://i.stack.imgur.com/90nPc.png&#xD;&#xA;  [4]: http://hmmer.org/" />
  <row Id="2484" PostHistoryTypeId="2" PostId="812" RevisionGUID="153e1faa-5392-4175-b111-f2d25e2007a9" CreationDate="2017-06-19T13:57:44.610" UserId="73" Text="Properly parsing FASTQ is a nightmare. It's not possible to start at a random position in the file and guarantee that a seek forward (or backwards) to an '@' symbol will arrive at the start of a FASTQ header. That's a problem if `log(n)` time complexity is desired, because the problem means that it's not possible to read through the entire file to get a sample of sequences.&#xD;&#xA;&#xD;&#xA;If the allowed FASTQ format were more strict, i.e. 4 lines per record: [header, sequence, header, quality], then it *would* be possible to do this: seek to a random location in the file, seek to find the next line that contains only a `+` (preceded by a DNA sequence to exclude `+` in the quality string), seek backwards two lines, and there's the start of a FASTQ record.&#xD;&#xA;&#xD;&#xA;If the file were indexed, then it would be possible to do the shuffling on the index and use that to select reads from the file... but that would also require a complexity of over `n` (because at least indexing is needed).&#xD;&#xA;&#xD;&#xA;`samtools view` has the `-s &lt;float&gt;` option for subsampling aligned reads, but that's in BAM format, rather than FASTQ format (and would still go through the whole file before arriving at the last read).&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2485" PostHistoryTypeId="2" PostId="813" RevisionGUID="bae40f6e-dc4e-426b-822b-cf7e455aab3b" CreationDate="2017-06-19T14:06:33.253" UserId="492" Text="If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&#xD;&#xA;&#xD;&#xA;1. Paste each record into a single line.&#xD;&#xA;2. Add a line number as the first column.&#xD;&#xA;3. Compress with `bgzip`.&#xD;&#xA;4. Tabix the file and perform lookups with `tabix`.&#xD;&#xA;&#xD;&#xA;This is what the code looks like. Note that we need to remove leading spaces introduced by `nl` and we need to introduce a dummy chromosome column to keep `tabix` happy.&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &gt; output.fastq.gz&#xD;&#xA;    tabix -s 1 -b 2 -e 2 output.fastq.gz &#xD;&#xA;    &#xD;&#xA;    # now retrieve the 5th record (1-based) in log(n) time&#xD;&#xA;    tabix output.fastq.gz dummy:5-5&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;" />
  <row Id="2486" PostHistoryTypeId="5" PostId="813" RevisionGUID="a020fba7-f844-4e62-a8d6-b8377522a95f" CreationDate="2017-06-19T14:25:49.373" UserId="492" Comment="Also add an example of how to convert the one-line record into FASTQ format" Text="If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&#xD;&#xA;&#xD;&#xA;1. Paste each record into a single line.&#xD;&#xA;2. Add a line number as the first column.&#xD;&#xA;3. Compress with `bgzip`.&#xD;&#xA;4. Tabix the file and perform lookups with `tabix`.&#xD;&#xA;&#xD;&#xA;This is what the code looks like. Note that we need to remove leading spaces introduced by `nl` and we need to introduce a dummy chromosome column to keep `tabix` happy.&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &gt; output.fastq.gz&#xD;&#xA;    tabix -s 1 -b 2 -e 2 output.fastq.gz &#xD;&#xA;    &#xD;&#xA;    # now retrieve the 5th record (1-based) in log(n) time&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 &#xD;&#xA;&#xD;&#xA;    # This command will retrieve the 5th record and convert it record back into FASTQ format&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;" />
  <row Id="2488" PostHistoryTypeId="5" PostId="813" RevisionGUID="19872a4b-e7c3-4d9b-b0b5-43b3f7991a15" CreationDate="2017-06-19T14:43:53.577" UserId="492" Comment="Add some python that does the actual sampling" Text="# Arbitrary record access in `log(n)` time&#xD;&#xA;&#xD;&#xA;To get a random read in `log(n)` time, it is sufficient to get an arbitrary record in `log(n)` time.&#xD;&#xA; &#xD;&#xA;If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&#xD;&#xA;&#xD;&#xA;1. Paste each record into a single line.&#xD;&#xA;2. Add a line number as the first column.&#xD;&#xA;3. Compress with `bgzip`.&#xD;&#xA;4. Tabix the file and perform lookups with `tabix`.&#xD;&#xA;&#xD;&#xA;This is what the code looks like. Note that we need to remove leading spaces introduced by `nl` and we need to introduce a dummy chromosome column to keep `tabix` happy.&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &gt; output.fastq.gz&#xD;&#xA;    tabix -s 1 -b 2 -e 2 output.fastq.gz &#xD;&#xA;    &#xD;&#xA;    # now retrieve the 5th record (1-based) in log(n) time&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 &#xD;&#xA;&#xD;&#xA;    # This command will retrieve the 5th record and convert it record back into FASTQ format&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&#xD;&#xA;    &#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_RECORDS=$(gzip -dc output.fastq.gz | wc -l) &#xD;&#xA;    &#xD;&#xA;# Random record in `log(n)` time&#xD;&#xA;&#xD;&#xA;Now that we have a way of retrieving an arbitrary record in `log(n)` time, retrieving a random record is simply a matter of getting a good random number generator and sampling. Here is some example code to do this in python:&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_RECORDS&quot;])&#xD;&#xA;    rand_record_index = random.randrange(0, n_records) + 1&#xD;&#xA;    # super ugly, but works for now&#xD;&#xA;    os.system(&#xD;&#xA;        &quot;tabix output.fastq.gz dummy:{0}-{0} | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&quot;.format(&#xD;&#xA;            rand_record_index)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;And this works for me:&#xD;&#xA;&#xD;&#xA;    python3.5 random_read.py &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2489" PostHistoryTypeId="2" PostId="814" RevisionGUID="343951a7-97ca-45c9-8732-e2b80e108517" CreationDate="2017-06-19T15:02:15.580" UserId="425" Text="A quick and dirty solution could be to convert the FASTQ file to two FASTAs, one of the them storing the bases and the other one storing the qualities, and then use standard methods for random access for FASTA.&#xD;&#xA;&#xD;&#xA;The only complication is that base qualities can contain `&gt;`. However, this is a problem only when it is the first character of a line and we can fix it by prepending, e.g., `I` to each quality sequence.&#xD;&#xA;&#xD;&#xA;If also other people think that this could be a sufficient solution, I will expand the answer and add all the commands." />
  <row Id="2490" PostHistoryTypeId="2" PostId="815" RevisionGUID="a62e0972-1149-4d54-857f-b168ef7a7f2c" CreationDate="2017-06-19T15:25:03.003" UserId="302" Text="How to extract the sequence used to create a blast database.  This is useful when you download a blastdb from somewhere else e.g. one of the [databases provided by NCBI][1] including the 16SMicrobial database. Or alternatively, when you want to double check which version of a sequence you have included in a blastdb.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://ftp.ncbi.nlm.nih.gov/blast/db" />
  <row Id="2491" PostHistoryTypeId="1" PostId="815" RevisionGUID="a62e0972-1149-4d54-857f-b168ef7a7f2c" CreationDate="2017-06-19T15:25:03.003" UserId="302" Text="How to extract fasta from a blastdb" />
  <row Id="2492" PostHistoryTypeId="3" PostId="815" RevisionGUID="a62e0972-1149-4d54-857f-b168ef7a7f2c" CreationDate="2017-06-19T15:25:03.003" UserId="302" Text="&lt;fasta&gt;&lt;blast&gt;" />
  <row Id="2493" PostHistoryTypeId="2" PostId="816" RevisionGUID="dafe4946-cb99-449d-81f2-a97fb7a4a3cc" CreationDate="2017-06-19T15:25:03.003" UserId="302" Text="You can extract fasta sequence from a blastdb constructed from a fasta file using blastdbcmd which should be installed when you install blast/makeblastdb.&#xD;&#xA;&#xD;&#xA;    blastdbcmd -entry all -db &lt;database label&gt; -out &lt;outfile&gt;&#xD;&#xA;&#xD;&#xA;If you had a database called ``my_database`` which contained the files:&#xD;&#xA;&#xD;&#xA; - ``my_database.nhr`` &#xD;&#xA; - ``my_database.nsq`` &#xD;&#xA; - ``my_database.nin``&#xD;&#xA;&#xD;&#xA;and you wanted your fasta output file to be called ``reference.fasta`` you would run the following:&#xD;&#xA;&#xD;&#xA;    blastdbcmd -entry all -db my_database -out reference.fasta&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2494" PostHistoryTypeId="5" PostId="802" RevisionGUID="2126c97b-b305-47de-a2b5-e8a2aecd1350" CreationDate="2017-06-19T15:27:06.740" UserId="302" Comment="added 68 characters in body" Text="I'm looking for the exact invocation used to generate the 16SMicrobial database that you can download from here:&#xD;&#xA;&#xD;&#xA;https://ftp.ncbi.nlm.nih.gov/blast/db/&#xD;&#xA;&#xD;&#xA;I'm hoping to create the same type of blastdb with the same properties with custom sequences.&#xD;&#xA;&#xD;&#xA;Platform isn't an issue, but let's say on ubuntu 14.04 or 16.04." />
  <row Id="2495" PostHistoryTypeId="2" PostId="817" RevisionGUID="c1cec2c1-d4da-444f-a810-ce6e01260058" CreationDate="2017-06-19T15:37:17.460" UserId="842" Text="I used blastn to search one a genome database for the sequences in a [file][1]. &#xD;&#xA;&#xD;&#xA;    blastn -out output.txt -outfmt 6 -query sequence_list -db genome_database -perc_identity 100&#xD;&#xA;&#xD;&#xA;These sequences are all from one contig of the genome so the output was all hits in one contig. I then **appended** more sequences to the sequence file and used blastn again. However the weird thing is the output did not start with the same hits as the last blast. I expected if blastn iterated through sequences in a query file that it would output the original sequences first then go through the appended sequences. Since this was not the case, how does blast order its output?&#xD;&#xA;&#xD;&#xA;I also checked if it was sorted by which contig the hit was in and this is not the case. &#xD;&#xA;&#xD;&#xA;  [1]: http://ix.io/xFG" />
  <row Id="2496" PostHistoryTypeId="1" PostId="817" RevisionGUID="c1cec2c1-d4da-444f-a810-ce6e01260058" CreationDate="2017-06-19T15:37:17.460" UserId="842" Text="How does blastn sort output?" />
  <row Id="2497" PostHistoryTypeId="3" PostId="817" RevisionGUID="c1cec2c1-d4da-444f-a810-ce6e01260058" CreationDate="2017-06-19T15:37:17.460" UserId="842" Text="&lt;alignment&gt;&lt;blast&gt;&lt;sequence-analysis&gt;" />
  <row Id="2498" PostHistoryTypeId="5" PostId="807" RevisionGUID="f48b88cd-fae4-4969-aef2-ddd0773a3e77" CreationDate="2017-06-19T15:40:26.963" UserId="298" Comment="How to is a declaration not a question" Text="I would like to select a random record (that is, description, sequence, and quality scores) from a large set of `n` unaligned sequencing reads in `log(n)` time complexity or better. The records do not fit in RAM and would need to be stored on disk. Ideally, I would like to store the reads in a compressed format.&#xD;&#xA;&#xD;&#xA;I imagine that I could align the reads to a reference genome and then use the BAM format to do my lookups, but I would prefer a solution that does not require a reference genome.&#xD;&#xA;&#xD;&#xA;The title of this question mentions a FASTQ only because FASTQ is a common format for storing unaligned reads on disk. I am happy with answers that require a single limited transformation of the data to another file format in time complexity order `n`." />
  <row Id="2499" PostHistoryTypeId="4" PostId="807" RevisionGUID="f48b88cd-fae4-4969-aef2-ddd0773a3e77" CreationDate="2017-06-19T15:40:26.963" UserId="298" Comment="How to is a declaration not a question" Text="How can I perform random access on FASTQ file?" />
  <row Id="2500" PostHistoryTypeId="24" PostId="807" RevisionGUID="f48b88cd-fae4-4969-aef2-ddd0773a3e77" CreationDate="2017-06-19T15:40:26.963" Comment="Proposed by 298 approved by 492 edit id of 207" />
  <row Id="2501" PostHistoryTypeId="5" PostId="802" RevisionGUID="fbd6fece-4a9a-4558-92ce-8f192a5c3d38" CreationDate="2017-06-19T16:03:14.583" UserId="302" Comment="added 496 characters in body" Text="I'm looking for the exact invocation used to generate the 16SMicrobial database that you can download from here:&#xD;&#xA;&#xD;&#xA;https://ftp.ncbi.nlm.nih.gov/blast/db/&#xD;&#xA;&#xD;&#xA;I'm hoping to create the same type of blastdb with the same properties with custom sequences.&#xD;&#xA;&#xD;&#xA;Platform isn't an issue, but let's say on ubuntu 14.04 or 16.04.&#xD;&#xA;&#xD;&#xA;I would like to replicate the creation of the database as closely as possible. &#xD;&#xA; The most important feature is the taxonomic information as can be seen [here][1]:&#xD;&#xA;&#xD;&#xA;&gt; The databases on the FTP site contain taxonomic information for each&#xD;&#xA;&gt; sequence, include the identifier indices for lookups, and can be up to&#xD;&#xA;&gt; four times smaller than the FASTA. The original FASTA can be generated&#xD;&#xA;&gt; from the BLAST database using blastdbcmd&#xD;&#xA;&#xD;&#xA;Creation of a blastdb using makeblastdb from a set of a fasta sequences is not an issue and can be achieved via:&#xD;&#xA;&#xD;&#xA;    makeblastdb -in &lt;your_file.fasta&gt; -dbtype nucl -out &lt;database_name&gt;&#xD;&#xA;&#xD;&#xA;My question is specifically about the invocation NCBI uses to add the metadata that is present in the NCBI's 16SMicrobial blast database as I am keen to make sure I have replicated the process as closely as possible.&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/books/NBK279688/&#xD;&#xA;" />
  <row Id="2502" PostHistoryTypeId="5" PostId="802" RevisionGUID="d8ed92c1-9ffc-4393-b254-bcc6d37a4705" CreationDate="2017-06-19T16:11:40.913" UserId="302" Comment="added 6 characters in body" Text="I'm looking for the exact invocation used to generate the 16SMicrobial database that you can download from here:&#xD;&#xA;&#xD;&#xA;https://ftp.ncbi.nlm.nih.gov/blast/db/&#xD;&#xA;&#xD;&#xA;I'm hoping to create the same type of blastdb with the same type of metadata with custom sequences.&#xD;&#xA;&#xD;&#xA;Platform isn't an issue, but let's say on ubuntu 14.04 or 16.04.&#xD;&#xA;&#xD;&#xA;I would like to replicate the creation of the database as closely as possible. &#xD;&#xA; The most important feature is the taxonomic information as can be seen [here][1]:&#xD;&#xA;&#xD;&#xA;&gt; The databases on the FTP site contain taxonomic information for each&#xD;&#xA;&gt; sequence, include the identifier indices for lookups, and can be up to&#xD;&#xA;&gt; four times smaller than the FASTA. The original FASTA can be generated&#xD;&#xA;&gt; from the BLAST database using blastdbcmd&#xD;&#xA;&#xD;&#xA;Creation of a blastdb using makeblastdb from a set of a fasta sequences is not an issue and can be achieved via:&#xD;&#xA;&#xD;&#xA;    makeblastdb -in &lt;your_file.fasta&gt; -dbtype nucl -out &lt;database_name&gt;&#xD;&#xA;&#xD;&#xA;My question is specifically about the invocation NCBI uses to add the metadata that is present in the NCBI's 16SMicrobial blast database as I am keen to make sure I have replicated the process as closely as possible.&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/books/NBK279688/&#xD;&#xA;" />
  <row Id="2505" PostHistoryTypeId="5" PostId="813" RevisionGUID="d712d029-7fc9-4f6a-85f9-d907a90963a1" CreationDate="2017-06-19T17:08:04.937" UserId="492" Comment="Add grabix as an alternative solution" Text="# Arbitrary record access in `log(n)` time&#xD;&#xA;&#xD;&#xA;To get a random record in `log(n)` time, it is sufficient to get an arbitrary record in `log(n)` time.&#xD;&#xA; &#xD;&#xA;If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&#xD;&#xA;&#xD;&#xA;1. Paste each record into a single line.&#xD;&#xA;2. Add a line number as the first column.&#xD;&#xA;3. Compress with `bgzip`.&#xD;&#xA;4. Index the file and perform lookups with [`tabix`](http://www.htslib.org/doc/tabix.html) or [`grabix`](https://github.com/arq5x/grabix)&#xD;&#xA;&#xD;&#xA;I have two solutions here: One with `tabix` and one with `grabix`. I think the `grabix` solution is more elegant, but I am keeping the `tabix` solution below because `tabix` is a more mature tool than `grabix`.&#xD;&#xA;&#xD;&#xA;Thanks to [user172818](https://bioinformatics.stackexchange.com/users/37/user172818) for suggesting `grabix`.&#xD;&#xA;&#xD;&#xA;## Using `grabix`&#xD;&#xA;&#xD;&#xA;    grabix index input.fastq.gz&#xD;&#xA;&#xD;&#xA;    # retrieve 5-th record (1-based) in `log(n)` time (or better)&#xD;&#xA;    # requires some math to convert indices (4*4 + 1, 4*4 + 4) = (17, 20)&#xD;&#xA;    grabix grab input.fastq.gz 17 20&#xD;&#xA;&#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_LINES=$(gzip -dc input.fastq.gz | wc -l)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;## Using `tabix`&#xD;&#xA;&#xD;&#xA;The tabix code is more complicated and relies on the iffy assumption that `\t` is an acceptable character for replacement of `\n` in a FASTQ record.  Note that we need to remove leading spaces introduced by `nl` and we need to introduce a dummy chromosome column to keep `tabix` happy.&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &gt; output.fastq.gz&#xD;&#xA;    tabix -s 1 -b 2 -e 2 output.fastq.gz &#xD;&#xA;    &#xD;&#xA;    # now retrieve the 5th record (1-based) in log(n) time&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 &#xD;&#xA;&#xD;&#xA;    # This command will retrieve the 5th record and convert it record back into FASTQ format&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&#xD;&#xA;    &#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_RECORDS=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;    &#xD;&#xA;# Random record in `log(n)` time&#xD;&#xA;&#xD;&#xA;Now that we have a way of retrieving an arbitrary record in `log(n)` time, retrieving a random record is simply a matter of getting a good random number generator and sampling. Here is some example code to do this in python:&#xD;&#xA;&#xD;&#xA;## Using grabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_LINES&quot;]) // 4&#xD;&#xA;    rand_record_start = random.randrange(0, n_records) * 4 + 1&#xD;&#xA;    rand_record_end = rand_record_start + 3&#xD;&#xA;    # super ugly, but works for now&#xD;&#xA;    os.system(&#xD;&#xA;        &quot;grabix grab input.fastq.gz {0} {1}&quot;.format(rand_record_start, rand_record_end)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;## Using tabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_RECORDS&quot;])&#xD;&#xA;    rand_record_index = random.randrange(0, n_records) + 1&#xD;&#xA;    # super ugly, but works for now&#xD;&#xA;    os.system(&#xD;&#xA;        &quot;tabix output.fastq.gz dummy:{0}-{0} | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&quot;.format(&#xD;&#xA;            rand_record_index)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;And this works for me:&#xD;&#xA;&#xD;&#xA;    python3.5 random_read.py&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2506" PostHistoryTypeId="4" PostId="807" RevisionGUID="1f106204-21fa-4c35-b583-cdd7790b7edb" CreationDate="2017-06-19T17:09:03.640" UserId="492" Comment="edited title" Text="How can I perform random access on a FASTQ file?" />
  <row Id="2507" PostHistoryTypeId="5" PostId="808" RevisionGUID="a7e0d960-ef03-4257-b1dd-7cf880bbeeb9" CreationDate="2017-06-19T17:15:46.497" UserId="492" Comment="Incorporate comment" Text="You could shuffle the FASTQ once and then read sequences off the top of the file as you need them:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | shuf | tr '\t' '\n'| gzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;I would recommend [pigz](https://zlib.net/pigz/) as a replacement for gzip in the compression step if you have it available.&#xD;&#xA;&#xD;&#xA;The downside of this approach is that you only get `n` reads before you need to run the shuffle again. Furthermore, the `shuf` command should be `n*log(n)` complexity, which also does not conform to the requirements of the question." />
  <row Id="2508" PostHistoryTypeId="5" PostId="807" RevisionGUID="a12d2c05-091f-428f-b170-366450eb48cb" CreationDate="2017-06-19T17:22:13.887" UserId="492" Comment="Clarify question. Remove confusing reference to BAM format." Text="I would like to select a random record (that is, description, sequence, and quality scores) from a large set of `n` unaligned sequencing reads in `log(n)` time complexity or better. The records do not fit in RAM and would need to be stored on disk. Ideally, I would like to store the reads in a compressed format.&#xD;&#xA;&#xD;&#xA;I would prefer a solution that does not require any extra files such as for example a reference genome.&#xD;&#xA;&#xD;&#xA;The title of this question mentions a FASTQ only because FASTQ is a common format for storing unaligned reads on disk. I am happy with answers that require a single limited transformation of the data to another file format in time complexity order `n`." />
  <row Id="2509" PostHistoryTypeId="2" PostId="818" RevisionGUID="59fd6a6b-852d-461e-a54e-a4bf2f4894ff" CreationDate="2017-06-19T17:41:36.857" UserId="818" Text="I had a protein Refseq ID and I PSI-BLASTed this sequence against Refseq database. We all know that the Refseq is a Reference sequence database and it shouldn't have redundancy. After BLASTing my sequence, at first iteration I got 1000 hits that among them there were a lot of redundant sequences! My seqence had 241 amino acids and I found a lot of sequences with 100% identity, 100% cover and 0 E-value exactly the same as my sequence but with different IDs. All of these IDs were all from Refseq! In other iterations and after adjusting format options, I got this redundancy with other sequences from other species. I want to know what is wrong with Refseq? Is it really a Reference sequence database? If it is, where comes this redundancy and why?" />
  <row Id="2510" PostHistoryTypeId="1" PostId="818" RevisionGUID="59fd6a6b-852d-461e-a54e-a4bf2f4894ff" CreationDate="2017-06-19T17:41:36.857" UserId="818" Text="Is there anything wrong with Refseq?" />
  <row Id="2511" PostHistoryTypeId="3" PostId="818" RevisionGUID="59fd6a6b-852d-461e-a54e-a4bf2f4894ff" CreationDate="2017-06-19T17:41:36.857" UserId="818" Text="&lt;sequence-homology&gt;" />
  <row Id="2512" PostHistoryTypeId="5" PostId="813" RevisionGUID="3f4759b4-10c2-4f62-9695-ff671204d4fd" CreationDate="2017-06-19T17:59:33.837" UserId="492" Comment="log(n) is the theoretical limit of grabix AFAIK" Text="# Arbitrary record access in `log(n)` time&#xD;&#xA;&#xD;&#xA;To get a random record in `log(n)` time, it is sufficient to get an arbitrary record in `log(n)` time.&#xD;&#xA;&#xD;&#xA;I have two solutions here: One with `tabix` and one with `grabix`. I think the `grabix` solution is more elegant, but I am keeping the `tabix` solution below because `tabix` is a more mature tool than `grabix`.&#xD;&#xA; &#xD;&#xA;Thanks to [user172818](https://bioinformatics.stackexchange.com/users/37/user172818) for suggesting `grabix`.&#xD;&#xA;&#xD;&#xA;## Using `grabix`&#xD;&#xA;&#xD;&#xA;1. Compress with `bgzip`.&#xD;&#xA;2. Index the file and perform lookups with [`grabix`](https://github.com/arq5x/grabix)&#xD;&#xA;&#xD;&#xA;In bash:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | bgzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;    grabix index output.fastq.gz&#xD;&#xA;&#xD;&#xA;    # retrieve 5-th record (1-based) in log(n) time&#xD;&#xA;    # requires some math to convert indices (4*4 + 1, 4*4 + 4) = (17, 20)&#xD;&#xA;    grabix grab output.fastq.gz 17 20&#xD;&#xA;&#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_LINES=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;&#xD;&#xA;## Using `tabix`&#xD;&#xA;&#xD;&#xA;The tabix code is more complicated and relies on the iffy assumption that `\t` is an acceptable character for replacement of `\n` in a FASTQ record.  If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&#xD;&#xA;&#xD;&#xA;1. Paste each record into a single line.&#xD;&#xA;2. Add a dummy chromosome and line number as the first and second column.&#xD;&#xA;3. Compress with `bgzip`.&#xD;&#xA;4. Index the file and perform lookups with [`tabix`](http://www.htslib.org/doc/tabix.html)&#xD;&#xA;&#xD;&#xA;Note that we need to remove leading spaces introduced by `nl` and we need to introduce a dummy chromosome column to keep `tabix` happy:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &gt; output.fastq.gz&#xD;&#xA;    tabix -s 1 -b 2 -e 2 output.fastq.gz &#xD;&#xA;    &#xD;&#xA;    # now retrieve the 5th record (1-based) in log(n) time&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 &#xD;&#xA;&#xD;&#xA;    # This command will retrieve the 5th record and convert it record back into FASTQ format&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&#xD;&#xA;    &#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_RECORDS=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;    &#xD;&#xA;# Random record in `log(n)` time&#xD;&#xA;&#xD;&#xA;Now that we have a way of retrieving an arbitrary record in `log(n)` time, retrieving a random record is simply a matter of getting a good random number generator and sampling. Here is some example code to do this in python:&#xD;&#xA;&#xD;&#xA;## Using grabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_LINES&quot;]) // 4&#xD;&#xA;    rand_record_start = random.randrange(0, n_records) * 4 + 1&#xD;&#xA;    rand_record_end = rand_record_start + 3&#xD;&#xA;    os.system(&quot;grabix grab output.fastq.gz {0} {1}&quot;.format(rand_record_start, rand_record_end))&#xD;&#xA;&#xD;&#xA;## Using tabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_RECORDS&quot;])&#xD;&#xA;    rand_record_index = random.randrange(0, n_records) + 1&#xD;&#xA;    # super ugly, but works...&#xD;&#xA;    os.system(&#xD;&#xA;        &quot;tabix output.fastq.gz dummy:{0}-{0} | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&quot;.format(&#xD;&#xA;            rand_record_index)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;And this works for me:&#xD;&#xA;&#xD;&#xA;    python3.5 random_read.py&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2513" PostHistoryTypeId="4" PostId="818" RevisionGUID="6b8c905b-c3f5-49b9-ac6e-62a0f70ed357" CreationDate="2017-06-19T18:55:48.107" UserId="73" Comment="updated title to match question" Text="Duplicate long hits from PSI-BLAST" />
  <row Id="2514" PostHistoryTypeId="6" PostId="807" RevisionGUID="7131aeb9-d800-4a93-b86e-07d798d9715c" CreationDate="2017-06-19T18:57:38.533" UserId="73" Comment="added &quot;benchmarking&quot; tag because this is a &quot;which is fastest&quot; question" Text="&lt;bam&gt;&lt;fastq&gt;&lt;reads&gt;&lt;benchmarking&gt;" />
  <row Id="2515" PostHistoryTypeId="2" PostId="819" RevisionGUID="820ed833-3673-4b9f-a63a-a7df94250a8f" CreationDate="2017-06-19T19:55:37.043" UserId="77" Text="You should be able to remove any one of the following strains to end up with a rank-sufficient model matrix: 5, 10, 12, 13, 14, 15, 19, 26, 28, 3, 30, 32, 36, 39, 41, 45, 46, 49, 5, 50, 52, 53, 58, 59, 60, 69, 8. As an aside you can figure this sort of thing out as follows (I read your dataframe in a the `d` object):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: R --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; m = model.matrix(~batch+strain, d)&#xD;&#xA;    &gt; dim(m) # 142 row, 77 columns, so minimum rank is 77&#xD;&#xA;    &gt; qr(m)$rank # 76, so just barely rank insufficient&#xD;&#xA;    &gt; #see if we can remove a single column and still get rank 76&#xD;&#xA;    &gt; colnames(m)[which(sapply(1:77, function(x) qr(m[,-x])$rank) == 76)]&#xD;&#xA;&#xD;&#xA;You obviously don't want to remove the batch columns or the intercept. The normal tricks that you can sometimes use to get around this issue with case-control studies don't appear to help here, which is why I would just drop a strain and call it done. Keep in mind that your power is still likely terrible. I generally recommend at least 6 replicates per group (scale down the number of groups to fit your budget)." />
  <row Id="2516" PostHistoryTypeId="2" PostId="820" RevisionGUID="e6cc4d1e-7ded-409a-85cd-f6c6ee8535f8" CreationDate="2017-06-19T20:19:14.163" UserId="776" Text="I wrote [a tool called `sample`][1] that you can use to do random sampling without reading the entire file into memory. &#xD;&#xA;&#xD;&#xA;It can be used where GNU `shuf` fails for lack of sufficient memory. &#xD;&#xA;&#xD;&#xA;It requires two passes through the file to do a random sample, but the second pass is generally fast(er) as it uses `mmap` routines to do cached reads. &#xD;&#xA;&#xD;&#xA;If you do repeated samples, the repeated samples are also `mmap`-ed (cached) and will run quickly.&#xD;&#xA;&#xD;&#xA;You might use it on a FASTQ file like so:&#xD;&#xA;&#xD;&#xA;    $ sample -k 1234 -l 4 in.fq &gt; out.fq&#xD;&#xA;&#xD;&#xA;It parses the input file into records by every four newline characters (such as the format of a FASTQ file), reading line offset positions into memory. So the memory overhead is relatively very low.&#xD;&#xA;&#xD;&#xA;It then applies reservoir sampling on those line offsets to write out a random sample (say, `1234` records in this example) to standard output.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/alexpreynolds/sample" />
  <row Id="2517" PostHistoryTypeId="5" PostId="807" RevisionGUID="495f67a8-847b-463b-a619-e557c864d82c" CreationDate="2017-06-19T20:38:23.137" UserId="492" Comment="Specify big O notation + define &quot;record&quot;" Text="I would like to select a random record from a large set of `n` unaligned sequencing reads in `log(n)` time complexity ([big O notation](https://en.wikipedia.org/wiki/Big_O_notation)) or less. A record is defined as the equivalent of four lines in FASTQ format. The records do not fit in RAM and would need to be stored on disk. Ideally, I would like to store the reads in a compressed format.&#xD;&#xA;&#xD;&#xA;I would prefer a solution that does not require any extra files such as for example a reference genome.&#xD;&#xA;&#xD;&#xA;The title of this question mentions a FASTQ only because FASTQ is a common format for storing unaligned reads on disk. I am happy with answers that require a single limited transformation of the data to another file format in time complexity order `n`." />
  <row Id="2518" PostHistoryTypeId="2" PostId="821" RevisionGUID="27f216a4-b67b-4bbb-bcf4-4287c639b529" CreationDate="2017-06-19T20:49:17.313" UserId="146" Text="My understanding is that InDels are from 1bp to 10Kb, and a healthy genome has ~400K-500K Indels. Surely most of these are small. &#xD;&#xA;&#xD;&#xA;What is the distribution of insertion sizes in a healthy human genome? What is the distribution of deletion sizes? What is an average ratio of insertions:deletions? &#xD;&#xA;&#xD;&#xA;Apologies if this is common knowledge---I have not found a definitive reference. " />
  <row Id="2519" PostHistoryTypeId="1" PostId="821" RevisionGUID="27f216a4-b67b-4bbb-bcf4-4287c639b529" CreationDate="2017-06-19T20:49:17.313" UserId="146" Text="What is the distribution of InDel sizes in a healthy human genome? Insertion:deletion sizes?" />
  <row Id="2520" PostHistoryTypeId="3" PostId="821" RevisionGUID="27f216a4-b67b-4bbb-bcf4-4287c639b529" CreationDate="2017-06-19T20:49:17.313" UserId="146" Text="&lt;snp&gt;&lt;indel&gt;" />
  <row Id="2521" PostHistoryTypeId="5" PostId="808" RevisionGUID="d4b3bcdd-ece6-4b7b-bbe4-a997f7e3f04e" CreationDate="2017-06-19T21:16:32.037" UserId="492" Comment="Clarify language" Text="You could shuffle the FASTQ once and then read sequences off the top of the file as you need them:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | shuf | tr '\t' '\n'| gzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;I would recommend [pigz](https://zlib.net/pigz/) as a replacement for gzip in the compression step if you have it available.&#xD;&#xA;&#xD;&#xA;The downside of this approach is that you only get `n` reads before you need to run the shuffle again, and [apparently](https://stackoverflow.com/a/24492814/528691) `shuf` holds all data in RAM, so it would die with an out of memory error if the FASTQ file does not fit into RAM as is specified in the question.&#xD;&#xA;&#xD;&#xA;Using `sort -R` is complexity `n log(n)` and uses temporary files, but it groups identical records together so it would only work as a replacement for shuf if all records in the FASTQ are unique." />
  <row Id="2522" PostHistoryTypeId="2" PostId="822" RevisionGUID="9bcb3443-4197-44b4-aa7d-85b6ddfb243c" CreationDate="2017-06-19T21:30:20.647" UserId="776" Text="Here's another approach that doesn't require any indexing, using [BEDOPS `bedextract`][1] to do a `log(n)` sample on a sorted BED file.&#xD;&#xA;&#xD;&#xA;It does require a single `O(n)` pass through the file to transform it to a BED file:&#xD;&#xA;&#xD;&#xA;    $ cat records.fastq | paste - - - - | awk '{ print &quot;chrZ\t&quot;s&quot;\t&quot;(s+1)&quot;$0 }' &gt; records.bed&#xD;&#xA;&#xD;&#xA;Store the intervals in a separate file:&#xD;&#xA;&#xD;&#xA;    $ cut -f1-3 records.bed &gt; intervals.bed&#xD;&#xA;&#xD;&#xA;To do a random sample of `k` elements, shuffle the intervals file and preserve the order of shuffled elements. &#xD;&#xA;&#xD;&#xA;You can do this with the [`sample` tool][2] I outlined earlier:&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed &gt; intervals-sample.bed&#xD;&#xA;&#xD;&#xA;There's an `O(nlog(n))` cost here, but if `k` is small, this can be amortized over the `log(n)` search benefit below.&#xD;&#xA;&#xD;&#xA;Next, use `bedextract` to do a binary search on the records, and delinearize to get back to FASTQ:&#xD;&#xA;&#xD;&#xA;    $ bedextract records.bed intervals-sample.bed | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;With Unix I/O streams, this can be done in one pass:&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed | bedextract records.bed - | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;By building a sorted file in `records.bed`, you're guaranteed the ability to do a binary search, which is `log(n)`.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/content/reference/set-operations/bedextract.html&#xD;&#xA;  [2]: https://github.com/alexpreynolds/sample" />
  <row Id="2523" PostHistoryTypeId="2" PostId="823" RevisionGUID="80008bbf-890d-4403-86ff-39f9d08ecf39" CreationDate="2017-06-19T21:43:05.247" UserId="77" Text="One of the [2015 papers from the 1000 genomes project](https://www.nature.com/nature/journal/v526/n7571/full/nature15394.html) has a nice figure (figure 1) showing the size distribution of medium to large sized insertions and deletions:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;From [another 2015 1000 genomes paper](https://www.nature.com/nature/journal/v526/n7571/full/nature15393.html), one can see that the absolute number of smaller indels is much larger, though an exact size range isn't given (as far as I saw). If you really want to know that, just download [the most recent 1000 genomes VCF file(s)](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/) and compute the exact median size and/or distribution that you want.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/bFMCX.jpg" />
  <row Id="2524" PostHistoryTypeId="5" PostId="822" RevisionGUID="0508681d-9d59-49ea-a525-571b9dd75c72" CreationDate="2017-06-19T21:55:11.080" UserId="776" Comment="added 130 characters in body" Text="Here's another approach that doesn't require any indexing, using [BEDOPS `bedextract`][1] to do a `log(n)` sample on a sorted BED file.&#xD;&#xA;&#xD;&#xA;It does require a single `O(n)` pass through the file to transform it to a BED file:&#xD;&#xA;&#xD;&#xA;    $ cat records.fastq | paste - - - - | awk '{ print &quot;chrZ\t&quot;s&quot;\t&quot;(s+1)&quot;$0 }' &gt; records.bed&#xD;&#xA;&#xD;&#xA;Store the intervals in a separate file:&#xD;&#xA;&#xD;&#xA;    $ cut -f1-3 records.bed &gt; intervals.bed&#xD;&#xA;&#xD;&#xA;To do a random sample of `k` elements, shuffle the intervals file and preserve the order of shuffled elements. &#xD;&#xA;&#xD;&#xA;You can do this with the [`sample` tool][2] I outlined earlier:&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed &gt; intervals-sample.bed&#xD;&#xA;&#xD;&#xA;Or you can `shuf` and `sort-bed` to do the same thing:&#xD;&#xA;&#xD;&#xA;    $ shuf -n ${K} intervals.bed | sort-bed - &gt; intervals-sample.bed&#xD;&#xA;&#xD;&#xA;There's an `O(klog(k))` cost here, but if `k` is small, this can be amortized over the `log(n)` search benefit below.&#xD;&#xA;&#xD;&#xA;Next, use `bedextract` to do a binary search on the records, and delinearize to get back to FASTQ:&#xD;&#xA;&#xD;&#xA;    $ bedextract records.bed intervals-sample.bed | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;With Unix I/O streams, this can be done in one pass:&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed | bedextract records.bed - | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;By baking the sort order into `records.bed`, you're guaranteed the ability to do a binary search, which is `log(n)`.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/content/reference/set-operations/bedextract.html&#xD;&#xA;  [2]: https://github.com/alexpreynolds/sample" />
  <row Id="2525" PostHistoryTypeId="2" PostId="824" RevisionGUID="0a6b2fe6-ca85-483b-999a-51282788385f" CreationDate="2017-06-20T00:43:57.250" UserId="267" Text="I wrote a tool called [strandex](https://github.com/mdshw5/strandex) that matches entire FASTQ records starting at a random offset inside a decompressed file (since non-block compressed gzip files are not seek-able). It was a response to [this comment](https://github.com/dib-lab/khmer/issues/1002#issuecomment-102649433) about the `screed` FASTQ parser in [khmer](https://github.com/dib-lab/khmer), and was initially just a proof of concept but it's been useful for me and a few other people. The general idea is to:&#xD;&#xA;&#xD;&#xA;1. seek to a random offset in the file&#xD;&#xA;2. read a chunk of the file&#xD;&#xA;3. test if the regex pattern `@.+[\n\r]+.+[\n\r]+\+.*?[\n\r].+[\n\r]` matches&#xD;&#xA;4. if so, extract the matching FASTQ record using regex capture groups&#xD;&#xA;5. if no match, goto step 2&#xD;&#xA;&#xD;&#xA;The CLI script and python library can be installed with `pip install strandex`, and by default the script samples `-n` reads using file offsets starting at a random number (reproducable by setting `-s` seed), then moves a number of bytes to uniformly sample the entire file, determined by the file size. In this way the process is not truly random, but random enough, and avoids sampling any part of the file too much. &#xD;&#xA;&#xD;&#xA;If you specify `-n` reads less than the total number in the file you get *down*sampling, and if `-n` is greater than the total number of reads you get *up*sampling. &#xD;&#xA;&#xD;&#xA;The advantage of this method over others is - it's a one-pass method, and since it's using the python C internals regex engine it's quite fast." />
  <row Id="2526" PostHistoryTypeId="2" PostId="825" RevisionGUID="6e9af286-49a9-4ac5-9b33-499b43192613" CreationDate="2017-06-20T01:49:07.367" UserId="163" Text="[IGB][1] gives the intended result without much hassle. It doesn't have the bells and whistles of IGV (including Sashimi plots, which may be useful but work is needed to reduce the noise of poorly supported variants.)&#xD;&#xA;&#xD;&#xA;In IGB, each read is presented as a single, contiguous line. &#xD;&#xA;&#xD;&#xA;[![IGB zoomed out view][2]][2]&#xD;&#xA;&#xD;&#xA;If you zoom in you can see the read ID for each read.&#xD;&#xA;&#xD;&#xA;[![IGB zoomed in view][3]][3]&#xD;&#xA;&#xD;&#xA;Ideally, a modified Sashimi plot could be generated which excludes any junctions supported by only one read. In the meantime, this is good for anecdotal analysis.&#xD;&#xA;&#xD;&#xA;  [1]: http://bioviz.org/igb/&#xD;&#xA;  [2]: https://i.stack.imgur.com/n80lR.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/ntXKs.png" />
  <row Id="2527" PostHistoryTypeId="5" PostId="825" RevisionGUID="c8caa5a9-08f2-408c-8082-8b4b764abfe2" CreationDate="2017-06-20T02:12:39.600" UserId="163" Comment="add modified sashimi plot." Text="[IGB][1] gives the intended result without much hassle. It doesn't have the bells and whistles of IGV, but presents a clean and intuitive view of the individual reads.&#xD;&#xA;&#xD;&#xA;In IGB, each read is presented as a single, contiguous line. &#xD;&#xA;&#xD;&#xA;[![IGB zoomed out view][2]][2]&#xD;&#xA;&#xD;&#xA;If you zoom in you can see the read ID for each read.&#xD;&#xA;&#xD;&#xA;[![IGB zoomed in view][3]][3]&#xD;&#xA;&#xD;&#xA;For a more holistic view, IGV's Sashimi plot can be modified to exclude spurious junctions supported by few reads. Here is a section of the above gene with default settings:&#xD;&#xA;&#xD;&#xA;[![Sashimi plot with default settings][4]][4]&#xD;&#xA;&#xD;&#xA;The minimum junction coverage can be set by right-clicking on the Sashimi plot itself, or application-wide under View -&gt; Preferences -&gt; Alignments -&gt; Splice Junction Track Options. With minimum junction coverage 3, the noisy plot above now looks like this:&#xD;&#xA;&#xD;&#xA;[![Sashimi plot with min junction coverage 3][5]][5]&#xD;&#xA;&#xD;&#xA;The numbers are still hard to read due to numerous overlapping almost-identical junctions, but at least the plot is now readable.&#xD;&#xA;&#xD;&#xA;  [1]: http://bioviz.org/igb/&#xD;&#xA;  [2]: https://i.stack.imgur.com/n80lR.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/ntXKs.png&#xD;&#xA;  [4]: https://i.stack.imgur.com/uuaLY.png&#xD;&#xA;  [5]: https://i.stack.imgur.com/A4GTP.png" />
  <row Id="2528" PostHistoryTypeId="5" PostId="722" RevisionGUID="534a2b6b-a9d6-4e98-bfdd-9660331c73ce" CreationDate="2017-06-20T02:13:57.857" UserId="163" Comment="remove erroneous understanding of variants in sashimi plot" Text="I have a dataset of Oxford Nanopore cDNA reads. Many of my reads are full-length or close to full-length transcripts, and I and am interested in examining alternative splicing. For this, I would like to begin by visualising my reads and comparing variants qualitatively.&#xD;&#xA;&#xD;&#xA;I have tried visualising my data in both IGV and SeqMonk, but neither has given a satisfying result.&#xD;&#xA;&#xD;&#xA;**IGV**&#xD;&#xA;&#xD;&#xA;[![IGV Visualisation of NOTCH2][1]][1]&#xD;&#xA;&#xD;&#xA;IGV shows some links between aligned sections, shown as fine blue lines and thick black lines. I cannot work out what the difference between these lines is, and even more confusingly, we have numerous alignments where two aligned exons come from the same sequence but are not joined by any line. This means that, to confirm or deny a spliced variant, I need to manually examine the read ID of each exon.&#xD;&#xA;&#xD;&#xA;**SeqMonk**&#xD;&#xA;&#xD;&#xA;[![SeqMonk Visualisation of NOTCH2][2]][2]&#xD;&#xA;&#xD;&#xA;SeqMonk gives a much cleaner visualisation, but unfortunately shows no links between aligned exons, and worse, I cannot find the read IDs. This means there is functionally (as far as I can see) no way to tell which exons come from the same read.&#xD;&#xA;&#xD;&#xA;**EDIT: Sashimi Plot**&#xD;&#xA;&#xD;&#xA;As recommended in [this answer][3], I have tried using IGV's Sashimi Plot. Unfortunately, I believe the high error rate and relatively low coverage is causing this to create somewhat confusing output. The Sashimi Plot shows mostly nearly every junction to have just a single read supporting it.&#xD;&#xA;&#xD;&#xA;[![IGV Sashimi Plot of NOTCH2][4]][4]&#xD;&#xA;&#xD;&#xA;Is there a visualisation tool which allows simple examination of splicing of full-length transcripts?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/zM3Yn.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/ugYDT.png&#xD;&#xA;  [3]: https://bioinformatics.stackexchange.com/a/723/163&#xD;&#xA;  [4]: https://i.stack.imgur.com/5ah7z.png" />
  <row Id="2529" PostHistoryTypeId="2" PostId="826" RevisionGUID="f93ae55c-fc85-4c84-a3cc-66d5476c59b1" CreationDate="2017-06-20T02:17:29.127" UserId="37" Text="Genome-In-A-Bottle (GIAB; version 3.3.2) contains 3163k autosomal SNPs, 278k &lt;=50bp deletions and 257k &lt;=50bp insertions. This is on the lower end because GIAB ignores hard regions that tend to harbor more indels. On the CHM1-CHM13 pacbio assembly (European ancestry), there are 3460k autosomal SNPs, 507k &lt;=50bp deletions and 549k &lt;=50 insertions. However, due to PacBio consensus errors, this 507k+549k is probably an overestimate. I would *guess* for a non-African sample, there should be 900–1000k &lt;=50 indels, though only ~80% of them can be called with ~150bp short reads.&#xD;&#xA;&#xD;&#xA;For indels longer than 50bp, I would highly recommend to read [this paper](https://www.nature.com/nature/journal/v517/n7536/full/nature13907.html) by Chaisson et al. This call set is constructed from the whole-genome assembly of the CHM1 genome. It is far more comprehensive and probably more accurate than all the other call sets. The following is Table 1 from the paper:&#xD;&#xA;&#xD;&#xA;[![CHM1-SVs][1]][1]&#xD;&#xA;&#xD;&#xA;I have to say that the high ins-to-del ratio is surprising to me, but this is what the data tells us. I have done a similar analysis and reached a similar ratio.&#xD;&#xA;&#xD;&#xA;PS: These numbers are essentially from one haplotype. They will be higher for a diploid sample.&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/om410.png" />
  <row Id="2530" PostHistoryTypeId="5" PostId="750" RevisionGUID="a7cdeeb6-2c3b-4a40-8519-75f76ad6e436" CreationDate="2017-06-20T02:35:28.307" UserId="734" Comment="added 5 characters in body" Text="I saw [this nature news item][1], it sounds that [Cellminer][2] is obsolete, is it right?&#xA;&#xD;&#xA;What are the new tools to analyze the &quot;[new cell lines][3]&quot;? Where's the PDX repository?&#xD;&#xA;&#xD;&#xA;What about the European initiative that the article refers to? &#xA;&#xA;I would be happy to see correlation between drugs and genes and similar capabilities as we can see on Cellminer. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xA;&#xA;  [1]: http://www.nature.com/news/us-cancer-institute-to-overhaul-tumour-cell-lines-1.19364&#xA;  [2]: https://discover.nci.nih.gov/cellminer/&#xA;  [3]: https://www.jax.org/jax-mice-and-services/in-vivo-pharmacology/oncology-services/pdx-tumors#" />
  <row Id="2531" PostHistoryTypeId="2" PostId="827" RevisionGUID="7292ea59-02b9-4923-8907-2609adb1b18a" CreationDate="2017-06-20T03:16:04.833" UserId="96" Text="BLASTN should definitely report alignments in the order that the query sequences were provided.&#xD;&#xA;&#xD;&#xA;Based on your description and my personal experience, I think the most likely explanation is some kind of mix up. It happens to all of us, especially when we're in a rush to get an answer, but even sometimes when we're being careful and disciplined.&#xD;&#xA;&#xD;&#xA;If you can go back and recreate the result, I would be very interested to see the two pairs of input and output files. If not, I wouldn't spend too much more time worrying about it. :-)" />
  <row Id="2532" PostHistoryTypeId="5" PostId="821" RevisionGUID="0b53939b-90af-4e7d-bd5a-65f14019427e" CreationDate="2017-06-20T06:03:38.873" UserId="96" Comment="punctuation and tags" Text="My understanding is that indels are from 1bp to 10Kb, and a healthy genome has ~400K-500K Indels. Surely most of these are small. &#xD;&#xA;&#xD;&#xA;What is the distribution of insertion sizes in a healthy human genome? What is the distribution of deletion sizes? What is an average ratio of insertions:deletions? &#xD;&#xA;&#xD;&#xA;Apologies if this is common knowledge---I have not found a definitive reference." />
  <row Id="2533" PostHistoryTypeId="4" PostId="821" RevisionGUID="0b53939b-90af-4e7d-bd5a-65f14019427e" CreationDate="2017-06-20T06:03:38.873" UserId="96" Comment="punctuation and tags" Text="What is the distribution of indel sizes in a healthy human genome? of insertion:deletion ratios?" />
  <row Id="2534" PostHistoryTypeId="6" PostId="821" RevisionGUID="0b53939b-90af-4e7d-bd5a-65f14019427e" CreationDate="2017-06-20T06:03:38.873" UserId="96" Comment="punctuation and tags" Text="&lt;variants&gt;&lt;indel&gt;" />
  <row Id="2535" PostHistoryTypeId="24" PostId="821" RevisionGUID="0b53939b-90af-4e7d-bd5a-65f14019427e" CreationDate="2017-06-20T06:03:38.873" Comment="Proposed by 96 approved by 73, -1 edit id of 208" />
  <row Id="2536" PostHistoryTypeId="5" PostId="821" RevisionGUID="40c25bfd-c77a-48d4-95c7-661202b843ac" CreationDate="2017-06-20T06:03:38.873" UserId="57" Comment="punctuation and tags" Text="My understanding is that indels are from 1bp to 10Kb, and a healthy genome has ~400K-500K Indels. Surely most of these are small. &#xD;&#xA;&#xD;&#xA;What is the distribution of insertion sizes in a healthy human genome? What is the distribution of deletion sizes? What is an average ratio of insertions:deletions? &#xD;&#xA;&#xD;&#xA;I have not found a definitive reference." />
  <row Id="2537" PostHistoryTypeId="5" PostId="789" RevisionGUID="fa840aa8-e979-4801-8685-cdb12e043c6e" CreationDate="2017-06-20T07:33:09.990" UserId="48" Comment="added 486 characters in body" Text="This happen when the variables (strain +batch) create a design matrix like this:&#xD;&#xA;&#xD;&#xA;    batch strain&#xD;&#xA;    1 1 #&#xD;&#xA;    1 1 #&#xD;&#xA;    1 2&#xD;&#xA;    2 2&#xD;&#xA;    3 3&#xD;&#xA;    4 3&#xD;&#xA;    ...&#xD;&#xA;    16 72&#xD;&#xA;&#xD;&#xA;Which means that some of the covariates are not linearly independent (ie batch 1 and strain 1), all the strain 1 is in batch 1. &#xD;&#xA;&#xD;&#xA;You can correct for batch effects, but not in this design of the linear model (if you want to take into account the strain). You could do one batch more with those 15 strains that are in a single batch (if they are in different batch between them) that way you would get an independent design.&#xD;&#xA;&#xD;&#xA;Many strains in a single batch are in the same batch. You need to increase the number of samples (recommended anyway due to the low number of samples per strain) to avoid this problem. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&lt;sub&gt;There are a lot of related question in Bioconductor [support forum][1], from where I expanded an [answer][2].&lt;/sub&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.google.es/search?q=site%3Asupport.bioconductor.org%20%22system%20is%20computationally%20singular%22&amp;rlz=1C1WPZB_enES727ES727&amp;oq=site%3Asupport.bioconductor.org%20%22system%20is%20computationally%20singular%22&amp;ie=UTF-8&#xD;&#xA;  [2]: https://support.bioconductor.org/p/62058/#62059" />
  <row Id="2538" PostHistoryTypeId="5" PostId="807" RevisionGUID="8a75d91b-0eb3-4c7d-b3e6-1ab02b0fed95" CreationDate="2017-06-20T08:23:13.880" UserId="492" Comment="Clarify &quot;random record&quot;." Text="I would like to select a random record from a large set of `n` unaligned sequencing reads in `log(n)` time complexity ([big O notation](https://en.wikipedia.org/wiki/Big_O_notation)) or less. A record is defined as the equivalent of four lines in FASTQ format. The records do not fit in RAM and would need to be stored on disk. Ideally, I would like to store the reads in a compressed format.&#xD;&#xA;&#xD;&#xA;I would prefer a solution that does not require any extra files such as for example a reference genome.&#xD;&#xA;&#xD;&#xA;The title of this question mentions a FASTQ only because FASTQ is a common format for storing unaligned reads on disk. I am happy with answers that require a single limited transformation of the data to another file format in time complexity order `n`.&#xD;&#xA;&#xD;&#xA;## Update&#xD;&#xA;&#xD;&#xA;A clarification: I want the random record to be selected with probability `1/n`." />
  <row Id="2539" PostHistoryTypeId="5" PostId="353" RevisionGUID="971141bd-425d-4643-985a-6d2d76d7d6fb" CreationDate="2017-06-20T08:27:16.083" UserId="501" Comment="added 378 characters in body" Text="There is a very nice database, [pdbcull][1] (also known as the PISCES server in the literature). It filters the PDB for high resolution and reduced sequence identity. It also seems to be updated regularly. Depending on the cut-offs, you get between 3000 and 35000 structures.&#xD;&#xA;&#xD;&#xA;If you are specifically interested in rotamers, you may want to look at [top8000][2] instead, where they have checked for high resolution, and good MolProbity scores. They also provide a rotamer database.&#xD;&#xA;&#xD;&#xA;PDB also provides [their own clustering.][3] They first cluster the sequences, and then extract a representative structure for each one, based on the quality factor (`1/resolution - R_value`). This has the advantage of being comprehensive, but you will have bad structures when no good ones were ever obtained.&#xD;&#xA;&#xD;&#xA;   [1]:http://dunbrack.fccc.edu/Guoli/pisces_download.php&#xD;&#xA;   [2]:http://kinemage.biochem.duke.edu/databases/top8000.php&#xD;&#xA;   [3]:http://www.rcsb.org/pdb/statistics/clusterStatistics.do" />
  <row Id="2540" PostHistoryTypeId="2" PostId="828" RevisionGUID="4957fb11-c29a-436f-a636-1ae47926946f" CreationDate="2017-06-20T08:34:17.377" UserId="48" Text="In the [article][1] describing Cellminer says about the cross-correlations:&#xD;&#xA;&#xD;&#xA;&gt; ... a cross-correlation table of the resultant z-scores can be&#xD;&#xA;generated ...&#xD;&#xA;&#xD;&#xA;On page 3503&#xD;&#xA;&#xD;&#xA;And the z-scores are defined previously as:&#xD;&#xA;&#xD;&#xA;&gt; The tool output includes relative transcript intensity presented&#xD;&#xA;as z-scores...&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cancerres.aacrjournals.org/content/72/14/3499.full" />
  <row Id="2541" PostHistoryTypeId="2" PostId="829" RevisionGUID="1ac4532a-fba8-4dbf-bd52-95ef21abdca8" CreationDate="2017-06-20T08:58:36.057" UserId="678" Text="I would like to look if there are mutations in residues of human histones associated with any disease. For instance, if histone [H2A1A][1] K6 (lysine 6) residue mutation is associated with any human disease (with PUBMED evidence, preferably), but in a systematic way (for all types of histones and all residues). &#xD;&#xA;&#xD;&#xA;The purpose of that is to get some candidates residues to study. I have checked at [OMIM][2] database but I didn't found anything relevant. It seems that there is no connection between specific residues and diseases to be searched in a systematic way.  &#xD;&#xA;&#xD;&#xA;Is there any web database or API available for this purpose?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.uniprot.org/uniprot/Q96QV6#function&#xD;&#xA;  [2]: https://www.omim.org/" />
  <row Id="2542" PostHistoryTypeId="1" PostId="829" RevisionGUID="1ac4532a-fba8-4dbf-bd52-95ef21abdca8" CreationDate="2017-06-20T08:58:36.057" UserId="678" Text="How to find mutations associated with disease in human histones residues?" />
  <row Id="2543" PostHistoryTypeId="3" PostId="829" RevisionGUID="1ac4532a-fba8-4dbf-bd52-95ef21abdca8" CreationDate="2017-06-20T08:58:36.057" UserId="678" Text="&lt;database&gt;&lt;variants&gt;&lt;api&gt;" />
  <row Id="2544" PostHistoryTypeId="5" PostId="808" RevisionGUID="5896b370-212c-400f-a63c-74c64a16fa4f" CreationDate="2017-06-20T09:02:18.253" UserId="492" Comment="Give an example of sort -R" Text="You could shuffle the FASTQ once and then read sequences off the top of the file as you need them:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | shuf | tr '\t' '\n'| gzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;I would recommend [pigz](https://zlib.net/pigz/) as a replacement for gzip in the compression step if you have it available.&#xD;&#xA;&#xD;&#xA;The downside of this approach is that you only get `n` reads before you need to run the shuffle again, and [apparently](https://stackoverflow.com/a/24492814/528691) `shuf` holds all data in RAM, so it would die with an out of memory error if the FASTQ file does not fit into RAM as is specified in the question.&#xD;&#xA;&#xD;&#xA;Using `sort -R` is complexity `n log(n)` and uses temporary files, so it [should not](https://vkundeti.blogspot.co.uk/2008/03/tech-algorithmic-details-of-unix-sort.html) run out of memory:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sort -R | perl -pe 's/\s*\d+\t//' | tr '\t' '\n'| gzip -c &gt; output.fastq.gz&#xD;&#xA;    &#xD;&#xA;The `nl` and `perl` commands are necessary to make sure that identical records are not sorted next to each other." />
  <row Id="2545" PostHistoryTypeId="5" PostId="812" RevisionGUID="1ae1a3e3-ac48-4d40-9937-d1993d51551d" CreationDate="2017-06-20T10:13:08.820" UserId="73" Comment="updated to account for random record sampling" Text="To emphasise the issue (as outlined in the `1/n` update), consider an input file with one record that is 5 million bases long, and one records that is 100 bases long. You want an equal probability of selecting either of these two records. Any random seek methods will overwhelmingly pick out the long record.&#xD;&#xA;&#xD;&#xA;I expect that indexing the locations of record starts is really the only workable option here, particularly if multi-line records are possible. Create an index file containing the start locations of each record (as identically sized integers, e.g. 64-bit), then sample from the index file (which is an identical length for each record) to fetch the start location. I'd envisage that this file would *only* contain the start locations; any additional metadata (including sequence name) would require seeking in the original file.&#xD;&#xA;&#xD;&#xA;Once the indexing is done, the file can be compressed with bgzip, with specific offsets retrieved using the `-b` and `-s` options. However, I expect that compression would not be particularly efficient if multiple random records were desired." />
  <row Id="2546" PostHistoryTypeId="5" PostId="819" RevisionGUID="e14fd9c5-0bea-4be6-a9a5-48a62d004ff2" CreationDate="2017-06-20T11:00:23.183" UserId="123" Comment="Added a code snippet to show how to feed the model to sleuth" Text="You should be able to remove any one of the following strains to end up with a rank-sufficient model matrix: 5, 10, 12, 13, 14, 15, 19, 26, 28, 3, 30, 32, 36, 39, 41, 45, 46, 49, 5, 50, 52, 53, 58, 59, 60, 69, 8. As an aside you can figure this sort of thing out as follows (I read your dataframe in a the `d` object):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: R --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; m = model.matrix(~batch+strain, d)&#xD;&#xA;    &gt; dim(m) # 142 row, 77 columns, so minimum rank is 77&#xD;&#xA;    &gt; qr(m)$rank # 76, so just barely rank insufficient&#xD;&#xA;    &gt; #see if we can remove a single column and still get rank 76&#xD;&#xA;    &gt; colnames(m)[which(sapply(1:77, function(x) qr(m[,-x])$rank) == 76)]&#xD;&#xA;&#xD;&#xA;You obviously don't want to remove the batch columns or the intercept. The normal tricks that you can sometimes use to get around this issue with case-control studies don't appear to help here, which is why I would just drop a strain and call it done. Keep in mind that your power is still likely terrible. I generally recommend at least 6 replicates per group (scale down the number of groups to fit your budget).&#xD;&#xA;&#xD;&#xA;EDIT:&#xD;&#xA;&#xD;&#xA;Once the desired strain is removed from the model, it can be fit directly into the `sleuth_fit` function to obtain the full model:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: R --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; m = m[, -9] # whatever column to drop to get the appropriate rank&#xD;&#xA;    &gt; so &lt;- sleuth_fit(so, m, 'full')" />
  <row Id="2547" PostHistoryTypeId="24" PostId="819" RevisionGUID="e14fd9c5-0bea-4be6-a9a5-48a62d004ff2" CreationDate="2017-06-20T11:00:23.183" Comment="Proposed by 123 approved by 77 edit id of 209" />
  <row Id="2549" PostHistoryTypeId="2" PostId="831" RevisionGUID="14ea74d2-e924-4551-8a7b-d890bc35252c" CreationDate="2017-06-20T12:11:46.953" UserId="298" Text="This is what is known as a feature, not a bug. Note that your identical proteins all have accessions starting with `WP_`. These are special &quot;non-redundant proteins&quot;. Many sequences—particularly bacterial sequences—are identical between various different species so having a separate RefSeq entry for each of them would be inefficient. Therefore, RefSeq combines multiple such proteins into a single `WP_` record. This is documented [here](https://www.ncbi.nlm.nih.gov/refseq/about/nonredundantproteins/) (emphasis mine):&#xD;&#xA;&#xD;&#xA;&gt; A new type of RefSeq protein record which represents non-redundant protein sequences was introduced in mid-2013. This record type was introduced to address a growing issue with redundancy in the Prokaryotic RefSeq protein dataset that coincided with a significant increase in bacterial genome submissions from individual isolates and closely related bacterial strains. For example, a large number of high-quality bacterial genomes may be submitted during a disease outbreak. The submitted sequences may reflect pathogen evolution during the course of the outbreak but the majority of the encoded proteins from these genomes may be identical to each other. As RefSeq includes these genomes, per community requests, this resulted in increased redundancy. **By representing identical proteins using a single non-redundant protein accession number (with the prefix 'WP_'), redundancy in the database is significantly reduced.**&#xD;&#xA;&#xD;&#xA;&gt; [ . . . ]&#xD;&#xA;&#xD;&#xA;&gt; Because a non-redundant protein sequence may be found in RefSeq genomes from multiple species, **the organism information provided on the protein record reflects the lowest-common taxonomic node ranging from the genus species level to super-kingdom**. A non-redundant protein record that provides organism information at the level of a genus, family, or even super-kingdom does not mean that the protein is found in all RefSeq genomes below that taxonomic classification. It only indicates that the protein is found in more than one genome of different species for which the genus, family, or super-kingdom classification is the lowest common taxonomic node. &#xD;&#xA;&#xD;&#xA;So, your query was [NP_418578.1][1], anaerobic fumarate reductase catalytic and NAD/flavoprotein subunit from *E. coli* strain K-12, substrain MG1655. The first thing to notice is how specific that is. This is the protein found from one specific substrain of one specific strain of one specific bacterial species. It is reasonable to expect that there will be identical sequences from many, many closely related species. Both from, most probably, all other strains and substrains of *E. coli* and from other, related bacteria. &#xD;&#xA;&#xD;&#xA;Now, the specific sequences you mention are actually *slightly* different and not 100% identical. Below is a multiple alignment of NP_418578.1 and the 4 WP_ sequences you mentioned. Note that each of the 5 entries is slightly different. Each has one residue that differs from the rest. Look for the `:` in the identity line, there are 4 `:` and all others are `*` (I am only showing the relevant alignment blocks here and have removed those where all 4 sequences were identical):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    WP_001401474.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xD;&#xA;    WP_062863447.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xD;&#xA;    WP_064226696.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xD;&#xA;    NP_418578.1         MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xD;&#xA;    WP_078165098.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAIAQDH&#xD;&#xA;                        *******************************************************:****&#xD;&#xA;    &#xD;&#xA;    [ . . . ]&#xD;&#xA;    &#xD;&#xA;    WP_001401474.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xD;&#xA;    WP_062863447.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xD;&#xA;    WP_064226696.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHIRGLVAMNMMEGT&#xD;&#xA;    NP_418578.1         KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xD;&#xA;    WP_078165098.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xD;&#xA;                        ***********************************************:************&#xD;&#xA;    &#xD;&#xA;    [ . . . ]&#xD;&#xA;    &#xD;&#xA;    WP_001401474.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xD;&#xA;    WP_062863447.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKISQAFWHEWR&#xD;&#xA;    WP_064226696.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xD;&#xA;    NP_418578.1         GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xD;&#xA;    WP_078165098.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xD;&#xA;                        **************************************************:*********&#xD;&#xA;    &#xD;&#xA;    WP_001401474.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGIDPVKEPIPVRPTAHYTMG&#xD;&#xA;    WP_062863447.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xD;&#xA;    WP_064226696.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xD;&#xA;    NP_418578.1         KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xD;&#xA;    WP_078165098.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xD;&#xA;                        *****************************************:******************&#xD;&#xA;    &#xD;&#xA;    [ . . . ]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Your sequence (NP_418578.1) is only identical to one `WP_*` multi-species sequence, WP_001192973:&#xD;&#xA;&#xD;&#xA;    WP_001192973.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xD;&#xA;    NP_418578.1         MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      DSFEYHFHDTVAGGDWLCEQDVVDYFVHHCPTEMTQLELWGCPWSRRPDGSVNVRRFGGM&#xD;&#xA;    NP_418578.1         DSFEYHFHDTVAGGDWLCEQDVVDYFVHHCPTEMTQLELWGCPWSRRPDGSVNVRRFGGM&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xD;&#xA;    NP_418578.1         KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      LVQIRANAVVMATGGAGRVYRYNTNGGIVTGDGMGMALSHGVPLRDMEFVQYHPTGLPGS&#xD;&#xA;    NP_418578.1         LVQIRANAVVMATGGAGRVYRYNTNGGIVTGDGMGMALSHGVPLRDMEFVQYHPTGLPGS&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xD;&#xA;    NP_418578.1         GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xD;&#xA;    NP_418578.1         KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      GIETDQNCETRIKGLFAVGECSSVGLHGANRLGSNSLAELVVFGRLAGEQATERAATAGN&#xD;&#xA;    NP_418578.1         GIETDQNCETRIKGLFAVGECSSVGLHGANRLGSNSLAELVVFGRLAGEQATERAATAGN&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      GNEAAIEAQAAGVEQRLKDLVNQDGGENWAKIRDEMGLAMEEGCGIYRTPELMQKTIDKL&#xD;&#xA;    NP_418578.1         GNEAAIEAQAAGVEQRLKDLVNQDGGENWAKIRDEMGLAMEEGCGIYRTPELMQKTIDKL&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      AELQERFKRVRITDTSSVFNTDLLYTIELGHGLNVAECMAHSAMARKESRGAHQRLDEGC&#xD;&#xA;    NP_418578.1         AELQERFKRVRITDTSSVFNTDLLYTIELGHGLNVAECMAHSAMARKESRGAHQRLDEGC&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      TERDDVNFLKHTLAFRDADGTTRLEYSDVKITTLPPAKRVYGGEADAADKAEAANKKEKA&#xD;&#xA;    NP_418578.1         TERDDVNFLKHTLAFRDADGTTRLEYSDVKITTLPPAKRVYGGEADAADKAEAANKKEKA&#xD;&#xA;                        ************************************************************&#xD;&#xA;    &#xD;&#xA;    WP_001192973.1      NG&#xD;&#xA;    NP_418578.1         NG&#xD;&#xA;                        **&#xD;&#xA;&#xD;&#xA;So, in summary, RefSeq will combine multiple identical sequences into a single `WP_*` multi-species accession. You should therefore expect to find one 100% identical `WP_*` sequence for your query and multiple, *almost* identical `WP_*` entries. And that's precisely what you see here.&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/protein/NP_418578&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/protein/WP_078165098&#xD;&#xA;" />
  <row Id="2550" PostHistoryTypeId="5" PostId="818" RevisionGUID="0b734f0f-1397-4fe0-8539-d26e8a50378e" CreationDate="2017-06-20T13:18:59.283" UserId="298" Comment="Added information from the OP's comment and cleaned up the question. " Text="I had a protein Refseq ID and I PSI-BLASTed this sequence against Refseq database. We all know that the Refseq is a Reference sequence database and it shouldn't have redundancy. After BLASTing my sequence, at first iteration I got 1000 hits and among them there were a lot of redundant sequences! &#xD;&#xA;&#xD;&#xA;My sequence had 241 amino acids and I found a lot of sequences with 100% identity, 100% cover and 0 E-value exactly the same as my sequence but with different IDs. All of these IDs were from RefSeq! In other iterations and after adjusting format options, I got this redundancy with other sequences from other species. &#xD;&#xA;&#xD;&#xA;For example, when using NP_418578 as a query, I found WP_078165098.1, WP_064226696.1, WP_062863447.1, WP_001401474.1 and other that were identical.&#xD;&#xA;&#xD;&#xA;I want to know what is wrong with Refseq. Is it really a Reference sequence database? If it is, where does this redundancy come from and why?" />
  <row Id="2551" PostHistoryTypeId="6" PostId="818" RevisionGUID="0b734f0f-1397-4fe0-8539-d26e8a50378e" CreationDate="2017-06-20T13:18:59.283" UserId="298" Comment="Added information from the OP's comment and cleaned up the question. " Text="&lt;blast&gt;&lt;sequence-homology&gt;&lt;refseq&gt;" />
  <row Id="2552" PostHistoryTypeId="24" PostId="818" RevisionGUID="0b734f0f-1397-4fe0-8539-d26e8a50378e" CreationDate="2017-06-20T13:18:59.283" Comment="Proposed by 298 approved by 77, 37 edit id of 210" />
  <row Id="2553" PostHistoryTypeId="5" PostId="829" RevisionGUID="8fb0e170-7254-42e9-bbeb-b11b83c63019" CreationDate="2017-06-20T13:26:01.623" UserId="298" Comment="Corrections" Text="I would like to look if there are mutations in residues of human histones associated with any disease. For instance, if a mutation in residue K6 (lysine 6) of histone [H2A1A][1] is associated with any human disease, (with PUBMED evidence, preferably), but in a systematic way (for all types of histones and all residues). &#xD;&#xA;&#xD;&#xA;The purpose of this is to get some candidate residues to study. I have checked the [OMIM][2] database but I didn't find anything relevant. It seems that there is no systematic way to connect specific residues to diseases and search for them.&#xD;&#xA;&#xD;&#xA;Is there any web database or API available for this purpose?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.uniprot.org/uniprot/Q96QV6#function&#xD;&#xA;  [2]: https://www.omim.org/" />
  <row Id="2554" PostHistoryTypeId="4" PostId="829" RevisionGUID="8fb0e170-7254-42e9-bbeb-b11b83c63019" CreationDate="2017-06-20T13:26:01.623" UserId="298" Comment="Corrections" Text="How can I find mutations associated with disease in human histone residues?" />
  <row Id="2555" PostHistoryTypeId="24" PostId="829" RevisionGUID="8fb0e170-7254-42e9-bbeb-b11b83c63019" CreationDate="2017-06-20T13:26:01.623" Comment="Proposed by 298 approved by 678 edit id of 211" />
  <row Id="2556" PostHistoryTypeId="5" PostId="818" RevisionGUID="cfa52b79-74d5-4924-adc6-f85456abdc8c" CreationDate="2017-06-20T14:30:41.707" UserId="818" Comment="added 85 characters in body" Text="I had a protein Refseq ID and I PSI-BLASTed this sequence against Refseq database. We all know that the Refseq is a Reference sequence database and it shouldn't have redundancy. After BLASTing my sequence, at first iteration I got 1000 hits and among them there were a lot of redundant sequences! &#xD;&#xA;&#xD;&#xA;My sequence had 241 amino acids and I found a lot of sequences with 100% identity, 100% cover and 0 E-value exactly the same as my sequence but with different IDs. All of these IDs were from RefSeq! In other iterations and after adjusting format options, I got this redundancy with other sequences from other species. My sequence is related to a chain of a multichain protein (E.coli fumarate reductase)&#xD;&#xA;&#xD;&#xA;For example, when using NP_418578 as a query, I found WP_078165098.1, WP_064226696.1, WP_062863447.1, WP_001401474.1 and other that were identical.&#xD;&#xA;&#xD;&#xA;I want to know what is wrong with Refseq. Is it really a Reference sequence database? If it is, where does this redundancy come from and why?" />
  <row Id="2568" PostHistoryTypeId="2" PostId="834" RevisionGUID="70c94049-0bb9-4250-91b5-338301fdc73c" CreationDate="2017-06-20T16:44:09.773" UserId="842" Text="I have analyzed the situation more and found that blastn automatically sorts by e-value. http://ix.io/xLh (column 11)" />
  <row Id="2569" PostHistoryTypeId="2" PostId="835" RevisionGUID="44a52f9c-7259-4896-a151-34285c055f23" CreationDate="2017-06-20T16:44:39.103" UserId="924" Text="If you are looking for NGS QC for your fastq, bam, bed and vcf files I would suggest a commercial tool called [omnomicsQ][1]. &#xD;&#xA;&#xD;&#xA;It automates coverage analysis and sample pass, warn fail according according to your defined SOPs. Unlike fastqc it also comes with a database so that you can compare protocols, samples, and your performance to peer organisations. It also charts performance over time, exceptions,and  correlations between metrics&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://om.euformatics.com/apex/f?p=118:1:" />
  <row Id="2570" PostHistoryTypeId="5" PostId="822" RevisionGUID="3bc6fc69-47fe-45ff-a801-fd549b5fd13c" CreationDate="2017-06-20T17:06:47.033" UserId="776" Comment="added 245 characters in body" Text="Here's another approach that doesn't require any indexing, using [BEDOPS `bedextract`][1] to do a `log(n)` sample on a sorted BED file.&#xD;&#xA;&#xD;&#xA;It does require a single `O(n)` pass through the file to transform it to a BED file:&#xD;&#xA;&#xD;&#xA;    $ cat records.fastq | paste - - - - | awk '{ print &quot;chrZ\t&quot;s&quot;\t&quot;(s+1)&quot;$0 }' &gt; records.bed&#xD;&#xA;&#xD;&#xA;Store the intervals in a separate file:&#xD;&#xA;&#xD;&#xA;    $ cut -f1-3 records.bed &gt; intervals.bed&#xD;&#xA;&#xD;&#xA;To do a random sample of `k` elements, shuffle the intervals file and preserve the order of shuffled elements. &#xD;&#xA;&#xD;&#xA;You can do this with the [`sample` tool][2] I outlined earlier:&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed &gt; intervals-sample.bed&#xD;&#xA;&#xD;&#xA;Or you can `shuf` and `sort-bed` to do the same thing:&#xD;&#xA;&#xD;&#xA;    $ shuf -n ${K} intervals.bed | sort-bed - &gt; intervals-sample.bed&#xD;&#xA;&#xD;&#xA;There's an `O(klog(k))` cost here, but if `k &lt;&lt;&lt; n`, i.e., you're working with whole-genome scale input, this cost is amortized over the `log(n)` search performance.&#xD;&#xA;&#xD;&#xA;Next, use `bedextract` to do a binary search on the records, and delinearize to get back to FASTQ:&#xD;&#xA;&#xD;&#xA;    $ bedextract records.bed intervals-sample.bed | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;With Unix I/O streams, this can be done in one pass:&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed | bedextract records.bed - | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;By baking the sort order into `records.bed`, you're guaranteed the ability to do a binary search, which is `log(n)`.&#xD;&#xA;&#xD;&#xA;*Note:* Further, by linearizing the FASTQ input to a BED file and querying on BED intervals, you have equal probability of picking any one interval (interval == FQ record). You can draw an unbiased sample without the hassle of creating and storing a separate index.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/content/reference/set-operations/bedextract.html&#xD;&#xA;  [2]: https://github.com/alexpreynolds/sample" />
  <row Id="2571" PostHistoryTypeId="2" PostId="836" RevisionGUID="59cae29f-19c1-4155-a2c3-b4bbf84af124" CreationDate="2017-06-20T17:20:24.900" UserId="96" Text="One of the most thorough treatments of this question (or a similar question: grabbing a random subset of reads) was given by Jared Simpson in a blog post a few years ago. http://simpsonlab.github.io/2015/05/19/io-performance/&#xD;&#xA;&#xD;&#xA;If you just want to grab a single random read, Jared's benchmarks suggest that seeking to a random position in the file and then retrieving the next complete read should be the most performant option. &#xD;&#xA;&#xD;&#xA;However, more generally, if you want to take a random subset of reads, there are many factors that affect performance." />
  <row Id="2572" PostHistoryTypeId="5" PostId="822" RevisionGUID="a29730a9-d00f-43f9-abdd-296ad1b6a81d" CreationDate="2017-06-20T17:26:07.590" UserId="776" Comment="added 73 characters in body" Text="Here's another approach that doesn't require any indexing, using [BEDOPS `bedextract`][1] to do a `log(n)` sample on a sorted BED file. Your sample contains random records with equal probability `1/n`.&#xD;&#xA;&#xD;&#xA;This approach requires a single `O(n)` pass through the file to transform it to a BED file:&#xD;&#xA;&#xD;&#xA;    $ cat records.fastq | paste - - - - | awk '{ print &quot;chrZ\t&quot;s&quot;\t&quot;(s+1)&quot;$0 }' &gt; records.bed&#xD;&#xA;&#xD;&#xA;Store the intervals in a separate file:&#xD;&#xA;&#xD;&#xA;    $ cut -f1-3 records.bed &gt; intervals.bed&#xD;&#xA;&#xD;&#xA;To do a random sample of `k` elements, shuffle the intervals file and preserve the order of shuffled elements. &#xD;&#xA;&#xD;&#xA;You can do this with the [`sample` tool][2] I outlined earlier:&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed &gt; intervals-sample.bed&#xD;&#xA;&#xD;&#xA;Or you can `shuf` and `sort-bed` to do the same thing:&#xD;&#xA;&#xD;&#xA;    $ shuf -n ${K} intervals.bed | sort-bed - &gt; intervals-sample.bed&#xD;&#xA;&#xD;&#xA;There's an `O(klog(k))` cost here, but if `k &lt;&lt;&lt; n`, i.e., you're working with whole-genome scale input, this cost is amortized over the `log(n)` search performance.&#xD;&#xA;&#xD;&#xA;Next, use `bedextract` to do a binary search on the records, and delinearize to get back to FASTQ:&#xD;&#xA;&#xD;&#xA;    $ bedextract records.bed intervals-sample.bed | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;With Unix I/O streams, this can be done in one pass:&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed | bedextract records.bed - | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;By baking the sort order into `records.bed`, you're guaranteed the ability to do a binary search, which is `log(n)`.&#xD;&#xA;&#xD;&#xA;*Note:* Further, by linearizing the FASTQ input to a BED file and querying on BED intervals, you have equal probability of picking any one interval (interval == FQ record). You can draw an unbiased sample without the hassle of creating and storing a separate index.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/content/reference/set-operations/bedextract.html&#xD;&#xA;  [2]: https://github.com/alexpreynolds/sample" />
  <row Id="2573" PostHistoryTypeId="2" PostId="837" RevisionGUID="de2941fe-4a09-474c-924f-6b78d4c7d775" CreationDate="2017-06-20T19:23:49.107" UserId="146" Text="I'm beginning with the reference genome in FASTA format, hg19. &#xD;&#xA;&#xD;&#xA;My goal is to take the FASTA, and simulate artificial SNPs and InDels along the FASTA to benchmark various algorithms. If I was only creating SNPs in the FASTA, this would be a trivial problem. The reference FASTA has the original coordinates, and the SNPs would also be in a coordinate system which is the same. &#xD;&#xA;&#xD;&#xA;However, when I randomly insert an insertion (e.g. an insertion of 12 bp), I move all events into a new coordinate system such that &#xD;&#xA;&#xD;&#xA;    new coordinate system = old coordinate system + 15&#xD;&#xA;&#xD;&#xA;This becomes very difficult to keep track off throughout the entire genome after several interations. I therefore cannot keep track of the new and old coordinate system without running an algorithm and creating a VCF, which may have errors. This defeats the purpose of the simulation.&#xD;&#xA;&#xD;&#xA;What method/data structure is used to solve this problem? Would LiftOver work? Maybe CrossMap? &#xD;&#xA;&#xD;&#xA;http://genome.sph.umich.edu/wiki/LiftOver&#xD;&#xA;http://crossmap.sourceforge.net/&#xD;&#xA;" />
  <row Id="2574" PostHistoryTypeId="1" PostId="837" RevisionGUID="de2941fe-4a09-474c-924f-6b78d4c7d775" CreationDate="2017-06-20T19:23:49.107" UserId="146" Text="Benchmarking: after artificially creating events in a FASTA file, how do I keep track of the old coordinates?" />
  <row Id="2575" PostHistoryTypeId="3" PostId="837" RevisionGUID="de2941fe-4a09-474c-924f-6b78d4c7d775" CreationDate="2017-06-20T19:23:49.107" UserId="146" Text="&lt;vcf&gt;&lt;structural-variation&gt;&lt;indel&gt;&lt;snp&gt;&lt;simulation&gt;" />
  <row Id="2576" PostHistoryTypeId="2" PostId="838" RevisionGUID="500b8fb9-886c-4104-8fff-6ecaf9b97127" CreationDate="2017-06-20T19:26:57.350" UserId="926" Text="I have two files &#xD;&#xA;&#xD;&#xA;    s3.txt :&#xD;&#xA;    1	10	20&#xD;&#xA;    1	5	20&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    1	10	50&#xD;&#xA;    2	20	60&#xD;&#xA;    1	14	17&#xD;&#xA;&#xD;&#xA;    s4.txt:&#xD;&#xA;    1	10	20&#xD;&#xA;    2	20	30&#xD;&#xA;&#xD;&#xA;I am trying to match col0 of both the files and get rows that fall between range(inclusive of themselves) 10-20 and 20-30 as seen in s4 file.&#xD;&#xA;&#xD;&#xA;code so far:&#xD;&#xA;&#xD;&#xA;    d1 =[]&#xD;&#xA;    with open('s4.txt', 'r') as f:&#xD;&#xA;        for i in f:&#xD;&#xA;            j = i.strip().split('\t')&#xD;&#xA;            d1.append(j)&#xD;&#xA;    &#xD;&#xA;    d2 =[]       &#xD;&#xA;    with open('s3.txt', 'r') as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            j = line.strip().split('\t')&#xD;&#xA;            d2.append(j)&#xD;&#xA;    for i in d1:&#xD;&#xA;        for j in d2:&#xD;&#xA;            st = int(j[1])&#xD;&#xA;            en = int(j[2])&#xD;&#xA;            cst = int(i[1])&#xD;&#xA;            cen = int(i[2])&#xD;&#xA;            if  i[0] == j[0]:&#xD;&#xA;                l = cst &gt;= st and cen &lt;= en&#xD;&#xA;                if l == True:&#xD;&#xA;                   print j&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;Output with missing row(1	14	17) : &#xD;&#xA;&#xD;&#xA;    ['1', '10', '20']&#xD;&#xA;    ['1', '5', '20']&#xD;&#xA;    ['1', '10', '50']&#xD;&#xA;    ['2', '20', '30']&#xD;&#xA;    ['2', '20', '60']&#xD;&#xA;&#xD;&#xA;Desired output:&#xD;&#xA;&#xD;&#xA;    1	10	20&#xD;&#xA;    1	5	20&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    1	14	17&#xD;&#xA;&#xD;&#xA;Not sure if my logic is wrong and why does it miss 14-17 as it falls between 10-20&#xD;&#xA;    " />
  <row Id="2577" PostHistoryTypeId="1" PostId="838" RevisionGUID="500b8fb9-886c-4104-8fff-6ecaf9b97127" CreationDate="2017-06-20T19:26:57.350" UserId="926" Text="range overlap python error" />
  <row Id="2578" PostHistoryTypeId="3" PostId="838" RevisionGUID="500b8fb9-886c-4104-8fff-6ecaf9b97127" CreationDate="2017-06-20T19:26:57.350" UserId="926" Text="&lt;genomics&gt;" />
  <row Id="2580" PostHistoryTypeId="2" PostId="839" RevisionGUID="fc81ec1f-f783-4486-94fc-cdfe848f7259" CreationDate="2017-06-20T19:35:59.390" UserId="776" Text="You could use [BEDOPS][1], instead:&#xD;&#xA;&#xD;&#xA;    $ sort-bed s3.txt &gt; s3.bed&#xD;&#xA;    $ sort-bed s4.txt &gt; s4.bed&#xD;&#xA;    $ bedops --element-of 1 s3.bed s4.bed &gt; answer.bed&#xD;&#xA;&#xD;&#xA;If you need to run it from within Python, you could use `subprocess.check_output()`:&#xD;&#xA;&#xD;&#xA;    import subprocess&#xD;&#xA;    ...&#xD;&#xA;    try:&#xD;&#xA;        result = subprocess.check_output(&quot;bedops --element-of 1 %s %s &gt; %s&quot; % (set_a_fn, set_b_fn, answer_fn), shell=True)&#xD;&#xA;    except subprocess.CalledProcessError as err:&#xD;&#xA;        raise SystemExit(&quot;Could not run bedops\n&quot;)&#xD;&#xA;    # do stuff with 'result'&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/#" />
  <row Id="2581" PostHistoryTypeId="2" PostId="840" RevisionGUID="3631e204-67d6-4aef-a669-7745f50490b4" CreationDate="2017-06-20T19:38:59.547" UserId="77" Text="You're reinventing `bedtools intersect` (or bedops), for which there's [already a convenient python module](https://daler.github.io/pybedtools/):&#xD;&#xA;&#xD;&#xA;    from pybedtools import BedTool&#xD;&#xA;&#xD;&#xA;    s3 = BedTool('s3.bed')&#xD;&#xA;    s4 = BedTool('s4.bed')&#xD;&#xA;&#xD;&#xA;    print(s4.intersect(s3, wb=True))&#xD;&#xA;&#xD;&#xA;The `wb=True` is equivalent to `--wb` with `bedtools intersect` on the command line." />
  <row Id="2582" PostHistoryTypeId="5" PostId="837" RevisionGUID="dccf974d-514c-49e0-9824-976ae84e43fa" CreationDate="2017-06-20T19:46:43.263" UserId="146" Comment="added 327 characters in body; edited tags" Text="I'm beginning with the reference genome in FASTA format, hg19. I am reading the sequence into a Python dictionary with `BioPython`:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    genome_dictionary = {} &#xD;&#xA;    for seq_record.id in SeqIO.parse(input_fasta_file, &quot;fasta&quot;):&#xD;&#xA;        genome_dictionary[seq_record.id] = seq_record&#xD;&#xA;&#xD;&#xA;This creates a Python dictionary with keys as chromosome names and values as strings from the FASTA. &#xD;&#xA;&#xD;&#xA;My goal is to take the FASTA, and simulate artificial SNPs and InDels along the FASTA to benchmark various algorithms. If I was only creating SNPs in the FASTA, this would be a trivial problem. The reference FASTA has the original coordinates, and the SNPs would also be in a coordinate system which is the same. &#xD;&#xA;&#xD;&#xA;However, when I randomly insert an insertion (e.g. an insertion of 12 bp), I move all events into a new coordinate system such that &#xD;&#xA;&#xD;&#xA;    new coordinate system = old coordinate system + 15&#xD;&#xA;&#xD;&#xA;This becomes very difficult to keep track off throughout the entire genome after several interations. I therefore cannot keep track of the new and old coordinate system without running an algorithm and creating a VCF, which may have errors. This defeats the purpose of the simulation.&#xD;&#xA;&#xD;&#xA;What method/data structure is used to solve this problem? Would LiftOver work? Maybe CrossMap? &#xD;&#xA;&#xD;&#xA;http://genome.sph.umich.edu/wiki/LiftOver&#xD;&#xA;http://crossmap.sourceforge.net/&#xD;&#xA;" />
  <row Id="2583" PostHistoryTypeId="6" PostId="837" RevisionGUID="dccf974d-514c-49e0-9824-976ae84e43fa" CreationDate="2017-06-20T19:46:43.263" UserId="146" Comment="added 327 characters in body; edited tags" Text="&lt;structural-variation&gt;&lt;biopython&gt;&lt;indel&gt;&lt;snp&gt;&lt;simulation&gt;" />
  <row Id="2584" PostHistoryTypeId="6" PostId="837" RevisionGUID="a54f505f-5e3f-4ef3-89eb-da8947d72be2" CreationDate="2017-06-20T19:54:56.110" UserId="146" Comment="edited tags" Text="&lt;structural-variation&gt;&lt;biopython&gt;&lt;bed&gt;&lt;indel&gt;&lt;simulation&gt;" />
  <row Id="2585" PostHistoryTypeId="5" PostId="840" RevisionGUID="5229e89c-30a8-47d6-a716-87879a63e3dd" CreationDate="2017-06-20T19:55:50.410" UserId="77" Comment="added 64 characters in body" Text="You're reinventing `bedtools intersect` (or bedops), for which there's [already a convenient python module](https://daler.github.io/pybedtools/):&#xD;&#xA;&#xD;&#xA;    from pybedtools import BedTool&#xD;&#xA;&#xD;&#xA;    s3 = BedTool('s3.bed')&#xD;&#xA;    s4 = BedTool('s4.bed')&#xD;&#xA;&#xD;&#xA;    print(s4.intersect(s3, wb=True))&#xD;&#xA;&#xD;&#xA;The `wb=True` is equivalent to `--wb` with `bedtools intersect` on the command line. You can also use `f` and `F` to achieve what you actually want." />
  <row Id="2586" PostHistoryTypeId="5" PostId="837" RevisionGUID="a54f505f-5e3f-4ef3-89eb-da8947d72be2" CreationDate="2017-06-20T19:54:56.110" UserId="146" Comment="edited tags" Text="I'm beginning with the reference genome in FASTA format, hg19. I am reading the sequence into a Python dictionary with `BioPython`:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    genome_dictionary = {} &#xD;&#xA;    for seq_record.id in SeqIO.parse(input_fasta_file, &quot;fasta&quot;):&#xD;&#xA;        genome_dictionary[seq_record.id] = seq_record&#xD;&#xA;&#xD;&#xA;This creates a Python dictionary with keys as chromosome names and values as strings from the FASTA. &#xD;&#xA;&#xD;&#xA;My goal is to take the FASTA, and simulate artificial SNPs and InDels along the FASTA to benchmark various algorithms. If I was only creating SNPs in the FASTA, this would be a trivial problem. The reference FASTA has the original coordinates, and the SNPs would also be in a coordinate system which is the same. &#xD;&#xA;&#xD;&#xA;However, when I randomly insert an insertion (e.g. an insertion of 12 bp), I move all events into a new coordinate system such that &#xD;&#xA;&#xD;&#xA;    new coordinate system = old coordinate system + 15&#xD;&#xA;&#xD;&#xA;This becomes very difficult to keep track off throughout the entire genome after several interations. I therefore cannot keep track of the new and old coordinate system without running an algorithm and creating a VCF, which may have errors. This defeats the purpose of the simulation.&#xD;&#xA;&#xD;&#xA;The output would be either a BED file or VCF which has keep track of the changes.&#xD;&#xA;&#xD;&#xA;What method/data structure is used to solve this problem? Would LiftOver work? Maybe CrossMap? &#xD;&#xA;&#xD;&#xA;http://genome.sph.umich.edu/wiki/LiftOver&#xD;&#xA;http://crossmap.sourceforge.net/&#xD;&#xA;" />
  <row Id="2587" PostHistoryTypeId="4" PostId="837" RevisionGUID="a54f505f-5e3f-4ef3-89eb-da8947d72be2" CreationDate="2017-06-20T19:54:56.110" UserId="146" Comment="edited tags" Text="After artificially creating events in a FASTA file, how do I keep track of the old coordinates?" />
  <row Id="2588" PostHistoryTypeId="5" PostId="840" RevisionGUID="1412e42c-6b78-4566-a07a-574f9dae22d0" CreationDate="2017-06-20T19:57:59.007" UserId="77" Comment="deleted 11 characters in body" Text="You're reinventing `bedtools intersect` (or bedops), for which there's [already a convenient python module](https://daler.github.io/pybedtools/):&#xD;&#xA;&#xD;&#xA;    from pybedtools import BedTool&#xD;&#xA;&#xD;&#xA;    s3 = BedTool('s3.bed')&#xD;&#xA;    s4 = BedTool('s4.bed')&#xD;&#xA;&#xD;&#xA;    print(s4.intersect(s3, wa=True, wb=True, F=1))&#xD;&#xA;&#xD;&#xA;The `wb=True` is equivalent to `-wb` with `bedtools intersect` on the command line. Similarly, `F=1` is the same as `-F 1`." />
  <row Id="2589" PostHistoryTypeId="2" PostId="841" RevisionGUID="46229727-2ceb-4a73-8111-c6d4380d4a33" CreationDate="2017-06-20T20:04:39.883" UserId="425" Text="If you look for a program which would randomly introduce SNPs + short indels and then would save everything into a VCF file, [DWGsim](https://github.com/nh13/DWGSIM) or [Mason Variator](https://github.com/seqan/seqan/blob/master/apps/mason2/README.mason_variator) could be a good choice. Then you can create a corresponding Chain file using `bcftools consensus -c` and transform various formats between these two coordinate systems using [CrossMap](http://crossmap.sourceforge.net/).&#xD;&#xA;" />
  <row Id="2590" PostHistoryTypeId="5" PostId="838" RevisionGUID="21071faf-b245-4b78-9355-b588a4624994" CreationDate="2017-06-20T20:24:49.977" UserId="926" Comment="added 371 characters in body" Text="I have two files &#xD;&#xA;&#xD;&#xA;    s3.txt :&#xD;&#xA;    1	10	20&#xD;&#xA;    1	5	20&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    1	10	50&#xD;&#xA;    2	20	60&#xD;&#xA;    1	14	17&#xD;&#xA;&#xD;&#xA;    s4.txt:&#xD;&#xA;    1	10	20&#xD;&#xA;    2	20	30&#xD;&#xA;&#xD;&#xA;I am trying to match col0 of both the files and get rows that fall between range(inclusive of themselves) 10-20 and 20-30 as seen in s4 file.&#xD;&#xA;&#xD;&#xA;code so far:&#xD;&#xA;&#xD;&#xA;    d1 =[]&#xD;&#xA;    with open('s4.txt', 'r') as f:&#xD;&#xA;        for i in f:&#xD;&#xA;            j = i.strip().split('\t')&#xD;&#xA;            d1.append(j)&#xD;&#xA;    &#xD;&#xA;    d2 =[]       &#xD;&#xA;    with open('s3.txt', 'r') as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            j = line.strip().split('\t')&#xD;&#xA;            d2.append(j)&#xD;&#xA;    for i in d1:&#xD;&#xA;        for j in d2:&#xD;&#xA;            st = int(j[1])&#xD;&#xA;            en = int(j[2])&#xD;&#xA;            cst = int(i[1])&#xD;&#xA;            cen = int(i[2])&#xD;&#xA;            if  i[0] == j[0]:&#xD;&#xA;                l = cst &gt;= st and cen &lt;= en&#xD;&#xA;                if l == True:&#xD;&#xA;                   print j&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;Output with missing row(1	14	17) : &#xD;&#xA;&#xD;&#xA;    ['1', '10', '20']&#xD;&#xA;    ['1', '5', '20']&#xD;&#xA;    ['1', '10', '50']&#xD;&#xA;    ['2', '20', '30']&#xD;&#xA;    ['2', '20', '60']&#xD;&#xA;&#xD;&#xA;Desired output:&#xD;&#xA;&#xD;&#xA;    1	10	20&#xD;&#xA;    1	5	20&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    1	14	17&#xD;&#xA;&#xD;&#xA;Not sure if my logic is wrong and why does it miss 14-17 as it falls between 10-20&#xD;&#xA;    &#xD;&#xA;    [EDIT] using pybedtools&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; print(s4.intersect(s3, wb=True))&#xD;&#xA;    1	10	20	1	10	20&#xD;&#xA;    1	10	20	1	5	20&#xD;&#xA;    1	10	20	1	10	50&#xD;&#xA;    1	14	17	1	14	17&#xD;&#xA;    2	20	30	2	20	30&#xD;&#xA;    2	25	30	2	25	30&#xD;&#xA;    2	20	30	2	20	60&#xD;&#xA;    &#xD;&#xA;    &gt;&gt;&gt; print(s4.intersect(s3, wa=True, wb=True, F=1))&#xD;&#xA;    1	10	20	1	10	20&#xD;&#xA;    1	10	20	1	14	17&#xD;&#xA;    2	20	30	2	20	30&#xD;&#xA;    2	20	30	2	25	30&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2591" PostHistoryTypeId="6" PostId="838" RevisionGUID="4f622e80-5824-49aa-86cd-9a32c0b35f97" CreationDate="2017-06-20T20:29:16.823" UserId="77" Comment="Added homework tag" Text="&lt;genomics&gt;&lt;homework&gt;" />
  <row Id="2592" PostHistoryTypeId="4" PostId="838" RevisionGUID="1ca3fac1-1585-4e4c-b967-2588ae916376" CreationDate="2017-06-20T20:49:07.387" UserId="131" Comment="added python tag as the post title says &quot;python error&quot;" Text="Range overlap python error" />
  <row Id="2593" PostHistoryTypeId="6" PostId="838" RevisionGUID="1ca3fac1-1585-4e4c-b967-2588ae916376" CreationDate="2017-06-20T20:49:07.387" UserId="131" Comment="added python tag as the post title says &quot;python error&quot;" Text="&lt;genomics&gt;&lt;homework&gt;&lt;python&gt;" />
  <row Id="2594" PostHistoryTypeId="24" PostId="838" RevisionGUID="1ca3fac1-1585-4e4c-b967-2588ae916376" CreationDate="2017-06-20T20:49:07.387" Comment="Proposed by 131 approved by 926 edit id of 212" />
  <row Id="2595" PostHistoryTypeId="6" PostId="838" RevisionGUID="02f41200-cdd4-4362-a00a-feb6a18e6dcd" CreationDate="2017-06-20T22:06:39.137" UserId="57" Comment="edited tags" Text="&lt;genomics&gt;&lt;python&gt;" />
  <row Id="2596" PostHistoryTypeId="10" PostId="838" RevisionGUID="5b7cdd57-b002-410d-9d7d-6f0bb996b544" CreationDate="2017-06-20T23:04:11.243" UserId="55" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:425,&quot;DisplayName&quot;:&quot;Karel Brinda&quot;},{&quot;Id&quot;:55,&quot;DisplayName&quot;:&quot;Robert Cartaino&quot;}]}" />
  <row Id="2597" PostHistoryTypeId="2" PostId="842" RevisionGUID="92458b1f-3fb4-417a-83e8-c0a843dc71a1" CreationDate="2017-06-21T02:20:33.860" UserId="927" Text="I'm currently attempting association analysis with an extremely small set of patient exomes (n=10), with no control or parental exomes available. Downloading the ExAC VCF of variant sites (http://exac.broadinstitute.org/downloads) or the 1000G integrated call sets (http://ftp.1000genomes.ebi.ac.uk/) and combining this with our pooled patient VCFs has not been successful (I suspect the approach of attempting to merge such large VCFs generated from different pipelines is rather naive).&#xD;&#xA;&#xD;&#xA;Looking at the primary literature, I have gathered it should be possible to use these resources to help increase statistical power for our analysis. My question is how do I take these large .vcfs with many samples and successfully merge them to our patient .vcfs, such that the combined VCF can be used downstream to run analysis packages? (PODKAT, PLINK, etc.)" />
  <row Id="2598" PostHistoryTypeId="1" PostId="842" RevisionGUID="92458b1f-3fb4-417a-83e8-c0a843dc71a1" CreationDate="2017-06-21T02:20:33.860" UserId="927" Text="What is a good pipeline for using public domain exomes as controls?" />
  <row Id="2599" PostHistoryTypeId="3" PostId="842" RevisionGUID="92458b1f-3fb4-417a-83e8-c0a843dc71a1" CreationDate="2017-06-21T02:20:33.860" UserId="927" Text="&lt;public-databases&gt;&lt;sequencing&gt;&lt;exome&gt;" />
  <row Id="2600" PostHistoryTypeId="2" PostId="843" RevisionGUID="7b6f0505-c88b-411e-bb3d-191739acc08a" CreationDate="2017-06-21T02:49:42.480" UserId="96" Text="I've written a handful of programs from scratch to simulate mutations and variations in real or simulated sequences. The trick has always been to sort the variants by genomic coordinate, apply the variant with the largest coordinate first, then apply the variant with the second largest coordinate, all the way down to the variant with the smallest coordinate. Indels and other variants that affect sequence structure only affect *subsequent* coordinates, so going in reverse order ensures variants at the beginning of the sequence don't throw off variants at the end of the sequence." />
  <row Id="2601" PostHistoryTypeId="2" PostId="844" RevisionGUID="7b0a5ed2-3a4a-4763-96ee-abc5011af5fa" CreationDate="2017-06-21T04:38:04.053" UserId="734" Text="I have some difficulties to understand/interpret the [pathway map][1] and how a gene-gene interaction list or DNA sequencing can transform into such map.&#xD;&#xA;&#xD;&#xA;In addition what's the difference between [MARK/ERK pathway][1] and the output of [KEGG][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/MAPK/ERK_pathway&#xD;&#xA;  [2]: http://www.kegg.jp/kegg-bin/show_pathway?hsa05200" />
  <row Id="2602" PostHistoryTypeId="1" PostId="844" RevisionGUID="7b0a5ed2-3a4a-4763-96ee-abc5011af5fa" CreationDate="2017-06-21T04:38:04.053" UserId="734" Text="How to build a pathway map from bioinformatics data?" />
  <row Id="2603" PostHistoryTypeId="3" PostId="844" RevisionGUID="7b0a5ed2-3a4a-4763-96ee-abc5011af5fa" CreationDate="2017-06-21T04:38:04.053" UserId="734" Text="&lt;gene&gt;&lt;sequencing&gt;&lt;dna&gt;&lt;pathway&gt;&lt;kegg&gt;" />
  <row Id="2604" PostHistoryTypeId="4" PostId="838" RevisionGUID="b64f99ea-c7a4-4cca-9c2d-9e5e33acef82" CreationDate="2017-06-21T04:48:06.583" UserId="73" Comment="updated title to make it a bit more bioinformaticsy" Text="Range overlap python error with genomic regions" />
  <row Id="2605" PostHistoryTypeId="6" PostId="842" RevisionGUID="319a6a31-b538-47de-986e-ee5338e7abeb" CreationDate="2017-06-21T06:35:15.377" UserId="96" Comment="add variants tag" Text="&lt;public-databases&gt;&lt;variants&gt;&lt;sequencing&gt;&lt;exome&gt;" />
  <row Id="2606" PostHistoryTypeId="24" PostId="842" RevisionGUID="319a6a31-b538-47de-986e-ee5338e7abeb" CreationDate="2017-06-21T06:35:15.377" Comment="Proposed by 96 approved by 73, 77 edit id of 213" />
  <row Id="2607" PostHistoryTypeId="2" PostId="845" RevisionGUID="40694cd7-17e2-4d71-a4f0-771b773ab2ed" CreationDate="2017-06-21T08:07:33.170" UserId="48" Text="I am not sure if this has been done, is common, or are research lines but here is what I think it can be done with transcripts differences (aside from comparing the change of the expression of each transcript):&#xD;&#xA;&#xD;&#xA;Having the the different transcripts, enable to study also the relation between transcripts, which are co-expressed together or differently co-expressed. Maybe at individual level the switch between transcripts of a gene is not related to a condition but together with other transcripts it is.&#xD;&#xA;&#xD;&#xA;Transcripts usage could be also linked to structural or genomic changes. Either short/small changes or big. At genomic level SNP, inversions or methylation..., or structural changes such as compression of a region, different distances between chromosomes/regions... " />
  <row Id="2608" PostHistoryTypeId="2" PostId="846" RevisionGUID="4d1fd174-1499-4410-aecf-b1deaa0e55a6" CreationDate="2017-06-21T08:42:53.453" UserId="929" Text="Actually, by default, BLAST results are sorted in descending order by the &quot;Bit score&quot; (column 12). Sorting by E. value would not be quite the same.&#xD;&#xA;&#xD;&#xA;i.e. column 11 (E value) will be presented in ascending order (an E. value of 0 is as good as it gets), and as it gets higher, your hit is getting worse. Likewise, bitscores are highest at the top and decrease down the list. A hit with E. value = 0 should have a high score. How high exactly depends on the hit length, so it's common to see many E values with 0.0, but the scores differ. Here's a snippet of one of my blast results:&#xD;&#xA;&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  100.000  24180  0     0   1      24180  2233012  2257191  0.0  43606&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  78.098   5593   1164  16  2765   8323   3219956  3225521  0.0  4547&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  77.549   5394   1139  16  2948   8323   2376163  2370824  0.0  4260&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  74.140   5669   1342  34  8490   14100  3225688  3231290  0.0  3573&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  76.835   3665   825   9   16507  20150  3234191  3237852  0.0  2764&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  76.871   2646   571   11  16625  19248  2362597  2359971  0.0  2001&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  70.733   4131   1005  51  9047   13063  2368869  2364829  0.0  1954&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  75.040   1891   448   7   8500   10383  2370771  2368898  0.0  1272&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  78.986   1361   269   7   22766  24119  890051   891401   0.0  1150&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  77.819   1082   217   5   536    1610   3217983  3219048  0.0  868&#xD;&#xA;&#xD;&#xA;As you can see, they all have E values of 0.0, but they differ quite a lot in score, this is because of the length of the alignment, and the different percentage IDs.&#xD;&#xA;&#xD;&#xA;The reason for your appended sequences messing up the ordering, I would suspect, is that in your second sets of sequences, there were some better hits than in your first sets of sequences. Thus, they were sorted by the Bitscore and consequently your hits became interspersed." />
  <row Id="2609" PostHistoryTypeId="2" PostId="847" RevisionGUID="ed234691-ef6e-4186-aa3f-06fd22b49f0f" CreationDate="2017-06-21T08:45:44.693" UserId="818" Text="We believe that if after running blast, the global identity between a resulting sequence and our query is at least 30%, we can say that two sequences are homologs. what is the difference between local identity and global identity and how can we calculate them?" />
  <row Id="2610" PostHistoryTypeId="1" PostId="847" RevisionGUID="ed234691-ef6e-4186-aa3f-06fd22b49f0f" CreationDate="2017-06-21T08:45:44.693" UserId="818" Text="what is the difference between local identity and global identity for homolog finding?" />
  <row Id="2611" PostHistoryTypeId="3" PostId="847" RevisionGUID="ed234691-ef6e-4186-aa3f-06fd22b49f0f" CreationDate="2017-06-21T08:45:44.693" UserId="818" Text="&lt;sequence-homology&gt;" />
  <row Id="2612" PostHistoryTypeId="5" PostId="813" RevisionGUID="8e89b108-dd2c-4730-8a4f-953cbb1db01f" CreationDate="2017-06-21T09:08:15.343" UserId="492" Comment="Changed based on new understanding of complexity of tabix and grabix" Text="# Arbitrary record access in constant time&#xD;&#xA;&#xD;&#xA;To get a random record in constant time, it is sufficient to get an arbitrary record in constant time.&#xD;&#xA;&#xD;&#xA;I have two solutions here: One with `tabix` and one with `grabix`. I think the `grabix` solution is more elegant, but I am keeping the `tabix` solution below because `tabix` is a more mature tool than `grabix`.&#xD;&#xA; &#xD;&#xA;Thanks to [user172818](https://bioinformatics.stackexchange.com/users/37/user172818) for suggesting `grabix`.&#xD;&#xA;&#xD;&#xA;### Update&#xD;&#xA;&#xD;&#xA;This answer previously stated that `tabix` and `grabix` perform lookups in `log(n)` time. After taking a closer look at the grabix source code and the tabix paper I am now convinced that lookups are independent of `n` in complexity. However, both tools use an index that scales in size proportionally to `n`. So, the loading of the index is order `n`. However, if we consider the loading of the index as &quot;...a single limited transformation of the data to another file format...&quot;, then I think this answer is still is a valid one. If more than one record is to be retrieved, then the index needs to be stored in memory, perhaps with a framework such as pysam or htslib.&#xD;&#xA;&#xD;&#xA;## Using `grabix`&#xD;&#xA;&#xD;&#xA;1. Compress with `bgzip`.&#xD;&#xA;2. Index the file and perform lookups with [`grabix`](https://github.com/arq5x/grabix)&#xD;&#xA;&#xD;&#xA;In bash:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | bgzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;    grabix index output.fastq.gz&#xD;&#xA;&#xD;&#xA;    # retrieve 5-th record (1-based) in log(n) time&#xD;&#xA;    # requires some math to convert indices (4*4 + 1, 4*4 + 4) = (17, 20)&#xD;&#xA;    grabix grab output.fastq.gz 17 20&#xD;&#xA;&#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_LINES=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;&#xD;&#xA;## Using `tabix`&#xD;&#xA;&#xD;&#xA;The tabix code is more complicated and relies on the iffy assumption that `\t` is an acceptable character for replacement of `\n` in a FASTQ record.  If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&#xD;&#xA;&#xD;&#xA;1. Paste each record into a single line.&#xD;&#xA;2. Add a dummy chromosome and line number as the first and second column.&#xD;&#xA;3. Compress with `bgzip`.&#xD;&#xA;4. Index the file and perform lookups with [`tabix`](http://www.htslib.org/doc/tabix.html)&#xD;&#xA;&#xD;&#xA;Note that we need to remove leading spaces introduced by `nl` and we need to introduce a dummy chromosome column to keep `tabix` happy:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &gt; output.fastq.gz&#xD;&#xA;    tabix -s 1 -b 2 -e 2 output.fastq.gz &#xD;&#xA;    &#xD;&#xA;    # now retrieve the 5th record (1-based) in log(n) time&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 &#xD;&#xA;&#xD;&#xA;    # This command will retrieve the 5th record and convert it record back into FASTQ format&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&#xD;&#xA;    &#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_RECORDS=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;    &#xD;&#xA;# Random record in constant time&#xD;&#xA;&#xD;&#xA;Now that we have a way of retrieving an arbitrary record in `log(n)` time, retrieving a random record is simply a matter of getting a good random number generator and sampling. Here is some example code to do this in python:&#xD;&#xA;&#xD;&#xA;## Using grabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_LINES&quot;]) // 4&#xD;&#xA;    rand_record_start = random.randrange(0, n_records) * 4 + 1&#xD;&#xA;    rand_record_end = rand_record_start + 3&#xD;&#xA;    os.system(&quot;grabix grab output.fastq.gz {0} {1}&quot;.format(rand_record_start, rand_record_end))&#xD;&#xA;&#xD;&#xA;## Using tabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_RECORDS&quot;])&#xD;&#xA;    rand_record_index = random.randrange(0, n_records) + 1&#xD;&#xA;    # super ugly, but works...&#xD;&#xA;    os.system(&#xD;&#xA;        &quot;tabix output.fastq.gz dummy:{0}-{0} | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&quot;.format(&#xD;&#xA;            rand_record_index)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;And this works for me:&#xD;&#xA;&#xD;&#xA;    python3.5 random_read.py&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2613" PostHistoryTypeId="2" PostId="848" RevisionGUID="e816e490-6850-4590-9f3b-b43790ba684e" CreationDate="2017-06-21T09:15:19.583" UserId="931" Text="Defining homology isn't as simple as an identity cutoff, however, you're right that homology seems likely given a certain identity. This is where e-value comes into play. See:&#xD;&#xA;https://en.wikipedia.org/wiki/Expected_value&#xD;&#xA;&#xD;&#xA;To answer the question, global-global requires that the alignment account for every base in the query and the target sequence. local-global requires a the full length of the query on a part of the target. local-local allows only the highest scoring part of the alignment between the query and target to be reported. i.e. one functional domain against another. these hits are often called HSPs." />
  <row Id="2614" PostHistoryTypeId="2" PostId="849" RevisionGUID="d5c203a2-44b3-43a9-ab29-e1717c9144d1" CreationDate="2017-06-21T09:18:16.357" UserId="931" Text="For example, see this gene (nad1) in ENA:&#xD;&#xA;http://www.ebi.ac.uk/ena/data/view/ABI60879&#xD;&#xA;&#xD;&#xA;How to dump a 'non-canonically spliced' gene into GTF? i.e. what's the recommendation?" />
  <row Id="2615" PostHistoryTypeId="1" PostId="849" RevisionGUID="d5c203a2-44b3-43a9-ab29-e1717c9144d1" CreationDate="2017-06-21T09:18:16.357" UserId="931" Text="How to represent trans-spliced genes in GTF?" />
  <row Id="2616" PostHistoryTypeId="3" PostId="849" RevisionGUID="d5c203a2-44b3-43a9-ab29-e1717c9144d1" CreationDate="2017-06-21T09:18:16.357" UserId="931" Text="&lt;file-formats&gt;&lt;gtf&gt;" />
  <row Id="2617" PostHistoryTypeId="2" PostId="850" RevisionGUID="07772b95-0cbb-47c1-bb6f-fa2502b23709" CreationDate="2017-06-21T09:21:15.913" UserId="931" Text="See a project like Plant reactome, where they infer the pathways of all sequenced plant genomes using orthology to well annotated species.&#xD;&#xA;http://plantreactome.gramene.org/" />
  <row Id="2618" PostHistoryTypeId="2" PostId="851" RevisionGUID="46425d36-3bfa-4d9a-9277-d7b298a5aa16" CreationDate="2017-06-21T09:26:47.787" UserId="931" Text="Interesting question! Unfortunately I'm apparently too noob to comment, but not noob enough to answer...&#xD;&#xA;&#xD;&#xA;I gather that you want to use the background SNP frequency as a prior for input to your SNP calling algorithm?&#xD;&#xA;&#xD;&#xA;I'm not sure of a canned algorithm for doing this, but a quick google shows up some promising links:&#xD;&#xA;&#xD;&#xA; - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3593722/&#xD;&#xA; - https://software.broadinstitute.org/gatk/documentation/article.php?id=4723&#xD;&#xA; - https : // bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0489-0&#xD;&#xA;&#xD;&#xA;WHAT A JOKE! &quot;You need at least 10 reputation to post more than 2 links.&quot; THIS PLATFORM IS A JOKE!&#xD;&#xA;&#xD;&#xA;HTH!&#xD;&#xA;Dan." />
  <row Id="2619" PostHistoryTypeId="5" PostId="847" RevisionGUID="1b9c661c-ee44-4f97-900f-bbf175ff9809" CreationDate="2017-06-21T09:30:21.090" UserId="818" Comment="added 336 characters in body" Text="We believe that if after running blast, the global identity between a resulting sequence and our query is at least 30%, we can say that two sequences are homologs. what is the difference between local identity and global identity and how can we calculate them?&#xD;&#xA;the following file is the global alignment result of one of my PSI-BLAST hits against query:&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;the following file is the local alignment of that hit against query:[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/qRgS5.jpg&#xD;&#xA;  [2]: https://i.stack.imgur.com/7ZcH4.jpg" />
  <row Id="2620" PostHistoryTypeId="5" PostId="847" RevisionGUID="1a8fa361-66ed-4500-afe0-c3517710d0e9" CreationDate="2017-06-21T09:35:05.147" UserId="818" Comment="added 3006 characters in body" Text="We believe that if after running blast, the global identity between a resulting sequence and our query is at least 30%, we can say that two sequences are homologs. what is the difference between local identity and global identity and how can we calculate them?&#xD;&#xA;the following file is the global alignment result of one of my PSI-BLAST hits against query:&#xD;&#xA;&#xD;&#xA;Aligned_sequences: 2&#xD;&#xA;# 1: NP_418577.1&#xD;&#xA;# 2: WP_036312822.1&#xD;&#xA;# Matrix: EBLOSUM62&#xD;&#xA;# Gap_penalty: 10.0&#xD;&#xA;# Extend_penalty: 0.5&#xD;&#xA;#&#xD;&#xA;# Length: 264&#xD;&#xA;# Identity:      85/264 (32.2%)&#xD;&#xA;# Similarity:   125/264 (47.3%)&#xD;&#xA;# Gaps:          29/264 (11.0%)&#xD;&#xA;# Score: 376.5&#xD;&#xA;# &#xD;&#xA;#&#xD;&#xA;#=======================================&#xD;&#xA;&#xD;&#xA;NP_418577.1        1 -----------MAEMKNLKIE-------VVRYNPEVDTAPHSAFYEVPYD     32&#xD;&#xA;                                ..|..:..|:       :.|::||||..|....|:|...&#xD;&#xA;WP_036312822.      1 MTIVDSGAPADTQEANDSGIQSYLVTFIIRRFDPEVDAEPRWVDYDVEMY     50&#xD;&#xA;&#xD;&#xA;NP_418577.1       33 ATTSLLDALGYIKDNLAPDLSYRWSCRMAICGSCGMMVNNVPKLACKTFL     82&#xD;&#xA;                     .|..:||||..||.::...||:|.||...||||..|.:|...:|||||.:&#xD;&#xA;WP_036312822.     51 PTDRVLDALHRIKWDVDGTLSFRRSCAHGICGSDAMRINGRNRLACKTLI    100&#xD;&#xA;&#xD;&#xA;NP_418577.1       83 R--DYTDGMKVEALANFPIERDLVVDMTHFIESLEAIKPYIIGNSRTADQ    130&#xD;&#xA;                     :  |.:..:.|||:...|:|:||:|||..|.||...::|::...|.....&#xD;&#xA;WP_036312822.    101 KDLDISKPIYVEAIKGLPLEKDLIVDMDPFFESFRDVQPFLQPKSAPEPG    150&#xD;&#xA;&#xD;&#xA;NP_418577.1      131 GTNIQTPAQMAKYHQFSGCINCGLCYAACPQFGLNPEFIGPAAITLAHRY    180&#xD;&#xA;                     ....|:....|.|...:.||.|..|.::||.|..:.::.|||||..|||:&#xD;&#xA;WP_036312822.    151 KERFQSIKDRAVYDDTTKCILCAACTSSCPVFWTDGQYFGPAAIVNAHRF    200&#xD;&#xA;&#xD;&#xA;NP_418577.1      181 NEDSRDHGKKERMAQLNSQNGVWSCTFVGYCSEVCPKHVDPAAAIQQGKV    230&#xD;&#xA;                     ..||||.....|:..||.:.|||.|.....|:|.||:.::...||.:.|.&#xD;&#xA;WP_036312822.    201 IFDSRDDAADVRLDILNDKEGVWRCRTTFNCTEACPRGIEITKAIAEVKQ    250&#xD;&#xA;&#xD;&#xA;NP_418577.1      231 ESSKDFLIATLKPR    244&#xD;&#xA;                     ...:.         &#xD;&#xA;WP_036312822.    251 AVLRG---------    255&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;#---------------------------------------&#xD;&#xA;#---------------------------------------&#xD;&#xA;the following file is the local alignment of that hit against query:&#xD;&#xA;Aligned_sequences: 2&#xD;&#xA;# 1: NP_418577.1&#xD;&#xA;# 2: WP_036312822.1&#xD;&#xA;# Matrix: EBLOSUM62&#xD;&#xA;# Gap_penalty: 10.0&#xD;&#xA;# Extend_penalty: 0.5&#xD;&#xA;#&#xD;&#xA;# Length: 219&#xD;&#xA;# Identity:      83/219 (37.9%)&#xD;&#xA;# Similarity:   119/219 (54.3%)&#xD;&#xA;# Gaps:           2/219 ( 0.9%)&#xD;&#xA;# Score: 390.5&#xD;&#xA;# &#xD;&#xA;#&#xD;&#xA;#=======================================&#xD;&#xA;&#xD;&#xA;NP_418577.1       13 RYNPEVDTAPHSAFYEVPYDATTSLLDALGYIKDNLAPDLSYRWSCRMAI     62&#xD;&#xA;                     |::||||..|....|:|....|..:||||..||.::...||:|.||...|&#xD;&#xA;WP_036312822.     31 RFDPEVDAEPRWVDYDVEMYPTDRVLDALHRIKWDVDGTLSFRRSCAHGI     80&#xD;&#xA;&#xD;&#xA;NP_418577.1       63 CGSCGMMVNNVPKLACKTFLR--DYTDGMKVEALANFPIERDLVVDMTHF    110&#xD;&#xA;                     |||..|.:|...:|||||.::  |.:..:.|||:...|:|:||:|||..|&#xD;&#xA;WP_036312822.     81 CGSDAMRINGRNRLACKTLIKDLDISKPIYVEAIKGLPLEKDLIVDMDPF    130&#xD;&#xA;&#xD;&#xA;NP_418577.1      111 IESLEAIKPYIIGNSRTADQGTNIQTPAQMAKYHQFSGCINCGLCYAACP    160&#xD;&#xA;                     .||...::|::...|.........|:....|.|...:.||.|..|.::||&#xD;&#xA;WP_036312822.    131 FESFRDVQPFLQPKSAPEPGKERFQSIKDRAVYDDTTKCILCAACTSSCP    180&#xD;&#xA;&#xD;&#xA;NP_418577.1      161 QFGLNPEFIGPAAITLAHRYNEDSRDHGKKERMAQLNSQNGVWSCTFVGY    210&#xD;&#xA;                     .|..:.::.|||||..|||:..||||.....|:..||.:.|||.|.....&#xD;&#xA;WP_036312822.    181 VFWTDGQYFGPAAIVNAHRFIFDSRDDAADVRLDILNDKEGVWRCRTTFN    230&#xD;&#xA;&#xD;&#xA;NP_418577.1      211 CSEVCPKHVDPAAAIQQGK    229&#xD;&#xA;                     |:|.||:.::...||.:.|&#xD;&#xA;WP_036312822.    231 CTEACPRGIEITKAIAEVK    249&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;#---------------------------------------&#xD;&#xA;#---------------------------------------" />
  <row Id="2621" PostHistoryTypeId="2" PostId="852" RevisionGUID="5bee975d-c4e3-4270-91aa-4aeb69abac7a" CreationDate="2017-06-21T09:41:44.030" UserId="929" Text="If you want a purely alignment based approach to begin to _infer_ homology (note I say _infer_, as we stated in the comments, this isn't sufficient to say 2 sequences are homologs), I would use a Multiple Sequence Alignment (MSA) instead of a pairwise like you've done so far, and include as many similar proteins as you can from BLAST.&#xD;&#xA;&#xD;&#xA;The choice of algorithm for the MSA probably won't matter too much, as you will be looking for conserved regions anyway.&#xD;&#xA;&#xD;&#xA;Once you have your MSA, you can easily spot conserved domains. If anything is known about this protein structurally, it's quite trivial to work out whether a conserved region is a functional domain (active site, that kind of thing).&#xD;&#xA;&#xD;&#xA;With this information, you would be somewhat justified in calling these proteins structural homologs at least if they share domains, but, to reiterate, without doing phylogenetic analyses, you _cannot_ call them true homologs." />
  <row Id="2622" PostHistoryTypeId="5" PostId="849" RevisionGUID="de0ebadd-b2be-4622-b0b4-784e7d558990" CreationDate="2017-06-21T09:43:53.193" UserId="931" Comment="Edited to expand the example in line with a comment." Text="For example, see this gene (nad1) in ENA:&#xD;&#xA;http://www.ebi.ac.uk/ena/data/view/ABI60879&#xD;&#xA;&#xD;&#xA;If you look at the XML for that gene you see the following:&#xD;&#xA;&#xD;&#xA;    join(&#xD;&#xA;                 DQ984518.1: 324706 .. 325091 ,&#xD;&#xA;      complement(DQ984518.1:  24417 ..  24498),&#xD;&#xA;      complement(DQ984518.1:  22828 ..  23019),&#xD;&#xA;                 DQ984518.1:   3484 ..   3542 ,&#xD;&#xA;      complement(DQ984518.1: 153702 .. 153960)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;Which shows 5 exons joined out of phase and out of order. Is there a valid GTF representation of this?&#xD;&#xA;&#xD;&#xA;How to dump a 'non-canonically spliced' gene into GTF? i.e. what's the recommendation?" />
  <row Id="2623" PostHistoryTypeId="5" PostId="847" RevisionGUID="b4b3d126-e2d2-451f-9d4e-0ec682d47dd2" CreationDate="2017-06-21T09:58:00.247" UserId="298" Comment="Fixed formatting" Text="We believe that if after running blast, the global identity between a resulting sequence and our query is at least 30%, we can say that two sequences are homologs. what is the difference between local identity and global identity and how can we calculate them?&#xD;&#xA;the following file is the global alignment result of one of my PSI-BLAST hits against query:&#xD;&#xA;&#xD;&#xA;    Aligned_sequences: 2&#xD;&#xA;    # 1: NP_418577.1&#xD;&#xA;    # 2: WP_036312822.1&#xD;&#xA;    # Matrix: EBLOSUM62&#xD;&#xA;    # Gap_penalty: 10.0&#xD;&#xA;    # Extend_penalty: 0.5&#xD;&#xA;    #&#xD;&#xA;    # Length: 264&#xD;&#xA;    # Identity:      85/264 (32.2%)&#xD;&#xA;    # Similarity:   125/264 (47.3%)&#xD;&#xA;    # Gaps:          29/264 (11.0%)&#xD;&#xA;    # Score: 376.5&#xD;&#xA;    # &#xD;&#xA;    #&#xD;&#xA;    #=======================================&#xD;&#xA;    &#xD;&#xA;    NP_418577.1        1 -----------MAEMKNLKIE-------VVRYNPEVDTAPHSAFYEVPYD     32&#xD;&#xA;                                    ..|..:..|:       :.|::||||..|....|:|...&#xD;&#xA;    WP_036312822.      1 MTIVDSGAPADTQEANDSGIQSYLVTFIIRRFDPEVDAEPRWVDYDVEMY     50&#xD;&#xA;    &#xD;&#xA;    NP_418577.1       33 ATTSLLDALGYIKDNLAPDLSYRWSCRMAICGSCGMMVNNVPKLACKTFL     82&#xD;&#xA;                         .|..:||||..||.::...||:|.||...||||..|.:|...:|||||.:&#xD;&#xA;    WP_036312822.     51 PTDRVLDALHRIKWDVDGTLSFRRSCAHGICGSDAMRINGRNRLACKTLI    100&#xD;&#xA;    &#xD;&#xA;    NP_418577.1       83 R--DYTDGMKVEALANFPIERDLVVDMTHFIESLEAIKPYIIGNSRTADQ    130&#xD;&#xA;                         :  |.:..:.|||:...|:|:||:|||..|.||...::|::...|.....&#xD;&#xA;    WP_036312822.    101 KDLDISKPIYVEAIKGLPLEKDLIVDMDPFFESFRDVQPFLQPKSAPEPG    150&#xD;&#xA;    &#xD;&#xA;    NP_418577.1      131 GTNIQTPAQMAKYHQFSGCINCGLCYAACPQFGLNPEFIGPAAITLAHRY    180&#xD;&#xA;                         ....|:....|.|...:.||.|..|.::||.|..:.::.|||||..|||:&#xD;&#xA;    WP_036312822.    151 KERFQSIKDRAVYDDTTKCILCAACTSSCPVFWTDGQYFGPAAIVNAHRF    200&#xD;&#xA;    &#xD;&#xA;    NP_418577.1      181 NEDSRDHGKKERMAQLNSQNGVWSCTFVGYCSEVCPKHVDPAAAIQQGKV    230&#xD;&#xA;                         ..||||.....|:..||.:.|||.|.....|:|.||:.::...||.:.|.&#xD;&#xA;    WP_036312822.    201 IFDSRDDAADVRLDILNDKEGVWRCRTTFNCTEACPRGIEITKAIAEVKQ    250&#xD;&#xA;    &#xD;&#xA;    NP_418577.1      231 ESSKDFLIATLKPR    244&#xD;&#xA;                         ...:.         &#xD;&#xA;    WP_036312822.    251 AVLRG---------    255&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    #---------------------------------------&#xD;&#xA;    #---------------------------------------&#xD;&#xA;    the following file is the local alignment of that hit against query:&#xD;&#xA;    Aligned_sequences: 2&#xD;&#xA;    # 1: NP_418577.1&#xD;&#xA;    # 2: WP_036312822.1&#xD;&#xA;    # Matrix: EBLOSUM62&#xD;&#xA;    # Gap_penalty: 10.0&#xD;&#xA;    # Extend_penalty: 0.5&#xD;&#xA;    #&#xD;&#xA;    # Length: 219&#xD;&#xA;    # Identity:      83/219 (37.9%)&#xD;&#xA;    # Similarity:   119/219 (54.3%)&#xD;&#xA;    # Gaps:           2/219 ( 0.9%)&#xD;&#xA;    # Score: 390.5&#xD;&#xA;    # &#xD;&#xA;    #&#xD;&#xA;    #=======================================&#xD;&#xA;    &#xD;&#xA;    NP_418577.1       13 RYNPEVDTAPHSAFYEVPYDATTSLLDALGYIKDNLAPDLSYRWSCRMAI     62&#xD;&#xA;                         |::||||..|....|:|....|..:||||..||.::...||:|.||...|&#xD;&#xA;    WP_036312822.     31 RFDPEVDAEPRWVDYDVEMYPTDRVLDALHRIKWDVDGTLSFRRSCAHGI     80&#xD;&#xA;    &#xD;&#xA;    NP_418577.1       63 CGSCGMMVNNVPKLACKTFLR--DYTDGMKVEALANFPIERDLVVDMTHF    110&#xD;&#xA;                         |||..|.:|...:|||||.::  |.:..:.|||:...|:|:||:|||..|&#xD;&#xA;    WP_036312822.     81 CGSDAMRINGRNRLACKTLIKDLDISKPIYVEAIKGLPLEKDLIVDMDPF    130&#xD;&#xA;    &#xD;&#xA;    NP_418577.1      111 IESLEAIKPYIIGNSRTADQGTNIQTPAQMAKYHQFSGCINCGLCYAACP    160&#xD;&#xA;                         .||...::|::...|.........|:....|.|...:.||.|..|.::||&#xD;&#xA;    WP_036312822.    131 FESFRDVQPFLQPKSAPEPGKERFQSIKDRAVYDDTTKCILCAACTSSCP    180&#xD;&#xA;    &#xD;&#xA;    NP_418577.1      161 QFGLNPEFIGPAAITLAHRYNEDSRDHGKKERMAQLNSQNGVWSCTFVGY    210&#xD;&#xA;                         .|..:.::.|||||..|||:..||||.....|:..||.:.|||.|.....&#xD;&#xA;    WP_036312822.    181 VFWTDGQYFGPAAIVNAHRFIFDSRDDAADVRLDILNDKEGVWRCRTTFN    230&#xD;&#xA;    &#xD;&#xA;    NP_418577.1      211 CSEVCPKHVDPAAAIQQGK    229&#xD;&#xA;                         |:|.||:.::...||.:.|&#xD;&#xA;    WP_036312822.    231 CTEACPRGIEITKAIAEVK    249&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    #---------------------------------------&#xD;&#xA;    #---------------------------------------" />
  <row Id="2624" PostHistoryTypeId="24" PostId="847" RevisionGUID="b4b3d126-e2d2-451f-9d4e-0ec682d47dd2" CreationDate="2017-06-21T09:58:00.247" Comment="Proposed by 298 approved by 57, 818 edit id of 216" />
  <row Id="2625" PostHistoryTypeId="2" PostId="853" RevisionGUID="2d0541d8-2686-44ef-90df-530de316f12a" CreationDate="2017-06-21T10:17:57.267" UserId="931" Text="If I look at this record in GenBank I see about 6k genes:&#xD;&#xA;https://www.ncbi.nlm.nih.gov/nuccore/CM000760?report=gbwithparts&#xD;&#xA;&#xD;&#xA;I'd really like to be able to dump those genes in GFF3 format, but I'm guessing I can't do that in a single step, so...&#xD;&#xA;&#xD;&#xA; 1. How to dump all genes from a GenBank record in 'whatever' format?&#xD;&#xA; 2. How to convert 'whatever' format to GFF3?" />
  <row Id="2626" PostHistoryTypeId="1" PostId="853" RevisionGUID="2d0541d8-2686-44ef-90df-530de316f12a" CreationDate="2017-06-21T10:17:57.267" UserId="931" Text="How to dump genes from GenBank in GFF3 format?" />
  <row Id="2627" PostHistoryTypeId="3" PostId="853" RevisionGUID="2d0541d8-2686-44ef-90df-530de316f12a" CreationDate="2017-06-21T10:17:57.267" UserId="931" Text="&lt;gff3&gt;&lt;data-download&gt;" />
  <row Id="2628" PostHistoryTypeId="6" PostId="46" RevisionGUID="26d43edf-194f-4bc4-a0f6-3c8cceedc072" CreationDate="2017-06-21T10:24:47.297" UserId="131" Comment="removed useless tags" Text="&lt;genome&gt;&lt;transcriptome&gt;" />
  <row Id="2629" PostHistoryTypeId="24" PostId="46" RevisionGUID="26d43edf-194f-4bc4-a0f6-3c8cceedc072" CreationDate="2017-06-21T10:24:47.297" Comment="Proposed by 131 approved by 57, 77 edit id of 214" />
  <row Id="2630" PostHistoryTypeId="2" PostId="854" RevisionGUID="ad0e408f-c2be-4acf-98c3-e6542348e865" CreationDate="2017-06-21T10:27:10.710" UserId="678" Text="I found useful for my purpose [DisGeNET][1], a database that associates genes with diseases and, if known, gene variants with diseases. It integrates several sources of information for computing a score of association of gene/variation with the disease ( [Piñero et al., 2016][2], [Piñero et al., 2015][3]):&#xD;&#xA;    &#xD;&#xA;- **Gene Disease Associations** from UniProt, PsyGeNET, ClinVar, Orphanet, the GWAS Catalog, CTD (human data), and Human Phenotype Ontology, RGD, MGD, and CTD (mouse and rat data) GAD, LHGDN and BeFree&#xD;&#xA;&#xD;&#xA;- **Variants Disease Associations** from ClinVar, the GWAS Catalog, Uniprot, GAD, and from BEFREE&#xD;&#xA;&#xD;&#xA;If available, it provides the Pubmed ID of articles supporting the proposed association and the rs id from the dbSNP for variants associated to disease.&#xD;&#xA;&#xD;&#xA;In addition it provides an experimental R package, [disgenet2r][4], that allows to query the database in a programmatic manner and that provides some plot functions to generate representations of the associations founded.&#xD;&#xA;&#xD;&#xA;Using this approach, I have been able to found several human histone-disease associations (not my original purpose), together with some concrete histone residues that are mutated in specific diseases. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.disgenet.org/web/DisGeNET/menu&#xD;&#xA;  [2]: https://academic.oup.com/nar/article/45/D1/D833/2290909&#xD;&#xA;  [3]: http://www.ncbi.nlm.nih.gov/pubmed/25877637&#xD;&#xA;  [4]: https://bitbucket.org/ibi_group/disgenet2r" />
  <row Id="2631" PostHistoryTypeId="2" PostId="855" RevisionGUID="818df586-6707-42e6-a8c9-c1ae659771cc" CreationDate="2017-06-21T10:33:07.927" UserId="77" Text="To my knowledge there's no defined way to deal with that in GTF. [GFF3 handles trans-splicing](https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md) (you'll have to scroll down to &quot;trans-spliced transcript&quot;) by giving an individual transcript multiple parents (e.g., `ID=some_transspliced_gene;Parent=gene1,gene2`). You could use the same methodology with GTF files, but just note that it'll break most downstream programs." />
  <row Id="2632" PostHistoryTypeId="5" PostId="849" RevisionGUID="bf9330db-5da6-425b-af67-6afa110a969f" CreationDate="2017-06-21T10:33:52.827" UserId="77" Comment="added 138 characters in body" Text="For example, see this gene (nad1) in ENA:&#xD;&#xA;http://www.ebi.ac.uk/ena/data/view/ABI60879&#xD;&#xA;&#xD;&#xA;If you look at the XML for that gene you see the following:&#xD;&#xA;&#xD;&#xA;    join(&#xD;&#xA;                 DQ984518.1: 324706 .. 325091 ,&#xD;&#xA;      complement(DQ984518.1:  24417 ..  24498),&#xD;&#xA;      complement(DQ984518.1:  22828 ..  23019),&#xD;&#xA;                 DQ984518.1:   3484 ..   3542 ,&#xD;&#xA;      complement(DQ984518.1: 153702 .. 153960)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;Which shows 5 exons joined out of phase and out of order. Is there a valid GTF representation of this?&#xD;&#xA;&#xD;&#xA;How to dump a 'non-canonically spliced' gene into GTF? i.e. what's the recommendation?&#xD;&#xA;&#xD;&#xA;Cross-posted on [biostars](https://bioinformatics.stackexchange.com/questions/849/how-to-represent-trans-spliced-genes-in-gtf/855#855)" />
  <row Id="2633" PostHistoryTypeId="5" PostId="838" RevisionGUID="ced5538c-0f8a-49a0-b3cd-4d5a7b6487e4" CreationDate="2017-06-21T10:38:13.353" UserId="292" Comment="Added bed and homework tags, renamed variables, corrected expected output" Text="I have two files &#xD;&#xA;&#xD;&#xA;    s3.txt :&#xD;&#xA;    1	10	20&#xD;&#xA;    1	5	20&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    1	10	50&#xD;&#xA;    2	20	60&#xD;&#xA;    1	14	17&#xD;&#xA;&#xD;&#xA;    s4.txt:&#xD;&#xA;    1	10	20&#xD;&#xA;    2	20	30&#xD;&#xA;&#xD;&#xA;I am trying to match col0 of both the files and get rows that fall between range(inclusive of themselves) 10-20 and 20-30 as seen in s4 file.&#xD;&#xA;&#xD;&#xA;code so far:&#xD;&#xA;&#xD;&#xA;    containing_ranges = []&#xD;&#xA;    with open('s4.txt', 'r') as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            fields = line.strip().split('\t')&#xD;&#xA;            containing_ranges.append(fields)&#xD;&#xA;    &#xD;&#xA;    tested_ranges = []       &#xD;&#xA;    with open('s3.txt', 'r') as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            fields = line.strip().split('\t')&#xD;&#xA;            tested_ranges.append(fields)&#xD;&#xA;    &#xD;&#xA;    for c_range in containing_ranges:&#xD;&#xA;        for t_range in tested_ranges:&#xD;&#xA;            tst = int(t_range[1])&#xD;&#xA;            ten = int(t_range[2])&#xD;&#xA;            cst = int(c_range[1])&#xD;&#xA;            cen = int(c_range[2])&#xD;&#xA;            if  c_range[0] == t_range[0]:&#xD;&#xA;                included = cst &gt;= tst and cen &lt;= ten&#xD;&#xA;                if included == True:&#xD;&#xA;                   print t_range&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;Output with missing row(1	14	17) : &#xD;&#xA;&#xD;&#xA;    ['1', '10', '20']&#xD;&#xA;    ['1', '5', '20']&#xD;&#xA;    ['1', '10', '50']&#xD;&#xA;    ['2', '20', '30']&#xD;&#xA;    ['2', '20', '60']&#xD;&#xA;&#xD;&#xA;Desired output:&#xD;&#xA;&#xD;&#xA;    1	10	20&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    1	14	17&#xD;&#xA;&#xD;&#xA;Not sure if my logic is wrong and why does it miss 14-17 as it falls between 10-20&#xD;&#xA;    &#xD;&#xA;    [EDIT] using pybedtools&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; print(s4.intersect(s3, wb=True))&#xD;&#xA;    1	10	20	1	10	20&#xD;&#xA;    1	10	20	1	5	20&#xD;&#xA;    1	10	20	1	10	50&#xD;&#xA;    1	14	17	1	14	17&#xD;&#xA;    2	20	30	2	20	30&#xD;&#xA;    2	25	30	2	25	30&#xD;&#xA;    2	20	30	2	20	60&#xD;&#xA;    &#xD;&#xA;    &gt;&gt;&gt; print(s4.intersect(s3, wa=True, wb=True, F=1))&#xD;&#xA;    1	10	20	1	10	20&#xD;&#xA;    1	10	20	1	14	17&#xD;&#xA;    2	20	30	2	20	30&#xD;&#xA;    2	20	30	2	25	30&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2634" PostHistoryTypeId="6" PostId="838" RevisionGUID="ced5538c-0f8a-49a0-b3cd-4d5a7b6487e4" CreationDate="2017-06-21T10:38:13.353" UserId="292" Comment="Added bed and homework tags, renamed variables, corrected expected output" Text="&lt;bed&gt;&lt;genomics&gt;&lt;python&gt;&lt;homework&gt;" />
  <row Id="2635" PostHistoryTypeId="24" PostId="838" RevisionGUID="ced5538c-0f8a-49a0-b3cd-4d5a7b6487e4" CreationDate="2017-06-21T10:38:13.353" Comment="Proposed by 292 approved by 77, 57 edit id of 215" />
  <row Id="2636" PostHistoryTypeId="2" PostId="856" RevisionGUID="a33bc3d4-1e28-481a-9e32-0e9d9bbb7b71" CreationDate="2017-06-21T10:57:29.787" UserId="223" Text="I have a set of coordinates with SNPs from a bacterial genome and want to find if the SNPs are located inside a gene, is there some tool where you can pass a coordinate and a gff or gbk file and it returns the name of the gene?" />
  <row Id="2637" PostHistoryTypeId="1" PostId="856" RevisionGUID="a33bc3d4-1e28-481a-9e32-0e9d9bbb7b71" CreationDate="2017-06-21T10:57:29.787" UserId="223" Text="Find gene at position from gff-file" />
  <row Id="2638" PostHistoryTypeId="3" PostId="856" RevisionGUID="a33bc3d4-1e28-481a-9e32-0e9d9bbb7b71" CreationDate="2017-06-21T10:57:29.787" UserId="223" Text="&lt;snp&gt;" />
  <row Id="2639" PostHistoryTypeId="2" PostId="857" RevisionGUID="727d88ef-4adf-41c3-98b1-f81c27c6d3ca" CreationDate="2017-06-21T11:08:11.633" UserId="499" Text="I have a gene expression dataset that I want to investigate. Particularly, I would like to understand whether there is any correlation between each gene's expression and some quantitative or qualtitative data (say, correlation between gene 'XPTO' , body mass index, and race). &#xD;&#xA;&#xD;&#xA;One possible way to test this would be through logistic regression, but is this a good approach or are there caveats that I should know about using such a statistic?&#xD;&#xA;&#xD;&#xA;My question is the following: which methods would you advise to measure such correlations, and why?&#xD;&#xA;&#xD;&#xA;(this post was [crossposted on Biostars][1])&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/258889/" />
  <row Id="2640" PostHistoryTypeId="1" PostId="857" RevisionGUID="727d88ef-4adf-41c3-98b1-f81c27c6d3ca" CreationDate="2017-06-21T11:08:11.633" UserId="499" Text="Correlating gene expression with qualitative variables" />
  <row Id="2641" PostHistoryTypeId="3" PostId="857" RevisionGUID="727d88ef-4adf-41c3-98b1-f81c27c6d3ca" CreationDate="2017-06-21T11:08:11.633" UserId="499" Text="&lt;rna-seq&gt;&lt;r&gt;" />
  <row Id="2642" PostHistoryTypeId="2" PostId="858" RevisionGUID="63a6e173-4de5-4ca0-8abb-3f849d1fd0ff" CreationDate="2017-06-21T11:10:55.837" UserId="776" Text="Via [BEDOPS][1]:&#xD;&#xA;&#xD;&#xA;    $ gff2bed &lt; annotations.gff &gt; annotations.bed&#xD;&#xA;    $ vcf2bed &lt; snps.vcf &gt; snps.bed&#xD;&#xA;    $ bedmap --echo --echo-map-id-uniq snps.bed annotations.bed &gt; answer.bed&#xD;&#xA;&#xD;&#xA;This can be reduced to a one-liner if you're using `bash`:&#xD;&#xA;&#xD;&#xA;    $ bedmap --echo --echo-map-id-uniq &lt;(vcf2bed &lt; snps.vcf) &lt;(gff2bed &lt; annotations.gff) &gt; answer.bed&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/bedops/bedops" />
  <row Id="2643" PostHistoryTypeId="2" PostId="859" RevisionGUID="75433782-06d0-4e92-b692-d3ba95575896" CreationDate="2017-06-21T11:44:01.653" UserId="77" Text="Logistic regression would generally be a bad choice for non-binary outcomes. In such cases, linear regression (or a GLM more generally) still works fine. You can already do that in the standard R packages for RNAseq (DESeq2, edgeR, and limma), where the fold-change is in whatever units you're measuring your quantitative trait in." />
  <row Id="2644" PostHistoryTypeId="5" PostId="856" RevisionGUID="073b90c2-3a0f-40b1-b1a9-8eac46a96e06" CreationDate="2017-06-21T11:45:19.827" UserId="223" Comment="deleted 8 characters in body; edited title" Text="I have a VCF file with SNPs from a bacterial genome and want to find if the SNPs are located inside genes, is there some CLI-tool where you can pass a VCF file and a gff or gbk file and it returns the name of the genes?" />
  <row Id="2645" PostHistoryTypeId="4" PostId="856" RevisionGUID="073b90c2-3a0f-40b1-b1a9-8eac46a96e06" CreationDate="2017-06-21T11:45:19.827" UserId="223" Comment="deleted 8 characters in body; edited title" Text="Find gene at position from gff or gbk file" />
  <row Id="2646" PostHistoryTypeId="2" PostId="860" RevisionGUID="17418aa7-745d-4c74-a66a-e36bf6b1d491" CreationDate="2017-06-21T12:19:12.830" UserId="818" Text="We can calculate the distance between residues in a PDB file regarding different parameters like closest atoms, alpha carbon, beta carbon, centroid and etc. Which one of these parameters are better to show physical interaction between residues in a PDB file?" />
  <row Id="2647" PostHistoryTypeId="1" PostId="860" RevisionGUID="17418aa7-745d-4c74-a66a-e36bf6b1d491" CreationDate="2017-06-21T12:19:12.830" UserId="818" Text="Best distance parameter for estimating physical interaction between residues in a PDB file" />
  <row Id="2648" PostHistoryTypeId="3" PostId="860" RevisionGUID="17418aa7-745d-4c74-a66a-e36bf6b1d491" CreationDate="2017-06-21T12:19:12.830" UserId="818" Text="&lt;protein-structure&gt;&lt;pdb&gt;" />
  <row Id="2649" PostHistoryTypeId="2" PostId="861" RevisionGUID="9f5f1252-d0cf-400b-b4cc-c7474d4e25c5" CreationDate="2017-06-21T12:36:08.063" UserId="215" Text="As [wkretzsch](https://bioinformatics.stackexchange.com/users/492/wkretzsch) suggested this was worthy of an actual answer, I feel the obvious solution is missing here; index the `FASTQ`.&#xD;&#xA;&#xD;&#xA;Index it&#xD;&#xA;--------&#xD;&#xA;&#xD;&#xA;As much as I typically hesitate to jump to a solution that requires a script or framework (as opposed to unix tools), a quick and dirty approach might be to use `biopython`, seeing as it [already has this functionality implemented](http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc291) to do this, and if installed, is trivial to use:&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    fq = SeqIO.index(&quot;myfastq.fq&quot;, &quot;fastq&quot;)&#xD;&#xA;&#xD;&#xA;Once you've acquired an index, you'll get fast random access for any read:&#xD;&#xA;&#xD;&#xA;    # Random access by read name (I like owls)&#xD;&#xA;    record = fq[&quot;HOOT&quot;]&#xD;&#xA;    record&#xD;&#xA;    #&gt; SeqRecord(seq=Seq('ACGTACGT', SingleLetterAlphabet()), id='HOOT', name='HOOT', description='HOOT', dbxrefs=[])&#xD;&#xA;&#xD;&#xA;    # We can get the sequence&#xD;&#xA;    record.seq&#xD;&#xA;    #&gt; Seq('ACGTACGT', SingleLetterAlphabet())&#xD;&#xA;&#xD;&#xA;    # and qualities&#xD;&#xA;    record.letter_annotations&#xD;&#xA;    #&gt; {'phred_quality': [41, 41, 41, 41, 41, 41, 41, 41]}&#xD;&#xA;&#xD;&#xA;If you want to select arbitrary random records, you could use something like [`randrange`](https://docs.python.org/3.1/library/random.html#random.randrange) to select between 0 and the length of the references list.&#xD;&#xA;&#xD;&#xA;    from random import randint&#xD;&#xA;&#xD;&#xA;    # Coerce keys to a list, as `dictionary-keyiterator` has no&#xD;&#xA;    #   __getitem__ attribute that would allow integer indices&#xD;&#xA;    # Note also that this doesn't necessarily guarantee a sorted order&#xD;&#xA;    #  but I guess that doesn't matter if you just want random records&#xD;&#xA;    key_list = list(fq.keys())&#xD;&#xA;&#xD;&#xA;    # Select a random key&#xD;&#xA;    # Note we use len(key_list)-1 as randint endpoints are inclusive&#xD;&#xA;    random_readname = key_list[ randint(0, len(key_list)-1) ]&#xD;&#xA;&#xD;&#xA;    # Get your record&#xD;&#xA;    rand_record = fq.get( random_readname )&#xD;&#xA;&#xD;&#xA;If you want multiple records, you'll probably want [`sample`](https://docs.python.org/2/library/random.html#random.sample) (to avoid replacement) instead:&#xD;&#xA;&#xD;&#xA;    from random import sample&#xD;&#xA;    N = 100    &#xD;&#xA;    random_indices = sample(xrange(len(key_list)), N)&#xD;&#xA;&#xD;&#xA;    for key_i in random_indices:&#xD;&#xA;        random_readname = key_list[ key_i ]&#xD;&#xA;        rand_record = fq.get( random_readname )&#xD;&#xA;        # ...&#xD;&#xA;&#xD;&#xA;For what it's worth, I think `biopython` holds this index in RAM, so if your file is absolutely massive, you might need to be more clever. If that's the case, you could iterate through the `FASTQ` once, and output the readname, file offset and length - akin to a `FASTA` `fai`." />
  <row Id="2650" PostHistoryTypeId="5" PostId="854" RevisionGUID="b171c35d-eeeb-413a-b733-62ab02fff35d" CreationDate="2017-06-21T12:40:37.340" UserId="678" Comment="edited body" Text="I found [DisGeNET][1] useful for my purpose, a database that associates genes with diseases and, if known, gene variants with diseases. It integrates several sources of information for computing a score of association of gene/variation with the disease ( [Piñero et al., 2016][2], [Piñero et al., 2015][3]):&#xD;&#xA;    &#xD;&#xA;- **Gene Disease Associations** from UniProt, PsyGeNET, ClinVar, Orphanet, the GWAS Catalog, CTD (human data), and Human Phenotype Ontology, RGD, MGD, and CTD (mouse and rat data) GAD, LHGDN and BeFree&#xD;&#xA;&#xD;&#xA;- **Variants Disease Associations** from ClinVar, the GWAS Catalog, Uniprot, GAD, and from BEFREE&#xD;&#xA;&#xD;&#xA;If available, it provides the Pubmed ID of articles supporting the proposed association and the rs id from the dbSNP for variants associated to disease.&#xD;&#xA;&#xD;&#xA;In addition it provides an experimental R package, [disgenet2r][4], that allows to query the database in a programmatic manner and that provides some plot functions to generate representations of the associations founded.&#xD;&#xA;&#xD;&#xA;Using this approach, I have been able to found several human histone-disease associations (not my original purpose), together with some concrete histone residues that are mutated in specific diseases. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.disgenet.org/web/DisGeNET/menu&#xD;&#xA;  [2]: https://academic.oup.com/nar/article/45/D1/D833/2290909&#xD;&#xA;  [3]: http://www.ncbi.nlm.nih.gov/pubmed/25877637&#xD;&#xA;  [4]: https://bitbucket.org/ibi_group/disgenet2r" />
  <row Id="2651" PostHistoryTypeId="5" PostId="861" RevisionGUID="2b4368a8-cf73-4b10-9d37-942e4a10e913" CreationDate="2017-06-21T12:46:37.197" UserId="215" Comment="added 275 characters in body" Text="As [wkretzsch](https://bioinformatics.stackexchange.com/users/492/wkretzsch) suggested this was worthy of an actual answer, I feel the obvious solution is missing here; index the `FASTQ`.&#xD;&#xA;&#xD;&#xA;Index it&#xD;&#xA;--------&#xD;&#xA;&#xD;&#xA;As much as I typically hesitate to jump to a solution that requires a script or framework (as opposed to just unix command line tools), there is sadly no `samtools fqidx` (perhaps there should be), and existing answers suggest a lot of munging. Whilst they probably work, some appear cumbersome and have many steps in which you could make a mistake.&#xD;&#xA;&#xD;&#xA;So, to keep things simple - a quick and dirty alternative approach might be to use `biopython`, seeing as it [already has this functionality implemented](http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc291) to do this, and if installed, is trivial to use:&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    fq = SeqIO.index(&quot;myfastq.fq&quot;, &quot;fastq&quot;)&#xD;&#xA;&#xD;&#xA;Once you've acquired an index, you'll get fast random access for any read:&#xD;&#xA;&#xD;&#xA;    # Random access by read name (I like owls)&#xD;&#xA;    record = fq[&quot;HOOT&quot;]&#xD;&#xA;    record&#xD;&#xA;    #&gt; SeqRecord(seq=Seq('ACGTACGT', SingleLetterAlphabet()), id='HOOT', name='HOOT', description='HOOT', dbxrefs=[])&#xD;&#xA;&#xD;&#xA;    # We can get the sequence&#xD;&#xA;    record.seq&#xD;&#xA;    #&gt; Seq('ACGTACGT', SingleLetterAlphabet())&#xD;&#xA;&#xD;&#xA;    # and qualities&#xD;&#xA;    record.letter_annotations&#xD;&#xA;    #&gt; {'phred_quality': [41, 41, 41, 41, 41, 41, 41, 41]}&#xD;&#xA;&#xD;&#xA;If you want to select arbitrary random records, you could use something like [`randrange`](https://docs.python.org/3.1/library/random.html#random.randrange) to select between 0 and the length of the references list.&#xD;&#xA;&#xD;&#xA;    from random import randint&#xD;&#xA;&#xD;&#xA;    # Coerce keys to a list, as `dictionary-keyiterator` has no&#xD;&#xA;    #   __getitem__ attribute that would allow integer indices&#xD;&#xA;    # Note also that this doesn't necessarily guarantee a sorted order&#xD;&#xA;    #  but I guess that doesn't matter if you just want random records&#xD;&#xA;    key_list = list(fq.keys())&#xD;&#xA;&#xD;&#xA;    # Select a random key&#xD;&#xA;    # Note we use len(key_list)-1 as randint endpoints are inclusive&#xD;&#xA;    random_readname = key_list[ randint(0, len(key_list)-1) ]&#xD;&#xA;&#xD;&#xA;    # Get your record&#xD;&#xA;    rand_record = fq.get( random_readname )&#xD;&#xA;&#xD;&#xA;If you want multiple records, you'll probably want [`sample`](https://docs.python.org/2/library/random.html#random.sample) (to avoid replacement) instead:&#xD;&#xA;&#xD;&#xA;    from random import sample&#xD;&#xA;    N = 100    &#xD;&#xA;    random_indices = sample(xrange(len(key_list)), N)&#xD;&#xA;&#xD;&#xA;    for key_i in random_indices:&#xD;&#xA;        random_readname = key_list[ key_i ]&#xD;&#xA;        rand_record = fq.get( random_readname )&#xD;&#xA;        # ...&#xD;&#xA;&#xD;&#xA;For what it's worth, I think `biopython` holds this index in RAM, so if your file is absolutely massive, you might need to be more clever. If that's the case, you could iterate through the `FASTQ` once, and output the readname, file offset and length - akin to a `FASTA` `fai`." />
  <row Id="2653" PostHistoryTypeId="2" PostId="862" RevisionGUID="a5fef94f-40cf-4bc1-b92e-60fe87a6ab12" CreationDate="2017-06-21T13:27:37.420" UserId="931" Text="Turns out you can just grab the GFF3 from the NCBI's FTP site!&#xD;&#xA;&#xD;&#xA;ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/003/195/GCA_000003195.3_Sorghum_bicolor_NCBIv3/&#xD;&#xA;&#xD;&#xA;See:&#xD;&#xA;&#xD;&#xA; - https://www.ncbi.nlm.nih.gov/genome/doc/ftpfaq/#files&#xD;&#xA;&#xD;&#xA;Or &quot;Access the data&quot; on the right here:&#xD;&#xA;&#xD;&#xA; - https://www.ncbi.nlm.nih.gov/assembly/GCF_000003195.3" />
  <row Id="2654" PostHistoryTypeId="2" PostId="863" RevisionGUID="7c9e2d5b-0fc3-4446-a20a-5281a9579057" CreationDate="2017-06-21T13:36:08.693" UserId="931" Text="Tried looking for an explicit database? i.e.&#xD;&#xA;&#xD;&#xA; - ComSin: database of protein structures in bound (complex) and unbound (single) states in relation to their intrinsic disorder: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2808974/ or&#xD;&#xA; - LigASite: a database of biologically relevant binding sites in proteins with known apo-structures https://www.ncbi.nlm.nih.gov/pubmed/17933762&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Else the PDB itself has a very powerful advanced search that should allow you to do this: http://www.rcsb.org/pdb/search/advSearch.do" />
  <row Id="2655" PostHistoryTypeId="2" PostId="864" RevisionGUID="147ac0d2-8ae6-44a2-8b9d-c934e13caba7" CreationDate="2017-06-21T13:38:50.203" UserId="931" Text="You should read my paper, where I did a comprehensive parameter sweep across different thresholds and definitions and benchmarked the resulting empirical potentials on a decoy ranking challenge:&#xD;&#xA;&#xD;&#xA;https://bmcstructbiol.biomedcentral.com/articles/10.1186/1472-6807-8-53&#xD;&#xA;&#xD;&#xA;The relevant phrase from the abstract is &quot;compared using 90 different definitions of residue-residue contact&quot;. However, it boils down (as always) to the question, &quot;what do you want to do&quot;.&#xD;&#xA;&#xD;&#xA;No need to thank me... JUST CITE ME!!!" />
  <row Id="2656" PostHistoryTypeId="2" PostId="865" RevisionGUID="39c6d0b1-7812-41c2-98e6-663393d9cf13" CreationDate="2017-06-21T13:48:17.480" UserId="931" Text="Use a distance cutoff of 12Å between Cᵦ atoms." />
  <row Id="2657" PostHistoryTypeId="5" PostId="851" RevisionGUID="61e694c9-7159-451c-889a-1e16f393bc0d" CreationDate="2017-06-21T14:00:21.773" UserId="292" Comment="Made third url a link, removed some comments that are not really usful for the answer" Text="I gather that you want to use the background SNP frequency as a prior for input to your SNP calling algorithm?&#xD;&#xA;&#xD;&#xA;I'm not sure of a canned algorithm for doing this, but a quick google shows up some promising links:&#xD;&#xA;&#xD;&#xA; - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3593722/&#xD;&#xA; - https://software.broadinstitute.org/gatk/documentation/article.php?id=4723&#xD;&#xA; - https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0489-0&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2658" PostHistoryTypeId="24" PostId="851" RevisionGUID="61e694c9-7159-451c-889a-1e16f393bc0d" CreationDate="2017-06-21T14:00:21.773" Comment="Proposed by 292 approved by 77, 57 edit id of 217" />
  <row Id="2659" PostHistoryTypeId="11" PostId="838" RevisionGUID="d7ed9078-1182-44ae-8cf5-20b322dc41c8" CreationDate="2017-06-21T14:05:05.790" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:37,&quot;DisplayName&quot;:&quot;user172818&quot;},{&quot;Id&quot;:73,&quot;DisplayName&quot;:&quot;gringer&quot;},{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;},{&quot;Id&quot;:292,&quot;DisplayName&quot;:&quot;bli&quot;},{&quot;Id&quot;:57,&quot;DisplayName&quot;:&quot;Kamil S Jaron&quot;}]}" />
  <row Id="2664" PostHistoryTypeId="2" PostId="867" RevisionGUID="e861b5f6-c364-4cb7-98ae-688c9981fbe6" CreationDate="2017-06-21T14:46:48.633" UserId="425" Text="This is now easy to do with [RNFtools](http://karel-brinda.github.io/rnftools/) (from version 0.3.1). See the [tutorial](http://rnftools.readthedocs.io/en/latest/tutorial.html), especially section [Sequence extraction](http://rnftools.readthedocs.io/en/latest/tutorial/02_simulation.html#sequence-extraction).&#xD;&#xA;&#xD;&#xA;**Environment preparation**&#xD;&#xA;&#xD;&#xA;First, install [BioConda and add the required channels](https://bioconda.github.io/). Then either install RNFtools in the default Conda environment&#xD;&#xA;&#xD;&#xA;    conda install rnftools&#xD;&#xA;&#xD;&#xA;or create and activate a separate Conda environment (preferable)&#xD;&#xA;&#xD;&#xA;    conda create -n rnftools rnftools&#xD;&#xA;    source activate rnftools&#xD;&#xA;&#xD;&#xA;**Simulation**&#xD;&#xA;&#xD;&#xA;Assume that you have a reference file `ref.fa` and a coverage file `coverage.tsv` (from your example). Then the following `Snakefile` for RNFtools will do the job you want:&#xD;&#xA;&#xD;&#xA;    import rnftools&#xD;&#xA;    import csv&#xD;&#xA;&#xD;&#xA;    rnftools.mishmash.sample(&quot;simulation_with_coverage_control&quot;, reads_in_tuple=1)&#xD;&#xA;&#xD;&#xA;    fa = &quot;ref.fa&quot;&#xD;&#xA;    tsv = &quot;coverage.tsv&quot;&#xD;&#xA;&#xD;&#xA;    with open(tsv) as f:&#xD;&#xA;    	table = csv.reader(f, delimiter='\t')&#xD;&#xA;    	for seqname, cov in table:&#xD;&#xA;&#xD;&#xA;    		rnftools.mishmash.DwgSim(&#xD;&#xA;    			fasta=fa,&#xD;&#xA;    			sequences=[seqname],&#xD;&#xA;    			coverage=int(cov),&#xD;&#xA;    			read_length_1=10, # quick test with supershort reads&#xD;&#xA;    			read_length_2=0,&#xD;&#xA;    		)&#xD;&#xA;&#xD;&#xA;    include: rnftools.include()&#xD;&#xA;    rule: input: rnftools.input()&#xD;&#xA;&#xD;&#xA;When you save this file (`Snakefile`) and run `snakemake`, RNFtools will simulate reads using DWGsim with coverages defined your text file, and save all the simulated reads in `simulation_with_coverage_control.fq`.&#xD;&#xA;&#xD;&#xA;You can play with all the parameters. In particular, you can use a different simulator (e.g., Art-Illumina using `rnftools.mishmash.ArtIllumina`). See the [RNFtools documentation](http://rnftools.rtfd.org) for more information.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2665" PostHistoryTypeId="5" PostId="838" RevisionGUID="c1e49d89-7d5c-404a-a697-504d17b6cc13" CreationDate="2017-06-21T14:47:38.947" UserId="926" Comment="added 642 characters in body" Text="I have two files &#xD;&#xA;&#xD;&#xA;    s3.txt :&#xD;&#xA;    1	10	20&#xD;&#xA;    1	5	20&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    1	10	50&#xD;&#xA;    2	20	60&#xD;&#xA;    1	14	17&#xD;&#xA;&#xD;&#xA;    s4.txt:&#xD;&#xA;    1	10	20&#xD;&#xA;    2	20	30&#xD;&#xA;&#xD;&#xA;I am trying to match col0 of both the files and get rows that fall between range(inclusive of themselves) 10-20 and 20-30 as seen in s4 file.&#xD;&#xA;file s4 has co ordinates which can be used as reference range (chrom start and end) and s3 has list of co ordinates from an experimental condition what I am trying to achieve is to which co-ordinates from my file s3 fall on or between my reference co ordinates in s4.  &#xD;&#xA;&#xD;&#xA;code so far:&#xD;&#xA;&#xD;&#xA;    containing_ranges = []&#xD;&#xA;    with open('s4.txt', 'r') as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            fields = line.strip().split('\t')&#xD;&#xA;            containing_ranges.append(fields)&#xD;&#xA;    &#xD;&#xA;    tested_ranges = []       &#xD;&#xA;    with open('s3.txt', 'r') as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            fields = line.strip().split('\t')&#xD;&#xA;            tested_ranges.append(fields)&#xD;&#xA;    &#xD;&#xA;    for c_range in containing_ranges:&#xD;&#xA;        for t_range in tested_ranges:&#xD;&#xA;            tst = int(t_range[1])&#xD;&#xA;            ten = int(t_range[2])&#xD;&#xA;            cst = int(c_range[1])&#xD;&#xA;            cen = int(c_range[2])&#xD;&#xA;            if  c_range[0] == t_range[0]:&#xD;&#xA;                included = cst &gt;= tst and cen &lt;= ten&#xD;&#xA;                if included == True:&#xD;&#xA;                   print t_range&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;Output with missing row(1	14	17) : &#xD;&#xA;&#xD;&#xA;    ['1', '10', '20']&#xD;&#xA;    ['1', '5', '20']&#xD;&#xA;    ['1', '10', '50']&#xD;&#xA;    ['2', '20', '30']&#xD;&#xA;    ['2', '20', '60']&#xD;&#xA;&#xD;&#xA;Desired output:&#xD;&#xA;&#xD;&#xA;    1	10	20&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    1	14	17&#xD;&#xA;&#xD;&#xA;Not sure if my logic is wrong and why does it miss 14-17 as it falls between 10-20&#xD;&#xA;    &#xD;&#xA;    [EDIT] using pybedtools&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; print(s4.intersect(s3, wb=True))&#xD;&#xA;    1	10	20	1	10	20&#xD;&#xA;    1	10	20	1	5	20&#xD;&#xA;    1	10	20	1	10	50&#xD;&#xA;    1	14	17	1	14	17&#xD;&#xA;    2	20	30	2	20	30&#xD;&#xA;    2	25	30	2	25	30&#xD;&#xA;    2	20	30	2	20	60&#xD;&#xA;    &#xD;&#xA;    &gt;&gt;&gt; print(s4.intersect(s3, wa=True, wb=True, F=1))&#xD;&#xA;    1	10	20	1	10	20&#xD;&#xA;    1	10	20	1	14	17&#xD;&#xA;    2	20	30	2	20	30&#xD;&#xA;    2	20	30	2	25	30&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    using bedops &#xD;&#xA;    bin$ less answer.bed &#xD;&#xA;    1       5       20&#xD;&#xA;    1       10      20&#xD;&#xA;    1       10      50&#xD;&#xA;    1       14      17&#xD;&#xA;    2       20      30&#xD;&#xA;    2       20      60&#xD;&#xA;    2       25      30&#xD;&#xA;&#xD;&#xA;    using @bli code(on python2.7)&#xD;&#xA;    ('1', 10, 20)&#xD;&#xA;    ('1', 14, 17)&#xD;&#xA;    ('2', 20, 30)&#xD;&#xA;    ('2', 25, 30)&#xD;&#xA;     why can I not see the interval 1 5 20&#xD;&#xA;" />
  <row Id="2666" PostHistoryTypeId="5" PostId="867" RevisionGUID="ce78de1d-6e05-4c67-b3f3-b34975f637f8" CreationDate="2017-06-21T14:52:39.203" UserId="425" Comment="added 12 characters in body" Text="Simulating NGS reads while controlling sequence coverage is now easy with [RNFtools](http://karel-brinda.github.io/rnftools/) (from version 0.3.1). See the [tutorial](http://rnftools.readthedocs.io/en/latest/tutorial.html), especially section [Sequence extraction](http://rnftools.readthedocs.io/en/latest/tutorial/02_simulation.html#sequence-extraction).&#xD;&#xA;&#xD;&#xA;**Environment preparation**&#xD;&#xA;&#xD;&#xA;First, install [BioConda and add the required channels](https://bioconda.github.io/). Then either install RNFtools in the default Conda environment&#xD;&#xA;&#xD;&#xA;    conda install rnftools&#xD;&#xA;&#xD;&#xA;or create and activate a separate Conda environment (preferable)&#xD;&#xA;&#xD;&#xA;    conda create -n rnftools rnftools&#xD;&#xA;    source activate rnftools&#xD;&#xA;&#xD;&#xA;**Simulation**&#xD;&#xA;&#xD;&#xA;Assume that you have a reference file `ref.fa` and a tab-separated coverage file `coverage.tsv` (e.g., those from your example). Then the following `Snakefile` for RNFtools will do the job you want:&#xD;&#xA;&#xD;&#xA;    import rnftools&#xD;&#xA;    import csv&#xD;&#xA;&#xD;&#xA;    rnftools.mishmash.sample(&quot;simulation_with_coverage_control&quot;, reads_in_tuple=1)&#xD;&#xA;&#xD;&#xA;    fa = &quot;ref.fa&quot;&#xD;&#xA;    tsv = &quot;coverage.tsv&quot;&#xD;&#xA;&#xD;&#xA;    with open(tsv) as f:&#xD;&#xA;    	table = csv.reader(f, delimiter='\t')&#xD;&#xA;    	for seqname, cov in table:&#xD;&#xA;&#xD;&#xA;    		rnftools.mishmash.DwgSim(&#xD;&#xA;    			fasta=fa,&#xD;&#xA;    			sequences=[seqname],&#xD;&#xA;    			coverage=float(cov),&#xD;&#xA;    			read_length_1=10, # quick test with supershort reads&#xD;&#xA;    			read_length_2=0,&#xD;&#xA;    		)&#xD;&#xA;&#xD;&#xA;    include: rnftools.include()&#xD;&#xA;    rule: input: rnftools.input()&#xD;&#xA;&#xD;&#xA;When you save this file (`Snakefile`) and run `snakemake`, RNFtools will simulate reads using DWGsim with coverages defined your text file, and save all the simulated reads in `simulation_with_coverage_control.fq`.&#xD;&#xA;&#xD;&#xA;You can play with all the parameters. In particular, you can use a different simulator (e.g., Art-Illumina using `rnftools.mishmash.ArtIllumina`). See the [RNFtools documentation](http://rnftools.rtfd.org) for more information.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2667" PostHistoryTypeId="5" PostId="867" RevisionGUID="d3b49f70-2596-4dab-9144-a9465196837c" CreationDate="2017-06-21T14:58:16.040" UserId="425" Comment="deleted 4 characters in body" Text="Simulating NGS reads while controlling sequence coverage is now easy with [RNFtools](http://karel-brinda.github.io/rnftools/) (from version 0.3.1). See the [tutorial](http://rnftools.readthedocs.io/en/latest/tutorial.html), especially section [Sequence extraction](http://rnftools.readthedocs.io/en/latest/tutorial/02_simulation.html#sequence-extraction).&#xD;&#xA;&#xD;&#xA;**Environment preparation**&#xD;&#xA;&#xD;&#xA;First, install [BioConda and add the required channels](https://bioconda.github.io/). Then either install RNFtools in the default Conda environment&#xD;&#xA;&#xD;&#xA;    conda install rnftools&#xD;&#xA;&#xD;&#xA;or create and activate a separate Conda environment (preferable)&#xD;&#xA;&#xD;&#xA;    conda create -n rnftools rnftools&#xD;&#xA;    source activate rnftools&#xD;&#xA;&#xD;&#xA;**Simulation**&#xD;&#xA;&#xD;&#xA;Assume that you have a reference file `ref.fa` and a tab-separated coverage file `coverage.tsv` (e.g., those from your example). Then the following RNFtools `Snakefile` will do the job you want:&#xD;&#xA;&#xD;&#xA;    import rnftools&#xD;&#xA;    import csv&#xD;&#xA;&#xD;&#xA;    rnftools.mishmash.sample(&quot;simulation_with_coverage_control&quot;, reads_in_tuple=1)&#xD;&#xA;&#xD;&#xA;    fa = &quot;ref.fa&quot;&#xD;&#xA;    tsv = &quot;coverage.tsv&quot;&#xD;&#xA;&#xD;&#xA;    with open(tsv) as f:&#xD;&#xA;    	table = csv.reader(f, delimiter='\t')&#xD;&#xA;    	for seqname, cov in table:&#xD;&#xA;&#xD;&#xA;    		rnftools.mishmash.DwgSim(&#xD;&#xA;    			fasta=fa,&#xD;&#xA;    			sequences=[seqname],&#xD;&#xA;    			coverage=float(cov),&#xD;&#xA;    			read_length_1=10, # quick test with supershort reads&#xD;&#xA;    			read_length_2=0,&#xD;&#xA;    		)&#xD;&#xA;&#xD;&#xA;    include: rnftools.include()&#xD;&#xA;    rule: input: rnftools.input()&#xD;&#xA;&#xD;&#xA;When you save this file (`Snakefile`) and run `snakemake`, RNFtools will simulate reads using DWGsim with the coverages defined your text file, and save all the simulated reads in `simulation_with_coverage_control.fq`.&#xD;&#xA;&#xD;&#xA;You can play with all the parameters. In particular, you can use a different simulator (e.g., Art-Illumina using `rnftools.mishmash.ArtIllumina`). See the [RNFtools documentation](http://rnftools.rtfd.org) for more information.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2669" PostHistoryTypeId="2" PostId="868" RevisionGUID="eb96eeaa-b925-4b15-950c-9636edb6d5cd" CreationDate="2017-06-21T15:27:07.430" UserId="292" Text="### Regarding your python code&#xD;&#xA;&#xD;&#xA;If you want the **experimental ranges that are entirely contained in one of the reference ranges**, you need to have the coordinates in the following order:&#xD;&#xA;&#xD;&#xA;    cst &lt;= tst &lt; ten &lt;= cen&#xD;&#xA;&#xD;&#xA;If what you want are the **experimental ranges that overlap one of the reference ranges**, you need to have either the start or the end of the experimental range fall within the reference range:&#xD;&#xA;&#xD;&#xA;    (cst &lt;= tst &lt; cen) or (cst &lt; ten &lt;= cen)&#xD;&#xA;&#xD;&#xA;Your code is equivalent to neither possibilities:&#xD;&#xA;&#xD;&#xA;    cst &gt;= tst and cen &lt;= ten&#xD;&#xA;&#xD;&#xA;This is equivalent to:&#xD;&#xA;&#xD;&#xA;    tst &lt;= cst and cen &lt;= ten&#xD;&#xA;&#xD;&#xA;(Or `tst &lt;= cst &lt; cen &lt;= ten`, since we know that `cst &lt; cen`, by definition of a bed interval).&#xD;&#xA;&#xD;&#xA;With this rewriting, we can more easily see that are actually selecting the **experimental ranges that contain a reference range**.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Here is some (python3) code that gives you the results in the other two situations:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python3&#xD;&#xA;    &#xD;&#xA;    ref_intervals = []&#xD;&#xA;    with open(&quot;s4.txt&quot;, &quot;r&quot;) as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            (chr, start, end) = line.strip().split(&quot;\t&quot;)&#xD;&#xA;            ref_intervals.append((chr, int(start), int(end)))&#xD;&#xA;    &#xD;&#xA;    exp_intervals = []&#xD;&#xA;    with open(&quot;s3.txt&quot;, &quot;r&quot;) as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            (chr, start, end) = line.strip().split(&quot;\t&quot;)&#xD;&#xA;            exp_intervals.append((chr, int(start), int(end)))&#xD;&#xA;    &#xD;&#xA;    contained = []&#xD;&#xA;    overlapping = []&#xD;&#xA;    for (r_chr, r_start, r_end) in ref_intervals:&#xD;&#xA;        for (e_chr, e_start, e_end) in exp_intervals:&#xD;&#xA;            if e_chr == r_chr:&#xD;&#xA;                if r_start &lt;= e_start &lt; r_end or r_start &lt; e_end &lt;= r_end:&#xD;&#xA;                    overlapping.append((e_chr, e_start, e_end))&#xD;&#xA;                if r_start &lt;= e_start &lt; e_end &lt;= r_end:&#xD;&#xA;                    contained.append((e_chr, e_start, e_end))&#xD;&#xA;    &#xD;&#xA;    print(&quot;overlapping&quot;)&#xD;&#xA;    for (chr, start, end) in overlapping:&#xD;&#xA;        print(chr, start, end, sep=&quot;\t&quot;)&#xD;&#xA;    &#xD;&#xA;    print(&quot;contained&quot;)&#xD;&#xA;    for (chr, start, end) in contained:&#xD;&#xA;        print(chr, start, end, sep=&quot;\t&quot;)&#xD;&#xA;&#xD;&#xA;If I run it, I obtain the following results:&#xD;&#xA;&#xD;&#xA;    $ ./overlap.py &#xD;&#xA;    overlapping&#xD;&#xA;    1	10	20&#xD;&#xA;    1	5	20&#xD;&#xA;    1	10	50&#xD;&#xA;    1	14	17&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    2	20	60&#xD;&#xA;    contained&#xD;&#xA;    1	10	20&#xD;&#xA;    1	14	17&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;It is probably an good exercise to program this, but as the other answers point out, **there are efficient tools that would be a better solution in a professional setting**." />
  <row Id="2670" PostHistoryTypeId="5" PostId="867" RevisionGUID="1ff28e29-de67-49b6-b414-b90312c3f613" CreationDate="2017-06-21T15:32:54.330" UserId="425" Comment="Add language hints (highlighting)" Text="Simulating NGS reads while controlling sequence coverage is now easy with [RNFtools](http://karel-brinda.github.io/rnftools/) (from version 0.3.1). See the [tutorial](http://rnftools.readthedocs.io/en/latest/tutorial.html), especially section [Sequence extraction](http://rnftools.readthedocs.io/en/latest/tutorial/02_simulation.html#sequence-extraction).&#xD;&#xA;&#xD;&#xA;**Environment preparation**&#xD;&#xA;&#xD;&#xA;First, install [BioConda and add the required channels](https://bioconda.github.io/). Then either install RNFtools in the default Conda environment&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    conda install rnftools&#xD;&#xA;&#xD;&#xA;or create and activate a separate Conda environment (preferable)&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    conda create -n rnftools rnftools&#xD;&#xA;    source activate rnftools&#xD;&#xA;&#xD;&#xA;**Simulation**&#xD;&#xA;&#xD;&#xA;Assume that you have a reference file `ref.fa` and a tab-separated coverage file `coverage.tsv` (e.g., those from your example). Then the following RNFtools `Snakefile` will do the job you want:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    import rnftools&#xD;&#xA;    import csv&#xD;&#xA;&#xD;&#xA;    rnftools.mishmash.sample(&quot;simulation_with_coverage_control&quot;, reads_in_tuple=1)&#xD;&#xA;&#xD;&#xA;    fa = &quot;ref.fa&quot;&#xD;&#xA;    tsv = &quot;coverage.tsv&quot;&#xD;&#xA;&#xD;&#xA;    with open(tsv) as f:&#xD;&#xA;    	table = csv.reader(f, delimiter='\t')&#xD;&#xA;    	for seqname, cov in table:&#xD;&#xA;&#xD;&#xA;    		rnftools.mishmash.DwgSim(&#xD;&#xA;    			fasta=fa,&#xD;&#xA;    			sequences=[seqname],&#xD;&#xA;    			coverage=float(cov),&#xD;&#xA;    			read_length_1=10, # quick test with supershort reads&#xD;&#xA;    			read_length_2=0,&#xD;&#xA;    		)&#xD;&#xA;&#xD;&#xA;    include: rnftools.include()&#xD;&#xA;    rule: input: rnftools.input()&#xD;&#xA;&#xD;&#xA;When you save this file (`Snakefile`) and run `snakemake`, RNFtools will simulate reads using DWGsim with the coverages defined your text file, and save all the simulated reads in `simulation_with_coverage_control.fq`.&#xD;&#xA;&#xD;&#xA;You can play with all the parameters. In particular, you can use a different simulator (e.g., Art-Illumina using `rnftools.mishmash.ArtIllumina`). See the [RNFtools documentation](http://rnftools.rtfd.org) for more information." />
  <row Id="2671" PostHistoryTypeId="5" PostId="868" RevisionGUID="d743cddf-89d6-4875-b002-b08071eb5b8e" CreationDate="2017-06-21T15:34:15.073" UserId="292" Comment="added 4 characters in body" Text="### Regarding your python code&#xD;&#xA;&#xD;&#xA;If you want the **experimental ranges that are entirely contained in one of the reference ranges**, you need to have the coordinates in the following order:&#xD;&#xA;&#xD;&#xA;    cst &lt;= tst &lt; ten &lt;= cen&#xD;&#xA;&#xD;&#xA;If what you want are the **experimental ranges that overlap one of the reference ranges**, you need to have either the start or the end of the experimental range fall within the reference range:&#xD;&#xA;&#xD;&#xA;    (cst &lt;= tst &lt; cen) or (cst &lt; ten &lt;= cen)&#xD;&#xA;&#xD;&#xA;Your code is equivalent to neither possibilities:&#xD;&#xA;&#xD;&#xA;    cst &gt;= tst and cen &lt;= ten&#xD;&#xA;&#xD;&#xA;This is equivalent to:&#xD;&#xA;&#xD;&#xA;    tst &lt;= cst and cen &lt;= ten&#xD;&#xA;&#xD;&#xA;(Or `tst &lt;= cst &lt; cen &lt;= ten`, since we know that `cst &lt; cen`, by definition of a bed interval).&#xD;&#xA;&#xD;&#xA;With this rewriting, we can more easily see that you are actually selecting the **experimental ranges that contain a reference range**.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Here is some (python3) code that gives you the results in the other two situations:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python3&#xD;&#xA;    &#xD;&#xA;    ref_intervals = []&#xD;&#xA;    with open(&quot;s4.txt&quot;, &quot;r&quot;) as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            (chr, start, end) = line.strip().split(&quot;\t&quot;)&#xD;&#xA;            ref_intervals.append((chr, int(start), int(end)))&#xD;&#xA;    &#xD;&#xA;    exp_intervals = []&#xD;&#xA;    with open(&quot;s3.txt&quot;, &quot;r&quot;) as f:&#xD;&#xA;        for line in f:&#xD;&#xA;            (chr, start, end) = line.strip().split(&quot;\t&quot;)&#xD;&#xA;            exp_intervals.append((chr, int(start), int(end)))&#xD;&#xA;    &#xD;&#xA;    contained = []&#xD;&#xA;    overlapping = []&#xD;&#xA;    for (r_chr, r_start, r_end) in ref_intervals:&#xD;&#xA;        for (e_chr, e_start, e_end) in exp_intervals:&#xD;&#xA;            if e_chr == r_chr:&#xD;&#xA;                if r_start &lt;= e_start &lt; r_end or r_start &lt; e_end &lt;= r_end:&#xD;&#xA;                    overlapping.append((e_chr, e_start, e_end))&#xD;&#xA;                if r_start &lt;= e_start &lt; e_end &lt;= r_end:&#xD;&#xA;                    contained.append((e_chr, e_start, e_end))&#xD;&#xA;    &#xD;&#xA;    print(&quot;overlapping&quot;)&#xD;&#xA;    for (chr, start, end) in overlapping:&#xD;&#xA;        print(chr, start, end, sep=&quot;\t&quot;)&#xD;&#xA;    &#xD;&#xA;    print(&quot;contained&quot;)&#xD;&#xA;    for (chr, start, end) in contained:&#xD;&#xA;        print(chr, start, end, sep=&quot;\t&quot;)&#xD;&#xA;&#xD;&#xA;If I run it, I obtain the following results:&#xD;&#xA;&#xD;&#xA;    $ ./overlap.py &#xD;&#xA;    overlapping&#xD;&#xA;    1	10	20&#xD;&#xA;    1	5	20&#xD;&#xA;    1	10	50&#xD;&#xA;    1	14	17&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;    2	20	60&#xD;&#xA;    contained&#xD;&#xA;    1	10	20&#xD;&#xA;    1	14	17&#xD;&#xA;    2	20	30&#xD;&#xA;    2	25	30&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;It is probably an good exercise to program this, but as the other answers point out, **there are efficient tools that would be a better solution in a professional setting**." />
  <row Id="2673" PostHistoryTypeId="2" PostId="869" RevisionGUID="d5696bdb-d121-47bd-b9c9-e52c5e84b855" CreationDate="2017-06-21T16:30:49.260" UserId="935" Text="I am analyzing data from a quantitative polymerase chain reaction (qPCR) using R. After cleaning the raw data, it looks something like this:&#xD;&#xA;&#xD;&#xA;    &gt; dput(x)&#xD;&#xA;    structure(list(Reporter = c(&quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &#xD;&#xA;    &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &#xD;&#xA;    &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &#xD;&#xA;    &quot;VIC&quot;), Number = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, &#xD;&#xA;    12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L), A = c(22.19, &#xD;&#xA;    22.24, 22.5, 22.54, 22.6, 22.59, 23.07, 23.46, 22.43, 22.74, &#xD;&#xA;    24.09, 23.91, 24.52, 25.03, 25.25, 25.82, 25.13, 24.71, 25.34, &#xD;&#xA;    25.85, 25.25, 26.15, 25.81, 25.29), B = c(21.72, 21.78, 22.86, &#xD;&#xA;    22.73, 19.88, 20.07, 21.06, 21.06, 20.96, 21.11, 19.46, 19.43, &#xD;&#xA;    24.75, 24.69, 25.64, 25.19, 23.76, 23.69, 24.35, 25.05, 24.1, &#xD;&#xA;    23.81, 22.81, 23.13), C = c(21.37, 21.56, 20.07, 20.01, 21.17, &#xD;&#xA;    21.08, 20.54, 20.36, 33, NA, NA, NA, 23.91, 24.31, 23.61, 23.88, &#xD;&#xA;    24.33, 24.31, 23.37, 23.53, 33, NA, NA, NA), E = c(26.26, 27.33, &#xD;&#xA;    25.93, 26.56, 25.76, 23.03, 24.72, 25.27, 24.43, 24.31, 26.98, &#xD;&#xA;    23.33, 24.04, 25.02, 25.1, 25.1, 24.68, 25.48, 25.87, 26.22, &#xD;&#xA;    25.35, 25.36, 25.11, 25.98), F = c(25.81, 26.9, 25.58, 26.61, &#xD;&#xA;    25.06, 21.85, 23.59, 24.04, 23.19, 23.19, 25.17, 20.8, 24.12, &#xD;&#xA;    24.26, 25.32, 25.25, 24, 23.78, 24.7, 24.48, 23.52, 23.87, 23.05, &#xD;&#xA;    23.05), G = c(26.12, 27.02, 24.08, 25.15, 25.99, 23.18, 24.2, &#xD;&#xA;    24.05, 33, NA, NA, NA, 23.47, 23.45, 23.7, 23.74, 24.46, 24.19, &#xD;&#xA;    23.56, 23.53, 33, NA, NA, NA)), .Names = c(&quot;Reporter&quot;, &quot;Number&quot;, &#xD;&#xA;    &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;&#xD;&#xA;    ), row.names = c(NA, 24L))&#xD;&#xA;&#xD;&#xA;The triplicate A, B, and C is the control sample and the triplicate E, F, and G is the treatment sample. The target gene is FAM and the reference gene is VIC.&#xD;&#xA;&#xD;&#xA;I have created a function for analyzing the data with the ddCt algorithm (Livak &amp; Schmittgen, 2001):&#xD;&#xA;&#xD;&#xA;    ddCt_ &lt;- function(x) {&#xD;&#xA;      # Subset x by control/treatment and target/reference&#xD;&#xA;      # Then, calculate Ct averages for each triplicate&#xD;&#xA;      TC &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, A, B, C) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;FAM&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(A, B, C), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      RC &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, A, B, C) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;VIC&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(A, B, C), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      TT &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, E, F, G) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;FAM&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(E, F, G), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      RT &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, E, F, G) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;VIC&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(E, F, G), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      # Normalize Ct of the target gene to the Ct of the reference gene&#xD;&#xA;      dCt_control   &lt;- TC$Ct_Avg - RC$Ct_Avg&#xD;&#xA;      dCt_treatment &lt;- TT$Ct_Avg - RT$Ct_Avg&#xD;&#xA;    &#xD;&#xA;      # Normalize dCt of the treatment group to the dCt of the control group&#xD;&#xA;      ddCt &lt;- dCt_treatment - dCt_control&#xD;&#xA;    &#xD;&#xA;      # Calculate fold change&#xD;&#xA;      fc &lt;- 2^(-ddCt)&#xD;&#xA;    &#xD;&#xA;      # Calculate avg and sd&#xD;&#xA;      dCt_control_avg   &lt;- mean(dCt_control)&#xD;&#xA;      dCt_control_sd    &lt;- sd(dCt_control)&#xD;&#xA;      dCt_treatment_avg &lt;- mean(dCt_treatment)&#xD;&#xA;      dCt_treatment_sd  &lt;- sd(dCt_treatment)&#xD;&#xA;    &#xD;&#xA;      # Create output&#xD;&#xA;      df &lt;- data_frame(&#xD;&#xA;        Sample = 1:12, &quot;dCt Control&quot; = dCt_control, &quot;dCt Treatment&quot; = dCt_treatment, &#xD;&#xA;        &quot;ddCt&quot; = ddCt, &quot;Fold Change&quot; = fc, &quot;dCt Control Avg&quot; = dCt_control_avg, &#xD;&#xA;        &quot;dCt Control SD&quot; = dCt_control_sd, &quot;dCt Treatment Avg&quot; = dCt_treatment_avg, &#xD;&#xA;        &quot;dCt Treatment SD&quot; = dCt_treatment_sd&#xD;&#xA;        ) %&gt;% round(2)&#xD;&#xA;      write.csv(x = df, file = &quot;result.csv&quot;)&#xD;&#xA;      saveRDS(object = df, file = &quot;result.rds&quot;)&#xD;&#xA;      df&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;However, I am not a biochemist or molecular biologist, so I am unsure of the basic concepts involved and the overall approach. So, my questions are:&#xD;&#xA;&#xD;&#xA;1. Is the function correct?&#xD;&#xA;2. Why is it important to calculate fold change and standard deviation after normalization?&#xD;&#xA;" />
  <row Id="2674" PostHistoryTypeId="1" PostId="869" RevisionGUID="d5696bdb-d121-47bd-b9c9-e52c5e84b855" CreationDate="2017-06-21T16:30:49.260" UserId="935" Text="qPCR: Why is fold change and standard deviation calculated after transformation?" />
  <row Id="2675" PostHistoryTypeId="3" PostId="869" RevisionGUID="d5696bdb-d121-47bd-b9c9-e52c5e84b855" CreationDate="2017-06-21T16:30:49.260" UserId="935" Text="&lt;r&gt;&lt;normalization&gt;" />
  <row Id="2676" PostHistoryTypeId="2" PostId="870" RevisionGUID="436be0eb-a5de-4c29-b622-d4568b30c0c6" CreationDate="2017-06-21T16:43:27.950" UserId="77" Text="1. The functions look correct, but calculate a few by hand and ensure they match. &#xA;2. You have to normalize to a reference gene to control for how much cDNA was used, since that will alter the Ct values. " />
  <row Id="2677" PostHistoryTypeId="5" PostId="844" RevisionGUID="69c7b49f-8dbb-494e-bb94-08763fc203b5" CreationDate="2017-06-21T17:36:17.427" UserId="734" Comment="deleted 6 characters in body; edited title" Text="I have some difficulties to understand/interpret the [pathway map][1] and how a gene-gene interaction list or DNA sequencing can map into pathways.&#xD;&#xA;&#xD;&#xA;In addition what's the difference between [MARK/ERK pathway][1] and the output of [KEGG][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/MAPK/ERK_pathway&#xD;&#xA;  [2]: http://www.kegg.jp/kegg-bin/show_pathway?hsa05200" />
  <row Id="2678" PostHistoryTypeId="4" PostId="844" RevisionGUID="69c7b49f-8dbb-494e-bb94-08763fc203b5" CreationDate="2017-06-21T17:36:17.427" UserId="734" Comment="deleted 6 characters in body; edited title" Text="How to find the pathway map from bioinformatics data?" />
  <row Id="2679" PostHistoryTypeId="5" PostId="827" RevisionGUID="17388d47-f8ed-4200-9b99-e9441b2fc81b" CreationDate="2017-06-21T17:39:45.060" UserId="96" Comment="response to downvote" Text="BLASTN should definitely report alignments in the order that the query sequences were provided.&#xD;&#xA;&#xD;&#xA;Based on your description and my personal experience, I think the most likely explanation is some kind of mix up. It happens to all of us, especially when we're in a rush to get an answer, but even sometimes when we're being careful and disciplined.&#xD;&#xA;&#xD;&#xA;If you can go back and recreate the result, I would be very interested to see the two pairs of input and output files. If not, I wouldn't spend too much more time worrying about it. :-)&#xD;&#xA;&#xD;&#xA;**UPDATE** Alignments are reported in the order that the query sequences are provided, even in tabular mode. The order of each query's hits is sorted by bitscore or e-value, but that is a secondary sorting. I've included an example below that has two queries, `seq1` and `seq2` with `-outfmt 6`.&#xD;&#xA;&#xD;&#xA;    seq1    scaffold2       100.000 280     0       0       1       280     11131   11410   7.73e-146       518&#xD;&#xA;    seq1    scaffold294     88.693  283     25      4       1       279     12160   12439   6.46e-92        339&#xD;&#xA;    seq1    scaffold1525    85.315  286     29      9       1       277     27142   26861   3.07e-75        283&#xD;&#xA;    seq1    scaffold1874    83.505  291     32      10      1       280     8558    8273    1.86e-67        257&#xD;&#xA;    seq1    scaffold147     82.653  294     34      9       1       279     10797   10506   1.45e-63        244&#xD;&#xA;    seq1    scaffold2478    82.374  278     38      8       11      280     7622    7896    1.13e-59        231&#xD;&#xA;    seq1    scaffold3405    87.940  199     17      5       1       196     14390   14584   1.46e-58        228&#xD;&#xA;    seq1    scaffold395     83.060  183     23      4       106     280     12021   11839   5.40e-38        159&#xD;&#xA;    seq1    scaffold2853    85.965  114     13      3       1       113     20864   20753   9.16e-26        119&#xD;&#xA;    seq2    scaffold9       100.000 140     0       0       1       140     1121    1260    2.34e-68        259&#xD;&#xA;    seq2    scaffold2950    90.909  110     6       3       1       106     110     1       6.84e-34        145&#xD;&#xA;    seq2    scaffold3416    95.385  65      3       0       1       65      11882   11818   1.16e-21        104&#xD;&#xA;    seq2    scaffold3103    95.385  65      3       0       1       65      9736    9672    1.16e-21        104&#xD;&#xA;    seq2    scaffold5297    88.608  79      7       1       64      140     150     228     6.99e-19        95.3" />
  <row Id="2680" PostHistoryTypeId="5" PostId="846" RevisionGUID="cfd689ad-3a11-4ea9-b017-28d319ffb509" CreationDate="2017-06-21T18:01:18.207" UserId="929" Comment="added 215 characters in body; deleted 277 characters in body" Text="**Edit** I stand corrected. @Daniel Standage correctly pointed out that the below is only true for a single query, or for the sorting of hits within those found by a given query. Otherwise they're sorted in the order they were given. &#xA;&#xA; BLAST results are sorted in descending order by the &quot;Bit score&quot; (column 12). Sorting by E. value would not be quite the same.&#xD;&#xA;&#xD;&#xA;i.e. column 11 (E value) will be presented in ascending order (an E. value of 0 is as good as it gets), and as it gets higher, your hit is getting worse. Likewise, bitscores are highest at the top and decrease down the list. A hit with E. value = 0 should have a high score. How high exactly depends on the hit length, so it's common to see many E values with 0.0, but the scores differ. Here's a snippet of one of my blast results:&#xD;&#xA;&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  100.000  24180  0     0   1      24180  2233012  2257191  0.0  43606&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  78.098   5593   1164  16  2765   8323   3219956  3225521  0.0  4547&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  77.549   5394   1139  16  2948   8323   2376163  2370824  0.0  4260&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  74.140   5669   1342  34  8490   14100  3225688  3231290  0.0  3573&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  76.835   3665   825   9   16507  20150  3234191  3237852  0.0  2764&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  76.871   2646   571   11  16625  19248  2362597  2359971  0.0  2001&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  70.733   4131   1005  51  9047   13063  2368869  2364829  0.0  1954&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  75.040   1891   448   7   8500   10383  2370771  2368898  0.0  1272&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  78.986   1361   269   7   22766  24119  890051   891401   0.0  1150&#xD;&#xA;    PVCcif_ATCC43949  PAU_06042014  77.819   1082   217   5   536    1610   3217983  3219048  0.0  868&#xD;&#xA;&#xD;&#xA;As you can see, they all have E values of 0.0, but they differ quite a lot in score, this is because of the length of the alignment, and the different percentage IDs.&#xD;&#xA;" />
  <row Id="2681" PostHistoryTypeId="2" PostId="871" RevisionGUID="60ba5863-01dd-41e4-be4e-4519057f639f" CreationDate="2017-06-21T18:31:20.883" UserId="96" Text="When accessing all of the annotated genes for a reference genome, downloading a GFF3 file directly from the [Genbank](ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank) or [RefSeq](ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq) FTP sites is definitely the way to go.&#xD;&#xA;&#xD;&#xA;But when submitting a search query into NCBI Entrez, there is no easy way to download the results in GFF3 format. Probably the best route is to download the results in ASN.1 format, and then use `annotwriter` from the NCBI C++ toolkit to convert to GFF3. But getting the latter to compile and run properly isn't trivial in my experience." />
  <row Id="2682" PostHistoryTypeId="2" PostId="872" RevisionGUID="67160990-9850-42dd-bae2-2a2b742c985a" CreationDate="2017-06-21T19:34:57.207" UserId="824" Text="As you alluded to in your question, KEGG does provide a curated set of enzyme and metabolite information. This information can be parsed and used to create a network that you can analyze to look at how gene products could be working together. Additional steps could include mapping transcriptome data to the map.&#xD;&#xA;&#xD;&#xA;See [this paper][1] as an example of how this can be done.&#xD;&#xA;&#xD;&#xA;I hope this helps. :)&#xD;&#xA;&#xD;&#xA;  [1]: http://biorxiv.org/content/early/2016/12/07/092304" />
  <row Id="2683" PostHistoryTypeId="5" PostId="870" RevisionGUID="f0550935-a711-4411-86eb-a4d65551c431" CreationDate="2017-06-21T19:37:17.810" UserId="77" Comment="added 434 characters in body" Text="1. The functions look correct, but calculate a few by hand and ensure they match. One thing I should note is that the subtraction of the Ct values usually happens before an average is made, since generally both are in the same well. In other word, subtract the reference Ct from the gene of interest Ct from the same well and then average the dCts. This order is important since one usually does a multiplexed qPCR.&#xD;&#xA;2. You have to normalize to a reference gene to control for how much cDNA was used, since that will alter the Ct values. If you calculated the fold-changes without normalization then they could be purely due to using more/less cDNA in the reaction (i.e., the output would be meaningless)." />
  <row Id="2684" PostHistoryTypeId="5" PostId="869" RevisionGUID="478d0e2d-7b49-44d1-9cd6-63564234cb06" CreationDate="2017-06-21T19:45:18.447" UserId="935" Comment="added 88 characters in body" Text="I am analyzing data from a quantitative polymerase chain reaction (qPCR) using R. After cleaning the raw data, it looks something like this:&#xD;&#xA;&#xD;&#xA;    &gt; dput(x)&#xD;&#xA;    structure(list(Reporter = c(&quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &#xD;&#xA;    &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &#xD;&#xA;    &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &#xD;&#xA;    &quot;VIC&quot;), Number = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, &#xD;&#xA;    12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L), A = c(22.19, &#xD;&#xA;    22.24, 22.5, 22.54, 22.6, 22.59, 23.07, 23.46, 22.43, 22.74, &#xD;&#xA;    24.09, 23.91, 24.52, 25.03, 25.25, 25.82, 25.13, 24.71, 25.34, &#xD;&#xA;    25.85, 25.25, 26.15, 25.81, 25.29), B = c(21.72, 21.78, 22.86, &#xD;&#xA;    22.73, 19.88, 20.07, 21.06, 21.06, 20.96, 21.11, 19.46, 19.43, &#xD;&#xA;    24.75, 24.69, 25.64, 25.19, 23.76, 23.69, 24.35, 25.05, 24.1, &#xD;&#xA;    23.81, 22.81, 23.13), C = c(21.37, 21.56, 20.07, 20.01, 21.17, &#xD;&#xA;    21.08, 20.54, 20.36, 33, NA, NA, NA, 23.91, 24.31, 23.61, 23.88, &#xD;&#xA;    24.33, 24.31, 23.37, 23.53, 33, NA, NA, NA), E = c(26.26, 27.33, &#xD;&#xA;    25.93, 26.56, 25.76, 23.03, 24.72, 25.27, 24.43, 24.31, 26.98, &#xD;&#xA;    23.33, 24.04, 25.02, 25.1, 25.1, 24.68, 25.48, 25.87, 26.22, &#xD;&#xA;    25.35, 25.36, 25.11, 25.98), F = c(25.81, 26.9, 25.58, 26.61, &#xD;&#xA;    25.06, 21.85, 23.59, 24.04, 23.19, 23.19, 25.17, 20.8, 24.12, &#xD;&#xA;    24.26, 25.32, 25.25, 24, 23.78, 24.7, 24.48, 23.52, 23.87, 23.05, &#xD;&#xA;    23.05), G = c(26.12, 27.02, 24.08, 25.15, 25.99, 23.18, 24.2, &#xD;&#xA;    24.05, 33, NA, NA, NA, 23.47, 23.45, 23.7, 23.74, 24.46, 24.19, &#xD;&#xA;    23.56, 23.53, 33, NA, NA, NA)), .Names = c(&quot;Reporter&quot;, &quot;Number&quot;, &#xD;&#xA;    &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;&#xD;&#xA;    ), row.names = c(NA, 24L))&#xD;&#xA;&#xD;&#xA;The triplicate A, B, and C is the control sample and the triplicate E, F, and G is the treatment sample. The target gene is FAM and the reference gene is VIC.&#xD;&#xA;&#xD;&#xA;I have created a function `ddCt_` for analyzing the data with the ddCt algorithm (Livak &amp; Schmittgen, 2001). It takes one argument `x` which is a data.frame of the form exemplified above. &#xD;&#xA;&#xD;&#xA;    ddCt_ &lt;- function(x) {&#xD;&#xA;      # Subset x by control/treatment and target/reference&#xD;&#xA;      # Then, calculate Ct averages for each triplicate&#xD;&#xA;      TC &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, A, B, C) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;FAM&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(A, B, C), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      RC &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, A, B, C) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;VIC&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(A, B, C), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      TT &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, E, F, G) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;FAM&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(E, F, G), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      RT &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, E, F, G) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;VIC&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(E, F, G), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      # Normalize Ct of the target gene to the Ct of the reference gene&#xD;&#xA;      dCt_control   &lt;- TC$Ct_Avg - RC$Ct_Avg&#xD;&#xA;      dCt_treatment &lt;- TT$Ct_Avg - RT$Ct_Avg&#xD;&#xA;    &#xD;&#xA;      # Normalize dCt of the treatment group to the dCt of the control group&#xD;&#xA;      ddCt &lt;- dCt_treatment - dCt_control&#xD;&#xA;    &#xD;&#xA;      # Calculate fold change&#xD;&#xA;      fc &lt;- 2^(-ddCt)&#xD;&#xA;    &#xD;&#xA;      # Calculate avg and sd&#xD;&#xA;      dCt_control_avg   &lt;- mean(dCt_control)&#xD;&#xA;      dCt_control_sd    &lt;- sd(dCt_control)&#xD;&#xA;      dCt_treatment_avg &lt;- mean(dCt_treatment)&#xD;&#xA;      dCt_treatment_sd  &lt;- sd(dCt_treatment)&#xD;&#xA;    &#xD;&#xA;      # Create output&#xD;&#xA;      df &lt;- data_frame(&#xD;&#xA;        Sample = 1:12, &quot;dCt Control&quot; = dCt_control, &quot;dCt Treatment&quot; = dCt_treatment, &#xD;&#xA;        &quot;ddCt&quot; = ddCt, &quot;Fold Change&quot; = fc, &quot;dCt Control Avg&quot; = dCt_control_avg, &#xD;&#xA;        &quot;dCt Control SD&quot; = dCt_control_sd, &quot;dCt Treatment Avg&quot; = dCt_treatment_avg, &#xD;&#xA;        &quot;dCt Treatment SD&quot; = dCt_treatment_sd&#xD;&#xA;        ) %&gt;% round(2)&#xD;&#xA;      write.csv(x = df, file = &quot;result.csv&quot;)&#xD;&#xA;      saveRDS(object = df, file = &quot;result.rds&quot;)&#xD;&#xA;      df&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;However, I am not a biochemist or molecular biologist, so I am unsure of the basic concepts involved and the overall approach. So, my questions are:&#xD;&#xA;&#xD;&#xA;1. Is the function correct?&#xD;&#xA;2. Why is it important to calculate fold change and standard deviation after normalization?&#xD;&#xA;" />
  <row Id="2685" PostHistoryTypeId="6" PostId="849" RevisionGUID="49f8763a-30a2-4273-a38a-77abe954a2bb" CreationDate="2017-06-21T19:46:55.577" UserId="96" Comment="added splicing tag" Text="&lt;file-formats&gt;&lt;gtf&gt;&lt;rna-splicing&gt;" />
  <row Id="2686" PostHistoryTypeId="24" PostId="849" RevisionGUID="49f8763a-30a2-4273-a38a-77abe954a2bb" CreationDate="2017-06-21T19:46:55.577" Comment="Proposed by 96 approved by 77, 73 edit id of 218" />
  <row Id="2687" PostHistoryTypeId="5" PostId="869" RevisionGUID="85ed5217-2d4e-42c7-b9ac-746e2e360b66" CreationDate="2017-06-21T21:05:36.370" UserId="935" Comment="added 85 characters in body" Text="I am analyzing data from a quantitative polymerase chain reaction (qPCR) using R. After cleaning the raw data, it looks something like this:&#xD;&#xA;&#xD;&#xA;    &gt; dput(x)&#xD;&#xA;    structure(list(Reporter = c(&quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &#xD;&#xA;    &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &#xD;&#xA;    &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &#xD;&#xA;    &quot;VIC&quot;), Number = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, &#xD;&#xA;    12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L), A = c(22.19, &#xD;&#xA;    22.24, 22.5, 22.54, 22.6, 22.59, 23.07, 23.46, 22.43, 22.74, &#xD;&#xA;    24.09, 23.91, 24.52, 25.03, 25.25, 25.82, 25.13, 24.71, 25.34, &#xD;&#xA;    25.85, 25.25, 26.15, 25.81, 25.29), B = c(21.72, 21.78, 22.86, &#xD;&#xA;    22.73, 19.88, 20.07, 21.06, 21.06, 20.96, 21.11, 19.46, 19.43, &#xD;&#xA;    24.75, 24.69, 25.64, 25.19, 23.76, 23.69, 24.35, 25.05, 24.1, &#xD;&#xA;    23.81, 22.81, 23.13), C = c(21.37, 21.56, 20.07, 20.01, 21.17, &#xD;&#xA;    21.08, 20.54, 20.36, 33, NA, NA, NA, 23.91, 24.31, 23.61, 23.88, &#xD;&#xA;    24.33, 24.31, 23.37, 23.53, 33, NA, NA, NA), E = c(26.26, 27.33, &#xD;&#xA;    25.93, 26.56, 25.76, 23.03, 24.72, 25.27, 24.43, 24.31, 26.98, &#xD;&#xA;    23.33, 24.04, 25.02, 25.1, 25.1, 24.68, 25.48, 25.87, 26.22, &#xD;&#xA;    25.35, 25.36, 25.11, 25.98), F = c(25.81, 26.9, 25.58, 26.61, &#xD;&#xA;    25.06, 21.85, 23.59, 24.04, 23.19, 23.19, 25.17, 20.8, 24.12, &#xD;&#xA;    24.26, 25.32, 25.25, 24, 23.78, 24.7, 24.48, 23.52, 23.87, 23.05, &#xD;&#xA;    23.05), G = c(26.12, 27.02, 24.08, 25.15, 25.99, 23.18, 24.2, &#xD;&#xA;    24.05, 33, NA, NA, NA, 23.47, 23.45, 23.7, 23.74, 24.46, 24.19, &#xD;&#xA;    23.56, 23.53, 33, NA, NA, NA)), .Names = c(&quot;Reporter&quot;, &quot;Number&quot;, &#xD;&#xA;    &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;&#xD;&#xA;    ), row.names = c(NA, 24L))&#xD;&#xA;&#xD;&#xA;I think that each well measures two genes: target which is 12 different genes (color FAM), and the reference or housekeeping gene, GAPDH (color VIC). Also, control is triplicate A, B, C (3 x 12 wells), and treatment is E, F, G (3 x 12 wells). &#xD;&#xA;&#xD;&#xA;I have created a function `ddCt_` for analyzing the data with the ddCt algorithm (Livak &amp; Schmittgen, 2001). It takes one argument `x` which is a data.frame of the form exemplified above. &#xD;&#xA;&#xD;&#xA;    ddCt_ &lt;- function(x) {&#xD;&#xA;      # Subset x by control/treatment and target/reference&#xD;&#xA;      # Then, calculate Ct averages for each triplicate&#xD;&#xA;      TC &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, A, B, C) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;FAM&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(A, B, C), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      RC &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, A, B, C) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;VIC&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(A, B, C), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      TT &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, E, F, G) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;FAM&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(E, F, G), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      RT &lt;- x %&gt;% &#xD;&#xA;        select(Reporter, Number, E, F, G) %&gt;% &#xD;&#xA;        filter(Reporter == &quot;VIC&quot;) %&gt;% &#xD;&#xA;        rowwise() %&gt;%&#xD;&#xA;        mutate(Ct_Avg = mean(c(E, F, G), na.rm = TRUE))&#xD;&#xA;    &#xD;&#xA;      # Normalize Ct of the target gene to the Ct of the reference gene&#xD;&#xA;      dCt_control   &lt;- TC$Ct_Avg - RC$Ct_Avg&#xD;&#xA;      dCt_treatment &lt;- TT$Ct_Avg - RT$Ct_Avg&#xD;&#xA;    &#xD;&#xA;      # Normalize dCt of the treatment group to the dCt of the control group&#xD;&#xA;      ddCt &lt;- dCt_treatment - dCt_control&#xD;&#xA;    &#xD;&#xA;      # Calculate fold change&#xD;&#xA;      fc &lt;- 2^(-ddCt)&#xD;&#xA;    &#xD;&#xA;      # Calculate avg and sd&#xD;&#xA;      dCt_control_avg   &lt;- mean(dCt_control)&#xD;&#xA;      dCt_control_sd    &lt;- sd(dCt_control)&#xD;&#xA;      dCt_treatment_avg &lt;- mean(dCt_treatment)&#xD;&#xA;      dCt_treatment_sd  &lt;- sd(dCt_treatment)&#xD;&#xA;    &#xD;&#xA;      # Create output&#xD;&#xA;      df &lt;- data_frame(&#xD;&#xA;        Sample = 1:12, &quot;dCt Control&quot; = dCt_control, &quot;dCt Treatment&quot; = dCt_treatment, &#xD;&#xA;        &quot;ddCt&quot; = ddCt, &quot;Fold Change&quot; = fc, &quot;dCt Control Avg&quot; = dCt_control_avg, &#xD;&#xA;        &quot;dCt Control SD&quot; = dCt_control_sd, &quot;dCt Treatment Avg&quot; = dCt_treatment_avg, &#xD;&#xA;        &quot;dCt Treatment SD&quot; = dCt_treatment_sd&#xD;&#xA;        ) %&gt;% round(2)&#xD;&#xA;      write.csv(x = df, file = &quot;result.csv&quot;)&#xD;&#xA;      saveRDS(object = df, file = &quot;result.rds&quot;)&#xD;&#xA;      df&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;However, I am not a biochemist or molecular biologist, so I am unsure of the basic concepts involved and the overall approach. So, my questions are:&#xD;&#xA;&#xD;&#xA;1. Is the function correct?&#xD;&#xA;2. Why is it important to calculate fold change and standard deviation after normalization?&#xD;&#xA;" />
  <row Id="2688" PostHistoryTypeId="5" PostId="813" RevisionGUID="a0c8c3b3-0369-46bb-a178-28ce9eac5588" CreationDate="2017-06-21T23:07:21.610" UserId="492" Comment="Fix typo" Text="# Arbitrary record access in constant time&#xD;&#xA;&#xD;&#xA;To get a random record in constant time, it is sufficient to get an arbitrary record in constant time.&#xD;&#xA;&#xD;&#xA;I have two solutions here: One with `tabix` and one with `grabix`. I think the `grabix` solution is more elegant, but I am keeping the `tabix` solution below because `tabix` is a more mature tool than `grabix`.&#xD;&#xA; &#xD;&#xA;Thanks to [user172818](https://bioinformatics.stackexchange.com/users/37/user172818) for suggesting `grabix`.&#xD;&#xA;&#xD;&#xA;### Update&#xD;&#xA;&#xD;&#xA;This answer previously stated that `tabix` and `grabix` perform lookups in `log(n)` time. After taking a closer look at the grabix source code and the tabix paper I am now convinced that lookups are independent of `n` in complexity. However, both tools use an index that scales in size proportionally to `n`. So, the loading of the index is order `n`. However, if we consider the loading of the index as &quot;...a single limited transformation of the data to another file format...&quot;, then I think this answer is still is a valid one. If more than one record is to be retrieved, then the index needs to be stored in memory, perhaps with a framework such as pysam or htslib.&#xD;&#xA;&#xD;&#xA;## Using `grabix`&#xD;&#xA;&#xD;&#xA;1. Compress with `bgzip`.&#xD;&#xA;2. Index the file and perform lookups with [`grabix`](https://github.com/arq5x/grabix)&#xD;&#xA;&#xD;&#xA;In bash:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | bgzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;    grabix index output.fastq.gz&#xD;&#xA;&#xD;&#xA;    # retrieve 5-th record (1-based) in log(n) time&#xD;&#xA;    # requires some math to convert indices (4*4 + 1, 4*4 + 4) = (17, 20)&#xD;&#xA;    grabix grab output.fastq.gz 17 20&#xD;&#xA;&#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_LINES=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;&#xD;&#xA;## Using `tabix`&#xD;&#xA;&#xD;&#xA;The tabix code is more complicated and relies on the iffy assumption that `\t` is an acceptable character for replacement of `\n` in a FASTQ record.  If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&#xD;&#xA;&#xD;&#xA;1. Paste each record into a single line.&#xD;&#xA;2. Add a dummy chromosome and line number as the first and second column.&#xD;&#xA;3. Compress with `bgzip`.&#xD;&#xA;4. Index the file and perform lookups with [`tabix`](http://www.htslib.org/doc/tabix.html)&#xD;&#xA;&#xD;&#xA;Note that we need to remove leading spaces introduced by `nl` and we need to introduce a dummy chromosome column to keep `tabix` happy:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &gt; output.fastq.gz&#xD;&#xA;    tabix -s 1 -b 2 -e 2 output.fastq.gz &#xD;&#xA;    &#xD;&#xA;    # now retrieve the 5th record (1-based) in log(n) time&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 &#xD;&#xA;&#xD;&#xA;    # This command will retrieve the 5th record and convert it record back into FASTQ format&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&#xD;&#xA;    &#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_RECORDS=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;    &#xD;&#xA;# Random record in constant time&#xD;&#xA;&#xD;&#xA;Now that we have a way of retrieving an arbitrary record in `log(n)` time, retrieving a random record is simply a matter of getting a good random number generator and sampling. Here is some example code to do this in python:&#xD;&#xA;&#xD;&#xA;## Using grabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_LINES&quot;]) // 4&#xD;&#xA;    rand_record_start = random.randrange(0, n_records) * 4 + 1&#xD;&#xA;    rand_record_end = rand_record_start + 3&#xD;&#xA;    os.system(&quot;grabix grab output.fastq.gz {0} {1}&quot;.format(rand_record_start, rand_record_end))&#xD;&#xA;&#xD;&#xA;## Using tabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_RECORDS&quot;])&#xD;&#xA;    rand_record_index = random.randrange(0, n_records) + 1&#xD;&#xA;    # super ugly, but works...&#xD;&#xA;    os.system(&#xD;&#xA;        &quot;tabix output.fastq.gz dummy:{0}-{0} | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&quot;.format(&#xD;&#xA;            rand_record_index)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;And this works for me:&#xD;&#xA;&#xD;&#xA;    python3.5 random_read.py&#xD;&#xA;&#xD;&#xA;# Disclaimer&#xD;&#xA;&#xD;&#xA;Please note that `os.system` calls a system shell and is vulnerable to [shell injection vulnerabilities](https://en.wikipedia.org/wiki/Code_injection#Shell_injection). If you are writing production code, then you probably want to [take extra precautions](https://docs.python.org/3/library/subprocess.html#security-considerations).&#xD;&#xA;&#xD;&#xA;Thanks to @Chris_Rands for raising this issue." />
  <row Id="2689" PostHistoryTypeId="4" PostId="844" RevisionGUID="9626743c-837c-4275-832c-cb103840cef9" CreationDate="2017-06-22T03:02:52.990" UserId="734" Comment="edited title" Text="How can I find the relevant pathway map from gene-gene or protein-protein interaction list?" />
  <row Id="2690" PostHistoryTypeId="5" PostId="813" RevisionGUID="b37f956a-ba6c-42c4-855a-a300d183ef10" CreationDate="2017-06-22T07:23:52.890" UserId="492" Comment="Mentions don't work as expected in answers" Text="# Arbitrary record access in constant time&#xD;&#xA;&#xD;&#xA;To get a random record in constant time, it is sufficient to get an arbitrary record in constant time.&#xD;&#xA;&#xD;&#xA;I have two solutions here: One with `tabix` and one with `grabix`. I think the `grabix` solution is more elegant, but I am keeping the `tabix` solution below because `tabix` is a more mature tool than `grabix`.&#xD;&#xA; &#xD;&#xA;Thanks to [user172818](https://bioinformatics.stackexchange.com/users/37/user172818) for suggesting `grabix`.&#xD;&#xA;&#xD;&#xA;### Update&#xD;&#xA;&#xD;&#xA;This answer previously stated that `tabix` and `grabix` perform lookups in `log(n)` time. After taking a closer look at the grabix source code and the tabix paper I am now convinced that lookups are independent of `n` in complexity. However, both tools use an index that scales in size proportionally to `n`. So, the loading of the index is order `n`. However, if we consider the loading of the index as &quot;...a single limited transformation of the data to another file format...&quot;, then I think this answer is still is a valid one. If more than one record is to be retrieved, then the index needs to be stored in memory, perhaps with a framework such as pysam or htslib.&#xD;&#xA;&#xD;&#xA;## Using `grabix`&#xD;&#xA;&#xD;&#xA;1. Compress with `bgzip`.&#xD;&#xA;2. Index the file and perform lookups with [`grabix`](https://github.com/arq5x/grabix)&#xD;&#xA;&#xD;&#xA;In bash:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | bgzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;    grabix index output.fastq.gz&#xD;&#xA;&#xD;&#xA;    # retrieve 5-th record (1-based) in log(n) time&#xD;&#xA;    # requires some math to convert indices (4*4 + 1, 4*4 + 4) = (17, 20)&#xD;&#xA;    grabix grab output.fastq.gz 17 20&#xD;&#xA;&#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_LINES=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;&#xD;&#xA;## Using `tabix`&#xD;&#xA;&#xD;&#xA;The tabix code is more complicated and relies on the iffy assumption that `\t` is an acceptable character for replacement of `\n` in a FASTQ record.  If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&#xD;&#xA;&#xD;&#xA;1. Paste each record into a single line.&#xD;&#xA;2. Add a dummy chromosome and line number as the first and second column.&#xD;&#xA;3. Compress with `bgzip`.&#xD;&#xA;4. Index the file and perform lookups with [`tabix`](http://www.htslib.org/doc/tabix.html)&#xD;&#xA;&#xD;&#xA;Note that we need to remove leading spaces introduced by `nl` and we need to introduce a dummy chromosome column to keep `tabix` happy:&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &gt; output.fastq.gz&#xD;&#xA;    tabix -s 1 -b 2 -e 2 output.fastq.gz &#xD;&#xA;    &#xD;&#xA;    # now retrieve the 5th record (1-based) in log(n) time&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 &#xD;&#xA;&#xD;&#xA;    # This command will retrieve the 5th record and convert it record back into FASTQ format&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&#xD;&#xA;    &#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_RECORDS=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;    &#xD;&#xA;# Random record in constant time&#xD;&#xA;&#xD;&#xA;Now that we have a way of retrieving an arbitrary record in `log(n)` time, retrieving a random record is simply a matter of getting a good random number generator and sampling. Here is some example code to do this in python:&#xD;&#xA;&#xD;&#xA;## Using grabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_LINES&quot;]) // 4&#xD;&#xA;    rand_record_start = random.randrange(0, n_records) * 4 + 1&#xD;&#xA;    rand_record_end = rand_record_start + 3&#xD;&#xA;    os.system(&quot;grabix grab output.fastq.gz {0} {1}&quot;.format(rand_record_start, rand_record_end))&#xD;&#xA;&#xD;&#xA;## Using tabix&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_RECORDS&quot;])&#xD;&#xA;    rand_record_index = random.randrange(0, n_records) + 1&#xD;&#xA;    # super ugly, but works...&#xD;&#xA;    os.system(&#xD;&#xA;        &quot;tabix output.fastq.gz dummy:{0}-{0} | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&quot;.format(&#xD;&#xA;            rand_record_index)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;And this works for me:&#xD;&#xA;&#xD;&#xA;    python3.5 random_read.py&#xD;&#xA;&#xD;&#xA;# Disclaimer&#xD;&#xA;&#xD;&#xA;Please note that `os.system` calls a system shell and is vulnerable to [shell injection vulnerabilities](https://en.wikipedia.org/wiki/Code_injection#Shell_injection). If you are writing production code, then you probably want to [take extra precautions](https://docs.python.org/3/library/subprocess.html#security-considerations).&#xD;&#xA;&#xD;&#xA;Thanks to [Chris_Rands](https://bioinformatics.stackexchange.com/users/104/chris-rands) for raising this issue." />
  <row Id="2691" PostHistoryTypeId="5" PostId="864" RevisionGUID="c7f1284a-f4e4-4853-a161-8cbf567cab3e" CreationDate="2017-06-22T08:56:15.450" UserId="48" Comment="Remove capslock" Text="You should read my [paper][1], where I did a comprehensive parameter sweep across different thresholds and definitions and benchmarked the resulting empirical potentials on a decoy ranking challenge:&#xD;&#xA;&#xD;&#xA;The relevant phrase from the abstract is &quot;compared using 90 different definitions of residue-residue contact&quot;. However, it boils down (as always) to the question, &quot;what do you want to do&quot;.&#xD;&#xA;&#xD;&#xA;Cite me if you use my paper for your research. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bmcstructbiol.biomedcentral.com/articles/10.1186/1472-6807-8-53" />
  <row Id="2692" PostHistoryTypeId="24" PostId="864" RevisionGUID="c7f1284a-f4e4-4853-a161-8cbf567cab3e" CreationDate="2017-06-22T08:56:15.450" Comment="Proposed by 48 approved by 77, 57 edit id of 219" />
  <row Id="2693" PostHistoryTypeId="2" PostId="873" RevisionGUID="55d160a4-deca-4f79-965b-eb6863696487" CreationDate="2017-06-22T09:17:53.090" UserId="941" Text="Running `blastdbcmd -db foo -info` provides a little information but I haven't seen anything which will report exactly how a blastdb was created.&#xD;&#xA;&#xD;&#xA;A [blastdb readme][1] suggests that only the `parse_seqids` option has been added to the standard parameters.&#xD;&#xA;&#xD;&#xA;&gt;For those from NCBI, the following makeblastdb commands are recommended:&#xD;&#xA;&#xD;&#xA; &gt; - For nucleotide fasta file:   makeblastdb -in input_db -dbtype nucl -parse_seqids&#xD;&#xA; &gt; - For protein fasta file:      makeblastdb -in input_db -dbtype prot -parse_seqids&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: ftp://ftp.ncbi.nlm.nih.gov/blast/documents/blastdb.html" />
  <row Id="2694" PostHistoryTypeId="2" PostId="874" RevisionGUID="c01b9cf1-f5f7-4ce0-a4cc-a99ba0ea85b2" CreationDate="2017-06-22T10:17:23.743" UserId="943" Text="I am trying to find intersection of two genomic ranges (gr1 and gr2) and keep metadata from one of them&#xD;&#xA;&#xD;&#xA;    gr1&#xD;&#xA;    chrI [1, 100] * | 0.1&#xD;&#xA;    chrI [101, 200] * | 0.2&#xD;&#xA;&#xD;&#xA;    gr2&#xD;&#xA;    chrI [50, 150] + | &#xD;&#xA;&#xD;&#xA;intersect(gr1, gr2) will remove metadata, where&#xD;&#xA;subsetByOverlaps(gr1, gr2) return:&#xD;&#xA;&#xD;&#xA;    chrI [1, 100] * | 0.1&#xD;&#xA;    chrI [101, 200] * | 0.2&#xD;&#xA;where metadata is saved, but intersection is bigger than real (the same as in initial gr1 start/end)&#xD;&#xA;&#xD;&#xA;How can I get an intersection range and keep metadata?&#xD;&#xA;to get the range:&#xD;&#xA;&#xD;&#xA;    chrI [50, 100] * | 0.1&#xD;&#xA;    chrI [101, 150] * | 0.2&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2695" PostHistoryTypeId="1" PostId="874" RevisionGUID="c01b9cf1-f5f7-4ce0-a4cc-a99ba0ea85b2" CreationDate="2017-06-22T10:17:23.743" UserId="943" Text="Intersection of two genomic ranges to keep metadata (Bioconductor / GRanges)" />
  <row Id="2696" PostHistoryTypeId="3" PostId="874" RevisionGUID="c01b9cf1-f5f7-4ce0-a4cc-a99ba0ea85b2" CreationDate="2017-06-22T10:17:23.743" UserId="943" Text="&lt;r&gt;&lt;bioconductor&gt;" />
  <row Id="2697" PostHistoryTypeId="5" PostId="861" RevisionGUID="67b47c62-734f-4027-8f38-17d160914aa2" CreationDate="2017-06-22T11:04:45.133" UserId="191" Comment="add syntax highlighting" Text="As [wkretzsch](https://bioinformatics.stackexchange.com/users/492/wkretzsch) suggested this was worthy of an actual answer, I feel the obvious solution is missing here; index the `FASTQ`.&#xD;&#xA;&#xD;&#xA;Index it&#xD;&#xA;--------&#xD;&#xA;&#xD;&#xA;As much as I typically hesitate to jump to a solution that requires a script or framework (as opposed to just unix command line tools), there is sadly no `samtools fqidx` (perhaps there should be), and existing answers suggest a lot of munging. Whilst they probably work, some appear cumbersome and have many steps in which you could make a mistake.&#xD;&#xA;&#xD;&#xA;So, to keep things simple - a quick and dirty alternative approach might be to use `biopython`, seeing as it [already has this functionality implemented](http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc291) to do this, and if installed, is trivial to use:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    fq = SeqIO.index(&quot;myfastq.fq&quot;, &quot;fastq&quot;)&#xD;&#xA;&#xD;&#xA;Once you've acquired an index, you'll get fast random access for any read:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    # Random access by read name (I like owls)&#xD;&#xA;    record = fq[&quot;HOOT&quot;]&#xD;&#xA;    record&#xD;&#xA;    #&gt; SeqRecord(seq=Seq('ACGTACGT', SingleLetterAlphabet()), id='HOOT', name='HOOT', description='HOOT', dbxrefs=[])&#xD;&#xA;&#xD;&#xA;    # We can get the sequence&#xD;&#xA;    record.seq&#xD;&#xA;    #&gt; Seq('ACGTACGT', SingleLetterAlphabet())&#xD;&#xA;&#xD;&#xA;    # and qualities&#xD;&#xA;    record.letter_annotations&#xD;&#xA;    #&gt; {'phred_quality': [41, 41, 41, 41, 41, 41, 41, 41]}&#xD;&#xA;&#xD;&#xA;If you want to select arbitrary random records, you could use something like [`randrange`](https://docs.python.org/3.1/library/random.html#random.randrange) to select between 0 and the length of the references list.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    from random import randint&#xD;&#xA;&#xD;&#xA;    # Coerce keys to a list, as `dictionary-keyiterator` has no&#xD;&#xA;    #   __getitem__ attribute that would allow integer indices&#xD;&#xA;    # Note also that this doesn't necessarily guarantee a sorted order&#xD;&#xA;    #  but I guess that doesn't matter if you just want random records&#xD;&#xA;    key_list = list(fq.keys())&#xD;&#xA;&#xD;&#xA;    # Select a random key&#xD;&#xA;    # Note we use len(key_list)-1 as randint endpoints are inclusive&#xD;&#xA;    random_readname = key_list[ randint(0, len(key_list)-1) ]&#xD;&#xA;&#xD;&#xA;    # Get your record&#xD;&#xA;    rand_record = fq.get( random_readname )&#xD;&#xA;&#xD;&#xA;If you want multiple records, you'll probably want [`sample`](https://docs.python.org/2/library/random.html#random.sample) (to avoid replacement) instead:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    from random import sample&#xD;&#xA;    N = 100    &#xD;&#xA;    random_indices = sample(xrange(len(key_list)), N)&#xD;&#xA;&#xD;&#xA;    for key_i in random_indices:&#xD;&#xA;        random_readname = key_list[ key_i ]&#xD;&#xA;        rand_record = fq.get( random_readname )&#xD;&#xA;        # ...&#xD;&#xA;&#xD;&#xA;For what it's worth, I think `biopython` holds this index in RAM, so if your file is absolutely massive, you might need to be more clever. If that's the case, you could iterate through the `FASTQ` once, and output the readname, file offset and length - akin to a `FASTA` `fai`." />
  <row Id="2698" PostHistoryTypeId="5" PostId="813" RevisionGUID="07bdecb0-b3b2-4ed1-bfcb-4e04b163056a" CreationDate="2017-06-22T11:05:54.213" UserId="191" Comment="add syntax highlighting" Text="# Arbitrary record access in constant time&#xD;&#xA;&#xD;&#xA;To get a random record in constant time, it is sufficient to get an arbitrary record in constant time.&#xD;&#xA;&#xD;&#xA;I have two solutions here: One with `tabix` and one with `grabix`. I think the `grabix` solution is more elegant, but I am keeping the `tabix` solution below because `tabix` is a more mature tool than `grabix`.&#xD;&#xA; &#xD;&#xA;Thanks to [user172818](https://bioinformatics.stackexchange.com/users/37/user172818) for suggesting `grabix`.&#xD;&#xA;&#xD;&#xA;### Update&#xD;&#xA;&#xD;&#xA;This answer previously stated that `tabix` and `grabix` perform lookups in `log(n)` time. After taking a closer look at the grabix source code and the tabix paper I am now convinced that lookups are independent of `n` in complexity. However, both tools use an index that scales in size proportionally to `n`. So, the loading of the index is order `n`. However, if we consider the loading of the index as &quot;...a single limited transformation of the data to another file format...&quot;, then I think this answer is still is a valid one. If more than one record is to be retrieved, then the index needs to be stored in memory, perhaps with a framework such as pysam or htslib.&#xD;&#xA;&#xD;&#xA;## Using `grabix`&#xD;&#xA;&#xD;&#xA;1. Compress with `bgzip`.&#xD;&#xA;2. Index the file and perform lookups with [`grabix`](https://github.com/arq5x/grabix)&#xD;&#xA;&#xD;&#xA;In bash:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | bgzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;    grabix index output.fastq.gz&#xD;&#xA;&#xD;&#xA;    # retrieve 5-th record (1-based) in log(n) time&#xD;&#xA;    # requires some math to convert indices (4*4 + 1, 4*4 + 4) = (17, 20)&#xD;&#xA;    grabix grab output.fastq.gz 17 20&#xD;&#xA;&#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_LINES=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;&#xD;&#xA;## Using `tabix`&#xD;&#xA;&#xD;&#xA;The tabix code is more complicated and relies on the iffy assumption that `\t` is an acceptable character for replacement of `\n` in a FASTQ record.  If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&#xD;&#xA;&#xD;&#xA;1. Paste each record into a single line.&#xD;&#xA;2. Add a dummy chromosome and line number as the first and second column.&#xD;&#xA;3. Compress with `bgzip`.&#xD;&#xA;4. Index the file and perform lookups with [`tabix`](http://www.htslib.org/doc/tabix.html)&#xD;&#xA;&#xD;&#xA;Note that we need to remove leading spaces introduced by `nl` and we need to introduce a dummy chromosome column to keep `tabix` happy:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &gt; output.fastq.gz&#xD;&#xA;    tabix -s 1 -b 2 -e 2 output.fastq.gz &#xD;&#xA;    &#xD;&#xA;    # now retrieve the 5th record (1-based) in log(n) time&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 &#xD;&#xA;&#xD;&#xA;    # This command will retrieve the 5th record and convert it record back into FASTQ format&#xD;&#xA;    tabix output.fastq.gz dummy:5-5 | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&#xD;&#xA;    &#xD;&#xA;    # Count the number of records for part two of this question&#xD;&#xA;    export N_RECORDS=$(gzip -dc output.fastq.gz | wc -l)&#xD;&#xA;    &#xD;&#xA;# Random record in constant time&#xD;&#xA;&#xD;&#xA;Now that we have a way of retrieving an arbitrary record in `log(n)` time, retrieving a random record is simply a matter of getting a good random number generator and sampling. Here is some example code to do this in python:&#xD;&#xA;&#xD;&#xA;## Using grabix&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_LINES&quot;]) // 4&#xD;&#xA;    rand_record_start = random.randrange(0, n_records) * 4 + 1&#xD;&#xA;    rand_record_end = rand_record_start + 3&#xD;&#xA;    os.system(&quot;grabix grab output.fastq.gz {0} {1}&quot;.format(rand_record_start, rand_record_end))&#xD;&#xA;&#xD;&#xA;## Using tabix&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    # random_read.py&#xD;&#xA;    import os&#xD;&#xA;    import random&#xD;&#xA;    &#xD;&#xA;    n_records = int(os.environ[&quot;N_RECORDS&quot;])&#xD;&#xA;    rand_record_index = random.randrange(0, n_records) + 1&#xD;&#xA;    # super ugly, but works...&#xD;&#xA;    os.system(&#xD;&#xA;        &quot;tabix output.fastq.gz dummy:{0}-{0} | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&quot;.format(&#xD;&#xA;            rand_record_index)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;And this works for me:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    python3.5 random_read.py&#xD;&#xA;&#xD;&#xA;# Disclaimer&#xD;&#xA;&#xD;&#xA;Please note that `os.system` calls a system shell and is vulnerable to [shell injection vulnerabilities](https://en.wikipedia.org/wiki/Code_injection#Shell_injection). If you are writing production code, then you probably want to [take extra precautions](https://docs.python.org/3/library/subprocess.html#security-considerations).&#xD;&#xA;&#xD;&#xA;Thanks to [Chris_Rands](https://bioinformatics.stackexchange.com/users/104/chris-rands) for raising this issue." />
  <row Id="2699" PostHistoryTypeId="5" PostId="808" RevisionGUID="f20b6d58-c217-4f44-bf58-dc49e63e0a2e" CreationDate="2017-06-22T11:06:17.097" UserId="191" Comment="add syntax highlighting" Text="You could shuffle the FASTQ once and then read sequences off the top of the file as you need them:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | shuf | tr '\t' '\n'| gzip -c &gt; output.fastq.gz&#xD;&#xA;&#xD;&#xA;I would recommend [pigz](https://zlib.net/pigz/) as a replacement for gzip in the compression step if you have it available.&#xD;&#xA;&#xD;&#xA;The downside of this approach is that you only get `n` reads before you need to run the shuffle again, and [apparently](https://stackoverflow.com/a/24492814/528691) `shuf` holds all data in RAM, so it would die with an out of memory error if the FASTQ file does not fit into RAM as is specified in the question.&#xD;&#xA;&#xD;&#xA;Using `sort -R` is complexity `n log(n)` and uses temporary files, so it [should not](https://vkundeti.blogspot.co.uk/2008/03/tech-algorithmic-details-of-unix-sort.html) run out of memory:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    gzip -dc input.fastq.gz | paste - - - - | nl | sort -R | perl -pe 's/\s*\d+\t//' | tr '\t' '\n'| gzip -c &gt; output.fastq.gz&#xD;&#xA;    &#xD;&#xA;The `nl` and `perl` commands are necessary to make sure that identical records are not sorted next to each other." />
  <row Id="2700" PostHistoryTypeId="5" PostId="822" RevisionGUID="0846af6e-5c39-43c2-8f7b-667e2816c9d1" CreationDate="2017-06-22T11:07:13.503" UserId="191" Comment="add syntax highlighting" Text="Here's another approach that doesn't require any indexing, using [BEDOPS `bedextract`][1] to do a `log(n)` sample on a sorted BED file. Your sample contains random records with equal probability `1/n`.&#xD;&#xA;&#xD;&#xA;This approach requires a single `O(n)` pass through the file to transform it to a BED file:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    $ cat records.fastq | paste - - - - | awk '{ print &quot;chrZ\t&quot;s&quot;\t&quot;(s+1)&quot;$0 }' &gt; records.bed&#xD;&#xA;&#xD;&#xA;Store the intervals in a separate file:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    $ cut -f1-3 records.bed &gt; intervals.bed&#xD;&#xA;&#xD;&#xA;To do a random sample of `k` elements, shuffle the intervals file and preserve the order of shuffled elements. &#xD;&#xA;&#xD;&#xA;You can do this with the [`sample` tool][2] I outlined earlier:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed &gt; intervals-sample.bed&#xD;&#xA;&#xD;&#xA;Or you can `shuf` and `sort-bed` to do the same thing:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    $ shuf -n ${K} intervals.bed | sort-bed - &gt; intervals-sample.bed&#xD;&#xA;&#xD;&#xA;There's an `O(klog(k))` cost here, but if `k &lt;&lt;&lt; n`, i.e., you're working with whole-genome scale input, this cost is amortized over the `log(n)` search performance.&#xD;&#xA;&#xD;&#xA;Next, use `bedextract` to do a binary search on the records, and delinearize to get back to FASTQ:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    $ bedextract records.bed intervals-sample.bed | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;With Unix I/O streams, this can be done in one pass:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    $ sample -k ${K} -s intervals.bed | bedextract records.bed - | cut -f4 | tr '\t' '\n' &gt; sample.fq&#xD;&#xA;&#xD;&#xA;By baking the sort order into `records.bed`, you're guaranteed the ability to do a binary search, which is `log(n)`.&#xD;&#xA;&#xD;&#xA;*Note:* Further, by linearizing the FASTQ input to a BED file and querying on BED intervals, you have equal probability of picking any one interval (interval == FQ record). You can draw an unbiased sample without the hassle of creating and storing a separate index.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/content/reference/set-operations/bedextract.html&#xD;&#xA;  [2]: https://github.com/alexpreynolds/sample" />
  <row Id="2701" PostHistoryTypeId="5" PostId="820" RevisionGUID="b420c81b-5c07-4d7a-91a9-3aecbf5d3af1" CreationDate="2017-06-22T11:07:39.083" UserId="191" Comment="add syntax highlighting" Text="I wrote [a tool called `sample`][1] that you can use to do random sampling without reading the entire file into memory. &#xD;&#xA;&#xD;&#xA;It can be used where GNU `shuf` fails for lack of sufficient memory. &#xD;&#xA;&#xD;&#xA;It requires two passes through the file to do a random sample, but the second pass is generally fast(er) as it uses `mmap` routines to do cached reads. &#xD;&#xA;&#xD;&#xA;If you do repeated samples, the repeated samples are also `mmap`-ed (cached) and will run quickly.&#xD;&#xA;&#xD;&#xA;You might use it on a FASTQ file like so:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    $ sample -k 1234 -l 4 in.fq &gt; out.fq&#xD;&#xA;&#xD;&#xA;It parses the input file into records by every four newline characters (such as the format of a FASTQ file), reading line offset positions into memory. So the memory overhead is relatively very low.&#xD;&#xA;&#xD;&#xA;It then applies reservoir sampling on those line offsets to write out a random sample (say, `1234` records in this example) to standard output.&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/alexpreynolds/sample" />
  <row Id="2702" PostHistoryTypeId="2" PostId="875" RevisionGUID="6e430a8f-a4b2-4bd4-807c-914b165e1b95" CreationDate="2017-06-22T11:14:42.393" UserId="77" Text="Presuming you don't have cases of multiple ovelaps&#xD;&#xA;&#xD;&#xA;&lt;!-- language: R --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; library(GenomicRanges)&#xD;&#xA;    &gt; gr1 = GRanges(c(&quot;chrI&quot;, &quot;chrI&quot;), IRanges(start=c(1, 101), width=c(100, 100)), mcols=data.frame(foo=c(0.1, 0.2)))&#xD;&#xA;    &gt; gr2 = GRanges(c(&quot;chrI&quot;), IRanges(start=c(50), width=c(101)))&#xD;&#xA;    &gt; o = findOverlaps(gr1, gr2)&#xD;&#xA;    &gt; grl1 = split(gr1[queryHits(o)], 1:length(o)) # You can't mendoapply on a GRanges object&#xD;&#xA;    &gt; grl2 = split(gr2[subjectHits(o)], 1:length(o))&#xD;&#xA;    &gt; foo = function(x, y) {&#xD;&#xA;    +     rv = x&#xD;&#xA;    +     start(rv) = max(start(x), start(y))&#xD;&#xA;    +     end(rv) = min(end(x), end(y))&#xD;&#xA;    +     return(rv)&#xD;&#xA;    + }&#xD;&#xA;    &gt; unlist(mendoapply(foo, grl1, y=grl2))&#xD;&#xA;&#xD;&#xA;The output is:&#xD;&#xA;&#xD;&#xA;    GRanges object with 2 ranges and 1 metadata column:&#xD;&#xA;        seqnames     ranges strand | mcols.foo&#xD;&#xA;           &lt;Rle&gt;  &lt;IRanges&gt;  &lt;Rle&gt; | &lt;numeric&gt;&#xD;&#xA;      1     chrI [ 50, 100]      * |       0.1&#xD;&#xA;      2     chrI [101, 150]      * |       0.2&#xD;&#xA;      -------&#xD;&#xA;      seqinfo: 1 sequence from an unspecified genome; no seqlengths&#xD;&#xA;&#xD;&#xA;There may be better ways to do this (e.g., iterating over `o` would be less memory demanding)." />
  <row Id="2703" PostHistoryTypeId="5" PostId="875" RevisionGUID="9856d9cf-7791-461c-8318-971f3b650a29" CreationDate="2017-06-22T11:21:16.307" UserId="77" Comment="deleted 10 characters in body" Text="Presuming you don't have cases of multiple ovelaps&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; library(GenomicRanges)&#xD;&#xA;    &gt; gr1 = GRanges(c(&quot;chrI&quot;, &quot;chrI&quot;), IRanges(start=c(1, 101), width=c(100, 100)), mcols=data.frame(foo=c(0.1, 0.2)))&#xD;&#xA;    &gt; gr2 = GRanges(c(&quot;chrI&quot;), IRanges(start=c(50), width=c(101)))&#xD;&#xA;    &gt; o = findOverlaps(gr1, gr2)&#xD;&#xA;    &gt; grl1 = split(gr1[queryHits(o)], 1:length(o)) # You can't mendoapply on a GRanges object&#xD;&#xA;    &gt; grl2 = split(gr2[subjectHits(o)], 1:length(o))&#xD;&#xA;    &gt; foo = function(x, y) {&#xD;&#xA;    +     rv = x&#xD;&#xA;    +     start(rv) = max(start(x), start(y))&#xD;&#xA;    +     end(rv) = min(end(x), end(y))&#xD;&#xA;    +     return(rv)&#xD;&#xA;    + }&#xD;&#xA;    &gt; unlist(mendoapply(foo, grl1, y=grl2))&#xD;&#xA;&#xD;&#xA;The output is:&#xD;&#xA;&#xD;&#xA;    GRanges object with 2 ranges and 1 metadata column:&#xD;&#xA;        seqnames     ranges strand | mcols.foo&#xD;&#xA;           &lt;Rle&gt;  &lt;IRanges&gt;  &lt;Rle&gt; | &lt;numeric&gt;&#xD;&#xA;      1     chrI [ 50, 100]      * |       0.1&#xD;&#xA;      2     chrI [101, 150]      * |       0.2&#xD;&#xA;      -------&#xD;&#xA;      seqinfo: 1 sequence from an unspecified genome; no seqlengths&#xD;&#xA;&#xD;&#xA;It'd be more memory efficient to iterate over `o`, but this gives you the idea." />
  <row Id="2704" PostHistoryTypeId="5" PostId="840" RevisionGUID="cc6a8579-7c25-43ea-824f-1a5868941267" CreationDate="2017-06-22T11:22:23.687" UserId="77" Comment="syntax highlighting" Text="You're reinventing `bedtools intersect` (or bedops), for which there's [already a convenient python module](https://daler.github.io/pybedtools/):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    from pybedtools import BedTool&#xD;&#xA;&#xD;&#xA;    s3 = BedTool('s3.bed')&#xD;&#xA;    s4 = BedTool('s4.bed')&#xD;&#xA;&#xD;&#xA;    print(s4.intersect(s3, wa=True, wb=True, F=1))&#xD;&#xA;&#xD;&#xA;The `wb=True` is equivalent to `-wb` with `bedtools intersect` on the command line. Similarly, `F=1` is the same as `-F 1`." />
  <row Id="2705" PostHistoryTypeId="5" PostId="875" RevisionGUID="d691d6a3-ff94-4c5c-963a-65c4052728d2" CreationDate="2017-06-22T11:32:27.683" UserId="77" Comment="added 186 characters in body" Text="Presuming you don't have cases of multiple ovelaps&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; library(GenomicRanges)&#xD;&#xA;    &gt; gr1 = GRanges(c(&quot;chrI&quot;, &quot;chrI&quot;), IRanges(start=c(1, 101), width=c(100, 100)), mcols=data.frame(foo=c(0.1, 0.2)))&#xD;&#xA;    &gt; gr2 = GRanges(c(&quot;chrI&quot;), IRanges(start=c(50), width=c(101)))&#xD;&#xA;    &gt; o = findOverlaps(gr1, gr2)&#xD;&#xA;    &gt; grl1 = split(gr1[queryHits(o)], 1:length(o)) # You can't mendoapply on a GRanges object&#xD;&#xA;    &gt; grl2 = split(gr2[subjectHits(o)], 1:length(o))&#xD;&#xA;    &gt; foo = function(x, y) {&#xD;&#xA;    +     rv = x&#xD;&#xA;    +     start(rv) = max(start(x), start(y))&#xD;&#xA;    +     end(rv) = min(end(x), end(y))&#xD;&#xA;    +     return(rv)&#xD;&#xA;    + }&#xD;&#xA;    &gt; unlist(mendoapply(foo, grl1, y=grl2))&#xD;&#xA;&#xD;&#xA;The output is:&#xD;&#xA;&#xD;&#xA;    GRanges object with 2 ranges and 1 metadata column:&#xD;&#xA;        seqnames     ranges strand | mcols.foo&#xD;&#xA;           &lt;Rle&gt;  &lt;IRanges&gt;  &lt;Rle&gt; | &lt;numeric&gt;&#xD;&#xA;      1     chrI [ 50, 100]      * |       0.1&#xD;&#xA;      2     chrI [101, 150]      * |       0.2&#xD;&#xA;      -------&#xD;&#xA;      seqinfo: 1 sequence from an unspecified genome; no seqlengths&#xD;&#xA;&#xD;&#xA;It'd be more memory efficient to iterate over `o`, but this gives you the idea.&#xD;&#xA;&#xD;&#xA;In cases of multiple overlaps, one could use `intersect()`, then `findOverlaps()` with that and then continue on with the procedure demonstrated above (or iterate over the overlaps)." />
  <row Id="2706" PostHistoryTypeId="5" PostId="874" RevisionGUID="d68ff50c-b750-401b-a807-89677d3f5135" CreationDate="2017-06-22T12:25:41.493" UserId="131" Comment="removed tags from title" Text="I am trying to find intersection of two genomic ranges (gr1 and gr2) and keep metadata from one of them&#xD;&#xA;&#xD;&#xA;    gr1&#xD;&#xA;    chrI [1, 100] * | 0.1&#xD;&#xA;    chrI [101, 200] * | 0.2&#xD;&#xA;&#xD;&#xA;    gr2&#xD;&#xA;    chrI [50, 150] + | &#xD;&#xA;&#xD;&#xA;`intersect(gr1, gr2)` will remove metadata, where&#xD;&#xA;`subsetByOverlaps(gr1, gr2)` return:&#xD;&#xA;&#xD;&#xA;    chrI [1, 100] * | 0.1&#xD;&#xA;    chrI [101, 200] * | 0.2&#xD;&#xA;&#xD;&#xA;where metadata is saved, but intersection is bigger than real (the same as in initial gr1 start/end)&#xD;&#xA;&#xD;&#xA;How can I get an intersection range and keep metadata?&#xD;&#xA;to get the range:&#xD;&#xA;&#xD;&#xA;    chrI [50, 100] * | 0.1&#xD;&#xA;    chrI [101, 150] * | 0.2&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2707" PostHistoryTypeId="4" PostId="874" RevisionGUID="d68ff50c-b750-401b-a807-89677d3f5135" CreationDate="2017-06-22T12:25:41.493" UserId="131" Comment="removed tags from title" Text="Intersection of two genomic ranges to keep metadata" />
  <row Id="2708" PostHistoryTypeId="6" PostId="874" RevisionGUID="d68ff50c-b750-401b-a807-89677d3f5135" CreationDate="2017-06-22T12:25:41.493" UserId="131" Comment="removed tags from title" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;granges&gt;" />
  <row Id="2709" PostHistoryTypeId="24" PostId="874" RevisionGUID="d68ff50c-b750-401b-a807-89677d3f5135" CreationDate="2017-06-22T12:25:41.493" Comment="Proposed by 131 approved by 943 edit id of 220" />
  <row Id="2713" PostHistoryTypeId="4" PostId="807" RevisionGUID="e5385c22-b417-4867-b38f-960cc31dcb65" CreationDate="2017-06-22T16:32:55.203" UserId="492" Comment="Simplified title according to https://meta.stackexchange.com/questions/10647/how-do-i-write-a-good-title" Text="Random access on a FASTQ file" />
  <row Id="2714" PostHistoryTypeId="2" PostId="877" RevisionGUID="073e2af7-45f3-4928-a62a-389290a39cfb" CreationDate="2017-06-22T18:05:28.307" UserId="734" Text="I have seen that there are 2 used [Patient-derived models][1]: [PDX][2] and [PDO][3] to reflect tumor biology. To me it sounds very specific and I was curious if there's any common practice or possibility to use PDX and PDO in more generic way. For example to predict if a drug would work without wet experiment, purely computationally.  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&#xD;&#xA;  [3]: http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html" />
  <row Id="2715" PostHistoryTypeId="1" PostId="877" RevisionGUID="073e2af7-45f3-4928-a62a-389290a39cfb" CreationDate="2017-06-22T18:05:28.307" UserId="734" Text="What are the differences between PDXs and PDOs and how they are used in bioinformatics?" />
  <row Id="2716" PostHistoryTypeId="3" PostId="877" RevisionGUID="073e2af7-45f3-4928-a62a-389290a39cfb" CreationDate="2017-06-22T18:05:28.307" UserId="734" Text="&lt;cancer&gt;" />
  <row Id="2717" PostHistoryTypeId="5" PostId="877" RevisionGUID="9ef8ff2c-fc76-47a0-a695-ebb2e163626f" CreationDate="2017-06-22T18:17:34.363" UserId="734" Comment="added 11 characters in body" Text="I have seen that there are 2 common methods: [Patient-derived models][1]: [PDX][2] and [PDO][3] to reflect tumor biology. To me it sounds very specific and I was curious if there's any common practice or possibility to use PDX and PDO in more generic way. For example to predict if a drug would work without wet experiment, purely computationally.  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&#xD;&#xA;  [3]: http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html" />
  <row Id="2718" PostHistoryTypeId="5" PostId="877" RevisionGUID="82e0e666-d9bb-4762-a6e8-dc30607861bd" CreationDate="2017-06-22T18:23:17.520" UserId="734" Comment="added 43 characters in body; edited title" Text="I have seen that there are 2 common methods: [Patient-derived models][1]: [PDX][2] and [PDO][3] to reflect tumor biology. &#xD;&#xA;First, What are the differences between PDX and PDO. Secondly, they both sound very specific and I was curious if it's possible to use them to build a predictive model, if yes I would appreciate to have an example or guidance on how to start to work with such databases.&#xD;&#xA;&#xD;&#xA;  [1]: https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&#xD;&#xA;  [3]: http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html" />
  <row Id="2719" PostHistoryTypeId="4" PostId="877" RevisionGUID="82e0e666-d9bb-4762-a6e8-dc30607861bd" CreationDate="2017-06-22T18:23:17.520" UserId="734" Comment="added 43 characters in body; edited title" Text="Are there any databases of PDO/PDX experiments with their outcomes that one could use to create a predictive model?" />
  <row Id="2720" PostHistoryTypeId="5" PostId="877" RevisionGUID="8253752e-e108-4215-9f4f-58de91460ffa" CreationDate="2017-06-22T18:57:01.373" UserId="734" Comment="added 125 characters in body; edited title" Text="I have seen that there are 2 common methods: [Patient-derived models][1]: [PDX][2] and [PDO][3] to reflect tumor biology. &#xD;&#xA;First, What are the differences between PDX and PDO. Secondly, they both sound very specific and I was curious if it's possible to use them to build a predictive model, if yes I would appreciate to have an example or guidance on how to start to work with such databases.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Are there any databases of PDO/PDX experiments with their outcomes that one could use to create a predictive model?**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&#xD;&#xA;  [3]: http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html" />
  <row Id="2721" PostHistoryTypeId="4" PostId="877" RevisionGUID="8253752e-e108-4215-9f4f-58de91460ffa" CreationDate="2017-06-22T18:57:01.373" UserId="734" Comment="added 125 characters in body; edited title" Text="Are there databases and is it possible to build a predictive model based on PDO/PDX outcomes?" />
  <row Id="2722" PostHistoryTypeId="2" PostId="878" RevisionGUID="1e755336-8b2b-45d7-a7ff-61e414671375" CreationDate="2017-06-22T18:57:54.120" UserId="44" Text="This is a tough one I think: is there a publicly available, up-to-date, free, complete database for antibiotics names and classes?&#xD;&#xA;&#xD;&#xA;I am specifically looking for information like, e.g., `cefpirome (is a) cephalosporin`.&#xD;&#xA;&#xD;&#xA;I've looked in a couple of different places, but nothing really comes close. E.g.:&#xD;&#xA;&#xD;&#xA; 1. [CARD][1] comes closest, but is not free (companies need to pay a license) and not complete/up-to-date (e.g., cefpirome is missing)&#xD;&#xA; 2. [Novel Antiobiotics Database][2] has no class ... and stops in 2003&#xD;&#xA; 3. [The Antimicrobial Index][3] is web only, and terms of use apparently prohibit any kind of web-scraping.&#xD;&#xA; 4. A suggestion by @Pierre on Twitter was to try [Antimicrobial stubs][4] on Wikipedia. Good idea, but the class is not stored in the ChemBox and must be free-text parsed. Or writing a script which evaluates the Wikipedia category tags.&#xD;&#xA; 5. I drew blank at different government agencies (EMA, FDA, EFSA, NIH, etc.). The closest I found was from the WHO a list of critically important antibiotics. But not complete, and in a PDF.&#xD;&#xA;&#xD;&#xA;Any pointer appreciated.&#xD;&#xA;&#xD;&#xA;  [1]: https://card.mcmaster.ca/&#xD;&#xA;  [2]: http://www.antibiotics.or.jp/journal/database/database-top.htm&#xD;&#xA;  [3]: http://antibiotics.toku-e.com/&#xD;&#xA;  [4]: https://en.wikipedia.org/w/api.php?action=query&amp;list=categorymembers&amp;cmtitle=Category:Antibiotic%20stubs&amp;format=json&amp;cmlimit=500" />
  <row Id="2723" PostHistoryTypeId="1" PostId="878" RevisionGUID="1e755336-8b2b-45d7-a7ff-61e414671375" CreationDate="2017-06-22T18:57:54.120" UserId="44" Text="Publicly available, free, complete database for antibiotics names and classes?" />
  <row Id="2724" PostHistoryTypeId="3" PostId="878" RevisionGUID="1e755336-8b2b-45d7-a7ff-61e414671375" CreationDate="2017-06-22T18:57:54.120" UserId="44" Text="&lt;database&gt;&lt;antibiotics&gt;&lt;open-data&gt;" />
  <row Id="2725" PostHistoryTypeId="5" PostId="873" RevisionGUID="8914b80f-4f1e-4d3c-8d3c-211068b62845" CreationDate="2017-06-22T19:56:20.457" UserId="302" Comment="replaced ftp link with the https link and added link to the parse_seqs and fasta format for ncbi" Text="Running `blastdbcmd -db foo -info` provides a little information but I haven't seen anything which will report exactly how a blastdb was created.&#xD;&#xA;&#xD;&#xA;A [blastdb readme][1] suggests that only the `parse_seqids` option has been added to the standard parameters.&#xD;&#xA;&#xD;&#xA;&gt;For those from NCBI, the following makeblastdb commands are recommended:&#xD;&#xA;&#xD;&#xA; &gt; - For nucleotide fasta file:   makeblastdb -in input_db -dbtype nucl -parse_seqids&#xD;&#xA; &gt; - For protein fasta file:      makeblastdb -in input_db -dbtype prot -parse_seqids&#xD;&#xA;&#xD;&#xA;The ``-parse_seqids`` requires NCBI fasta headers in the correct format (spec can be found [here][2]).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://ftp.ncbi.nlm.nih.gov/blast/documents/blastdb.html&#xD;&#xA;  [2]: http://ncbi.github.io/cxx-toolkit/pages/ch_demo#ch_demo.id1_fetch.html_ref_fasta" />
  <row Id="2726" PostHistoryTypeId="24" PostId="873" RevisionGUID="8914b80f-4f1e-4d3c-8d3c-211068b62845" CreationDate="2017-06-22T19:56:20.457" Comment="Proposed by 302 approved by 77, 73 edit id of 221" />
  <row Id="2727" PostHistoryTypeId="5" PostId="843" RevisionGUID="1923a7cd-1b4a-4982-a17b-a107527b3e3b" CreationDate="2017-06-22T20:27:43.903" UserId="843" Comment="Increase readability: highlight key point, separate into multiple, short paragraphs." Text="I've written a handful of programs from scratch to simulate mutations and variations in real or simulated sequences. &#xD;&#xA;&#xD;&#xA;The trick has always been to sort the variants by genomic coordinate, **apply the variant with the largest coordinate first**, then apply the variant with the second largest coordinate, all the way down to the variant with the smallest coordinate.&#xD;&#xA;&#xD;&#xA;Indels and other variants that affect sequence structure only affect *subsequent* coordinates. So going in reverse order ensures variants at the beginning of the sequence don't throw off variants at the end of the sequence. " />
  <row Id="2728" PostHistoryTypeId="24" PostId="843" RevisionGUID="1923a7cd-1b4a-4982-a17b-a107527b3e3b" CreationDate="2017-06-22T20:27:43.903" Comment="Proposed by 843 approved by -1 edit id of 223" />
  <row Id="2729" PostHistoryTypeId="5" PostId="843" RevisionGUID="0a18b757-d1bb-475e-bf1b-6e5eec744596" CreationDate="2017-06-22T20:27:43.903" UserId="96" Comment="Increase readability: highlight key point, separate into multiple, short paragraphs." Text="I've written a handful of programs from scratch to simulate mutations and variations in real or simulated sequences.&#xD;&#xA;&#xD;&#xA;The trick has always been to sort the variants by genomic coordinate, **apply the variant with the largest coordinate first**, then apply the variant with the second largest coordinate, all the way down to the variant with the smallest coordinate.&#xD;&#xA;&#xD;&#xA;Indels and other variants that affect sequence structure only affect *subsequent* coordinates. So going in reverse order ensures variants at the beginning of the sequence don't throw off variants at the end of the sequence." />
  <row Id="2733" PostHistoryTypeId="2" PostId="880" RevisionGUID="b959575e-1760-4ed8-9605-2fc6e7029db9" CreationDate="2017-06-22T21:33:58.947" UserId="425" Text="I think that you could use [KEGG: Kyoto Encyclopedia of Genes and Genomes](http://www.genome.jp/kegg/). The [exported drug table](ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug) should be easy to parse (e.g., in Python).&#xD;&#xA;&#xD;&#xA;For a quick verification of the completeness, you can download the table, filter out everything except antibacterial agents and check the number of records:&#xD;&#xA;&#xD;&#xA;    curl &quot;ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug&quot; &gt; keg_all.txt&#xD;&#xA;    &#xD;&#xA;    cat keg_all.txt \&#xD;&#xA;       | perl -pe 's/\n/BREAK/g' \&#xD;&#xA;       | perl -pe 's@///BREAK@///\n@g' \&#xD;&#xA;       | grep 'CLASS       Antibacterial' \&#xD;&#xA;       | perl -pe 's/BREAK/\n/g' \&#xD;&#xA;       &gt; keg_antibacterial.txt&#xD;&#xA;&#xD;&#xA;    cat keg_antibacterial.txt | grep &quot;///&quot; | wc -l&#xD;&#xA;&#xD;&#xA;There are 646 records. " />
  <row Id="2734" PostHistoryTypeId="5" PostId="880" RevisionGUID="670a1d8a-026d-4817-b02d-cde72108eccc" CreationDate="2017-06-22T21:42:19.107" UserId="425" Comment="added 152 characters in body" Text="I think that you could use [KEGG: Kyoto Encyclopedia of Genes and Genomes](http://www.genome.jp/kegg/). The [exported drug table](ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug) should be easy to parse (e.g., in Python).&#xD;&#xA;&#xD;&#xA;For a quick verification of the completeness, you can download the table, filter out everything except antibacterial agents and check the number of records:&#xD;&#xA;&#xD;&#xA;    curl &quot;ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug&quot; &gt; keg_all.txt&#xD;&#xA;    &#xD;&#xA;    cat keg_all.txt \&#xD;&#xA;       | perl -pe 's/\n/BREAK/g' \&#xD;&#xA;       | perl -pe 's@///BREAK@///\n@g' \&#xD;&#xA;       | grep 'CLASS       Antibacterial' \&#xD;&#xA;       | perl -pe 's/BREAK/\n/g' \&#xD;&#xA;       &gt; keg_antibacterial.txt&#xD;&#xA;&#xD;&#xA;    cat keg_antibacterial.txt | grep &quot;///&quot; | wc -l&#xD;&#xA;&#xD;&#xA;There are 646 records. However, some of them are almost identical (e.g., only 1 atom difference).&#xD;&#xA;&#xD;&#xA;If you add `\| grep -i 'antibiotic'` to the pipeline, you get 445 records." />
  <row Id="2735" PostHistoryTypeId="5" PostId="878" RevisionGUID="8ca76cec-f72d-4c82-9f97-cabd27a557f1" CreationDate="2017-06-22T21:43:20.580" UserId="44" Comment="added KEGG ass possibility which does not fir entirely" Text="This is a tough one I think: is there a publicly available, up-to-date, free, complete database for antibiotics names and classes?&#xD;&#xA;&#xD;&#xA;I am specifically looking for information like, e.g., `cefpirome (is a) cephalosporin`.&#xD;&#xA;&#xD;&#xA;I've looked in a couple of different places, but nothing really comes close. E.g.:&#xD;&#xA;&#xD;&#xA; 1. [CARD][1] comes closest, but is not free (companies need to pay a license) and not complete/up-to-date (e.g., cefpirome is missing)&#xD;&#xA; 2. @Karel mentioned [KEGG][2] in his answer, but this also needs a license (companies for sure, academics if they download data via FTP).&#xD;&#xA; 2. [Novel Antiobiotics Database][3] has no class ... and stops in 2003&#xD;&#xA; 3. [The Antimicrobial Index][4] is web only, and terms of use apparently prohibit any kind of web-scraping.&#xD;&#xA; 4. A suggestion by @Pierre on Twitter was to try [Antimicrobial stubs][5] on Wikipedia. Good idea, but the class is not stored in the ChemBox and must be free-text parsed. Or writing a script which evaluates the Wikipedia category tags.&#xD;&#xA; 5. I drew blank at different government agencies (EMA, FDA, EFSA, NIH, etc.). The closest I found was from the WHO a list of critically important antibiotics. But not complete, and in a PDF.&#xD;&#xA;&#xD;&#xA;Any pointer appreciated.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://card.mcmaster.ca/&#xD;&#xA;  [2]: http://www.kegg.jp/&#xD;&#xA;  [3]: http://www.antibiotics.or.jp/journal/database/database-top.htm&#xD;&#xA;  [4]: http://antibiotics.toku-e.com/&#xD;&#xA;  [5]: https://en.wikipedia.org/w/api.php?action=query&amp;list=categorymembers&amp;cmtitle=Category:Antibiotic%20stubs&amp;format=json&amp;cmlimit=500" />
  <row Id="2737" PostHistoryTypeId="2" PostId="882" RevisionGUID="a9db87b5-1e24-4008-b82c-df674a6ee6b8" CreationDate="2017-06-22T23:42:46.283" UserId="136" Text="I'm contributing to a python-based project that uses `Biopython` to analyze fastq files. It currently uses `SeqIO.parse`, which populates various structures with all of the fastq information (including converting quality scores). There is apparently a faster (lighter-weight) parser called [FastqGeneralIterator][1] that doesn't populate all of these items.&#xD;&#xA;&#xD;&#xA;I'd like to use `FasqGeneralIterator`, but be able to perform some of the functions `SeqIO.parse` offers on the fly (not for *every* sequence). For example, I'd like to convert base quality to PHRED scores for specific sequences, but I don't see that function available.&#xD;&#xA;&#xD;&#xA;Has anyone done this before? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biopython.org/DIST/docs/api/Bio.SeqIO.QualityIO-module.html#FastqGeneralIterator" />
  <row Id="2738" PostHistoryTypeId="1" PostId="882" RevisionGUID="a9db87b5-1e24-4008-b82c-df674a6ee6b8" CreationDate="2017-06-22T23:42:46.283" UserId="136" Text="How to maximize fastq parsing with FastqGeneralIterator (Bio.SeqIO.QualityIO)" />
  <row Id="2739" PostHistoryTypeId="3" PostId="882" RevisionGUID="a9db87b5-1e24-4008-b82c-df674a6ee6b8" CreationDate="2017-06-22T23:42:46.283" UserId="136" Text="&lt;fastq&gt;&lt;biopython&gt;" />
  <row Id="2740" PostHistoryTypeId="5" PostId="880" RevisionGUID="6a6bda34-6838-4ebf-b565-8c4eee2a5da8" CreationDate="2017-06-23T01:54:33.920" UserId="425" Comment="added 31 characters in body" Text="I think that you could use [KEGG: Kyoto Encyclopedia of Genes and Genomes](http://www.genome.jp/kegg/). The [exported drug table](ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug) should be easy to parse (e.g., in Python).&#xD;&#xA;&#xD;&#xA;For a quick verification of the completeness, you can download the table, filter out everything except antibacterial agents and check the number of records:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    curl &quot;ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug&quot; &gt; keg_all.txt&#xD;&#xA;    &#xD;&#xA;    cat keg_all.txt \&#xD;&#xA;       | perl -pe 's/\n/BREAK/g' \&#xD;&#xA;       | perl -pe 's@///BREAK@///\n@g' \&#xD;&#xA;       | grep 'CLASS       Antibacterial' \&#xD;&#xA;       | perl -pe 's/BREAK/\n/g' \&#xD;&#xA;       &gt; keg_antibacterial.txt&#xD;&#xA;&#xD;&#xA;    cat keg_antibacterial.txt | grep &quot;///&quot; | wc -l&#xD;&#xA;&#xD;&#xA;There are 646 records. However, some of them are almost identical (e.g., only 1 atom difference).&#xD;&#xA;&#xD;&#xA;If you add `| grep -i &quot;antibiotic&quot; \` to the pipeline, then you get only 445 records." />
  <row Id="2742" PostHistoryTypeId="11" PostId="676" RevisionGUID="bc424c59-ef9c-450f-a9a5-6a4ab55ea422" CreationDate="2017-06-23T09:48:59.290" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:292,&quot;DisplayName&quot;:&quot;bli&quot;},{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;},{&quot;Id&quot;:425,&quot;DisplayName&quot;:&quot;Karel Brinda&quot;},{&quot;Id&quot;:96,&quot;DisplayName&quot;:&quot;Daniel Standage&quot;},{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;}]}" />
  <row Id="2743" PostHistoryTypeId="2" PostId="883" RevisionGUID="7780423a-4275-4a06-be02-0188ce451666" CreationDate="2017-06-23T13:01:54.793" UserId="492" Text="I had not come across `FastqGeneralIterator` before, but I will start using it for some of my work!&#xD;&#xA;&#xD;&#xA;One answer is to replicate the code in the [`FastqPhredIterator`](http://biopython.org/DIST/docs/api/Bio.SeqIO.QualityIO-pysrc.html#FastqPhredIterator), which does exactly what you are looking to do.  &#xD;&#xA;&#xD;&#xA;However, My personal preference would be to use the biopython infrastructure as much as possible, especially when it comes to the tricky aspects of record parsing.  If the records you want extra info on are uncommon, then using `FastqPhredIterator` for phred quality extraction might work for you:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    import io&#xD;&#xA;    from Bio.SeqIO.QualityIO import FastqGeneralIterator, FastqPhredIterator&#xD;&#xA;    &#xD;&#xA;    with open(&quot;input.fastq&quot;, &quot;rU&quot;) as handle:&#xD;&#xA;        for title, sequence, quality in FastqGeneralIterator(handle):&#xD;&#xA;            # do something with title, sequence, and quality&#xD;&#xA;            # Set special_case to True when you want more info on the record&#xD;&#xA;            special_case = False&#xD;&#xA;            if special_case:&#xD;&#xA;                record_stream = io.StringIO(&quot;\n&quot;.join([title, sequence, &quot;+&quot;, quality]))&#xD;&#xA;                record = next(FastqPhredIterator(record_stream))&#xD;&#xA;                # do something special with record&#xD;&#xA;&#xD;&#xA;What we are doing here is combining the record strings into one and dumping them into a `StringIO` object, which implements the file handle duck-type. We then hand the in-memory handle to `FstqPhredIterator` to parse the FASTQ record again. Make sure to set `special_case` to `True` only when you want extra info on a record, as the `special_case` branch makes at least two extra copies of the FASTQ record. &#xD;&#xA;" />
  <row Id="2745" PostHistoryTypeId="5" PostId="380" RevisionGUID="7078ca15-792f-40f2-bc53-25ce04e7e1c7" CreationDate="2017-06-23T13:39:36.970" UserId="292" Comment="Added fasta benchmark for scikit-bio" Text="## Using bioawk&#xD;&#xA;&#xD;&#xA;With [bioawk](https://github.com/lh3/bioawk) (counting &quot;A&quot; in the *C. elegans* genome, because there seem to be no &quot;N&quot; in this file), on my computer:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.645s&#xD;&#xA;    user	0m1.548s&#xD;&#xA;    sys 	0m0.088s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;bioawk is an extension of awk with convenient parsing options. For instance `-c fastx` makes the sequence available as `$seq` in fasta and fastq format (**including gzipped versions**), so the above command **should also handle fastq format robustly**.&#xD;&#xA;&#xD;&#xA;The `gsub` command is a normal awk function. It returns the number of substituted characters (got it from &lt;https://unix.stackexchange.com/a/169575/55127&gt;). I welcome comments about how to count inside awk in a less hackish way.&#xD;&#xA;&#xD;&#xA;On this fasta example it is almost 3 times slower than the `grep`, `tr`, `wc` pipeline:&#xD;&#xA;&#xD;&#xA;    $ time grep -v &quot;^&gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.609s&#xD;&#xA;    user	0m0.744s&#xD;&#xA;    sys 	0m0.184s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(I repeated the timings, and the above values seem representative)&#xD;&#xA;&#xD;&#xA;## Using python&#xD;&#xA;&#xD;&#xA;### readfq&#xD;&#xA;&#xD;&#xA;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;https://bioinformatics.stackexchange.com/a/365/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.976s&#xD;&#xA;    user	0m0.876s&#xD;&#xA;    sys 	0m0.100s&#xD;&#xA;&#xD;&#xA;### &quot;pure python&quot;&#xD;&#xA;&#xD;&#xA;Comparing with something adapted from this answer: &lt;https://bioinformatics.stackexchange.com/a/363/292&gt;&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&gt;')))&quot; &lt; genome.fa&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.143s&#xD;&#xA;    user	0m1.100s&#xD;&#xA;    sys 	0m0.040s&#xD;&#xA;&#xD;&#xA;(It is slower with python 3.6 than with python 2.7 on my computer)&#xD;&#xA;&#xD;&#xA;### pyGATB&#xD;&#xA;&#xD;&#xA;I just learned (08/06/2017) that [GATB](http://gatb.inria.fr/) includes a fasta/fastq parser and has recently released a python API. I tried to use it yesterday to test another answer to the present question and [found a bug](https://github.com/GATB/pyGATB/issues/2). This bug is now fixed, so here is a [pyGATB](https://pypi.python.org/pypi/pyGATB/0.1.2)-based answer:&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.count(b\&quot;A\&quot;) for seq in Bank(\&quot;genome.fa\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m0.663s&#xD;&#xA;    user	0m0.568s&#xD;&#xA;    sys 	0m0.092s&#xD;&#xA;&#xD;&#xA;(You can also do `sequence.decode(&quot;utf-8&quot;).count(&quot;A&quot;)` but this seems [a little slower](https://github.com/GATB/pyGATB/issues/2#issuecomment-307131176).)&#xD;&#xA;&#xD;&#xA;Although I used python3.6 here (pyGATB seems python3-only), this is faster than the other two python approaches (for which the reported timings are obtained with python 2.7). This is even almost as fast as the `grep`, `tr`, `wc` pipeline.&#xD;&#xA;&#xD;&#xA;### Biopython&#xD;&#xA;&#xD;&#xA;And, to have even more comparisons, here is a solution using `SeqIO.parse` from Biopython (with python2.7):&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from Bio import SeqIO; print(sum((rec.seq.count(\&quot;A\&quot;) for rec in SeqIO.parse(\&quot;genome.fa\&quot;, \&quot;fasta\&quot;))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.632s&#xD;&#xA;    user	0m1.532s&#xD;&#xA;    sys 	0m0.096s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;This is a bit slower than the &quot;pure python&quot; solution, but perhaps more robust.&#xD;&#xA;&#xD;&#xA;There seems to be a slight improvement with @peterjc's suggestion to use the lower level `SimpleFastaParser`:&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from Bio.SeqIO.FastaIO import SimpleFastaParser; print(sum(seq.count('A') for title, seq in SimpleFastaParser(open('genome.fa'))))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m1.618s&#xD;&#xA;    user	0m1.500s&#xD;&#xA;    sys 	0m0.116s&#xD;&#xA;&#xD;&#xA;(I did a series of timings and tried to take one that seemed representative, but there's a lot of overlap with the higher-level parser's timings.)&#xD;&#xA;&#xD;&#xA;### scikit-bio&#xD;&#xA;&#xD;&#xA;I read about [scikit-bio](http://scikit-bio.org) today (23/06/2017), which has a sequence reader.&#xD;&#xA;&#xD;&#xA;    $ time python3 -c  &quot;import skbio; print(sum(seq.count('A') for seq in skbio.io.read('genome.fa', format='fasta', verify=False)))&quot;&#xD;&#xA;    32371810&#xD;&#xA;    &#xD;&#xA;    real	0m3.643s&#xD;&#xA;    user	0m3.440s&#xD;&#xA;    sys 	0m1.228s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;## Benchmarks on a fastq.gz file&#xD;&#xA;&#xD;&#xA;I tested some of the above solutions (or adaptations thereof), counting &quot;N&quot; in the same file that was used here: &lt;https://bioinformatics.stackexchange.com/a/400/292&gt;&#xD;&#xA;&#xD;&#xA;### bioawk&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{n+=gsub(/N/, &quot;&quot;, $seq)} END {print n}' SRR077487_2.filt.fastq.gz&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	1m9.686s&#xD;&#xA;    user	1m9.376s&#xD;&#xA;    sys 	0m0.304s&#xD;&#xA;&#xD;&#xA;### pigz + readfq python module&#xD;&#xA;&#xD;&#xA;readfq doesn't complain and is very fast when I pass directly the compressed fastq, but returns something wrong, so don't forget to manually take care of the decompression.&#xD;&#xA;&#xD;&#xA;Here I tried with `pigz`:&#xD;&#xA;&#xD;&#xA;    $ time pigz -dc SRR077487_2.filt.fastq.gz | python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('N') for _, seq, _ in readfq(stdin)))&quot;&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	0m52.347s&#xD;&#xA;    user	1m40.716s&#xD;&#xA;    sys 	0m8.604s&#xD;&#xA;&#xD;&#xA;The computer has 16 cores, but I suspect the limiting factor for `pigz` is reading from the disk: the processors are very far from running full speed.&#xD;&#xA;&#xD;&#xA;And with `gzip`:&#xD;&#xA;&#xD;&#xA;    $ time gzip -dc SRR077487_2.filt.fastq.gz | python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('N') for _, seq, _ in readfq(stdin)))&quot;&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	0m49.448s&#xD;&#xA;    user	1m31.984s&#xD;&#xA;    sys 	0m2.312s&#xD;&#xA;&#xD;&#xA;Here gzip and python both used a full processor. Resource usage balance was slightly better. I work on a desktop computer. I suppose a computing server would take advantage of `pigz` better.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### pyGATB&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.count(b\&quot;N\&quot;) for seq in Bank(\&quot;SRR077487_2.filt.fastq.gz\&quot;))))&quot;&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	0m46.784s&#xD;&#xA;    user	0m46.404s&#xD;&#xA;    sys 	0m0.296s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### biopython&#xD;&#xA;&#xD;&#xA;    $ time python -c &quot;from gzip import open as gzopen; from Bio.SeqIO.QualityIO import FastqGeneralIterator; print(sum(seq.count('N') for (_, seq, _) in FastqGeneralIterator(gzopen('SRR077487_2.filt.fastq.gz'))))&quot;&#xD;&#xA;    306072&#xD;&#xA;    &#xD;&#xA;    real	3m18.103s&#xD;&#xA;    user	3m17.676s&#xD;&#xA;    sys 	0m0.428s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;## Conclusion&#xD;&#xA;&#xD;&#xA;pyGATB and bioawk both handle transparently compression (gzipped or not) and format differences (fasta or fastq). pyGATB is quite a new tool, but seems more efficient compared to the other python modules I tested.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2752" PostHistoryTypeId="10" PostId="844" RevisionGUID="c9c7885a-3d4a-48a8-a168-3dc7c25d1c13" CreationDate="2017-06-23T15:02:07.927" UserId="77" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;},{&quot;Id&quot;:96,&quot;DisplayName&quot;:&quot;Daniel Standage&quot;},{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;},{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;}]}" />
  <row Id="2757" PostHistoryTypeId="5" PostId="882" RevisionGUID="1b54e23e-c0dc-4ee8-a5eb-9bcb5f03423c" CreationDate="2017-06-23T15:04:59.877" UserId="492" Comment="Make question more specific (addresses comments) + fix typo" Text="I'm contributing to a python-based project that uses `Biopython` to analyze fastq files. It currently uses `SeqIO.parse`, which populates various structures with all of the fastq information (including converting quality scores). There is apparently a faster (lighter-weight) parser called [FastqGeneralIterator][1] that doesn't populate all of these items.&#xD;&#xA;&#xD;&#xA;I'd like to use `FastqGeneralIterator`, but be able to perform some of the functions `SeqIO.parse` offers on the fly (not for *every* sequence). For example, I'd like to convert base quality to PHRED scores for specific sequences, but I don't see that function available.&#xD;&#xA;&#xD;&#xA;Is there an easy way to take a tuple output by `FastqGeneralIterator` and convert it into a proper `Bio.SeqIO` `SeqRecord`? &#xD;&#xA;&#xD;&#xA;  [1]: http://biopython.org/DIST/docs/api/Bio.SeqIO.QualityIO-module.html#FastqGeneralIterator" />
  <row Id="2758" PostHistoryTypeId="24" PostId="882" RevisionGUID="1b54e23e-c0dc-4ee8-a5eb-9bcb5f03423c" CreationDate="2017-06-23T15:04:59.877" Comment="Proposed by 492 approved by 77, 37 edit id of 224" />
  <row Id="2759" PostHistoryTypeId="5" PostId="576" RevisionGUID="f5efbbbd-fd45-462b-8c10-a991b62bd8bf" CreationDate="2017-06-23T15:05:12.343" UserId="292" Comment="added 123 characters in body" Text="When dealing with associating sequencing reads to a matching position in a set of reference sequences (typically a genome)." />
  <row Id="2760" PostHistoryTypeId="24" PostId="576" RevisionGUID="f5efbbbd-fd45-462b-8c10-a991b62bd8bf" CreationDate="2017-06-23T15:05:12.343" Comment="Proposed by 292 approved by 73, 37 edit id of 153" />
  <row Id="2761" PostHistoryTypeId="5" PostId="880" RevisionGUID="82bba0c9-7548-4719-8adb-3e2e46947f77" CreationDate="2017-06-23T15:55:14.457" UserId="425" Comment="Add ChEBI" Text="**ChEBI**&#xD;&#xA;&#xD;&#xA;It should be possible to get this information from the [ChEBI database](https://www.ebi.ac.uk/chebi/), see the [exported tables](https://www.ebi.ac.uk/chebi/downloadsForward.do). You could download the ontologies (in OWL / OBO), parse them using some ad-hoc parser or using a dedicated library (e.g., [Pronto](https://pypi.python.org/pypi/pronto)), build a directed acyclic graph based on the `is_a` relationship, and extract the antibiotic subtree.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**KEGG** &#xD;&#xA;&#xD;&#xA;You could also use [KEGG: Kyoto Encyclopedia of Genes and Genomes](http://www.genome.jp/kegg/), but a [subscription](http://www.kegg.jp/kegg/legal.html) is probably required. The [exported drug table](ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug) should be easy to parse (e.g., in Python).&#xD;&#xA;&#xD;&#xA;For a quick verification of the completeness, you can download the table, filter out everything except antibacterial agents and check the number of records:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    curl &quot;ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug&quot; &gt; keg_all.txt&#xD;&#xA;    &#xD;&#xA;    cat keg_all.txt \&#xD;&#xA;       | perl -pe 's/\n/BREAK/g' \&#xD;&#xA;       | perl -pe 's@///BREAK@///\n@g' \&#xD;&#xA;       | grep 'CLASS       Antibacterial' \&#xD;&#xA;       | perl -pe 's/BREAK/\n/g' \&#xD;&#xA;       &gt; keg_antibacterial.txt&#xD;&#xA;&#xD;&#xA;    cat keg_antibacterial.txt | grep &quot;///&quot; | wc -l&#xD;&#xA;&#xD;&#xA;There are 646 records. However, some of them are almost identical (e.g., only 1 atom difference).&#xD;&#xA;&#xD;&#xA;If you add `| grep -i &quot;antibiotic&quot; \` to the pipeline, then you get only 445 records." />
  <row Id="2762" PostHistoryTypeId="2" PostId="884" RevisionGUID="618da05b-c462-4e15-8887-4af266d4c025" CreationDate="2017-06-23T15:59:35.730" UserId="640" Text="I would like to add a text to PDB files that I'm processing with my tool, rna-pdb-tools. Someone points that the way I'm using it right now it's not correct (https://github.com/mmagnus/rna-pdb-tools/issues/48).&#xD;&#xA;&#xD;&#xA;I use HEADER right now which is not correct.&#xD;&#xA;&#xD;&#xA;    HEADER Generated with rna-pdb-tools&#xD;&#xA;    HEADER ver 37c5b4e-dirty &#xD;&#xA;    HEADER https://github.com/mmagnus/rna-pdb-tools &#xD;&#xA;    HEADER Mon Oct 10 22:18:19 2016&#xD;&#xA;    ATOM      1  P     C A   1     -19.687  -3.296  65.469  1.00  0.00           P &#xD;&#xA;&#xD;&#xA;Do you happen to know which remark number to use? (http://www.wwpdb.org/documentation/file-format-content/format33/remarks1.html)?" />
  <row Id="2763" PostHistoryTypeId="1" PostId="884" RevisionGUID="618da05b-c462-4e15-8887-4af266d4c025" CreationDate="2017-06-23T15:59:35.730" UserId="640" Text="PDB format: remark number for free text" />
  <row Id="2764" PostHistoryTypeId="3" PostId="884" RevisionGUID="618da05b-c462-4e15-8887-4af266d4c025" CreationDate="2017-06-23T15:59:35.730" UserId="640" Text="&lt;pdb&gt;" />
  <row Id="2765" PostHistoryTypeId="2" PostId="885" RevisionGUID="4e7d7d12-4f63-4d04-8f39-096b8a25c390" CreationDate="2017-06-23T20:03:46.210" UserId="136" Text="Do you know which quality score encoding PacBio uses now? I know some of their file formats have changed in the past year or two, but I haven't found much on their quality score encoding.&#xD;&#xA;&#xD;&#xA;The most recent answer I found is from 2012, where one user states it's &quot;phred style&quot;, but doesn't state whether its 32 or 64: [DeNovo assembly using pacBio data][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://seqanswers.com/forums/archive/index.php/t-16895.html" />
  <row Id="2766" PostHistoryTypeId="1" PostId="885" RevisionGUID="4e7d7d12-4f63-4d04-8f39-096b8a25c390" CreationDate="2017-06-23T20:03:46.210" UserId="136" Text="Which quality score encoding does PacBio use?" />
  <row Id="2767" PostHistoryTypeId="3" PostId="885" RevisionGUID="4e7d7d12-4f63-4d04-8f39-096b8a25c390" CreationDate="2017-06-23T20:03:46.210" UserId="136" Text="&lt;sequencing&gt;&lt;pacbio&gt;" />
  <row Id="2768" PostHistoryTypeId="5" PostId="885" RevisionGUID="36921a7c-bfd2-41d1-b2fd-56ff06bbc54d" CreationDate="2017-06-23T20:30:21.750" UserId="136" Comment="added 1 character in body" Text="Do you know which quality score encoding PacBio uses now? I know some of their file formats have changed in the past year or two, but I haven't found much on their quality score encoding.&#xD;&#xA;&#xD;&#xA;The most recent answer I found is from 2012, where one user states it's &quot;phred style&quot;, but doesn't state whether it's 33 or 64: [DeNovo assembly using pacBio data][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://seqanswers.com/forums/archive/index.php/t-16895.html" />
  <row Id="2769" PostHistoryTypeId="2" PostId="886" RevisionGUID="2d98c0a4-0051-4ac8-a28c-ea60aded0475" CreationDate="2017-06-23T21:07:18.203" UserId="962" Text="I have FPKM-UQ data from COAD-TCGA.&#xD;&#xA;&#xD;&#xA;I generated an expression set of this data using:&#xD;&#xA;&#xD;&#xA;    &gt; edata = log2(data + 1)&#xD;&#xA;    &gt; edata[1:2,1:2]&#xD;&#xA;          X01240896.3f3f.4bf9.9799.55c87bfacf36&#xD;&#xA;    1                                  8.540967&#xD;&#xA;    10                                13.528968&#xD;&#xA;    100                               16.296422&#xD;&#xA;    1000                              13.658199&#xD;&#xA;    10000                             15.143788&#xD;&#xA;          X01ad5016.f691.4bca.82a0.910429d8d25b&#xD;&#xA;    1                                  9.886537&#xD;&#xA;    10                                16.719682&#xD;&#xA;    100                               17.212312&#xD;&#xA;    1000                              10.317842&#xD;&#xA;    10000                             13.166767&#xD;&#xA;&#xD;&#xA;Row name is entrezid, and column names are case_ids.&#xD;&#xA;&#xD;&#xA;The batch data provided by TCGA which indicates a total batch num = 42&#xD;&#xA;&#xD;&#xA;    &gt; omics[1:5,]&#xD;&#xA;                                          snf    boo order batch&#xD;&#xA;    X01240896.3f3f.4bf9.9799.55c87bfacf36   3 192807     1    22&#xD;&#xA;    X01ad5016.f691.4bca.82a0.910429d8d25b   2  82213     2     2&#xD;&#xA;    X01f493d4.229d.47a6.baa8.32a342c65d01   1 A08907     3    29&#xD;&#xA;    X022f39e9.57ee.4b2b.8b3a.8929e3d69a37   2 A16S13     4    32&#xD;&#xA;    X02f9668c.71e6.485f.88b1.b37dc8bdd2ab   3 102113     5     7&#xD;&#xA;    &gt; batch = omics$batch&#xD;&#xA;    &gt; batch&#xD;&#xA;      [1] 22  2 29 32  7  1  8  8  7 41 26 14 26 26  1  5 22  5 15 18 28 12 12  8 19&#xD;&#xA;     [26] 13 38  5 27  7 41  1 22 26 28 22 33 14 13  8 10 13 11 17  5 38  5  7  7 31&#xD;&#xA;     [51] 22 42 11  8  7 17 15 17 12 17  2 19  2 12 18 14 22 11 21 15 17 17 33 15 19&#xD;&#xA;     [76] 15  6  1 10 11  2 28 36 17  9 15 12 17 15 10  6  8 13 25 11 15  1 13 24  8&#xD;&#xA;    [101] 13 15 17  8  2 22 17 11 13 37 19 38 13 19  5 16  5  5 22 35 35 19 13 17 22&#xD;&#xA;    [126]  7  8  8 41 39 14  6 19 18 38 22 14 30 19 24 11 14 13 16  7 13  8 22 12  8&#xD;&#xA;    [151] 22 17  2 40 13 35 15  5 24 19 41 22 22 19 17  8 13 33 10 17 29 12  9 11 24&#xD;&#xA;    [176] 27  5 14  7 25 13 35  9 18 16 13 31 25 18 10 35 18 14 33  5 19 17  8 11 11&#xD;&#xA;    [201] 42  7 10  8 39 19  8  7 11 19 31 35 28 36 13 37 10 38  3 18 13 34 39 13  3&#xD;&#xA;    [226] 40 31 17 19 26  7 19 12 22 19 17 12 17 13 12 28 33 37 17 35 18 40 13 20 35&#xD;&#xA;    [251] 17  6 15 14 12 27  5 41 31 41 18 21 19 17 40  8 41  2  5 15  5 15 15  5 39&#xD;&#xA;    [276]  1 22 33 13 17  7 11 15  3  7 38 15  7 14 14 22 31 14 18  3 19  7  9  6  5&#xD;&#xA;    [301] 22  8 11  1 13 10 19  8 14 15  6 32  1  2 19  4  5 13 18 42 11 17  8 36 19&#xD;&#xA;    [326] 15 22 28 15  8 11  8 15 38 19 39  7 11 42 23  5  1 22 17 35 13 40 25  7 41&#xD;&#xA;    [351] 38  5  8 19 31  6 38  8 36  4 15 15  7 12 22 38 29 35 15  2 10  2 19 15 25&#xD;&#xA;    [376]  5 39 30 19 18 15 10 10 33  1 19 32 19 19 15 20 15  7 22 35 22 30 14 12 16&#xD;&#xA;    [401] 21 13 15 18  7  5  6 31 12  7 15 32 33 18 14 15 15 26 31  1 14 19 17 18  1&#xD;&#xA;    [426]  5 10 32&#xD;&#xA;&#xD;&#xA;Unfortunately, when I run the ComBat analysis it generates all NANs, and no values:&#xD;&#xA;&#xD;&#xA;    &gt; modcombat = model.matrix(~1, data=omics)&#xD;&#xA;    &gt; combat_edata = ComBat(dat=edata, batch=batch, mod=modcombat, par.prior=TRUE, prior.plots=FALSE)&#xD;&#xA;    &gt; combat_edata[1:5,1:2]&#xD;&#xA;          X01240896.3f3f.4bf9.9799.55c87bfacf36&#xD;&#xA;    1                                       NaN&#xD;&#xA;    10                                      NaN&#xD;&#xA;    100                                     NaN&#xD;&#xA;    1000                                    NaN&#xD;&#xA;    10000                                   NaN&#xD;&#xA;          X01ad5016.f691.4bca.82a0.910429d8d25b&#xD;&#xA;    1                                       NaN&#xD;&#xA;    10                                      NaN&#xD;&#xA;    100                                     NaN&#xD;&#xA;    1000                                    NaN&#xD;&#xA;    10000                                   NaN&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2770" PostHistoryTypeId="1" PostId="886" RevisionGUID="2d98c0a4-0051-4ac8-a28c-ea60aded0475" CreationDate="2017-06-23T21:07:18.203" UserId="962" Text="Issues with ComBat on TCGA COAD RNAseq" />
  <row Id="2771" PostHistoryTypeId="3" PostId="886" RevisionGUID="2d98c0a4-0051-4ac8-a28c-ea60aded0475" CreationDate="2017-06-23T21:07:18.203" UserId="962" Text="&lt;r&gt;" />
  <row Id="2772" PostHistoryTypeId="2" PostId="887" RevisionGUID="15b02e91-184c-4786-bd8e-5cdd6a606226" CreationDate="2017-06-23T21:12:05.243" UserId="727" Text="I am attempting to run a BLAST search remotely using BLAST+.&#xD;&#xA;&#xD;&#xA;I can get search to work correctly at the command line with the following commands:&#xD;&#xA;&#xD;&#xA;    blastp -query proteins.fasta -remote -db nr -out proteins_nr.txt -outfmt 6 -evalue 1e-30&#xD;&#xA;&#xD;&#xA;However, I would like to us the remote database titled &quot;Microbial proteins from nr&quot; which is what would be used for the microbial blast search accessing BLAST from the NCBI site.&#xD;&#xA;&#xD;&#xA;I'm struggling to find the correct code to access this database instead of just &quot;nr&quot;.&#xD;&#xA;&#xD;&#xA;Is there a list with the codes of different databases?&#xD;&#xA;&#xD;&#xA;Thanks in advance&#xD;&#xA;" />
  <row Id="2773" PostHistoryTypeId="1" PostId="887" RevisionGUID="15b02e91-184c-4786-bd8e-5cdd6a606226" CreationDate="2017-06-23T21:12:05.243" UserId="727" Text="how to set database other than nr for remote blast+ search" />
  <row Id="2774" PostHistoryTypeId="3" PostId="887" RevisionGUID="15b02e91-184c-4786-bd8e-5cdd6a606226" CreationDate="2017-06-23T21:12:05.243" UserId="727" Text="&lt;database&gt;&lt;blast&gt;" />
  <row Id="2775" PostHistoryTypeId="2" PostId="888" RevisionGUID="00ac5b1a-f512-4e7e-97ac-a4df7d493415" CreationDate="2017-06-23T21:30:22.757" UserId="136" Text="`PacBio` does use `PHRED 33`, ***but it turns out the question may be irrelevant*** for the newer `PacBio Sequel Sequencer`, because it reports all base qualities as `PHRED 0` (`ASCII '!'`). The `RS-II` reports `PHRED 33` quality scores.&#xD;&#xA;&#xD;&#xA;The `Sequel` provides data in the form of `subreads`, which are the `circular consensus sequence` (`CCS`) from a single `zero-mode waveguide` (`ZMW`). Essentially, a single circularized DNA molecule enters a `ZMW` well, and is sequenced as many times as possible. (Sometimes multiple molecules get into a single well, but that's not the normal case.) The number of times this single molecule can be sequenced depends on several variables, including molecule length. The `consensus sequence` for this single molecule is then reported out as a `subread`.&#xD;&#xA;&#xD;&#xA;As I started playing with the quality scores in my data, I realized the quality score for every base is '!', which equates to a `PHRED` quality of `0`. I contacted someone at PacBio who informed me that they learned the base quality values were not any more useful than the bases themselves for generating a high-quality `circular consensus sequence`. Thus, rather than spend the computational effort to calculate the scores, they skipped it, and reported `!` for every base. They also report an overall read quality of `0.8`, but this is simply a placeholder. They are working to make this information more readily available." />
  <row Id="2776" PostHistoryTypeId="4" PostId="886" RevisionGUID="cdcd7a94-1147-4480-8820-85e5bc434061" CreationDate="2017-06-23T21:48:47.553" UserId="73" Comment="updated title to match question" Text="NaN values after ComBat analysis on TCGA COAD RNAseq" />
  <row Id="2777" PostHistoryTypeId="6" PostId="886" RevisionGUID="cdcd7a94-1147-4480-8820-85e5bc434061" CreationDate="2017-06-23T21:48:47.553" UserId="73" Comment="updated title to match question" Text="&lt;r&gt;&lt;combat&gt;" />
  <row Id="2778" PostHistoryTypeId="2" PostId="889" RevisionGUID="27cdea95-de36-4021-a43c-bdd6391fc0dc" CreationDate="2017-06-23T22:07:12.503" UserId="962" Text="I think I might know what the problem is. &#xD;&#xA;&#xD;&#xA;I plotted the log2 corrected values, and the distribution may not be normal because of the 0 values present in RNASeq data. If I understand correctly, this data is actually missing, whereas with my current analysis it is viewed by R as an actual value of 0. So either the missing data needs to be imputed, or the data needs to be scaled and normalized to mean of 0, then subsequently log2 transformed.&#xD;&#xA;&#xD;&#xA;    &gt; data &lt;- standardNormalization(data)&#xD;&#xA;    &gt; data &lt;- log2(data + 1)&#xD;&#xA;    &gt; combat_data = ComBat(dat=data, batch=batch, mod=modcombat, par.prior=TRUE, prior.plots=FALSE)&#xD;&#xA;    &gt; combat_data[1:5,1:2]&#xD;&#xA;              X01240896.3f3f.4bf9.9799.55c87bfacf36&#xD;&#xA;    100507661                           -0.13857428&#xD;&#xA;    57103                               -0.06700506&#xD;&#xA;    22838                                0.16295096&#xD;&#xA;    55567                               -0.13579991&#xD;&#xA;    6147                                 1.64990772&#xD;&#xA;              X01ad5016.f691.4bca.82a0.910429d8d25b&#xD;&#xA;    100507661                           -0.13789534&#xD;&#xA;    57103                               -0.06210232&#xD;&#xA;    22838                                0.01546224&#xD;&#xA;    55567                               -0.13443289&#xD;&#xA;    6147                                 1.48473072" />
  <row Id="2779" PostHistoryTypeId="2" PostId="890" RevisionGUID="d8ed366b-c154-478d-8e9f-49b1013d25cf" CreationDate="2017-06-23T23:18:32.853" UserId="541" Text="I believe you're looking for `env_nr`? It's listed as such, under `Metagenomic proteins` in the blastp webpage. It appears that the word within brackets should be supplied alongside the `-db` parameter. A quick test with a dummy amino acid fasta file does turn up a result to a valid NCBI protein accession.&#xD;&#xA;&#xD;&#xA;[![partial screenshot from NCBI's blastp webpage][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/hNm3k.png&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2780" PostHistoryTypeId="4" PostId="877" RevisionGUID="dcf7800a-dee2-4ceb-bbcf-d0da8c3eaed1" CreationDate="2017-06-24T00:28:40.910" UserId="734" Comment="edited title" Text="Is it possible to build a predictive model based on PDO/PDX outcomes?" />
  <row Id="2782" PostHistoryTypeId="2" PostId="892" RevisionGUID="db640b45-5751-406c-9d7b-d2f8f3406f4d" CreationDate="2017-06-24T03:12:53.440" UserId="136" Text="How do you write a `.gz` (or `.bgz`) `fastq` file using `Biopython`? &#xD;&#xA;&#xD;&#xA;I'd rather avoid a separate system call.&#xD;&#xA;&#xD;&#xA;The typical way to write an `ASCII` `.fastq` is done as follows:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    for record in SeqIO.parse(fasta, &quot;fasta&quot;):&#xD;&#xA;        SeqIO.write(record, fastq, &quot;fastq&quot;)&#xD;&#xA;&#xD;&#xA;The `record` is a `SeqRecord` object, `fastq` is the `file handle`, and `&quot;fastq&quot;` is the requested file format. The file format may be `fastq`, `fasta`, etc., but I do not see an option for `.gz`. &#xD;&#xA;&#xD;&#xA;Here is the [SeqIO][1] API.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://biopython.org/DIST/docs/api/Bio.SeqIO-module.html" />
  <row Id="2783" PostHistoryTypeId="1" PostId="892" RevisionGUID="db640b45-5751-406c-9d7b-d2f8f3406f4d" CreationDate="2017-06-24T03:12:53.440" UserId="136" Text="How do you write a .gz fastq file with Biopython?" />
  <row Id="2784" PostHistoryTypeId="3" PostId="892" RevisionGUID="db640b45-5751-406c-9d7b-d2f8f3406f4d" CreationDate="2017-06-24T03:12:53.440" UserId="136" Text="&lt;file-formats&gt;&lt;biopython&gt;&lt;python&gt;" />
  <row Id="2785" PostHistoryTypeId="2" PostId="893" RevisionGUID="afd4f64c-7bda-4ff5-bd95-c0a21cc22e26" CreationDate="2017-06-24T04:11:01.577" UserId="964" Text="As far as I know you can not write compressed fastq on the fly.  You will have to write the entire fastq and the compress it." />
  <row Id="2786" PostHistoryTypeId="2" PostId="894" RevisionGUID="5f50fefc-47d1-47d3-8cb3-e1ac8f7b13d2" CreationDate="2017-06-24T06:49:27.180" UserId="292" Text="I'm not sure I'm doing it the best way, but here is an example where I read a compressed gzip fastq file and write the records in block gzip fastq:&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO, bgzf&#xD;&#xA;    # Used to convert the fastq stream into a file handle&#xD;&#xA;    from io import StringIO&#xD;&#xA;    from gzip import open as gzopen&#xD;&#xA;    records = SeqIO.parse(&#xD;&#xA;        StringIO(gzopen(&quot;random_10.fastq.gz&quot;).read().decode(&quot;utf-8&quot;)),&#xD;&#xA;        format=&quot;fastq&quot;)&#xD;&#xA;    with bgzf.BgzfWriter(&quot;test.fastq.bgz&quot;, &quot;wb&quot;) as outgz:&#xD;&#xA;        SeqIO.write(sequences=records, handle=outgz, format=&quot;fastq&quot;)" />
  <row Id="2787" PostHistoryTypeId="5" PostId="888" RevisionGUID="2c2c3ec4-6368-4025-b8a8-e875cdd4e368" CreationDate="2017-06-24T11:01:12.103" UserId="73" Comment="fixed formatting to be a bit easier to read" Text="PacBio does use PHRED 33, *but it turns out the question may be irrelevant* for the newer PacBio Sequel Sequencer, because it reports all base qualities as PHRED 0 (ASCII `!`). The RS-II reports PHRED 33 quality scores.&#xD;&#xA;&#xD;&#xA;The Sequel provides data in the form of *subreads*, which are the circular consensus sequence (CCS) from a single zero-mode waveguide (ZMW). Essentially, a single circularized DNA molecule enters a ZMW well, and is sequenced as many times as possible. (Sometimes multiple molecules get into a single well, but that's not the normal case.) The number of times this single molecule can be sequenced depends on several variables, including molecule length. The consensus sequence for this single molecule is then reported out as a subread.&#xD;&#xA;&#xD;&#xA;As I started playing with the quality scores in my data, I realized the quality score for every base is `!`, which equates to a PHRED quality of 0. I contacted someone at PacBio who informed me that they learned the base quality values were not any more useful than the bases themselves for generating a high-quality circular consensus sequence. Thus, rather than spend the computational effort to calculate the scores, they skipped it, and reported `!` for every base. They also report an overall read quality of 0.8, but this is simply a placeholder. They are working to make this information more readily available." />
  <row Id="2788" PostHistoryTypeId="2" PostId="895" RevisionGUID="3e2ac615-f2c7-4305-96ef-18cc41f194f1" CreationDate="2017-06-24T11:19:23.527" UserId="349" Text="I want to get a .bed file with the genes' names and canonical coordinates, also I would like to have coordinates of exons, too. I can get the list from UCSC, however, if I choose UCSC Genes - knownCanonical, I can not extract coordinates of exons. If I use other options - I am getting coordinates of as many transcriptional isoforms as were detected while I need only one canonical form.&#xD;&#xA;&#xD;&#xA;How can I get such bed file?" />
  <row Id="2789" PostHistoryTypeId="1" PostId="895" RevisionGUID="3e2ac615-f2c7-4305-96ef-18cc41f194f1" CreationDate="2017-06-24T11:19:23.527" UserId="349" Text="How to obtain .bed file with coordinates of all genes" />
  <row Id="2790" PostHistoryTypeId="3" PostId="895" RevisionGUID="3e2ac615-f2c7-4305-96ef-18cc41f194f1" CreationDate="2017-06-24T11:19:23.527" UserId="349" Text="&lt;public-databases&gt;" />
  <row Id="2791" PostHistoryTypeId="5" PostId="893" RevisionGUID="cfaaff48-a178-4ea0-94f7-a9e092c56796" CreationDate="2017-06-24T12:41:55.300" UserId="73" Comment="minor grammar fix" Text="As far as I know you can not write compressed fastq on the fly. You will have to write the entire fastq and then compress it." />
  <row Id="2796" PostHistoryTypeId="2" PostId="897" RevisionGUID="e84d5d32-2848-471a-83dc-a1e713ba0dbc" CreationDate="2017-06-24T15:37:00.853" UserId="776" Text="Via Gencode and BEDOPS `convert2bed`:&#xD;&#xA;  &#xD;&#xA;    $ wget -qO- ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_21/gencode.v21.annotation.gff3.gz \&#xD;&#xA;        | gunzip --stdout - \&#xD;&#xA;        | awk '$3 == &quot;gene&quot;' \&#xD;&#xA;        | convert2bed -i gff - \&#xD;&#xA;        &gt; genes.bed&#xD;&#xA;&#xD;&#xA;BEDOPS: https://github.com/bedops/bedops" />
  <row Id="2798" PostHistoryTypeId="2" PostId="898" RevisionGUID="f933534a-b590-40fa-a7b8-d04b41085e6a" CreationDate="2017-06-24T17:29:15.333" UserId="136" Text="I've had a recent spike of interest in similar plots. I came across a project called [pauvre][1] (french for 'poor', play on 'pore') through the Oxford Nanopore Technologies (ONT) community that I think is even better than MinKNOW's base calling plot.  Plus, you can generate these plots from a fastq file when ever you want to, unlike with MinKNOW. &#xD;&#xA;&#xD;&#xA;***[Note: I am not the original author, but I am now contributing to this project]***&#xD;&#xA;&#xD;&#xA;[![My description][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/conchoecia/pauvre&#xD;&#xA;  [2]: https://i.stack.imgur.com/Jl7JS.png&#xD;&#xA;&#xD;&#xA;Pauvre currently uses Biopython to parse the fastq and `matplotlib` for the actual plot, and will let you choose the output image format (e.g., .png, .pdf, etc.). You can even choose whether the background is transparent or white (for .png output). &#xD;&#xA;&#xD;&#xA;Pauvre will also report useful statistics:&#xD;&#xA;&#xD;&#xA;    fastq stats for fastq_runid_bb8b8ddedb22bdd6802b2bfa2b4e424c92c30d28_0.fastq&#xD;&#xA;    numReads: 2164829&#xD;&#xA;    numBasepairs: 4970615217&#xD;&#xA;    meanLen: 2296.077527139557&#xD;&#xA;    medianLen: 1495.0&#xD;&#xA;    minLen: 5&#xD;&#xA;    maxLen: 392031&#xD;&#xA;    N50: 3450&#xD;&#xA;    L50: 402786&#xD;&#xA;&#xD;&#xA;                                   Basepairs &gt;= bin by mean PHRED and length&#xD;&#xA;    minLen          Q0          Q5         Q10         Q15       Q17.5         Q20      Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  4970615217  4970611559  4835461771  3889995868  2900103275  1087779109  165637656  429      0    0&#xD;&#xA;     50000    11531044    11531044      270324      160128       50729       50729          0    0      0    0&#xD;&#xA;    100000     6260554     6260554           0           0           0           0          0    0      0    0&#xD;&#xA;    150000     3504240     3504240           0           0           0           0          0    0      0    0&#xD;&#xA;    200000     2501101     2501101           0           0           0           0          0    0      0    0&#xD;&#xA;    250000     1609592     1609592           0           0           0           0          0    0      0    0&#xD;&#xA;    300000     1033423     1033423           0           0           0           0          0    0      0    0&#xD;&#xA;    350000      392031      392031           0           0           0           0          0    0      0    0&#xD;&#xA;&#xD;&#xA;                        Number of reads &gt;= bin by mean Phred+Len&#xD;&#xA;    minLen       Q0       Q5      Q10      Q15    Q17.5     Q20  Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  2164829  2164605  2083436  1626706  1183812  435687  77341    1      0    0&#xD;&#xA;     50000      109      109        5        3        1       1      0    0      0    0&#xD;&#xA;    100000       36       36        0        0        0       0      0    0      0    0&#xD;&#xA;    150000       15       15        0        0        0       0      0    0      0    0&#xD;&#xA;    200000        9        9        0        0        0       0      0    0      0    0&#xD;&#xA;    250000        5        5        0        0        0       0      0    0      0    0&#xD;&#xA;    300000        3        3        0        0        0       0      0    0      0    0&#xD;&#xA;    350000        1        1        0        0        0       0      0    0      0    0&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The parser is currently super slow because it's using `SeqIO.parse`, but we're changing parsers to speed that up. We're also adding some extra features (e.g., choose whether to include y-axes in margin histograms, print some stats directly to the plot, etc.)&#xD;&#xA;&#xD;&#xA;Purple is currently the only color choice (which I personally love), but adding options to change that will be super easy.&#xD;&#xA;" />
  <row Id="2799" PostHistoryTypeId="5" PostId="898" RevisionGUID="711a373c-e4f1-423c-8009-1debee2c4366" CreationDate="2017-06-24T18:00:12.117" UserId="136" Comment="Added information why this isn't currently useful for PacBio data" Text="***You specifically asked about FASTA files, but it's important to always consider read length and quality jointly, which FASTA files cannot provide.***&#xD;&#xA;&#xD;&#xA;I've had a recent spike of interest in similar plots. I came across a project called [pauvre][1] (french for 'poor', play on 'pore') through the Oxford Nanopore Technologies (ONT) community that I think is even better than MinKNOW's base calling plot.  Plus, you can generate these plots from a fastq file when ever you want to, unlike with MinKNOW. &#xD;&#xA;&#xD;&#xA;***[Note: I am not the original author, but I am now contributing to this project]***&#xD;&#xA;&#xD;&#xA;[![My description][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Pauvre currently uses Biopython to parse the fastq and `matplotlib` for the actual plot, and will let you choose the output image format (e.g., .png, .pdf, etc.). You can even choose whether the background is transparent or white (for .png output). &#xD;&#xA;&#xD;&#xA;Pauvre will also report useful statistics:&#xD;&#xA;&#xD;&#xA;    fastq stats for fastq_runid_bb8b8ddedb22bdd6802b2bfa2b4e424c92c30d28_0.fastq&#xD;&#xA;    numReads: 2164829&#xD;&#xA;    numBasepairs: 4970615217&#xD;&#xA;    meanLen: 2296.077527139557&#xD;&#xA;    medianLen: 1495.0&#xD;&#xA;    minLen: 5&#xD;&#xA;    maxLen: 392031&#xD;&#xA;    N50: 3450&#xD;&#xA;    L50: 402786&#xD;&#xA;&#xD;&#xA;                                   Basepairs &gt;= bin by mean PHRED and length&#xD;&#xA;    minLen          Q0          Q5         Q10         Q15       Q17.5         Q20      Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  4970615217  4970611559  4835461771  3889995868  2900103275  1087779109  165637656  429      0    0&#xD;&#xA;     50000    11531044    11531044      270324      160128       50729       50729          0    0      0    0&#xD;&#xA;    100000     6260554     6260554           0           0           0           0          0    0      0    0&#xD;&#xA;    150000     3504240     3504240           0           0           0           0          0    0      0    0&#xD;&#xA;    200000     2501101     2501101           0           0           0           0          0    0      0    0&#xD;&#xA;    250000     1609592     1609592           0           0           0           0          0    0      0    0&#xD;&#xA;    300000     1033423     1033423           0           0           0           0          0    0      0    0&#xD;&#xA;    350000      392031      392031           0           0           0           0          0    0      0    0&#xD;&#xA;&#xD;&#xA;                        Number of reads &gt;= bin by mean Phred+Len&#xD;&#xA;    minLen       Q0       Q5      Q10      Q15    Q17.5     Q20  Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  2164829  2164605  2083436  1626706  1183812  435687  77341    1      0    0&#xD;&#xA;     50000      109      109        5        3        1       1      0    0      0    0&#xD;&#xA;    100000       36       36        0        0        0       0      0    0      0    0&#xD;&#xA;    150000       15       15        0        0        0       0      0    0      0    0&#xD;&#xA;    200000        9        9        0        0        0       0      0    0      0    0&#xD;&#xA;    250000        5        5        0        0        0       0      0    0      0    0&#xD;&#xA;    300000        3        3        0        0        0       0      0    0      0    0&#xD;&#xA;    350000        1        1        0        0        0       0      0    0      0    0&#xD;&#xA;&#xD;&#xA;These plots would be equally useful with PacBio, but that's not super easy with current raw output from the Sequel sequencer: [Which quality score encoding does PacBio use?][3]&#xD;&#xA;&#xD;&#xA;The parser is currently super slow because it's using `SeqIO.parse`, but we're changing parsers to speed that up. We're also adding some extra features (e.g., choose whether to include y-axes in margin histograms, print some stats directly to the plot, etc.)&#xD;&#xA;&#xD;&#xA;Purple is currently the only color choice (which I personally love), but adding options to change that will be super easy.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/conchoecia/pauvre&#xD;&#xA;  [2]: https://i.stack.imgur.com/Jl7JS.png&#xD;&#xA;  [3]: https://bioinformatics.stackexchange.com/questions/885/which-quality-score-encoding-does-pacbio-use/888" />
  <row Id="2800" PostHistoryTypeId="2" PostId="899" RevisionGUID="7ca85550-731f-4c1f-8ff6-3a836bc55707" CreationDate="2017-06-24T18:16:49.600" UserId="136" Text="How do you generate read-length vs read-quality plot (heat map with histograms in the margin) for long-read sequencing data from the Oxford Nanopore Technologies (ONT) MinION? The MinKNOW software from ONT provides a plot like this during base calling.&#xD;&#xA;&#xD;&#xA;This would also be very helpful for PacBio data." />
  <row Id="2801" PostHistoryTypeId="1" PostId="899" RevisionGUID="7ca85550-731f-4c1f-8ff6-3a836bc55707" CreationDate="2017-06-24T18:16:49.600" UserId="136" Text="How do you generate read-length vs read-quality plot for long-read sequencing data (e.g., MinION)?" />
  <row Id="2802" PostHistoryTypeId="3" PostId="899" RevisionGUID="7ca85550-731f-4c1f-8ff6-3a836bc55707" CreationDate="2017-06-24T18:16:49.600" UserId="136" Text="&lt;sequencing&gt;&lt;biopython&gt;&lt;fastq&gt;&lt;python&gt;" />
  <row Id="2803" PostHistoryTypeId="2" PostId="900" RevisionGUID="4b66134f-805b-4b99-8abc-f33d680a61a2" CreationDate="2017-06-24T18:16:49.600" UserId="136" Text="It's important to always consider read length and quality jointly with high-error read data. Current long-read technologies (e.g., MinION and PacBio) have high error.&#xD;&#xA;&#xD;&#xA;I've had a recent spike of interest in similar plots and came across a project called [pauvre][1] (french for 'poor', play on 'pore') through the Oxford Nanopore Technologies (ONT) community that I think is even better than MinKNOW's base calling plot.  Plus, you can generate these plots from a fastq file when ever you want to, unlike with MinKNOW. &#xD;&#xA;&#xD;&#xA;***[Note: I am not the original author, but I am now contributing to this project]***&#xD;&#xA;&#xD;&#xA;[![My description][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Pauvre currently uses Biopython to parse the fastq and `matplotlib` for the actual plot, and will let you choose the output image format (e.g., .png, .pdf, etc.). You can even choose whether the background is transparent or white (for .png output). &#xD;&#xA;&#xD;&#xA;Pauvre will also report useful statistics:&#xD;&#xA;&#xD;&#xA;    fastq stats for fastq_runid_bb8b8ddedb22bdd6802b2bfa2b4e424c92c30d28_0.fastq&#xD;&#xA;    numReads: 2164829&#xD;&#xA;    numBasepairs: 4970615217&#xD;&#xA;    meanLen: 2296.077527139557&#xD;&#xA;    medianLen: 1495.0&#xD;&#xA;    minLen: 5&#xD;&#xA;    maxLen: 392031&#xD;&#xA;    N50: 3450&#xD;&#xA;    L50: 402786&#xD;&#xA;&#xD;&#xA;                                   Basepairs &gt;= bin by mean PHRED and length&#xD;&#xA;    minLen          Q0          Q5         Q10         Q15       Q17.5         Q20      Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  4970615217  4970611559  4835461771  3889995868  2900103275  1087779109  165637656  429      0    0&#xD;&#xA;     50000    11531044    11531044      270324      160128       50729       50729          0    0      0    0&#xD;&#xA;    100000     6260554     6260554           0           0           0           0          0    0      0    0&#xD;&#xA;    150000     3504240     3504240           0           0           0           0          0    0      0    0&#xD;&#xA;    200000     2501101     2501101           0           0           0           0          0    0      0    0&#xD;&#xA;    250000     1609592     1609592           0           0           0           0          0    0      0    0&#xD;&#xA;    300000     1033423     1033423           0           0           0           0          0    0      0    0&#xD;&#xA;    350000      392031      392031           0           0           0           0          0    0      0    0&#xD;&#xA;&#xD;&#xA;                        Number of reads &gt;= bin by mean Phred+Len&#xD;&#xA;    minLen       Q0       Q5      Q10      Q15    Q17.5     Q20  Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  2164829  2164605  2083436  1626706  1183812  435687  77341    1      0    0&#xD;&#xA;     50000      109      109        5        3        1       1      0    0      0    0&#xD;&#xA;    100000       36       36        0        0        0       0      0    0      0    0&#xD;&#xA;    150000       15       15        0        0        0       0      0    0      0    0&#xD;&#xA;    200000        9        9        0        0        0       0      0    0      0    0&#xD;&#xA;    250000        5        5        0        0        0       0      0    0      0    0&#xD;&#xA;    300000        3        3        0        0        0       0      0    0      0    0&#xD;&#xA;    350000        1        1        0        0        0       0      0    0      0    0&#xD;&#xA;&#xD;&#xA;These plots would be equally useful with PacBio, but that's not super easy with current raw output from the Sequel sequencer: [Which quality score encoding does PacBio use?][3]&#xD;&#xA;&#xD;&#xA;The parser is currently super slow because it's using `SeqIO.parse`, but we're changing parsers to speed that up. We're also adding some extra features (e.g., choose whether to include y-axes in margin histograms, print some stats directly to the plot, etc.)&#xD;&#xA;&#xD;&#xA;Purple is currently the only color choice (which I personally love), but adding options to change that will be super easy.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/conchoecia/pauvre&#xD;&#xA;  [2]: https://i.stack.imgur.com/Jl7JS.png&#xD;&#xA;  [3]: https://bioinformatics.stackexchange.com/questions/885/which-quality-score-encoding-does-pacbio-use/888" />
  <row Id="2804" PostHistoryTypeId="5" PostId="898" RevisionGUID="b40efff6-57b6-49ef-a55b-ad14e4fa37e9" CreationDate="2017-06-24T18:24:57.810" UserId="136" Comment="deleted 2704 characters in body" Text="***You specifically asked about FASTA files, but it's important to always consider read length and quality jointly when assessing high-error long-read data. FASTA files do not provide the quality.*** This information will help you determine how successful the run was, how many reads were 'high quality', etc.&#xD;&#xA;&#xD;&#xA;I originally posted a full answer here, suggesting [pauvre][1], but I decided it was a little off topic since you seem to only have the FASTA files.&#xD;&#xA;&#xD;&#xA;I recommend generating FASTQ files, but I'm not sure whether you have the original base-called fast5 files. If so, generate FASTQs using `poretools` as follows ([poretools doc for generating FASTQ files][2]):&#xD;&#xA;&#xD;&#xA;    poretools fastq fast5/&#xD;&#xA;&#xD;&#xA;Then I recommend [generating a heat map and histogram margin plot][3] using [pauvre][1] with both read length and quality like shown below.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[![My description][4]][4]&#xD;&#xA;&#xD;&#xA;***[Note: I am not the original author for pauvre, but I am now contributing to this project]***&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/conchoecia/pauvre&#xD;&#xA;  [2]: https://poretools.readthedocs.io/en/latest/&#xD;&#xA;  [3]: https://bioinformatics.stackexchange.com/questions/899/how-do-you-generate-read-length-vs-read-quality-plot-for-long-read-sequencing-da/900#900&#xD;&#xA;  [4]: https://i.stack.imgur.com/Jl7JS.png&#xD;&#xA;  [5]: https://bioinformatics.stackexchange.com/questions/885/which-quality-score-encoding-does-pacbio-use/888" />
  <row Id="2805" PostHistoryTypeId="5" PostId="898" RevisionGUID="53fb91c5-713d-461f-877f-87b02631c770" CreationDate="2017-06-24T18:33:29.243" UserId="136" Comment="edited body" Text="***You specifically asked about FASTA files, but it's important to always consider read length and quality jointly when assessing high-error long-read data. FASTA files do not provide the quality. This information will help you determine how successful the run was, how many reads were 'high quality', etc.***&#xD;&#xA;&#xD;&#xA;I originally posted a full answer here, suggesting [pauvre][1], but I decided it was a little off topic since you seem to only have the FASTA files.&#xD;&#xA;&#xD;&#xA;I recommend generating FASTQ files, but I'm not sure whether you have the original base-called fast5 files. If so, generate FASTQs using `poretools` as follows ([poretools doc for generating FASTQ files][2]):&#xD;&#xA;&#xD;&#xA;    poretools fastq fast5/&#xD;&#xA;&#xD;&#xA;Then I recommend [generating a heat map and histogram margin plot][3] using [pauvre][1] with both read length and quality like shown below.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[![My description][4]][4]&#xD;&#xA;&#xD;&#xA;***[Note: I am not the original author for pauvre, but I am now contributing to this project]***&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/conchoecia/pauvre&#xD;&#xA;  [2]: https://poretools.readthedocs.io/en/latest/&#xD;&#xA;  [3]: https://bioinformatics.stackexchange.com/questions/899/how-do-you-generate-read-length-vs-read-quality-plot-for-long-read-sequencing-da/900#900&#xD;&#xA;  [4]: https://i.stack.imgur.com/Jl7JS.png&#xD;&#xA;  [5]: https://bioinformatics.stackexchange.com/questions/885/which-quality-score-encoding-does-pacbio-use/888" />
  <row Id="2806" PostHistoryTypeId="6" PostId="899" RevisionGUID="ea26eb3c-81c5-4848-ba9f-2387769905a6" CreationDate="2017-06-24T18:35:31.240" UserId="136" Comment="edited tags" Text="&lt;nanopore&gt;&lt;sequencing&gt;&lt;biopython&gt;&lt;fastq&gt;&lt;python&gt;" />
  <row Id="2807" PostHistoryTypeId="5" PostId="900" RevisionGUID="ae164e29-323d-4ee6-9c4b-0ccc78b90819" CreationDate="2017-06-24T18:37:52.247" UserId="136" Comment="added 205 characters in body" Text="It's important to always consider read length and quality jointly with high-error read data, and current long-read technologies (e.g., MinION and PacBio) have high error rates. ***Considering read length and quality jointly will help you determine how successful the run was, how many reads were 'high quality', whether the longer reads are 'real' (or just pore noise), etc.***&#xD;&#xA;&#xD;&#xA;I've had a recent spike of interest in similar plots and came across a project called [pauvre][1] (french for 'poor', play on 'pore') through the Oxford Nanopore Technologies (ONT) community that I think is even better than MinKNOW's base calling plot.  Plus, you can generate these plots from a fastq file when ever you want to, unlike with MinKNOW. &#xD;&#xA;&#xD;&#xA;***[Note: I am not the original author, but I am now contributing to this project]***&#xD;&#xA;&#xD;&#xA;[![My description][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Pauvre currently uses Biopython to parse the fastq and `matplotlib` for the actual plot, and will let you choose the output image format (e.g., .png, .pdf, etc.). You can even choose whether the background is transparent or white (for .png output). &#xD;&#xA;&#xD;&#xA;Pauvre will also report useful statistics:&#xD;&#xA;&#xD;&#xA;    fastq stats for fastq_runid_bb8b8ddedb22bdd6802b2bfa2b4e424c92c30d28_0.fastq&#xD;&#xA;    numReads: 2164829&#xD;&#xA;    numBasepairs: 4970615217&#xD;&#xA;    meanLen: 2296.077527139557&#xD;&#xA;    medianLen: 1495.0&#xD;&#xA;    minLen: 5&#xD;&#xA;    maxLen: 392031&#xD;&#xA;    N50: 3450&#xD;&#xA;    L50: 402786&#xD;&#xA;&#xD;&#xA;                                   Basepairs &gt;= bin by mean PHRED and length&#xD;&#xA;    minLen          Q0          Q5         Q10         Q15       Q17.5         Q20      Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  4970615217  4970611559  4835461771  3889995868  2900103275  1087779109  165637656  429      0    0&#xD;&#xA;     50000    11531044    11531044      270324      160128       50729       50729          0    0      0    0&#xD;&#xA;    100000     6260554     6260554           0           0           0           0          0    0      0    0&#xD;&#xA;    150000     3504240     3504240           0           0           0           0          0    0      0    0&#xD;&#xA;    200000     2501101     2501101           0           0           0           0          0    0      0    0&#xD;&#xA;    250000     1609592     1609592           0           0           0           0          0    0      0    0&#xD;&#xA;    300000     1033423     1033423           0           0           0           0          0    0      0    0&#xD;&#xA;    350000      392031      392031           0           0           0           0          0    0      0    0&#xD;&#xA;&#xD;&#xA;                        Number of reads &gt;= bin by mean Phred+Len&#xD;&#xA;    minLen       Q0       Q5      Q10      Q15    Q17.5     Q20  Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  2164829  2164605  2083436  1626706  1183812  435687  77341    1      0    0&#xD;&#xA;     50000      109      109        5        3        1       1      0    0      0    0&#xD;&#xA;    100000       36       36        0        0        0       0      0    0      0    0&#xD;&#xA;    150000       15       15        0        0        0       0      0    0      0    0&#xD;&#xA;    200000        9        9        0        0        0       0      0    0      0    0&#xD;&#xA;    250000        5        5        0        0        0       0      0    0      0    0&#xD;&#xA;    300000        3        3        0        0        0       0      0    0      0    0&#xD;&#xA;    350000        1        1        0        0        0       0      0    0      0    0&#xD;&#xA;&#xD;&#xA;These plots would be equally useful with PacBio, but that's not super easy with current raw output from the Sequel sequencer: [Which quality score encoding does PacBio use?][3]&#xD;&#xA;&#xD;&#xA;The parser is currently super slow because it's using `SeqIO.parse`, but we're changing parsers to speed that up. We're also adding some extra features (e.g., choose whether to include y-axes in margin histograms, print some stats directly to the plot, etc.)&#xD;&#xA;&#xD;&#xA;Purple is currently the only color choice (which I personally love), but adding options to change that will be super easy.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/conchoecia/pauvre&#xD;&#xA;  [2]: https://i.stack.imgur.com/Jl7JS.png&#xD;&#xA;  [3]: https://bioinformatics.stackexchange.com/questions/885/which-quality-score-encoding-does-pacbio-use/888" />
  <row Id="2808" PostHistoryTypeId="5" PostId="900" RevisionGUID="a06b1027-d0a1-4cde-9107-9696a8f1487d" CreationDate="2017-06-24T18:43:59.043" UserId="136" Comment="added 205 characters in body" Text="It's important to always consider read length and quality jointly with high-error read data, and current long-read technologies (e.g., MinION and PacBio) have high error rates. ***Considering read length and quality jointly will help you determine how successful the run was, how many reads were 'high quality', whether the longer reads are 'real' (or just pore noise), etc.***&#xD;&#xA;&#xD;&#xA;I've had a recent spike of interest in similar plots and came across a project called [pauvre][1] (french for 'poor', play on 'pore') through the Oxford Nanopore Technologies (ONT) community that I think is even better than MinKNOW's base calling plot.  Plus, you can generate these plots from a fastq file when ever you want to, unlike with MinKNOW. &#xD;&#xA;&#xD;&#xA;***[Note: I am not the original author, but I am now contributing because I liked (and needed) it so much.]***&#xD;&#xA;&#xD;&#xA;[![My description][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Pauvre will also report useful statistics:&#xD;&#xA;&#xD;&#xA;    fastq stats for fastq_runid_bb8b8ddedb22bdd6802b2bfa2b4e424c92c30d28_0.fastq&#xD;&#xA;    numReads: 2164829&#xD;&#xA;    numBasepairs: 4970615217&#xD;&#xA;    meanLen: 2296.077527139557&#xD;&#xA;    medianLen: 1495.0&#xD;&#xA;    minLen: 5&#xD;&#xA;    maxLen: 392031&#xD;&#xA;    N50: 3450&#xD;&#xA;    L50: 402786&#xD;&#xA;&#xD;&#xA;                                   Basepairs &gt;= bin by mean PHRED and length&#xD;&#xA;    minLen          Q0          Q5         Q10         Q15       Q17.5         Q20      Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  4970615217  4970611559  4835461771  3889995868  2900103275  1087779109  165637656  429      0    0&#xD;&#xA;     50000    11531044    11531044      270324      160128       50729       50729          0    0      0    0&#xD;&#xA;    100000     6260554     6260554           0           0           0           0          0    0      0    0&#xD;&#xA;    150000     3504240     3504240           0           0           0           0          0    0      0    0&#xD;&#xA;    200000     2501101     2501101           0           0           0           0          0    0      0    0&#xD;&#xA;    250000     1609592     1609592           0           0           0           0          0    0      0    0&#xD;&#xA;    300000     1033423     1033423           0           0           0           0          0    0      0    0&#xD;&#xA;    350000      392031      392031           0           0           0           0          0    0      0    0&#xD;&#xA;&#xD;&#xA;                        Number of reads &gt;= bin by mean Phred+Len&#xD;&#xA;    minLen       Q0       Q5      Q10      Q15    Q17.5     Q20  Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  2164829  2164605  2083436  1626706  1183812  435687  77341    1      0    0&#xD;&#xA;     50000      109      109        5        3        1       1      0    0      0    0&#xD;&#xA;    100000       36       36        0        0        0       0      0    0      0    0&#xD;&#xA;    150000       15       15        0        0        0       0      0    0      0    0&#xD;&#xA;    200000        9        9        0        0        0       0      0    0      0    0&#xD;&#xA;    250000        5        5        0        0        0       0      0    0      0    0&#xD;&#xA;    300000        3        3        0        0        0       0      0    0      0    0&#xD;&#xA;    350000        1        1        0        0        0       0      0    0      0    0&#xD;&#xA;&#xD;&#xA;***These plots and stats would be equally useful with PacBio, but that's not super easy (though it is possible) with current raw output from the Sequel sequencer: [Which quality score encoding does PacBio use?][3]***&#xD;&#xA;&#xD;&#xA;Pauvre currently uses Biopython to parse the fastq and `matplotlib` for the actual plot, and will let you choose the output image format (e.g., .png, .pdf, etc.). You can also choose whether the background is transparent or white (for .png output). &#xD;&#xA;&#xD;&#xA;The parser is currently super slow because it's using `SeqIO.parse`, but we're changing parsers to speed that up. We're also adding some extra features (e.g., choose whether to include y-axes in margin histograms, print some stats directly to the plot for documentation, etc.)&#xD;&#xA;&#xD;&#xA;Purple is currently the only color choice (which I personally love), but adding options to change that will be super easy.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/conchoecia/pauvre&#xD;&#xA;  [2]: https://i.stack.imgur.com/Jl7JS.png&#xD;&#xA;  [3]: https://bioinformatics.stackexchange.com/questions/885/which-quality-score-encoding-does-pacbio-use/888" />
  <row Id="2809" PostHistoryTypeId="5" PostId="894" RevisionGUID="d2adfee4-809e-4b1c-a049-9e3bc4d6bdce" CreationDate="2017-06-24T18:45:00.210" UserId="136" Comment="Added syntax highlighting. " Text="I'm not sure I'm doing it the best way, but here is an example where I read a compressed gzip fastq file and write the records in block gzip fastq:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO, bgzf&#xD;&#xA;    # Used to convert the fastq stream into a file handle&#xD;&#xA;    from io import StringIO&#xD;&#xA;    from gzip import open as gzopen&#xD;&#xA;    records = SeqIO.parse(&#xD;&#xA;        StringIO(gzopen(&quot;random_10.fastq.gz&quot;).read().decode(&quot;utf-8&quot;)),&#xD;&#xA;        format=&quot;fastq&quot;)&#xD;&#xA;    with bgzf.BgzfWriter(&quot;test.fastq.bgz&quot;, &quot;wb&quot;) as outgz:&#xD;&#xA;        SeqIO.write(sequences=records, handle=outgz, format=&quot;fastq&quot;)" />
  <row Id="2810" PostHistoryTypeId="24" PostId="894" RevisionGUID="d2adfee4-809e-4b1c-a049-9e3bc4d6bdce" CreationDate="2017-06-24T18:45:00.210" Comment="Proposed by 136 approved by 57, 77 edit id of 225" />
  <row Id="2811" PostHistoryTypeId="5" PostId="897" RevisionGUID="4c7ca2ee-6a41-43ea-9628-2a7862d7ba8a" CreationDate="2017-06-24T19:07:49.853" UserId="776" Comment="added 183 characters in body" Text="Via Gencode and BEDOPS `convert2bed`:&#xD;&#xA;  &#xD;&#xA;    $ wget -qO- ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_21/gencode.v21.annotation.gff3.gz \&#xD;&#xA;        | gunzip --stdout - \&#xD;&#xA;        | awk '$3 == &quot;gene&quot;' \&#xD;&#xA;        | convert2bed -i gff - \&#xD;&#xA;        &gt; genes.bed&#xD;&#xA;&#xD;&#xA;BEDOPS: https://github.com/bedops/bedops&#xD;&#xA;&#xD;&#xA;This is based off an answer I wrote on Biostars, which includes a Perl script for generating a BED file of exons from intron annotations: https://www.biostars.org/p/124515/#124522" />
  <row Id="2814" PostHistoryTypeId="5" PostId="888" RevisionGUID="505c7340-8139-4925-94b9-23b722d8ad32" CreationDate="2017-06-24T20:23:05.160" UserId="136" Comment="added 1 character in body" Text="PacBio does use PHRED 33, *but it turns out the question may be irrelevant* for the newer PacBio Sequel Sequencer, because it reports all base qualities as PHRED 0 (ASCII `!`). The RS-II reports PHRED 33 quality scores.&#xD;&#xA;&#xD;&#xA;The Sequel provides data in the form of *subreads*, which are the circular consensus sequences (CCS) from a single zero-mode waveguide (ZMW). Essentially, a single circularized DNA molecule enters a ZMW well, and is sequenced as many times as possible. (Sometimes multiple molecules get into a single well, but that's not the normal case.) The number of times this single molecule can be sequenced depends on several variables, including molecule length. The consensus sequence for this single molecule is then reported out as a subread.&#xD;&#xA;&#xD;&#xA;As I started playing with the quality scores in my data, I realized the quality score for every base is `!`, which equates to a PHRED quality of 0. I contacted someone at PacBio who informed me that they learned the base quality values were not any more useful than the bases themselves for generating a high-quality circular consensus sequence. Thus, rather than spend the computational effort to calculate the scores, they skipped it, and reported `!` for every base. They also report an overall read quality of 0.8, but this is simply a placeholder. They are working to make this information more readily available." />
  <row Id="2815" PostHistoryTypeId="5" PostId="900" RevisionGUID="d4d6d664-114d-4dc0-b860-cacade939d6b" CreationDate="2017-06-24T22:07:08.513" UserId="136" Comment="deleted 8 characters in body" Text="It's important to always consider read length and quality jointly with high-error read data, and current long-read technologies (e.g., MinION and PacBio) have high error rates. ***Considering read length and quality jointly will help you determine how successful the run was, how many reads were 'high quality', whether the longer reads are 'real' (or just pore noise), etc.***&#xD;&#xA;&#xD;&#xA;I've had a recent spike of interest in similar plots and came across a project called [pauvre][1] (french for 'poor', play on 'pore') through the Oxford Nanopore Technologies (ONT) community that I think is even better than MinKNOW's base calling plot.  Plus, you can generate these plots from a fastq file when ever you want to, unlike with MinKNOW. &#xD;&#xA;&#xD;&#xA;***[Note: I am not the original author, but I am now contributing because I liked (and needed) it.]***&#xD;&#xA;&#xD;&#xA;[![My description][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Pauvre will also report useful statistics:&#xD;&#xA;&#xD;&#xA;    fastq stats for fastq_runid_bb8b8ddedb22bdd6802b2bfa2b4e424c92c30d28_0.fastq&#xD;&#xA;    numReads: 2164829&#xD;&#xA;    numBasepairs: 4970615217&#xD;&#xA;    meanLen: 2296.077527139557&#xD;&#xA;    medianLen: 1495.0&#xD;&#xA;    minLen: 5&#xD;&#xA;    maxLen: 392031&#xD;&#xA;    N50: 3450&#xD;&#xA;    L50: 402786&#xD;&#xA;&#xD;&#xA;                                   Basepairs &gt;= bin by mean PHRED and length&#xD;&#xA;    minLen          Q0          Q5         Q10         Q15       Q17.5         Q20      Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  4970615217  4970611559  4835461771  3889995868  2900103275  1087779109  165637656  429      0    0&#xD;&#xA;     50000    11531044    11531044      270324      160128       50729       50729          0    0      0    0&#xD;&#xA;    100000     6260554     6260554           0           0           0           0          0    0      0    0&#xD;&#xA;    150000     3504240     3504240           0           0           0           0          0    0      0    0&#xD;&#xA;    200000     2501101     2501101           0           0           0           0          0    0      0    0&#xD;&#xA;    250000     1609592     1609592           0           0           0           0          0    0      0    0&#xD;&#xA;    300000     1033423     1033423           0           0           0           0          0    0      0    0&#xD;&#xA;    350000      392031      392031           0           0           0           0          0    0      0    0&#xD;&#xA;&#xD;&#xA;                        Number of reads &gt;= bin by mean Phred+Len&#xD;&#xA;    minLen       Q0       Q5      Q10      Q15    Q17.5     Q20  Q21.5  Q25  Q25.5  Q30&#xD;&#xA;         0  2164829  2164605  2083436  1626706  1183812  435687  77341    1      0    0&#xD;&#xA;     50000      109      109        5        3        1       1      0    0      0    0&#xD;&#xA;    100000       36       36        0        0        0       0      0    0      0    0&#xD;&#xA;    150000       15       15        0        0        0       0      0    0      0    0&#xD;&#xA;    200000        9        9        0        0        0       0      0    0      0    0&#xD;&#xA;    250000        5        5        0        0        0       0      0    0      0    0&#xD;&#xA;    300000        3        3        0        0        0       0      0    0      0    0&#xD;&#xA;    350000        1        1        0        0        0       0      0    0      0    0&#xD;&#xA;&#xD;&#xA;***These plots and stats would be equally useful with PacBio, but that's not super easy (though it is possible) with current raw output from the Sequel sequencer: [Which quality score encoding does PacBio use?][3]***&#xD;&#xA;&#xD;&#xA;Pauvre currently uses Biopython to parse the fastq and `matplotlib` for the actual plot, and will let you choose the output image format (e.g., .png, .pdf, etc.). You can also choose whether the background is transparent or white (for .png output). &#xD;&#xA;&#xD;&#xA;The parser is currently super slow because it's using `SeqIO.parse`, but we're changing parsers to speed that up. We're also adding some extra features (e.g., choose whether to include y-axes in margin histograms, print some stats directly to the plot for documentation, etc.)&#xD;&#xA;&#xD;&#xA;Purple is currently the only color choice (which I personally love), but adding options to change that will be super easy.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/conchoecia/pauvre&#xD;&#xA;  [2]: https://i.stack.imgur.com/Jl7JS.png&#xD;&#xA;  [3]: https://bioinformatics.stackexchange.com/questions/885/which-quality-score-encoding-does-pacbio-use/888" />
  <row Id="2816" PostHistoryTypeId="2" PostId="901" RevisionGUID="b6208633-6f29-4156-883d-a35107a50161" CreationDate="2017-06-24T23:33:03.743" UserId="146" Text="At the moment, the standard reference genomes (e.g. hg19, hg38) are haploid genomes. We know that the human genome is diploid. Naturally, the latter would be the respectively correct representation of the human genome. &#xD;&#xA;&#xD;&#xA;More and more biologists are using emerging technologies into order to capture the diploid nature of genetic information, e.g. phasing SNPs between mother and father chromosomes. &#xD;&#xA;&#xD;&#xA;What is the standard way that bioinformaticians generated a standard diploid reference genome? The easiest thing to do would be simply combine two reference genomes together. &#xD;&#xA;&#xD;&#xA;More importantly, how have large-scale genomic studies dealt with the fact that the haploid reference genome is a consensus-based &quot;half&quot; of a human genome? " />
  <row Id="2817" PostHistoryTypeId="1" PostId="901" RevisionGUID="b6208633-6f29-4156-883d-a35107a50161" CreationDate="2017-06-24T23:33:03.743" UserId="146" Text="What is the standard way to work with a diploid reference genome?" />
  <row Id="2818" PostHistoryTypeId="3" PostId="901" RevisionGUID="b6208633-6f29-4156-883d-a35107a50161" CreationDate="2017-06-24T23:33:03.743" UserId="146" Text="&lt;human-genome&gt;&lt;genome&gt;&lt;reference&gt;&lt;reference-genome&gt;" />
  <row Id="2819" PostHistoryTypeId="5" PostId="901" RevisionGUID="2fae1378-8203-4274-87d8-3b058d338c6d" CreationDate="2017-06-25T01:27:34.313" UserId="146" Comment="added 229 characters in body; edited title" Text="At the moment, the standard reference genomes (e.g. hg19, hg38) are haploid genomes. We know that the human genome is diploid. Naturally, the latter would be the respectively correct representation of the human genome. &#xD;&#xA;&#xD;&#xA;More and more biologists are using emerging technologies into order to capture the diploid nature of genetic information, e.g. phasing SNPs between mother and father chromosomes. &#xD;&#xA;&#xD;&#xA;What is the standard way that bioinformaticians generated a standard diploid reference genome? &#xD;&#xA;&#xD;&#xA;Actually, reference genomes are not truly haploid (by my understanding). Given that reference genomes are 5'-3' coordinated, in order to create a complementary strand, one would need to take the 3'-5' complement. In order to have a diploid genome, you need two reference genomes and two 3'-5' complements.&#xD;&#xA;&#xD;&#xA;More importantly, how have large-scale genomic studies dealt with the fact that the haploid reference genome is a consensus-based &quot;half&quot; of a human genome? " />
  <row Id="2820" PostHistoryTypeId="4" PostId="901" RevisionGUID="2fae1378-8203-4274-87d8-3b058d338c6d" CreationDate="2017-06-25T01:27:34.313" UserId="146" Comment="added 229 characters in body; edited title" Text="What is the standard way to work with a diploid reference genome? Complementary strands?" />
  <row Id="2821" PostHistoryTypeId="2" PostId="902" RevisionGUID="35ac449a-0eee-49af-908c-42c8b80131f2" CreationDate="2017-06-25T02:00:16.993" UserId="37" Text="For calling small variants, the *standard way* is to simply call diploid genotypes. You can already do a variety of research with unphased genotypes. You may further phase genotypes with imputation, pedigree or with long reads/linked reads, but not many are doing this because phasing is more difficult, may add cost and may not always give you new insight into your data. For these analyses, we use a haploid genome. For human samples, the vast majority of &quot;large-scale genomic studies&quot; are done this way.&#xD;&#xA;&#xD;&#xA;A diploid reference actually doesn't help much with reference-based analysis; it only complicates algorithms. What could help a lot is a population reference, which may be represented by a graph or a compressed full-text index or both. In theory, if you have a comprehensive population reference and a capable mapping algorithm, you may call extra variants that would not be callable with short reads. In practice, however, there are quite a few technical challenges. Handling population references is a research topic. There are no &quot;standards&quot; yet.&#xD;&#xA;&#xD;&#xA;If the goal is to assemble a new reference genome from a diploid sample, we almost always prefer to produce a diploid assembly. Unfortunately, I believe there are no &quot;standard&quot; procedures, either. SuperNova from 10x genomics builds the diploid information into a graph. Falcon from PacBio uses &quot;unzip&quot;. I don't think they have got widely used and evaluated so far.&#xD;&#xA;&#xD;&#xA;PS: saw your edit while writing the above. The fact that the genome only represents one strand does not mean we have to create the complement strand explicitly in analyses. We do most of reverse complement on the fly in algorithms as well as in mind.&#xD;&#xA;&#xD;&#xA;&gt; reference genomes are not truly haploid&#xD;&#xA;&#xD;&#xA;That depends on how the reference is assembled. If you sequence a haploid sample (e.g. bacteria), your assembly will be haploid. If you sequence an inbreed lab strain that is almost homozygous (e.g. mouse and fruit fly), your assembly will be nearly haploid. If you sequence a diploid sample, your assembly is very likely to be a mosaic of the two haplotypes. In case of the human reference genome, it is more complicated. It is largely a mosaic of several humans by stitching ~150kb haplotypes from these samples." />
  <row Id="2822" PostHistoryTypeId="6" PostId="676" RevisionGUID="4fc88cc4-432e-4d53-be0b-cd41c1c6644a" CreationDate="2017-06-25T02:02:35.567" UserId="37" Comment="edited tags" Text="&lt;biopython&gt;&lt;perl&gt;" />
  <row Id="2824" PostHistoryTypeId="6" PostId="901" RevisionGUID="092286e3-916f-47ea-9e0c-741948541934" CreationDate="2017-06-25T02:07:50.473" UserId="37" Comment="edited tags" Text="&lt;human-genome&gt;&lt;genome&gt;&lt;reference-genome&gt;" />
  <row Id="2825" PostHistoryTypeId="6" PostId="27" RevisionGUID="566c7f45-36e4-4839-80d7-58c13d18ac75" CreationDate="2017-06-25T02:10:00.757" UserId="37" Comment="edited tags" Text="&lt;bam&gt;&lt;file-formats&gt;&lt;bed&gt;&lt;format-conversion&gt;&lt;reference-genome&gt;" />
  <row Id="2826" PostHistoryTypeId="6" PostId="396" RevisionGUID="dce23ec5-b011-47f2-9423-501414850a81" CreationDate="2017-06-25T02:11:15.843" UserId="37" Comment="edited tags" Text="&lt;bwa&gt;&lt;samtools&gt;&lt;reference-genome&gt;" />
  <row Id="2827" PostHistoryTypeId="6" PostId="322" RevisionGUID="f53d6437-ff46-49f9-88e3-9bbfd9ee8e42" CreationDate="2017-06-25T02:11:38.870" UserId="37" Comment="edited tags" Text="&lt;human-genome&gt;&lt;reference-genome&gt;&lt;genome-sequencing&gt;" />
  <row Id="2828" PostHistoryTypeId="5" PostId="184" RevisionGUID="2d1e7e82-4796-4ebc-ad85-77f64a9422c5" CreationDate="2017-06-25T02:12:47.427" UserId="37" Comment="Clarify that BED is a file format." Text="BED is a file format that has been adapted for a variety of other more generic genome annotation use cases.&#xD;&#xA;&#xD;&#xA;- [UCSC description](https://genome.ucsc.edu/FAQ/FAQformat.html)&#xD;&#xA;&#xD;&#xA;- [Ensembl description](http://www.ensembl.org/info/website/upload/bed.html)" />
  <row Id="2829" PostHistoryTypeId="2" PostId="903" RevisionGUID="1bde8b2e-51c8-4465-8f8d-6f8ea8614108" CreationDate="2017-06-25T02:14:49.307" UserId="37" Text="" />
  <row Id="2830" PostHistoryTypeId="1" PostId="903" RevisionGUID="1bde8b2e-51c8-4465-8f8d-6f8ea8614108" CreationDate="2017-06-25T02:14:49.307" UserId="37" />
  <row Id="2831" PostHistoryTypeId="2" PostId="904" RevisionGUID="1a5e601f-b370-47d9-aee5-deb28500eae0" CreationDate="2017-06-25T02:14:49.307" UserId="37" Comment="edited body" Text="An insertion/deletion sequence variation." />
  <row Id="2832" PostHistoryTypeId="1" PostId="904" RevisionGUID="1a5e601f-b370-47d9-aee5-deb28500eae0" CreationDate="2017-06-25T02:14:49.307" UserId="37" Comment="edited body" />
  <row Id="2833" PostHistoryTypeId="2" PostId="905" RevisionGUID="7a9ae58c-2d2e-4942-b248-c3269409dfbc" CreationDate="2017-06-25T03:48:52.500" UserId="73" Text="There are some assemblers that produce assembly graphs that attempt to describe all the possible haploid paths within a set of reads. Such an assembly attempts to capture all the diploid variation (and/or population variation) in a sample at the expense of not having full-length chromosomes.&#xD;&#xA;&#xD;&#xA;Canu (for example) will produce contigs that are extended as long as consensus is maintained across different reads, but when there is a reliable break in coverage (i.e. an area where chromosomes are heterozygous) then the contigs will be broken up. Canu provides as output a GFA file (assembly graph) that can be used to determine which paths might combine together into a single chromosome." />
  <row Id="2834" PostHistoryTypeId="5" PostId="890" RevisionGUID="129eef76-edf8-4839-8ba4-b3dd36772072" CreationDate="2017-06-25T07:25:07.777" UserId="541" Comment="improved the answer with what I believe the OP is actually after." Text="I believe you're looking for `env_nr`? It's listed as such, under `Metagenomic proteins` in the blastp webpage. It appears that the word within brackets should be supplied alongside the `-db` parameter. A quick test with a dummy amino acid fasta file does turn up a result to a valid NCBI protein accession.&#xD;&#xA;&#xD;&#xA;[![partial screenshot from NCBI's blastp webpage][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/hNm3k.png&#xD;&#xA;&#xD;&#xA;Edit: I followed up on this with a little more digging around, and it appears the database you're looking for is called `Microbial_proteins`. I ended up quote-searching `Microbial proteins from nr` on Google and exactly 2 hits turn up. The first hit, I think, is what the OP posted a screenshot of in a comment to this answer. The second is a list of databases along with keywords from a Spain-based bioinformatics firm: http://data.biobam.com/ncbi_blast_dbs_protein.pdf. I retried the dummy example with the `Microbial_proteins` database, and it seems to work. Again, I am not sure why I can't easily find this on an NCBI resource. Also, I am curious as to how different this database is from `env_nr`.&#xD;&#xA;" />
  <row Id="2836" PostHistoryTypeId="2" PostId="906" RevisionGUID="6aca06e4-2ff1-485f-b39e-627fc5eea75a" CreationDate="2017-06-25T22:03:05.130" UserId="931" Text="From [this document][1], it looks like REMARK 250 is the way to go.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.bmsc.washington.edu/CrystaLinks/man/pdb/part_31.html" />
  <row Id="2837" PostHistoryTypeId="2" PostId="907" RevisionGUID="19792714-4211-4065-9ad8-7e2bc661bc0f" CreationDate="2017-06-26T07:23:26.087" UserId="492" Text="&gt; At the moment, the standard reference genomes (e.g. hg19, hg38) are haploid genomes. We know that the human genome is diploid. Naturally, the latter would be the respectively correct representation of the human genome.&#xD;&#xA;&#xD;&#xA;The premise of your question is false.  The natural reference representation of the human genome is not diploid.&#xD;&#xA;&#xD;&#xA;### Think of a reference genome as a map, and not as a specific example of a human being's DNA.&#xD;&#xA;&#xD;&#xA;Not only is the human genome reference haploid, but [it is also a composite genome][1]. This means that the human genome reference sequence is composed of sequences from multiple individuals.  In other words, the human reference does not correspond to any one human sequence. &#xD;&#xA;&#xD;&#xA;Any particular read from a DNA sequencer will be a read from a human genome that diverges from the reference genome. So an algorithm that tries to match the read to the reference genome will always need to handle potential discrepancies. Adding a second map against which to match a read would not change that fact.  Therefore, there is little value in providing a second haploid reference genome.&#xD;&#xA;&#xD;&#xA;**Side note**:&#xD;&#xA;There are parts of the human genome that &quot;are too complex to be represented by a single path&quot;, and the [Genome Reference Consortium][2] provides &quot;alternate loci&quot; for such regions of the genome.&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Human_genome#Human_reference_genome&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/grc/human" />
  <row Id="2838" PostHistoryTypeId="2" PostId="908" RevisionGUID="97c056b0-c5ce-4997-8676-ec2b5d3bc22e" CreationDate="2017-06-26T08:06:51.857" UserId="57" Text="I have three unsorted `.sam` files I would like to merge, so I can call variants and heterozygosity estimates. For both these estimates I would like to have separate estimation of error profiles of different libraries, therefore I should specify a read groups.&#xD;&#xA;&#xD;&#xA;How can I merge multiple sam files? Preferably avoiding java (Picard tools).&#xD;&#xA;&#xD;&#xA;I figured out, that I can sort individual files using `samtools sort`, then use `samtools merge -r ...` on the sorted files. However, the read group wont make it to header (and variant called I use is complaining about it), also the read group is stupidly, the file name.&#xD;&#xA;&#xD;&#xA;I tried to define meaningful readgroup names using precedure described [here](https://www.biostars.org/p/47487/), but I found that this will not adjust the names of readgroups defined by `samtools merge` (the file names).&#xD;&#xA;&#xD;&#xA;So, I assume that If I will define the correct header with readgroups corresponding to merged filenames it could eventually work. Something like&#xD;&#xA;&#xD;&#xA;    samtools -rh rg.txt merged.bam s1.sort.sam s2.sort.sam s3.sort.sam&#xD;&#xA;&#xD;&#xA;where `rg.txt` is file with&#xD;&#xA;&#xD;&#xA;    @RG s1.sort&#xD;&#xA;    @RG s2.sort&#xD;&#xA;    @RG s3.sort&#xD;&#xA;    ...&#xD;&#xA;    output of samtools view -h s1.sort&#xD;&#xA;&#xD;&#xA;should do the job, but I do not like on this approach for couple of aspects :&#xD;&#xA;&#xD;&#xA;  - readgroups are not really a standard readgroup names (Sample/Library format)&#xD;&#xA;  - it is annoying that I have to generate the header file I will have to feed to the merge process.&#xD;&#xA;  - sam -&gt; sorted.sam -&gt; merged.bam requires ridiculous amount of space" />
  <row Id="2839" PostHistoryTypeId="1" PostId="908" RevisionGUID="97c056b0-c5ce-4997-8676-ec2b5d3bc22e" CreationDate="2017-06-26T08:06:51.857" UserId="57" Text="How to merge sam files together with adding read groups" />
  <row Id="2840" PostHistoryTypeId="3" PostId="908" RevisionGUID="97c056b0-c5ce-4997-8676-ec2b5d3bc22e" CreationDate="2017-06-26T08:06:51.857" UserId="57" Text="&lt;sam&gt;" />
  <row Id="2841" PostHistoryTypeId="5" PostId="908" RevisionGUID="ca79bc55-49f4-45b4-8957-57d437ca037d" CreationDate="2017-06-26T08:27:16.630" UserId="57" Comment="language improvements" Text="I have three unsorted `.sam` files I would like to merge, so I can call variants and heterozygosity estimates. For both these estimates I would like to have separate estimation of error profiles of different libraries, therefore I should specify a read groups.&#xD;&#xA;&#xD;&#xA;How can I merge multiple sam files? Preferably avoiding java (Picard tools).&#xD;&#xA;&#xD;&#xA;I figured out, that I can sort individual files using `samtools sort`, then used `samtools merge -r merged.bam s1.sort.sam s2.sort.sam s3.sort.sam` on the sorted files. However, the read group didn't make it to the header (and the variant caller I use is complaining about it), also the read group is stupidly the file name.&#xD;&#xA;&#xD;&#xA;I tried to define meaningful readgroup names using precedure described [here](https://www.biostars.org/p/47487/), but I found that this will not adjust the names of readgroups defined by `samtools merge` (the file names).&#xD;&#xA;&#xD;&#xA;So, I assume that If I will define the correct header with readgroups corresponding to merged filenames it could eventually work. Something like&#xD;&#xA;&#xD;&#xA;    samtools -rh rg.txt merged.bam s1.sort.sam s2.sort.sam s3.sort.sam&#xD;&#xA;&#xD;&#xA;where `rg.txt` is file with&#xD;&#xA;&#xD;&#xA;    @RG s1.sort&#xD;&#xA;    @RG s2.sort&#xD;&#xA;    @RG s3.sort&#xD;&#xA;    ...&#xD;&#xA;    output of samtools view -h s1.sort&#xD;&#xA;&#xD;&#xA;should do the job, but I do not like on this approach for couple of aspects :&#xD;&#xA;&#xD;&#xA;  - readgroups are not really a standard readgroup names (Sample/Library format)&#xD;&#xA;  - it is annoying that I have to generate the header file I will have to feed to the merge process.&#xD;&#xA;  - sam -&gt; sorted.sam -&gt; merged.bam requires ridiculous amount of space" />
  <row Id="2842" PostHistoryTypeId="5" PostId="908" RevisionGUID="f95d9632-71bf-4cd6-9a57-b9834020f41a" CreationDate="2017-06-26T09:12:50.953" UserId="57" Comment="adding the final question" Text="I have three unsorted `.sam` files I would like to merge, so I can call variants and heterozygosity estimates. For both these estimates I would like to have separate estimation of error profiles of different libraries, therefore I should specify a read groups.&#xD;&#xA;&#xD;&#xA;How can I merge multiple sam files? Preferably avoiding java (Picard tools).&#xD;&#xA;&#xD;&#xA;I figured out, that I can sort individual files using `samtools sort`, then used `samtools merge -r merged.bam s1.sort.sam s2.sort.sam s3.sort.sam` on the sorted files. However, the read group didn't make it to the header (and the variant caller I use is complaining about it), also the read group is stupidly the file name.&#xD;&#xA;&#xD;&#xA;I tried to define meaningful readgroup names using procedure described on [BioStars][1], but I found that this will just change the header, it does not adjust the names of read groups defined by `samtools merge` (the file names).&#xD;&#xA;&#xD;&#xA;I think, given this related thread on [SeqAnswers][2], that If I will define the correct header with read groups corresponding to merged file names it could eventually work. Something like&#xD;&#xA;&#xD;&#xA;    samtools -rh rg.txt merged.bam s1.sort.sam s2.sort.sam s3.sort.sam&#xD;&#xA;&#xD;&#xA;where `rg.txt` is file with&#xD;&#xA;&#xD;&#xA;    @RG s1.sort&#xD;&#xA;    @RG s2.sort&#xD;&#xA;    @RG s3.sort&#xD;&#xA;    ...&#xD;&#xA;    output of samtools view -h s1.sort&#xD;&#xA;&#xD;&#xA;should do the job, but I do not like on this approach for couple of aspects :&#xD;&#xA;&#xD;&#xA;  - readgroups are not really a standard readgroup names (Sample/Library format)&#xD;&#xA;  - it is annoying that I have to generate the header file I will have to feed to the merge process.&#xD;&#xA;  - sam -&gt; sorted.sam -&gt; merged.bam requires ridiculous amount of space&#xD;&#xA;&#xD;&#xA;Is there an elegant solution to merging of multiple `sam` files?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/47487/&#xD;&#xA;  [2]: http://seqanswers.com/forums/showthread.php?t=4180" />
  <row Id="2843" PostHistoryTypeId="2" PostId="909" RevisionGUID="872e1e3a-d032-4602-8c76-7f540b7fc048" CreationDate="2017-06-26T09:14:18.923" UserId="292" Text="Note: I also posted this issue (with less context) in the bioconductor support site: &lt;https://support.bioconductor.org/p/97424/&gt;&#xD;&#xA;&#xD;&#xA;I'm working on a snakemake workflow that identifies various small RNA species in _C. elegans_ small RNA-seq libraries. Some are endogenous siRNAs supposedly generated from RNA templates (through RNA-dependant RNA polymerases (RdRP)) that can be variously classified (protein-coding genes, transposons, and other repeat types).&#xD;&#xA;&#xD;&#xA;I count such small RNA reads in a set of libraries and try to identify differentially producing sources. For this, I use DESeq2 (that I run [using rpy2](https://stackoverflow.com/q/42157335/1878788) from within snakemake).&#xD;&#xA;&#xD;&#xA;I'm not sure DESeq2 is always appropriate for this kind of data, but so far, the analyses would at least complete.&#xD;&#xA;However, I recently added new potential types of small RNA source (simple repeats and satellites), and these happen to have low counts. I'm not 100% sure, but I suspect these low counts are the reason for failures during DESeq2 analyses (for debugging purposes, I ran this manually in R):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T)&#xD;&#xA;    estimating size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    -- note: fitType='parametric', but the dispersion trend was not well captured by the&#xD;&#xA;       function: y = a/x + b, and a local regression fit was automatically substituted.&#xD;&#xA;       specify fitType='local' or 'mean' to avoid this message next time.&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;    &gt; warnings()&#xD;&#xA;    Warning messages:&#xD;&#xA;    1: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    2: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    3: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    4: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    5: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    6: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    7: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    8: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    9: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    10: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    11: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    12: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The count matrix indeed contains a high proportion of zeroes, the rest are mostly ones, but there are some sources that seem a little more significantly producing small RNAs:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; mean(counts_data == 0)&#xD;&#xA;    [1] 0.7488889&#xD;&#xA;    &gt; mean(counts_data == 1)&#xD;&#xA;    [1] 0.1507407&#xD;&#xA;    &gt; max(counts_data)&#xD;&#xA;    [1] 34&#xD;&#xA;&#xD;&#xA;**How would you recommend handling such kind of data?**&#xD;&#xA;&#xD;&#xA;Should I for instance discard rows without enough counts?&#xD;&#xA;If so, what would be a reasonable threshold?" />
  <row Id="2844" PostHistoryTypeId="1" PostId="909" RevisionGUID="872e1e3a-d032-4602-8c76-7f540b7fc048" CreationDate="2017-06-26T09:14:18.923" UserId="292" Text="Running differential expression analyses on count matrices with many zeroes" />
  <row Id="2845" PostHistoryTypeId="3" PostId="909" RevisionGUID="872e1e3a-d032-4602-8c76-7f540b7fc048" CreationDate="2017-06-26T09:14:18.923" UserId="292" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;differential-expression&gt;&lt;deseq2&gt;" />
  <row Id="2846" PostHistoryTypeId="5" PostId="909" RevisionGUID="6c9e6d75-1408-4726-bf82-9259ac78a391" CreationDate="2017-06-26T09:28:45.880" UserId="292" Comment="Added attempt to obtain dispersion plots" Text="Note: I also posted this issue (with less context) in the bioconductor support site: &lt;https://support.bioconductor.org/p/97424/&gt;&#xD;&#xA;&#xD;&#xA;I'm working on a snakemake workflow that identifies various small RNA species in _C. elegans_ small RNA-seq libraries. Some are endogenous siRNAs supposedly generated from RNA templates (through RNA-dependant RNA polymerases (RdRP)) that can be variously classified (protein-coding genes, transposons, and other repeat types).&#xD;&#xA;&#xD;&#xA;I count such small RNA reads in a set of libraries and try to identify differentially producing sources. For this, I use DESeq2 (that I run [using rpy2](https://stackoverflow.com/q/42157335/1878788) from within snakemake).&#xD;&#xA;&#xD;&#xA;I'm not sure DESeq2 is always appropriate for this kind of data, but so far, the analyses would at least complete.&#xD;&#xA;However, I recently added new potential types of small RNA source (simple repeats and satellites), and these happen to have low counts. I'm not 100% sure, but I suspect these low counts are the reason for failures during DESeq2 analyses (for debugging purposes, I ran this manually in R):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T)&#xD;&#xA;    estimating size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    -- note: fitType='parametric', but the dispersion trend was not well captured by the&#xD;&#xA;       function: y = a/x + b, and a local regression fit was automatically substituted.&#xD;&#xA;       specify fitType='local' or 'mean' to avoid this message next time.&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;    &gt; warnings()&#xD;&#xA;    Warning messages:&#xD;&#xA;    1: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    2: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    3: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    4: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    5: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    6: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    7: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    8: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    9: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    10: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    11: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    12: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The count matrix indeed contains a high proportion of zeroes, the rest are mostly ones, but there are some sources that seem a little more significantly producing small RNAs:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; mean(counts_data == 0)&#xD;&#xA;    [1] 0.7488889&#xD;&#xA;    &gt; mean(counts_data == 1)&#xD;&#xA;    [1] 0.1507407&#xD;&#xA;    &gt; max(counts_data)&#xD;&#xA;    [1] 34&#xD;&#xA;&#xD;&#xA;**How would you recommend handling such kind of data?**&#xD;&#xA;&#xD;&#xA;Should I for instance discard rows without enough counts?&#xD;&#xA;If so, what would be a reasonable threshold?&#xD;&#xA;&#xD;&#xA;### Edit: trying to get a dispersion plot:&#xD;&#xA;&#xD;&#xA;The same error occurs when trying to estimate dispersion:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- estimateSizeFactors(dds)&#xD;&#xA;    &gt; dds &lt;- estimateDispersions(dds)&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    -- note: fitType='parametric', but the dispersion trend was not well captured by the&#xD;&#xA;       function: y = a/x + b, and a local regression fit was automatically substituted.&#xD;&#xA;       specify fitType='local' or 'mean' to avoid this message next time.&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;&#xD;&#xA;This seems to prevent the generation of dispersion plots:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; plotDispEsts(dds)&#xD;&#xA;    Error in min(py[py &gt; 0], na.rm = TRUE) : &#xD;&#xA;      invalid 'type' (list) of argument&#xD;&#xA;    In addition: Warning message:&#xD;&#xA;    In structure(x, class = unique(c(&quot;AsIs&quot;, oldClass(x)))) :&#xD;&#xA;      Calling 'structure(NULL, *)' is deprecated, as NULL cannot have attributes.&#xD;&#xA;      Consider 'structure(list(), *)' instead.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2847" PostHistoryTypeId="5" PostId="909" RevisionGUID="12b066d7-f96d-4dca-a6ef-f871e8401611" CreationDate="2017-06-26T09:34:30.173" UserId="292" Comment="added 457 characters in body" Text="Note: I also posted this issue (with less context) in the bioconductor support site: &lt;https://support.bioconductor.org/p/97424/&gt;&#xD;&#xA;&#xD;&#xA;I'm working on a snakemake workflow that identifies various small RNA species in _C. elegans_ small RNA-seq libraries. Some are endogenous siRNAs supposedly generated from RNA templates (through RNA-dependant RNA polymerases (RdRP)) that can be variously classified (protein-coding genes, transposons, and other repeat types).&#xD;&#xA;&#xD;&#xA;I count such small RNA reads in a set of libraries and try to identify differentially producing sources. For this, I use DESeq2 (that I run [using rpy2](https://stackoverflow.com/q/42157335/1878788) from within snakemake).&#xD;&#xA;&#xD;&#xA;I'm not sure DESeq2 is always appropriate for this kind of data, but so far, the analyses would at least complete.&#xD;&#xA;However, I recently added new potential types of small RNA source (simple repeats and satellites), and these happen to have low counts. I'm not 100% sure, but I suspect these low counts are the reason for failures during DESeq2 analyses (for debugging purposes, I ran this manually in R):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T)&#xD;&#xA;    estimating size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    -- note: fitType='parametric', but the dispersion trend was not well captured by the&#xD;&#xA;       function: y = a/x + b, and a local regression fit was automatically substituted.&#xD;&#xA;       specify fitType='local' or 'mean' to avoid this message next time.&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;    &gt; warnings()&#xD;&#xA;    Warning messages:&#xD;&#xA;    1: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    2: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    3: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    4: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    5: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    6: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    7: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    8: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    9: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    10: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    11: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    12: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The count matrix indeed contains a high proportion of zeroes, the rest are mostly ones, but there are some sources that seem a little more significantly producing small RNAs:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; mean(counts_data == 0)&#xD;&#xA;    [1] 0.7488889&#xD;&#xA;    &gt; mean(counts_data == 1)&#xD;&#xA;    [1] 0.1507407&#xD;&#xA;    &gt; max(counts_data)&#xD;&#xA;    [1] 34&#xD;&#xA;&#xD;&#xA;**How would you recommend handling such kind of data?**&#xD;&#xA;&#xD;&#xA;Should I for instance discard rows without enough counts?&#xD;&#xA;If so, what would be a reasonable threshold?&#xD;&#xA;&#xD;&#xA;### Edits: trying to get a dispersion plot:&#xD;&#xA;&#xD;&#xA;The same error occurs when trying to estimate dispersion:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- estimateSizeFactors(dds)&#xD;&#xA;    &gt; dds &lt;- estimateDispersions(dds)&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    -- note: fitType='parametric', but the dispersion trend was not well captured by the&#xD;&#xA;       function: y = a/x + b, and a local regression fit was automatically substituted.&#xD;&#xA;       specify fitType='local' or 'mean' to avoid this message next time.&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;&#xD;&#xA;This seems to prevent the generation of dispersion plots:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; plotDispEsts(dds)&#xD;&#xA;    Error in min(py[py &gt; 0], na.rm = TRUE) : &#xD;&#xA;      invalid 'type' (list) of argument&#xD;&#xA;    In addition: Warning message:&#xD;&#xA;    In structure(x, class = unique(c(&quot;AsIs&quot;, oldClass(x)))) :&#xD;&#xA;      Calling 'structure(NULL, *)' is deprecated, as NULL cannot have attributes.&#xD;&#xA;      Consider 'structure(list(), *)' instead.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Using fitType=&quot;local&quot; also fails:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T, fitType=&quot;local&quot;)&#xD;&#xA;    using pre-existing size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2848" PostHistoryTypeId="5" PostId="909" RevisionGUID="0fc98b34-6ebe-4b63-9973-88156839a74a" CreationDate="2017-06-26T09:42:47.240" UserId="292" Comment="added 439 characters in body" Text="Note: I also posted this issue (with less context) in the bioconductor support site: &lt;https://support.bioconductor.org/p/97424/&gt;&#xD;&#xA;&#xD;&#xA;I'm working on a snakemake workflow that identifies various small RNA species in _C. elegans_ small RNA-seq libraries. Some are endogenous siRNAs supposedly generated from RNA templates (through RNA-dependant RNA polymerases (RdRP)) that can be variously classified (protein-coding genes, transposons, and other repeat types).&#xD;&#xA;&#xD;&#xA;I count such small RNA reads in a set of libraries and try to identify differentially producing sources. For this, I use DESeq2 (that I run [using rpy2](https://stackoverflow.com/q/42157335/1878788) from within snakemake).&#xD;&#xA;&#xD;&#xA;I'm not sure DESeq2 is always appropriate for this kind of data, but so far, the analyses would at least complete.&#xD;&#xA;However, I recently added new potential types of small RNA source (simple repeats and satellites), and these happen to have low counts. I'm not 100% sure, but I suspect these low counts are the reason for failures during DESeq2 analyses (for debugging purposes, I ran this manually in R):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T)&#xD;&#xA;    estimating size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    -- note: fitType='parametric', but the dispersion trend was not well captured by the&#xD;&#xA;       function: y = a/x + b, and a local regression fit was automatically substituted.&#xD;&#xA;       specify fitType='local' or 'mean' to avoid this message next time.&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;    &gt; warnings()&#xD;&#xA;    Warning messages:&#xD;&#xA;    1: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    2: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    3: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    4: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    5: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    6: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    7: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    8: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    9: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    10: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    11: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    12: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The count matrix indeed contains a high proportion of zeroes, the rest are mostly ones, but there are some sources that seem a little more significantly producing small RNAs:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; mean(counts_data == 0)&#xD;&#xA;    [1] 0.7488889&#xD;&#xA;    &gt; mean(counts_data == 1)&#xD;&#xA;    [1] 0.1507407&#xD;&#xA;    &gt; max(counts_data)&#xD;&#xA;    [1] 34&#xD;&#xA;&#xD;&#xA;**How would you recommend handling such kind of data?**&#xD;&#xA;&#xD;&#xA;Should I for instance discard rows without enough counts?&#xD;&#xA;If so, what would be a reasonable threshold?&#xD;&#xA;&#xD;&#xA;### Edits: trying to get a dispersion plot:&#xD;&#xA;&#xD;&#xA;The same error occurs when trying to estimate dispersion:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- estimateSizeFactors(dds)&#xD;&#xA;    &gt; dds &lt;- estimateDispersions(dds)&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    -- note: fitType='parametric', but the dispersion trend was not well captured by the&#xD;&#xA;       function: y = a/x + b, and a local regression fit was automatically substituted.&#xD;&#xA;       specify fitType='local' or 'mean' to avoid this message next time.&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;&#xD;&#xA;This seems to prevent the generation of dispersion plots:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; plotDispEsts(dds)&#xD;&#xA;    Error in min(py[py &gt; 0], na.rm = TRUE) : &#xD;&#xA;      invalid 'type' (list) of argument&#xD;&#xA;    In addition: Warning message:&#xD;&#xA;    In structure(x, class = unique(c(&quot;AsIs&quot;, oldClass(x)))) :&#xD;&#xA;      Calling 'structure(NULL, *)' is deprecated, as NULL cannot have attributes.&#xD;&#xA;      Consider 'structure(list(), *)' instead.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Using `fitType=&quot;local&quot;` also fails:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T, fitType=&quot;local&quot;)&#xD;&#xA;    using pre-existing size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;&#xD;&#xA;But `fitType=&quot;mean&quot;` works:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T, fitType=&quot;mean&quot;)&#xD;&#xA;    estimating size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    final dispersion estimates&#xD;&#xA;    fitting model and testing&#xD;&#xA;&#xD;&#xA;**What do these options mean, and are there good reasons why they would have to be changed in order to accomodate for low counts?**" />
  <row Id="2849" PostHistoryTypeId="2" PostId="910" RevisionGUID="bcec6511-53fd-4e75-83a1-88347762e39f" CreationDate="2017-06-26T09:44:14.527" UserId="599" Text="Remove low-count features in advance. This is standard for most tools including [DESeq2][1] and [edgeR][2] (see section 2.6). &#xD;&#xA;&#xD;&#xA;This will keep you from testing a lot of features that cannot be differentially expressed (using NB model you need at &gt;3 reads - below that the uncertainty is to high to ever be DE) and you will make the false discovery correction unnecessarily hard.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#pre-filtering&#xD;&#xA;  [2]: http://bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf" />
  <row Id="2850" PostHistoryTypeId="5" PostId="909" RevisionGUID="5f1c98ca-9113-4066-9521-bea4ced9b179" CreationDate="2017-06-26T09:48:42.643" UserId="292" Comment="Added dispersion plot" Text="Note: I also posted this issue (with less context) in the bioconductor support site: &lt;https://support.bioconductor.org/p/97424/&gt;&#xD;&#xA;&#xD;&#xA;I'm working on a snakemake workflow that identifies various small RNA species in _C. elegans_ small RNA-seq libraries. Some are endogenous siRNAs supposedly generated from RNA templates (through RNA-dependant RNA polymerases (RdRP)) that can be variously classified (protein-coding genes, transposons, and other repeat types).&#xD;&#xA;&#xD;&#xA;I count such small RNA reads in a set of libraries and try to identify differentially producing sources. For this, I use DESeq2 (that I run [using rpy2](https://stackoverflow.com/q/42157335/1878788) from within snakemake).&#xD;&#xA;&#xD;&#xA;I'm not sure DESeq2 is always appropriate for this kind of data, but so far, the analyses would at least complete.&#xD;&#xA;However, I recently added new potential types of small RNA source (simple repeats and satellites), and these happen to have low counts. I'm not 100% sure, but I suspect these low counts are the reason for failures during DESeq2 analyses (for debugging purposes, I ran this manually in R):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T)&#xD;&#xA;    estimating size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    -- note: fitType='parametric', but the dispersion trend was not well captured by the&#xD;&#xA;       function: y = a/x + b, and a local regression fit was automatically substituted.&#xD;&#xA;       specify fitType='local' or 'mean' to avoid this message next time.&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;    &gt; warnings()&#xD;&#xA;    Warning messages:&#xD;&#xA;    1: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    2: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    3: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    4: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    5: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    6: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    7: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    8: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    9: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    10: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    11: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;    12: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xD;&#xA;      procv: no points with non-zero weight&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The count matrix indeed contains a high proportion of zeroes, the rest are mostly ones, but there are some sources that seem a little more significantly producing small RNAs:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; mean(counts_data == 0)&#xD;&#xA;    [1] 0.7488889&#xD;&#xA;    &gt; mean(counts_data == 1)&#xD;&#xA;    [1] 0.1507407&#xD;&#xA;    &gt; max(counts_data)&#xD;&#xA;    [1] 34&#xD;&#xA;&#xD;&#xA;**How would you recommend handling such kind of data?**&#xD;&#xA;&#xD;&#xA;Should I for instance discard rows without enough counts?&#xD;&#xA;If so, what would be a reasonable threshold?&#xD;&#xA;&#xD;&#xA;### Edits: trying to get a dispersion plot:&#xD;&#xA;&#xD;&#xA;The same error occurs when trying to estimate dispersion:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- estimateSizeFactors(dds)&#xD;&#xA;    &gt; dds &lt;- estimateDispersions(dds)&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    -- note: fitType='parametric', but the dispersion trend was not well captured by the&#xD;&#xA;       function: y = a/x + b, and a local regression fit was automatically substituted.&#xD;&#xA;       specify fitType='local' or 'mean' to avoid this message next time.&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;&#xD;&#xA;This seems to prevent the generation of dispersion plots:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; plotDispEsts(dds)&#xD;&#xA;    Error in min(py[py &gt; 0], na.rm = TRUE) : &#xD;&#xA;      invalid 'type' (list) of argument&#xD;&#xA;    In addition: Warning message:&#xD;&#xA;    In structure(x, class = unique(c(&quot;AsIs&quot;, oldClass(x)))) :&#xD;&#xA;      Calling 'structure(NULL, *)' is deprecated, as NULL cannot have attributes.&#xD;&#xA;      Consider 'structure(list(), *)' instead.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Using `fitType=&quot;local&quot;` also fails:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T, fitType=&quot;local&quot;)&#xD;&#xA;    using pre-existing size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xD;&#xA;      newsplit: out of vertex space&#xD;&#xA;    In addition: There were 12 warnings (use warnings() to see them)&#xD;&#xA;&#xD;&#xA;But `fitType=&quot;mean&quot;` works:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeq(dds, betaPrior=T, fitType=&quot;mean&quot;)&#xD;&#xA;    estimating size factors&#xD;&#xA;    estimating dispersions&#xD;&#xA;    gene-wise dispersion estimates&#xD;&#xA;    mean-dispersion relationship&#xD;&#xA;    final dispersion estimates&#xD;&#xA;    fitting model and testing&#xD;&#xA;&#xD;&#xA;The dispersion plot then looks as follows:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;**What do these `fitType` options mean, and are there good reasons why they would have to be changed in order to accomodate for low counts?**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/647fF.png" />
  <row Id="2851" PostHistoryTypeId="2" PostId="911" RevisionGUID="623fbd57-7480-456e-94d6-6a9bc8c5637b" CreationDate="2017-06-26T09:51:54.100" UserId="681" Text="I also wrote a package to create various plots from Oxford Nanopore sequencing data and alignments: [NanoPlot][1]. It can be installed through pip (see also [the README on Github][1]). In addition to multiple plots also a limited NanoStats output is created (see also [NanoStat][2]). Data can be presented using:&#xD;&#xA;&#xD;&#xA; - A fastq file (optionally compressed)&#xD;&#xA; - A bam file&#xD;&#xA; - The sequencing_summary.txt file generated by albacore&#xD;&#xA;&#xD;&#xA;Using optional flags you can:&#xD;&#xA;&#xD;&#xA; - Log transform the read lengths&#xD;&#xA; - Use aligned reads rather than sequenced reads&#xD;&#xA; - Downsample the reads&#xD;&#xA; - Set a maximum read length&#xD;&#xA;&#xD;&#xA;I've added an example below, plotting log transformed read length vs average read quality (using a kernel density estimate). More examples can be found in the [gallery on my blog][3].&#xD;&#xA;&#xD;&#xA;I welcome all feedback and suggestions!&#xD;&#xA;&#xD;&#xA;[![enter image description here][4]][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/wdecoster/NanoPlot&#xD;&#xA;  [2]: https://github.com/wdecoster/nanostat&#xD;&#xA;  [3]: https://gigabaseorgigabyte.wordpress.com/2017/06/01/example-gallery-of-nanoplot/&#xD;&#xA;  [4]: https://i.stack.imgur.com/ZwsHX.png" />
  <row Id="2852" PostHistoryTypeId="5" PostId="908" RevisionGUID="0e3d82f2-e52c-4c12-8f87-abd4f4e64210" CreationDate="2017-06-26T10:35:47.607" UserId="57" Comment="fixed the samtools view to show header only (-H) instead of whole files with header (-h)" Text="I have three unsorted `.sam` files I would like to merge, so I can call variants and heterozygosity estimates. For both these estimates I would like to have separate estimation of error profiles of different libraries, therefore I should specify a read groups.&#xD;&#xA;&#xD;&#xA;How can I merge multiple sam files? Preferably avoiding java (Picard tools).&#xD;&#xA;&#xD;&#xA;I figured out, that I can sort individual files using `samtools sort`, then used `samtools merge -r merged.bam s1.sort.sam s2.sort.sam s3.sort.sam` on the sorted files. However, the read group didn't make it to the header (and the variant caller I use is complaining about it), also the read group is stupidly the file name.&#xD;&#xA;&#xD;&#xA;I tried to define meaningful readgroup names using procedure described on [BioStars][1], but I found that this will just change the header, it does not adjust the names of read groups defined by `samtools merge` (the file names).&#xD;&#xA;&#xD;&#xA;I think, given this related thread on [SeqAnswers][2], that If I will define the correct header with read groups corresponding to merged file names it could eventually work. Something like&#xD;&#xA;&#xD;&#xA;    samtools -rh rg.txt merged.bam s1.sort.sam s2.sort.sam s3.sort.sam&#xD;&#xA;&#xD;&#xA;where `rg.txt` is file with&#xD;&#xA;&#xD;&#xA;    @RG s1.sort&#xD;&#xA;    @RG s2.sort&#xD;&#xA;    @RG s3.sort&#xD;&#xA;    ...&#xD;&#xA;    output of samtools view -H s1.sort&#xD;&#xA;&#xD;&#xA;should do the job, but I do not like on this approach for couple of aspects :&#xD;&#xA;&#xD;&#xA;  - readgroups are not really a standard readgroup names (Sample/Library format)&#xD;&#xA;  - it is annoying that I have to generate the header file I will have to feed to the merge process.&#xD;&#xA;  - sam -&gt; sorted.sam -&gt; merged.bam requires ridiculous amount of space&#xD;&#xA;&#xD;&#xA;Is there an elegant solution to merging of multiple `sam` files?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/47487/&#xD;&#xA;  [2]: http://seqanswers.com/forums/showthread.php?t=4180" />
  <row Id="2853" PostHistoryTypeId="5" PostId="910" RevisionGUID="7713793e-6ac8-422b-9751-62f9b1c02414" CreationDate="2017-06-26T10:48:12.570" UserId="73" Comment="slight copy editing for grammar to clarify answer" Text="Remove low-count features in advance. This is standard for most tools including [DESeq2][1] and [edgeR][2] (see section 2.6). &#xD;&#xA;&#xD;&#xA;This will keep you from testing a lot of features that cannot be differentially expressed (using NB model you need at least four reads - below that the uncertainty is too high to ever be DE) and you will make the false discovery correction unnecessarily hard.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#pre-filtering&#xD;&#xA;  [2]: http://bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf" />
  <row Id="2854" PostHistoryTypeId="2" PostId="912" RevisionGUID="98a0af3a-d2e4-4dc0-8b3c-161782a4ead7" CreationDate="2017-06-26T10:55:41.433" UserId="77" Text="I don't think that the issue is the low counts, but rather the number of features without any real variance (the black dots at the bottom).&#xD;&#xA;&#xD;&#xA;# So what the heck is the dispersion plot and why does one need to fit it anyway?&#xD;&#xA;&#xD;&#xA;In a typical RNAseq experiment, one measures many thousands of genes with only a few replicates per biological group. This then leads to issues when performing testing, since your variance measurement for any given gene will be larger or smaller than reality, simply due to the lack of a large number of replicates. Consequently, at least since the `limma` package came out, most RNAseq packages have tried to fit the mean-dispersion with some sort of line and then used that as a background distribution toward which the variance assigned to each gene should be &quot;shrunken&quot; (i.e., moved toward). Getting this at least approximately correct is important, since if you have few replicates and the final &quot;shrunken variance&quot; is significantly wrong, then your p-values will also be wrong.&#xD;&#xA;&#xD;&#xA;# What are the various fitType options and what are they doing?&#xD;&#xA;&#xD;&#xA;The three options that one can use to fit this relationship are `parametric`, `local`, and `mean`. `parametric` is the default and basically results in fitting to a bent line. This usually works quite well, but you can see that you have two clouds of data, one shaped like a bent line (at the top) and another (the black dots at the bottom) with no variance. That's causing this fit type to fail. Likewise, `local` is doing a local fit and failing for the same reason. `mean` is just taking the mean (average) of all of the estimates. This is fool-proof, but you're then regressing to something that really doesn't fit your data. `mean` is really a last choice if nothing else works.&#xD;&#xA;&#xD;&#xA;For your particular dataset, my main concerns would be figuring out (1) why the maximum mean counts are around 20 (that's really low) and (2) why so many of these features have incredibly low variance." />
  <row Id="2855" PostHistoryTypeId="5" PostId="908" RevisionGUID="f83cdc88-f078-47ab-a986-7ef721924da8" CreationDate="2017-06-26T13:24:14.393" UserId="57" Comment="added more to the story - my proposed non-elegant solution in fact does not work." Text="I have three sequencing libraries mapped to reference using `bwa` stored in unsorted `.sam` files that I would like to merge. I want to call variants and heterozygosity estimates using [atlas][1]. Atlas requires one input mapping file with defined read groups because it estimates the error profiles of different libraries separately.&#xD;&#xA;&#xD;&#xA;How can I merge multiple sam files? Preferably avoiding java (Picard tools).&#xD;&#xA;&#xD;&#xA;I figured out, that I can sort individual files using `samtools sort`, then used `samtools merge -r merged.bam s1.sort.sam s2.sort.sam s3.sort.sam` on the sorted files. However, the read group didn't make it to the header (and the variant caller I use is complaining about it), also the read group is stupidly the file name.&#xD;&#xA;&#xD;&#xA;I tried to define meaningful readgroup names using procedure described on [BioStars][2], but I found that this will just change the header, it does not adjust the names of read groups defined by `samtools merge` (the file names).&#xD;&#xA;&#xD;&#xA;Following this related thread on [SeqAnswers][3], I tried to define the correct header with read groups corresponding to merged file names:&#xD;&#xA;&#xD;&#xA;    samtools -rh rg.txt merged.bam s1.sort.sam s2.sort.sam s3.sort.sam&#xD;&#xA;&#xD;&#xA;where `rg.txt` is a file with header&#xD;&#xA;&#xD;&#xA;    @RG s1.sort&#xD;&#xA;    @RG s2.sort&#xD;&#xA;    @RG s3.sort&#xD;&#xA;    ...&#xD;&#xA;    output of samtools view -H s1.sort&#xD;&#xA;&#xD;&#xA;However, the header still had not the read group, I guess because sam header accepts only tagged items to be specified (something like `@RG    XY:s1.sort`). So, I looked into the merged `bam` and I found that the tag of RG is `Z:`. So I tried to just rename header of the merged file using `samtools reheader`, but then samtools complain about the fact that a tag needs to be of length 2:&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;Malformed key:value pair at line 123: &quot;@RG    Z:s1.sort&quot;&#xD;&#xA;Segmentation fault (core dumped)&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;I have opened an [issue][4] to report this strange incompatibility of read groups generated by `samtools merge` with `samtools reheader`.&#xD;&#xA;&#xD;&#xA;I would like solution to :&#xD;&#xA;&#xD;&#xA;  - create a bit more standardized read group names (`SM:Sample\tLB:library` format)&#xD;&#xA;  - avoid pointless writing to disk like in sam -&gt; sorted.sam -&gt; merged.bam case (can be probably achieved through &quot;pipes and tees&quot;, thanks @bli)&#xD;&#xA;&#xD;&#xA;I also know that I can specify RG to bwa, so the sam files will have read groups defined in the first place. But I do not like the idea of remapping three libraries just to create correct formatting of read groups.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bitbucket.org/phaentu/atlas/wiki/Home&#xD;&#xA;  [2]: https://www.biostars.org/p/47487/&#xD;&#xA;  [3]: http://seqanswers.com/forums/showthread.php?t=4180&#xD;&#xA;  [4]: https://github.com/samtools/samtools/issues/698" />
  <row Id="2856" PostHistoryTypeId="5" PostId="908" RevisionGUID="023b1b3b-f6a7-40db-a107-eadce3abd288" CreationDate="2017-06-26T13:34:29.710" UserId="57" Comment="more sequencing details" Text="I have three sequencing libraries of single individual mapped to a reference using `bwa-mem`. I would like to merge the three unsorted `.sam` files I have so, I can call variants and heterozygosity estimates using [atlas][1]. Atlas requires one input mapping file (`bam`) with defined read groups because it estimates the error profiles of different libraries separately.&#xD;&#xA;&#xD;&#xA;How can I merge multiple sam files? Preferably avoiding java (Picard tools).&#xD;&#xA;&#xD;&#xA;I figured out, that I can sort individual files using `samtools sort`, then used `samtools merge -r merged.bam s1.sort.sam s2.sort.sam s3.sort.sam` on the sorted files. However, the read group didn't make it to the header (and the variant caller I use is complaining about it), also the read group is stupidly the file name.&#xD;&#xA;&#xD;&#xA;I tried to define meaningful readgroup names using procedure described on [BioStars][2], but I found that this will just change the header, it does not adjust the names of read groups defined by `samtools merge` (the file names).&#xD;&#xA;&#xD;&#xA;Following this related thread on [SeqAnswers][3], I tried to define the correct header with read groups corresponding to merged file names:&#xD;&#xA;&#xD;&#xA;    samtools -rh rg.txt merged.bam s1.sort.sam s2.sort.sam s3.sort.sam&#xD;&#xA;&#xD;&#xA;where `rg.txt` is a file with header&#xD;&#xA;&#xD;&#xA;    @RG s1.sort&#xD;&#xA;    @RG s2.sort&#xD;&#xA;    @RG s3.sort&#xD;&#xA;    ...&#xD;&#xA;    output of samtools view -H s1.sort&#xD;&#xA;&#xD;&#xA;However, the header still had not the read group, I guess because sam header accepts only tagged items to be specified (something like `@RG    XY:s1.sort`). So, I looked into the merged `bam` and I found that the tag of RG is `Z:`. So I tried to just rename header of the merged file using `samtools reheader`, but then samtools complain about the fact that a tag needs to be of length 2:&#xD;&#xA;&#xD;&#xA;    Malformed key:value pair at line 123: &quot;@RG    Z:s1.sort&quot;&#xD;&#xA;    Segmentation fault (core dumped)&#xD;&#xA;&#xD;&#xA;I have opened an [issue][4] to report this strange incompatibility of read groups generated by `samtools merge` with `samtools reheader`.&#xD;&#xA;&#xD;&#xA;I would like solution to :&#xD;&#xA;&#xD;&#xA;  - create a bit more standardized read group names (`SM:Sample\tLB:library` format)&#xD;&#xA;  - avoid pointless writing to disk like in sam -&gt; sorted.sam -&gt; merged.bam case (can be probably achieved through &quot;pipes and tees&quot;, thanks @bli)&#xD;&#xA;&#xD;&#xA;I also know that I can specify RG to bwa, so the sam files will have read groups defined in the first place. But I do not like the idea of remapping three libraries just to create correct formatting of read groups.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bitbucket.org/phaentu/atlas/wiki/Home&#xD;&#xA;  [2]: https://www.biostars.org/p/47487/&#xD;&#xA;  [3]: http://seqanswers.com/forums/showthread.php?t=4180&#xD;&#xA;  [4]: https://github.com/samtools/samtools/issues/698" />
  <row Id="2857" PostHistoryTypeId="5" PostId="908" RevisionGUID="c6ef4a61-8716-431d-adcf-b38c9e0bcbb8" CreationDate="2017-06-26T14:37:37.387" UserId="57" Comment="added version of samtools" Text="I have three sequencing libraries of single individual mapped to a reference using `bwa-mem`. I would like to merge the three unsorted `.sam` files I have so, I can call variants and heterozygosity estimates using [atlas][1]. Atlas requires one input mapping file (`bam`) with defined read groups because it estimates the error profiles of different libraries separately.&#xD;&#xA;&#xD;&#xA;How can I merge multiple sam files? Preferably avoiding java (Picard tools).&#xD;&#xA;&#xD;&#xA;I tried to figure out a solution using `samtools 1.3`. I sorted individual files using `samtools sort`, then I used `samtools merge -r merged.bam s1.sort.sam s2.sort.sam s3.sort.sam` to merge the sorted files. However, the read group didn't make it to the header (and the variant caller I use is complaining about it), also the read group is stupidly the file name.&#xD;&#xA;&#xD;&#xA;I tried to define meaningful readgroup names using procedure described on [BioStars][2], but I found that this will just change the header, it does not adjust the names of read groups defined by `samtools merge` (the file names).&#xD;&#xA;&#xD;&#xA;Following this related thread on [SeqAnswers][3], I tried to define the correct header with read groups corresponding to merged file names:&#xD;&#xA;&#xD;&#xA;    samtools -rh rg.txt merged.bam s1.sort.sam s2.sort.sam s3.sort.sam&#xD;&#xA;&#xD;&#xA;where `rg.txt` is a file with header&#xD;&#xA;&#xD;&#xA;    @RG s1.sort&#xD;&#xA;    @RG s2.sort&#xD;&#xA;    @RG s3.sort&#xD;&#xA;    ...&#xD;&#xA;    output of samtools view -H s1.sort&#xD;&#xA;&#xD;&#xA;However, the header still had not the read group, I guess because sam header accepts only tagged items to be specified (something like `@RG    XY:s1.sort`). So, I looked into the merged `bam` and I found that the tag of RG is `Z:`. So I tried to just rename header of the merged file using `samtools reheader`, but then samtools complain about the fact that a tag needs to be of length 2:&#xD;&#xA;&#xD;&#xA;    Malformed key:value pair at line 123: &quot;@RG    Z:s1.sort&quot;&#xD;&#xA;    Segmentation fault (core dumped)&#xD;&#xA;&#xD;&#xA;I have opened an [issue][4] to report this strange incompatibility of read groups generated by `samtools merge` with `samtools reheader`.&#xD;&#xA;&#xD;&#xA;I would like solution to :&#xD;&#xA;&#xD;&#xA;  - create a bit more standardized read group names (`SM:Sample\tLB:library` format)&#xD;&#xA;  - avoid pointless writing to disk like in sam -&gt; sorted.sam -&gt; merged.bam case (can be probably achieved through &quot;pipes and tees&quot;, thanks @bli)&#xD;&#xA;&#xD;&#xA;I also know that I can specify RG to bwa, so the sam files will have read groups defined in the first place. But I do not like the idea of remapping three libraries just to create correct formatting of read groups.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bitbucket.org/phaentu/atlas/wiki/Home&#xD;&#xA;  [2]: https://www.biostars.org/p/47487/&#xD;&#xA;  [3]: http://seqanswers.com/forums/showthread.php?t=4180&#xD;&#xA;  [4]: https://github.com/samtools/samtools/issues/698" />
  <row Id="2858" PostHistoryTypeId="5" PostId="897" RevisionGUID="2e16f829-149e-404c-a2d2-09096f7b5be7" CreationDate="2017-06-26T15:54:47.770" UserId="776" Comment="added 9 characters in body" Text="Via Gencode and BEDOPS `convert2bed`:&#xD;&#xA;  &#xD;&#xA;    $ wget -qO- ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_21/gencode.v21.annotation.gff3.gz \&#xD;&#xA;        | gunzip --stdout - \&#xD;&#xA;        | awk '$3 == &quot;gene&quot;' \&#xD;&#xA;        | convert2bed -i gff - \&#xD;&#xA;        &gt; genes.bed&#xD;&#xA;&#xD;&#xA;You can modify the `awk` statement to get exons, by replacing `gene` with `exon`.&#xD;&#xA;&#xD;&#xA;BEDOPS: https://github.com/bedops/bedops&#xD;&#xA;&#xD;&#xA;This is based off an answer I wrote on Biostars, which includes a Perl script for generating a BED file of introns from gene and exon annotations: https://www.biostars.org/p/124515/#124522" />
  <row Id="2859" PostHistoryTypeId="2" PostId="913" RevisionGUID="bb24e5c3-647c-4804-9477-651f52a430e5" CreationDate="2017-06-26T16:50:19.213" UserId="57" Text="I found a solution that satisfy several of my conditions, basically I just have to assign read groups to individual mapping files and then merge them&#xD;&#xA;&#xD;&#xA;    function sort_and_assign_RG {&#xD;&#xA;        # $1 input file&#xD;&#xA;        # $2 read group id&#xD;&#xA;        # $3 library&#xD;&#xA;    &#xD;&#xA;        OFILE=$(basename $1 .sam).sort.RG.sam&#xD;&#xA;        HEADER=$(basename $1 .sam).header.sam&#xD;&#xA;    &#xD;&#xA;        # since I take header from unsorted file, so I need to add this information to header as well as the readgroup&#xD;&#xA;        echo -e &quot;@HD\tVN:1.3\tSO:coordinate&quot; &gt; $HEADER&#xD;&#xA;        # add read group to header&#xD;&#xA;        echo -e &quot;@RG\tID:&quot;$2&quot;\tLB:&quot;&quot;$3&quot; &gt;&gt; $HEADER&#xD;&#xA;        # and the rest of the header&#xD;&#xA;        samtools view -H $1 &gt;&gt; $HEADER&#xD;&#xA;    &#xD;&#xA;        # now sort the input sam, remove header, &#xD;&#xA;        # attach to each alignment read group ID (RGID) &#xD;&#xA;and cat header and alignment together&#xD;&#xA;        samtools sort -- $1 | samtools view - | \&#xD;&#xA;          awk -v RGID=&quot;$2&quot; '{ printf &quot;%s\tRG:Z:%s\n&quot;, $0, RGID; }' | cat $HEADER - &gt; $OFILE&#xD;&#xA;    &#xD;&#xA;        # remove temp header&#xD;&#xA;        rm $HEADER&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;It would be nice to avoid creation of three bam files on the way, but at least I get standardized read groups in merged bam tile.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2860" PostHistoryTypeId="5" PostId="913" RevisionGUID="81e0878c-a325-4cdc-9ebb-49b819438823" CreationDate="2017-06-26T17:00:59.280" UserId="57" Comment="fixed code" Text="I found a solution that satisfy several of my conditions, basically I just have to assign read groups to individual mapping files and then merge them.&#xD;&#xA;&#xD;&#xA;    function sort_and_assign_RG {&#xD;&#xA;        # $1 input file&#xD;&#xA;        # $2 read group id&#xD;&#xA;        # $3 library&#xD;&#xA;    &#xD;&#xA;        OFILE=$(basename $1 .sam).sort.RG.sam&#xD;&#xA;        HEADER=$(basename $1 .sam).header.sam&#xD;&#xA;    &#xD;&#xA;        # since I take header from unsorted file, so I need to add this information to header as well as the readgroup&#xD;&#xA;        echo -e &quot;@HD\tVN:1.3\tSO:coordinate&quot; &gt; $HEADER&#xD;&#xA;        # add read group to header&#xD;&#xA;        echo -e &quot;@RG\tID:&quot;$2&quot;\tLB:&quot;&quot;$3&quot; &gt;&gt; $HEADER&#xD;&#xA;        # and the rest of the header&#xD;&#xA;        samtools view -H $1 &gt;&gt; $HEADER&#xD;&#xA;    &#xD;&#xA;        # now sort the input sam, remove header, &#xD;&#xA;        # attach to each alignment read group ID (RGID) &#xD;&#xA;        # and cat header and alignment together&#xD;&#xA;        samtools sort -- $1 | samtools view - | \&#xD;&#xA;          awk -v RGID=&quot;$2&quot; '{ printf &quot;%s\tRG:Z:%s\n&quot;, $0, RGID; }' | \&#xD;&#xA;          cat $HEADER - &gt; $OFILE&#xD;&#xA;    &#xD;&#xA;        # remove temp header&#xD;&#xA;        rm $HEADER&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    sort_and_assign_RG s1.sam s1 is180&#xD;&#xA;    sort_and_assign_RG s2.sam s2 is350&#xD;&#xA;    sort_and_assign_RG s3.sam s3 is550&#xD;&#xA;&#xD;&#xA;    samtools merge merged.bam s{1,2,3}.sort.RG.sam&#xD;&#xA;&#xD;&#xA;It would be nice to avoid creation of three `sam` files on the way, but at least I get standardized read groups in a merged bam tile.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2861" PostHistoryTypeId="5" PostId="913" RevisionGUID="eb88bd93-17b3-40b1-bb79-f71cea5aaa35" CreationDate="2017-06-26T17:07:22.327" UserId="57" Comment="fixed code + bit more of explanation" Text="**Not so elegant but working solution**&#xD;&#xA;&#xD;&#xA;I found a solution that satisfy several of my conditions, basically I just have to assign read groups to individual mapping files, which can be just added to sorting process and then merge the sorted sam files with read groups.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-js --&gt;&#xD;&#xA;&#xD;&#xA;    function sort_and_assign_RG {&#xD;&#xA;        # $1 input file&#xD;&#xA;        # $2 read group id&#xD;&#xA;        # $3 library&#xD;&#xA;    &#xD;&#xA;        OFILE=$(basename $1 .sam).sort.RG.sam&#xD;&#xA;        HEADER=$(basename $1 .sam).header.sam&#xD;&#xA;    &#xD;&#xA;        # since I take header from unsorted file, so I need to add this information to header as well as the readgroup&#xD;&#xA;        echo -e &quot;@HD\tVN:1.3\tSO:coordinate&quot; &gt; $HEADER&#xD;&#xA;        # add read group to header&#xD;&#xA;        echo -e &quot;@RG\tID:&quot;$2&quot;\tLB:&quot;&quot;$3&quot; &gt;&gt; $HEADER&#xD;&#xA;        # and the rest of the header&#xD;&#xA;        samtools view -H $1 &gt;&gt; $HEADER&#xD;&#xA;    &#xD;&#xA;        # now sort the input sam, remove header, &#xD;&#xA;        # attach to each alignment read group ID (RGID) &#xD;&#xA;        # and cat header and alignment together&#xD;&#xA;        samtools sort -- $1 | samtools view - | \&#xD;&#xA;          awk -v RGID=&quot;$2&quot; '{ printf &quot;%s\tRG:Z:%s\n&quot;, $0, RGID; }' | \&#xD;&#xA;          cat $HEADER - &gt; $OFILE&#xD;&#xA;    &#xD;&#xA;        # remove temp header&#xD;&#xA;        rm $HEADER&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    sort_and_assign_RG s1.sam s1 is180&#xD;&#xA;    sort_and_assign_RG s2.sam s2 is350&#xD;&#xA;    sort_and_assign_RG s3.sam s3 is550&#xD;&#xA;&#xD;&#xA;    samtools merge merged.bam s{1,2,3}.sort.RG.sam&#xD;&#xA;&#xD;&#xA;It would be nice to avoid creation of three `sam` files on the way, but at least I get standardized read groups in a merged bam tile.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2862" PostHistoryTypeId="5" PostId="894" RevisionGUID="acb10151-7ce5-4ff4-ab23-c5759ecfb5e9" CreationDate="2017-06-26T17:18:28.973" UserId="292" Comment="Simplified according to comments" Text="I'm not sure I'm doing it the best way, but here is an example where I read a compressed gzip fastq file and write the records in block gzip fastq:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO, bgzf&#xD;&#xA;    # Used to convert the fastq stream into a file handle&#xD;&#xA;    from io import StringIO&#xD;&#xA;    from gzip import open as gzopen&#xD;&#xA;&#xD;&#xA;    records = SeqIO.parse(&#xD;&#xA;        # There is actually simpler (thanks @peterjc)&#xD;&#xA;        # StringIO(gzopen(&quot;random_10.fastq.gz&quot;).read().decode(&quot;utf-8&quot;)),&#xD;&#xA;        gzopen(&quot;random_10.fastq.gz&quot;, &quot;rt&quot;),&#xD;&#xA;        format=&quot;fastq&quot;)&#xD;&#xA;&#xD;&#xA;    with bgzf.BgzfWriter(&quot;test.fastq.bgz&quot;, &quot;wb&quot;) as outgz:&#xD;&#xA;        SeqIO.write(sequences=records, handle=outgz, format=&quot;fastq&quot;)&#xD;&#xA;" />
  <row Id="2863" PostHistoryTypeId="5" PostId="913" RevisionGUID="149697ac-1c41-4ae1-880e-99b9b56bdb5c" CreationDate="2017-06-26T17:23:21.913" UserId="57" Comment="fixed code + bit more of explanation + bit bit more explanation" Text="**Not so elegant but working solution**&#xD;&#xA;&#xD;&#xA;I found a solution that satisfy several of my conditions, basically I just have to assign read groups to individual mapping files, which can be just added to sorting process and then merge the sorted sam files with read groups.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-js --&gt;&#xD;&#xA;&#xD;&#xA;    function sort_and_assign_RG {&#xD;&#xA;        # $1 input file&#xD;&#xA;        # $2 read group id&#xD;&#xA;        # $3 library&#xD;&#xA;    &#xD;&#xA;        OFILE=$(basename $1 .sam).sort.RG.sam&#xD;&#xA;        HEADER=$(basename $1 .sam).header.sam&#xD;&#xA;    &#xD;&#xA;        # since I take header from unsorted file, so I need to add this information to header as well as the readgroup&#xD;&#xA;        echo -e &quot;@HD\tVN:1.3\tSO:coordinate&quot; &gt; $HEADER&#xD;&#xA;        # add read group to header&#xD;&#xA;        echo -e &quot;@RG\tID:&quot;$2&quot;\tLB:&quot;&quot;$3&quot; &gt;&gt; $HEADER&#xD;&#xA;        # and the rest of the header&#xD;&#xA;        samtools view -H $1 &gt;&gt; $HEADER&#xD;&#xA;    &#xD;&#xA;        # now sort the input sam, remove header, &#xD;&#xA;        # attach to each alignment read group ID (RGID) &#xD;&#xA;        # and cat header and alignment together&#xD;&#xA;        samtools sort -- $1 | samtools view - | \&#xD;&#xA;          awk -v RGID=&quot;$2&quot; '{ printf &quot;%s\tRG:Z:%s\n&quot;, $0, RGID; }' | \&#xD;&#xA;          cat $HEADER - &gt; $OFILE&#xD;&#xA;    &#xD;&#xA;        # remove temp header&#xD;&#xA;        rm $HEADER&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    sort_and_assign_RG s1.sam s1 is180&#xD;&#xA;    sort_and_assign_RG s2.sam s2 is350&#xD;&#xA;    sort_and_assign_RG s3.sam s3 is550&#xD;&#xA;&#xD;&#xA;    samtools merge merged.bam s{1,2,3}.sort.RG.sam&#xD;&#xA;&#xD;&#xA;It would be nice to avoid creation of three `sam` files on the way, but at least I get standardized read groups in a merged bam tile.&#xD;&#xA;&#xD;&#xA;*--- edit ---*&#xD;&#xA;&#xD;&#xA;I also found where my confusion is coming from. The read group in header is in format &#xD;&#xA;&#xD;&#xA;    @RG    ID:foo    LB:lib_foo    ...`&#xD;&#xA;    @RG    ID:bar    LB:lib_bar    ...`&#xD;&#xA;&#xD;&#xA;where `ID` is read group ID (not sample ID as I thought) and other tags are just specifications of the read group. Then individual mapping have read group assigned in different format&#xD;&#xA;&#xD;&#xA;    {aliment1}    RG:Z:foo&#xD;&#xA;    {aliment2}    RG:Z:bar&#xD;&#xA;&#xD;&#xA;where `RG` is just a tag for read group and `Z` is just a mark that says that this tag is just a &quot;printable string&quot;. Therefore I think that another solution would just to merge these sorted files and then just adding three **correct** lines to the header.&#xD;&#xA;&#xD;&#xA;I have to say, `.sam` is a unbelievable intuitive format." />
  <row Id="2864" PostHistoryTypeId="2" PostId="914" RevisionGUID="083aba40-4d5e-44ad-8c24-72898f0754aa" CreationDate="2017-06-26T20:06:27.333" UserId="823" Text="What tools are available to predict, based on the structure of a certain species, which species it might interact with within a cell?&#xD;&#xA;&#xD;&#xA;For example, I am considering the split GFP protein and I am trying to predict if there will be any interactions which might hinder its binding efficiency to the other half of the GFP protein. &#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="2865" PostHistoryTypeId="1" PostId="914" RevisionGUID="083aba40-4d5e-44ad-8c24-72898f0754aa" CreationDate="2017-06-26T20:06:27.333" UserId="823" Text="Tool for predicting interactions in the cell" />
  <row Id="2866" PostHistoryTypeId="3" PostId="914" RevisionGUID="083aba40-4d5e-44ad-8c24-72898f0754aa" CreationDate="2017-06-26T20:06:27.333" UserId="823" Text="&lt;protein-structure&gt;&lt;interactions&gt;&lt;3d-structure&gt;" />
  <row Id="2867" PostHistoryTypeId="2" PostId="915" RevisionGUID="eddad024-839c-42c5-a452-47edb020506d" CreationDate="2017-06-26T20:07:16.807" UserId="35" Text="To add to all the other great answers, I would mention that the question is somewhat misleading. If the reference genome is for a single individual, then it should be diploid. However, it's a reference for all humans. It should really contain billions of copies to fully account for all the diversity. Since that is not realistic, the reference serves as a simple approximation.&#xD;&#xA;&#xD;&#xA;This point was addressed by the recent [Korean genome paper][1]:&#xD;&#xA;&#xD;&#xA;&gt; Human genomes are routinely compared against a universal reference.&#xD;&#xA;&gt; However, this strategy could miss population-specific and personal&#xD;&#xA;&gt; genomic variations, which may be detected more efficiently using an&#xD;&#xA;&gt; ethnically relevant or personal reference. ... Systematic comparison&#xD;&#xA;&gt; of human assemblies shows the importance of assembly quality,&#xD;&#xA;&gt; suggesting the necessity of new technologies to comprehensively map&#xD;&#xA;&gt; ethnic and personal genomic structure variations.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.nature.com/articles/ncomms13637" />
  <row Id="2868" PostHistoryTypeId="5" PostId="914" RevisionGUID="273edfe5-6c93-477b-b802-b4d64ef06275" CreationDate="2017-06-26T20:15:53.860" UserId="823" Comment="deleted 3 characters in body" Text="What tools are available to predict, based on the structure of a certain protein, its interactions within a cell?&#xD;&#xA;&#xD;&#xA;For example, I am considering the split GFP protein and I am trying to predict if there will be any interactions which might hinder its binding efficiency to the other half of the GFP protein. &#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="2869" PostHistoryTypeId="2" PostId="916" RevisionGUID="8b5b6f61-3d02-44c1-ac36-7ba4025e9498" CreationDate="2017-06-27T04:10:47.523" UserId="964" Text="I have used https://github.com/ekg/bamaddrg to add read groups quickly to multiple sam files.  And then you can do a samtools merge of the tagged files." />
  <row Id="2870" PostHistoryTypeId="2" PostId="917" RevisionGUID="cef8fc5f-3d49-41e9-a154-a7e0cb242ea3" CreationDate="2017-06-27T08:06:22.357" UserId="518" Text="For pre-existing reliabe TE libraries it is a bit of a mess, because not everybody deposits the species-specific TE libraries to a database like RepBase. And as far as I know DFAM contains only human resources, or am I wrong?&#xD;&#xA;&#xD;&#xA;As for *de novo* generation of species-specific TE libraries (which should be done for any species not already present in eg. RepBase):&#xD;&#xA;There is no &quot;gold-standard&quot; how to tackle this best.&#xD;&#xA;In principle one has to think about two main parts&#xD;&#xA;-repeat detection&#xD;&#xA;-annotation&#xD;&#xA;&#xD;&#xA;For **repeat detection** I would recommend using a combination of two things (which is necessary, because TE copies might miss in the assemblies as repetitive regions tend to be difficult to assemble and thrown away in the final assembly).&#xD;&#xA;&#xD;&#xA;I) Repeat detection from raw reads (as with e.g. DNApipeTE or tedna or RepeatExplorer). For me, DNAPipeTE worked quite nicely, but everything has pros and cons.&#xD;&#xA;II) Repeat detection from assemblies (as with e.g. REPET or as mentioned before RepeatModeler)&#xD;&#xA;&#xD;&#xA;Then the **annotation** of these repeats is tricky too, because most methods are relying on homology between the *de novo* TEs and the TEs from some (probably distantly) related species. But some programs take also structure into account (like REPCLASS). REPET can do both detection and annotation, but is a pain to get to run. &#xD;&#xA;&#xD;&#xA;I would recommend using some programs to do de novo repeat detection on your species of interest on both the raw reads and assembly, clustering these libraries together (with e.g. uclust and 95% identity) and then run an annotation with homology and structural identification.&#xD;&#xA;&#xD;&#xA;Probably the programs will **not give you complete, full-length TE**s but rather consensus sequences of several copies from TE families.&#xD;&#xA;If you want, you could search all copies of one family, extract them from the contigs plus boundaries and align them manually and curate boundaries manually. Then extend boundaries if not hitting the surrounding (non-alignable) regions or landmarks of TEs like LTRs or TIRs or so. But this is very time consuming if you only want to compare TE abundance between species for example, I would not do this and rather compare the abundance using read coverage (as in [Bast et al. 2016][1]). Depends all on the questions you want to ask.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://doi.org/10.1093/molbev/msv261" />
  <row Id="2871" PostHistoryTypeId="2" PostId="918" RevisionGUID="738a6d47-d13f-4493-bad4-8f5039803f52" CreationDate="2017-06-27T08:17:20.487" UserId="298" Text="You could try one of:&#xD;&#xA;&#xD;&#xA;1. [Struct2Net][1]&#xD;&#xA;&#xD;&#xA; &gt; Given two protein sequences, the structure-based interaction prediction technique threads these two sequences to all the protein complexes in the PDB and then chooses the best potential match. Based on this match, the method generates alignment scores, z-scores, and an interfacial energy for the sequence pair. Logistic regression is then used to evaluate whether a set of scores corresponds to an interaction or not. The algorithm is also extended to find all potential partners given a single protein sequence. Further details about the method are described [here][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;2. [PRED][3]&#xD;&#xA;&#xD;&#xA; &gt; Pred_PPI is a web-based system that serves for predicting PPIs from different organisms. This server is freely available to any researcher wishing to use it for non-commercial purposes. Based on auto covariance (AC) and support vector machine (SVM), this tool is capable of predicting PPIs for any target protein pair only using their primary sequences, and assigning an interaction probability to each SVM prediction as well. So the user can use this tool to predict novel PPIs with high confidence.&#xD;&#xA;&#xD;&#xA;  [1]: http://cb.csail.mit.edu/cb/struct2net/webserver/about.html&#xD;&#xA;  [2]: http://helix-web.stanford.edu/psb06/singh.pdf&#xD;&#xA;  [3]: http://cic.scu.edu.cn/bioinformatics/predict_ppi/default.html" />
  <row Id="2873" PostHistoryTypeId="2" PostId="920" RevisionGUID="987a2f5b-a79c-4d29-9717-a4fbfdd05671" CreationDate="2017-06-27T10:17:45.383" UserId="982" Text="I want to compare two phylogenies and colour the association lines based on some metadata I have. I have been using ape cophyloplot but I have not had any success in getting the lines to colour accurately according to my data. &#xD;&#xA;&#xD;&#xA;[See previous question here][1] But to note in my actual work flow I define the colour scheme using palette to control the colour outcome.&#xD;&#xA;&#xD;&#xA;I want a mean to make a tangle using phylogenies which I can format. Preferably in R. I like to get an [output like this][2]&#xD;&#xA;Thanks &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://stackoverflow.com/questions/44006500/r-coloured-lines-for-tangelgram-package-ape-function-cophyloplot&#xD;&#xA;  [2]: http://www.frontiersin.org/files/Articles/210214/fmicb-07-01581-HTML-r1/image_m/fmicb-07-01581-g006.jpg" />
  <row Id="2874" PostHistoryTypeId="1" PostId="920" RevisionGUID="987a2f5b-a79c-4d29-9717-a4fbfdd05671" CreationDate="2017-06-27T10:17:45.383" UserId="982" Text="Tanglegram - software suggestions" />
  <row Id="2875" PostHistoryTypeId="3" PostId="920" RevisionGUID="987a2f5b-a79c-4d29-9717-a4fbfdd05671" CreationDate="2017-06-27T10:17:45.383" UserId="982" Text="&lt;r&gt;&lt;phylogeny&gt;&lt;phylogenetics&gt;" />
  <row Id="2876" PostHistoryTypeId="5" PostId="920" RevisionGUID="235318da-86ab-4835-bebf-e702b212c1f5" CreationDate="2017-06-27T12:15:53.220" UserId="29" Comment="improve title, phrasing, and inline picture." Text="I want to compare two phylogenies and colour the association lines based on some metadata I have. I have been using ape cophyloplot but I have not had any success in getting the lines to colour accurately according to my data ([see previous question][1]).&#xD;&#xA;&#xD;&#xA;Note that in my actual work flow I define the colour scheme using a palette to control the colour outcome.&#xD;&#xA;&#xD;&#xA;I want a means to make a tangle using phylogenies which I can format. Preferably in R. I like to get an output like this: ![][2]&#xD;&#xA;&#xD;&#xA;  [1]: https://stackoverflow.com/questions/44006500/r-coloured-lines-for-tangelgram-package-ape-function-cophyloplot&#xD;&#xA;  [2]: https://i.stack.imgur.com/HTDfR.jpg" />
  <row Id="2877" PostHistoryTypeId="4" PostId="920" RevisionGUID="235318da-86ab-4835-bebf-e702b212c1f5" CreationDate="2017-06-27T12:15:53.220" UserId="29" Comment="improve title, phrasing, and inline picture." Text="How do I generate a color-coded tanglegram?" />
  <row Id="2878" PostHistoryTypeId="6" PostId="920" RevisionGUID="235318da-86ab-4835-bebf-e702b212c1f5" CreationDate="2017-06-27T12:15:53.220" UserId="29" Comment="improve title, phrasing, and inline picture." Text="&lt;r&gt;&lt;phylogeny&gt;&lt;software-recommendation&gt;&lt;phylogenetics&gt;" />
  <row Id="2879" PostHistoryTypeId="2" PostId="921" RevisionGUID="cceeeb22-d2fb-4bb3-9e87-704be0bb6fc1" CreationDate="2017-06-27T12:30:55.550" UserId="939" Text="I think you can try dendextend, in this [manual][1] there is an example of coloring the branches. I don't think it is exactly like your coloring, but with a little tweaking you might get your colorscheme in there.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html" />
  <row Id="2886" PostHistoryTypeId="5" PostId="921" RevisionGUID="6d4a7d87-7970-4a6c-9b87-60ac351c84cc" CreationDate="2017-06-27T13:29:24.197" UserId="939" Comment="added 135 characters in body" Text="I think you can try dendextend, in this [manual][1] there is an example of coloring the branches. I don't think it is exactly like your coloring, but with a little tweaking you might get your colorscheme in there.&#xD;&#xA;&#xD;&#xA;Edit:&#xD;&#xA;&#xD;&#xA;In the function `tanglegram()`, there is an argument `color_lines`. Try to see if you can fit your color palette in there.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html" />
  <row Id="2887" PostHistoryTypeId="5" PostId="921" RevisionGUID="13c0cc75-ff46-4344-847e-002e19ec5225" CreationDate="2017-06-27T13:35:46.243" UserId="298" Comment="Included relevant details from the link. " Text="I think you can try dendextend, in this [manual][1] there is an example of coloring the branches. I don't think it is exactly like your coloring, but with a little tweaking you might get your colorscheme in there.&#xD;&#xA;&#xD;&#xA;The manual mentions an argument called `color_lines` for the function  `tanglegram()`:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    # The `which` parameter allows us to pick the elements in the list to compare&#xD;&#xA;    iris_dendlist %&gt;% dendlist(which = c(1,4)) %&gt;% ladderize %&gt;% &#xD;&#xA;       # untangle(method = &quot;step1side&quot;, k_seq = 3:20) %&gt;%&#xD;&#xA;       set(&quot;rank_branches&quot;) %&gt;%&#xD;&#xA;       tanglegram(common_subtrees_color_branches = TRUE)&#xD;&#xA;&#xD;&#xA;According to the manual, the code above produces an image like this:&#xD;&#xA;&#xD;&#xA;[![example of a colored tanglegram][2]][2]&#xD;&#xA;&#xD;&#xA; Try to see if you can fit your color palette in there.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html&#xD;&#xA;  [2]: https://i.stack.imgur.com/v8kWS.png" />
  <row Id="2888" PostHistoryTypeId="2" PostId="922" RevisionGUID="f2cab759-5e41-4702-9bb2-7852be006915" CreationDate="2017-06-27T13:44:49.057" UserId="929" Text="As an alternative (though I realise this doesn't really answer the OPs question directly), [Dendroscope][1] and [Treemap][2] can also produce these types of plots. I don't know if they'll do coloured connections off the top of my head, but I think they can differentially colour the tree branches.&#xD;&#xA;&#xD;&#xA;Just in case that is of use!&#xD;&#xA;&#xD;&#xA;  [1]: http://dendroscope.org/&#xD;&#xA;  [2]: https://sites.google.com/site/cophylogeny/treemap" />
  <row Id="2890" PostHistoryTypeId="5" PostId="914" RevisionGUID="6d9c1201-43ee-49fe-a7b5-90aafebd7264" CreationDate="2017-06-27T15:34:30.763" UserId="823" Comment="added 68 characters in body" Text="What tools are available to predict, based on the structure of a certain protein, its interactions within a cell?&#xD;&#xA;&#xD;&#xA;For example, I am considering the split GFP protein and I am trying to predict if there will be any interactions which might hinder its binding efficiency to the other half of the GFP protein. &#xD;&#xA;&#xD;&#xA;Note that I am not only looking for protein interactions, but other molecules as well." />
  <row Id="2891" PostHistoryTypeId="2" PostId="923" RevisionGUID="5739d581-d601-4885-b902-3dff8633f6a0" CreationDate="2017-06-27T16:18:53.597" UserId="982" Text="I want to convert my phylogeny into a dendrogram so I can use it with dendextend in R to produce a tanglegram. I have made some progress but I keep encountering errors, see below:&#xD;&#xA;&#xD;&#xA;    library(ape)&#xD;&#xA;    library(dendextend)&#xD;&#xA;&#xD;&#xA;    Tree &lt;- read.tree(file=&quot;clipboard&quot;, text=NULL) &#xD;&#xA;    # test &lt;- as.dendrogram(Tree) &#xD;&#xA;    ## Error in ape::as.hclust.phylo(object) : the tree is not ultrametric&#xD;&#xA;&#xD;&#xA;    Tree.ultra &lt;- chronos(Tree)  &#xD;&#xA;    # test &lt;- as.dendrogram(Tree.ultra) &#xD;&#xA;    ## Error in ape::as.hclust.phylo(object) : the tree is not rooted&#xD;&#xA;&#xD;&#xA;    Tree.ultra$root.edge &lt;- 0&#xD;&#xA;    # test &lt;- as.dendrogram(Tree.ultra) &#xD;&#xA;    ## Error in ape::as.hclust.phylo(object) : the tree is not binary&#xD;&#xA;&#xD;&#xA;This [thread][1] seems to be the closest to answering my issues but the workflow is different and Im not sure how to address this binary issue. I have tried using as.hclust, as.hclust.phylo and hclust. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/4576/" />
  <row Id="2892" PostHistoryTypeId="1" PostId="923" RevisionGUID="5739d581-d601-4885-b902-3dff8633f6a0" CreationDate="2017-06-27T16:18:53.597" UserId="982" Text="How to convert a phylogeny to a dendrogram in R" />
  <row Id="2893" PostHistoryTypeId="3" PostId="923" RevisionGUID="5739d581-d601-4885-b902-3dff8633f6a0" CreationDate="2017-06-27T16:18:53.597" UserId="982" Text="&lt;r&gt;&lt;phylogeny&gt;" />
  <row Id="2894" PostHistoryTypeId="2" PostId="924" RevisionGUID="3ecb3587-950e-47d2-9d66-ce8877651595" CreationDate="2017-06-27T16:55:20.077" UserId="988" Text="I have a protein of my interest and I would like to now how it interacts with RNA? I have structures of both molecules What tool I should use it?" />
  <row Id="2895" PostHistoryTypeId="1" PostId="924" RevisionGUID="3ecb3587-950e-47d2-9d66-ce8877651595" CreationDate="2017-06-27T16:55:20.077" UserId="988" Text="How to dock a protein to a nucleic acid?" />
  <row Id="2896" PostHistoryTypeId="3" PostId="924" RevisionGUID="3ecb3587-950e-47d2-9d66-ce8877651595" CreationDate="2017-06-27T16:55:20.077" UserId="988" Text="&lt;proteins&gt;&lt;rna&gt;&lt;docking&gt;" />
  <row Id="2898" PostHistoryTypeId="5" PostId="924" RevisionGUID="c9c34e04-e0fc-4f9f-a9bb-ef0e7b7ae06c" CreationDate="2017-06-27T18:00:37.120" UserId="298" Comment="Copy edit" Text="I have a protein of interest and I would like to now how it interacts with RNA. I have structures of both molecules.&#xD;&#xA;&#xD;&#xA;What tool can I use?" />
  <row Id="2899" PostHistoryTypeId="4" PostId="924" RevisionGUID="c9c34e04-e0fc-4f9f-a9bb-ef0e7b7ae06c" CreationDate="2017-06-27T18:00:37.120" UserId="298" Comment="Copy edit" Text="How can I dock a protein to a nucleic acid?" />
  <row Id="2900" PostHistoryTypeId="5" PostId="923" RevisionGUID="fd987af7-586c-4716-9a5c-3b9b56f76359" CreationDate="2017-06-27T18:54:03.003" UserId="982" Comment="added 35 characters in body" Text="I want to convert my phylogeny into a dendrogram so I can use it with dendextend in R to produce a tanglegram. I have made some progress but I keep encountering errors, see below:&#xD;&#xA;&#xD;&#xA;    library(ape)&#xD;&#xA;    library(dendextend)&#xD;&#xA;&#xD;&#xA;    Tree &lt;- rtree(10, rooted=F)&#xD;&#xA;&#xD;&#xA;    Tree &lt;- read.tree(file=&quot;clipboard&quot;, text=NULL) &#xD;&#xA;    # test &lt;- as.dendrogram(Tree) &#xD;&#xA;    ## Error in ape::as.hclust.phylo(object) : the tree is not ultrametric&#xD;&#xA;&#xD;&#xA;    Tree.ultra &lt;- chronos(Tree)  &#xD;&#xA;    # test &lt;- as.dendrogram(Tree.ultra) &#xD;&#xA;    ## Error in ape::as.hclust.phylo(object) : the tree is not rooted&#xD;&#xA;&#xD;&#xA;    Tree.ultra$root.edge &lt;- 0&#xD;&#xA;    # test &lt;- as.dendrogram(Tree.ultra) &#xD;&#xA;    ## Error in ape::as.hclust.phylo(object) : the tree is not binary&#xD;&#xA;&#xD;&#xA;This [thread][1] seems to be the closest to answering my issues but the workflow is different and Im not sure how to address this binary issue. I have tried using as.hclust, as.hclust.phylo and hclust. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/4576/" />
  <row Id="2902" PostHistoryTypeId="2" PostId="925" RevisionGUID="e04d5c4b-fc29-486d-b6cf-ddd0af74ba71" CreationDate="2017-06-27T19:56:19.770" UserId="939" Text="For your example to work you should replace the `rtree` function with the `rcoal` function, see example below.&#xD;&#xA;&#xD;&#xA;    library(ape)  &#xD;&#xA;    library(dendextend)   &#xD;&#xA;    Tree &lt;- rtree(10, rooted=F)&#xD;&#xA;    is.ultrametric(Tree) &#xD;&#xA;    [1] FALSE &#xD;&#xA;    is.binary.tree(Tree) &#xD;&#xA;    [1] TRUE&#xD;&#xA;    &#xD;&#xA;    Tree_rcoal &lt;- rcoal(10, rooted=F) &#xD;&#xA;    is.ultrametric(Tree_rcoal) &#xD;&#xA;    [1] TRUE&#xD;&#xA;    is.binary.tree(Tree_rcoal) &#xD;&#xA;    [1] TRUE&#xD;&#xA;     &#xD;&#xA;    Tree_rcoal2 &lt;- rcoal(10, rooted=F)&#xD;&#xA;&#xD;&#xA;    tanglegram(Tree_rcoal, Tree_rcoal2)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2903" PostHistoryTypeId="2" PostId="926" RevisionGUID="323757c9-3dae-400e-a6d8-b0bddac73651" CreationDate="2017-06-27T21:11:07.147" UserId="470" Text="Firstly, I would download some sort of editor that is built to handle nucleotide sequences. There are a handful out there, but APE is a fairly popular software designed for plasmid editing. It is much more efficient at searching for nucleotide strings as it can search for reverse complements and to a certain extent, mismatches. It's also free!&#xD;&#xA;&#xD;&#xA;http://biologylabs.utah.edu/jorgensen/wayned/ape/&#xD;&#xA;&#xD;&#xA;For your question, I'm not exactly sure what you are asking, but I would use the BLAST alignment tool to try to see similarity between your RNAi sequences and the target mRNA. You input fasta files and the program searches for any possible overlap between the two inputs. The higher the &quot;max-score&quot; the more likely the two strands (or subsections of strands) have high similarity. This program also considers the complement&#xD;&#xA;&#xD;&#xA;https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&amp;PAGE_TYPE=BlastSearch&amp;LINK_LOC=blasthome&#xD;&#xA;&#xD;&#xA;You can use the results from BLAST to annotate in APE using different colors which is extremely helpful. You can also use BLAST in the user interface of APE. &#xD;&#xA;&#xD;&#xA;Also keep in mind RNAi's are not always effective, even though they are predicted to bind at a given location, that may not be a biological reality &#xD;&#xA;&#xD;&#xA;Good luck! &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2904" PostHistoryTypeId="6" PostId="924" RevisionGUID="06a07433-a6df-4d4f-bd11-e2c0d5d02694" CreationDate="2017-06-27T21:26:30.170" UserId="77" Comment="edited tags" Text="&lt;proteins&gt;&lt;software-recommendation&gt;&lt;rna&gt;&lt;docking&gt;" />
  <row Id="2905" PostHistoryTypeId="2" PostId="927" RevisionGUID="35d03972-0103-4922-a347-b980098ad74c" CreationDate="2017-06-27T23:24:38.537" UserId="640" Text="I'm not sure what you meant but you can take a look at NPDock http://genesilico.pl/NPDock (disclaimer we wrote that tool). If you have a structure of your protein of interest, you can dock it to the structure of your DNA/RNA of interest. Mind that this is a rigid body docking which means that the structure will not change upon binding. I hope this helps! Happy predictions!" />
  <row Id="2906" PostHistoryTypeId="2" PostId="928" RevisionGUID="4116d848-c09e-4c27-9c18-da41e31725a5" CreationDate="2017-06-28T05:28:39.973" UserId="993" Text="I have a distance matrix generated by hierfstat, thusly:&#xD;&#xA;&#xD;&#xA;    snps &lt;- fasta2DNAbin('test.fa', chunkSize = 50)&#xD;&#xA;    gi &lt;- DNAbin2genind(snps)&#xD;&#xA;    # manually define the populations for passing to pairwise.fst&#xD;&#xA;    p &lt;- c('Botswana', 'Botswana', 'Botswana', 'Botswana', 'France', 'France', 'Vietnam', 'Vietnam', 'Uganda', 'Uganda', 'Uganda', 'Uganda', 'Vietnam', 'Vietnam', 'Laos', 'Laos', 'Laos', 'Vietnam', 'Vietnam', 'Vietnam', 'Vietnam', 'Vietnam')&#xD;&#xA;    f &lt;- pairwise.fst(gi, p, res.type = c('dist', 'matrix'))&#xD;&#xA;    as.matrix(f)&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;which produces&#xD;&#xA;&#xD;&#xA;    	1	2	3	4	5&#xD;&#xA;    1	0	0.2189008	0.225567409	0.1821518	0.259409722&#xD;&#xA;    2	0.2189008	0	0.130736953	0.1648034	0.191050772&#xD;&#xA;    3	0.2255674	0.130737	0	0.1669077	0.006396789&#xD;&#xA;    4	0.1821518	0.1648034	0.166907683	0	0.203931457&#xD;&#xA;    5	0.2594097	0.1910508	0.006396789	0.2039315	0&#xD;&#xA;&#xD;&#xA;Is it safe to assume that 1 = Botswana, 2 = France, 3 = Vietnam, 4 = Uganda and 5 = Laos? i.e. that the distance matrix rows/columns follow the order that each population first appeared in `p`?&#xD;&#xA;&#xD;&#xA;Is there a way to determine for certain, which row/column in the distance matrix corresponds to which population?" />
  <row Id="2907" PostHistoryTypeId="1" PostId="928" RevisionGUID="4116d848-c09e-4c27-9c18-da41e31725a5" CreationDate="2017-06-28T05:28:39.973" UserId="993" Text="What are the labels in my rstats distance matrix?" />
  <row Id="2908" PostHistoryTypeId="3" PostId="928" RevisionGUID="4116d848-c09e-4c27-9c18-da41e31725a5" CreationDate="2017-06-28T05:28:39.973" UserId="993" Text="&lt;r&gt;" />
  <row Id="2909" PostHistoryTypeId="5" PostId="877" RevisionGUID="bf4ce219-a3bf-4c49-bce4-86839d26876f" CreationDate="2017-06-28T08:02:38.893" UserId="734" Comment="deleted 89 characters in body" Text="I have seen that there are 2 common methods: [Patient-derived models][1]: [PDX][2] and [PDO][3] to reflect tumor biology. &#xA;&#xD;&#xA;First, What are the differences between PDX and PDO?&#xA;Secondly, they both look very specific and I was curious if it's possible to use them to build a predictive model?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Are there any databases/resources/computational tools that use the outcomes of PDO/PDX experiments to create a predictive model?**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&#xD;&#xA;  [3]: http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html" />
  <row Id="2910" PostHistoryTypeId="5" PostId="928" RevisionGUID="fc9e224b-e97a-44ea-85b4-3956e15e181e" CreationDate="2017-06-28T08:04:01.480" UserId="993" Comment="added libraries to code" Text="I have a distance matrix generated by hierfstat, thusly, with a link to the fasta file [here](https://www.dropbox.com/s/z9n89y080hvl3s2/2017.06.13.strucutre_test.masked.snp_sites.v2_3_2.subsample.fa?dl=0):&#xD;&#xA;&#xD;&#xA;    library(adegenet)&#xD;&#xA;    library(hierfstat)&#xD;&#xA;    snps &lt;- fasta2DNAbin('test.fa', chunkSize = 50)&#xD;&#xA;    gi &lt;- DNAbin2genind(snps)&#xD;&#xA;    # manually define the populations for passing to pairwise.fst&#xD;&#xA;    p &lt;- c('Botswana', 'Botswana', 'Botswana', 'Botswana', 'France', 'France', 'Vietnam', 'Vietnam', 'Uganda', 'Uganda', 'Uganda', 'Uganda', 'Vietnam', 'Vietnam', 'Laos', 'Laos', 'Laos', 'Vietnam', 'Vietnam', 'Vietnam', 'Vietnam', 'Vietnam')&#xD;&#xA;    f &lt;- pairwise.fst(gi, p, res.type = c('dist', 'matrix'))&#xD;&#xA;    as.matrix(f)&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;which produces&#xD;&#xA;&#xD;&#xA;    	1	2	3	4	5&#xD;&#xA;    1	0	0.2189008	0.225567409	0.1821518	0.259409722&#xD;&#xA;    2	0.2189008	0	0.130736953	0.1648034	0.191050772&#xD;&#xA;    3	0.2255674	0.130737	0	0.1669077	0.006396789&#xD;&#xA;    4	0.1821518	0.1648034	0.166907683	0	0.203931457&#xD;&#xA;    5	0.2594097	0.1910508	0.006396789	0.2039315	0&#xD;&#xA;&#xD;&#xA;Is it safe to assume that 1 = Botswana, 2 = France, 3 = Vietnam, 4 = Uganda and 5 = Laos? i.e. that the distance matrix rows/columns follow the order that each population first appeared in `p`?&#xD;&#xA;&#xD;&#xA;Is there a way to determine for certain, which row/column in the distance matrix corresponds to which population?" />
  <row Id="2911" PostHistoryTypeId="5" PostId="925" RevisionGUID="12e62507-0ac0-49f8-86a5-ca60e86d38ca" CreationDate="2017-06-28T08:04:56.623" UserId="939" Comment="Adding an example of how to convert newick file to usable dendrogram " Text="For your example to work you should replace the `rtree` function with the `rcoal` function, see example below.&#xD;&#xA;&#xD;&#xA;    library(ape)  &#xD;&#xA;    library(dendextend)   &#xD;&#xA;    Tree &lt;- rtree(10, rooted=F)&#xD;&#xA;    is.ultrametric(Tree) &#xD;&#xA;    [1] FALSE &#xD;&#xA;    is.binary.tree(Tree) &#xD;&#xA;    [1] TRUE&#xD;&#xA;    &#xD;&#xA;    Tree_rcoal &lt;- rcoal(10, rooted=F) &#xD;&#xA;    is.ultrametric(Tree_rcoal) &#xD;&#xA;    [1] TRUE&#xD;&#xA;    is.binary.tree(Tree_rcoal) &#xD;&#xA;    [1] TRUE&#xD;&#xA;     &#xD;&#xA;    Tree_rcoal2 &lt;- rcoal(10, rooted=F)&#xD;&#xA;&#xD;&#xA;    tanglegram(Tree_rcoal, Tree_rcoal2)&#xD;&#xA;&#xD;&#xA;To make use of a file in newick format, you can try to convert with the `chronos` function.&#xD;&#xA;&#xD;&#xA;    s &lt;- &quot;owls(((Strix_aluco:4.2,Asio_otus:4.2):3.1,Athene_noctua:7.3):6.3,Tyto_alba:13.5);&quot;&#xD;&#xA;    cat(s, file = &quot;ex.tre&quot;, sep = &quot;\n&quot;)&#xD;&#xA;    tree.owls &lt;- read.tree(&quot;ex.tre&quot;)&#xD;&#xA;    is.ultrametric(tree.owls)&#xD;&#xA;    [1] FALSE&#xD;&#xA;    dendrogram &lt;- chronos(tree.owls)&#xD;&#xA;    &#xD;&#xA;    Setting initial dates...&#xD;&#xA;    Fitting in progress... get a first set of estimates&#xD;&#xA;             Penalised log-lik = -24.04992 &#xD;&#xA;    Optimising rates... dates... -24.04992 &#xD;&#xA;    Optimising rates... dates... -24.04908 &#xD;&#xA;    &#xD;&#xA;    Done.&#xD;&#xA;    is.ultrametric(dendrogram)&#xD;&#xA;    [1] TRUE&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2912" PostHistoryTypeId="2" PostId="929" RevisionGUID="59dbc039-2772-493a-90f5-24a1820cf17c" CreationDate="2017-06-28T08:23:44.230" UserId="787" Text="Disclaimer: I don't have adagenet and hierfstat installed, I just looked at the source code. It seems like pairwise.fst (the deprecated code from the adagenet package, which you seem to be using) already returns a matrix. Set res.type=&quot;matrix&quot; and try calling `rownames(f)` and `colnames(f)` afterwards." />
  <row Id="2913" PostHistoryTypeId="5" PostId="918" RevisionGUID="88840d2b-51a4-4ed4-a359-1b762609b06a" CreationDate="2017-06-28T08:57:51.490" UserId="298" Comment="added 440 characters in body" Text="You could try one of these tools to predict protein-protein interactions:&#xD;&#xA;&#xD;&#xA;1. [Struct2Net][1]&#xD;&#xA;&#xD;&#xA; &gt; Given two protein sequences, the structure-based interaction prediction technique threads these two sequences to all the protein complexes in the PDB and then chooses the best potential match. Based on this match, the method generates alignment scores, z-scores, and an interfacial energy for the sequence pair. Logistic regression is then used to evaluate whether a set of scores corresponds to an interaction or not. The algorithm is also extended to find all potential partners given a single protein sequence. Further details about the method are described [here][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;2. [PRED][3]&#xD;&#xA;&#xD;&#xA; &gt; Pred_PPI is a web-based system that serves for predicting PPIs from different organisms. This server is freely available to any researcher wishing to use it for non-commercial purposes. Based on auto covariance (AC) and support vector machine (SVM), this tool is capable of predicting PPIs for any target protein pair only using their primary sequences, and assigning an interaction probability to each SVM prediction as well. So the user can use this tool to predict novel PPIs with high confidence.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;For protein-RNA interactions, you can try:&#xD;&#xA;&#xD;&#xA;1. [*cat*Rapid][4]&#xD;&#xA;&#xD;&#xA; &gt; Through the calculation of secondary structure, hydrogen bonding and van der Waals contributions, catRAPID is able predict protein-RNA interaction propensities with great accuracy (up to 89% on the ncRNA-protein interaction database, NPinter).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cb.csail.mit.edu/cb/struct2net/webserver/about.html&#xD;&#xA;  [2]: http://helix-web.stanford.edu/psb06/singh.pdf&#xD;&#xA;  [3]: http://cic.scu.edu.cn/bioinformatics/predict_ppi/default.html&#xD;&#xA;  [4]: http://service.tartaglialab.com/page/catrapid_omics_group" />
  <row Id="2914" PostHistoryTypeId="5" PostId="927" RevisionGUID="43f70df2-fc1e-4043-9dec-e2c7df006c9c" CreationDate="2017-06-28T09:05:20.913" UserId="48" Comment="Removed chatty line, hide the link as hypertext. " Text="I'm not sure what you meant but you can take a look at [NPDock][1] (disclaimer we wrote that tool). If you have a structure of your protein of interest, you can dock it to the structure of your DNA/RNA of interest. Mind that this is a rigid body docking which means that the structure will not change upon binding. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://genesilico.pl/NPDock" />
  <row Id="2915" PostHistoryTypeId="24" PostId="927" RevisionGUID="43f70df2-fc1e-4043-9dec-e2c7df006c9c" CreationDate="2017-06-28T09:05:20.913" Comment="Proposed by 48 approved by 57, 77 edit id of 227" />
  <row Id="2916" PostHistoryTypeId="5" PostId="877" RevisionGUID="aec36c1c-525c-4dec-afcd-8770f2b5059e" CreationDate="2017-06-28T09:05:30.097" UserId="48" Comment="Expand two acronyms " Text="I have seen that there are 2 common methods: [Patient-derived models][1]: [Patient Derived Xenograft (PDX)][2] and [Patient Derived Organoids (PDO)][3] to reflect tumor biology. &#xD;&#xA;&#xD;&#xA;First, What are the differences between PDX and PDO?  &#xD;&#xA;Secondly, they both look very specific and I was curious if it's possible to use them to build a predictive model?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Are there any databases/resources/computational tools that use the outcomes of PDO/PDX experiments to create a predictive model?**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&#xD;&#xA;  [3]: http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html" />
  <row Id="2917" PostHistoryTypeId="24" PostId="877" RevisionGUID="aec36c1c-525c-4dec-afcd-8770f2b5059e" CreationDate="2017-06-28T09:05:30.097" Comment="Proposed by 48 approved by 57, 77 edit id of 228" />
  <row Id="2918" PostHistoryTypeId="2" PostId="930" RevisionGUID="6981774d-0c67-4ab1-9085-0dfb6c86c5d8" CreationDate="2017-06-28T09:23:27.727" UserId="982" Text="After searching high and low I have found an answer from this [thread][1]&#xD;&#xA;&#xD;&#xA;Workflow goes like:&#xD;&#xA;&#xD;&#xA;    library(DECIPHER)&#xD;&#xA;    dend1 &lt;- ReadDendrogram(file=&quot;clipboard&quot;)   &#xD;&#xA;&#xD;&#xA;Note the newick file cannot be an existing object in the R environment. Its must be 'read in'. Available in [Bioconductor][2].  &#xD;&#xA;This object can then be used in the dendextend package and tanglegram function. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://stackoverflow.com/questions/7445684/how-to-convert-a-tree-to-a-dendrogram-in-r&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/bioc/html/DECIPHER.html" />
  <row Id="2919" PostHistoryTypeId="2" PostId="931" RevisionGUID="272ba86a-4afa-42f9-966e-ab78b0ebd712" CreationDate="2017-06-28T11:19:21.197" UserId="982" Text="Thanks everyone for the suggestions @b.nota answer is useful to colour according to clade groups but does not address my actual question on using metadata to colour the lines. For this answer see below:&#xD;&#xA;&#xD;&#xA;dendextend states for `color_lines`&#xD;&#xA;a vector of colors for the lines connected the labels. If the colors are shorter than the number of labels, they are recycled (and a warning is issued). The colors in the vector are applied on the lines from the bottom up.&#xD;&#xA;&#xD;&#xA;To that end I used the workflow from [my previous thread again][1] to generate an ordered character vector of colours `(x1)`. I then used the following command to get the figure:&#xD;&#xA;  &#xD;&#xA;    x1 &lt;- rev(x1)  # The colors in the vector are applied on the lines from the bottom up.&#xD;&#xA;    tanglegram(dendA, dendC, color_lines = x1)&#xD;&#xA;&#xD;&#xA;Now I still have an issue with the colours in my vector still not accurately mapping in the figure but I feel this has to do with the topology of one of my phylogenies, Im going to ask/raise this on the dendextend github. &#xD;&#xA;&#xD;&#xA;Side note if anyone with a phylogeny needs to convert it to a dendrogram to try this on their data see [my other question][2]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://stackoverflow.com/questions/44006500/coloured-lines-for-tangelgram-package-ape-function-cophyloplot&#xD;&#xA;  [2]: https://bioinformatics.stackexchange.com/questions/923/how-to-convert-a-phylogeny-to-a-dendrogram-in-r/930?noredirect=1#comment1714_930" />
  <row Id="2920" PostHistoryTypeId="2" PostId="932" RevisionGUID="a60515a7-4132-4430-bcd7-cbe09681e749" CreationDate="2017-06-28T12:40:25.230" UserId="939" Text="To get your colors in the original order, you can make a dataframe with the labels in the right order with your metadata. You can use `merge` for this.&#xD;&#xA;&#xD;&#xA;So from your example code it would be something like this:&#xD;&#xA;&#xD;&#xA;    site &lt;- structure(list(name = structure(c(1L, 3L, 4L, 5L, 6L, 7L, 8L,9L, 10L, 2L), .Label = c(&quot;t1&quot;, &quot;t10&quot;, &quot;t2&quot;, &quot;t3&quot;, &quot;t4&quot;, &quot;t5&quot;,&quot;t6&quot;, &quot;t7&quot;, &quot;t8&quot;, &quot;t9&quot;), class = &quot;factor&quot;), site = c(1L, 1L,1L, 2L, 2L, 3L, 1L, 3L, 2L, 2L)), .Names = c(&quot;name&quot;, &quot;site&quot;), row.names = c(NA,10L), class = &quot;data.frame&quot;) &#xD;&#xA;    &#xD;&#xA;    library(ape)&#xD;&#xA;    library(dendextend)&#xD;&#xA;    &#xD;&#xA;    t1 &lt;- rcoal(10)&#xD;&#xA;    t2 &lt;- rcoal(10)&#xD;&#xA;    &#xD;&#xA;    str(site)&#xD;&#xA;    # The name are factors now, so make chr first&#xD;&#xA;    site$name &lt;- as.character(site$name)&#xD;&#xA;    &#xD;&#xA;    # make a data.frame of your labels&#xD;&#xA;    labels_df &lt;- data.frame(t1$tip.label)&#xD;&#xA;    &#xD;&#xA;    #merge the 2 data.frames in the right order (hence sort=F)&#xD;&#xA;    colors &lt;- merge(labels_df,site,by.x=&quot;t1.tip.label&quot;, by.y=&quot;name&quot;,all.x=T, all.y=F,sort=F)&#xD;&#xA;    &#xD;&#xA;    tanglegram(t1, t2, color_lines=colors$site)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2921" PostHistoryTypeId="2" PostId="933" RevisionGUID="3f6b67b0-1c8b-4d55-8560-77ad652b9665" CreationDate="2017-06-28T12:53:59.550" UserId="195" Text="I currently find Harvard's RESTful API for ExAC (http://exac.hms.harvard.edu/) extremely useful and I was hoping that a similar resource is available for Gnomad (http://gnomad.broadinstitute.org/)?&#xD;&#xA;&#xD;&#xA; Does anyone know of a public access API for Gnomad or possibly any plans to integrate Gnomad into the Harvard API?" />
  <row Id="2922" PostHistoryTypeId="1" PostId="933" RevisionGUID="3f6b67b0-1c8b-4d55-8560-77ad652b9665" CreationDate="2017-06-28T12:53:59.550" UserId="195" Text="Is there public RESTful api for Gnomad?" />
  <row Id="2923" PostHistoryTypeId="3" PostId="933" RevisionGUID="3f6b67b0-1c8b-4d55-8560-77ad652b9665" CreationDate="2017-06-28T12:53:59.550" UserId="195" Text="&lt;variants&gt;" />
  <row Id="2924" PostHistoryTypeId="2" PostId="934" RevisionGUID="2bcc50ca-f38f-4d9c-9490-a267dcd80cb8" CreationDate="2017-06-28T13:04:58.663" UserId="71" Text="As far as I know, no but the vcf.gz files are behind a http server that support *Byte-Range*, so you can use tabix or any related API:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    $ tabix &quot;https://storage.googleapis.com/gnomad-public/release-170228/vcf/exomes/gnomad.exomes.r2.0.1.sites.vcf.gz&quot; &quot;22:17265182-17265182&quot;&#xD;&#xA;    22	17265182	.	A	T	762.04	PASS	AC=1;AF=4.78057e-06;AN=209180;BaseQRankSum=-4.59400e+00;ClippingRankSum=2.18000e+00;DP=4906893;FS=1.00270e+01;InbreedingCoeff=4.40000e-03;MQ=3.15200e+01;MQRankSum=1.40000e+00;QD=1.31400e+01;ReadPosRankSum=2.23000e-01;SOR=9.90000e-02;VQSLOD=-5.12800e+00;VQSR_culprit=MQ;GQ_HIST_ALT=0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1;DP_HIST_ALT=0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0;AB_HIST_ALT=0|0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0;GQ_HIST_ALL=1591|589|120|301|650|589|1854|2745|1815|4297|5061|2921|10164|1008|6489|1560|7017|457|6143|52950;DP_HIST_ALL=2249|1418|6081|11707|16538|9514|28624|23829|7391|853|95|19|1|0|0|1|0|1|0|0;AB_HIST_ALL=0|0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0;AC_AFR=0;AC_AMR=0;AC_ASJ=0;AC_EAS=0;AC_FIN=1;AC_NFE=0;AC_OTH=0;AC_SAS=0;AC_Male=1;AC_Female=0;AN_AFR=11994;AN_AMR=31324;AN_ASJ=7806;AN_EAS=13112;AN_FIN=20076;AN_NFE=94516;AN_OTH=4656;AN_SAS=25696;AN_Male=114366;AN_Female=94814;AF_AFR=0.00000e+00;AF_AMR=0.00000e+00;AF_ASJ=0.00000e+00;AF_EAS=0.00000e+00;AF_FIN=4.98107e-05;AF_NFE=0.00000e+00;AF_OTH=0.00000e+00;AF_SAS=0.00000e+00;AF_Male=8.74386e-06;AF_Female=0.00000e+00;GC_AFR=5997,0,0;GC_AMR=15662,0,0;GC_ASJ=3903,0,0;GC_EAS=6556,0,0;GC_FIN=10037,1,0;GC_NFE=47258,0,0;GC_OTH=2328,0,0;GC_SAS=12848,0,0;GC_Male=57182,1,0;GC_Female=47407,0,0;AC_raw=1;AN_raw=216642;AF_raw=4.61591e-06;GC_raw=108320,1,0;GC=104589,1,0;Hom_AFR=0;Hom_AMR=0;Hom_ASJ=0;Hom_EAS=0;Hom_FIN=0;Hom_NFE=0;Hom_OTH=0;Hom_SAS=0;Hom_Male=0;Hom_Female=0;Hom_raw=0;Hom=0;POPMAX=FIN;AC_POPMAX=1;AN_POPMAX=20076;AF_POPMAX=4.98107e-05;DP_MEDIAN=58;DREF_MEDIAN=5.01187e-84;GQ_MEDIAN=99;AB_MEDIAN=6.03448e-01;AS_RF=9.18451e-01;AS_FilterStatus=PASS;CSQ=T|missense_variant|MODERATE|XKR3|ENSG00000172967|Transcript|ENST00000331428|protein_coding|4/4||ENST00000331428.5:c.707T&gt;A|ENSP00000331704.5:p.Phe236Tyr|810|707|236|F/Y|tTc/tAc||1||-1||SNV|1|HGNC|28778|YES|||CCDS42975.1|ENSP00000331704|Q5GH77||UPI000013EFAE||deleterious(0)|benign(0.055)|hmmpanther:PTHR14297&amp;hmmpanther:PTHR14297:SF7&amp;Pfam_domain:PF09815||||||||||||||||||||||||||||||,T|regulatory_region_variant|MODIFIER|||RegulatoryFeature|ENSR00000672806|TF_binding_site|||||||||||1||||SNV|1||||||||||||||||||||||||||||||||||||||||||||,T|regulatory_region_variant|MODIFIER|||RegulatoryFeature|ENSR00001729562|CTCF_binding_site|||||||||||1||||SNV|1||||||||||||||||||||||||||||||||||||||||||||&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2925" PostHistoryTypeId="5" PostId="933" RevisionGUID="f4866ae9-ff34-4291-b0a6-f00dc6cdb44d" CreationDate="2017-06-28T13:06:05.817" UserId="77" Comment="added 25 characters in body; edited tags" Text="I currently find Harvard's RESTful API for [ExAC][1] extremely useful and I was hoping that a similar resource is available for [Gnomad][2]?&#xD;&#xA;&#xD;&#xA;Does anyone know of a public access API for Gnomad or possibly any plans to integrate Gnomad into the Harvard API?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://exac.hms.harvard.edu/&#xD;&#xA;  [2]: http://gnomad.broadinstitute.org/" />
  <row Id="2926" PostHistoryTypeId="6" PostId="933" RevisionGUID="f4866ae9-ff34-4291-b0a6-f00dc6cdb44d" CreationDate="2017-06-28T13:06:05.817" UserId="77" Comment="added 25 characters in body; edited tags" Text="&lt;variants&gt;&lt;gnomad&gt;" />
  <row Id="2927" PostHistoryTypeId="5" PostId="934" RevisionGUID="84b0eb06-ef00-4f12-8e7b-9cff0ad1890f" CreationDate="2017-06-28T14:20:08.960" UserId="298" Comment="added 3 characters in body" Text="As far as I know, no but the vcf.gz files are behind a http server that supports *Byte-Range*, so you can use `tabix` or any related API:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    $ tabix &quot;https://storage.googleapis.com/gnomad-public/release-170228/vcf/exomes/gnomad.exomes.r2.0.1.sites.vcf.gz&quot; &quot;22:17265182-17265182&quot;&#xD;&#xA;    22	17265182	.	A	T	762.04	PASS	AC=1;AF=4.78057e-06;AN=209180;BaseQRankSum=-4.59400e+00;ClippingRankSum=2.18000e+00;DP=4906893;FS=1.00270e+01;InbreedingCoeff=4.40000e-03;MQ=3.15200e+01;MQRankSum=1.40000e+00;QD=1.31400e+01;ReadPosRankSum=2.23000e-01;SOR=9.90000e-02;VQSLOD=-5.12800e+00;VQSR_culprit=MQ;GQ_HIST_ALT=0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1;DP_HIST_ALT=0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0;AB_HIST_ALT=0|0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0;GQ_HIST_ALL=1591|589|120|301|650|589|1854|2745|1815|4297|5061|2921|10164|1008|6489|1560|7017|457|6143|52950;DP_HIST_ALL=2249|1418|6081|11707|16538|9514|28624|23829|7391|853|95|19|1|0|0|1|0|1|0|0;AB_HIST_ALL=0|0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0;AC_AFR=0;AC_AMR=0;AC_ASJ=0;AC_EAS=0;AC_FIN=1;AC_NFE=0;AC_OTH=0;AC_SAS=0;AC_Male=1;AC_Female=0;AN_AFR=11994;AN_AMR=31324;AN_ASJ=7806;AN_EAS=13112;AN_FIN=20076;AN_NFE=94516;AN_OTH=4656;AN_SAS=25696;AN_Male=114366;AN_Female=94814;AF_AFR=0.00000e+00;AF_AMR=0.00000e+00;AF_ASJ=0.00000e+00;AF_EAS=0.00000e+00;AF_FIN=4.98107e-05;AF_NFE=0.00000e+00;AF_OTH=0.00000e+00;AF_SAS=0.00000e+00;AF_Male=8.74386e-06;AF_Female=0.00000e+00;GC_AFR=5997,0,0;GC_AMR=15662,0,0;GC_ASJ=3903,0,0;GC_EAS=6556,0,0;GC_FIN=10037,1,0;GC_NFE=47258,0,0;GC_OTH=2328,0,0;GC_SAS=12848,0,0;GC_Male=57182,1,0;GC_Female=47407,0,0;AC_raw=1;AN_raw=216642;AF_raw=4.61591e-06;GC_raw=108320,1,0;GC=104589,1,0;Hom_AFR=0;Hom_AMR=0;Hom_ASJ=0;Hom_EAS=0;Hom_FIN=0;Hom_NFE=0;Hom_OTH=0;Hom_SAS=0;Hom_Male=0;Hom_Female=0;Hom_raw=0;Hom=0;POPMAX=FIN;AC_POPMAX=1;AN_POPMAX=20076;AF_POPMAX=4.98107e-05;DP_MEDIAN=58;DREF_MEDIAN=5.01187e-84;GQ_MEDIAN=99;AB_MEDIAN=6.03448e-01;AS_RF=9.18451e-01;AS_FilterStatus=PASS;CSQ=T|missense_variant|MODERATE|XKR3|ENSG00000172967|Transcript|ENST00000331428|protein_coding|4/4||ENST00000331428.5:c.707T&gt;A|ENSP00000331704.5:p.Phe236Tyr|810|707|236|F/Y|tTc/tAc||1||-1||SNV|1|HGNC|28778|YES|||CCDS42975.1|ENSP00000331704|Q5GH77||UPI000013EFAE||deleterious(0)|benign(0.055)|hmmpanther:PTHR14297&amp;hmmpanther:PTHR14297:SF7&amp;Pfam_domain:PF09815||||||||||||||||||||||||||||||,T|regulatory_region_variant|MODIFIER|||RegulatoryFeature|ENSR00000672806|TF_binding_site|||||||||||1||||SNV|1||||||||||||||||||||||||||||||||||||||||||||,T|regulatory_region_variant|MODIFIER|||RegulatoryFeature|ENSR00001729562|CTCF_binding_site|||||||||||1||||SNV|1||||||||||||||||||||||||||||||||||||||||||||&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2928" PostHistoryTypeId="2" PostId="935" RevisionGUID="e22f7c59-e55f-4c33-af06-9bfc794a1dda" CreationDate="2017-06-28T14:50:27.330" UserId="298" Text="I am looking for a tool, preferably written in C or C++, that can quickly and efficiently count the number of reads and the number of bases in a compressed fastq file. I am currently doing this using `zgrep` and `awk`:&#xD;&#xA;&#xD;&#xA;    zgrep . foo.fasq.gz |&#xD;&#xA;         awk 'NR%4==2{c++; l+=length($0)}&#xD;&#xA;              END{&#xD;&#xA;                    print &quot;Number of reads: &quot;c; &#xD;&#xA;                    print &quot;Number of bases in reads: &quot;l&#xD;&#xA;                  }'&#xD;&#xA;&#xD;&#xA;The `zgrep .` will print non-blank lines from the input file and the `awk 'NR%4==2` will process every 4th line starting with the second (the sequence). &#xD;&#xA;This works fine, but can take a *very* long time when dealing with large files such as WGS data. Is there a tool I can use (on Linux) that will give me these values? Or, if not, I'm also open to suggestions for speeding up the above command.&#xD;&#xA;" />
  <row Id="2929" PostHistoryTypeId="1" PostId="935" RevisionGUID="e22f7c59-e55f-4c33-af06-9bfc794a1dda" CreationDate="2017-06-28T14:50:27.330" UserId="298" Text="Fast way to count number of reads and number of bases in a fastq file?" />
  <row Id="2930" PostHistoryTypeId="3" PostId="935" RevisionGUID="e22f7c59-e55f-4c33-af06-9bfc794a1dda" CreationDate="2017-06-28T14:50:27.330" UserId="298" Text="&lt;ngs&gt;&lt;fastq&gt;&lt;software-recommendation&gt;" />
  <row Id="2931" PostHistoryTypeId="2" PostId="936" RevisionGUID="5e5cbe96-9ff9-42c7-b41d-f0528f2d41b7" CreationDate="2017-06-28T15:22:24.697" UserId="29" Text="The following is more than twice as fast; however, `wc` counts newline characters as well. We thus need to subtract the line count from the base count:&#xD;&#xA;&#xD;&#xA;    fix_base_count() {&#xD;&#xA;        local counts=($(cat))&#xD;&#xA;        echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;    gunzip -c &quot;$file&quot; \&#xD;&#xA;        | awk 'NR % 4 == 2' \&#xD;&#xA;        | wc -cl \&#xD;&#xA;        | fix_base_count&#xD;&#xA;&#xD;&#xA;However, the character counts newline characters as well. We thus need to subtract the line count from it:&#xD;&#xA;&#xD;&#xA;All the caveats from Simon’s comment apply: this assumes the “simple” FASTQ format, where each record consists of exactly four lines. I think this is true for all files produced by Illumina sequencers and downstream tools." />
  <row Id="2932" PostHistoryTypeId="2" PostId="937" RevisionGUID="05df59a4-9839-4a2e-b49b-2b107ac6990e" CreationDate="2017-06-28T16:15:06.267" UserId="194" Text="It's difficult to get this to go massively quicker I think - as with [this question](https://bioinformatics.stackexchange.com/questions/361/what-is-the-fastest-way-to-calculate-the-number-of-unknown-nucleotides-in-fasta) working with large gzipped FASTQ files is mostly IO-bound. We could instead focus on making sure we are getting the *right* answer.&#xD;&#xA;&#xD;&#xA;People deride them too often, but this is where a well-written parser is worth it's weight in gold. Heng Li gives us this [FASTQ Parser in C](http://lh3lh3.users.sourceforge.net/parsefastq.shtml). &#xD;&#xA;&#xD;&#xA;I downloaded the [example tarball](http://lh3lh3.users.sourceforge.net/download/kseq.tar.bz2) and modified the example code (excuse my C...):&#xD;&#xA;&#xD;&#xA;    #include &lt;zlib.h&gt;&#xD;&#xA;    #include &lt;stdio.h&gt;&#xD;&#xA;    #include &quot;kseq.h&quot;&#xD;&#xA;    KSEQ_INIT(gzFile, gzread)&#xD;&#xA;    &#xD;&#xA;    int main(int argc, char *argv[])&#xD;&#xA;    {&#xD;&#xA;        gzFile fp;&#xD;&#xA;        kseq_t *seq;&#xD;&#xA;        int l;&#xD;&#xA;        if (argc == 1) {&#xD;&#xA;            fprintf(stderr, &quot;Usage: %s &lt;in.seq&gt;\n&quot;, argv[0]);&#xD;&#xA;            return 1;&#xD;&#xA;        }&#xD;&#xA;        fp = gzopen(argv[1], &quot;r&quot;);&#xD;&#xA;        seq = kseq_init(fp);&#xD;&#xA;        int seqcount = 0;&#xD;&#xA;        long seqlen = 0;&#xD;&#xA;        while ((l = kseq_read(seq)) &gt;= 0) {&#xD;&#xA;            seqcount = seqcount + 1;&#xD;&#xA;            seqlen = seqlen + (long)strlen(seq-&gt;seq.s);&#xD;&#xA;        }&#xD;&#xA;        kseq_destroy(seq);&#xD;&#xA;        gzclose(fp);&#xD;&#xA;        printf(&quot;Number of sequences: %d\n&quot;, seqcount);&#xD;&#xA;        printf(&quot;Number of bases in sequences: %ld\n&quot;, seqlen);&#xD;&#xA;        return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Then `make` and `kseq_test foo.fastq.gz`.&#xD;&#xA;&#xD;&#xA;For my example file (~35m reads of ~75bp) this took:&#xD;&#xA;&#xD;&#xA;    real    0m49.670s&#xD;&#xA;    user    0m49.364s&#xD;&#xA;    sys     0m0.304s&#xD;&#xA;&#xD;&#xA;Compared with your example:&#xD;&#xA;&#xD;&#xA;    real    0m43.616s&#xD;&#xA;    user    1m35.060s&#xD;&#xA;    sys     0m5.240s&#xD;&#xA;&#xD;&#xA;Konrad's solution (in my hands):&#xD;&#xA;&#xD;&#xA;    real    0m39.682s&#xD;&#xA;    user    1m11.900s&#xD;&#xA;    sys     0m5.112s&#xD;&#xA;&#xD;&#xA;(By the way, just zcat-ing the data file to /dev/null):&#xD;&#xA;&#xD;&#xA;    real    0m38.736s&#xD;&#xA;    user    0m38.356s&#xD;&#xA;    sys     0m0.308s&#xD;&#xA;&#xD;&#xA;So, I get pretty close in speed, but am likely to be more standards compliant. Also this solution gives you more flexibility with what you can do with the data.&#xD;&#xA;&#xD;&#xA;And my horrible C can almost certainly be optimised. " />
  <row Id="2933" PostHistoryTypeId="2" PostId="938" RevisionGUID="ea283c46-5888-4c15-8a8e-d0ad522cc8df" CreationDate="2017-06-28T16:15:13.173" UserId="1000" Text="You can indeed get this information from the UCSC Table Browser. Select knownGene as your primary table, make a filter, add knownCanonical to as a linked table to filter on, then in the free-form query section add &quot;1&quot; without the quotes. Then click submit and select Bed output, where you can choose &quot;exons plus&quot; as an output option. This will lead to output like the following:&#xD;&#xA;&#xD;&#xA;    chr1	17368	17436	uc031tla.1_exon_0_0_chr1_17369_r	0	-&#xD;&#xA;    chr1	29553	30039	uc057aty.1_exon_0_0_chr1_29554_f	0	+&#xD;&#xA;    chr1	30563	30667	uc057aty.1_exon_1_0_chr1_30564_f	0	+&#xD;&#xA;    chr1	30975	31097	uc057aty.1_exon_2_0_chr1_30976_f	0	+&#xD;&#xA;    chr1	30365	30503	uc031tlb.1_exon_0_0_chr1_30366_f	0	+&#xD;&#xA;    chr1	34553	35174	uc001aak.4_exon_0_0_chr1_34554_r	0	-&#xD;&#xA;    chr1	35276	35481	uc001aak.4_exon_1_0_chr1_35277_r	0	-&#xD;&#xA;    chr1	35720	36081	uc001aak.4_exon_2_0_chr1_35721_r	0	-&#xD;&#xA;For more step-by-step information, please check the answer from the mailing list archives here (in particular the Exon Method 1 section):&#xD;&#xA;https://groups.google.com/a/soe.ucsc.edu/d/msg/genome/BJ-6DlaZNCY/grgGIpuJAwAJ&#xD;&#xA;&#xD;&#xA;Also note that UCSC provides several mailing lists for support, if you have futher questions please send them there:&#xD;&#xA;&#xD;&#xA; - General questions: genome@soe.ucsc.edu &#xD;&#xA; - Questions involving private&#xD;&#xA;   data: genome-www@soe.ucsc.edu &#xD;&#xA; - Questions involving mirror sites:&#xD;&#xA;   genome-mirror@ose.ucsc.edu" />
  <row Id="2934" PostHistoryTypeId="5" PostId="937" RevisionGUID="d39baea3-f08f-448a-8ce8-953c1c1f0518" CreationDate="2017-06-28T16:20:09.807" UserId="194" Comment="added 29 characters in body" Text="It's difficult to get this to go massively quicker I think - as with [this question](https://bioinformatics.stackexchange.com/questions/361/what-is-the-fastest-way-to-calculate-the-number-of-unknown-nucleotides-in-fasta) working with large gzipped FASTQ files is mostly IO-bound. We could instead focus on making sure we are getting the *right* answer.&#xD;&#xA;&#xD;&#xA;People deride them too often, but this is where a well-written parser is worth it's weight in gold. Heng Li gives us this [FASTQ Parser in C](http://lh3lh3.users.sourceforge.net/parsefastq.shtml). &#xD;&#xA;&#xD;&#xA;I downloaded the [example tarball](http://lh3lh3.users.sourceforge.net/download/kseq.tar.bz2) and modified the example code (excuse my C...):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;    #include &lt;zlib.h&gt;&#xD;&#xA;    #include &lt;stdio.h&gt;&#xD;&#xA;    #include &quot;kseq.h&quot;&#xD;&#xA;    KSEQ_INIT(gzFile, gzread)&#xD;&#xA;    &#xD;&#xA;    int main(int argc, char *argv[])&#xD;&#xA;    {&#xD;&#xA;        gzFile fp;&#xD;&#xA;        kseq_t *seq;&#xD;&#xA;        int l;&#xD;&#xA;        if (argc == 1) {&#xD;&#xA;            fprintf(stderr, &quot;Usage: %s &lt;in.seq&gt;\n&quot;, argv[0]);&#xD;&#xA;            return 1;&#xD;&#xA;        }&#xD;&#xA;        fp = gzopen(argv[1], &quot;r&quot;);&#xD;&#xA;        seq = kseq_init(fp);&#xD;&#xA;        int seqcount = 0;&#xD;&#xA;        long seqlen = 0;&#xD;&#xA;        while ((l = kseq_read(seq)) &gt;= 0) {&#xD;&#xA;            seqcount = seqcount + 1;&#xD;&#xA;            seqlen = seqlen + (long)strlen(seq-&gt;seq.s);&#xD;&#xA;        }&#xD;&#xA;        kseq_destroy(seq);&#xD;&#xA;        gzclose(fp);&#xD;&#xA;        printf(&quot;Number of sequences: %d\n&quot;, seqcount);&#xD;&#xA;        printf(&quot;Number of bases in sequences: %ld\n&quot;, seqlen);&#xD;&#xA;        return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Then `make` and `kseq_test foo.fastq.gz`.&#xD;&#xA;&#xD;&#xA;For my example file (~35m reads of ~75bp) this took:&#xD;&#xA;&#xD;&#xA;    real    0m49.670s&#xD;&#xA;    user    0m49.364s&#xD;&#xA;    sys     0m0.304s&#xD;&#xA;&#xD;&#xA;Compared with your example:&#xD;&#xA;&#xD;&#xA;    real    0m43.616s&#xD;&#xA;    user    1m35.060s&#xD;&#xA;    sys     0m5.240s&#xD;&#xA;&#xD;&#xA;Konrad's solution (in my hands):&#xD;&#xA;&#xD;&#xA;    real    0m39.682s&#xD;&#xA;    user    1m11.900s&#xD;&#xA;    sys     0m5.112s&#xD;&#xA;&#xD;&#xA;(By the way, just zcat-ing the data file to /dev/null):&#xD;&#xA;&#xD;&#xA;    real    0m38.736s&#xD;&#xA;    user    0m38.356s&#xD;&#xA;    sys     0m0.308s&#xD;&#xA;&#xD;&#xA;So, I get pretty close in speed, but am likely to be more standards compliant. Also this solution gives you more flexibility with what you can do with the data.&#xD;&#xA;&#xD;&#xA;And my horrible C can almost certainly be optimised. " />
  <row Id="2935" PostHistoryTypeId="2" PostId="939" RevisionGUID="0138c61a-0f19-454a-8639-a68568890fa3" CreationDate="2017-06-28T17:56:42.243" UserId="506" Text="I have a matrix of gene counts which I'm going to use as input for DESeq.  Right now, each gene is labeled by it's ensemble transcript ID, but I'd like to convert these to their hgnc symbols before I input them into DESeq for analysis.  I'm attempting to do this conversion using biomaRt, however, this is my first time using the program, and I'm running into issues.  Here is my current code:&#xD;&#xA;&#xD;&#xA;    library(&quot;biomaRt&quot;)&#xD;&#xA;    mart &lt;- useMart(biomart = &quot;ENSEMBL_MART_ENSEMBL&quot;, dataset = &quot;mmusculus_gene_ensembl&quot;)&#xD;&#xA;    transcript.ids &lt;- rownames(txi.kallisto$counts)&#xD;&#xA;    hgnc_symbols &lt;- getBM(attributes = &quot;hgnc_symbol&quot;, filters = &quot;ensembl_transcript_id&quot;, values = transcript.ids, mart = mart)&#xD;&#xA;&#xD;&#xA;Printing hgnc_symbols shows that it isn't what I want:&#xD;&#xA;&#xD;&#xA;    &gt;ghnc_symbols&#xD;&#xA;    [1] hgnc_symbol&#xD;&#xA;    &lt;0 rows&gt; (or 0-length row.names)&#xD;&#xA;&#xD;&#xA;Lastly, here is what the input transcript IDs look like:&#xD;&#xA;&#xD;&#xA;    &gt; head(rownames(txi.kallisto$counts))&#xD;&#xA;    [1] &quot;ENSMUST00000178862.1&quot; &quot;ENSMUST00000178537.1&quot; &quot;ENSMUST00000196221.1&quot;&#xD;&#xA;    [4] &quot;ENSMUST00000179664.1&quot; &quot;ENSMUST00000177564.1&quot; &quot;ENSMUST00000179520.1&quot;" />
  <row Id="2936" PostHistoryTypeId="1" PostId="939" RevisionGUID="0138c61a-0f19-454a-8639-a68568890fa3" CreationDate="2017-06-28T17:56:42.243" UserId="506" Text="Trouble using biomaRt to retrieve hgnc symbols from ensembl transcript ids?" />
  <row Id="2937" PostHistoryTypeId="3" PostId="939" RevisionGUID="0138c61a-0f19-454a-8639-a68568890fa3" CreationDate="2017-06-28T17:56:42.243" UserId="506" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;deseq2&gt;&lt;biomart&gt;&lt;ensembl&gt;" />
  <row Id="2938" PostHistoryTypeId="2" PostId="940" RevisionGUID="9e77ca0d-2f1b-4551-beaf-b242433e81ff" CreationDate="2017-06-28T20:00:12.073" UserId="73" Text="I get fairly quick results with my [fastx-length.pl](https://github.com/gringer/bioinfscripts/blob/master/fastx-fetch.pl) script, with the added bonus of being able to handle multi-line FASTQ files and displaying additional read-length QC statistics:&#xD;&#xA;&#xD;&#xA;    time zcat albacored_all.fastq.gz | /bioinf/scripts/fastx-length.pl &gt; /dev/null&#xD;&#xA;    Total sequences: 301135&#xD;&#xA;    Total length: 283.902419 Mb&#xD;&#xA;    Longest sequence: 5.601 kb&#xD;&#xA;    Shortest sequence: 6 b&#xD;&#xA;    Mean Length: 942 b&#xD;&#xA;    Median Length: 999 b&#xD;&#xA;    N50: 111835 sequences; L50: 1.103 kb&#xD;&#xA;    N90: 245243 sequences; L90: 608 b&#xD;&#xA;    &#xD;&#xA;    real	0m8,802s&#xD;&#xA;    user	0m16,584s&#xD;&#xA;    sys	0m0,260s&#xD;&#xA;&#xD;&#xA;Versus the script you have provided:&#xD;&#xA;&#xD;&#xA;    zcat albacored_all.fastq.gz | awk 'NR%4==2{c++; l+=length($0)}&#xD;&#xA;              END{&#xD;&#xA;                    print &quot;Number of reads: &quot;c; &#xD;&#xA;                    print &quot;Number of bases in reads: &quot;l&#xD;&#xA;                  }'&#xD;&#xA;    Number of reads: 301135&#xD;&#xA;    Number of bases in reads: 283902419&#xD;&#xA;    &#xD;&#xA;    real	0m8,382s&#xD;&#xA;    user	0m10,216s&#xD;&#xA;    sys	0m0,332s&#xD;&#xA;&#xD;&#xA;Cat to `/dev/null` for comparison:&#xD;&#xA;&#xD;&#xA;    time zcat albacored_all.fastq.gz &gt; /dev/null&#xD;&#xA;    &#xD;&#xA;    real	0m7,877s&#xD;&#xA;    user	0m7,856s&#xD;&#xA;    sys	0m0,020s&#xD;&#xA;&#xD;&#xA;I suspect that something using [bioawk](https://github.com/lh3/bioawk) might be a bit faster (and similarly FASTQ-compliant)." />
  <row Id="2939" PostHistoryTypeId="2" PostId="941" RevisionGUID="7198b999-68e7-4b6d-82d1-a6920c2c2a1e" CreationDate="2017-06-28T21:16:04.060" UserId="48" Text="You need to specify the number without the version. Instead of &quot;ENSMUST00000178862.1&quot; just &quot;ENSMUST00000178862&quot;:&#xD;&#xA;&#xD;&#xA;You can do this with one more line:&#xD;&#xA;&#xD;&#xA;    g &lt;- gsub(&quot;\\..*&quot;, &quot;&quot;, rownames(txi.kallisto$counts))&#xD;&#xA;    (hgnc_symbols &lt;- getBM(attributes = c(&quot;hgnc_symbol&quot;, &quot;chromosome_name&quot;, &quot;ensembl_transcript_id&quot;), filters = &quot;ensembl_transcript_id&quot;, values = g, mart = mart))&#xD;&#xA;    ##  hgnc_symbol chromosome_name ensembl_transcript_id&#xD;&#xA;    ##1          NA              14    ENSMUST00000177564&#xD;&#xA;    ##2          NA               6    ENSMUST00000178537&#xD;&#xA;    ##3          NA               6    ENSMUST00000178862&#xD;&#xA;    ##4          NA              12    ENSMUST00000179520&#xD;&#xA;    ##5          NA              14    ENSMUST00000179664&#xD;&#xA;    ##6          NA              14    ENSMUST00000196221&#xD;&#xA;With these transcripts it doesn't seem that there is an equivalent hgnc_symbol. But is recognized, otherwise they are not recognized:&#xD;&#xA;&#xD;&#xA;    (hgnc_symbols &lt;- getBM(attributes = c(&quot;hgnc_symbol&quot;, &quot;chromosome_name&quot;, &quot;ensembl_transcript_id&quot;), filters = &quot;ensembl_transcript_id&quot;, values = rownames(txi.kallisto$counts), mart = mart))&#xD;&#xA;    ## [1] hgnc_symbol           chromosome_name       ensembl_transcript_id&#xD;&#xA;    ##&lt;0 rows&gt; (or 0-length row.names)&#xD;&#xA;" />
  <row Id="2940" PostHistoryTypeId="5" PostId="939" RevisionGUID="502671d7-960f-4316-a2b0-239fc8e73865" CreationDate="2017-06-28T21:40:49.913" UserId="298" Comment="Minor fixes" Text="I have a matrix of gene counts which I'm going to use as input for DESeq.  Right now, each gene is labeled by its Ensemble transcript ID, but I'd like to convert these to their HGNC symbols before I input them into DESeq for analysis. I'm attempting to do this conversion using biomaRt, however, this is my first time using the program, and I'm running into issues.  Here is my current code:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-R --&gt;&#xD;&#xA;&#xD;&#xA;    library(&quot;biomaRt&quot;)&#xD;&#xA;    mart &lt;- useMart(biomart = &quot;ENSEMBL_MART_ENSEMBL&quot;, dataset = &quot;mmusculus_gene_ensembl&quot;)&#xD;&#xA;    transcript.ids &lt;- rownames(txi.kallisto$counts)&#xD;&#xA;    hgnc_symbols &lt;- getBM(attributes = &quot;hgnc_symbol&quot;, filters = &quot;ensembl_transcript_id&quot;, values = transcript.ids, mart = mart)&#xD;&#xA;&#xD;&#xA;Printing hgnc_symbols shows that it isn't what I want:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-R --&gt;&#xD;&#xA;&#xD;&#xA;    &gt;ghnc_symbols&#xD;&#xA;    [1] hgnc_symbol&#xD;&#xA;    &lt;0 rows&gt; (or 0-length row.names)&#xD;&#xA;&#xD;&#xA;Lastly, here is what the input transcript IDs look like:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-R --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; head(rownames(txi.kallisto$counts))&#xD;&#xA;    [1] &quot;ENSMUST00000178862.1&quot; &quot;ENSMUST00000178537.1&quot; &quot;ENSMUST00000196221.1&quot;&#xD;&#xA;    [4] &quot;ENSMUST00000179664.1&quot; &quot;ENSMUST00000177564.1&quot; &quot;ENSMUST00000179520.1&quot;" />
  <row Id="2941" PostHistoryTypeId="4" PostId="939" RevisionGUID="502671d7-960f-4316-a2b0-239fc8e73865" CreationDate="2017-06-28T21:40:49.913" UserId="298" Comment="Minor fixes" Text="Trouble using biomaRt to retrieve hgnc symbols from Ensembl transcript ids" />
  <row Id="2942" PostHistoryTypeId="5" PostId="941" RevisionGUID="4afd6808-2069-4aa6-9ff9-b69cc34110d0" CreationDate="2017-06-29T07:09:36.770" UserId="48" Comment="added 647 characters in body" Text="You need to specify the number without the version. Instead of &quot;ENSMUST00000178862.1&quot; just &quot;ENSMUST00000178862&quot;:&#xD;&#xA;&#xD;&#xA;You can do this with one more line:&#xD;&#xA;&#xD;&#xA;    g &lt;- gsub(&quot;\\..*&quot;, &quot;&quot;, rownames(txi.kallisto$counts))&#xD;&#xA;    (hgnc_symbols &lt;- getBM(attributes = c(&quot;hgnc_symbol&quot;, &quot;chromosome_name&quot;, &quot;ensembl_transcript_id&quot;), filters = &quot;ensembl_transcript_id&quot;, values = g, mart = mart))&#xD;&#xA;    ##  hgnc_symbol chromosome_name ensembl_transcript_id&#xD;&#xA;    ##1          NA              14    ENSMUST00000177564&#xD;&#xA;    ##2          NA               6    ENSMUST00000178537&#xD;&#xA;    ##3          NA               6    ENSMUST00000178862&#xD;&#xA;    ##4          NA              12    ENSMUST00000179520&#xD;&#xA;    ##5          NA              14    ENSMUST00000179664&#xD;&#xA;    ##6          NA              14    ENSMUST00000196221&#xD;&#xA;With these transcripts it doesn't seem that there is an equivalent hgnc_symbol. But is recognized, otherwise they are not recognized:&#xD;&#xA;&#xD;&#xA;    (hgnc_symbols &lt;- getBM(attributes = c(&quot;hgnc_symbol&quot;, &quot;chromosome_name&quot;, &quot;ensembl_transcript_id&quot;), filters = &quot;ensembl_transcript_id&quot;, values = rownames(txi.kallisto$counts), mart = mart))&#xD;&#xA;    ## [1] hgnc_symbol           chromosome_name       ensembl_transcript_id&#xD;&#xA;    ##&lt;0 rows&gt; (or 0-length row.names)&#xD;&#xA;&#xD;&#xA;However you might be interested in doing something like [this][1]:&#xD;&#xA;&#xD;&#xA;    human = useMart(&quot;ensembl&quot;, dataset = &quot;hsapiens_gene_ensembl&quot;)&#xD;&#xA;    mouse = useMart(&quot;ensembl&quot;, dataset = &quot;mmusculus_gene_ensembl&quot;)&#xD;&#xA;    getLDS(attributes = &quot;ensembl_transcript_id&quot;,&#xD;&#xA;           filters = &quot;ensembl_transcript_id&quot;, values = g,mart = mouse,&#xD;&#xA;          attributesL = &quot;hgnc_symbol&quot;, martL = human)&#xD;&#xA;    ##&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/vignettes/biomaRt/inst/doc/biomaRt.html#given-the-human-gene-tp53-retrieve-the-human-chromosomal-location-of-this-gene-and-also-retrieve-the-chromosomal-location-and-refseq-id-of-its-homolog-in-mouse." />
  <row Id="2943" PostHistoryTypeId="5" PostId="941" RevisionGUID="1b307f06-33df-421f-b5c6-15ea87d1defe" CreationDate="2017-06-29T07:18:05.463" UserId="48" Comment="adding language highlithing " Text="You need to specify the number without the version. Instead of &quot;ENSMUST00000178862.1&quot; just &quot;ENSMUST00000178862&quot;:&#xD;&#xA;&#xD;&#xA;You can do this with one more line:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-R --&gt;&#xD;&#xA;&#xD;&#xA;    g &lt;- gsub(&quot;\\..*&quot;, &quot;&quot;, rownames(txi.kallisto$counts))&#xD;&#xA;    (hgnc_symbols &lt;- getBM(attributes = c(&quot;hgnc_symbol&quot;, &quot;chromosome_name&quot;, &quot;ensembl_transcript_id&quot;), filters = &quot;ensembl_transcript_id&quot;, values = g, mart = mart))&#xD;&#xA;    ##  hgnc_symbol chromosome_name ensembl_transcript_id&#xD;&#xA;    ##1          NA              14    ENSMUST00000177564&#xD;&#xA;    ##2          NA               6    ENSMUST00000178537&#xD;&#xA;    ##3          NA               6    ENSMUST00000178862&#xD;&#xA;    ##4          NA              12    ENSMUST00000179520&#xD;&#xA;    ##5          NA              14    ENSMUST00000179664&#xD;&#xA;    ##6          NA              14    ENSMUST00000196221&#xD;&#xA;With these transcripts it doesn't seem that there is an equivalent hgnc_symbol. But is recognized, otherwise they are not recognized:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-R --&gt;&#xD;&#xA;&#xD;&#xA;    (hgnc_symbols &lt;- getBM(attributes = c(&quot;hgnc_symbol&quot;, &quot;chromosome_name&quot;, &quot;ensembl_transcript_id&quot;), filters = &quot;ensembl_transcript_id&quot;, values = rownames(txi.kallisto$counts), mart = mart))&#xD;&#xA;    ## [1] hgnc_symbol           chromosome_name       ensembl_transcript_id&#xD;&#xA;    ##&lt;0 rows&gt; (or 0-length row.names)&#xD;&#xA;&#xD;&#xA;However you might be interested in doing something like [this][1]:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-R --&gt;&#xD;&#xA;&#xD;&#xA;    human = useMart(&quot;ensembl&quot;, dataset = &quot;hsapiens_gene_ensembl&quot;)&#xD;&#xA;    mouse = useMart(&quot;ensembl&quot;, dataset = &quot;mmusculus_gene_ensembl&quot;)&#xD;&#xA;    getLDS(attributes = &quot;ensembl_transcript_id&quot;,&#xD;&#xA;           filters = &quot;ensembl_transcript_id&quot;, values = g,mart = mouse,&#xD;&#xA;          attributesL = &quot;hgnc_symbol&quot;, martL = human)&#xD;&#xA;    ## The query to the BioMart webservice returned an invalid result: the number of &#xD;&#xA;    ## columns in the result table does not equal the number of &#xD;&#xA;    ## attributes in the query. Please report this to the mailing list.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/vignettes/biomaRt/inst/doc/biomaRt.html#given-the-human-gene-tp53-retrieve-the-human-chromosomal-location-of-this-gene-and-also-retrieve-the-chromosomal-location-and-refseq-id-of-its-homolog-in-mouse." />
  <row Id="2944" PostHistoryTypeId="2" PostId="942" RevisionGUID="865e8b0a-7b6a-4220-8312-4022b20f885c" CreationDate="2017-06-29T08:42:12.243" UserId="1008" Text="I have done de novo assembly of pair end raw read sequences and resulted transcripts sequences were separated based on coding potential into two categories: long non-coding RNA transcripts and coding RNA transcripts.&#xD;&#xA;&#xD;&#xA;I want to find how many of long non coding can make hair pin loop structure and how can I annotate these miRNA which are generated from long non coding RNA in plants." />
  <row Id="2945" PostHistoryTypeId="1" PostId="942" RevisionGUID="865e8b0a-7b6a-4220-8312-4022b20f885c" CreationDate="2017-06-29T08:42:12.243" UserId="1008" Text="How to find hair pin loop structure in a large set of long non coding RNA transcripts" />
  <row Id="2946" PostHistoryTypeId="3" PostId="942" RevisionGUID="865e8b0a-7b6a-4220-8312-4022b20f885c" CreationDate="2017-06-29T08:42:12.243" UserId="1008" Text="&lt;bioconductor&gt;" />
  <row Id="2947" PostHistoryTypeId="4" PostId="942" RevisionGUID="4ea3ccb2-2aca-4dc0-a692-0f5cd5c0b9cd" CreationDate="2017-06-29T08:48:18.753" UserId="1008" Comment="edited title" Text="How to find hairpin loop structure in a large set of long non coding RNA transcripts" />
  <row Id="2948" PostHistoryTypeId="2" PostId="943" RevisionGUID="3667cc28-3fbe-409f-bffc-4e04f032d520" CreationDate="2017-06-29T08:58:04.413" UserId="747" Text="I have the following mwe for filtering a Swissprot file based on a certain feature, in this case, transmembrane regions. &#xD;&#xA;  &#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    records=[]&#xD;&#xA;    for record in SeqIO.parse(&quot;Input.txt&quot;, &quot;swiss&quot;):&#xD;&#xA;        transmembrane_protein=False&#xD;&#xA;        print record.id&#xD;&#xA;        for i, feature in enumerate(record.features):&#xD;&#xA;            if feature.type == &quot;TRANSMEM&quot;:&#xD;&#xA;                transmembrane_protein=True&#xD;&#xA;        if transmembrane_protein==True:&#xD;&#xA;            records.append(record)&#xD;&#xA;    SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)&#xD;&#xA;&#xD;&#xA;However, such a method is not yet supported.  &#xD;&#xA;&#xD;&#xA;    ValueError: Reading format 'swiss' is supported, but not writing&#xD;&#xA;&#xD;&#xA;The script works when ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)``` becomes ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;fasta&quot;)```&#xD;&#xA;&#xD;&#xA;Is there any way of using biopython to write swissprot files from parsed swissprot files? According to the docs, [reading and writing are both supported for swiss](http://biopython.org/DIST/docs/api/Bio.SeqIO-module.html)." />
  <row Id="2949" PostHistoryTypeId="1" PostId="943" RevisionGUID="3667cc28-3fbe-409f-bffc-4e04f032d520" CreationDate="2017-06-29T08:58:04.413" UserId="747" Text="Is there any way of using biopython to write Swissprot files?" />
  <row Id="2950" PostHistoryTypeId="3" PostId="943" RevisionGUID="3667cc28-3fbe-409f-bffc-4e04f032d520" CreationDate="2017-06-29T08:58:04.413" UserId="747" Text="&lt;database&gt;&lt;biopython&gt;&lt;python&gt;" />
  <row Id="2951" PostHistoryTypeId="5" PostId="942" RevisionGUID="9787a32c-dd42-4724-991e-e0d83cd90075" CreationDate="2017-06-29T08:58:48.657" UserId="1008" Comment="deleted 1 character in body" Text="I have done de novo assembly of pair end raw read sequences and resulted transcripts sequences were separated based on coding potential into two categories: long non-coding RNA transcripts and coding RNA transcripts.&#xD;&#xA;&#xD;&#xA;I want to find how many of long non coding can make hairpin loop structure and how can I annotate these miRNA which are generated from long non coding RNA in plants." />
  <row Id="2952" PostHistoryTypeId="2" PostId="944" RevisionGUID="7aaa61fd-44a7-4945-b87c-e07ef73b816c" CreationDate="2017-06-29T09:05:43.780" UserId="652" Text="I hava implemented [seqtk_counts][1] using kseq.h from  [klib][2] &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/jameslz/seqtk_utils/blob/master/seqtk_counts&#xD;&#xA;  [2]: https://github.com/attractivechaos/klib" />
  <row Id="2953" PostHistoryTypeId="5" PostId="943" RevisionGUID="5e95a1d8-cf9a-44da-a107-693b756381f1" CreationDate="2017-06-29T09:16:16.467" UserId="747" Comment="deleted 8 characters in body" Text="I have the following mwe for filtering a Swissprot file based on a certain feature, in this case, transmembrane regions. &#xD;&#xA;  &#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    records=[]&#xD;&#xA;    for record in SeqIO.parse(&quot;Input.txt&quot;, &quot;swiss&quot;):&#xD;&#xA;        transmembrane_protein=False&#xD;&#xA;        print record.id&#xD;&#xA;        for i, feature in enumerate(record.features):&#xD;&#xA;            if feature.type == &quot;TRANSMEM&quot;:&#xD;&#xA;                transmembrane_protein=True&#xD;&#xA;        if transmembrane_protein==True:&#xD;&#xA;            records.append(record)&#xD;&#xA;    SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)&#xD;&#xA;&#xD;&#xA;However, such a method is not yet supported.  &#xD;&#xA;&#xD;&#xA;    ValueError: Reading format 'swiss' is supported, but not writing&#xD;&#xA;&#xD;&#xA;The script works when ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)``` becomes ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;fasta&quot;)```&#xD;&#xA;&#xD;&#xA;Is there any way of using biopython to write swissprot files from parsed swissprot files? Nothing in the docs suggests [writing is not supported for swiss](http://biopython.org/DIST/docs/api/Bio.SeqIO-module.html)." />
  <row Id="2954" PostHistoryTypeId="5" PostId="937" RevisionGUID="308786d2-8c79-4e3b-b240-be4c5cfb7fd6" CreationDate="2017-06-29T09:25:40.987" UserId="194" Comment="added 383 characters in body" Text="It's difficult to get this to go massively quicker I think - as with [this question](https://bioinformatics.stackexchange.com/questions/361/what-is-the-fastest-way-to-calculate-the-number-of-unknown-nucleotides-in-fasta) working with large gzipped FASTQ files is mostly IO-bound. We could instead focus on making sure we are getting the *right* answer.&#xD;&#xA;&#xD;&#xA;People deride them too often, but this is where a well-written parser is worth it's weight in gold. Heng Li gives us this [FASTQ Parser in C](http://lh3lh3.users.sourceforge.net/parsefastq.shtml). &#xD;&#xA;&#xD;&#xA;I downloaded the [example tarball](http://lh3lh3.users.sourceforge.net/download/kseq.tar.bz2) and modified the example code (excuse my C...):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;    #include &lt;zlib.h&gt;&#xD;&#xA;    #include &lt;stdio.h&gt;&#xD;&#xA;    #include &quot;kseq.h&quot;&#xD;&#xA;    KSEQ_INIT(gzFile, gzread)&#xD;&#xA;    &#xD;&#xA;    int main(int argc, char *argv[])&#xD;&#xA;    {&#xD;&#xA;        gzFile fp;&#xD;&#xA;        kseq_t *seq;&#xD;&#xA;        int l;&#xD;&#xA;        if (argc == 1) {&#xD;&#xA;            fprintf(stderr, &quot;Usage: %s &lt;in.seq&gt;\n&quot;, argv[0]);&#xD;&#xA;            return 1;&#xD;&#xA;        }&#xD;&#xA;        fp = gzopen(argv[1], &quot;r&quot;);&#xD;&#xA;        seq = kseq_init(fp);&#xD;&#xA;        int seqcount = 0;&#xD;&#xA;        long seqlen = 0;&#xD;&#xA;        while ((l = kseq_read(seq)) &gt;= 0) {&#xD;&#xA;            seqcount = seqcount + 1;&#xD;&#xA;            seqlen = seqlen + (long)strlen(seq-&gt;seq.s);&#xD;&#xA;        }&#xD;&#xA;        kseq_destroy(seq);&#xD;&#xA;        gzclose(fp);&#xD;&#xA;        printf(&quot;Number of sequences: %d\n&quot;, seqcount);&#xD;&#xA;        printf(&quot;Number of bases in sequences: %ld\n&quot;, seqlen);&#xD;&#xA;        return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Then `make` and `kseq_test foo.fastq.gz`.&#xD;&#xA;&#xD;&#xA;For my example file (~35m reads of ~75bp) this took:&#xD;&#xA;&#xD;&#xA;    real    0m49.670s&#xD;&#xA;    user    0m49.364s&#xD;&#xA;    sys     0m0.304s&#xD;&#xA;&#xD;&#xA;Compared with your example:&#xD;&#xA;&#xD;&#xA;    real    0m43.616s&#xD;&#xA;    user    1m35.060s&#xD;&#xA;    sys     0m5.240s&#xD;&#xA;&#xD;&#xA;Konrad's solution (in my hands):&#xD;&#xA;&#xD;&#xA;    real    0m39.682s&#xD;&#xA;    user    1m11.900s&#xD;&#xA;    sys     0m5.112s&#xD;&#xA;&#xD;&#xA;(By the way, just zcat-ing the data file to /dev/null):&#xD;&#xA;&#xD;&#xA;    real    0m38.736s&#xD;&#xA;    user    0m38.356s&#xD;&#xA;    sys     0m0.308s&#xD;&#xA;&#xD;&#xA;So, I get pretty close in speed, but am likely to be more standards compliant. Also this solution gives you more flexibility with what you can do with the data.&#xD;&#xA;&#xD;&#xA;And my horrible C can almost certainly be optimised.&#xD;&#xA;&#xD;&#xA;**EDIT**:&#xD;&#xA;&#xD;&#xA;Same test, with `kseq.h` from Github, as suggested in the comments:&#xD;&#xA;&#xD;&#xA;My machine is under different load this morning, so I've retested. Wall clock times:&#xD;&#xA;&#xD;&#xA;OP: 0m44.813s&#xD;&#xA;&#xD;&#xA;Konrad: 0m40.061s&#xD;&#xA;&#xD;&#xA;zcat &gt; /dev/null: 0m34.508s&#xD;&#xA;&#xD;&#xA;kseq.h (Github): 0m32.909s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So most recent version of `kseq.h` is faster than simply zcat-ing the file (consistently in my tests...).&#xD;&#xA;" />
  <row Id="2955" PostHistoryTypeId="5" PostId="943" RevisionGUID="d6f4a37c-86a0-4387-9fa3-4fe5dd8a0654" CreationDate="2017-06-29T09:27:57.060" UserId="747" Comment="added 70 characters in body" Text="I have the following mwe for filtering a Swissprot file based on a certain feature, in this case, transmembrane regions. &#xD;&#xA;  &#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    records=[]&#xD;&#xA;    for record in SeqIO.parse(&quot;Input.txt&quot;, &quot;swiss&quot;):&#xD;&#xA;        transmembrane_protein=False&#xD;&#xA;        print record.id&#xD;&#xA;        for i, feature in enumerate(record.features):&#xD;&#xA;            if feature.type == &quot;TRANSMEM&quot;:&#xD;&#xA;                transmembrane_protein=True&#xD;&#xA;        if transmembrane_protein==True:&#xD;&#xA;            records.append(record)&#xD;&#xA;    SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)&#xD;&#xA;&#xD;&#xA;However, such a method is not yet supported.  &#xD;&#xA;&#xD;&#xA;    ValueError: Reading format 'swiss' is supported, but not writing&#xD;&#xA;&#xD;&#xA;The script works when ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)``` becomes ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;fasta&quot;)```&#xD;&#xA;&#xD;&#xA;Is there any unofficial way of using biopython to write swissprot files from parsed swissprot files? [Writing is not supported for swiss](http://biopython.org/DIST/docs/api/Bio.SeqIO-module.html):&#xD;&#xA;&#xD;&#xA;&gt;Note that while Bio.SeqIO can read all the above file formats, it cannot write to all of them." />
  <row Id="2956" PostHistoryTypeId="5" PostId="937" RevisionGUID="3a48d89f-6137-4b5b-9502-faa88614c584" CreationDate="2017-06-29T09:28:15.997" UserId="298" Comment="Please don't add &quot;Edit&quot; to your questions, just edit them so that anyone who reads them for the first time can understand them." Text="It's difficult to get this to go massively quicker I think - as with [this question](https://bioinformatics.stackexchange.com/questions/361/what-is-the-fastest-way-to-calculate-the-number-of-unknown-nucleotides-in-fasta) working with large gzipped FASTQ files is mostly IO-bound. We could instead focus on making sure we are getting the *right* answer.&#xD;&#xA;&#xD;&#xA;People deride them too often, but this is where a well-written parser is worth it's weight in gold. Heng Li gives us this [FASTQ Parser in C](http://lh3lh3.users.sourceforge.net/parsefastq.shtml). &#xD;&#xA;&#xD;&#xA;I downloaded the [example tarball](http://lh3lh3.users.sourceforge.net/download/kseq.tar.bz2) and modified the example code (excuse my C...):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-c --&gt;&#xD;&#xA;&#xD;&#xA;    #include &lt;zlib.h&gt;&#xD;&#xA;    #include &lt;stdio.h&gt;&#xD;&#xA;    #include &quot;kseq.h&quot;&#xD;&#xA;    KSEQ_INIT(gzFile, gzread)&#xD;&#xA;    &#xD;&#xA;    int main(int argc, char *argv[])&#xD;&#xA;    {&#xD;&#xA;        gzFile fp;&#xD;&#xA;        kseq_t *seq;&#xD;&#xA;        int l;&#xD;&#xA;        if (argc == 1) {&#xD;&#xA;            fprintf(stderr, &quot;Usage: %s &lt;in.seq&gt;\n&quot;, argv[0]);&#xD;&#xA;            return 1;&#xD;&#xA;        }&#xD;&#xA;        fp = gzopen(argv[1], &quot;r&quot;);&#xD;&#xA;        seq = kseq_init(fp);&#xD;&#xA;        int seqcount = 0;&#xD;&#xA;        long seqlen = 0;&#xD;&#xA;        while ((l = kseq_read(seq)) &gt;= 0) {&#xD;&#xA;            seqcount = seqcount + 1;&#xD;&#xA;            seqlen = seqlen + (long)strlen(seq-&gt;seq.s);&#xD;&#xA;        }&#xD;&#xA;        kseq_destroy(seq);&#xD;&#xA;        gzclose(fp);&#xD;&#xA;        printf(&quot;Number of sequences: %d\n&quot;, seqcount);&#xD;&#xA;        printf(&quot;Number of bases in sequences: %ld\n&quot;, seqlen);&#xD;&#xA;        return 0;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Then `make` and `kseq_test foo.fastq.gz`.&#xD;&#xA;&#xD;&#xA;For my example file (~35m reads of ~75bp) this took:&#xD;&#xA;&#xD;&#xA;    real    0m49.670s&#xD;&#xA;    user    0m49.364s&#xD;&#xA;    sys     0m0.304s&#xD;&#xA;&#xD;&#xA;Compared with your example:&#xD;&#xA;&#xD;&#xA;    real    0m43.616s&#xD;&#xA;    user    1m35.060s&#xD;&#xA;    sys     0m5.240s&#xD;&#xA;&#xD;&#xA;Konrad's solution (in my hands):&#xD;&#xA;&#xD;&#xA;    real    0m39.682s&#xD;&#xA;    user    1m11.900s&#xD;&#xA;    sys     0m5.112s&#xD;&#xA;&#xD;&#xA;(By the way, just zcat-ing the data file to /dev/null):&#xD;&#xA;&#xD;&#xA;    real    0m38.736s&#xD;&#xA;    user    0m38.356s&#xD;&#xA;    sys     0m0.308s&#xD;&#xA;&#xD;&#xA;So, I get pretty close in speed, but am likely to be more standards compliant. Also this solution gives you more flexibility with what you can do with the data.&#xD;&#xA;&#xD;&#xA;And my horrible C can almost certainly be optimised.&#xD;&#xA;&#xD;&#xA;----&#xD;&#xA;&#xD;&#xA;Same test, with `kseq.h` from Github, as suggested in the comments:&#xD;&#xA;&#xD;&#xA;My machine is under different load this morning, so I've retested. Wall clock times:&#xD;&#xA;&#xD;&#xA;OP: 0m44.813s&#xD;&#xA;&#xD;&#xA;Konrad: 0m40.061s&#xD;&#xA;&#xD;&#xA;`zcat &gt; /dev/null`: 0m34.508s&#xD;&#xA;&#xD;&#xA;kseq.h (Github): 0m32.909s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So most recent version of `kseq.h` is faster than simply zcat-ing the file (consistently in my tests...).&#xD;&#xA;" />
  <row Id="2957" PostHistoryTypeId="5" PostId="944" RevisionGUID="3630915f-10e2-4252-b71a-746874fff431" CreationDate="2017-06-29T09:33:47.703" UserId="652" Comment="added 945 characters in body" Text="I hava implemented [seqtk_counts][1] using kseq.h from  [klib][2] &#xD;&#xA;&#xD;&#xA;Just a few line of Codes:&#xD;&#xA;&#xD;&#xA;    #include &lt;stdio.h&gt;&#xD;&#xA;    #include &lt;zlib.h&gt;&#xD;&#xA;    #include &quot;kseq.h&quot;&#xD;&#xA;&#xD;&#xA;    KSEQ_INIT(gzFile, gzread)&#xD;&#xA;&#xD;&#xA;    int main(int argc, char *argv[]){&#xD;&#xA;    &#xD;&#xA;    gzFile fp;&#xD;&#xA;    kseq_t *seq;&#xD;&#xA;    &#xD;&#xA;    int l = 0;&#xD;&#xA;&#xD;&#xA;    int64_t total = 0;&#xD;&#xA;    int64_t lines = 0;&#xD;&#xA;&#xD;&#xA;    if (argc == 1) {&#xD;&#xA;        fprintf(stderr, &quot;Usage: %s &lt;fastq&gt; &lt;sample&gt;\n&quot;, argv[0]);&#xD;&#xA;&#xD;&#xA;        return 1;&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    fp = strcmp(argv[1], &quot;-&quot;)? gzopen(argv[1], &quot;r&quot;) : gzdopen(fileno(stdin), &quot;r&quot;);&#xD;&#xA;    seq = kseq_init(fp);&#xD;&#xA;    &#xD;&#xA;    while ((l = kseq_read(seq)) &gt;= 0){&#xD;&#xA;        total += seq-&gt;seq.l;&#xD;&#xA;        lines += 1;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;    printf(&quot;%s\t%lld\t%lld\n&quot;, argv[2] ,(long long)lines, (long long)total);&#xD;&#xA;    kseq_destroy(seq);&#xD;&#xA;    gzclose(fp);&#xD;&#xA;    return 0;&#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;Compile it:&#xD;&#xA;&#xD;&#xA;    gcc  -O2  seqtk_counts.c  -o  seqtk_counts  -Iklib  -lz&#xD;&#xA;&#xD;&#xA;Usage:&#xD;&#xA;   &#xD;&#xA;    seqtk_counts foo.fasq.gz foo &#xD;&#xA;    or&#xD;&#xA;    cat foo.fasq.gz | seqtk_counts  - foo&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/jameslz/seqtk_utils/blob/master/seqtk_counts&#xD;&#xA;  [2]: https://github.com/attractivechaos/klib" />
  <row Id="2958" PostHistoryTypeId="5" PostId="943" RevisionGUID="825473ee-cfdf-426d-9d4a-54ddc704f31a" CreationDate="2017-06-29T09:54:39.873" UserId="747" Comment="added 7 characters in body" Text="I have the following mwe for filtering a Swissprot file based on a certain feature, in this case, transmembrane regions. &#xD;&#xA;  &#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    records=[]&#xD;&#xA;    for record in SeqIO.parse(&quot;Input.txt&quot;, &quot;swiss&quot;):&#xD;&#xA;        transmembrane_protein=False&#xD;&#xA;        print record.id&#xD;&#xA;        for i, feature in enumerate(record.features):&#xD;&#xA;            if feature.type == &quot;TRANSMEM&quot;:&#xD;&#xA;                transmembrane_protein=True&#xD;&#xA;        if transmembrane_protein==True:&#xD;&#xA;            records.append(record)&#xD;&#xA;    SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)&#xD;&#xA;&#xD;&#xA;However, such a method is not yet supported.  &#xD;&#xA;&#xD;&#xA;    ValueError: Reading format 'swiss' is supported, but not writing&#xD;&#xA;&#xD;&#xA;The script works when ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)``` becomes ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;fasta&quot;)```&#xD;&#xA;&#xD;&#xA;Is there any unofficial way of using biopython/python to write swissprot files from parsed swissprot files? [Writing is not supported for swiss](http://biopython.org/DIST/docs/api/Bio.SeqIO-module.html):&#xD;&#xA;&#xD;&#xA;&gt;Note that while Bio.SeqIO can read all the above file formats, it cannot write to all of them." />
  <row Id="2959" PostHistoryTypeId="2" PostId="945" RevisionGUID="9fc01493-dc08-4a0b-9aac-e4416b35c11e" CreationDate="2017-06-29T10:21:41.013" UserId="73" Text="I'm working on a human genome project, trying to find variants that are unique to one person, but not to three other relatives of that person. Unfortunately, one of our regions of interest contains a long tandem repeat region (30kb, repeated 3 times in tandem). I'd like to be able to properly map these regions with WGS reads, but suspect that I'm coming up against a mapping preference problem with my current methods (Bowtie2 / HISAT2):&#xD;&#xA;&#xD;&#xA;1. Read A1 can potentially map to three different locations near the same locus in the genome (L1, L2, L3)&#xD;&#xA;2. L1 is most similar to A1, so is preferentially mapped. Because this similarity is greater than that of L2 or L3, the read is treated as a unique mapping&#xD;&#xA;3. Most other reads are similarly mapped to L1, possibly because it is the locus that has had the most correction applied to it in the reference genome&#xD;&#xA;4. An analysis of the mapping results indicates huge deletions at L2 and L3&#xD;&#xA;&#xD;&#xA;Is BWA any more resistant to this issue? If so, how can I get it to report randomly one of the three mappable loci (assuming they all appear in the results)?&#xD;&#xA;&#xD;&#xA;Are there any other ways I can deal with this issue of distributing mapped reads throughout tandem repeats?" />
  <row Id="2960" PostHistoryTypeId="1" PostId="945" RevisionGUID="9fc01493-dc08-4a0b-9aac-e4416b35c11e" CreationDate="2017-06-29T10:21:41.013" UserId="73" Text="How can I map short reads to large tandem repeat regions?" />
  <row Id="2961" PostHistoryTypeId="3" PostId="945" RevisionGUID="9fc01493-dc08-4a0b-9aac-e4416b35c11e" CreationDate="2017-06-29T10:21:41.013" UserId="73" Text="&lt;read-mapping&gt;&lt;bwa&gt;&lt;repeat-elements&gt;&lt;bowtie2&gt;" />
  <row Id="2962" PostHistoryTypeId="5" PostId="943" RevisionGUID="78461edc-9d31-426a-b93c-34297a2456ff" CreationDate="2017-06-29T10:26:09.477" UserId="747" Comment="added 2 characters in body" Text="I have the following mwe for filtering a Swissprot file based on a certain feature, in this case, transmembrane regions. &#xD;&#xA;  &#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    records=[]&#xD;&#xA;    for record in SeqIO.parse(&quot;Input.txt&quot;, &quot;swiss&quot;):&#xD;&#xA;        transmembrane_protein=False&#xD;&#xA;        print record.id&#xD;&#xA;        for i, feature in enumerate(record.features):&#xD;&#xA;            if feature.type == &quot;TRANSMEM&quot;:&#xD;&#xA;                transmembrane_protein=True&#xD;&#xA;        if transmembrane_protein==True:&#xD;&#xA;            records.append(record)&#xD;&#xA;    SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)&#xD;&#xA;&#xD;&#xA;However, such a method is not yet supported.  &#xD;&#xA;&#xD;&#xA;    ValueError: Reading format 'swiss' is supported, but not writing&#xD;&#xA;&#xD;&#xA;The script works when ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)``` becomes ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;fasta&quot;)```&#xD;&#xA;[Writing is not supported for swiss](http://biopython.org/DIST/docs/api/Bio.SeqIO-module.html):&#xD;&#xA;&#xD;&#xA;&gt;Note that while Bio.SeqIO can read all the above file formats, it cannot write to all of them.&#xD;&#xA;&#xD;&#xA;Is there any unofficial way of using biopython/python to write swissprot files from parsed swissprot files? " />
  <row Id="2963" PostHistoryTypeId="2" PostId="946" RevisionGUID="19c8f8e2-766b-4e33-bb45-58c566ad8bac" CreationDate="2017-06-29T11:14:06.497" UserId="194" Text="Using `SeqIO.index` rather than `SeqIO.parse` lets you read all the records into a `dict`, from which you can then extract the raw entry:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;    &#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;&#xD;&#xA;    record_dict = SeqIO.index('Input.txt', 'swiss')&#xD;&#xA;    for key in record_dict:&#xD;&#xA;        print(record_dict.get_raw(key).decode())&#xD;&#xA;&#xD;&#xA;Now you should be able to apply your test for a transmembrane protein to each entry, and write out only the ones you want to keep.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;    &#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;&#xD;&#xA;    record_dict = SeqIO.index('Input.txt', 'swiss')&#xD;&#xA;    out = open('Output.txt', 'wb')&#xD;&#xA;    for key in record_dict:&#xD;&#xA;        record = record_dict[key]&#xD;&#xA;        for i, feature in enumerate(record.features):&#xD;&#xA;        if feature.type == &quot;TRANSMEM&quot;:&#xD;&#xA;            out.write(record_dict.get_raw(key).decode())" />
  <row Id="2964" PostHistoryTypeId="2" PostId="947" RevisionGUID="6974d77c-4eec-4567-bb7e-e6f842fcf30c" CreationDate="2017-06-29T11:26:35.603" UserId="298" Text="I don't know how to do this with biopython, but it's simple enough in Perl using [Swisskinfe][1]:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-perl --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env perl&#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    use SWISS::Entry;&#xD;&#xA;    &#xD;&#xA;    my $flat=$ARGV[0]||die(&quot;Need a flat file\n&quot;);&#xD;&#xA;    &#xD;&#xA;    ##############&#xD;&#xA;    ## Change the line termination string so we read an &#xD;&#xA;    ## entire entry at a time&#xD;&#xA;    ##############&#xD;&#xA;    local $/ = &quot;\n//\n&quot;;&#xD;&#xA;    my $fh;&#xD;&#xA;    if($flat=~/\.gz$/){&#xD;&#xA;        open($fh,&quot;zcat $flat |&quot;)|| die(&quot;cannot open flat file $flat : $! \n&quot;);&#xD;&#xA;    }&#xD;&#xA;    else{&#xD;&#xA;        open($fh,&quot;$flat&quot;)|| die(&quot;cannot open flat file $flat : $!: $@\n&quot;);&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    while (&lt;$fh&gt;) {&#xD;&#xA;    	##############&#xD;&#xA;    	## Read in all the entries and fill %entries &#xD;&#xA;    	##############&#xD;&#xA;    	my $entry = SWISS::Entry-&gt;fromText($_);&#xD;&#xA;    	my $id=${$entry-&gt;IDs-&gt;list}[0];&#xD;&#xA;    	my $wantRecord = 0;&#xD;&#xA;    	foreach my $feature ($entry-&gt;FTs-&gt;elements()) {&#xD;&#xA;          if ($feature-&gt;[0] eq 'TRANSMEM') {&#xD;&#xA;    			$wantRecord = 1;&#xD;&#xA;    			last;&#xD;&#xA;    		}&#xD;&#xA;    	}&#xD;&#xA;    	if ($wantRecord == 1) {&#xD;&#xA;    		print;&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;    			&#xD;&#xA;You need to install the Swissknife module first:&#xD;&#xA;&#xD;&#xA;1. Download Swissknife_1.73.tar.gz from [here](https://sourceforge.net/projects/swissknife/files/latest/download) (SourceForge link).&#xD;&#xA;&#xD;&#xA;2. Install&#xD;&#xA;&#xD;&#xA;        tar xvzf Swissknife.tar.gz&#xD;&#xA;        cd Swissknife_1.73/&#xD;&#xA;        perl Makefile.PL&#xD;&#xA;        sudo make install&#xD;&#xA;&#xD;&#xA;Now, make the script above executable and run with:&#xD;&#xA;&#xD;&#xA;    foo.pl file.flat&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://swissknife.sourceforge.net/&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2965" PostHistoryTypeId="5" PostId="947" RevisionGUID="f8f07d45-1570-42a8-89e3-1e3263a5f7cf" CreationDate="2017-06-29T11:33:38.093" UserId="298" Comment="added 81 characters in body" Text="I don't know how to do this with biopython, but it's simple enough in Perl using [Swisskinfe][1]:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-perl --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env perl&#xD;&#xA;    use strict;&#xD;&#xA;    use warnings;&#xD;&#xA;    use SWISS::Entry;&#xD;&#xA;    &#xD;&#xA;    my $flat=$ARGV[0]||die(&quot;Need a flat file\n&quot;);&#xD;&#xA;    &#xD;&#xA;    ## Change the line termination string so we read an &#xD;&#xA;    ## entire entry at a time&#xD;&#xA;    local $/ = &quot;\n//\n&quot;;&#xD;&#xA;    &#xD;&#xA;    my $fh;&#xD;&#xA;    ## Deal with compressed or uncompressed files&#xD;&#xA;    if($flat=~/\.gz$/){&#xD;&#xA;        open($fh,&quot;zcat $flat |&quot;)|| die(&quot;cannot open flat file $flat : $! \n&quot;);&#xD;&#xA;    }&#xD;&#xA;    else{&#xD;&#xA;        open($fh,&quot;$flat&quot;)|| die(&quot;cannot open flat file $flat : $!: $@\n&quot;);&#xD;&#xA;    }&#xD;&#xA;    ## Parse the file. &#xD;&#xA;    entry: while (my $current_entry = &lt;$fh&gt;) {&#xD;&#xA;    	## Read the current entry as an &quot;entry&quot; object&#xD;&#xA;    	my $entry = SWISS::Entry-&gt;fromText($current_entry);&#xD;&#xA;    	## Iterate over the entry's features&#xD;&#xA;    	foreach my $feature ($entry-&gt;FTs-&gt;elements()) {&#xD;&#xA;    		## If any of the features are 'TRANSMEM', print the entry&#xD;&#xA;    		if ($feature-&gt;[0] eq 'TRANSMEM') {&#xD;&#xA;    			print &quot;$current_entry\n&quot;;&#xD;&#xA;    			## Skip to the next entry&#xD;&#xA;    			next entry;&#xD;&#xA;    		}&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;        			&#xD;&#xA;You need to install the Swissknife module first:&#xD;&#xA;&#xD;&#xA;1. Download Swissknife_1.73.tar.gz from [here](https://sourceforge.net/projects/swissknife/files/latest/download) (SourceForge link).&#xD;&#xA;&#xD;&#xA;2. Install&#xD;&#xA;&#xD;&#xA;        tar xvzf Swissknife.tar.gz&#xD;&#xA;        cd Swissknife_1.73/&#xD;&#xA;        perl Makefile.PL&#xD;&#xA;        sudo make install&#xD;&#xA;&#xD;&#xA;Now, make the script above executable and run with:&#xD;&#xA;&#xD;&#xA;    foo.pl file.flat&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://swissknife.sourceforge.net/&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2966" PostHistoryTypeId="5" PostId="920" RevisionGUID="dbcd63ec-ccc8-48f8-a8c8-7dbc9f0f983c" CreationDate="2017-06-29T13:17:57.643" UserId="982" Comment="added 422 characters in body" Text="I want to compare two phylogenies and colour the association lines based on some metadata I have. I have been using ape cophyloplot but I have not had any success in getting the lines to colour accurately according to my data ([see previous question][1]).&#xD;&#xA;&#xD;&#xA;Note that in my actual work flow I define the colour scheme using a palette to control the colour outcome.&#xD;&#xA;&#xD;&#xA;I want a means to make a tangle using phylogenies which I can format. Preferably in R. I like to get an output like this: ![][2]&#xD;&#xA;&#xD;&#xA;  [1]: https://stackoverflow.com/questions/44006500/r-coloured-lines-for-tangelgram-package-ape-function-cophyloplot&#xD;&#xA;  [2]: https://i.stack.imgur.com/HTDfR.jpg&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**UPDATE**&#xD;&#xA;So my answer below is correct on how to use metadata to colour association lines. But for whatever reason I cannot correctly extract the order of my tip labels for my phylogeny, there is some slight discrepancy. which I am unsure to how to address. If anyone has tips on how to extract tip label order please let me know. &#xD;&#xA;&#xD;&#xA;The answer and the links below works for examples I done with `rtree`.  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2968" PostHistoryTypeId="2" PostId="949" RevisionGUID="412cfb29-119d-4f66-999d-e176a7dc7d98" CreationDate="2017-06-29T15:45:09.317" UserId="1012" Text="I have the genotyped data from impute2 output in .gen format (imputed to 1000G P3). The file has genotype posterior probabilities (GP:3 values per variant). I have converted .gen to .vcf using qctools and the .vcf file has GT:GP format. I need to convert the .vcf file with GT:GP format to GT:DS. Genotype dosages are recommended for use in qtltools/fastqtl analysis. However, I cannot find a tool that would keep the .vcf format and convert GP to DS. Any help much appreciated!&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2969" PostHistoryTypeId="1" PostId="949" RevisionGUID="412cfb29-119d-4f66-999d-e176a7dc7d98" CreationDate="2017-06-29T15:45:09.317" UserId="1012" Text="How to convert the .vcf (imputed) file with GT:GP format to GT:DS?" />
  <row Id="2970" PostHistoryTypeId="3" PostId="949" RevisionGUID="412cfb29-119d-4f66-999d-e176a7dc7d98" CreationDate="2017-06-29T15:45:09.317" UserId="1012" Text="&lt;file-formats&gt;&lt;vcf&gt;&lt;format-conversion&gt;&lt;impute2&gt;" />
  <row Id="2971" PostHistoryTypeId="5" PostId="931" RevisionGUID="eb79a50b-74ee-4ed5-a951-75651eac9324" CreationDate="2017-06-29T15:48:38.730" UserId="982" Comment="deleted 124 characters in body" Text="Thanks everyone for the suggestions @b.nota answer is useful to colour according to clade groups but does not address my actual question on using metadata to colour the lines. For this answer see below:&#xD;&#xA;&#xD;&#xA;dendextend states for `color_lines`&#xD;&#xA;a vector of colors for the lines connected the labels. If the colors are shorter than the number of labels, they are recycled (and a warning is issued). The colors in the vector are applied on the lines from the bottom up.&#xD;&#xA; &#xD;&#xA;    # Extract labels from dendrogram on the left&#xD;&#xA;    labels &lt;- dendA %&gt;% set(&quot;labels_to_char&quot;) %&gt;% labels &#xD;&#xA;    &#xD;&#xA;    #Using a metadata table with colours create a vector of colours&#xD;&#xA;    labels &lt;- as.data.frame(labels)&#xD;&#xA;    labels2 &lt;- merge(labels, metadata, by.x=&quot;labels&quot;, by.y=&quot;Sample.name&quot;, sort=F)&#xD;&#xA;    cols &lt;- as.character(labels2$Colours) &#xD;&#xA;    &#xD;&#xA;    # Make tanglegram&#xD;&#xA;    tanglegram(dendA, dendC, color_lines = cols)&#xD;&#xA;&#xD;&#xA;Side note if anyone with a phylogeny needs to convert it to a dendrogram to try this on their data see [my other question][2]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://stackoverflow.com/questions/44006500/coloured-lines-for-tangelgram-package-ape-function-cophyloplot&#xD;&#xA;  [2]: https://bioinformatics.stackexchange.com/questions/923/how-to-convert-a-phylogeny-to-a-dendrogram-in-r/930?noredirect=1#comment1714_930" />
  <row Id="2972" PostHistoryTypeId="5" PostId="920" RevisionGUID="fed49aed-1fad-47bc-bf94-2ba132fdb919" CreationDate="2017-06-29T15:49:07.763" UserId="982" Comment="deleted 409 characters in body" Text="I want to compare two phylogenies and colour the association lines based on some metadata I have. I have been using ape cophyloplot but I have not had any success in getting the lines to colour accurately according to my data ([see previous question][1]).&#xD;&#xA;&#xD;&#xA;Note that in my actual work flow I define the colour scheme using a palette to control the colour outcome.&#xD;&#xA;&#xD;&#xA;I want a means to make a tangle using phylogenies which I can format. Preferably in R. I like to get an output like this: ![][2]&#xD;&#xA;&#xD;&#xA;  [1]: https://stackoverflow.com/questions/44006500/r-coloured-lines-for-tangelgram-package-ape-function-cophyloplot&#xD;&#xA;  [2]: https://i.stack.imgur.com/HTDfR.jpg&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2973" PostHistoryTypeId="2" PostId="950" RevisionGUID="8db5f86a-132a-4c14-a6e2-056b50d6d6d2" CreationDate="2017-06-29T16:20:35.530" UserId="1016" Text="This is an interesting problem - I was wondering if anyone has a creative solution.&#xD;&#xA;&#xD;&#xA;So I have a vector of vertices representing atoms in a protein, as well as 6 variables containing the absolute minimum/maximum bound of the set at each direction.&#xD;&#xA;&#xD;&#xA;I need to build a grid that surrounds the protein/vertices in a homogeneous layer.&#xD;&#xA;&#xD;&#xA;Right now I just loop through every square of the grid bound by the minimum-maximum values and then loop through every vertex to calculate the minimum distance between my point and every atom in the protein, and see if that point is at an acceptable distance. This process is quite expensive.&#xD;&#xA;&#xD;&#xA;Do you guys have any creative answers as to how minimize the time?&#xD;&#xA;&#xD;&#xA;Thanks!&#xD;&#xA;&#xD;&#xA;P.S. the list of atoms in the protein are ordered more according the the amino acid sequence, which means it has almost no relation to the location of the atom in space" />
  <row Id="2974" PostHistoryTypeId="1" PostId="950" RevisionGUID="8db5f86a-132a-4c14-a6e2-056b50d6d6d2" CreationDate="2017-06-29T16:20:35.530" UserId="1016" Text="Minimizing Grid Mapping time of Protein Surface" />
  <row Id="2975" PostHistoryTypeId="3" PostId="950" RevisionGUID="8db5f86a-132a-4c14-a6e2-056b50d6d6d2" CreationDate="2017-06-29T16:20:35.530" UserId="1016" Text="&lt;protein-structure&gt;&lt;proteins&gt;&lt;3d-structure&gt;" />
  <row Id="2976" PostHistoryTypeId="2" PostId="951" RevisionGUID="96cdf3da-3bcb-4d0c-8459-9545b9195b14" CreationDate="2017-06-29T16:24:27.427" UserId="626" Text="I have 12 human gut microbiome WGS Nextseq reads 151 bp paired end. What will be an effective strategy to assemble metagenome?&#xD;&#xA;&#xD;&#xA;Let us say, I have already filtered the fastq for quality, Adaptor sequence and Host contamination (Human in this case). &#xD;&#xA;&#xD;&#xA;1) Should I concatenate all the R1 reads as one single R1 read and one Single R2 read?&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;cat Sample[1..12].R1 &gt; Single_R1.fastq&#xD;&#xA;cat Sample[1..12].R2 &gt; Single_R2.fastq&#xD;&#xA;```&#xD;&#xA;and then use Diginorm to normalize the Single_R1.fastq and Single_R2.fastq. Subsequently, feed these fastq files into any metagenome assembler such as Megahit, MetaSPAdes?&#xD;&#xA;&#xD;&#xA;Normalize the output by using CD-HIT or similar tool to remove duplicates and filter by contig length. &#xD;&#xA;&#xD;&#xA;OR &#xD;&#xA;&#xD;&#xA;2) Perform metagenome assembly for each of the samples individually after applying filtering, removing adapters and Host contamination. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;R1=(*_R1_001.filtered.fastq)&#xD;&#xA;R2=(*_R2_001.filtered.fastq)&#xD;&#xA;for ((i=0;i&lt;=${#R1[@]};i++));&#xD;&#xA;do  /usr/bin/metagenome-assembler  -1 &quot;${R1[i]}&quot; -2 &quot;${R2[i]}&quot; -o ${R1[i]%.*}.contigs.fa;&#xD;&#xA;done&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;Followed by combining all the contigs.fa into one mega_contigs.fa &#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;cat *.contigs.fa &gt; Mega_contigs.fa&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;and use CD-HIT or similar tool to remove duplicates. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Thank you for reading my question. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2977" PostHistoryTypeId="1" PostId="951" RevisionGUID="96cdf3da-3bcb-4d0c-8459-9545b9195b14" CreationDate="2017-06-29T16:24:27.427" UserId="626" Text="Pooling data in metagenome assembly" />
  <row Id="2978" PostHistoryTypeId="3" PostId="951" RevisionGUID="96cdf3da-3bcb-4d0c-8459-9545b9195b14" CreationDate="2017-06-29T16:24:27.427" UserId="626" Text="&lt;assembly&gt;&lt;metagenome&gt;" />
  <row Id="2979" PostHistoryTypeId="2" PostId="952" RevisionGUID="57a5e55d-8257-408b-ae6e-c77fb0853c1c" CreationDate="2017-06-29T16:57:57.123" UserId="1017" Text="You can do this in [Hail](https://hail.is/).&#xD;&#xA;&#xD;&#xA;Here's the rough code to do it.&#xD;&#xA;&#xD;&#xA;Setup:&#xD;&#xA;&#xD;&#xA;    from hail import *&#xD;&#xA;    hc = HailContext()&#xD;&#xA;    &#xD;&#xA;Import the .gen file. VCF works too:&#xD;&#xA;&#xD;&#xA;    dataset = hc.import_gen(&#xD;&#xA;        'src/test/resources/example.gen', &#xD;&#xA;        'src/test/resources/example.sample')&#xD;&#xA;    &#xD;&#xA;Remap the genotype schema and export to VCF:&#xD;&#xA;&#xD;&#xA;    dataset.annotate_genotypes_expr('g = {GT: g.gt, DS: g.dosage()}')\&#xD;&#xA;        .export_vcf('/tmp/out.vcf.bgz')&#xD;&#xA;&#xD;&#xA;Take a look at the [getting started page](https://hail.is/hail/getting_started.html) if you want to try it out!&#xD;&#xA;&#xD;&#xA;I should note that you may be able to do QTL analyses in Hail, depending on the method you want to use. See [blog post here](http://discuss.hail.is/t/fast-linear-regression-for-multiple-phenotypes-eqtls/190)." />
  <row Id="2980" PostHistoryTypeId="5" PostId="951" RevisionGUID="c0453428-1ba3-45f4-9293-11943e963c91" CreationDate="2017-06-29T18:03:20.433" UserId="298" Comment="Fixed formatting; removed the '/usr/' to avoid horizontal scrolling; removed thanks (they're implied here and we like to keep questions streamlined)" Text="I have 12 human gut microbiome WGS Nextseq reads (151 bp paired end). What will be an effective strategy to assemble a metagenome?&#xD;&#xA;&#xD;&#xA;Let us say I have already filtered the fastq for quality, adapter sequence and host contamination (human, in this case). &#xD;&#xA;&#xD;&#xA;1) Should I concatenate all the R1 reads as one single R1 read and one Single R2 read?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    cat Sample[1..12].R1 &gt; Single_R1.fastq&#xD;&#xA;    cat Sample[1..12].R2 &gt; Single_R2.fastq&#xD;&#xA;    &#xD;&#xA;and then use Diginorm to normalize the Single_R1.fastq and Single_R2.fastq. Subsequently, feed these fastq files into any metagenome assembler such as Megahit, MetaSPAdes?&#xD;&#xA;&#xD;&#xA;Normalize the output by using CD-HIT or similar tool to remove duplicates and filter by contig length. &#xD;&#xA;&#xD;&#xA;OR &#xD;&#xA;&#xD;&#xA;2) Perform metagenome assembly for each of the samples individually after applying filtering, removing adapters and host contamination. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    R1=(*_R1_001.filtered.fastq)&#xD;&#xA;    R2=(*_R2_001.filtered.fastq)&#xD;&#xA;    for ((i=0;i&lt;=${#R1[@]};i++)); do  &#xD;&#xA;      /bin/metagenome-assembler -1 &quot;${R1[i]}&quot; -2 &quot;${R2[i]}&quot; -o ${R1[i]%.*}.contigs.fa;&#xD;&#xA;    done&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Followed by combining all the contigs.fa into one mega_contigs.fa &#xD;&#xA;&#xD;&#xA;    cat *.contigs.fa &gt; Mega_contigs.fa&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;and use CD-HIT or similar tool to remove duplicates. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2981" PostHistoryTypeId="2" PostId="953" RevisionGUID="0cbd60fa-1317-4a22-a31e-744192d07ac8" CreationDate="2017-06-29T19:20:15.247" UserId="9" Text="For the first question: &quot;how many of long non coding can make hairpin loop structure?&quot;&#xD;&#xA;&#xD;&#xA;The easiest thing to would be to run your sequences through an RNA secondary structure prediction tool. There's a few tools for doing this but the most commonly used are [RNAfold](http://rna.tbi.univie.ac.at/cgi-bin/RNAWebSuite/RNAfold.cgi) from the [ViennaRNA package](https://www.tbi.univie.ac.at/RNA/) and [mfold](http://unafold.rna.albany.edu/?q=mfold). &#xD;&#xA;&#xD;&#xA;This will give you an output that looks like this, where the parentheses indicate which bases are paired with each other. &#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;GGGCUAUUAGCUCAGUUGGUUAGAGCGCACCCCUGAUAAGGGUGAGGUCGCUGAUUCGAAUUCAGCAUAGCCCA&#xD;&#xA;(((((((..((((.........)))).(((((.......))))).....(((((.......)))))))))))).&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;To see if there's hairpins, you'll need to annotate the elements within it. Take a look at [this question on biostars](https://www.biostars.org/p/4300/) for an example of how to do this. In short, you can use the [forgi](https://github.com/ViennaRNA/forgi) library to produce an annotation like this:&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;(((((((..((((.........)))).(((((.......))))).....(((((.......)))))))))))).&#xD;&#xA;sssssssmmsssshhhhhhhhhssssmssssshhhhhhhsssssmmmmmssssshhhhhhhsssssssssssse&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;If there's an `h` in the output annotation, you have a hairpin. The remainder of your question about annotating miRNAs will be left to somebody else." />
  <row Id="2982" PostHistoryTypeId="2" PostId="954" RevisionGUID="a74db7e6-e090-4160-94e9-d14dc08c7c22" CreationDate="2017-06-29T21:19:48.177" UserId="779" Text="I am on a Mac using UNIX. I am trying to use the kallisto quant command on all files in a directory (instead of manually entering them). Because I'm running the analysis against the same index file, I first enter the following:&#xD;&#xA;&#xD;&#xA;    ./kallisto index -i --index --make-unique index.fa &#xD;&#xA;&#xD;&#xA;This successfully creates an index file. Then, I tried this:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta; do kallisto quant --single -l 200 -s 0.1 -o $file-aligned &#xD;&#xA;    &quot;$file&quot;; done&#xD;&#xA;While the $file-aligned folders are created, they're empty. I get an error that states that the index file is missing. So I assumed I would need to specify the index file in the command line. I then tried this:&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    for file in *.fasta; do kallisto quant -i index.fa --single -l 200 -s 0.1 -o&#xD;&#xA;    $file-aligned  &quot;$file&quot;; done&#xD;&#xA;&#xD;&#xA;I get an error that says &quot;Error: incompatible indices. Found version 0, expected version 10. &#xD;&#xA;&#xD;&#xA;I searched around for solutions to that error message, and only found that rebuilding the index file should resolve the error. I don't really care if I have to build an index for each file uniquely (I have 40 samples and it only takes 2 minutes to build an index) so I can run it over night on a local machine. However, I'm not entirely sure how to include that in what I've gotten so far." />
  <row Id="2983" PostHistoryTypeId="1" PostId="954" RevisionGUID="a74db7e6-e090-4160-94e9-d14dc08c7c22" CreationDate="2017-06-29T21:19:48.177" UserId="779" Text="Run kallisto iteratively across many samples" />
  <row Id="2984" PostHistoryTypeId="3" PostId="954" RevisionGUID="a74db7e6-e090-4160-94e9-d14dc08c7c22" CreationDate="2017-06-29T21:19:48.177" UserId="779" Text="&lt;rna-seq&gt;" />
  <row Id="2985" PostHistoryTypeId="5" PostId="945" RevisionGUID="9105b250-40d2-4fa6-97f6-59bd54aeb15a" CreationDate="2017-06-29T23:25:31.937" UserId="73" Comment="added 6 characters in body" Text="I'm working on a human genome project, trying to find variants that are unique to one person, but not to three other relatives of that person. Unfortunately, one of our regions of interest contains a long tandem repeat region (***30kb***, repeated 3 times in tandem). I'd like to be able to properly map these regions with WGS reads, but suspect that I'm coming up against a mapping preference problem with my current methods (Bowtie2 / HISAT2):&#xD;&#xA;&#xD;&#xA;1. Read A1 can potentially map to three different locations near the same locus in the genome (L1, L2, L3)&#xD;&#xA;2. L1 is most similar to A1, so is preferentially mapped. Because this similarity is greater than that of L2 or L3, the read is treated as a unique mapping&#xD;&#xA;3. Most other reads are similarly mapped to L1, possibly because it is the locus that has had the most correction applied to it in the reference genome&#xD;&#xA;4. An analysis of the mapping results indicates huge deletions at L2 and L3&#xD;&#xA;&#xD;&#xA;Is BWA any more resistant to this issue? If so, how can I get it to report randomly one of the three mappable loci (assuming they all appear in the results)?&#xD;&#xA;&#xD;&#xA;Are there any other ways I can deal with this issue of distributing mapped reads throughout tandem repeats?" />
  <row Id="2986" PostHistoryTypeId="2" PostId="955" RevisionGUID="3235db7e-9dac-4c2a-a69b-9e597f7571d6" CreationDate="2017-06-29T23:43:58.127" UserId="73" Text="You need to point kallisto to the index file (usually has a `.idx` extension), rather than the fasta file:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta;&#xD;&#xA;      do kallisto quant -i index.fa.idx --single -l 200 -s 0.1 -o &quot;${file}-aligned&quot; &quot;${file}&quot;;&#xD;&#xA;    done" />
  <row Id="2987" PostHistoryTypeId="6" PostId="935" RevisionGUID="ef156b4e-3154-4997-b7e5-68ce2d3a64c2" CreationDate="2017-06-29T23:50:12.637" UserId="73" Comment="added 'benchmarking' tag; this seems to be a programming exercise" Text="&lt;ngs&gt;&lt;fastq&gt;&lt;software-recommendation&gt;&lt;benchmarking&gt;" />
  <row Id="2989" PostHistoryTypeId="5" PostId="943" RevisionGUID="52c1d513-23d5-431a-8c72-3aa6218f6bb0" CreationDate="2017-06-30T05:31:46.470" UserId="747" Comment="added 30 characters in body" Text="I have the following mwe for filtering a Swissprot file based on a certain feature, in this case, transmembrane regions. &#xD;&#xA;  &#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    records=[]&#xD;&#xA;    for record in SeqIO.parse(&quot;Input.txt&quot;, &quot;swiss&quot;):&#xD;&#xA;        transmembrane_protein=False&#xD;&#xA;        print record.id&#xD;&#xA;        for i, feature in enumerate(record.features):&#xD;&#xA;            if feature.type == &quot;TRANSMEM&quot;:&#xD;&#xA;                transmembrane_protein=True&#xD;&#xA;        if transmembrane_protein==True:&#xD;&#xA;            records.append(record)&#xD;&#xA;    SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)&#xD;&#xA;&#xD;&#xA;The script works when ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)``` becomes ```SeqIO.write(records, &quot;Output.txt&quot;, &quot;fasta&quot;)```&#xD;&#xA;&#xD;&#xA;However, such a method is not yet supported.  &#xD;&#xA;&#xD;&#xA;    ValueError: Reading format 'swiss' is supported, but not writing&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;From the docs, I see that [writing is not supported for swiss](http://biopython.org/DIST/docs/api/Bio.SeqIO-module.html):&#xD;&#xA;&#xD;&#xA;&gt;Note that while Bio.SeqIO can read all the above file formats, it cannot write to all of them.&#xD;&#xA;&#xD;&#xA;Is there any unofficial way of using biopython/python to write swissprot files from parsed swissprot files? " />
  <row Id="2990" PostHistoryTypeId="2" PostId="956" RevisionGUID="5e5cdcaf-3f45-4615-ac2b-01de258acaa8" CreationDate="2017-06-30T10:37:39.143" UserId="345" Text="FCS is a patented data format used for storing flow cytometry data. The most recent version is FCS3.1. There is some [documentation][1] on the format but there is no information on how to read these files. There are some R packages and a MATLAB code to read FCS file but I am looking for standard libraries developed either by the FCS consortium or any other group. I also wish to know if FCS is a subset of an existing standard data format that can be read by a standard library using any programming language.&#xD;&#xA;&#xD;&#xA;Finally, I would want to convert these files to a easily readable format like HDF5. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://isac-net.org/PDFS/90/9090600d-19be-460d-83fc-f8a8b004e0f9.pdf" />
  <row Id="2991" PostHistoryTypeId="1" PostId="956" RevisionGUID="5e5cdcaf-3f45-4615-ac2b-01de258acaa8" CreationDate="2017-06-30T10:37:39.143" UserId="345" Text="How to read FCS files using open source libraries?" />
  <row Id="2992" PostHistoryTypeId="3" PostId="956" RevisionGUID="5e5cdcaf-3f45-4615-ac2b-01de258acaa8" CreationDate="2017-06-30T10:37:39.143" UserId="345" Text="&lt;file-formats&gt;" />
  <row Id="2993" PostHistoryTypeId="5" PostId="956" RevisionGUID="1720dc13-8587-46c4-be8f-c02d45435734" CreationDate="2017-06-30T10:40:07.777" UserId="298" Comment="added 5 characters in body; edited tags; edited title" Text="FCS is a patented data format used for storing flow cytometry data. The most recent version is FCS3.1. There is some [documentation][1] on the format, but there is no information on how to read these files. There are some R packages and a MATLAB code to read an FCS file, but I am looking for standard libraries developed either by the FCS consortium or any other group. I also wish to know if FCS is a subset of an existing standard data format that can be read by a standard library using any programming language.&#xD;&#xA;&#xD;&#xA;Finally, I would want to convert these files to a easily readable format like HDF5. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://isac-net.org/PDFS/90/9090600d-19be-460d-83fc-f8a8b004e0f9.pdf" />
  <row Id="2994" PostHistoryTypeId="4" PostId="956" RevisionGUID="1720dc13-8587-46c4-be8f-c02d45435734" CreationDate="2017-06-30T10:40:07.777" UserId="298" Comment="added 5 characters in body; edited tags; edited title" Text="How can I read FCS files using open source libraries?" />
  <row Id="2995" PostHistoryTypeId="6" PostId="956" RevisionGUID="1720dc13-8587-46c4-be8f-c02d45435734" CreationDate="2017-06-30T10:40:07.777" UserId="298" Comment="added 5 characters in body; edited tags; edited title" Text="&lt;file-formats&gt;&lt;fcs&gt;" />
  <row Id="2996" PostHistoryTypeId="5" PostId="956" RevisionGUID="745a913c-349e-4c6a-ae7e-747f88e20b40" CreationDate="2017-06-30T10:41:19.763" UserId="345" Comment="added 1 character in body" Text="FCS is a patented data format used for storing flow cytometry data. The most recent version is FCS3.1. There is some [documentation][1] on the format, but there is no information on how to read these files. There are some R packages and a MATLAB code to read an FCS file, but I am looking for standard libraries developed either by the FCS consortium or any other group. I also wish to know if FCS is a subset of an existing standard data format that can be read by a standard library using any programming language.&#xD;&#xA;&#xD;&#xA;Finally, I would want to convert these files to an easily readable format like HDF5. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://isac-net.org/PDFS/90/9090600d-19be-460d-83fc-f8a8b004e0f9.pdf" />
  <row Id="2997" PostHistoryTypeId="2" PostId="957" RevisionGUID="60e0ab0c-0bef-4fb1-917c-de198d934935" CreationDate="2017-06-30T11:56:45.483" UserId="73" Text="R/Bioconductor has a number of different flow cytometry processing packages. One place to start for looking at cytometry data from a high level would be [openCyto](http://opencyto.org/) (or its [vignette](http://opencyto.org/openCytoVignette.html)), which is a large set of tools for basic extraction and analysis of FCS files.&#xD;&#xA;&#xD;&#xA;I have looked in the past at the FCS files as an R structure using [flowCore](http://bioconductor.org/packages/release/bioc/html/flowCore.html). Loading a single FCS file is fairly straightforward and follows a familiar R pattern:&#xD;&#xA;&#xD;&#xA;    file.name &lt;- &quot;/dir/file.fcs&quot;&#xD;&#xA;    x &lt;- read.FCS(file.name, transformation=FALSE)&#xD;&#xA;    summary(x)&#xD;&#xA;&#xD;&#xA;Asking about converting FCS files into an &quot;easily readable format like HDF5&quot; doesn't seem like the right question. [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format#HDF5) is a container format and shares a lot of similarity to file systems. I've found it best to keep FCS files as they are, as it is a compact, standardised binary format." />
  <row Id="2998" PostHistoryTypeId="6" PostId="942" RevisionGUID="565546af-4bab-416c-bd9d-fb125c6e0ad4" CreationDate="2017-06-30T12:04:12.420" UserId="37" Comment="edited tags" Text="&lt;bioconductor&gt;&lt;rna-structure&gt;" />
  <row Id="2999" PostHistoryTypeId="5" PostId="877" RevisionGUID="daa56c76-344f-44cd-98b2-df095fa4f4ab" CreationDate="2017-06-30T12:25:04.830" UserId="734" Comment="deleted 84 characters in body; edited tags; edited title" Text="I have seen that there are 2 common methods: [Patient-derived models][1]: [Patient Derived Xenograft (PDX)][2] and [Patient Derived Organoids (PDO)][3] to reflect tumor biology. &#xD;&#xA;&#xD;&#xA;First, What are the differences between PDX and PDO?  &#xD;&#xA;Secondly, **are there any databases/resources/computational tools that use the outcomes of PDO/PDX experiments to create a predictive model for cancer or Alzheimer's?**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&#xD;&#xA;  [3]: http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html" />
  <row Id="3000" PostHistoryTypeId="4" PostId="877" RevisionGUID="daa56c76-344f-44cd-98b2-df095fa4f4ab" CreationDate="2017-06-30T12:25:04.830" UserId="734" Comment="deleted 84 characters in body; edited tags; edited title" Text="How are PDO and PDX used in computational and predicative models?" />
  <row Id="3001" PostHistoryTypeId="6" PostId="877" RevisionGUID="daa56c76-344f-44cd-98b2-df095fa4f4ab" CreationDate="2017-06-30T12:25:04.830" UserId="734" Comment="deleted 84 characters in body; edited tags; edited title" Text="&lt;cancer&gt;&lt;drugs&gt;&lt;modelling&gt;" />
  <row Id="3004" PostHistoryTypeId="2" PostId="958" RevisionGUID="cc30f430-1e33-4d29-94d1-c15c55c85213" CreationDate="2017-06-30T12:44:22.447" UserId="492" Text="There's [the dosage plugin for bcftools](https://samtools.github.io/bcftools/bcftools-man.html#plugin), but it only outputs tab separated values. It would not be too hard to extend the plugin to output a VCF with the DS tag instead, but it has not been done yet.  There's a good chance the bcftools devs would respond to a [feature request](https://github.com/samtools/bcftools/issues/new)...&#xD;&#xA;&#xD;&#xA;In any case, this code:&#xD;&#xA;&#xD;&#xA;    curl https://raw.githubusercontent.com/samtools/bcftools/develop/test/convert.vcf &gt; convert.vcf&#xD;&#xA;    bcftools +dosage convert.vcf &gt; output.tsv&#xD;&#xA;    head -2 output.tsv &#xD;&#xA;&#xD;&#xA;has the output:&#xD;&#xA;&#xD;&#xA;    #[1]CHROM       [2]POS  [3]REF  [4]ALT  [5]NA00001      [6]NA00002      [7]NA00003      [8]NA00004      [9]NA00005      [10]NA00006     [11]NA00007     [12]NA00008    [13]NA00009     [14]NA00010&#xD;&#xA;    X       2698560 G       A       0.1     0.0     0.1     0.2     0.3     0.2     0.2     0.2     0.2     0.1&#xD;&#xA;&#xD;&#xA;This is using bcftools version 1.3.1.  &#xD;&#xA;&#xD;&#xA;Here is an excerpt from the bcftools manual for the dosage plugin:&#xD;&#xA;&#xD;&#xA;&gt; dosage&#xD;&#xA;&#xD;&#xA;&gt; print genotype dosage. By default the plugin searches for PL, GL and GT, in that order.&#xD;&#xA;" />
  <row Id="3012" PostHistoryTypeId="2" PostId="959" RevisionGUID="66174575-d2b6-4c4c-b5a5-644d2c74f8d1" CreationDate="2017-06-30T12:51:15.207" UserId="292" Text="A few years ago, I wrote a python script to convert FCS files into tab-separated format. It was far from handling all the possibilities that the format description offers, but at least it worked for some of the files produced on one of our machine: &lt;http://www.igh.cnrs.fr/equip/Seitz/en_equipe-programmes.html&gt;&#xD;&#xA;&#xD;&#xA;The format documentation I found enabled decoding (see section 3 of the pdf you mention), but it requires reading data in binary mode.&#xD;&#xA;&#xD;&#xA;In case this may be useful, and for the record, here is the code from the above-mentioned script (after stripping comments, some of which are merely copied from the format documentation):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    &quot;&quot;&quot;This script tries to read FCS flow cytometry data.&#xD;&#xA;    Format parsing inspired by information found here:&#xD;&#xA;    http://isac-net.org/Resources-for-Cytometrists/Data-Standards/Data-File-Standards/Flow-Cytometry-Data-File-Format-Standards.aspx&#xD;&#xA;    &quot;&quot;&quot;&#xD;&#xA;    &#xD;&#xA;    import re&#xD;&#xA;    import struct&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    class Parameter(object):&#xD;&#xA;        &quot;&quot;&quot;This object represents one of the parameter types that are present in a DATA segment of a FCS file.&quot;&quot;&quot;&#xD;&#xA;        __slots__ = (&quot;p_name&quot;, &quot;p_bits&quot;, &quot;p_range&quot;, &quot;p_ampl&quot;, &quot;parser&quot;)&#xD;&#xA;        def __init__(self, p_name, p_bits, p_range, p_ampl):&#xD;&#xA;            self.p_name = p_name&#xD;&#xA;            self.p_bits = p_bits&#xD;&#xA;            self.p_range = p_range&#xD;&#xA;            self.p_ampl = p_ampl&#xD;&#xA;            # Function for parsing a value of the parameter in the data segment&#xD;&#xA;            self.parser = None&#xD;&#xA;    &#xD;&#xA;    f = open(sys.argv[1], &quot;rb&quot;)&#xD;&#xA;    file_format = &quot;&quot;.join([f.read(1) for __ in range(6)])&#xD;&#xA;    sys.stdout.write(&quot;Format: %s\n&quot; % file_format)&#xD;&#xA;    skip = f.read(4)&#xD;&#xA;    text_start = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    text_end = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    data_start = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    data_end = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    analysis_start = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    analysis_end = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    &#xD;&#xA;    if (analysis_start and analysis_end):&#xD;&#xA;        sys.stderr.write(&quot;Cannot deal with ANALYSIS segment of an FCS file.\n&quot;)&#xD;&#xA;    &#xD;&#xA;    f.seek(text_start)&#xD;&#xA;    #The first character in the primary TEXT segment is the ASCII delimiter character.&#xD;&#xA;    sep = f.read(1)&#xD;&#xA;    if sep not in [&quot;_&quot;, &quot;@&quot;]:&#xD;&#xA;        alt_sep = &quot;_@_&quot;&#xD;&#xA;    elif sep not in [&quot;_&quot;, &quot;|&quot;]:&#xD;&#xA;        alt_sep = &quot;_|_&quot;&#xD;&#xA;    else:&#xD;&#xA;        assert sep not in [&quot;+&quot;, &quot;|&quot;]&#xD;&#xA;        alt_sep = &quot;+|+&quot;&#xD;&#xA;    text_segment = f.read(text_end - text_start)&#xD;&#xA;    &#xD;&#xA;    fields = text_segment.split(sep)&#xD;&#xA;    &#xD;&#xA;    info = {}&#xD;&#xA;    &#xD;&#xA;    i = 0&#xD;&#xA;    while i &lt; len(fields) - 1:&#xD;&#xA;        key = fields[i]&#xD;&#xA;        i += 1&#xD;&#xA;        val = fields[i]&#xD;&#xA;        i += 1&#xD;&#xA;        # Keywords are case insensitive, they may be written in a file in lower case, upper case, or a&#xD;&#xA;        # mixture of the two. However, an FCS file reader must ignore keyword case. A keyword value may&#xD;&#xA;        # be in lower case, upper case or a mixture of the two. Keyword values are case sensitive.&#xD;&#xA;        info[key.upper()] = val&#xD;&#xA;    print &quot;%s events were detected.&quot; % info[&quot;$TOT&quot;]&#xD;&#xA;    print &quot;Each event is characterized by %s parameters&quot; % info[&quot;$PAR&quot;]&#xD;&#xA;    &#xD;&#xA;    if info[&quot;$NEXTDATA&quot;] != &quot;0&quot;:&#xD;&#xA;        sys.stderr.write(&quot;Some other data exist in the file but hasn't been parsed.\n&quot;)&#xD;&#xA;    &#xD;&#xA;    # L - List mode. For each event, the value of each parameter is stored in the order in which the&#xD;&#xA;    # parameters are described. The number of bits reserved for parameter 1 is described using the&#xD;&#xA;    # $P1B keyword. There can be only one set of list mode data per data set. The $DATATYPE&#xD;&#xA;    # keyword describes the data format. This is the most versatile mode for the storage of flow&#xD;&#xA;    # cytometry data because mode C and mode U data can be created from mode L data.&#xD;&#xA;    assert info[&quot;$MODE&quot;] == &quot;L&quot;&#xD;&#xA;    &#xD;&#xA;    parameters = []&#xD;&#xA;    &#xD;&#xA;    # indices of the parameters&#xD;&#xA;    p_indices = range(1, int(info[&quot;$PAR&quot;]) + 1)&#xD;&#xA;    for i in p_indices:&#xD;&#xA;        p_name = info[&quot;$P%dN&quot; % i]&#xD;&#xA;        p_bits =  info[&quot;$P%dB&quot; % i]&#xD;&#xA;        p_range = info[&quot;$P%dR&quot; % i]&#xD;&#xA;        p_ampl =  info[&quot;$P%dE&quot; % i]&#xD;&#xA;        parameters.append(Parameter(p_name, p_bits, p_range, p_ampl))&#xD;&#xA;    &#xD;&#xA;    sys.stdout.write(&quot;The parameters are:\n%s\n&quot; % &quot;\t&quot;.join([par.p_name for par in parameters]))&#xD;&#xA;    &#xD;&#xA;    # How are 32 bit words organized&#xD;&#xA;    if info[&quot;$BYTEORD&quot;] == &quot;4,3,2,1&quot;:&#xD;&#xA;        endianness = &quot;&gt;&quot;&#xD;&#xA;    else:&#xD;&#xA;        endianness = &quot;&lt;&quot;&#xD;&#xA;        assert info[&quot;$BYTEORD&quot;] == &quot;1,2,3,4&quot;&#xD;&#xA;    &#xD;&#xA;        # I stripped a long comment which is just a copy of the documentation&#xD;&#xA;    # Type of data:&#xD;&#xA;    if info[&quot;$DATATYPE&quot;] == &quot;I&quot;:&#xD;&#xA;        for par in parameters:&#xD;&#xA;            nb_bits = int(par.p_bits)&#xD;&#xA;            assert nb_bits % 8 == 0&#xD;&#xA;            nb_bytes = nb_bits / 8&#xD;&#xA;            # Determine format string for unpacking (see https://docs.python.org/2/library/struct.html)&#xD;&#xA;            if nb_bytes == 1:&#xD;&#xA;                c_type = &quot;B&quot; # unsigned char&#xD;&#xA;            elif nb_bytes == 2:&#xD;&#xA;                c_type = &quot;H&quot; # unsigned short&#xD;&#xA;            elif nb_bytes == 4:&#xD;&#xA;                c_type = &quot;L&quot; # unsigned long&#xD;&#xA;            elif nb_bytes == 8:&#xD;&#xA;                c_type = &quot;Q&quot; # unsigned long long&#xD;&#xA;            else:&#xD;&#xA;                raise ValueError, &quot;Number of bytes (%d) not valid for an integer (see https://docs.python.org/2/library/struct.html#byte-order-size-and-alignment).&quot; % nb_bytes&#xD;&#xA;            fmt = &quot;%s%s&quot; % (endianness, c_type)&#xD;&#xA;            p_range = int(par.p_range)&#xD;&#xA;            def parser(data):&#xD;&#xA;                value = struct.unpack(fmt, data.read(nb_bytes))[0]&#xD;&#xA;                try:&#xD;&#xA;                    assert value &lt; p_range&#xD;&#xA;                except AssertionError:&#xD;&#xA;                    print &quot;Value %s higher than %d&quot; % (str(value), p_range)&#xD;&#xA;                return value&#xD;&#xA;            par.parser = parser&#xD;&#xA;        pass&#xD;&#xA;    else:&#xD;&#xA;        raise NotImplementedError, &quot;Only the parsing of integer value has been implemented so far.&quot;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    out_file = open(sys.argv[2], &quot;w&quot;)&#xD;&#xA;    out_file.write(&quot;#amplification_types\t&quot; + &quot;\t&quot;.join([par.p_ampl for par in parameters]) + &quot;\n&quot;)&#xD;&#xA;    out_file.write(&quot;parameters\t&quot; + &quot;\t&quot;.join([par.p_name for par in parameters]) + &quot;\n&quot;)&#xD;&#xA;    i = 1&#xD;&#xA;    f.seek(data_start)&#xD;&#xA;    while f.tell() &lt; data_end:&#xD;&#xA;        values = []&#xD;&#xA;        for par in parameters:&#xD;&#xA;            values.append(par.parser(f))&#xD;&#xA;        out_file.write(&quot;%d\t&quot; % i + &quot;\t&quot;.join(map(str, values)) + &quot;\n&quot;)&#xD;&#xA;        i += 1&#xD;&#xA;    out_file.close()&#xD;&#xA;    f.close()" />
  <row Id="3013" PostHistoryTypeId="5" PostId="952" RevisionGUID="dc061e6c-410b-4a43-9b0a-a8ddc441c1bd" CreationDate="2017-06-30T12:56:00.550" UserId="1017" Comment="added 15 characters in body" Text="You can do this in [Hail](https://hail.is/).&#xD;&#xA;&#xD;&#xA;Here's the rough code to do it (0.1 versions).&#xD;&#xA;&#xD;&#xA;Setup:&#xD;&#xA;&#xD;&#xA;    from hail import *&#xD;&#xA;    hc = HailContext()&#xD;&#xA;    &#xD;&#xA;Import the .gen file. VCF works too:&#xD;&#xA;&#xD;&#xA;    dataset = hc.import_gen(&#xD;&#xA;        'src/test/resources/example.gen', &#xD;&#xA;        'src/test/resources/example.sample')&#xD;&#xA;    &#xD;&#xA;Remap the genotype schema and export to VCF:&#xD;&#xA;&#xD;&#xA;    dataset.annotate_genotypes_expr('g = {GT: g.gt, DS: g.dosage()}')\&#xD;&#xA;        .export_vcf('/tmp/out.vcf.bgz')&#xD;&#xA;&#xD;&#xA;Take a look at the [getting started page](https://hail.is/hail/getting_started.html) if you want to try it out!&#xD;&#xA;&#xD;&#xA;I should note that you may be able to do QTL analyses in Hail, depending on the method you want to use. See [blog post here](http://discuss.hail.is/t/fast-linear-regression-for-multiple-phenotypes-eqtls/190)." />
  <row Id="3014" PostHistoryTypeId="5" PostId="952" RevisionGUID="c30059cc-5481-4911-9fdc-9056eb89edd4" CreationDate="2017-06-30T13:12:31.343" UserId="1017" Comment="Corrected 'g.gt' =&gt; 'g.call'" Text="You can do this in [Hail](https://hail.is/).&#xD;&#xA;&#xD;&#xA;Here's the rough code to do it (0.1 versions).&#xD;&#xA;&#xD;&#xA;Setup:&#xD;&#xA;&#xD;&#xA;    from hail import *&#xD;&#xA;    hc = HailContext()&#xD;&#xA;    &#xD;&#xA;Import the .gen file. VCF works too:&#xD;&#xA;&#xD;&#xA;    dataset = hc.import_gen(&#xD;&#xA;        'src/test/resources/example.gen', &#xD;&#xA;        'src/test/resources/example.sample')&#xD;&#xA;    &#xD;&#xA;Remap the genotype schema and export to VCF:&#xD;&#xA;&#xD;&#xA;    dataset.annotate_genotypes_expr('g = {GT: g.call(), DS: g.dosage()}')\&#xD;&#xA;        .export_vcf('/tmp/out.vcf.bgz')&#xD;&#xA;&#xD;&#xA;Take a look at the [getting started page](https://hail.is/hail/getting_started.html) if you want to try it out!&#xD;&#xA;&#xD;&#xA;I should note that you may be able to do QTL analyses in Hail, depending on the method you want to use. See [blog post here](http://discuss.hail.is/t/fast-linear-regression-for-multiple-phenotypes-eqtls/190)." />
  <row Id="3015" PostHistoryTypeId="2" PostId="960" RevisionGUID="b16ac6ba-3390-49db-b8d5-b8a230867dec" CreationDate="2017-06-30T13:19:17.627" UserId="818" Text="I have some protein sequences and I want to build a PSSM matrix for them and then Upload this PSSM matrix to NCBI PSI-BLAST. I used CHAPS program for this pupose but uploading the output PSSM matrix gave me an error in NCBI PSI-BLAST. Do you know any tool or webserver for getting PSSM matrix for a group of protein sequences which then can work in NCBI PSI-BLAST?" />
  <row Id="3016" PostHistoryTypeId="1" PostId="960" RevisionGUID="b16ac6ba-3390-49db-b8d5-b8a230867dec" CreationDate="2017-06-30T13:19:17.627" UserId="818" Text="A tool or webserver for building PSSM matrix" />
  <row Id="3017" PostHistoryTypeId="3" PostId="960" RevisionGUID="b16ac6ba-3390-49db-b8d5-b8a230867dec" CreationDate="2017-06-30T13:19:17.627" UserId="818" Text="&lt;blast&gt;&lt;phylogeny&gt;" />
  <row Id="3018" PostHistoryTypeId="2" PostId="961" RevisionGUID="804cf87d-6b3d-465a-813d-88327d59db41" CreationDate="2017-06-30T13:50:55.503" UserId="292" Text="### Using pyGATB&#xD;&#xA;&#xD;&#xA;(I use the same file as in &lt;https://bioinformatics.stackexchange.com/a/400/292&gt;, same workstation as in &lt;https://bioinformatics.stackexchange.com/a/380/292&gt;)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    $ time python3 -c &quot;from gatb import Bank; seq_lens = [len(seq) for seq in Bank('SRR077487_2.filt.fastq.gz')]; print('Number of reads: %d' % len(seq_lens), 'Number of bases in reads: %d' % sum(seq_lens), sep='\n')&quot;&#xD;&#xA;    Number of reads: 23861612&#xD;&#xA;    Number of bases in reads: 2386161200&#xD;&#xA;    &#xD;&#xA;    real	0m41.122s&#xD;&#xA;    user	0m40.788s&#xD;&#xA;    sys 	0m0.312s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;It is quite faster than bioawk:&#xD;&#xA;&#xD;&#xA;    $ time bioawk -c fastx '{nb_seq+=1; nb_char+=length($seq)} END {print &quot;Number of reads: &quot;nb_seq&quot;\nNumber of bases in reads: &quot;nb_char}' SRR077487_2.filt.fastq.gz&#xD;&#xA;    Number of reads: 23861612&#xD;&#xA;    Number of bases in reads: 2386161200&#xD;&#xA;    &#xD;&#xA;    real	1m3.182s&#xD;&#xA;    user	1m2.916s&#xD;&#xA;    sys 	0m0.268s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;But not so much than the OP example:&#xD;&#xA;&#xD;&#xA;    $ time zgrep . SRR077487_2.filt.fastq.gz | awk 'NR%4==2{c++; l+=length($0)} END{print &quot;Number of reads: &quot;c; print &quot;Number of bases in reads: &quot;l}'&#xD;&#xA;    Number of reads: 23861612&#xD;&#xA;    Number of bases in reads: 2386161200&#xD;&#xA;    &#xD;&#xA;    real	0m47.127s&#xD;&#xA;    user	1m36.292s&#xD;&#xA;    sys 	0m6.796s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Or than [the `wc` based solution](https://bioinformatics.stackexchange.com/a/936/292):&#xD;&#xA;&#xD;&#xA;    $ fix_base_count() {&#xD;&#xA;    &gt;     local counts=($(cat))&#xD;&#xA;    &gt;     echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xD;&#xA;    &gt; }&#xD;&#xA;    $ time gunzip -c SRR077487_2.filt.fastq.gz | awk 'NR % 4 == 2' | wc -cl | fix_base_count&#xD;&#xA;    23861612 2386161200&#xD;&#xA;    &#xD;&#xA;    real	0m44.915s&#xD;&#xA;    user	1m12.000s&#xD;&#xA;    sys 	0m6.972s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I didn't compare with C-based solutions.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The [`zcat` to `/dev/null` reference](https://bioinformatics.stackexchange.com/a/937/292) is the following:&#xD;&#xA;&#xD;&#xA;    $ time zcat SRR077487_2.filt.fastq.gz &gt; /dev/null&#xD;&#xA;    &#xD;&#xA;    real	0m39.745s&#xD;&#xA;    user	0m39.464s&#xD;&#xA;    sys 	0m0.252s&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I'm still impressed by pyGATB speed" />
  <row Id="3019" PostHistoryTypeId="5" PostId="960" RevisionGUID="431ae0a0-cb79-42fa-b256-71436e2b6d2f" CreationDate="2017-06-30T15:14:29.243" UserId="818" Comment="added 5145 characters in body" Text="I have some protein sequences and I want to build a PSSM matrix for them and then Upload this PSSM matrix to NCBI PSI-BLAST. I used CHAPS program for this pupose but uploading the output PSSM matrix gave me an error in NCBI PSI-BLAST. Do you know any tool or webserver for getting PSSM matrix for a group of protein sequences which then can work in NCBI PSI-BLAST?&#xD;&#xA;THis is a part of my PSSM file: &#xD;&#xA;&#xD;&#xA;    PssmWithParameters ::= {&#xD;&#xA;  pssm {&#xD;&#xA;    isProtein TRUE ,&#xD;&#xA;    numRows 28 ,&#xD;&#xA;    numColumns 131 ,&#xD;&#xA;    byRow FALSE ,&#xD;&#xA;    query&#xD;&#xA;      seq {&#xD;&#xA;        id {&#xD;&#xA;          other {&#xD;&#xA;            accession &quot;WP_000208753&quot; } } ,&#xD;&#xA;        inst {&#xD;&#xA;          repr raw ,&#xD;&#xA;          mol aa ,&#xD;&#xA;          length 131 ,&#xD;&#xA;          seq-data&#xD;&#xA;            ncbieaa &quot;MTTKRKPYVRPMTSTWWKKLPFYRFYMLREGTAVPAVWFSIELIFGLFALKNGPEAW&#xD;&#xA;AGFIDFLQNPVIVIINLITLAAALLHTKTWFELAPKAANIIVKDEKMGPEPIIKSLWAVTVVATIVILFVALYW&quot; } } ,&#xD;&#xA;    intermediateData {&#xD;&#xA;      freqRatios {&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 564418841, 10, -10 } ,&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 11768571, 10, -9 } ,&#xD;&#xA;        { 185838265, 10, -10 } ,&#xD;&#xA;        { 31496547, 10, -9 } ,&#xD;&#xA;        { 3872857, 10, -8 } ,&#xD;&#xA;        { 291750464, 10, -10 } ,&#xD;&#xA;        { 128450763, 10, -10 } ,&#xD;&#xA;        { 759856221, 10, -10 } ,&#xD;&#xA;        { 359173937, 10, -10 } ,&#xD;&#xA;        { 179865517, 10, -9 } ,&#xD;&#xA;        { 14537895, 10, -8 } ,&#xD;&#xA;        { 212921456, 10, -10 } ,&#xD;&#xA;        { 220554141, 10, -10 } ,&#xD;&#xA;        { 368516324, 10, -10 } ,&#xD;&#xA;        { 319343525, 10, -10 } ,&#xD;&#xA;        { 426173953, 10, -10 } ,&#xD;&#xA;        { 463659523, 10, -10 } ,&#xD;&#xA;        { 817322186, 10, -10 } ,&#xD;&#xA;        { 811693964, 10, -11 } ,&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 227810064, 10, -10 } ,&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 768325726, 10, -10 } ,&#xD;&#xA;        { 0, 10, 0 } ,&#xD;&#xA;        { 1425562, 10, -8 } ,&#xD;&#xA;        { 372685285, 10, -10 } ,&#xD;&#xA;        { 466727421, 10, -10 } ,&#xD;&#xA;        { 185741084, 10, -10 } ,&#xD;&#xA;        { 427328646, 10, -10 } ,&#xD;&#xA;        { 122594965, 10, -10 } , &#xD;&#xA;&#xD;&#xA;" />
  <row Id="3020" PostHistoryTypeId="5" PostId="954" RevisionGUID="7908279d-9b5f-496b-ac34-439a24acdc53" CreationDate="2017-06-30T15:38:48.030" UserId="779" Comment="added 217 characters in body" Text="I am on a Mac using UNIX. I am trying to use the kallisto quant command on all files in a directory (instead of manually entering them). Because I'm running the analysis against the same index file, I first enter the following:&#xD;&#xA;&#xD;&#xA;    ./kallisto index -i --index --make-unique index.fa &#xD;&#xA;&#xD;&#xA;This successfully creates an index file. Then, I tried this:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta; do kallisto quant --single -l 200 -s 0.1 -o $file-aligned &#xD;&#xA;    &quot;$file&quot;; done&#xD;&#xA;While the $file-aligned folders are created, they're empty. I get an error that states that the index file is missing. So I assumed I would need to specify the index file in the command line. I then tried this:&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    for file in *.fasta; do kallisto quant -i index.fa --single -l 200 -s 0.1 -o&#xD;&#xA;    $file-aligned  &quot;$file&quot;; done&#xD;&#xA;&#xD;&#xA;I get an error that says &quot;Error: incompatible indices. Found version 0, expected version 10. &#xD;&#xA;&#xD;&#xA;I have now also tried the following:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta; do ./kallisto quant -i index --single - 200 -s 0.01 -o &#xD;&#xA;    &quot;${file}-aligned&quot; &quot;${file}&quot;; &#xD;&#xA;&#xD;&#xA;but I get an error saying the index file is not found.&#xD;&#xA;&#xD;&#xA;I searched around for solutions to that error message, and only found that rebuilding the index file should resolve the error. I don't really care if I have to build an index for each file uniquely (I have 40 samples and it only takes 2 minutes to build an index) so I can run it over night on a local machine. However, I'm not entirely sure how to include that in what I've gotten so far." />
  <row Id="3021" PostHistoryTypeId="5" PostId="960" RevisionGUID="69afbafa-08c9-4298-a272-592b6097fd9b" CreationDate="2017-06-30T16:09:15.420" UserId="298" Comment="added 392 characters in body" Text="I have some protein sequences and I want to build a PSSM matrix for them and then Upload this PSSM matrix to NCBI PSI-BLAST. I used CHAPS program for this pupose but uploading the output PSSM matrix gave me an error in NCBI PSI-BLAST. Do you know any tool or webserver for getting PSSM matrix for a group of protein sequences which then can work in NCBI PSI-BLAST?&#xD;&#xA;THis is a part of my PSSM file: &#xD;&#xA;&#xD;&#xA;    PssmWithParameters ::= {&#xD;&#xA;    pssm {&#xD;&#xA;        isProtein TRUE ,&#xD;&#xA;        numRows 28 ,&#xD;&#xA;        numColumns 131 ,&#xD;&#xA;        byRow FALSE ,&#xD;&#xA;        query&#xD;&#xA;        seq {&#xD;&#xA;            id {&#xD;&#xA;                other {&#xD;&#xA;                    accession &quot;WP_000208753&quot; } } ,&#xD;&#xA;            inst {&#xD;&#xA;                repr raw ,&#xD;&#xA;                mol aa ,&#xD;&#xA;                length 131 ,&#xD;&#xA;                seq-data&#xD;&#xA;                ncbieaa &quot;MTTKRKPYVRPMTSTWWKKLPFYRFYMLREGTAVPAVWFSIELIFGLFALKNGPEAW&#xD;&#xA;    AGFIDFLQNPVIVIINLITLAAALLHTKTWFELAPKAANIIVKDEKMGPEPIIKSLWAVTVVATIVILFVALYW&quot; } } ,&#xD;&#xA;        intermediateData {&#xD;&#xA;            freqRatios {&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 564418841, 10, -10 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 11768571, 10, -9 } ,&#xD;&#xA;                { 185838265, 10, -10 } ,&#xD;&#xA;                { 31496547, 10, -9 } ,&#xD;&#xA;                { 3872857, 10, -8 } ,&#xD;&#xA;                { 291750464, 10, -10 } ,&#xD;&#xA;                { 128450763, 10, -10 } ,&#xD;&#xA;                { 759856221, 10, -10 } ,&#xD;&#xA;                { 359173937, 10, -10 } ,&#xD;&#xA;                { 179865517, 10, -9 } ,&#xD;&#xA;                { 14537895, 10, -8 } ,&#xD;&#xA;                { 212921456, 10, -10 } ,&#xD;&#xA;                { 220554141, 10, -10 } ,&#xD;&#xA;                { 368516324, 10, -10 } ,&#xD;&#xA;                { 319343525, 10, -10 } ,&#xD;&#xA;                { 426173953, 10, -10 } ,&#xD;&#xA;                { 463659523, 10, -10 } ,&#xD;&#xA;                { 817322186, 10, -10 } ,&#xD;&#xA;                { 811693964, 10, -11 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 227810064, 10, -10 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 768325726, 10, -10 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 1425562, 10, -8 } ,&#xD;&#xA;                { 372685285, 10, -10 } ,&#xD;&#xA;                { 466727421, 10, -10 } ,&#xD;&#xA;                { 185741084, 10, -10 } ,&#xD;&#xA;                { 427328646, 10, -10 } ,&#xD;&#xA;                { 122594965, 10, -10 } , &#xD;&#xA;        &#xD;&#xA;" />
  <row Id="3022" PostHistoryTypeId="5" PostId="955" RevisionGUID="f8202e83-eb62-435d-91c8-8aae9975c787" CreationDate="2017-06-30T16:13:19.153" UserId="73" Comment="added 163 characters in body" Text="For generating the index, you should be using one of `-i` or `--index`, but not both:&#xD;&#xA;&#xD;&#xA;    ./kallisto index --make-unique -i index.fa.idx index.fa &#xD;&#xA;&#xD;&#xA;You need to point kallisto to the generated index file (usually has a `.idx` extension), rather than the fasta file:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta;&#xD;&#xA;      do kallisto quant -i index.fa.idx --single -l 200 -s 0.1 -o &quot;${file}-aligned&quot; &quot;${file}&quot;;&#xD;&#xA;    done" />
  <row Id="3023" PostHistoryTypeId="2" PostId="962" RevisionGUID="202520b7-e410-4231-850c-6d33d1670632" CreationDate="2017-06-30T16:21:57.023" UserId="1025" Text="For a study in GEO, I would like to obtain the data table header descriptions, specifically the &quot;VALUE&quot; column for all samples in the study.&#xD;&#xA;&#xD;&#xA;I tried using assayData() from the Biobase package but I didn't know whether the method takes a sample, a matrix of samples, or something else as a parameter. Apparently it takes a &quot;S4&quot; and then returns some random text but I don't know how to use that random text.&#xD;&#xA;&#xD;&#xA;For example, if I do:&#xD;&#xA;&#xD;&#xA;    library(Biobase)&#xD;&#xA;&#xD;&#xA;    assayData(something)&#xD;&#xA;Then I would want it to return &quot;Average Beta (normalized)&quot;&#xD;&#xA;&#xD;&#xA;If the question doesn't make sense or if there's another method that I should be using please tell me.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3024" PostHistoryTypeId="1" PostId="962" RevisionGUID="202520b7-e410-4231-850c-6d33d1670632" CreationDate="2017-06-30T16:21:57.023" UserId="1025" Text="Obtaining data table headers from GEO" />
  <row Id="3025" PostHistoryTypeId="3" PostId="962" RevisionGUID="202520b7-e410-4231-850c-6d33d1670632" CreationDate="2017-06-30T16:21:57.023" UserId="1025" Text="&lt;r&gt;&lt;database&gt;&lt;bioconductor&gt;&lt;text&gt;" />
  <row Id="3026" PostHistoryTypeId="4" PostId="962" RevisionGUID="e7b9325d-1ced-40a1-b47e-ba1eb6aa5f6a" CreationDate="2017-06-30T16:33:56.940" UserId="1025" Comment="edited title" Text="Obtaining data table headers from GEO using GEOquery" />
  <row Id="3028" PostHistoryTypeId="5" PostId="954" RevisionGUID="7d97a987-14e7-4048-9c2e-36664fa73fb7" CreationDate="2017-06-30T16:45:24.140" UserId="779" Comment="deleted 110 characters in body" Text="I am on a Mac using UNIX. I am trying to use the kallisto quant command on all files in a directory (instead of manually entering them). Because I'm running the analysis against the same index file, I first enter the following:&#xD;&#xA;&#xD;&#xA;    ./kallisto index -i --index --make-unique index.fa &#xD;&#xA;&#xD;&#xA;This successfully creates an index file. Then, I tried this:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta; do kallisto quant --single -l 200 -s 0.1 -o $file-aligned &#xD;&#xA;    &quot;$file&quot;; done&#xD;&#xA;While the $file-aligned folders are created, they're empty. I get an error that states that the index file is missing. So I assumed I would need to specify the index file in the command line. I then tried this:&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    for file in *.fasta; do kallisto quant -i index.fa --single -l 200 -s 0.1 -o&#xD;&#xA;    $file-aligned  &quot;$file&quot;; done&#xD;&#xA;&#xD;&#xA;I get an error that says &quot;Error: incompatible indices. Found version 0, expected version 10. &#xD;&#xA;&#xD;&#xA;I have now also tried the following:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta; do ./kallisto quant -i index --single - 200 -s 0.01 -o &#xD;&#xA;    &quot;${file}-aligned&quot; &quot;${file}&quot;; &#xD;&#xA;&#xD;&#xA;but I get an error saying the index file is not found.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;I am now running &#xD;&#xA;&#xD;&#xA;    or file in *.fasta; do ./kallisto quant -i index --single -l 200 -s 0.1 -o $file-aligned &#xD;&#xA;&quot;$file&quot;; done&#xD;&#xA;&#xD;&#xA;This sort of works. The index file error messages are gone, but it runs quant on the same file 10 times instead of each file once." />
  <row Id="3029" PostHistoryTypeId="5" PostId="954" RevisionGUID="c52bf2da-2d90-4d37-8dd5-38eecccf02d7" CreationDate="2017-06-30T18:11:32.550" UserId="779" Comment="added 83 characters in body" Text="I am on a Mac using UNIX. I am trying to use the kallisto quant command on all files in a directory (instead of manually entering them). Because I'm running the analysis against the same index file, I first enter the following:&#xD;&#xA;&#xD;&#xA;    ./kallisto index -i --index --make-unique index.fa &#xD;&#xA;&#xD;&#xA;This successfully creates an index file. Then, I tried this:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta; do kallisto quant --single -l 200 -s 0.1 -o $file-aligned &#xD;&#xA;    &quot;$file&quot;; done&#xD;&#xA;While the $file-aligned folders are created, they're empty. I get an error that states that the index file is missing. So I assumed I would need to specify the index file in the command line. I then tried this:&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    for file in *.fasta; do kallisto quant -i index.fa --single -l 200 -s 0.1 -o&#xD;&#xA;    $file-aligned  &quot;$file&quot;; done&#xD;&#xA;&#xD;&#xA;I get an error that says &quot;Error: incompatible indices. Found version 0, expected version 10. &#xD;&#xA;&#xD;&#xA;I have now also tried the following:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta; do ./kallisto quant -i index --single - 200 -s 0.01 -o &#xD;&#xA;    &quot;${file}-aligned&quot; &quot;${file}&quot;; &#xD;&#xA;&#xD;&#xA;but I get an error saying the index file is not found.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;I am now running &#xD;&#xA;&#xD;&#xA;    or file in *.fasta; do ./kallisto quant -i index --single -l 200 -s 0.1 -o $file-aligned &#xD;&#xA;&quot;$file&quot;; done&#xD;&#xA;&#xD;&#xA;This sort of works. The index file error messages are gone, but it runs quant on the same file 10 times instead of each file once. If I run echo $file it spits back the file that my command is running repeatedly. " />
  <row Id="3032" PostHistoryTypeId="5" PostId="954" RevisionGUID="2ff7b13f-8a7a-4878-9410-696771a5dd27" CreationDate="2017-06-30T18:52:35.010" UserId="779" Comment="added 108 characters in body" Text="I am on a Mac using UNIX. I am trying to use the kallisto quant command on all files in a directory (instead of manually entering them). Because I'm running the analysis against the same index file, I first enter the following:&#xD;&#xA;&#xD;&#xA;    ./kallisto index -i --index --make-unique index.fa &#xD;&#xA;&#xD;&#xA;This successfully creates an index file. Then, I tried this:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta; do kallisto quant --single -l 200 -s 0.1 -o $file-aligned &#xD;&#xA;    &quot;$file&quot;; done&#xD;&#xA;While the $file-aligned folders are created, they're empty. I get an error that states that the index file is missing. So I assumed I would need to specify the index file in the command line. I then tried this:&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    for file in *.fasta; do kallisto quant -i index.fa --single -l 200 -s 0.1 -o&#xD;&#xA;    $file-aligned  &quot;$file&quot;; done&#xD;&#xA;&#xD;&#xA;I get an error that says &quot;Error: incompatible indices. Found version 0, expected version 10. &#xD;&#xA;&#xD;&#xA;I have now also tried the following:&#xD;&#xA;&#xD;&#xA;    for file in *.fasta; do ./kallisto quant -i index --single - 200 -s 0.01 -o &#xD;&#xA;    &quot;${file}-aligned&quot; &quot;${file}&quot;; &#xD;&#xA;&#xD;&#xA;but I get an error saying the index file is not found.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;I am now running &#xD;&#xA;&#xD;&#xA;    or file in *.fasta; do ./kallisto quant -i index --single -l 200 -s 0.1 -o $file-aligned &#xD;&#xA;&quot;$file&quot;; done&#xD;&#xA;&#xD;&#xA;This sort of works. The index file error messages are gone, but it runs quant on the same file 10 times instead of each file once. If I run echo $file it spits back the file that my command is running repeatedly. &#xD;&#xA;&#xD;&#xA;If I close terminal, start a new session and run the same command, I get&#xD;&#xA;&#xD;&#xA;    Error: file not found &quot;&quot;" />
  <row Id="3033" PostHistoryTypeId="5" PostId="962" RevisionGUID="b56f6fb7-1f53-46eb-bca5-54bea0e6d4d9" CreationDate="2017-06-30T23:45:53.233" UserId="1025" Comment="provided an example. made it more descriptive." Text="For a study in GEO, I would like to obtain the data table header descriptions, specifically the &quot;VALUE&quot; column for all samples in the study.&#xD;&#xA;&#xD;&#xA;If you go here: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE99511)&#xD;&#xA;and then scroll down and click one of the samples: let's choose &quot;GSM2644971&quot;. Then scroll down and you should see &quot;Data table header descriptions&quot; and below that you should see &quot;**VALUE** Normalized (provided the normalization method) Average Beta&quot;. That information is what I want.&#xD;&#xA;&#xD;&#xA;I tried using assayData() from the Biobase package but I didn't know whether the method takes a sample, a matrix of samples, or something else as a parameter. Apparently it takes a S4 and then returns some random text but I don't know how to use that random text.&#xD;&#xA;&#xD;&#xA;EDITED:&#xD;&#xA;&#xD;&#xA;For example, if I do:&#xD;&#xA;&#xD;&#xA;    library(Biobase)&#xD;&#xA;    library(GEOquery)&#xD;&#xA;    &#xD;&#xA;    getgeo&lt;-getGEO(&quot;GSE99511&quot;)&#xD;&#xA;    assayData(getgeo$GSE99511_series_matrix.txt.gz)&#xD;&#xA;&#xD;&#xA;When I use assayData() I would want it to return &quot;Normalized (provided the normalization method) Average Beta&quot;.&#xD;&#xA;&#xD;&#xA;Instead it returns:&#xD;&#xA;&#xD;&#xA;    &lt;environment: 0x110674178&gt;&#xD;&#xA;&#xD;&#xA;Please let me know if the question doesn't make sense or if there's another method that I should be using.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3034" PostHistoryTypeId="5" PostId="959" RevisionGUID="8e6b800a-245b-466f-b6f7-707814035143" CreationDate="2017-07-01T08:34:09.113" UserId="292" Comment="Started commenting the code" Text="A few years ago, I wrote a python script to convert FCS files into tab-separated format. It was far from handling all the possibilities that the format description offers, but at least it worked for some of the files produced on one of our machine: &lt;http://www.igh.cnrs.fr/equip/Seitz/en_equipe-programmes.html&gt;&#xD;&#xA;&#xD;&#xA;The format documentation I found enabled decoding (see section 3 of the pdf you mention), but it requires reading data in binary mode.&#xD;&#xA;&#xD;&#xA;In case this may be useful, and for the record, here is the code from the above-mentioned script (after stripping comments, some of which are merely copied from the format documentation):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    &quot;&quot;&quot;This script tries to read FCS flow cytometry data.&#xD;&#xA;    Format parsing inspired by information found here:&#xD;&#xA;    http://isac-net.org/Resources-for-Cytometrists/Data-Standards/Data-File-Standards/Flow-Cytometry-Data-File-Format-Standards.aspx&#xD;&#xA;    &quot;&quot;&quot;&#xD;&#xA;    &#xD;&#xA;    import re&#xD;&#xA;    # To decode binary-encoded data&#xD;&#xA;    import struct&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    class Parameter(object):&#xD;&#xA;        &quot;&quot;&quot;This object represents one of the parameter types that are present in a DATA segment of a FCS file.&quot;&quot;&quot;&#xD;&#xA;        __slots__ = (&quot;p_name&quot;, &quot;p_bits&quot;, &quot;p_range&quot;, &quot;p_ampl&quot;, &quot;parser&quot;)&#xD;&#xA;        def __init__(self, p_name, p_bits, p_range, p_ampl):&#xD;&#xA;            self.p_name = p_name&#xD;&#xA;            self.p_bits = p_bits&#xD;&#xA;            self.p_range = p_range&#xD;&#xA;            self.p_ampl = p_ampl&#xD;&#xA;            # Function for parsing a value of the parameter in the data segment&#xD;&#xA;            self.parser = None&#xD;&#xA;    &#xD;&#xA;    ##############################################&#xD;&#xA;    # Here starts the parsing of the header part #&#xD;&#xA;    ##############################################&#xD;&#xA;    &#xD;&#xA;    f = open(sys.argv[1], &quot;rb&quot;)&#xD;&#xA;    # The format name is encoded in 6 letters&#xD;&#xA;    # An ASCII letter is coded with one octet&#xD;&#xA;    file_format = &quot;&quot;.join([f.read(1) for __ in range(6)])&#xD;&#xA;    sys.stdout.write(&quot;Format: %s\n&quot; % file_format)&#xD;&#xA;    # The format descriptions reserves 4 octets that we skip&#xD;&#xA;    skip = f.read(4)&#xD;&#xA;    # 8 octet chunks encode the start and end positions&#xD;&#xA;    # of different parts of the data&#xD;&#xA;    text_start = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    text_end = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    data_start = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    data_end = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    analysis_start = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    analysis_end = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    &#xD;&#xA;    if (analysis_start and analysis_end):&#xD;&#xA;        sys.stderr.write(&quot;Cannot deal with ANALYSIS segment of an FCS file.\n&quot;)&#xD;&#xA;    &#xD;&#xA;    #################################################&#xD;&#xA;    # Here starts the parsing of the &quot;TEXT&quot; portion #&#xD;&#xA;    #################################################&#xD;&#xA;    f.seek(text_start)&#xD;&#xA;    #The first character in the primary TEXT segment is the ASCII delimiter character.&#xD;&#xA;    sep = f.read(1)&#xD;&#xA;    if sep not in [&quot;_&quot;, &quot;@&quot;]:&#xD;&#xA;        alt_sep = &quot;_@_&quot;&#xD;&#xA;    elif sep not in [&quot;_&quot;, &quot;|&quot;]:&#xD;&#xA;        alt_sep = &quot;_|_&quot;&#xD;&#xA;    else:&#xD;&#xA;        assert sep not in [&quot;+&quot;, &quot;|&quot;]&#xD;&#xA;        alt_sep = &quot;+|+&quot;&#xD;&#xA;    text_segment = f.read(text_end - text_start)&#xD;&#xA;    &#xD;&#xA;    fields = text_segment.split(sep)&#xD;&#xA;    &#xD;&#xA;    info = {}&#xD;&#xA;    &#xD;&#xA;    i = 0&#xD;&#xA;    while i &lt; len(fields) - 1:&#xD;&#xA;        key = fields[i]&#xD;&#xA;        i += 1&#xD;&#xA;        val = fields[i]&#xD;&#xA;        i += 1&#xD;&#xA;        # Keywords are case insensitive, they may be written in a file in lower case, upper case, or a&#xD;&#xA;        # mixture of the two. However, an FCS file reader must ignore keyword case. A keyword value may&#xD;&#xA;        # be in lower case, upper case or a mixture of the two. Keyword values are case sensitive.&#xD;&#xA;        info[key.upper()] = val&#xD;&#xA;    print &quot;%s events were detected.&quot; % info[&quot;$TOT&quot;]&#xD;&#xA;    print &quot;Each event is characterized by %s parameters&quot; % info[&quot;$PAR&quot;]&#xD;&#xA;    &#xD;&#xA;    if info[&quot;$NEXTDATA&quot;] != &quot;0&quot;:&#xD;&#xA;        sys.stderr.write(&quot;Some other data exist in the file but hasn't been parsed.\n&quot;)&#xD;&#xA;    &#xD;&#xA;    # L - List mode. For each event, the value of each parameter is stored in the order in which the&#xD;&#xA;    # parameters are described. The number of bits reserved for parameter 1 is described using the&#xD;&#xA;    # $P1B keyword. There can be only one set of list mode data per data set. The $DATATYPE&#xD;&#xA;    # keyword describes the data format. This is the most versatile mode for the storage of flow&#xD;&#xA;    # cytometry data because mode C and mode U data can be created from mode L data.&#xD;&#xA;    assert info[&quot;$MODE&quot;] == &quot;L&quot;&#xD;&#xA;    &#xD;&#xA;    parameters = []&#xD;&#xA;    &#xD;&#xA;    # indices of the parameters&#xD;&#xA;    p_indices = range(1, int(info[&quot;$PAR&quot;]) + 1)&#xD;&#xA;    for i in p_indices:&#xD;&#xA;        p_name = info[&quot;$P%dN&quot; % i]&#xD;&#xA;        p_bits =  info[&quot;$P%dB&quot; % i]&#xD;&#xA;        p_range = info[&quot;$P%dR&quot; % i]&#xD;&#xA;        p_ampl =  info[&quot;$P%dE&quot; % i]&#xD;&#xA;        parameters.append(Parameter(p_name, p_bits, p_range, p_ampl))&#xD;&#xA;    &#xD;&#xA;    sys.stdout.write(&quot;The parameters are:\n%s\n&quot; % &quot;\t&quot;.join([par.p_name for par in parameters]))&#xD;&#xA;    &#xD;&#xA;    # How are 32 bit words organized&#xD;&#xA;    if info[&quot;$BYTEORD&quot;] == &quot;4,3,2,1&quot;:&#xD;&#xA;        endianness = &quot;&gt;&quot;&#xD;&#xA;    else:&#xD;&#xA;        endianness = &quot;&lt;&quot;&#xD;&#xA;        assert info[&quot;$BYTEORD&quot;] == &quot;1,2,3,4&quot;&#xD;&#xA;    &#xD;&#xA;        # I stripped a long comment which is just a copy of the documentation&#xD;&#xA;    # Type of data:&#xD;&#xA;    if info[&quot;$DATATYPE&quot;] == &quot;I&quot;:&#xD;&#xA;        for par in parameters:&#xD;&#xA;            nb_bits = int(par.p_bits)&#xD;&#xA;            assert nb_bits % 8 == 0&#xD;&#xA;            nb_bytes = nb_bits / 8&#xD;&#xA;            # Determine format string for unpacking (see https://docs.python.org/2/library/struct.html)&#xD;&#xA;            if nb_bytes == 1:&#xD;&#xA;                c_type = &quot;B&quot; # unsigned char&#xD;&#xA;            elif nb_bytes == 2:&#xD;&#xA;                c_type = &quot;H&quot; # unsigned short&#xD;&#xA;            elif nb_bytes == 4:&#xD;&#xA;                c_type = &quot;L&quot; # unsigned long&#xD;&#xA;            elif nb_bytes == 8:&#xD;&#xA;                c_type = &quot;Q&quot; # unsigned long long&#xD;&#xA;            else:&#xD;&#xA;                raise ValueError, &quot;Number of bytes (%d) not valid for an integer (see https://docs.python.org/2/library/struct.html#byte-order-size-and-alignment).&quot; % nb_bytes&#xD;&#xA;            fmt = &quot;%s%s&quot; % (endianness, c_type)&#xD;&#xA;            p_range = int(par.p_range)&#xD;&#xA;            def parser(data):&#xD;&#xA;                value = struct.unpack(fmt, data.read(nb_bytes))[0]&#xD;&#xA;                try:&#xD;&#xA;                    assert value &lt; p_range&#xD;&#xA;                except AssertionError:&#xD;&#xA;                    print &quot;Value %s higher than %d&quot; % (str(value), p_range)&#xD;&#xA;                return value&#xD;&#xA;            par.parser = parser&#xD;&#xA;        pass&#xD;&#xA;    else:&#xD;&#xA;        raise NotImplementedError, &quot;Only the parsing of integer value has been implemented so far.&quot;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    out_file = open(sys.argv[2], &quot;w&quot;)&#xD;&#xA;    out_file.write(&quot;#amplification_types\t&quot; + &quot;\t&quot;.join([par.p_ampl for par in parameters]) + &quot;\n&quot;)&#xD;&#xA;    out_file.write(&quot;parameters\t&quot; + &quot;\t&quot;.join([par.p_name for par in parameters]) + &quot;\n&quot;)&#xD;&#xA;    i = 1&#xD;&#xA;    f.seek(data_start)&#xD;&#xA;    while f.tell() &lt; data_end:&#xD;&#xA;        values = []&#xD;&#xA;        for par in parameters:&#xD;&#xA;            values.append(par.parser(f))&#xD;&#xA;        out_file.write(&quot;%d\t&quot; % i + &quot;\t&quot;.join(map(str, values)) + &quot;\n&quot;)&#xD;&#xA;        i += 1&#xD;&#xA;    out_file.close()&#xD;&#xA;    f.close()" />
  <row Id="3035" PostHistoryTypeId="50" PostId="266" RevisionGUID="eba8543b-eed0-496b-9986-12ec0a332be2" CreationDate="2017-07-01T12:54:01.177" UserId="-1" />
  <row Id="3037" PostHistoryTypeId="5" PostId="877" RevisionGUID="ec28cbae-5aaf-4376-87f1-17965c5adeb6" CreationDate="2017-07-01T22:24:07.253" UserId="734" Comment="deleted 69 characters in body" Text="There are 2 new wet methods: [Patient-derived models][1]: [Patient Derived Xenograft (PDX)][2] and [Patient Derived Organoids (PDO)][3] to reflect tumor biology. &#xD;&#xA;&#xD;&#xA;Are there any databases/resources/computational tools/previous work that use the outcomes of PDO/PDX experiments to create a predictive model for cancer or other diseases?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&#xD;&#xA;  [3]: http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html" />
  <row Id="3038" PostHistoryTypeId="4" PostId="877" RevisionGUID="6612f049-f172-473e-85f3-6c8ff2337344" CreationDate="2017-07-02T01:20:34.167" UserId="73" Comment="clarify question" Text="How are PDO and PDX used in computational and predicative models for tumour biology?" />
  <row Id="3039" PostHistoryTypeId="2" PostId="964" RevisionGUID="97ae1e5c-7a62-42e1-a8be-f2806d815329" CreationDate="2017-07-02T12:54:12.990" UserId="150" Text="The short answer is that if you are seeing information in a GEO sample, then it is the sample that you need to download and access using the appropriate method. In this case, that method is `Columns()`:&#xD;&#xA;&#xD;&#xA;    library(GEOquery)&#xD;&#xA;    gsm &lt;- getGEO(&quot;GSM2644971&quot;)&#xD;&#xA;    Columns(gsm)&#xD;&#xA;&#xD;&#xA;              Column      Description&#xD;&#xA;    1         ID_REF                                                            &#xD;&#xA;    2         VALUE       Normalized (provided the normalization method) Average Beta&#xD;&#xA;    3         Detection   Pval&#xD;&#xA;&#xD;&#xA;The longer answer: you will really benefit from sitting down with the GEOquery documentation and trying to understand the objects that it uses and their methods. It is not something you can work out by trial and error for every case-by-case basis. [The tutorial][1] is a really good starting point (you will find `Columns()` in there, for example).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.bioconductor.org/packages/release/bioc/vignettes/GEOquery/inst/doc/GEOquery.html" />
  <row Id="3040" PostHistoryTypeId="2" PostId="965" RevisionGUID="8ef80320-15d1-419d-bda3-b7f4a2d52ce0" CreationDate="2017-07-02T21:04:35.520" UserId="929" Text="If you take a look at my answer in this BioStars post, you can generate a PSSM using `AlignIO` in Biopython:&#xD;&#xA;&#xD;&#xA;https://www.biostars.org/p/259190/#259265" />
  <row Id="3041" PostHistoryTypeId="2" PostId="966" RevisionGUID="f0d57c12-4797-4676-a912-c556ac891f7c" CreationDate="2017-07-02T21:56:20.447" UserId="532" Text="#pigz | awk | wc is the fastest method#&#xD;&#xA;&#xD;&#xA;First off for benchmarks with FASTQ it's best to use a specific real-world example with a known answer.  I've chosen this file:&#xD;&#xA;&#xD;&#xA;ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG01815/sequence_read/ERR047740_1.filt.fastq.gz&#xD;&#xA;&#xD;&#xA;as my test file, the correct answers being:&#xD;&#xA;&#xD;&#xA;    Number of reads: 67051220&#xD;&#xA;    Number of bases in reads: 6034609800&#xD;&#xA;&#xD;&#xA;Next we want to find the fastest way possible to count these, all timings are the average wall-clock time (real) of 10 runs collected with the bash `time` on an otherwise unloaded system:&#xD;&#xA;&#xD;&#xA;###zgrep###&#xD;&#xA;&#xD;&#xA;    zgrep . ERR047740_1.filt.fastq.gz |&#xD;&#xA;         awk 'NR%4==2{c++; l+=length($0)}&#xD;&#xA;              END{&#xD;&#xA;                    print &quot;Number of reads: &quot;c; &#xD;&#xA;                    print &quot;Number of bases in reads: &quot;l&#xD;&#xA;                  }'&#xD;&#xA;&#xD;&#xA;This is the slowest method with an average run-time of 125.35 seconds&#xD;&#xA;&#xD;&#xA;###gzip awk###&#xD;&#xA;&#xD;&#xA;Using `gzip` we gain about another 10 seconds:&#xD;&#xA;&#xD;&#xA;    gzip -dc ERR047740_1.filt.fastq.gz |&#xD;&#xA;         awk 'NR%4==2{c++; l+=length($0)}&#xD;&#xA;              END{&#xD;&#xA;                    print &quot;Number of reads: &quot;c; &#xD;&#xA;                    print &quot;Number of bases in reads: &quot;l&#xD;&#xA;                  }'&#xD;&#xA;&#xD;&#xA;Average run-time is 116.69 seconds&#xD;&#xA;&#xD;&#xA;###Konrad's gzip awk wc variant###&#xD;&#xA;&#xD;&#xA;    fix_base_count() {&#xD;&#xA;        local counts=($(cat))&#xD;&#xA;        echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    gzip -dc ERR047740_1.filt.fastq.gz \&#xD;&#xA;        | awk 'NR % 4 == 2' \&#xD;&#xA;        | wc -cl \&#xD;&#xA;        | fix_base_count&#xD;&#xA;&#xD;&#xA;This runs slower on this test file than the `gzip awk` variant of the solution, average run-time is 122.28 seconds.&#xD;&#xA;&#xD;&#xA;###kseq_test using latest `kseq.h` from [klib](https://github.com/attractivechaos/klib)###&#xD;&#xA;&#xD;&#xA;Code compiled with: `gcc -O2 -o kseq_test kseq_test.c -lz` where `kseq_test.c` is Simon's adaptation of Heng Li's FASTQ parser. &#xD;&#xA;&#xD;&#xA;`kseq_test ERR047740_1.filt.fastq.gz`&#xD;&#xA;&#xD;&#xA;Average run-time is 99.14 seconds, which is better than the `gzip` core utilities based solution so far, but we can do better! &#xD;&#xA;&#xD;&#xA;###piz awk###&#xD;&#xA;&#xD;&#xA;Using Mark Adler's [pigz](https://zlib.net/pigz/) as a drop-in replacement for `gzip`, note that `pigz` gives us a speed gain as on top of gzip as in addition to the main deflate thread it uses another 3 threads for reading, writing and checksum calculations, see the [man page](https://zlib.net/pigz/pigz.pdf) for details.    &#xD;&#xA;&#xD;&#xA;    pigz -dc ERR047740_1.filt.fastq.gz |&#xD;&#xA;         awk 'NR%4==2{c++; l+=length($0)}&#xD;&#xA;              END{&#xD;&#xA;                    print &quot;Number of reads: &quot;c; &#xD;&#xA;                    print &quot;Number of bases in reads: &quot;l&#xD;&#xA;                  }'&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Average run-time is now 93.86 seconds, **this is ~5 seconds faster than the kseq based C code but we can further improve the benchmark**.&#xD;&#xA;&#xD;&#xA;###pigz awk wc###&#xD;&#xA;&#xD;&#xA;Next we use `pigz` as a drop in replacment for Konrad's `wc` variant of the awk based solution.&#xD;&#xA;&#xD;&#xA;    fix_base_count() {&#xD;&#xA;        local counts=($(cat))&#xD;&#xA;        echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    gzip -dc ERR047740_1.filt.fastq.gz \&#xD;&#xA;        | awk 'NR % 4 == 2' \&#xD;&#xA;        | wc -cl \&#xD;&#xA;        | fix_base_count&#xD;&#xA;&#xD;&#xA;**Average run-time is now down to 83.03 seconds, this is ~16 seconds faster than the kseq based solution and ~42 seconds faster than the OPs `zgrep` based solution.**&#xD;&#xA;&#xD;&#xA;Next as a base line lets see just how much of this run-time is due to decompression for the input `fastq.gz` file.&#xD;&#xA;&#xD;&#xA;###gzip alone###&#xD;&#xA;&#xD;&#xA;`gzip -dc ERR047740_1.filt.fastq.gz &gt; /dev/null`&#xD;&#xA;&#xD;&#xA;Average run-time: 105.95 seconds, so the `gzip` based solutions (which also includes `zcat` and `zgrep` as these are provided by `gzip`) are never going to be faster than kseq.&#xD;&#xA;&#xD;&#xA;###pigz alone###&#xD;&#xA;&#xD;&#xA;`pigz -dc ERR047740_1.filt.fastq.gz &gt; /dev/null`&#xD;&#xA; &#xD;&#xA;Average run-time: 77.66 seconds, so quite clearly the additional three threads for read, write and checksum calculation offer a useful advantage.  What's more this speed-up is greater when leveraging the `awk | wc` based solution, it's not clear why, but I expect this is due to the extra write thread.  &#xD;&#xA;&#xD;&#xA;Interestingly average CPU usage across all threads is quite revealing for the various answers, I've collated these stats using GNU time `/usr/bin/time --verbose`&#xD;&#xA;&#xD;&#xA;`zgrep` based solution 133% - must be more than one thread somehow&#xD;&#xA;&#xD;&#xA;`gzip awk` based solution 99% - all gzip based solutions run single threaded at 99% CPU usage&#xD;&#xA;&#xD;&#xA;`pigz | awk` 147% &#xD;&#xA;&#xD;&#xA;`gzip | awk | wc` 99% as with `gzip`&#xD;&#xA;&#xD;&#xA;`pgiz | awk | wc` 155%&#xD;&#xA;&#xD;&#xA;`kseq_test` 99%&#xD;&#xA;&#xD;&#xA;`gzip &gt; dev/null` 99%&#xD;&#xA;&#xD;&#xA;`pigz &gt; dev/null` 155%&#xD;&#xA;&#xD;&#xA;Whilst the main deflate thread in `pigz` will run at 100% CPU load the extra 3 don't quite fully occupy additional cores to 100% (as is evidenced by average CPU usage of ~150%) they do however clearly result in reduced run-time.&#xD;&#xA;&#xD;&#xA;I'm using Ubuntu 16.04.2 LTS**, my `gzip`, `zcat`, `zgrep` versions are all gzip 1.6 and `pigz` is version 2.3.1.  `gcc` is version 5.4.0 &#xD;&#xA;&#xD;&#xA;** I think my patch level is actually 16.04.4 but I've not rebooted for 170 days :p" />
  <row Id="3042" PostHistoryTypeId="5" PostId="966" RevisionGUID="76659974-9a04-4026-bf04-d2cd2b239d47" CreationDate="2017-07-02T22:46:28.427" UserId="532" Comment="Formatting" Text="#pigz | awk | wc is the fastest method#&#xD;&#xA;&#xD;&#xA;First off for benchmarks with FASTQ it's best to use a specific real-world example with a known answer.  I've chosen this file:&#xD;&#xA;&#xD;&#xA;ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG01815/sequence_read/ERR047740_1.filt.fastq.gz&#xD;&#xA;&#xD;&#xA;as my test file, the correct answers being:&#xD;&#xA;&#xD;&#xA;    Number of reads: 67051220&#xD;&#xA;    Number of bases in reads: 6034609800&#xD;&#xA;&#xD;&#xA;Next we want to find the fastest way possible to count these, all timings are the average wall-clock time (real) of 10 runs collected with the bash `time` on an otherwise unloaded system:&#xD;&#xA;&#xD;&#xA;###zgrep###&#xD;&#xA;&#xD;&#xA;    zgrep . ERR047740_1.filt.fastq.gz |&#xD;&#xA;         awk 'NR%4==2{c++; l+=length($0)}&#xD;&#xA;              END{&#xD;&#xA;                    print &quot;Number of reads: &quot;c; &#xD;&#xA;                    print &quot;Number of bases in reads: &quot;l&#xD;&#xA;                  }'&#xD;&#xA;&#xD;&#xA;This is the slowest method with an average run-time of 125.35 seconds&#xD;&#xA;&#xD;&#xA;###gzip awk###&#xD;&#xA;&#xD;&#xA;Using `gzip` we gain about another 10 seconds:&#xD;&#xA;&#xD;&#xA;    gzip -dc ERR047740_1.filt.fastq.gz |&#xD;&#xA;         awk 'NR%4==2{c++; l+=length($0)}&#xD;&#xA;              END{&#xD;&#xA;                    print &quot;Number of reads: &quot;c; &#xD;&#xA;                    print &quot;Number of bases in reads: &quot;l&#xD;&#xA;                  }'&#xD;&#xA;&#xD;&#xA;Average run-time is 116.69 seconds&#xD;&#xA;&#xD;&#xA;###Konrad's gzip awk wc variant###&#xD;&#xA;&#xD;&#xA;    fix_base_count() {&#xD;&#xA;        local counts=($(cat))&#xD;&#xA;        echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    gzip -dc ERR047740_1.filt.fastq.gz \&#xD;&#xA;        | awk 'NR % 4 == 2' \&#xD;&#xA;        | wc -cl \&#xD;&#xA;        | fix_base_count&#xD;&#xA;&#xD;&#xA;This runs slower on this test file than the `gzip awk` variant of the solution, average run-time is 122.28 seconds.&#xD;&#xA;&#xD;&#xA;###kseq_test using latest `kseq.h` from [klib](https://github.com/attractivechaos/klib)###&#xD;&#xA;&#xD;&#xA;Code compiled with: `gcc -O2 -o kseq_test kseq_test.c -lz` where `kseq_test.c` is Simon's adaptation of Heng Li's FASTQ parser. &#xD;&#xA;&#xD;&#xA;`kseq_test ERR047740_1.filt.fastq.gz`&#xD;&#xA;&#xD;&#xA;Average run-time is 99.14 seconds, which is better than the `gzip` core utilities based solution so far, but we can do better! &#xD;&#xA;&#xD;&#xA;###piz awk###&#xD;&#xA;&#xD;&#xA;Using Mark Adler's [pigz](https://zlib.net/pigz/) as a drop-in replacement for `gzip`, note that `pigz` gives us a speed gain as on top of `gzip` as in addition to the main deflate thread it uses another 3 threads for reading, writing and checksum calculations, see the [man page](https://zlib.net/pigz/pigz.pdf) for details.    &#xD;&#xA;&#xD;&#xA;    pigz -dc ERR047740_1.filt.fastq.gz |&#xD;&#xA;         awk 'NR%4==2{c++; l+=length($0)}&#xD;&#xA;              END{&#xD;&#xA;                    print &quot;Number of reads: &quot;c; &#xD;&#xA;                    print &quot;Number of bases in reads: &quot;l&#xD;&#xA;                  }'&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Average run-time is now 93.86 seconds, **this is ~5 seconds faster than the kseq based C code but we can further improve the benchmark**.&#xD;&#xA;&#xD;&#xA;###pigz awk wc###&#xD;&#xA;&#xD;&#xA;Next we use `pigz` as a drop in replacment for Konrad's `wc` variant of the `awk` based solution.&#xD;&#xA;&#xD;&#xA;    fix_base_count() {&#xD;&#xA;        local counts=($(cat))&#xD;&#xA;        echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    gzip -dc ERR047740_1.filt.fastq.gz \&#xD;&#xA;        | awk 'NR % 4 == 2' \&#xD;&#xA;        | wc -cl \&#xD;&#xA;        | fix_base_count&#xD;&#xA;&#xD;&#xA;**Average run-time is now down to 83.03 seconds, this is ~16 seconds faster than the kseq based solution and ~42 seconds faster than the OPs `zgrep` based solution.**&#xD;&#xA;&#xD;&#xA;Next as a baseline lets see just how much of this run-time is due to decompression of the input `fastq.gz` file.&#xD;&#xA;&#xD;&#xA;###gzip alone###&#xD;&#xA;&#xD;&#xA;`gzip -dc ERR047740_1.filt.fastq.gz &gt; /dev/null`&#xD;&#xA;&#xD;&#xA;Average run-time: 105.95 seconds, so the `gzip` based solutions (which also includes `zcat` and `zgrep` as these are provided by `gzip`) are never going to be faster than `kseq_test`.&#xD;&#xA;&#xD;&#xA;###pigz alone###&#xD;&#xA;&#xD;&#xA;`pigz -dc ERR047740_1.filt.fastq.gz &gt; /dev/null`&#xD;&#xA; &#xD;&#xA;**Average run-time: 77.66 seconds, so quite clearly the additional three threads for read, write and checksum calculation offer a useful advantage**.  What's more this speed-up is greater when leveraging the `awk | wc` based solution, it's not clear why, but I expect this is due to the extra write thread.  &#xD;&#xA;&#xD;&#xA;Interestingly average CPU usage across all threads is quite revealing for the various answers, I've collated these stats using GNU time `/usr/bin/time --verbose`&#xD;&#xA;&#xD;&#xA;`zgrep` based solution 133% - must be more than one thread somehow&#xD;&#xA;&#xD;&#xA;`gzip | awk` based solution 99% - all `gzip` based solutions run single-threaded at 99% CPU usage&#xD;&#xA;&#xD;&#xA;`pigz | awk` 147% &#xD;&#xA;&#xD;&#xA;`gzip | awk | wc` 99% as with `gzip`&#xD;&#xA;&#xD;&#xA;`pgiz | awk | wc` 155%&#xD;&#xA;&#xD;&#xA;`kseq_test` 99%&#xD;&#xA;&#xD;&#xA;`gzip &gt; dev/null` 99%&#xD;&#xA;&#xD;&#xA;`pigz &gt; dev/null` 155%&#xD;&#xA;&#xD;&#xA;Whilst the main deflate thread in `pigz` will run at 100% CPU load the extra 3 don't quite fully occupy additional cores to 100% (as is evidenced by average CPU usage of ~150%) they do however clearly result in reduced run-time.&#xD;&#xA;&#xD;&#xA;I'm using Ubuntu 16.04.2 LTS**, my `gzip`, `zcat`, `zgrep` versions are all gzip 1.6 and `pigz` is version 2.3.1.  `gcc` is version 5.4.0 &#xD;&#xA;&#xD;&#xA;** I think my patch level is actually 16.04.4 but I've not rebooted for 170 days :p" />
  <row Id="3043" PostHistoryTypeId="2" PostId="967" RevisionGUID="6a3cdd0d-996c-4514-8ebd-2e699f3890cd" CreationDate="2017-07-02T23:22:34.500" UserId="532" Text="**Potential pitfall!**&#xD;&#xA;&#xD;&#xA;I'm not sure about predictive models, but you need to be aware of a potential pitfall in blindly aligning PDX or PDO based sequencing data without first removing contaminating host organism reads, as otherwise these will lead to a lot of false positive variants caused by miss alignment.  In my experience even a small mount of host material can lead to a ten fold increase in called variants due to miss aligned host reads looking like true variants.  I'd recommend using [Xenome](https://academic.oup.com/bioinformatics/article/28/12/i172/269972), [source](https://github.com/data61/gossamer/blob/master/docs/xenome.md).  &#xD;&#xA;&#xD;&#xA;Note Xenome was designed for DNA sequencing it's not splice-aware, so for RNA-Seq there is no optimal solution other than removing reads which align to the host organisms genome.  Although the issue here is that in conjunction with a splice-aware read aligner synteny between the host and grafted genomes might create some interesting problems.  However for RNA-Seq provided [FACS](https://en.wikipedia.org/wiki/Flow_cytometry#Fluorescence-activated_cell_sorting_.28FACS.29) or similar confirms low-levels of host contamination I expect levels of expression will not be badly affected.  Although this really needs investigating.&#xD;&#xA;&#xD;&#xA;Finally and rather annoyingly Xenome produces none-standard FASTQ so you'll need to fix it's output with:&#xD;&#xA;&#xD;&#xA;`awk '{if (NR % 4 == 1) print \&quot;@\&quot;$0; else if (NR % 4 == 3) print \&quot;+\&quot;$0; else print $0 }'` as reported [here](http://seqanswers.com/forums/showthread.php?t=43872) on seqanswers." />
  <row Id="3046" PostHistoryTypeId="2" PostId="968" RevisionGUID="40aaa879-118d-41b6-89db-d2a929a8d0d5" CreationDate="2017-07-03T09:02:32.610" UserId="532" Text="**Running the jobs in parallel**&#xD;&#xA;&#xD;&#xA;Iteratively is one solution, but since you're not using bootstrapping (which can be run multithread and which is not needed if you're taking the popular [tximport](http://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html) route) you can do better using [GNU parallel](https://www.gnu.org/software/parallel/), this will enable you to run as many jobs as you have execution threads on your system simultaneously. &#xD;&#xA;&#xD;&#xA;1) First off lets define the input FASTQ files and the sample IDs you want to use using a tab-delimited input file like so:&#xD;&#xA;&#xD;&#xA;    sample_1	sample_1_R1.fastq.gz&#xD;&#xA;    sample_2	sample_2_R1.fastq.gz&#xD;&#xA;    sample_3	sample_3_R1.fastq.gz&#xD;&#xA;&#xD;&#xA;Here the first column is the sample ID which will be used for output and the second separated by a tab character is the input file. &#xD;&#xA;&#xD;&#xA;You can produce this using a text editor but as a short-cut you can feed the list of fastq.gz you have into a list via:&#xD;&#xA;&#xD;&#xA;`ls -1 *.fastq.gz &gt; editme`&#xD;&#xA;&#xD;&#xA;2) Next we create a list of jobs to run on our system using a utility script `Make_job_list_kallisto.sh` which consumes our tab delimited input file list above.  I've used your `-l` and `-s` parameters below, note that with paired end data normally these are estimated for you.&#xD;&#xA;&#xD;&#xA;    #!/bin/bash -eu&#xD;&#xA;    &#xD;&#xA;    [ $# -ne 2 ] &amp;&amp; { echo -en &quot;\n*** This script generates jobs for GNU parallel. *** \n\n Error Nothing to do, usage: &lt; input tab delimited list &gt; &lt; output run list file &gt;\n\n&quot; ; exit 1; }&#xD;&#xA;    set -o pipefail&#xD;&#xA;    &#xD;&#xA;    # Get command-line args&#xD;&#xA;    INPUT_LIST=$1&#xD;&#xA;    OUTPUT=$2&#xD;&#xA;    &#xD;&#xA;    # Set counter&#xD;&#xA;    COUNT=1&#xD;&#xA;    END=$(wc -l $INPUT_LIST | awk '{print $1}')&#xD;&#xA;    &#xD;&#xA;    echo &quot; &quot;&#xD;&#xA;    echo &quot; * Input file is: $INPUT_LIST&quot;&#xD;&#xA;    echo &quot; * Number of runs: $END&quot;&#xD;&#xA;    echo &quot; * Output job list for GNU parallel saved to: $OUTPUT&quot;&#xD;&#xA;    echo &quot; &quot;&#xD;&#xA;    &#xD;&#xA;    # Main bit of command-line for job&#xD;&#xA;    CMD=&quot;kallisto quant -I index.fa.idx --single -l 200 -s 0.1&quot;&#xD;&#xA;    &#xD;&#xA;    # Main Loop&#xD;&#xA;    [ -e $OUTPUT ] &amp;&amp; rm $OUTPUT&#xD;&#xA;    while [ $COUNT -le $END ];&#xD;&#xA;    do&#xD;&#xA;        LINE=( $(awk &quot;NR==$COUNT&quot; $INPUT_LIST) )&#xD;&#xA;        # Make file list&#xD;&#xA;        echo &quot;Working on $COUNT of $END Sample ID: ${LINE[0]}, Files ${LINE[@]:1}&quot;&#xD;&#xA;        echo &quot;$CMD -o ${LINE[0]} ${LINE[@]:1}&quot; &gt;&gt; $OUTPUT&#xD;&#xA;        ((COUNT++))&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;We use the script like so:&#xD;&#xA;&#xD;&#xA;`Make_job_list_kallisto.sh input_file_list.txt job_list.txt`&#xD;&#xA;&#xD;&#xA;3) Now we have a job list for GNU parallel we can run our jobs in parallel like so:&#xD;&#xA;&#xD;&#xA;`parallel --progress --jobs 4 --joblog kallisto_joblog.txt &lt; job_list.txt`&#xD;&#xA;&#xD;&#xA;This will run 4 of the kallisto jobs simultaneously (Increase this if you have more threads - don't mind consuming all the IO/CPU threads on your system) producing a nice progress report of those completed, additionally a job log will be written to the file `kallisto_joblog.txt`.  &#xD;&#xA;&#xD;&#xA;For paired end data I already have these scripts + similar instructions on [GitHub](https://github.com/MattBashton/grolar) which people might find useful. " />
  <row Id="3047" PostHistoryTypeId="2" PostId="969" RevisionGUID="cada43fe-91e5-4aee-902a-516889f14a67" CreationDate="2017-07-03T09:09:33.200" UserId="294" Text="I would like to ask if anyone has experience in running a subset of the PASA pipeline, in particular for the reconciliation of some experimental 'transcripts' with the reference annotation.&#xD;&#xA;&#xD;&#xA;In more detail, I am working with RNA-seq data from D. melanogaster. I have reconstructed the 'transcripts' using Trinity. I have aligned these 'transcripts' to the reference genome using GMAP. Now I would like to match these 'experimental transcripts' with the reference annotation, to see how they compare.&#xD;&#xA;&#xD;&#xA;I was wondering if I can run just a subset of the whole PASA pipeline, so basically skipping the step of alignment to the reference and using the alignment file that I generated externally.&#xD;&#xA;&#xD;&#xA;Many thanks, &#xD;&#xA;&#xD;&#xA;Amina&#xD;&#xA;&#xD;&#xA;cross-posted on the PASA Google Group pasapipeline-users -&gt; https://groups.google.com/forum/#!topic/pasapipeline-users/cyG9e8-HjG8&#xD;&#xA;" />
  <row Id="3048" PostHistoryTypeId="1" PostId="969" RevisionGUID="cada43fe-91e5-4aee-902a-516889f14a67" CreationDate="2017-07-03T09:09:33.200" UserId="294" Text="PASA pipeline: compare experimental transcripts to the reference annotation" />
  <row Id="3049" PostHistoryTypeId="3" PostId="969" RevisionGUID="cada43fe-91e5-4aee-902a-516889f14a67" CreationDate="2017-07-03T09:09:33.200" UserId="294" Text="&lt;rna-seq&gt;&lt;assembly&gt;&lt;transcriptome&gt;&lt;isoform&gt;" />
  <row Id="3050" PostHistoryTypeId="5" PostId="969" RevisionGUID="e4e80963-8739-4883-bc28-1f14bd0e54a7" CreationDate="2017-07-03T09:11:54.907" UserId="77" Comment="added 40 characters in body; edited tags" Text="I would like to ask if anyone has experience in running a subset of the PASA pipeline, in particular for the reconciliation of some experimental 'transcripts' with the reference annotation.&#xD;&#xA;&#xD;&#xA;In more detail, I am working with RNA-seq data from D. melanogaster. I have reconstructed the 'transcripts' using Trinity. I have aligned these 'transcripts' to the reference genome using GMAP. Now I would like to match these 'experimental transcripts' with the reference annotation, to see how they compare.&#xD;&#xA;&#xD;&#xA;I was wondering if I can run just a subset of the whole PASA pipeline, so basically skipping the step of alignment to the reference and using the alignment file that I generated externally.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Cross-posted on the [PASA Google Group pasapipeline-users](https://groups.google.com/forum/#!topic/pasapipeline-users/cyG9e8-HjG8)&#xD;&#xA;&#xD;&#xA;Cross-posted [on biostars](https://www.biostars.org/p/260594/)" />
  <row Id="3051" PostHistoryTypeId="6" PostId="969" RevisionGUID="e4e80963-8739-4883-bc28-1f14bd0e54a7" CreationDate="2017-07-03T09:11:54.907" UserId="77" Comment="added 40 characters in body; edited tags" Text="&lt;rna-seq&gt;&lt;assembly&gt;&lt;transcriptome&gt;&lt;isoform&gt;&lt;pasa&gt;" />
  <row Id="3052" PostHistoryTypeId="5" PostId="969" RevisionGUID="d2a4eb0a-f91e-4be2-b4c1-b6abf0da9c9e" CreationDate="2017-07-03T09:12:02.720" UserId="294" Comment="added 168 characters in body; edited tags" Text="I would like to ask if anyone has experience in running a subset of the PASA pipeline, in particular for the reconciliation of some experimental 'transcripts' with the reference annotation.&#xD;&#xA;&#xD;&#xA;In more detail, I am working with RNA-seq data from D. melanogaster. I have reconstructed the 'transcripts' using Trinity. I have aligned these 'transcripts' to the reference genome using GMAP. Now I would like to match these 'experimental transcripts' with the reference annotation, to see how they compare.&#xD;&#xA;&#xD;&#xA;I was wondering if I can run just a subset of the whole PASA pipeline, so basically skipping the step of alignment to the reference and using the alignment file that I generated externally.&#xD;&#xA;&#xD;&#xA;Also, I would be glad to receive more general redirecting to any released software allowing the comparison of experimental transcripts to the reference annotation. &#xD;&#xA;&#xD;&#xA;Many thanks, &#xD;&#xA;&#xD;&#xA;Amina&#xD;&#xA;&#xD;&#xA;cross-posted on the PASA Google Group pasapipeline-users -&gt; https://groups.google.com/forum/#!topic/pasapipeline-users/cyG9e8-HjG8&#xD;&#xA;" />
  <row Id="3053" PostHistoryTypeId="6" PostId="969" RevisionGUID="d2a4eb0a-f91e-4be2-b4c1-b6abf0da9c9e" CreationDate="2017-07-03T09:12:02.720" UserId="294" Comment="added 168 characters in body; edited tags" Text="&lt;rna-seq&gt;&lt;assembly&gt;&lt;transcriptome&gt;&lt;isoform&gt;&lt;pasa&gt;" />
  <row Id="3054" PostHistoryTypeId="5" PostId="969" RevisionGUID="ab03d96b-181d-4d7d-b724-ec788660a0e9" CreationDate="2017-07-03T09:12:40.647" UserId="77" Comment="added 40 characters in body" Text="I would like to ask if anyone has experience in running a subset of the PASA pipeline, in particular for the reconciliation of some experimental 'transcripts' with the reference annotation.&#xD;&#xA;&#xD;&#xA;In more detail, I am working with RNA-seq data from D. melanogaster. I have reconstructed the 'transcripts' using Trinity. I have aligned these 'transcripts' to the reference genome using GMAP. Now I would like to match these 'experimental transcripts' with the reference annotation, to see how they compare.&#xD;&#xA;&#xD;&#xA;I was wondering if I can run just a subset of the whole PASA pipeline, so basically skipping the step of alignment to the reference and using the alignment file that I generated externally.&#xD;&#xA;&#xD;&#xA;Also, I would be glad to receive more general redirecting to any released software allowing the comparison of experimental transcripts to the reference annotation. &#xD;&#xA;&#xD;&#xA;Cross-posted on the [PASA Google Group pasapipeline-users](https://groups.google.com/forum/#!topic/pasapipeline-users/cyG9e8-HjG8)&#xD;&#xA;&#xD;&#xA;Cross-posted [on biostars](https://www.biostars.org/p/260594/)&#xD;&#xA;" />
  <row Id="3055" PostHistoryTypeId="5" PostId="968" RevisionGUID="96e0f2b1-5b1e-49a9-bda8-71d229241334" CreationDate="2017-07-03T09:18:28.663" UserId="532" Comment="Added index generation step" Text="**Running the jobs in parallel**&#xD;&#xA;&#xD;&#xA;Iteratively is one solution, but since you're not using bootstrapping (which can be run multithread and which is not needed if you're taking the popular [tximport](http://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html) route) you can do better using [GNU parallel](https://www.gnu.org/software/parallel/), this will enable you to run as many jobs as you have execution threads on your system simultaneously. &#xD;&#xA;&#xD;&#xA;0) You'll need to make the index first:&#xD;&#xA;&#xD;&#xA;`kallisto index --make-unique -I index.fa.idx index.fa` &#xD;&#xA;&#xD;&#xA;1) Next lets define the input FASTQ files and the sample IDs you want to use using a tab-delimited input file like so:&#xD;&#xA;&#xD;&#xA;    sample_1	sample_1_R1.fastq.gz&#xD;&#xA;    sample_2	sample_2_R1.fastq.gz&#xD;&#xA;    sample_3	sample_3_R1.fastq.gz&#xD;&#xA;&#xD;&#xA;Here the first column is the sample ID which will be used for output and the second separated by a tab character is the input file. &#xD;&#xA;&#xD;&#xA;You can produce this using a text editor but as a short-cut you can feed the list of fastq.gz you have into a list via:&#xD;&#xA;&#xD;&#xA;`ls -1 *.fastq.gz &gt; editme`&#xD;&#xA;&#xD;&#xA;2) Next we create a list of jobs to run on our system using a utility script `Make_job_list_kallisto.sh` which consumes our tab delimited input file list above.  I've used your `-l` and `-s` parameters below, note that with paired end data normally these are estimated for you.&#xD;&#xA;&#xD;&#xA;    #!/bin/bash -eu&#xD;&#xA;    &#xD;&#xA;    [ $# -ne 2 ] &amp;&amp; { echo -en &quot;\n*** This script generates jobs for GNU parallel. *** \n\n Error Nothing to do, usage: &lt; input tab delimited list &gt; &lt; output run list file &gt;\n\n&quot; ; exit 1; }&#xD;&#xA;    set -o pipefail&#xD;&#xA;    &#xD;&#xA;    # Get command-line args&#xD;&#xA;    INPUT_LIST=$1&#xD;&#xA;    OUTPUT=$2&#xD;&#xA;    &#xD;&#xA;    # Set counter&#xD;&#xA;    COUNT=1&#xD;&#xA;    END=$(wc -l $INPUT_LIST | awk '{print $1}')&#xD;&#xA;    &#xD;&#xA;    echo &quot; &quot;&#xD;&#xA;    echo &quot; * Input file is: $INPUT_LIST&quot;&#xD;&#xA;    echo &quot; * Number of runs: $END&quot;&#xD;&#xA;    echo &quot; * Output job list for GNU parallel saved to: $OUTPUT&quot;&#xD;&#xA;    echo &quot; &quot;&#xD;&#xA;    &#xD;&#xA;    # Main bit of command-line for job&#xD;&#xA;    CMD=&quot;kallisto quant -I index.fa.idx --single -l 200 -s 0.1&quot;&#xD;&#xA;    &#xD;&#xA;    # Main Loop&#xD;&#xA;    [ -e $OUTPUT ] &amp;&amp; rm $OUTPUT&#xD;&#xA;    while [ $COUNT -le $END ];&#xD;&#xA;    do&#xD;&#xA;        LINE=( $(awk &quot;NR==$COUNT&quot; $INPUT_LIST) )&#xD;&#xA;        # Make file list&#xD;&#xA;        echo &quot;Working on $COUNT of $END Sample ID: ${LINE[0]}, Files ${LINE[@]:1}&quot;&#xD;&#xA;        echo &quot;$CMD -o ${LINE[0]} ${LINE[@]:1}&quot; &gt;&gt; $OUTPUT&#xD;&#xA;        ((COUNT++))&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;We use the script like so:&#xD;&#xA;&#xD;&#xA;`Make_job_list_kallisto.sh input_file_list.txt job_list.txt`&#xD;&#xA;&#xD;&#xA;3) Now we have a job list for GNU parallel we can run our jobs in parallel like so:&#xD;&#xA;&#xD;&#xA;`parallel --progress --jobs 4 --joblog kallisto_joblog.txt &lt; job_list.txt`&#xD;&#xA;&#xD;&#xA;This will run 4 of the kallisto jobs simultaneously (Increase this if you have more threads - don't mind consuming all the IO/CPU threads on your system) producing a nice progress report of those completed, additionally a job log will be written to the file `kallisto_joblog.txt`.  &#xD;&#xA;&#xD;&#xA;For paired end data I already have these scripts + similar instructions on [GitHub](https://github.com/MattBashton/grolar) which people might find useful. " />
  <row Id="3056" PostHistoryTypeId="5" PostId="968" RevisionGUID="84fa0d37-ddc0-4408-84f6-946062eadee1" CreationDate="2017-07-03T09:52:40.590" UserId="532" Comment="Added note about -t" Text="**Running the jobs in parallel**&#xD;&#xA;&#xD;&#xA;Iteratively is one solution, but since you're not using bootstrapping, which can be run multithread (and is not needed if you're taking the popular [tximport](http://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html) route) you can do better using [GNU parallel](https://www.gnu.org/software/parallel/), this will enable you to run as many jobs as you have execution threads on your system simultaneously.  Note that Kalliso only runs multithreaded with `-t` if you're using bootstrapping.&#xD;&#xA;&#xD;&#xA;0) You'll need to make the index first:&#xD;&#xA;&#xD;&#xA;`kallisto index --make-unique -I index.fa.idx index.fa` &#xD;&#xA;&#xD;&#xA;1) Next lets define the input FASTQ files and the sample IDs you want to use using a tab-delimited input file like so:&#xD;&#xA;&#xD;&#xA;    sample_1	sample_1_R1.fastq.gz&#xD;&#xA;    sample_2	sample_2_R1.fastq.gz&#xD;&#xA;    sample_3	sample_3_R1.fastq.gz&#xD;&#xA;&#xD;&#xA;Here the first column is the sample ID which will be used for output and the second separated by a tab character is the input file. &#xD;&#xA;&#xD;&#xA;You can produce this using a text editor but as a short-cut you can feed the list of fastq.gz you have into a list via:&#xD;&#xA;&#xD;&#xA;`ls -1 *.fastq.gz &gt; editme`&#xD;&#xA;&#xD;&#xA;2) Next we create a list of jobs to run on our system using a utility script `Make_job_list_kallisto.sh` which consumes our tab delimited input file list above.  I've used your `-l` and `-s` parameters below, note that with paired end data normally these are estimated for you.&#xD;&#xA;&#xD;&#xA;    #!/bin/bash -eu&#xD;&#xA;    &#xD;&#xA;    [ $# -ne 2 ] &amp;&amp; { echo -en &quot;\n*** This script generates jobs for GNU parallel. *** \n\n Error Nothing to do, usage: &lt; input tab delimited list &gt; &lt; output run list file &gt;\n\n&quot; ; exit 1; }&#xD;&#xA;    set -o pipefail&#xD;&#xA;    &#xD;&#xA;    # Get command-line args&#xD;&#xA;    INPUT_LIST=$1&#xD;&#xA;    OUTPUT=$2&#xD;&#xA;    &#xD;&#xA;    # Set counter&#xD;&#xA;    COUNT=1&#xD;&#xA;    END=$(wc -l $INPUT_LIST | awk '{print $1}')&#xD;&#xA;    &#xD;&#xA;    echo &quot; &quot;&#xD;&#xA;    echo &quot; * Input file is: $INPUT_LIST&quot;&#xD;&#xA;    echo &quot; * Number of runs: $END&quot;&#xD;&#xA;    echo &quot; * Output job list for GNU parallel saved to: $OUTPUT&quot;&#xD;&#xA;    echo &quot; &quot;&#xD;&#xA;    &#xD;&#xA;    # Main bit of command-line for job&#xD;&#xA;    CMD=&quot;kallisto quant -I index.fa.idx --single -l 200 -s 0.1&quot;&#xD;&#xA;    &#xD;&#xA;    # Main Loop&#xD;&#xA;    [ -e $OUTPUT ] &amp;&amp; rm $OUTPUT&#xD;&#xA;    while [ $COUNT -le $END ];&#xD;&#xA;    do&#xD;&#xA;        LINE=( $(awk &quot;NR==$COUNT&quot; $INPUT_LIST) )&#xD;&#xA;        # Make file list&#xD;&#xA;        echo &quot;Working on $COUNT of $END Sample ID: ${LINE[0]}, Files ${LINE[@]:1}&quot;&#xD;&#xA;        echo &quot;$CMD -o ${LINE[0]} ${LINE[@]:1}&quot; &gt;&gt; $OUTPUT&#xD;&#xA;        ((COUNT++))&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;We use the script like so:&#xD;&#xA;&#xD;&#xA;`Make_job_list_kallisto.sh input_file_list.txt job_list.txt`&#xD;&#xA;&#xD;&#xA;3) Now we have a job list for GNU parallel we can run our jobs in parallel like so:&#xD;&#xA;&#xD;&#xA;`parallel --progress --jobs 4 --joblog kallisto_joblog.txt &lt; job_list.txt`&#xD;&#xA;&#xD;&#xA;This will run 4 of the kallisto jobs simultaneously (Increase this if you have more threads - don't mind consuming all the IO/CPU threads on your system) producing a nice progress report of those completed, additionally a job log will be written to the file `kallisto_joblog.txt`.  &#xD;&#xA;&#xD;&#xA;For paired end data I already have these scripts + similar instructions on [GitHub](https://github.com/MattBashton/grolar) which people might find useful. " />
  <row Id="3057" PostHistoryTypeId="5" PostId="968" RevisionGUID="f03bc374-64a8-4070-acf6-4b22f2b8f7ea" CreationDate="2017-07-03T10:47:05.697" UserId="29" Comment="formatting" Text="**Running the jobs in parallel**&#xD;&#xA;&#xD;&#xA;Iteratively is one solution, but since you're not using bootstrapping, which can be run multithread (and is not needed if you're taking the popular [tximport](http://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html) route) you can do better using [GNU parallel](https://www.gnu.org/software/parallel/), this will enable you to run as many jobs as you have execution threads on your system simultaneously.  Note that Kalliso only runs multithreaded with `-t` if you're using bootstrapping.&#xD;&#xA;&#xD;&#xA;0. You'll need to make the index first:&#xD;&#xA;&#xD;&#xA;        kallisto index --make-unique -I index.fa.idx index.fa&#xD;&#xA;&#xD;&#xA;1. Next lets define the input FASTQ files and the sample IDs you want to use using a tab-delimited input file like so:&#xD;&#xA;&#xD;&#xA;        sample_1	sample_1_R1.fastq.gz&#xD;&#xA;        sample_2	sample_2_R1.fastq.gz&#xD;&#xA;        sample_3	sample_3_R1.fastq.gz&#xD;&#xA;&#xD;&#xA;    Here the first column is the sample ID which will be used for output and the second separated by a tab character is the input file. &#xD;&#xA;    &#xD;&#xA;    You can produce this using a text editor but as a short-cut you can feed the list of fastq.gz you have into a list via:&#xD;&#xA;&#xD;&#xA;        ls -1 *.fastq.gz &gt; editme&#xD;&#xA;&#xD;&#xA;2. Next we create a list of jobs to run on our system using a utility script `Make_job_list_kallisto.sh` which consumes our tab delimited input file list above.  I've used your `-l` and `-s` parameters below, note that with paired end data normally these are estimated for you.&#xD;&#xA;&#xD;&#xA;        #!/bin/bash -eu&#xD;&#xA;        &#xD;&#xA;        [ $# -ne 2 ] &amp;&amp; { echo -en &quot;\n*** This script generates jobs for GNU parallel. *** \n\n Error Nothing to do, usage: &lt; input tab delimited list &gt; &lt; output run list file &gt;\n\n&quot; ; exit 1; }&#xD;&#xA;        set -o pipefail&#xD;&#xA;        &#xD;&#xA;        # Get command-line args&#xD;&#xA;        INPUT_LIST=$1&#xD;&#xA;        OUTPUT=$2&#xD;&#xA;        &#xD;&#xA;        # Set counter&#xD;&#xA;        COUNT=1&#xD;&#xA;        END=$(wc -l $INPUT_LIST | awk '{print $1}')&#xD;&#xA;        &#xD;&#xA;        echo &quot; &quot;&#xD;&#xA;        echo &quot; * Input file is: $INPUT_LIST&quot;&#xD;&#xA;        echo &quot; * Number of runs: $END&quot;&#xD;&#xA;        echo &quot; * Output job list for GNU parallel saved to: $OUTPUT&quot;&#xD;&#xA;        echo &quot; &quot;&#xD;&#xA;        &#xD;&#xA;        # Main bit of command-line for job&#xD;&#xA;        CMD=&quot;kallisto quant -I index.fa.idx --single -l 200 -s 0.1&quot;&#xD;&#xA;        &#xD;&#xA;        # Main Loop&#xD;&#xA;        [ -e $OUTPUT ] &amp;&amp; rm $OUTPUT&#xD;&#xA;        while [ $COUNT -le $END ];&#xD;&#xA;        do&#xD;&#xA;            LINE=( $(awk &quot;NR==$COUNT&quot; $INPUT_LIST) )&#xD;&#xA;            # Make file list&#xD;&#xA;            echo &quot;Working on $COUNT of $END Sample ID: ${LINE[0]}, Files ${LINE[@]:1}&quot;&#xD;&#xA;            echo &quot;$CMD -o ${LINE[0]} ${LINE[@]:1}&quot; &gt;&gt; $OUTPUT&#xD;&#xA;            ((COUNT++))&#xD;&#xA;        done&#xD;&#xA;&#xD;&#xA;    We use the script like so:&#xD;&#xA;    &#xD;&#xA;         Make_job_list_kallisto.sh input_file_list.txt job_list.txt&#xD;&#xA;&#xD;&#xA;3. Now we have a job list for GNU parallel we can run our jobs in parallel like so:&#xD;&#xA;&#xD;&#xA;        parallel --progress --jobs 4 --joblog kallisto_joblog.txt &lt; job_list.txt&#xD;&#xA;&#xD;&#xA;    This will run 4 of the kallisto jobs simultaneously (Increase this if you have more threads - don't mind consuming all the IO/CPU threads on your system) producing a nice progress report of those completed, additionally a job log will be written to the file `kallisto_joblog.txt`.  &#xD;&#xA;&#xD;&#xA;For paired end data I already have these scripts + similar instructions on [GitHub](https://github.com/MattBashton/grolar) which people might find useful. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3058" PostHistoryTypeId="2" PostId="970" RevisionGUID="789b1a99-c0f8-4d21-bab3-e6ad9246e389" CreationDate="2017-07-03T11:49:21.247" UserId="1037" Text="For a diet analysis of an insect-eating animal, all species in a sample shall be identified. For this a sequencing of the metagenomic sample was done, where the COI/COX region was used as a barcode and amplified. This resulted in 20 million reads (after quality control). &#xD;&#xA;&#xD;&#xA;While looking for an apropriate workflow, I am faced with these issues:&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA; - For many tools 20 million reads are to much to handle (BLAST for&#xD;&#xA;   instance)&#xD;&#xA;  &#xD;&#xA; - For the majority of metagenomic tools the focus is on micro-organisms, not insects&#xD;&#xA; &#xD;&#xA; - The database to search against must contain COI/COX data, mainly from insects, however, the animal could also eat snails or frog eggs...&#xD;&#xA;   &#xD;&#xA;&#xD;&#xA; - Since the reads are amplicons some analysis are not recommended&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Leaving me with these questions:&#xD;&#xA;&#xD;&#xA; 1. What are the typical analysis steps in a workflow like this?&#xD;&#xA; 2. Which tools could I use for my purpose?&#xD;&#xA; 3. Where could I get a database of COI/COX data?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3059" PostHistoryTypeId="1" PostId="970" RevisionGUID="789b1a99-c0f8-4d21-bab3-e6ad9246e389" CreationDate="2017-07-03T11:49:21.247" UserId="1037" Text="Workflow of metabarcoding analysis" />
  <row Id="3060" PostHistoryTypeId="3" PostId="970" RevisionGUID="789b1a99-c0f8-4d21-bab3-e6ad9246e389" CreationDate="2017-07-03T11:49:21.247" UserId="1037" Text="&lt;metagenome&gt;&lt;barcode&gt;" />
  <row Id="3061" PostHistoryTypeId="2" PostId="971" RevisionGUID="5b82cd30-3a76-4551-9790-8d669f76b924" CreationDate="2017-07-03T12:13:22.150" UserId="581" Text="I'm working on something similar as a side project. I'll detail the steps that I'm using, but I'm curious if others can suggest a better way.&#xD;&#xA;&#xD;&#xA;- You don't mention if your data are paired-end, but in my case I have paired-end sequences that overlap in the middle. I used [FLASh](https://ccb.jhu.edu/software/FLASH) to merge the reads and [seqtk](https://github.com/lh3/seqtk) to convert the reads to fasta format.&#xD;&#xA;&#xD;&#xA;- I used the cluster_fast algorithm in [vsearch](https://github.com/torognes/vsearch) to cluster sequences at 100% identity, then filtered out rare sequence types (&lt;10 sequences) to account for sequencing errors.&#xD;&#xA;&#xD;&#xA;- This resulted in a manageable number of unique sequences, which I searched against a custom database using vsearch, but which could also have been searched against the nr database using blast." />
  <row Id="3062" PostHistoryTypeId="2" PostId="972" RevisionGUID="42eee476-ebca-4eb9-87e4-2e065d516777" CreationDate="2017-07-03T14:25:39.637" UserId="818" Text="I want to get the alignment of chain A of 1kf6 (PDB ID) from pfam database [1kf6 chain A][1]. This protein chain has two main domains (FAD_binding_2 and 	Succ_DH_flav_C). In pfam there is a link to one of these domains and after clicking in one of the mentioned domains in the table below the page, another page comes at the top of which there is a link to architectures. For example if we click on &quot;FAD_binding_2&quot; in the table, there are 220 architectures that have this domain and after clicking on that button (at the top of page), a page opens [sequences haveing that domain][2]. In this page 7471 sequences with the following architecture: FAD_binding_2, Succ_DH_flav_C, resembles most to my chain (because it has both FAD_binding_2 and Succ_DH_flav_C domains). If we want to see all 7471 sequences with that architecture we can click on the show button under that architecture and finally we can see all of that 7471 architecture resembling our chain A sequence. Down the page we can see other architectures that all have FAD_binding_2, but some of them do not have both FAD_binding_2 and Succ_DH_flav_C domains (or some of them have other extra domains that we are not interested in) and so we do not need their sequences and their alignments. when we want to get the alignment of full sequences at the left side of the page, we can see that we can just get the alignment of all 15696 sequences and it seems that there is no way to get the alignment of our favourite 7471 sequences. I would like to know if there is any way to get the alignment or at least the fasta file of our favourite 7471 sequences (which have both of our favourite domains not just one of them)?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://pfam.xfam.org/protein/FRDA_ECOLI&#xD;&#xA;  [2]: http://pfam.xfam.org/family/Succ_DH_flav_C#tabview=tab1" />
  <row Id="3063" PostHistoryTypeId="1" PostId="972" RevisionGUID="42eee476-ebca-4eb9-87e4-2e065d516777" CreationDate="2017-07-03T14:25:39.637" UserId="818" Text="How to get a protein family sequence from Pfam database" />
  <row Id="3064" PostHistoryTypeId="3" PostId="972" RevisionGUID="42eee476-ebca-4eb9-87e4-2e065d516777" CreationDate="2017-07-03T14:25:39.637" UserId="818" Text="&lt;sequence-homology&gt;" />
  <row Id="3065" PostHistoryTypeId="2" PostId="973" RevisionGUID="3a82579e-4f90-4d05-9818-1f12d67c7f0f" CreationDate="2017-07-03T14:28:22.810" UserId="292" Text="Regarding your third question (&quot;Where could I get a database of COI/COX data?&quot;), you can apparently search and download fasta sequences from [BOLD](http://www.boldsystems.org/index.php/Public_SearchTerms)." />
  <row Id="3066" PostHistoryTypeId="5" PostId="973" RevisionGUID="3cadd375-a340-4650-aba1-84defcc91c12" CreationDate="2017-07-03T14:37:40.740" UserId="292" Comment="added 204 characters in body" Text="Regarding your third question (&quot;Where could I get a database of COI/COX data?&quot;), you can apparently search and download fasta sequences from [BOLD](http://www.boldsystems.org/index.php/Public_SearchTerms).&#xD;&#xA;&#xD;&#xA;However, I don't see a &quot;select all&quot; button to select all records found by one search in a single fasta file.&#xD;&#xA;&#xD;&#xA;There seems to exist an R API: &lt;https://cran.r-project.org/web/packages/bold/bold.pdf&gt;&#xD;&#xA;" />
  <row Id="3067" PostHistoryTypeId="2" PostId="974" RevisionGUID="1409ccbe-b4ab-4ce7-b771-cf302e35aef7" CreationDate="2017-07-03T14:41:40.387" UserId="532" Text="I have high depth variant calling created using the HaplotypeCaller with `--output_mode EMIT_ALL_SITES` I'm interested in finding all sites (regardless of genotype call heterozygous or homozygous) where at least one of the alternative alleles have an `AD` value (Allelic Depth) greater than 10, *i*.*e*. are supported by more than 10 reads. &#xD;&#xA;&#xD;&#xA;So in the example VCF snippet below I'm wanting to select lines: 6,7,8,12,13 and 14, which have GT:AD values `1/1:1,988:989` `0/1:116,92` `0/1:220,234` `0/1:62,611` `1/1:0,109` respectively.&#xD;&#xA;&#xD;&#xA;    #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	12908_DIAG&#xD;&#xA;    3	187446740	.	T	.	Infinity	.	AN=2;DP=1095;MQ=60.00	GT:AD:DP	0/0:1095:1095&#xD;&#xA;    3	187446741	.	C	.	Infinity	.	AN=2;DP=1117;MQ=60.00	GT:AD:DP	0/0:1117:1117&#xD;&#xA;    3	187446752	.	A	.	Infinity	.	AN=2;DP=1297;MQ=60.00	GT:AD:DP	0/0:1297:1297&#xD;&#xA;    3	187446763	.	C	.	Infinity	.	AN=2;DP=1494;MQ=60.00	GT:AD:DP	0/0:1494:1494&#xD;&#xA;    3	187451574	.	C	.	Infinity	.	AN=2;DP=1493;MQ=60.00	GT:AD:DP	0/0:1493:1493&#xD;&#xA;    3	187451609	rs1880101	A	G	39794.03	.	AC=2;AF=1.00;AN=2;BaseQRankSum=1.859;ClippingRankSum=0.000;DB;DP=995;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQRankSum=0.000;QD=24.56;ReadPosRankSum=0.406;SOR=8.234	GT:AD:DP:GQ:PL	1/1:1,988:989:99:39808,2949,0&#xD;&#xA;    4	1803279	.	T	G	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-6.652;ClippingRankSum=0.000;DP=245;ExcessHet=3.0103;FS=89.753;MLEAC=0;MLEAF=0.00;MQ=59.97;MQRankSum=0.000;ReadPosRankSum=-2.523;SOR=6.357	GT:AD:DP:GQ:PL	0/0:211,23:234:99:0,364,6739&#xD;&#xA;    4	1803307	rs2305183	T	C	2486.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=-5.049;ClippingRankSum=0.000;DB;DP=215;ExcessHet=3.0103;FS=1.110;MLEAC=1;MLEAF=0.500;MQ=59.97;MQRankSum=0.000;QD=11.95;ReadPosRankSum=-0.045;SOR=0.809	GT:AD:DP:GQ:PL	0/1:116,92:208:99:2494,0,3673&#xD;&#xA;    4	1803671	.	C	A	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-0.880;ClippingRankSum=0.000;DP=450;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.953;SOR=0.572	GT:AD:DP:GQ:PL	0/0:445,2:447:99:0,1272,15958&#xD;&#xA;    4	1803681	.	T	C	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-1.654;ClippingRankSum=0.000;DP=483;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.422;SOR=0.664	GT:AD:DP:GQ:PL	0/0:479,2:481:99:0,1408,18538&#xD;&#xA;    4	1803703	.	A	G	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-1.704;ClippingRankSum=0.000;DP=458;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=0.299;SOR=0.497	GT:AD:DP:GQ:PL	0/0:454,2:456:99:0,1325,18095&#xD;&#xA;    4	1803704	rs2234909	T	C	6676.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=-2.605;ClippingRankSum=0.000;DB;DP=456;ExcessHet=3.0103;FS=1.753;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=14.71;ReadPosRankSum=0.324;SOR=0.849	GT:AD:DP:GQ:PL	0/1:220,234:454:99:6684,0,6366&#xD;&#xA;    4	1803824	rs2305184	C	G	2030.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=8.083;ClippingRankSum=0.000;DB;DP=124;ExcessHet=3.0103;FS=6.128;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=16.51;ReadPosRankSum=0.180;SOR=0.096	GT:AD:DP:GQ:PL	0/1:62,61:123:99:2038,0,1766&#xD;&#xA;    4	1805296	rs3135883	G	A	3876.03	.	AC=2;AF=1.00;AN=2;DB;DP=110;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.22;SOR=9.401	GT:AD:DP:GQ:PL	1/1:0,109:109:99:3890,326,0&#xD;&#xA;&#xD;&#xA;[A dropbox link for file](https://www.dropbox.com/s/sflg3eokl8aoeqc/Test.vcf?dl=1)&#xD;&#xA;&#xD;&#xA;I'd initially considered using GATK's SelectVariants but I'm not sure JEXL has the ability to select out what I want specifically other than a blanket AD &gt; 10 which will give me both ref and alt alleles with AD &gt; 10.  Perhaps there is a bioawk solution or something more elaborate with coreutils which could successfully return sites with an alt AD count &gt; 10? " />
  <row Id="3068" PostHistoryTypeId="1" PostId="974" RevisionGUID="1409ccbe-b4ab-4ce7-b771-cf302e35aef7" CreationDate="2017-07-03T14:41:40.387" UserId="532" Text="Selecting sites from VCF which have an alt AD &gt; 10" />
  <row Id="3069" PostHistoryTypeId="3" PostId="974" RevisionGUID="1409ccbe-b4ab-4ce7-b771-cf302e35aef7" CreationDate="2017-07-03T14:41:40.387" UserId="532" Text="&lt;variant-calling&gt;&lt;vcf&gt;&lt;bioawk&gt;&lt;coreutils&gt;&lt;selectvariants&gt;" />
  <row Id="3070" PostHistoryTypeId="5" PostId="972" RevisionGUID="b6882e6a-8273-4a4f-abaf-cf121155d645" CreationDate="2017-07-03T14:45:34.553" UserId="818" Comment="deleted 33 characters in body" Text="I want to get the alignment of chain A of 1kf6 (PDB ID) from pfam database [here][1]. This protein chain has two main domains (FAD_binding_2 and 	Succ_DH_flav_C). In pfam there is a link to one of these domains and after clicking in one of the mentioned domains in the table below the page, another page comes at the top of which there is a link to architectures. For example if we click on &quot;FAD_binding_2&quot; in the table, there are 220 architectures that have this domain and after clicking on that button (at the top of page), a page opens [here][2]. In this page 7471 sequences with the following architecture: FAD_binding_2, Succ_DH_flav_C, resembles most to my chain (because it has both FAD_binding_2 and Succ_DH_flav_C domains). If we want to see all 7471 sequences with that architecture we can click on the show button under that architecture and finally we can see all of that 7471 architecture resembling our chain A sequence. Down the page we can see other architectures that all have FAD_binding_2, but some of them do not have both FAD_binding_2 and Succ_DH_flav_C domains (or some of them have other extra domains that we are not interested in) and so we do not need their sequences and their alignments. when we want to get the alignment of full sequences at the left side of the page, we can see that we can just get the alignment of all 15696 sequences and it seems that there is no way to get the alignment of our favourite 7471 sequences. I would like to know if there is any way to get the alignment or at least the fasta file of our favourite 7471 sequences (which have both of our favourite domains not just one of them)?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://pfam.xfam.org/protein/FRDA_ECOLI&#xD;&#xA;  [2]: http://pfam.xfam.org/family/Succ_DH_flav_C#tabview=tab1" />
  <row Id="3071" PostHistoryTypeId="5" PostId="974" RevisionGUID="8eddf142-8850-4ac5-951c-cc5825fa8956" CreationDate="2017-07-03T15:05:25.437" UserId="532" Comment="Formatting " Text="I have high-depth variant calling created using the HaplotypeCaller with `--output_mode EMIT_ALL_SITES` I'm interested in finding all sites (regardless of genotype call heterozygous or homozygous) where at least one of the alternative alleles have an `AD` value (Allelic Depth) greater than 10, *I*.*e*. are supported by more than 10 reads. &#xD;&#xA;&#xD;&#xA;So in the example VCF snippet below I'm wanting to select lines: 6,7,8,12,13 and 14, which have GT:AD values `1/1:1,988:989` `0/1:116,92` `0/1:220,234` `0/1:62,611` `1/1:0,109` respectively.&#xD;&#xA;&#xD;&#xA;    #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	12908_DIAG&#xD;&#xA;    3	187446740	.	T	.	Infinity	.	AN=2;DP=1095;MQ=60.00	GT:AD:DP	0/0:1095:1095&#xD;&#xA;    3	187446741	.	C	.	Infinity	.	AN=2;DP=1117;MQ=60.00	GT:AD:DP	0/0:1117:1117&#xD;&#xA;    3	187446752	.	A	.	Infinity	.	AN=2;DP=1297;MQ=60.00	GT:AD:DP	0/0:1297:1297&#xD;&#xA;    3	187446763	.	C	.	Infinity	.	AN=2;DP=1494;MQ=60.00	GT:AD:DP	0/0:1494:1494&#xD;&#xA;    3	187451574	.	C	.	Infinity	.	AN=2;DP=1493;MQ=60.00	GT:AD:DP	0/0:1493:1493&#xD;&#xA;    3	187451609	rs1880101	A	G	39794.03	.	AC=2;AF=1.00;AN=2;BaseQRankSum=1.859;ClippingRankSum=0.000;DB;DP=995;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQRankSum=0.000;QD=24.56;ReadPosRankSum=0.406;SOR=8.234	GT:AD:DP:GQ:PL	1/1:1,988:989:99:39808,2949,0&#xD;&#xA;    4	1803279	.	T	G	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-6.652;ClippingRankSum=0.000;DP=245;ExcessHet=3.0103;FS=89.753;MLEAC=0;MLEAF=0.00;MQ=59.97;MQRankSum=0.000;ReadPosRankSum=-2.523;SOR=6.357	GT:AD:DP:GQ:PL	0/0:211,23:234:99:0,364,6739&#xD;&#xA;    4	1803307	rs2305183	T	C	2486.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=-5.049;ClippingRankSum=0.000;DB;DP=215;ExcessHet=3.0103;FS=1.110;MLEAC=1;MLEAF=0.500;MQ=59.97;MQRankSum=0.000;QD=11.95;ReadPosRankSum=-0.045;SOR=0.809	GT:AD:DP:GQ:PL	0/1:116,92:208:99:2494,0,3673&#xD;&#xA;    4	1803671	.	C	A	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-0.880;ClippingRankSum=0.000;DP=450;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.953;SOR=0.572	GT:AD:DP:GQ:PL	0/0:445,2:447:99:0,1272,15958&#xD;&#xA;    4	1803681	.	T	C	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-1.654;ClippingRankSum=0.000;DP=483;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.422;SOR=0.664	GT:AD:DP:GQ:PL	0/0:479,2:481:99:0,1408,18538&#xD;&#xA;    4	1803703	.	A	G	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-1.704;ClippingRankSum=0.000;DP=458;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=0.299;SOR=0.497	GT:AD:DP:GQ:PL	0/0:454,2:456:99:0,1325,18095&#xD;&#xA;    4	1803704	rs2234909	T	C	6676.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=-2.605;ClippingRankSum=0.000;DB;DP=456;ExcessHet=3.0103;FS=1.753;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=14.71;ReadPosRankSum=0.324;SOR=0.849	GT:AD:DP:GQ:PL	0/1:220,234:454:99:6684,0,6366&#xD;&#xA;    4	1803824	rs2305184	C	G	2030.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=8.083;ClippingRankSum=0.000;DB;DP=124;ExcessHet=3.0103;FS=6.128;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=16.51;ReadPosRankSum=0.180;SOR=0.096	GT:AD:DP:GQ:PL	0/1:62,61:123:99:2038,0,1766&#xD;&#xA;    4	1805296	rs3135883	G	A	3876.03	.	AC=2;AF=1.00;AN=2;DB;DP=110;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.22;SOR=9.401	GT:AD:DP:GQ:PL	1/1:0,109:109:99:3890,326,0&#xD;&#xA;&#xD;&#xA;[A dropbox link for file](https://www.dropbox.com/s/sflg3eokl8aoeqc/Test.vcf?dl=1)&#xD;&#xA;&#xD;&#xA;I'd initially considered using GATK's SelectVariants but I'm not sure JEXL has the ability to select out what I want specifically other than a blanket AD &gt; 10 which will give me both ref and alt alleles with AD &gt; 10.  Perhaps there is a bioawk solution or something more elaborate with coreutils which could successfully return sites with an alt AD count &gt; 10? " />
  <row Id="3072" PostHistoryTypeId="4" PostId="972" RevisionGUID="87a1fbfe-5519-4fc0-bb33-210bbecfab53" CreationDate="2017-07-03T15:18:40.033" UserId="532" Comment="Improved title" Text="Obtaining all protein sequences with a particular domain architecture from Pfam" />
  <row Id="3073" PostHistoryTypeId="6" PostId="972" RevisionGUID="87a1fbfe-5519-4fc0-bb33-210bbecfab53" CreationDate="2017-07-03T15:18:40.033" UserId="532" Comment="Improved title" Text="&lt;sequence-homology&gt;&lt;pfam&gt;&lt;domains&gt;" />
  <row Id="3074" PostHistoryTypeId="24" PostId="972" RevisionGUID="87a1fbfe-5519-4fc0-bb33-210bbecfab53" CreationDate="2017-07-03T15:18:40.033" Comment="Proposed by 532 approved by 818 edit id of 229" />
  <row Id="3075" PostHistoryTypeId="5" PostId="973" RevisionGUID="8fdf7be6-887a-4c02-b494-76f14d86059c" CreationDate="2017-07-03T15:34:15.717" UserId="292" Comment="Added example query" Text="Regarding your third question (&quot;Where could I get a database of COI/COX data?&quot;), you can apparently search and download fasta sequences from [BOLD](http://www.boldsystems.org/index.php/Public_SearchTerms).&#xD;&#xA;&#xD;&#xA;However, I don't see a &quot;select all&quot; button to select all records found by one search in a single fasta file.&#xD;&#xA;&#xD;&#xA;There seems to exist an R API: &lt;https://cran.r-project.org/web/packages/bold/bold.pdf&gt; which refers to this: http://www.boldsystems.org/index.php/resources/api#sequenceParameters&gt;.&#xD;&#xA;&#xD;&#xA;From this I seem to understand that one can directly query using urls:&#xD;&#xA;&#xD;&#xA;    wget -O Gastropoda.fa http://www.boldsystems.org/index.php/API_Public/sequence?taxon=Gastropoda&amp;marker=COI&#xD;&#xA;&#xD;&#xA;And I get a fasta file containing COI sequences for Gastropoda. There may be ways to refine the query geographically, using the `geo` parameter." />
  <row Id="3076" PostHistoryTypeId="2" PostId="975" RevisionGUID="a0995c72-1f2c-429b-914f-e6717a43e1ab" CreationDate="2017-07-03T15:41:14.367" UserId="71" Text="using [vcfilterjs][1]&#xD;&#xA;&#xD;&#xA;and the following script:&#xD;&#xA;&#xD;&#xA;    function accept(vc)&#xD;&#xA;    	{&#xD;&#xA;    	var i,j;&#xD;&#xA;    	for(i=0;i&lt; vc.getNSamples();++i)&#xD;&#xA;    		{&#xD;&#xA;    		var genotype = vc.getGenotype(i);&#xD;&#xA;    		if(!genotype.hasAD()) continue;&#xD;&#xA;    		var ad = genotype.getAD();&#xD;&#xA;    		for(j=0;j&lt; ad.length;++j)&#xD;&#xA;    			{&#xD;&#xA;    			if(ad[j]&gt;10) return true;&#xD;&#xA;    			}&#xD;&#xA;    		}&#xD;&#xA;    	return false;&#xD;&#xA;    	}&#xD;&#xA;    accept(variant);&#xD;&#xA;&#xD;&#xA;usage:&#xD;&#xA;&#xD;&#xA;    java -jar dist/vcffilterjs.jar -f script.js Test.vcf &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://lindenb.github.io/jvarkit/VCFFilterJS.html" />
  <row Id="3077" PostHistoryTypeId="5" PostId="972" RevisionGUID="498f8dae-16ec-43ac-9fdc-38de6c9b8ccb" CreationDate="2017-07-03T16:20:39.590" UserId="298" Comment="Paragraphs are your friends. They make reading your question much easier. " Text="I want to get the alignment of chain A of 1kf6 (PDB ID) from the pfam database [here][1]. This protein chain has two main domains (FAD_binding_2 and	Succ_DH_flav_C). In pfam there is a link to one of these domains and after clicking in one of the mentioned domains in the table below the page, another page comes at the top of which there is a link to architectures. &#xD;&#xA;&#xD;&#xA;For example, if we click on &quot;FAD_binding_2&quot; in the table, there are 220 architectures that have this domain and after clicking on that button (at the top of page), a page opens [here][2]. In this page 7471 sequences with the following architecture: FAD_binding_2, Succ_DH_flav_C, resembles most to my chain (because it has both FAD_binding_2 and Succ_DH_flav_C domains). &#xD;&#xA;&#xD;&#xA;If we want to see all 7471 sequences with that architecture we can click on the show button under that architecture and finally we can see all of that 7471 architecture resembling our chain A sequence. Down the page we can see other architectures that all have FAD_binding_2, but some of them do not have both FAD_binding_2 and Succ_DH_flav_C domains (or some of them have other extra domains that we are not interested in) and so we do not need their sequences and their alignments. &#xD;&#xA;&#xD;&#xA;When we want to get the alignment of full sequences at the left side of the page, we can see that we can just get the alignment of all 15696 sequences and it seems that there is no way to get the alignment of our favourite 7471 sequences. I would like to know if there is any way to get the alignment or at least the fasta file of our favourite 7471 sequences (which have both of our favourite domains not just one of them)?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://pfam.xfam.org/protein/FRDA_ECOLI&#xD;&#xA;  [2]: http://pfam.xfam.org/family/Succ_DH_flav_C#tabview=tab1" />
  <row Id="3078" PostHistoryTypeId="5" PostId="877" RevisionGUID="25fd2c2e-1d3d-4905-a9b0-1d270b11c819" CreationDate="2017-07-03T17:03:11.717" UserId="734" Comment="added 1 character in body; added 13 characters in body" Text="There are 2 new wet methods: [Patient-derived models][1]: [Patient Derived Xenograft (PDX)][2] and [Patient Derived Organoids (PDO)][3] to reflect tumor biology. &#xD;&#xA;&#xD;&#xA;Are there any databases/resources/computational tools/previous work that use the outcomes of PDO/PDX experiments to create a predictive computational model for cancer or other diseases?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&#xD;&#xA;  [3]: http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html" />
  <row Id="3082" PostHistoryTypeId="5" PostId="968" RevisionGUID="1bac2fc2-c78c-49f5-a1d3-76d372be42f5" CreationDate="2017-07-03T20:50:27.253" UserId="532" Comment="-I should be -i" Text="**Running the jobs in parallel**&#xD;&#xA;&#xD;&#xA;Iteratively is one solution, but since you're not using bootstrapping, which can be run multithread (and is not needed if you're taking the popular [tximport](http://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html) route) you can do better using [GNU parallel](https://www.gnu.org/software/parallel/), this will enable you to run as many jobs as you have execution threads on your system simultaneously.  Note that Kalliso only runs multithreaded with `-t` if you're using bootstrapping.&#xD;&#xA;&#xD;&#xA;0. You'll need to make the index first:&#xD;&#xA;&#xD;&#xA;        kallisto index --make-unique -I index.fa.idx index.fa&#xD;&#xA;&#xD;&#xA;1. Next lets define the input FASTQ files and the sample IDs you want to use using a tab-delimited input file like so:&#xD;&#xA;&#xD;&#xA;        sample_1	sample_1_R1.fastq.gz&#xD;&#xA;        sample_2	sample_2_R1.fastq.gz&#xD;&#xA;        sample_3	sample_3_R1.fastq.gz&#xD;&#xA;&#xD;&#xA;    Here the first column is the sample ID which will be used for output and the second separated by a tab character is the input file. &#xD;&#xA;    &#xD;&#xA;    You can produce this using a text editor but as a short-cut you can feed the list of fastq.gz you have into a list via:&#xD;&#xA;&#xD;&#xA;        ls -1 *.fastq.gz &gt; editme&#xD;&#xA;&#xD;&#xA;2. Next we create a list of jobs to run on our system using a utility script `Make_job_list_kallisto.sh` which consumes our tab delimited input file list above.  I've used your `-l` and `-s` parameters below, note that with paired end data normally these are estimated for you.&#xD;&#xA;&#xD;&#xA;        #!/bin/bash -eu&#xD;&#xA;        &#xD;&#xA;        [ $# -ne 2 ] &amp;&amp; { echo -en &quot;\n*** This script generates jobs for GNU parallel. *** \n\n Error Nothing to do, usage: &lt; input tab delimited list &gt; &lt; output run list file &gt;\n\n&quot; ; exit 1; }&#xD;&#xA;        set -o pipefail&#xD;&#xA;        &#xD;&#xA;        # Get command-line args&#xD;&#xA;        INPUT_LIST=$1&#xD;&#xA;        OUTPUT=$2&#xD;&#xA;        &#xD;&#xA;        # Set counter&#xD;&#xA;        COUNT=1&#xD;&#xA;        END=$(wc -l $INPUT_LIST | awk '{print $1}')&#xD;&#xA;        &#xD;&#xA;        echo &quot; &quot;&#xD;&#xA;        echo &quot; * Input file is: $INPUT_LIST&quot;&#xD;&#xA;        echo &quot; * Number of runs: $END&quot;&#xD;&#xA;        echo &quot; * Output job list for GNU parallel saved to: $OUTPUT&quot;&#xD;&#xA;        echo &quot; &quot;&#xD;&#xA;        &#xD;&#xA;        # Main bit of command-line for job&#xD;&#xA;        CMD=&quot;kallisto quant -i index.fa.idx --single -l 200 -s 0.1&quot;&#xD;&#xA;        &#xD;&#xA;        # Main Loop&#xD;&#xA;        [ -e $OUTPUT ] &amp;&amp; rm $OUTPUT&#xD;&#xA;        while [ $COUNT -le $END ];&#xD;&#xA;        do&#xD;&#xA;            LINE=( $(awk &quot;NR==$COUNT&quot; $INPUT_LIST) )&#xD;&#xA;            # Make file list&#xD;&#xA;            echo &quot;Working on $COUNT of $END Sample ID: ${LINE[0]}, Files ${LINE[@]:1}&quot;&#xD;&#xA;            echo &quot;$CMD -o ${LINE[0]} ${LINE[@]:1}&quot; &gt;&gt; $OUTPUT&#xD;&#xA;            ((COUNT++))&#xD;&#xA;        done&#xD;&#xA;&#xD;&#xA;    We use the script like so:&#xD;&#xA;    &#xD;&#xA;         Make_job_list_kallisto.sh input_file_list.txt job_list.txt&#xD;&#xA;&#xD;&#xA;3. Now we have a job list for GNU parallel we can run our jobs in parallel like so:&#xD;&#xA;&#xD;&#xA;        parallel --progress --jobs 4 --joblog kallisto_joblog.txt &lt; job_list.txt&#xD;&#xA;&#xD;&#xA;    This will run 4 of the kallisto jobs simultaneously (Increase this if you have more threads - don't mind consuming all the IO/CPU threads on your system) producing a nice progress report of those completed, additionally a job log will be written to the file `kallisto_joblog.txt`.  &#xD;&#xA;&#xD;&#xA;For paired end data I already have these scripts + similar instructions on [GitHub](https://github.com/MattBashton/grolar) which people might find useful. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3084" PostHistoryTypeId="2" PostId="977" RevisionGUID="5ffc29d1-4356-4376-9704-5a57f173835c" CreationDate="2017-07-03T21:16:19.757" UserId="1044" Text="If your goal is to bin the resulting contigs into genomes, then you should do option #1, pooling the reads and assembling into one set of contigs. " />
  <row Id="3088" PostHistoryTypeId="5" PostId="895" RevisionGUID="d1820e67-cf45-432a-a1c3-5d846bd7cd09" CreationDate="2017-07-03T21:24:03.917" UserId="96" Comment="added tag, capitalization" Text="I want to get a .bed file with the genes' names and canonical coordinates, also I would like to have coordinates of exons, too. I can get the list from UCSC, however, if I choose UCSC Genes - knownCanonical, I can not extract coordinates of exons. If I use other options - I am getting coordinates of as many transcriptional isoforms as were detected while I need only one canonical form.&#xD;&#xA;&#xD;&#xA;How can I get such BED file?" />
  <row Id="3089" PostHistoryTypeId="6" PostId="895" RevisionGUID="d1820e67-cf45-432a-a1c3-5d846bd7cd09" CreationDate="2017-07-03T21:24:03.917" UserId="96" Comment="added tag, capitalization" Text="&lt;bed&gt;&lt;public-databases&gt;" />
  <row Id="3090" PostHistoryTypeId="24" PostId="895" RevisionGUID="d1820e67-cf45-432a-a1c3-5d846bd7cd09" CreationDate="2017-07-03T21:24:03.917" Comment="Proposed by 96 approved by 77 edit id of 230" />
  <row Id="3095" PostHistoryTypeId="5" PostId="959" RevisionGUID="36dd8728-a106-4d3b-ad0b-efb9ed581a59" CreationDate="2017-07-04T07:01:36.560" UserId="292" Comment="Copied comment from comments, added more code comments" Text="A few years ago, I wrote a python script to convert FCS files into tab-separated format. It was far from handling all the possibilities that the format description offers, but at least it worked for some of the files produced on one of our machine: &lt;http://www.igh.cnrs.fr/equip/Seitz/en_equipe-programmes.html&gt;&#xD;&#xA;&#xD;&#xA;The format documentation I found enabled decoding (see section 3 of the pdf you mention), but it requires reading data in binary mode.&#xD;&#xA;&#xD;&#xA;The general idea of this format (and, I guess, many other binary formats), is that there is a header zone at the beginning of the file with a defined number of fields encoding numbers indicating how the rest of the file is structured. So a first phase is to parse this header, following the description given in the documentation of the format. The information extracted from the header tells where to find the data and how it is encoded, still according to rules described in the format documentation.&#xD;&#xA;&#xD;&#xA;In case this may be useful, and for the record, here is the code from the above-mentioned script (after stripping comments, some of which are merely copied from the format documentation, and adding a few ones):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    &quot;&quot;&quot;This script tries to read FCS flow cytometry data.&#xD;&#xA;    Format parsing inspired by information found here:&#xD;&#xA;    http://isac-net.org/Resources-for-Cytometrists/Data-Standards/Data-File-Standards/Flow-Cytometry-Data-File-Format-Standards.aspx&#xD;&#xA;    &quot;&quot;&quot;&#xD;&#xA;    &#xD;&#xA;    import re&#xD;&#xA;    # To decode binary-encoded data&#xD;&#xA;    import struct&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    class Parameter(object):&#xD;&#xA;        &quot;&quot;&quot;This object represents one of the parameter types that are present in a DATA segment of a FCS file.&quot;&quot;&quot;&#xD;&#xA;        __slots__ = (&quot;p_name&quot;, &quot;p_bits&quot;, &quot;p_range&quot;, &quot;p_ampl&quot;, &quot;parser&quot;)&#xD;&#xA;        def __init__(self, p_name, p_bits, p_range, p_ampl):&#xD;&#xA;            self.p_name = p_name&#xD;&#xA;            self.p_bits = p_bits&#xD;&#xA;            self.p_range = p_range&#xD;&#xA;            self.p_ampl = p_ampl&#xD;&#xA;            # Function for parsing a value of the parameter in the data segment&#xD;&#xA;            self.parser = None&#xD;&#xA;    &#xD;&#xA;    ##############################################&#xD;&#xA;    # Here starts the parsing of the header part #&#xD;&#xA;    # which tells where the other parts are.     #&#xD;&#xA;    ##############################################&#xD;&#xA;    &#xD;&#xA;    f = open(sys.argv[1], &quot;rb&quot;)&#xD;&#xA;    # The format name is encoded in 6 letters&#xD;&#xA;    # An ASCII letter is coded with one octet&#xD;&#xA;    file_format = &quot;&quot;.join([f.read(1) for __ in range(6)])&#xD;&#xA;    sys.stdout.write(&quot;Format: %s\n&quot; % file_format)&#xD;&#xA;    # The format descriptions reserves 4 octets that we skip&#xD;&#xA;    skip = f.read(4)&#xD;&#xA;    # 8 octet chunks encode the start and end positions&#xD;&#xA;    # of different parts of the data&#xD;&#xA;    text_start = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    text_end = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    data_start = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    data_end = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    analysis_start = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    analysis_end = int(f.read(8).strip(&quot; &quot;))&#xD;&#xA;    &#xD;&#xA;    if (analysis_start and analysis_end):&#xD;&#xA;        sys.stderr.write(&quot;Cannot deal with ANALYSIS segment of an FCS file.\n&quot;)&#xD;&#xA;    &#xD;&#xA;    ####################################################&#xD;&#xA;    # Here starts the parsing of the &quot;TEXT&quot; portion    #&#xD;&#xA;    # which describes how the data proper is organized #&#xD;&#xA;    ####################################################&#xD;&#xA;    f.seek(text_start)&#xD;&#xA;    # The first character in the primary TEXT segment is the ASCII delimiter character.&#xD;&#xA;    sep = f.read(1)&#xD;&#xA;    if sep not in [&quot;_&quot;, &quot;@&quot;]:&#xD;&#xA;        alt_sep = &quot;_@_&quot;&#xD;&#xA;    elif sep not in [&quot;_&quot;, &quot;|&quot;]:&#xD;&#xA;        alt_sep = &quot;_|_&quot;&#xD;&#xA;    else:&#xD;&#xA;        assert sep not in [&quot;+&quot;, &quot;|&quot;]&#xD;&#xA;        alt_sep = &quot;+|+&quot;&#xD;&#xA;    text_segment = f.read(text_end - text_start)&#xD;&#xA;    &#xD;&#xA;    fields = text_segment.split(sep)&#xD;&#xA;    &#xD;&#xA;    info = {}&#xD;&#xA;    &#xD;&#xA;    i = 0&#xD;&#xA;    while i &lt; len(fields) - 1:&#xD;&#xA;        key = fields[i]&#xD;&#xA;        i += 1&#xD;&#xA;        val = fields[i]&#xD;&#xA;        i += 1&#xD;&#xA;        # Keywords are case insensitive, they may be written in a file in lower case, upper case, or a&#xD;&#xA;        # mixture of the two. However, an FCS file reader must ignore keyword case. A keyword value may&#xD;&#xA;        # be in lower case, upper case or a mixture of the two. Keyword values are case sensitive.&#xD;&#xA;        info[key.upper()] = val&#xD;&#xA;    print &quot;%s events were detected.&quot; % info[&quot;$TOT&quot;]&#xD;&#xA;    print &quot;Each event is characterized by %s parameters&quot; % info[&quot;$PAR&quot;]&#xD;&#xA;    &#xD;&#xA;    if info[&quot;$NEXTDATA&quot;] != &quot;0&quot;:&#xD;&#xA;        sys.stderr.write(&quot;Some other data exist in the file but hasn't been parsed.\n&quot;)&#xD;&#xA;    &#xD;&#xA;    # L - List mode. For each event, the value of each parameter is stored in the order in which the&#xD;&#xA;    # parameters are described. The number of bits reserved for parameter 1 is described using the&#xD;&#xA;    # $P1B keyword. There can be only one set of list mode data per data set. The $DATATYPE&#xD;&#xA;    # keyword describes the data format. This is the most versatile mode for the storage of flow&#xD;&#xA;    # cytometry data because mode C and mode U data can be created from mode L data.&#xD;&#xA;    assert info[&quot;$MODE&quot;] == &quot;L&quot;&#xD;&#xA;    &#xD;&#xA;    parameters = []&#xD;&#xA;    &#xD;&#xA;    # indices of the parameters&#xD;&#xA;    p_indices = range(1, int(info[&quot;$PAR&quot;]) + 1)&#xD;&#xA;    for i in p_indices:&#xD;&#xA;        p_name = info[&quot;$P%dN&quot; % i]&#xD;&#xA;        p_bits =  info[&quot;$P%dB&quot; % i]&#xD;&#xA;        p_range = info[&quot;$P%dR&quot; % i]&#xD;&#xA;        p_ampl =  info[&quot;$P%dE&quot; % i]&#xD;&#xA;        parameters.append(Parameter(p_name, p_bits, p_range, p_ampl))&#xD;&#xA;    &#xD;&#xA;    sys.stdout.write(&quot;The parameters are:\n%s\n&quot; % &quot;\t&quot;.join([par.p_name for par in parameters]))&#xD;&#xA;    &#xD;&#xA;    # How are 32 bit words organized&#xD;&#xA;    if info[&quot;$BYTEORD&quot;] == &quot;4,3,2,1&quot;:&#xD;&#xA;        endianness = &quot;&gt;&quot;&#xD;&#xA;    else:&#xD;&#xA;        endianness = &quot;&lt;&quot;&#xD;&#xA;        assert info[&quot;$BYTEORD&quot;] == &quot;1,2,3,4&quot;&#xD;&#xA;    &#xD;&#xA;        # I stripped a long comment which is just a copy of the documentation&#xD;&#xA;    # Type of data:&#xD;&#xA;    if info[&quot;$DATATYPE&quot;] == &quot;I&quot;:&#xD;&#xA;        for par in parameters:&#xD;&#xA;            nb_bits = int(par.p_bits)&#xD;&#xA;            assert nb_bits % 8 == 0&#xD;&#xA;            nb_bytes = nb_bits / 8&#xD;&#xA;            # Determine format string for unpacking (see https://docs.python.org/2/library/struct.html)&#xD;&#xA;            if nb_bytes == 1:&#xD;&#xA;                c_type = &quot;B&quot; # unsigned char&#xD;&#xA;            elif nb_bytes == 2:&#xD;&#xA;                c_type = &quot;H&quot; # unsigned short&#xD;&#xA;            elif nb_bytes == 4:&#xD;&#xA;                c_type = &quot;L&quot; # unsigned long&#xD;&#xA;            elif nb_bytes == 8:&#xD;&#xA;                c_type = &quot;Q&quot; # unsigned long long&#xD;&#xA;            else:&#xD;&#xA;                raise ValueError, &quot;Number of bytes (%d) not valid for an integer (see https://docs.python.org/2/library/struct.html#byte-order-size-and-alignment).&quot; % nb_bytes&#xD;&#xA;            fmt = &quot;%s%s&quot; % (endianness, c_type)&#xD;&#xA;            p_range = int(par.p_range)&#xD;&#xA;            def parser(data):&#xD;&#xA;                value = struct.unpack(fmt, data.read(nb_bytes))[0]&#xD;&#xA;                try:&#xD;&#xA;                    assert value &lt; p_range&#xD;&#xA;                except AssertionError:&#xD;&#xA;                    print &quot;Value %s higher than %d&quot; % (str(value), p_range)&#xD;&#xA;                return value&#xD;&#xA;            par.parser = parser&#xD;&#xA;        pass&#xD;&#xA;    else:&#xD;&#xA;        raise NotImplementedError, &quot;Only the parsing of integer value has been implemented so far.&quot;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    out_file = open(sys.argv[2], &quot;w&quot;)&#xD;&#xA;    out_file.write(&quot;#amplification_types\t&quot; + &quot;\t&quot;.join([par.p_ampl for par in parameters]) + &quot;\n&quot;)&#xD;&#xA;    out_file.write(&quot;parameters\t&quot; + &quot;\t&quot;.join([par.p_name for par in parameters]) + &quot;\n&quot;)&#xD;&#xA;    i = 1&#xD;&#xA;    ##############################################&#xD;&#xA;    # Here starts the parsing of the data proper #&#xD;&#xA;    ##############################################&#xD;&#xA;    f.seek(data_start)&#xD;&#xA;    while f.tell() &lt; data_end:&#xD;&#xA;        values = []&#xD;&#xA;        for par in parameters:&#xD;&#xA;            values.append(par.parser(f))&#xD;&#xA;        out_file.write(&quot;%d\t&quot; % i + &quot;\t&quot;.join(map(str, values)) + &quot;\n&quot;)&#xD;&#xA;        i += 1&#xD;&#xA;    out_file.close()&#xD;&#xA;    f.close()&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3096" PostHistoryTypeId="2" PostId="979" RevisionGUID="ae17328c-06b2-4c1d-b958-96c8f8749c06" CreationDate="2017-07-04T11:53:41.607" UserId="982" Text="I have a reference database with contains 100s of sequences in fasta format. Some of these sequences have duplicate names like so:&#xD;&#xA;&#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc&#xD;&#xA;&#xD;&#xA;Is is possible to run through a large file like this and change the names of only the duplicates?&#xD;&#xA;&#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName_1&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc" />
  <row Id="3097" PostHistoryTypeId="1" PostId="979" RevisionGUID="ae17328c-06b2-4c1d-b958-96c8f8749c06" CreationDate="2017-07-04T11:53:41.607" UserId="982" Text="How do you deal with duplicate sequence names? [append numbers only on duplicates]" />
  <row Id="3098" PostHistoryTypeId="3" PostId="979" RevisionGUID="ae17328c-06b2-4c1d-b958-96c8f8749c06" CreationDate="2017-07-04T11:53:41.607" UserId="982" Text="&lt;fasta&gt;&lt;text&gt;" />
  <row Id="3099" PostHistoryTypeId="2" PostId="980" RevisionGUID="e0db3cfa-d3ef-40af-a974-88f0d77ab423" CreationDate="2017-07-04T11:58:43.597" UserId="298" Text="Sure, this little Perl snippet should do it:&#xD;&#xA;&#xD;&#xA;    $ perl -pe 's/$/_$seen{$_}/ if ++$seen{$_}&gt;1; ' file.fa &#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc_2&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc_3&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc_4&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Note that the first occurence of a duplicate name isn't changed, the second will become `_2`, the third `_3` etc. " />
  <row Id="3100" PostHistoryTypeId="2" PostId="981" RevisionGUID="ae91415a-f70f-4ee4-a0ce-df8356c8a605" CreationDate="2017-07-04T12:02:17.610" UserId="77" Text="Sure, using biopython:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;&#xD;&#xA;    records = set()&#xD;&#xA;    of = open(&quot;output.fa&quot;, &quot;w&quot;)&#xD;&#xA;    for record in SeqIO.parse(&quot;foo.fa&quot;, &quot;fasta&quot;):&#xD;&#xA;        ID = record.id&#xD;&#xA;        num = 1&#xD;&#xA;        while ID in records:&#xD;&#xA;            if &quot;{}_{}&quot;.format(ID, num) not in records:&#xD;&#xA;                ID = &quot;{}_{}&quot;.format(ID, num)&#xD;&#xA;                break&#xD;&#xA;            num += 1&#xD;&#xA;        records.add(ID)&#xD;&#xA;        record.id = ID&#xD;&#xA;        record.name = ID&#xD;&#xA;        record.description = ID&#xD;&#xA;        SeqIO.write(record, of, &quot;fasta&quot;)&#xD;&#xA;    of.close()&#xD;&#xA;&#xD;&#xA;Change `output.fa` and `foo.fa`. One doesn't need to explicitly change the `.name` and `.description`, but that's handy to prevent the original ID from not appearing still (after a space)." />
  <row Id="3101" PostHistoryTypeId="5" PostId="980" RevisionGUID="baff30b2-873c-4283-914f-79392e38145c" CreationDate="2017-07-04T12:02:27.400" UserId="298" Comment="added 533 characters in body" Text="Sure, this little Perl snippet should do it:&#xD;&#xA;&#xD;&#xA;    $ perl -pe 's/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/; ' file.fa &#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc_2&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc_3&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc_4&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Note that the first occurence of a duplicate name isn't changed, the second will become `_2`, the third `_3` etc. &#xD;&#xA;&#xD;&#xA;### Explanation&#xD;&#xA;* `perl -pe` : **p**rint each input line after applying the script given by `-e` to it. &#xD;&#xA;* `++$seen{$_}&gt;1` : increment the current value stored in the hash `%seen` for this line (`$_`) by 1 and compare it to `1`. &#xD;&#xA;* `s/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/` : if the current line starts with a `&gt;` and the  value stored in the hash `%seen` for this line is greater than 1 (if this isn't the first time we see this line), replace the end of the line (`$`) with a `_` and the current value in the hash" />
  <row Id="3102" PostHistoryTypeId="5" PostId="980" RevisionGUID="d8c114bb-e384-49f5-9766-1eefcaa52e11" CreationDate="2017-07-04T12:08:57.697" UserId="298" Comment="added 533 characters in body" Text="Sure, this little Perl snippet should do it:&#xD;&#xA;&#xD;&#xA;    $ perl -pe 's/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/; ' file.fa &#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc_2&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc_3&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc_4&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Note that the first occurence of a duplicate name isn't changed, the second will become `_2`, the third `_3` etc. &#xD;&#xA;&#xD;&#xA;### Explanation&#xD;&#xA;* `perl -pe` : **p**rint each input line after applying the script given by `-e` to it. &#xD;&#xA;* `++$seen{$_}&gt;1` : increment the current value stored in the hash `%seen` for this line (`$_`) by 1 and compare it to `1`. &#xD;&#xA;* `s/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/` : if the current line starts with a `&gt;` and the  value stored in the hash `%seen` for this line is greater than 1 (if this isn't the first time we see this line), replace the end of the line (`$`) with a `_` and the current value in the hash&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;Alternatively, here's the same idea in `awk`:&#xD;&#xA;&#xD;&#xA;    $ awk '(/^&gt;/ &amp;&amp; s[$0]++){$0=$0&quot;_&quot;s[$0]}1;' file.fa &#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc&#xD;&#xA;    &#xD;&#xA;###Explanation&#xD;&#xA;&#xD;&#xA;In awk, the special variable `$0` is the current line. &#xD;&#xA;&#xD;&#xA;* `(/^&gt;/ &amp;&amp; s[$0]++)` : if this line starts with a `&gt;` and incrementing the value stored in the array `s` for this line by 1 evaluates to true (is greater than 0).&#xD;&#xA;* `$0=$0&quot;_&quot;s[$0]` :  make the current line be itself with a `_` and the value from `s` appended.&#xD;&#xA;* `1;` : this is just shorthand for &quot;print this line&quot;. If an expression evaluates to true, awk will print the current line. Since `1` is always true, this will print every line. " />
  <row Id="3103" PostHistoryTypeId="2" PostId="982" RevisionGUID="0d0f3c44-7b3d-48f5-a431-dab7a187d71c" CreationDate="2017-07-04T12:53:14.190" UserId="704" Text="I was wondering which is the best and most accurate way to calculate computationally the charge of a protein peptide like this one &quot;RKTTLVPNTQTASPR&quot; in the R or different environment." />
  <row Id="3104" PostHistoryTypeId="1" PostId="982" RevisionGUID="0d0f3c44-7b3d-48f5-a431-dab7a187d71c" CreationDate="2017-07-04T12:53:14.190" UserId="704" Text="Calculating the charge of a peptide computationally" />
  <row Id="3105" PostHistoryTypeId="3" PostId="982" RevisionGUID="0d0f3c44-7b3d-48f5-a431-dab7a187d71c" CreationDate="2017-07-04T12:53:14.190" UserId="704" Text="&lt;r&gt;&lt;proteins&gt;" />
  <row Id="3106" PostHistoryTypeId="2" PostId="983" RevisionGUID="1b8d4696-15dc-4378-bf1c-45015a38e9e3" CreationDate="2017-07-04T13:05:35.823" UserId="77" Text="A quick google search turns up [protcalc](http://protcalc.sourceforge.net/), which is able to give a nice pH-dependent table of peptide charges (yours ranges from 3.1 at pH 4 to 1.5 at pH 10). It's on sourceforge, so hopefully the source code (and maybe a publication) is somewhere in there. Granted, this isn't R, but it's not clear from your post how necessary that really is.&#xD;&#xA;&#xD;&#xA;BTW, in general &quot;best and most accurate&quot; turns out to be highly subjective (at least the &quot;best&quot; part of that)." />
  <row Id="3107" PostHistoryTypeId="2" PostId="984" RevisionGUID="5e540122-3174-44e0-bf12-95a81e88581d" CreationDate="2017-07-04T13:10:58.163" UserId="29" Text="And here’s a solution using R (with the Bioconductor):&#xD;&#xA;&#xD;&#xA;    fa = ShortRead::readFasta(infile)&#xD;&#xA;    ids = as.character(ShortRead::id(fa))&#xD;&#xA;    fa@id = Biostrings::BStringSet(make.unique(ids, sep = '_'))&#xD;&#xA;    ShortRead::writeFasta(fa, outfile)" />
  <row Id="3108" PostHistoryTypeId="5" PostId="980" RevisionGUID="5533a8c1-58db-4d6c-b20b-add5a56adcc7" CreationDate="2017-07-04T13:12:32.980" UserId="298" Comment="added 869 characters in body" Text="Sure, this little Perl snippet should do it:&#xD;&#xA;&#xD;&#xA;    $ perl -pe 's/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/; ' file.fa &#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc_2&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc_3&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc_4&#xD;&#xA;Or, to make the changes in the original file, use `-i`:    &#xD;&#xA;&#xD;&#xA;    perl -i.bak -pe 's/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/; ' file.fa &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Note that the first occurrence of a duplicate name isn't changed, the second will become `_2`, the third `_3` etc. &#xD;&#xA;&#xD;&#xA;### Explanation&#xD;&#xA;* `perl -pe` : **p**rint each input line after applying the script given by `-e` to it. &#xD;&#xA;* `++$seen{$_}&gt;1` : increment the current value stored in the hash `%seen` for this line (`$_`) by 1 and compare it to `1`. &#xD;&#xA;* `s/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/` : if the current line starts with a `&gt;` and the  value stored in the hash `%seen` for this line is greater than 1 (if this isn't the first time we see this line), replace the end of the line (`$`) with a `_` and the current value in the hash&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;Alternatively, here's the same idea in `awk`:&#xD;&#xA;&#xD;&#xA;    $ awk '(/^&gt;/ &amp;&amp; s[$0]++){$0=$0&quot;_&quot;s[$0]}1;' file.fa &#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc&#xD;&#xA;&#xD;&#xA;To make the changes in the original file (assuming you are using GNU `awk` which is the default on most Linux versions), use `-i inplace`:&#xD;&#xA;&#xD;&#xA;    awk -iinplace '(/^&gt;/ &amp;&amp; s[$0]++){$0=$0&quot;_&quot;s[$0]}1;' file.fa &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;###Explanation&#xD;&#xA;&#xD;&#xA;In awk, the special variable `$0` is the current line. &#xD;&#xA;&#xD;&#xA;* `(/^&gt;/ &amp;&amp; s[$0]++)` : if this line starts with a `&gt;` and incrementing the value stored in the array `s` for this line by 1 evaluates to true (is greater than 0).&#xD;&#xA;* `$0=$0&quot;_&quot;s[$0]` :  make the current line be itself with a `_` and the value from `s` appended.&#xD;&#xA;* `1;` : this is just shorthand for &quot;print this line&quot;. If an expression evaluates to true, awk will print the current line. Since `1` is always true, this will print every line. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;------&#xD;&#xA;&#xD;&#xA;If you want all of the duplicates to be marked, you need to read the file twice. Once to collect the names and a second to mark them:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    $ awk '{&#xD;&#xA;    	if (NR==FNR){&#xD;&#xA;    		if(/^&gt;/){&#xD;&#xA;    			s[$0]++&#xD;&#xA;    		}&#xD;&#xA;    		next;&#xD;&#xA;    	}&#xD;&#xA;    	if(/^&gt;/){&#xD;&#xA;    		k[$0]++;&#xD;&#xA;    		if(s[$0]&gt;1){&#xD;&#xA;    			$0=$0&quot;_&quot;k[$0]&#xD;&#xA;    		}&#xD;&#xA;    	}&#xD;&#xA;    	print&#xD;&#xA;    }' file.fa file.fa&#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName_1&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc&#xD;&#xA;    &#xD;&#xA;" />
  <row Id="3109" PostHistoryTypeId="5" PostId="779" RevisionGUID="cb6371c5-50e8-44ca-8606-8e4666546aaf" CreationDate="2017-07-04T13:52:23.310" UserId="208" Comment="typo" Text="`cellranger aggr` can combine multiple libraries (samples), and appends each barcode with an integer (e.g. AGACCATTGAGACTTA-1). The sample identity is not recorded in the combined `matrix.mtx` file.&#xD;&#xA;&#xD;&#xA;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate#gem_groups&#xD;&#xA;&#xD;&#xA;How can we keep and assign sample information to each cell after loading the data into `R`? (e.g. `Seurat::Read10X()`)&#xD;&#xA;" />
  <row Id="3110" PostHistoryTypeId="5" PostId="980" RevisionGUID="1c4f6d7e-fbab-403d-8100-b002728111d4" CreationDate="2017-07-04T13:59:59.517" UserId="298" Comment="added 588 characters in body" Text="Sure, this little Perl snippet should do it:&#xD;&#xA;&#xD;&#xA;    $ perl -pe 's/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/; ' file.fa &#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc_2&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc_3&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc_4&#xD;&#xA;Or, to make the changes in the original file, use `-i`:    &#xD;&#xA;&#xD;&#xA;    perl -i.bak -pe 's/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/; ' file.fa &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Note that the first occurrence of a duplicate name isn't changed, the second will become `_2`, the third `_3` etc. &#xD;&#xA;&#xD;&#xA;### Explanation&#xD;&#xA;* `perl -pe` : **p**rint each input line after applying the script given by `-e` to it. &#xD;&#xA;* `++$seen{$_}&gt;1` : increment the current value stored in the hash `%seen` for this line (`$_`) by 1 and compare it to `1`. &#xD;&#xA;* `s/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/` : if the current line starts with a `&gt;` and the  value stored in the hash `%seen` for this line is greater than 1 (if this isn't the first time we see this line), replace the end of the line (`$`) with a `_` and the current value in the hash&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;Alternatively, here's the same idea in `awk`:&#xD;&#xA;&#xD;&#xA;    $ awk '(/^&gt;/ &amp;&amp; s[$0]++){$0=$0&quot;_&quot;s[$0]}1;' file.fa &#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc&#xD;&#xA;&#xD;&#xA;To make the changes in the original file (assuming you are using GNU `awk` which is the default on most Linux versions), use `-i inplace`:&#xD;&#xA;&#xD;&#xA;    awk -iinplace '(/^&gt;/ &amp;&amp; s[$0]++){$0=$0&quot;_&quot;s[$0]}1;' file.fa &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;###Explanation&#xD;&#xA;&#xD;&#xA;In awk, the special variable `$0` is the current line. &#xD;&#xA;&#xD;&#xA;* `(/^&gt;/ &amp;&amp; s[$0]++)` : if this line starts with a `&gt;` and incrementing the value stored in the array `s` for this line by 1 evaluates to true (is greater than 0).&#xD;&#xA;* `$0=$0&quot;_&quot;s[$0]` :  make the current line be itself with a `_` and the value from `s` appended.&#xD;&#xA;* `1;` : this is just shorthand for &quot;print this line&quot;. If an expression evaluates to true, awk will print the current line. Since `1` is always true, this will print every line. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;------&#xD;&#xA;&#xD;&#xA;If you want all of the duplicates to be marked, you need to read the file twice. Once to collect the names and a second to mark them:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    $ awk '{&#xD;&#xA;    	if (NR==FNR){&#xD;&#xA;    		if(/^&gt;/){&#xD;&#xA;    			s[$0]++&#xD;&#xA;    		}&#xD;&#xA;    		next;&#xD;&#xA;    	}&#xD;&#xA;    	if(/^&gt;/){&#xD;&#xA;    		k[$0]++;&#xD;&#xA;    		if(s[$0]&gt;1){&#xD;&#xA;    			$0=$0&quot;_&quot;k[$0]&#xD;&#xA;    		}&#xD;&#xA;    	}&#xD;&#xA;    	print&#xD;&#xA;    }' file.fa file.fa&#xD;&#xA;    &gt;1_uniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_anotherUniqueGeneName&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName_1&#xD;&#xA;    atgc&#xD;&#xA;    &gt;1_duplicateName_2&#xD;&#xA;    atgc&#xD;&#xA;    &#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;**IMPORTANT**: note that all of these approaches assume you don't already have sequence names ending with `_N` where `N` is a number. If your input file has 2 sequences called `foo` and one called `foo_2`, then you will end up with two `foo_2`:&#xD;&#xA;&#xD;&#xA;    $ cat test.fa&#xD;&#xA;    &gt;foo_2&#xD;&#xA;    actg&#xD;&#xA;    &gt;foo&#xD;&#xA;    actg&#xD;&#xA;    &gt;foo&#xD;&#xA;    actg&#xD;&#xA;    $ perl -pe 's/$/_$seen{$_}/ if ++$seen{$_}&gt;1 and /^&gt;/; ' test.fa &#xD;&#xA;    &gt;foo_2&#xD;&#xA;    actg&#xD;&#xA;    &gt;foo&#xD;&#xA;    actg&#xD;&#xA;    &gt;foo_2&#xD;&#xA;    actg&#xD;&#xA;    &#xD;&#xA;If this can be an issue for you, use one of the more sophisticated approaches suggested by the other answers." />
  <row Id="3111" PostHistoryTypeId="2" PostId="985" RevisionGUID="9e91b72c-95d5-4b56-b21d-21b7fa90f50d" CreationDate="2017-07-04T14:13:22.690" UserId="208" Text="### An example solution for Seurat:&#xD;&#xA;&#xD;&#xA;Retrieve sample IDs from the .csv used with `cellranger`:&#xD;&#xA; &#xD;&#xA;    samples &lt;- read.csv(file.path(&quot;/path/to/csv&quot;, &quot;nameof.csv&quot;), stringsAsFactors=F)&#xD;&#xA;&#xD;&#xA;Load the 10x dataset and initialize the Seurat object:&#xD;&#xA;&#xD;&#xA;    cells.data &lt;- Read10X(&quot;path/to/filtered_gene_bc_matrices&quot;)&#xD;&#xA;&#xD;&#xA;    cells &lt;- new(&quot;seurat&quot;, raw.data=cells.data)&#xD;&#xA;&#xD;&#xA;Get barcodes and suffix:&#xD;&#xA;&#xD;&#xA;    cellcodes &lt;- as.data.frame(cells@raw.data@Dimnames[[2]])&#xD;&#xA;    colnames(cellcodes) &lt;- &quot;barcodes&quot;&#xD;&#xA;    rownames(cellcodes) &lt;- cellcodes$barcodes&#xD;&#xA;&#xD;&#xA;    cellcodes$libcodes &lt;- as.factor(gsub(pattern=&quot;.+-&quot;, replacement=&quot;&quot;, cellcodes$barcodes))&#xD;&#xA;    cellcodes$samples &lt;- as.vector(samples$library_id[cellcodes$libcodes])&#xD;&#xA;&#xD;&#xA;Create dataframe for `meta.data` argument and set up object:&#xD;&#xA;&#xD;&#xA;    sampleidentity &lt;- cellcodes[&quot;samples&quot;]&#xD;&#xA;&#xD;&#xA;    cells &lt;- Setup(cells,&#xD;&#xA;                   meta.data=sampleidentity,&#xD;&#xA;                   min.cells=3,&#xD;&#xA;                   min.genes=200,&#xD;&#xA;                   do.logNormalize=T, total.expr=1e4, project=&quot;projectname&quot;)&#xD;&#xA;&#xD;&#xA;Use the `group.by` argument of various functions." />
  <row Id="3112" PostHistoryTypeId="5" PostId="982" RevisionGUID="74f70f47-c262-4758-bebc-cfd1b93ae40f" CreationDate="2017-07-04T14:56:02.313" UserId="298" Comment="Made the question less subjective." Text="I was wondering how I can calculate the charge of a protein peptide (e.g. &quot;RKTTLVPNTQTASPR&quot;) computationally in R or another tool." />
  <row Id="3114" PostHistoryTypeId="2" PostId="986" RevisionGUID="30fabad0-2394-44c7-9036-c031412632b5" CreationDate="2017-07-04T16:06:20.003" UserId="9" Text="You can download a list of transcript annotations as a flat file from UCSC:&#xD;&#xA;&#xD;&#xA;[http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz](http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz)&#xD;&#xA;&#xD;&#xA;It is not exactly a BED file, but it does contain information about the known transcripts for this assembly (hg19, in this case):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    585	NR_046018	chr1	+	11873	14409	14409	14409	3	11873,12612,13220,	12227,12721,14409,	0	DDX11L1	unk	unk	-1,-1,-1,&#xD;&#xA;    585	NR_024540	chr1	-	14361	29370	29370	29370	11	14361,14969,15795,16606,16857,17232,17605,17914,18267,24737,29320,	14829,15038,15947,16765,17055,17368,17742,18061,18366,24891,29370,	0	WASH7P	unk	unk	-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,&#xD;&#xA;&#xD;&#xA;The gene name can be found in column 13. The exon start and end positions are in columns 10 and 11, respectively. More information can be found in the other columns:&#xD;&#xA;&#xD;&#xA;1. bin&#xD;&#xA;1. name&#xD;&#xA;1. chrom&#xD;&#xA;1. strand&#xD;&#xA;1. txStart&#xD;&#xA;1. txEnd&#xD;&#xA;1. cdsStart&#xD;&#xA;1. cdsEnd&#xD;&#xA;1. exonCount&#xD;&#xA;1. exonStarts&#xD;&#xA;1. exonEnds&#xD;&#xA;1. score&#xD;&#xA;1. name2&#xD;&#xA;1. cdsStartStat&#xD;&#xA;1. cdsEndStat&#xD;&#xA;1. exonFrames&#xD;&#xA;&#xD;&#xA;(The column information was obtained from the [sql file here](http://hgdownload.cse.ucsc.edu/goldenpath/hg19/database/refGene.sql))" />
  <row Id="3115" PostHistoryTypeId="2" PostId="987" RevisionGUID="928045c4-3c3e-47dc-a648-012091f42666" CreationDate="2017-07-04T16:24:13.710" UserId="492" Text="Bcftools version 1.3.1 seems to work for me.  Only looks at first alternate allele:&#xD;&#xA;&#xD;&#xA;    bgzip Test.vcf&#xD;&#xA;    bcftools index Test.vcf.gz&#xD;&#xA;    bcftools filter -i 'FORMAT/AD[1] &gt; 10' Test.vcf.gz &#xD;&#xA;&#xD;&#xA;Output (without header):&#xD;&#xA;&#xD;&#xA;    6* 3	187451609	rs1880101	A	G	39794	PASS	AC=2;AF=1;AN=2;BaseQRankSum=1.859;ClippingRankSum=0;DB;DP=995;ExcessHet=3.0103;FS=0;MLEAC=2;MLEAF=1;MQ=60;MQRankSum=0;QD=24.56;ReadPosRankSum=0.406;SOR=8.234	GT:AD:DP:GQ:PL	1/1:1,988:989:99:39808,2949,0&#xD;&#xA;    7* 4	1803279	.	T	G	0	PASS	AC=0;AF=0;AN=2;BaseQRankSum=-6.652;ClippingRankSum=0;DP=245;ExcessHet=3.0103;FS=89.753;MLEAC=0;MLEAF=0;MQ=59.97;MQRankSum=0;ReadPosRankSum=-2.523;SOR=6.357	GT:AD:DP:GQ:PL	0/0:211,23:234:99:0,364,6739&#xD;&#xA;    8* 4	1803307	rs2305183	T	C	2486.6	PASS	AC=1;AF=0.5;AN=2;BaseQRankSum=-5.049;ClippingRankSum=0;DB;DP=215;ExcessHet=3.0103;FS=1.11;MLEAC=1;MLEAF=0.5;MQ=59.97;MQRankSum=0;QD=11.95;ReadPosRankSum=-0.045;SOR=0.809	GT:AD:DP:GQ:PL	0/1:116,92:208:99:2494,0,3673&#xD;&#xA;    12* 4	1803704	rs2234909	T	C	6676.6	PASS	AC=1;AF=0.5;AN=2;BaseQRankSum=-2.605;ClippingRankSum=0;DB;DP=456;ExcessHet=3.0103;FS=1.753;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=14.71;ReadPosRankSum=0.324;SOR=0.849	GT:AD:DP:GQ:PL	0/1:220,234:454:99:6684,0,6366&#xD;&#xA;    13* 4	1803824	rs2305184	C	G	2030.6	PASS	AC=1;AF=0.5;AN=2;BaseQRankSum=8.083;ClippingRankSum=0;DB;DP=124;ExcessHet=3.0103;FS=6.128;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=16.51;ReadPosRankSum=0.18;SOR=0.096	GT:AD:DP:GQ:PL	0/1:62,61:123:99:2038,0,1766&#xD;&#xA;    14* 4	1805296	rs3135883	G	A	3876.03	PASS	AC=2;AF=1;AN=2;DB;DP=110;ExcessHet=3.0103;FS=0;MLEAC=2;MLEAF=1;MQ=60;QD=29.22;SOR=9.401	GT:AD:DP:GQ:PL	1/1:0,109:109:99:3890,326,0&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3116" PostHistoryTypeId="5" PostId="984" RevisionGUID="2f93e117-fd83-4d9d-9964-0931df9ca4e6" CreationDate="2017-07-04T17:14:57.440" UserId="29" Comment="added 29 characters in body" Text="And here’s a solution using R (with the Bioconductor):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    fa = ShortRead::readFasta(infile)&#xD;&#xA;    ids = as.character(ShortRead::id(fa))&#xD;&#xA;    fa@id = Biostrings::BStringSet(make.unique(ids, sep = '_'))&#xD;&#xA;    ShortRead::writeFasta(fa, outfile)" />
  <row Id="3117" PostHistoryTypeId="5" PostId="897" RevisionGUID="99b1ac5e-3aff-4aee-a573-b3e2e653f27e" CreationDate="2017-07-04T17:42:55.687" UserId="776" Comment="added 2 characters in body" Text="Via Gencode and BEDOPS `convert2bed`:&#xD;&#xA;  &#xD;&#xA;    $ wget -qO- ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_21/gencode.v21.annotation.gff3.gz \&#xD;&#xA;        | gunzip --stdout - \&#xD;&#xA;        | awk '$3 == &quot;gene&quot;' - \&#xD;&#xA;        | convert2bed -i gff - \&#xD;&#xA;        &gt; genes.bed&#xD;&#xA;&#xD;&#xA;You can modify the `awk` statement to get exons, by replacing `gene` with `exon`.&#xD;&#xA;&#xD;&#xA;BEDOPS: https://github.com/bedops/bedops&#xD;&#xA;&#xD;&#xA;This is based off an answer I wrote on Biostars, which includes a Perl script for generating a BED file of introns from gene and exon annotations: https://www.biostars.org/p/124515/#124522" />
  <row Id="3118" PostHistoryTypeId="5" PostId="981" RevisionGUID="3a2b1dd9-5f69-4ff5-a91a-33c4001b2e83" CreationDate="2017-07-04T19:06:59.237" UserId="77" Comment="added 1149 characters in body" Text="Sure, using biopython:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;&#xD;&#xA;    records = set()&#xD;&#xA;    of = open(&quot;output.fa&quot;, &quot;w&quot;)&#xD;&#xA;    for record in SeqIO.parse(&quot;foo.fa&quot;, &quot;fasta&quot;):&#xD;&#xA;        ID = record.id&#xD;&#xA;        num = 1&#xD;&#xA;        while ID in records:&#xD;&#xA;            ID = &quot;{}_{}&quot;.format(record.id, num)&#xD;&#xA;            break&#xD;&#xA;            num += 1&#xD;&#xA;        records.add(ID)&#xD;&#xA;        record.id = ID&#xD;&#xA;        record.name = ID&#xD;&#xA;        record.description = ID&#xD;&#xA;        SeqIO.write(record, of, &quot;fasta&quot;)&#xD;&#xA;    of.close()&#xD;&#xA;&#xD;&#xA;Change `output.fa` and `foo.fa`. One doesn't need to explicitly change the `.name` and `.description`, but that's handy to prevent the original ID from not appearing still (after a space).&#xD;&#xA;&#xD;&#xA;# Explanation&#xD;&#xA;&#xD;&#xA; * `records = set()` This will create a lookup table of all IDs **written**&#xD;&#xA; * `for record in SeqIO.parse(&quot;foo.fa&quot;, &quot;fasta&quot;):` Open `foo.fa` as a fasta file and iterate over the **entries** in it. These are objects with an ID, name, description, and sequence attribute (the name and description are the same as the ID if not present).&#xD;&#xA; * `ID = record.id` Memoize the entry ID (e.g., `1_duplicateName`)&#xD;&#xA; * `while ID in records:` As long as the ID has already been seen, keep looping.&#xD;&#xA; * `ID = &quot;{}_{}&quot;.format(record.id, num)` Start adding increasing numbers after the ID, such as `1_duplicateName_1` and `1_duplicateName_2`. This will continue until the ID has not been seen.&#xD;&#xA; * `records.add(ID)` Add the unseen ID to the set.&#xD;&#xA; * `record.id = ID` Update the ID, the `.name` and `.description` are the same. If you don't do that, then you get output like `&gt;1_duplicateName_1 1_duplicateName`. That's not really a problem, but it's excessive.&#xD;&#xA; * `SeqIO.write(record, of, &quot;fasta&quot;)` Write the record to the output file in fasta format.&#xD;&#xA;&#xD;&#xA;A benefit of biopython is the easy ability to change formats, so instead of fasta one could have substituted Genbank or another format if needed." />
  <row Id="3119" PostHistoryTypeId="5" PostId="981" RevisionGUID="eff6bf0f-450e-4568-84e0-7433858b78a3" CreationDate="2017-07-04T22:57:26.473" UserId="298" Comment="typo" Text="Sure, using biopython:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;&#xD;&#xA;    records = set()&#xD;&#xA;    of = open(&quot;output.fa&quot;, &quot;w&quot;)&#xD;&#xA;    for record in SeqIO.parse(&quot;foo.fa&quot;, &quot;fasta&quot;):&#xD;&#xA;        ID = record.id&#xD;&#xA;        num = 1&#xD;&#xA;        while ID in records:&#xD;&#xA;            ID = &quot;{}_{}&quot;.format(record.id, num)&#xD;&#xA;            break&#xD;&#xA;            num += 1&#xD;&#xA;        records.add(ID)&#xD;&#xA;        record.id = ID&#xD;&#xA;        record.name = ID&#xD;&#xA;        record.description = ID&#xD;&#xA;        SeqIO.write(record, of, &quot;fasta&quot;)&#xD;&#xA;    of.close()&#xD;&#xA;&#xD;&#xA;Change `output.fa` and `foo.fa`. One doesn't need to explicitly change the `.name` and `.description`, but that's handy to prevent the original ID from not appearing still (after a space).&#xD;&#xA;&#xD;&#xA;# Explanation&#xD;&#xA;&#xD;&#xA; * `records = set()` This will create a lookup table of all IDs **written**&#xD;&#xA; * `for record in SeqIO.parse(&quot;foo.fa&quot;, &quot;fasta&quot;):` Open `foo.fa` as a fasta file and iterate over the **entries** in it. These are objects with an ID, name, description, and sequence attribute (the name and description are the same as the ID if not present).&#xD;&#xA; * `ID = record.id` Memorize the entry ID (e.g., `1_duplicateName`)&#xD;&#xA; * `while ID in records:` As long as the ID has already been seen, keep looping.&#xD;&#xA; * `ID = &quot;{}_{}&quot;.format(record.id, num)` Start adding increasing numbers after the ID, such as `1_duplicateName_1` and `1_duplicateName_2`. This will continue until the ID has not been seen.&#xD;&#xA; * `records.add(ID)` Add the unseen ID to the set.&#xD;&#xA; * `record.id = ID` Update the ID, the `.name` and `.description` are the same. If you don't do that, then you get output like `&gt;1_duplicateName_1 1_duplicateName`. That's not really a problem, but it's excessive.&#xD;&#xA; * `SeqIO.write(record, of, &quot;fasta&quot;)` Write the record to the output file in fasta format.&#xD;&#xA;&#xD;&#xA;A benefit of biopython is the easy ability to change formats, so instead of fasta one could have substituted Genbank or another format if needed." />
  <row Id="3123" PostHistoryTypeId="2" PostId="989" RevisionGUID="98fa05a4-b594-4511-bff1-1560fcf5170c" CreationDate="2017-07-05T06:55:02.657" UserId="174" Text="I'd like to simulate 10% sequencing error using `art_illumina`. The simulator doesn't have a parameter that I can just give the 10%, but it has this:&#xD;&#xA;&#xD;&#xA;    -qs  --qShift   the amount to shift every first-read quality score by&#xD;&#xA;    -qs2 --qShift2  the amount to shift every second-read quality score by&#xD;&#xA;                      NOTE: For -qs/-qs2 option, a positive number will shift up quality scores (the max is 93) &#xD;&#xA;                      that reduce substitution sequencing errors and a negative number will shift down &#xD;&#xA;                      quality scores that increase sequencing errors. If shifting scores by x, the error&#xD;&#xA;                      rate will be 1/(10^(x/10)) of the default profile.&#xD;&#xA;&#xD;&#xA;**Q:** Does the &quot;error rate&quot; in the description mean sequencing error? If I set both `qShift` and `qShift2` to 10, `1/(10^(x/10)) == 0.1, where x is 10`, does that mean 10 is the value I should give to the simulator?&#xD;&#xA;" />
  <row Id="3124" PostHistoryTypeId="1" PostId="989" RevisionGUID="98fa05a4-b594-4511-bff1-1560fcf5170c" CreationDate="2017-07-05T06:55:02.657" UserId="174" Text="Shift every quality score == sequencing error?" />
  <row Id="3125" PostHistoryTypeId="3" PostId="989" RevisionGUID="98fa05a4-b594-4511-bff1-1560fcf5170c" CreationDate="2017-07-05T06:55:02.657" UserId="174" Text="&lt;simulation&gt;" />
  <row Id="3126" PostHistoryTypeId="5" PostId="989" RevisionGUID="67f7f101-f2c5-49fb-8d64-5b01f8b7dd96" CreationDate="2017-07-05T07:30:06.700" UserId="174" Comment="added 96 characters in body; edited title" Text="I'd like to simulate 10% sequencing error using `art_illumina`. The simulator doesn't have a parameter that I can just give the 10%, but it has this:&#xD;&#xA;&#xD;&#xA;    -qs  --qShift   the amount to shift every first-read quality score by&#xD;&#xA;    -qs2 --qShift2  the amount to shift every second-read quality score by&#xD;&#xA;                      NOTE: For -qs/-qs2 option, a positive number will shift up quality scores (the max is 93) &#xD;&#xA;                      that reduce substitution sequencing errors and a negative number will shift down &#xD;&#xA;                      quality scores that increase sequencing errors. If shifting scores by x, the error&#xD;&#xA;                      rate will be 1/(10^(x/10)) of the default profile.&#xD;&#xA;&#xD;&#xA;Heng Li's `wgsim` has an option ofr &quot;base error rate&quot;. Can I do the same for `art_illumina`?&#xD;&#xA;&#xD;&#xA;**Q:** Does the &quot;error rate&quot; in the description mean sequencing error? If I set both `qShift` and `qShift2` to 10, `1/(10^(x/10)) == 0.1, where x is 10`, does that mean 10 is the value I should give to the simulator?&#xD;&#xA;" />
  <row Id="3127" PostHistoryTypeId="4" PostId="989" RevisionGUID="67f7f101-f2c5-49fb-8d64-5b01f8b7dd96" CreationDate="2017-07-05T07:30:06.700" UserId="174" Comment="added 96 characters in body; edited title" Text="How to simulate &quot;base error rate&quot; in art_illumina?" />
  <row Id="3128" PostHistoryTypeId="5" PostId="989" RevisionGUID="e73c2dbd-20c3-4108-9d99-34b96181f91f" CreationDate="2017-07-05T08:07:56.757" UserId="174" Comment="edited body" Text="I'd like to simulate 10% sequencing error using `art_illumina`. The simulator doesn't have a parameter that I can just give the 10%, but it has this:&#xD;&#xA;&#xD;&#xA;    -qs  --qShift   the amount to shift every first-read quality score by&#xD;&#xA;    -qs2 --qShift2  the amount to shift every second-read quality score by&#xD;&#xA;                      NOTE: For -qs/-qs2 option, a positive number will shift up quality scores (the max is 93) &#xD;&#xA;                      that reduce substitution sequencing errors and a negative number will shift down &#xD;&#xA;                      quality scores that increase sequencing errors. If shifting scores by x, the error&#xD;&#xA;                      rate will be 1/(10^(x/10)) of the default profile.&#xD;&#xA;&#xD;&#xA;Heng Li's `wgsim` has an option for base error rate. Can I do the same for `art_illumina`?&#xD;&#xA;&#xD;&#xA;**Q:** Does the &quot;error rate&quot; in the description mean sequencing error? If I set both `qShift` and `qShift2` to 10, `1/(10^(x/10)) == 0.1, where x is 10`, does that mean 10 is the value I should give to the simulator?&#xD;&#xA;" />
  <row Id="3130" PostHistoryTypeId="5" PostId="989" RevisionGUID="a11bebd7-61eb-438c-83c5-603a1661742d" CreationDate="2017-07-05T08:31:34.743" UserId="298" Comment="Fixed formatting" Text="I'd like to simulate 10% sequencing error using `art_illumina`. The simulator doesn't have a parameter that I can just give the 10%, but it has this:&#xD;&#xA;&#xD;&#xA;    -qs  --qShift   the amount to shift every first-read quality score by&#xD;&#xA;    -qs2 --qShift2  the amount to shift every second-read quality score by&#xD;&#xA;                    NOTE: For -qs/-qs2 option, a positive number will shift up &#xD;&#xA;                    quality scores (the max is 93) that reduce substitution sequencing &#xD;&#xA;                    errors and a negative number will shift down quality scores that &#xD;&#xA;                    increase sequencing errors. If shifting scores by x, the error&#xD;&#xA;                    rate will be 1/(10^(x/10)) of the default profile.&#xD;&#xA;&#xD;&#xA;Heng Li's `wgsim` has an option for base error rate. Can I do the same for `art_illumina`?&#xD;&#xA;&#xD;&#xA;**Q:** Does the &quot;error rate&quot; in the description mean sequencing error? If I set both `qShift` and `qShift2` to 10, `1/(10^(x/10)) == 0.1, where x is 10`, does that mean 10 is the value I should give to the simulator?&#xD;&#xA;" />
  <row Id="3131" PostHistoryTypeId="6" PostId="989" RevisionGUID="a11bebd7-61eb-438c-83c5-603a1661742d" CreationDate="2017-07-05T08:31:34.743" UserId="298" Comment="Fixed formatting" Text="&lt;ngs&gt;&lt;simulated-data&gt;" />
  <row Id="3132" PostHistoryTypeId="6" PostId="81" RevisionGUID="55dd7445-5f62-4668-bb37-53af0213c987" CreationDate="2017-07-05T08:32:11.957" UserId="298" Comment="edited tags" Text="&lt;fasta&gt;&lt;ngs&gt;&lt;simulated-data&gt;" />
  <row Id="3133" PostHistoryTypeId="6" PostId="837" RevisionGUID="f9efbaa9-95fa-4e47-95c1-19dfbdb39c35" CreationDate="2017-07-05T08:32:22.120" UserId="298" Comment="edited tags" Text="&lt;biopython&gt;&lt;bed&gt;&lt;structural-variation&gt;&lt;indel&gt;&lt;simulated-data&gt;" />
  <row Id="3134" PostHistoryTypeId="2" PostId="990" RevisionGUID="2a124e88-215d-4ecc-80ae-836f22b6540d" CreationDate="2017-07-05T08:56:46.243" UserId="369" Text="I am interested in  which are the most commonly used databases / resources for investigating microbial eukaryotes, in terms of finding and analyzing Single Amplified Genomes, and genomes in general." />
  <row Id="3135" PostHistoryTypeId="1" PostId="990" RevisionGUID="2a124e88-215d-4ecc-80ae-836f22b6540d" CreationDate="2017-07-05T08:56:46.243" UserId="369" Text="Genome databases of microbial eukaryotes" />
  <row Id="3136" PostHistoryTypeId="3" PostId="990" RevisionGUID="2a124e88-215d-4ecc-80ae-836f22b6540d" CreationDate="2017-07-05T08:56:46.243" UserId="369" Text="&lt;database&gt;&lt;genome&gt;" />
  <row Id="3137" PostHistoryTypeId="5" PostId="990" RevisionGUID="c66d9a8c-e40f-4dd9-8294-e608d97348bc" CreationDate="2017-07-05T09:24:30.257" UserId="369" Comment="added 16 characters in body" Text="I am interested in collecting the reference genomes of marine protists and would like to know which databases can be used for such tasks, and is there a single, centralized database of protistological genomic data?" />
  <row Id="3138" PostHistoryTypeId="2" PostId="991" RevisionGUID="b591c93c-3ee7-429a-942f-33d1bbf5552a" CreationDate="2017-07-05T09:28:22.160" UserId="939" Text="You can find many protist genomes on ensembl&#xD;&#xA;&#xD;&#xA;http://protists.ensembl.org/index.html" />
  <row Id="3140" PostHistoryTypeId="2" PostId="992" RevisionGUID="0c9115a2-7fe4-4b91-8c63-7828e9f07cdf" CreationDate="2017-07-05T10:01:28.437" UserId="369" Text="That does not appear to be possible using PFAM. I would suggest trying a different database. I managed to perform the same query using SMART (http://smart.embl-heidelberg.de/), via entering &quot;Pfam:FAD_binding_2 AND Pfam:Succ_DH_flav_C&quot; under &quot;Domain selection&quot; in the &quot;Architecture Analysis&quot; section of the search screen, which resulted in 9921 protein sequences with those two domains as you can see [here](http://smart.embl-heidelberg.de/smart/selective.cgi?domains=Pfam%3AFAD_binding_2+AND+Pfam%3ASucc_DH_flav_C&amp;terms=&amp;taxon_text=&amp;input=Architecture+query).&#xD;&#xA;&#xD;&#xA;On the results screen you can select all the sequences and under &quot;Action&quot; at the top of the screen choose &quot;Download Protein Sequences as FASTA file&quot;." />
  <row Id="3141" PostHistoryTypeId="2" PostId="993" RevisionGUID="c6209b46-8f88-4edc-b70b-2c41d5850aa3" CreationDate="2017-07-05T12:00:40.840" UserId="191" Text="I believe you can do this using SQL files of the database.&#xD;&#xA;&#xD;&#xA;First you need to get the integer accession of this architecture. [`architecture` table][1] has the following columns:&#xD;&#xA;&#xD;&#xA;- `auto_architecture` (integer id, **this is what we need**)&#xD;&#xA;- `architecture` (a string describing the architecture)&#xD;&#xA;- `type_example` (example sequence)&#xD;&#xA;- `no_seqs` (number of sequences)&#xD;&#xA;- `architecture_acc` (accessions of the architectures)&#xD;&#xA;&#xD;&#xA;You need the following line:&#xD;&#xA;&#xD;&#xA;    3900719357      FAD_binding_2~Succ_DH_flav_C    Z9JRB3  7471    PF00890 PF02910&#xD;&#xA;&#xD;&#xA;Now you need to get the sequences with this architecture id. First, download the [file with all the sequences][2] (**warning**, this one is large). With this one you need column #1 (pfam sequence accesion), #12 (sequence itself) and #16 (architecture integer id).&#xD;&#xA;&#xD;&#xA;If you do not want to import files in the database, you can use `awk` to export all the sequences to `fasta` format.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    zcat pfamseq.txt.gz | awk -F\\t '{ if ($16==&quot;3900719357&quot;) print &quot;&gt;&quot; $1 &quot;\n&quot; $12}' &gt; sequences.fasta&#xD;&#xA;&#xD;&#xA;Of couse, if you need to perform multiple queries like this, it is much easier to import tables into the database and to perform an SQL query:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-sql --&gt;&#xD;&#xA;&#xD;&#xA;    SELECT pfamseq_acc, sequence&#xD;&#xA;        FROM architecture&#xD;&#xA;        JOIN pfamseq ON pfamseq.auto_architecture=architecture.auto_architecture&#xD;&#xA;        WHERE architecture=&quot;FAD_binding_2~Succ_DH_flav_C&quot;&#xD;&#xA;&#xD;&#xA;(I haven't tried this SQL query, so maybe you need to correct it a bit)&#xD;&#xA;&#xD;&#xA;  [1]: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/architecture.txt.gz&#xD;&#xA;  [2]: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/pfamseq.txt.gz" />
  <row Id="3142" PostHistoryTypeId="5" PostId="993" RevisionGUID="32d97fa0-2598-41a7-9cc4-5ad675953a30" CreationDate="2017-07-05T13:40:43.893" UserId="191" Comment="specify file size" Text="I believe you can do this using SQL files of the database.&#xD;&#xA;&#xD;&#xA;First you need to get the integer accession of this architecture. [`architecture` table][1] has the following columns:&#xD;&#xA;&#xD;&#xA;- `auto_architecture` (integer id, **this is what we need**)&#xD;&#xA;- `architecture` (a string describing the architecture)&#xD;&#xA;- `type_example` (example sequence)&#xD;&#xA;- `no_seqs` (number of sequences)&#xD;&#xA;- `architecture_acc` (accessions of the architectures)&#xD;&#xA;&#xD;&#xA;You need the following line:&#xD;&#xA;&#xD;&#xA;    3900719357      FAD_binding_2~Succ_DH_flav_C    Z9JRB3  7471    PF00890 PF02910&#xD;&#xA;&#xD;&#xA;Now you need to get the sequences with this architecture id. First, download the [file with all the sequences][2] (**warning**, this one is more than 7Gb). With this one you need column #1 (pfam sequence accesion), #12 (sequence itself) and #16 (architecture integer id).&#xD;&#xA;&#xD;&#xA;If you do not want to import files in the database, you can use `awk` to export all the sequences to `fasta` format.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    zcat pfamseq.txt.gz | awk -F\\t '{ if ($16==&quot;3900719357&quot;) print &quot;&gt;&quot; $1 &quot;\n&quot; $12}' &gt; sequences.fasta&#xD;&#xA;&#xD;&#xA;Of couse, if you need to perform multiple queries like this, it is much easier to import tables into the database and to perform an SQL query:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-sql --&gt;&#xD;&#xA;&#xD;&#xA;    SELECT pfamseq_acc, sequence&#xD;&#xA;        FROM architecture&#xD;&#xA;        JOIN pfamseq ON pfamseq.auto_architecture=architecture.auto_architecture&#xD;&#xA;        WHERE architecture=&quot;FAD_binding_2~Succ_DH_flav_C&quot;&#xD;&#xA;&#xD;&#xA;(I haven't tried this SQL query, so maybe you need to correct it a bit)&#xD;&#xA;&#xD;&#xA;  [1]: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/architecture.txt.gz&#xD;&#xA;  [2]: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/pfamseq.txt.gz" />
  <row Id="3143" PostHistoryTypeId="5" PostId="969" RevisionGUID="463b2f6b-b0d7-45a8-b276-e12a5c41ed02" CreationDate="2017-07-05T13:47:56.627" UserId="191" Comment="formatting" Text="I would like to ask if anyone has experience in running a subset of the PASA pipeline, in particular for the reconciliation of some experimental 'transcripts' with the reference annotation.&#xD;&#xA;&#xD;&#xA;In more detail, I am working with RNA-seq data from *D. melanogaster*. I have reconstructed the 'transcripts' using Trinity. I have aligned these 'transcripts' to the reference genome using GMAP. Now I would like to match these 'experimental transcripts' with the reference annotation, to see how they compare.&#xD;&#xA;&#xD;&#xA;I was wondering if I can run just a subset of the whole PASA pipeline, so basically skipping the step of alignment to the reference and using the alignment file that I generated externally.&#xD;&#xA;&#xD;&#xA;Also, I would be glad to receive more general redirecting to any released software allowing the comparison of experimental transcripts to the reference annotation. &#xD;&#xA;&#xD;&#xA;Cross-posted on the [PASA Google Group pasapipeline-users](https://groups.google.com/forum/#!topic/pasapipeline-users/cyG9e8-HjG8)&#xD;&#xA;&#xD;&#xA;Cross-posted [on biostars](https://www.biostars.org/p/260594/)&#xD;&#xA;" />
  <row Id="3144" PostHistoryTypeId="2" PostId="994" RevisionGUID="0a24910a-8752-4976-ad9a-7ce8af698f73" CreationDate="2017-07-05T14:20:21.857" UserId="842" Text="I have 446 whole Klebsiella Pneumoniae genomes I want to build a phylogenetic tree from. After reading about constructing phylogenetic trees it seems the only option for large numbers of genomes is to isolate a gene with low variability from generation to generation and use this gene to build a tree. For example Lars Jensen recommends using &quot;16S rRNA [or] all ribosomal-protein-coding genes&quot; https://www.biostars.org/p/1930/. What program isolates these genes of interest from the whole genome fasta files and can put them into a multiple alignment file? Or outputs them in a formate ready for a multiple alignment program such as Muave? The reason I say a multiple alignment file is because this it the type of file most phylogenetic tree programs take (I.E. clonalframe)." />
  <row Id="3145" PostHistoryTypeId="1" PostId="994" RevisionGUID="0a24910a-8752-4976-ad9a-7ce8af698f73" CreationDate="2017-07-05T14:20:21.857" UserId="842" Text="How to isolate genes from whole genomes for phylogenetic tree analysis?" />
  <row Id="3146" PostHistoryTypeId="3" PostId="994" RevisionGUID="0a24910a-8752-4976-ad9a-7ce8af698f73" CreationDate="2017-07-05T14:20:21.857" UserId="842" Text="&lt;gene&gt;&lt;phylogeny&gt;" />
  <row Id="3147" PostHistoryTypeId="5" PostId="993" RevisionGUID="0ffc02da-d257-4b8a-a043-d35ade6d91a6" CreationDate="2017-07-05T14:23:45.510" UserId="191" Comment="add semicolon to sql" Text="I believe you can do this using SQL files of the database.&#xD;&#xA;&#xD;&#xA;First you need to get the integer accession of this architecture. [`architecture` table][1] has the following columns:&#xD;&#xA;&#xD;&#xA;- `auto_architecture` (integer id, **this is what we need**)&#xD;&#xA;- `architecture` (a string describing the architecture)&#xD;&#xA;- `type_example` (example sequence)&#xD;&#xA;- `no_seqs` (number of sequences)&#xD;&#xA;- `architecture_acc` (accessions of the architectures)&#xD;&#xA;&#xD;&#xA;You need the following line:&#xD;&#xA;&#xD;&#xA;    3900719357      FAD_binding_2~Succ_DH_flav_C    Z9JRB3  7471    PF00890 PF02910&#xD;&#xA;&#xD;&#xA;Now you need to get the sequences with this architecture id. First, download the [file with all the sequences][2] (**warning**, this one is more than 7Gb). With this one you need column #1 (pfam sequence accesion), #12 (sequence itself) and #16 (architecture integer id).&#xD;&#xA;&#xD;&#xA;If you do not want to import files in the database, you can use `awk` to export all the sequences to `fasta` format.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    zcat pfamseq.txt.gz | awk -F\\t '{ if ($16==&quot;3900719357&quot;) print &quot;&gt;&quot; $1 &quot;\n&quot; $12}' &gt; sequences.fasta&#xD;&#xA;&#xD;&#xA;Of couse, if you need to perform multiple queries like this, it is much easier to import tables into the database and to perform an SQL query:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-sql --&gt;&#xD;&#xA;&#xD;&#xA;    SELECT pfamseq_acc, sequence&#xD;&#xA;        FROM architecture&#xD;&#xA;        JOIN pfamseq ON pfamseq.auto_architecture=architecture.auto_architecture&#xD;&#xA;        WHERE architecture=&quot;FAD_binding_2~Succ_DH_flav_C&quot;;&#xD;&#xA;&#xD;&#xA;(I haven't tried this SQL query, so maybe you need to correct it a bit)&#xD;&#xA;&#xD;&#xA;  [1]: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/architecture.txt.gz&#xD;&#xA;  [2]: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/pfamseq.txt.gz" />
  <row Id="3148" PostHistoryTypeId="5" PostId="974" RevisionGUID="4ebcafc1-8df1-4961-8d34-19c567b29ea8" CreationDate="2017-07-05T14:33:37.963" UserId="532" Comment="edited body" Text="I have high-depth variant calling created using the HaplotypeCaller with `--output_mode EMIT_ALL_SITES` I'm interested in finding all sites (regardless of genotype call heterozygous or homozygous) where at least one of the **alternative** alleles have an `AD` value (Allelic Depth) greater than 10, *I*.*e*. are supported by more than 10 reads. &#xD;&#xA;&#xD;&#xA;So in the example VCF snippet below I'm wanting to select lines: 6,7,8,12,13 and 14, which have GT:AD values `1/1:1,988:989` `0/1:116,92` `0/1:220,234` `0/1:62,611` `1/1:0,109` respectively.&#xD;&#xA;&#xD;&#xA;    #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	12908_DIAG&#xD;&#xA;    3	187446740	.	T	.	Infinity	.	AN=2;DP=1095;MQ=60.00	GT:AD:DP	0/0:1095:1095&#xD;&#xA;    3	187446741	.	C	.	Infinity	.	AN=2;DP=1117;MQ=60.00	GT:AD:DP	0/0:1117:1117&#xD;&#xA;    3	187446752	.	A	.	Infinity	.	AN=2;DP=1297;MQ=60.00	GT:AD:DP	0/0:1297:1297&#xD;&#xA;    3	187446763	.	C	.	Infinity	.	AN=2;DP=1494;MQ=60.00	GT:AD:DP	0/0:1494:1494&#xD;&#xA;    3	187451574	.	C	.	Infinity	.	AN=2;DP=1493;MQ=60.00	GT:AD:DP	0/0:1493:1493&#xD;&#xA;    3	187451609	rs1880101	A	G	39794.03	.	AC=2;AF=1.00;AN=2;BaseQRankSum=1.859;ClippingRankSum=0.000;DB;DP=995;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQRankSum=0.000;QD=24.56;ReadPosRankSum=0.406;SOR=8.234	GT:AD:DP:GQ:PL	1/1:1,988:989:99:39808,2949,0&#xD;&#xA;    4	1803279	.	T	G	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-6.652;ClippingRankSum=0.000;DP=245;ExcessHet=3.0103;FS=89.753;MLEAC=0;MLEAF=0.00;MQ=59.97;MQRankSum=0.000;ReadPosRankSum=-2.523;SOR=6.357	GT:AD:DP:GQ:PL	0/0:211,23:234:99:0,364,6739&#xD;&#xA;    4	1803307	rs2305183	T	C	2486.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=-5.049;ClippingRankSum=0.000;DB;DP=215;ExcessHet=3.0103;FS=1.110;MLEAC=1;MLEAF=0.500;MQ=59.97;MQRankSum=0.000;QD=11.95;ReadPosRankSum=-0.045;SOR=0.809	GT:AD:DP:GQ:PL	0/1:116,92:208:99:2494,0,3673&#xD;&#xA;    4	1803671	.	C	A	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-0.880;ClippingRankSum=0.000;DP=450;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.953;SOR=0.572	GT:AD:DP:GQ:PL	0/0:445,2:447:99:0,1272,15958&#xD;&#xA;    4	1803681	.	T	C	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-1.654;ClippingRankSum=0.000;DP=483;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.422;SOR=0.664	GT:AD:DP:GQ:PL	0/0:479,2:481:99:0,1408,18538&#xD;&#xA;    4	1803703	.	A	G	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-1.704;ClippingRankSum=0.000;DP=458;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=0.299;SOR=0.497	GT:AD:DP:GQ:PL	0/0:454,2:456:99:0,1325,18095&#xD;&#xA;    4	1803704	rs2234909	T	C	6676.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=-2.605;ClippingRankSum=0.000;DB;DP=456;ExcessHet=3.0103;FS=1.753;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=14.71;ReadPosRankSum=0.324;SOR=0.849	GT:AD:DP:GQ:PL	0/1:220,234:454:99:6684,0,6366&#xD;&#xA;    4	1803824	rs2305184	C	G	2030.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=8.083;ClippingRankSum=0.000;DB;DP=124;ExcessHet=3.0103;FS=6.128;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=16.51;ReadPosRankSum=0.180;SOR=0.096	GT:AD:DP:GQ:PL	0/1:62,61:123:99:2038,0,1766&#xD;&#xA;    4	1805296	rs3135883	G	A	3876.03	.	AC=2;AF=1.00;AN=2;DB;DP=110;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.22;SOR=9.401	GT:AD:DP:GQ:PL	1/1:0,109:109:99:3890,326,0&#xD;&#xA;&#xD;&#xA;[A dropbox link for file](https://www.dropbox.com/s/gm4pftuk9i1gx6p/test.vcf?dl=1)&#xD;&#xA;&#xD;&#xA;I'd initially considered using GATK's SelectVariants but I'm not sure JEXL has the ability to select out what I want specifically other than a blanket AD &gt; 10 which will give me both ref and alt alleles with AD &gt; 10.  Perhaps there is a bioawk solution or something more elaborate with coreutils which could successfully return sites with an alt AD count &gt; 10? " />
  <row Id="3149" PostHistoryTypeId="5" PostId="993" RevisionGUID="e74e1a1a-8366-49be-81b1-9e3c6365996c" CreationDate="2017-07-05T14:35:18.587" UserId="191" Comment="clarify second approach" Text="I believe you can do this using SQL files of the database.&#xD;&#xA;&#xD;&#xA;First you need to get the integer accession of this architecture. [`architecture` table][1] has the following columns:&#xD;&#xA;&#xD;&#xA;- `auto_architecture` (integer id, **this is what we need**)&#xD;&#xA;- `architecture` (a string describing the architecture)&#xD;&#xA;- `type_example` (example sequence)&#xD;&#xA;- `no_seqs` (number of sequences)&#xD;&#xA;- `architecture_acc` (accessions of the architectures)&#xD;&#xA;&#xD;&#xA;You need the following line:&#xD;&#xA;&#xD;&#xA;    3900719357      FAD_binding_2~Succ_DH_flav_C    Z9JRB3  7471    PF00890 PF02910&#xD;&#xA;&#xD;&#xA;Now you need to get the sequences with this architecture id. First, download the [file with all the sequences][2] (**warning**, this one is more than 7Gb). With this one you need column #1 (pfam sequence accesion), #12 (sequence itself) and #16 (architecture integer id).&#xD;&#xA;&#xD;&#xA;If you do not want to import files in the database, you can use `awk` to export all the sequences to `fasta` format.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-bash --&gt;&#xD;&#xA;&#xD;&#xA;    zcat pfamseq.txt.gz | awk -F\\t '{ if ($16==&quot;3900719357&quot;) print &quot;&gt;&quot; $1 &quot;\n&quot; $12}' &gt; sequences.fasta&#xD;&#xA;&#xD;&#xA;Of couse, if you need to perform multiple queries like this, it is much easier to import tables into the database. Then you can perform both queries in a single SQL operation (assuming you know the architecture string):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-sql --&gt;&#xD;&#xA;&#xD;&#xA;    SELECT pfamseq_acc, sequence&#xD;&#xA;        FROM architecture&#xD;&#xA;        JOIN pfamseq ON pfamseq.auto_architecture=architecture.auto_architecture&#xD;&#xA;        WHERE architecture=&quot;FAD_binding_2~Succ_DH_flav_C&quot;;&#xD;&#xA;&#xD;&#xA;(I haven't tried this SQL query, so maybe you need to correct it a bit)&#xD;&#xA;&#xD;&#xA;  [1]: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/architecture.txt.gz&#xD;&#xA;  [2]: ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/pfamseq.txt.gz" />
  <row Id="3150" PostHistoryTypeId="2" PostId="995" RevisionGUID="a52234c8-4695-4c7d-8e36-db7c5d9dcffa" CreationDate="2017-07-05T14:50:16.667" UserId="581" Text="There are a lot of ways to do this. I suggest using Prokka/Roary to produce a core genome alignment. There's a useful tutorial on the [Roary](https://sanger-pathogens.github.io/Roary/) website." />
  <row Id="3151" PostHistoryTypeId="2" PostId="996" RevisionGUID="db517b21-2ffd-4a7d-8da3-5d925c6c1b10" CreationDate="2017-07-05T14:54:52.480" UserId="982" Text="Extract desired gene sequences using [standalone blast][1] &#xD;&#xA;&#xD;&#xA;Simply provide a reference database with your desired output. &#xD;&#xA;Set up your command and away you go. You can set the search up with a for loop for a batch of sequences. Command may look like &#xD;&#xA;&#xD;&#xA;    for f in *.fasta; do&#xD;&#xA;       f=$(basename $f .fasta)&#xD;&#xA;       blastn \&#xD;&#xA;       -outfmt &quot;6 sseqid qseq %&quot; \&#xD;&#xA;       -query $f.fasta \&#xD;&#xA;       -subject reference.fna \&#xD;&#xA;       &gt; out/$f.fna&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi" />
  <row Id="3152" PostHistoryTypeId="2" PostId="997" RevisionGUID="4eccc895-d29f-4026-a544-f1be132c4eba" CreationDate="2017-07-05T14:55:12.183" UserId="191" Text="Sometimes it useful to perform a nucleotide protein coding gene sequence alignment based on codons, not on individual nucleotides. For example for further codon model analysis it is important to have full codons.&#xD;&#xA;&#xD;&#xA;A widely used approach here is to perform a protein sequence alignment first and then impose this alignment to the nucleotide sequences using [PAL2NAL][1], [CodonAlign][2] or something similar.&#xD;&#xA;&#xD;&#xA;This is how [transAlign][3] or [GUIDANCE][4] (in codon mode) work.&#xD;&#xA;&#xD;&#xA;The problem here is that you are discarding part of the information which could be potentially used for the sequence alignment.&#xD;&#xA;&#xD;&#xA;I'm aware of two programs which can do true codon alignment. First, [PRANK][5] has a dedicated codon model, but it is rather slow and using it is overkill for certain problems. Second, [Sequence Manipulation Suite][6] can perform codon alignments, but only for a pair of sequences; also it's javascript based, therefore it is hard to run it for a large number of sequences.&#xD;&#xA;&#xD;&#xA;Can you recommend any software for multiple codon sequence alignment? Preferably available for offline use.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.bork.embl.de/pal2nal/&#xD;&#xA;  [2]: http://wbiomed.curtin.edu.au/bioinf/CodonAlign.php&#xD;&#xA;  [3]: https://dx.doi.org/10.1186/1471-2105-6-156&#xD;&#xA;  [4]: http://guidance.tau.ac.il/ver2/&#xD;&#xA;  [5]: http://wasabiapp.org/software/prank/&#xD;&#xA;  [6]: http://www.bioinformatics.org/sms2/pairwise_align_codons.html" />
  <row Id="3153" PostHistoryTypeId="1" PostId="997" RevisionGUID="4eccc895-d29f-4026-a544-f1be132c4eba" CreationDate="2017-07-05T14:55:12.183" UserId="191" Text="Which sequence alignment tools support codon alignment?" />
  <row Id="3154" PostHistoryTypeId="3" PostId="997" RevisionGUID="4eccc895-d29f-4026-a544-f1be132c4eba" CreationDate="2017-07-05T14:55:12.183" UserId="191" Text="&lt;sequence-alignemnt&gt;&lt;codon&gt;" />
  <row Id="3155" PostHistoryTypeId="2" PostId="998" RevisionGUID="d1517464-c350-4719-8ea5-f52749a50ef2" CreationDate="2017-07-05T15:23:26.553" UserId="1063" Text="I'm looking for a light Knowledge base describing human body to annotate disease sites. &#xD;&#xA;I do not need a great level of detail, I just need kind of like basic organs/sub-organs taxonomy. I checked out the *Foundational Model of Anatomy ontology*, but it's over-killing for my purpose and really cumbersome (Protege fails while tries to load FMA.owl).&#xD;&#xA;&#xD;&#xA;Is there a simple knowlege base available over the Internet?&#xD;&#xA;&#xD;&#xA;*(I'm new to this community, hope my question is not ill posed or completely OT)*" />
  <row Id="3156" PostHistoryTypeId="1" PostId="998" RevisionGUID="d1517464-c350-4719-8ea5-f52749a50ef2" CreationDate="2017-07-05T15:23:26.553" UserId="1063" Text="Human body sites knowledge base" />
  <row Id="3157" PostHistoryTypeId="3" PostId="998" RevisionGUID="d1517464-c350-4719-8ea5-f52749a50ef2" CreationDate="2017-07-05T15:23:26.553" UserId="1063" Text="&lt;database&gt;&lt;ontology&gt;" />
  <row Id="3158" PostHistoryTypeId="5" PostId="998" RevisionGUID="6530b2e7-d722-4cec-94d6-d8983c054130" CreationDate="2017-07-05T15:28:58.790" UserId="298" Comment="Minor corrections" Text="I'm looking for a lightweight knowledgebase describing the human body to annotate disease sites. &#xD;&#xA;I do not need a great level of detail, I just need a kind of basic organs/sub-organs taxonomy. I checked out the [*Foundational Model of Anatomy ontology*][1], but it's over-kill for my purposes and really cumbersome (Protege fails when it tries to load FMA.owl).&#xD;&#xA;&#xD;&#xA;Is there a simple online knowlegebase available for this?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://si.washington.edu/projects/fma" />
  <row Id="3159" PostHistoryTypeId="2" PostId="999" RevisionGUID="e9cafedd-4ca2-4e17-a964-8a4444707bde" CreationDate="2017-07-05T15:33:55.510" UserId="71" Text="search in [bioportal][1]. &#xD;&#xA;&#xD;&#xA;For example:  **SNOMED**: https://bioportal.bioontology.org/ontologies/SNOMEDCT?p=summary , **UBERON** : https://bioportal.bioontology.org/ontologies/UBERON?p=summary &#xD;&#xA;&#xD;&#xA;please, note that **FMA** https://bioportal.bioontology.org/ontologies/FMA?p=summary is also available as RDF, csv , etc...  &#xD;&#xA;&#xD;&#xA;you could also try to convert FMA to obo and try to open it with http://oboedit.org/&#xD;&#xA;  [1]: https://bioportal.bioontology.org" />
  <row Id="3160" PostHistoryTypeId="5" PostId="981" RevisionGUID="4c6bbaf7-df8a-48af-95f2-cb6382368e51" CreationDate="2017-07-05T15:49:58.337" UserId="77" Comment="Define &quot;memoize&quot;, since that's not commonly known" Text="Sure, using biopython:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;&#xD;&#xA;    records = set()&#xD;&#xA;    of = open(&quot;output.fa&quot;, &quot;w&quot;)&#xD;&#xA;    for record in SeqIO.parse(&quot;foo.fa&quot;, &quot;fasta&quot;):&#xD;&#xA;        ID = record.id&#xD;&#xA;        num = 1&#xD;&#xA;        while ID in records:&#xD;&#xA;            ID = &quot;{}_{}&quot;.format(record.id, num)&#xD;&#xA;            num += 1&#xD;&#xA;        records.add(ID)&#xD;&#xA;        record.id = ID&#xD;&#xA;        record.name = ID&#xD;&#xA;        record.description = ID&#xD;&#xA;        SeqIO.write(record, of, &quot;fasta&quot;)&#xD;&#xA;    of.close()&#xD;&#xA;&#xD;&#xA;Change `output.fa` and `foo.fa`. One doesn't need to explicitly change the `.name` and `.description`, but that's handy to prevent the original ID from not appearing still (after a space).&#xD;&#xA;&#xD;&#xA;# Explanation&#xD;&#xA;&#xD;&#xA; * `records = set()` This will create a lookup table of all IDs **written**&#xD;&#xA; * `for record in SeqIO.parse(&quot;foo.fa&quot;, &quot;fasta&quot;):` Open `foo.fa` as a fasta file and iterate over the **entries** in it. These are objects with an ID, name, description, and sequence attribute (the name and description are the same as the ID if not present).&#xD;&#xA; * `ID = record.id` [Memoize](https://en.wikipedia.org/wiki/Memoization) the entry ID (e.g., `1_duplicateName`)&#xD;&#xA; * `while ID in records:` As long as the ID has already been seen, keep looping.&#xD;&#xA; * `ID = &quot;{}_{}&quot;.format(record.id, num)` Start adding increasing numbers after the ID, such as `1_duplicateName_1` and `1_duplicateName_2`. This will continue until the ID has not been seen.&#xD;&#xA; * `records.add(ID)` Add the unseen ID to the set.&#xD;&#xA; * `record.id = ID` Update the ID, the `.name` and `.description` are the same. If you don't do that, then you get output like `&gt;1_duplicateName_1 1_duplicateName`. That's not really a problem, but it's excessive.&#xD;&#xA; * `SeqIO.write(record, of, &quot;fasta&quot;)` Write the record to the output file in fasta format.&#xD;&#xA;&#xD;&#xA;A benefit of biopython is the easy ability to change formats, so instead of fasta one could have substituted Genbank or another format if needed." />
  <row Id="3161" PostHistoryTypeId="5" PostId="974" RevisionGUID="115a83d9-1196-4174-983c-719d5bc7b8e4" CreationDate="2017-07-05T16:02:23.770" UserId="532" Comment="added 165 characters in body" Text="I have high-depth variant calling created using the HaplotypeCaller with `--output_mode EMIT_ALL_SITES` I'm interested in finding all sites (regardless of genotype call heterozygous or homozygous) where at least one of the **alternative** alleles have an `AD` value (Allelic Depth) greater than 10, *I*.*e*. are supported by more than 10 reads. Also ideally I want back more than just the first alternative allele.  Note that I don't want back lines of VCF were we only see an AD count for the ref allele only.&#xD;&#xA;&#xD;&#xA;So in the example VCF snippet below I'm wanting to select lines: 6,7,8,12,13 and 14, which have GT:AD values `1/1:1,988:989` `0/1:116,92` `0/1:220,234` `0/1:62,611` `1/1:0,109` respectively.&#xD;&#xA;&#xD;&#xA;    #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	12908_DIAG&#xD;&#xA;    3	187446740	.	T	.	Infinity	.	AN=2;DP=1095;MQ=60.00	GT:AD:DP	0/0:1095:1095&#xD;&#xA;    3	187446741	.	C	.	Infinity	.	AN=2;DP=1117;MQ=60.00	GT:AD:DP	0/0:1117:1117&#xD;&#xA;    3	187446752	.	A	.	Infinity	.	AN=2;DP=1297;MQ=60.00	GT:AD:DP	0/0:1297:1297&#xD;&#xA;    3	187446763	.	C	.	Infinity	.	AN=2;DP=1494;MQ=60.00	GT:AD:DP	0/0:1494:1494&#xD;&#xA;    3	187451574	.	C	.	Infinity	.	AN=2;DP=1493;MQ=60.00	GT:AD:DP	0/0:1493:1493&#xD;&#xA;    3	187451609	rs1880101	A	G	39794.03	.	AC=2;AF=1.00;AN=2;BaseQRankSum=1.859;ClippingRankSum=0.000;DB;DP=995;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQRankSum=0.000;QD=24.56;ReadPosRankSum=0.406;SOR=8.234	GT:AD:DP:GQ:PL	1/1:1,988:989:99:39808,2949,0&#xD;&#xA;    4	1803279	.	T	G	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-6.652;ClippingRankSum=0.000;DP=245;ExcessHet=3.0103;FS=89.753;MLEAC=0;MLEAF=0.00;MQ=59.97;MQRankSum=0.000;ReadPosRankSum=-2.523;SOR=6.357	GT:AD:DP:GQ:PL	0/0:211,23:234:99:0,364,6739&#xD;&#xA;    4	1803307	rs2305183	T	C	2486.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=-5.049;ClippingRankSum=0.000;DB;DP=215;ExcessHet=3.0103;FS=1.110;MLEAC=1;MLEAF=0.500;MQ=59.97;MQRankSum=0.000;QD=11.95;ReadPosRankSum=-0.045;SOR=0.809	GT:AD:DP:GQ:PL	0/1:116,92:208:99:2494,0,3673&#xD;&#xA;    4	1803671	.	C	A	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-0.880;ClippingRankSum=0.000;DP=450;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.953;SOR=0.572	GT:AD:DP:GQ:PL	0/0:445,2:447:99:0,1272,15958&#xD;&#xA;    4	1803681	.	T	C	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-1.654;ClippingRankSum=0.000;DP=483;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.422;SOR=0.664	GT:AD:DP:GQ:PL	0/0:479,2:481:99:0,1408,18538&#xD;&#xA;    4	1803703	.	A	G	0	.	AC=0;AF=0.00;AN=2;BaseQRankSum=-1.704;ClippingRankSum=0.000;DP=458;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=0.299;SOR=0.497	GT:AD:DP:GQ:PL	0/0:454,2:456:99:0,1325,18095&#xD;&#xA;    4	1803704	rs2234909	T	C	6676.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=-2.605;ClippingRankSum=0.000;DB;DP=456;ExcessHet=3.0103;FS=1.753;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=14.71;ReadPosRankSum=0.324;SOR=0.849	GT:AD:DP:GQ:PL	0/1:220,234:454:99:6684,0,6366&#xD;&#xA;    4	1803824	rs2305184	C	G	2030.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=8.083;ClippingRankSum=0.000;DB;DP=124;ExcessHet=3.0103;FS=6.128;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=16.51;ReadPosRankSum=0.180;SOR=0.096	GT:AD:DP:GQ:PL	0/1:62,61:123:99:2038,0,1766&#xD;&#xA;    4	1805296	rs3135883	G	A	3876.03	.	AC=2;AF=1.00;AN=2;DB;DP=110;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.22;SOR=9.401	GT:AD:DP:GQ:PL	1/1:0,109:109:99:3890,326,0&#xD;&#xA;&#xD;&#xA;[A dropbox link for file](https://www.dropbox.com/s/gm4pftuk9i1gx6p/test.vcf?dl=1)&#xD;&#xA;&#xD;&#xA;I'd initially considered using GATK's SelectVariants but I'm not sure JEXL has the ability to select out what I want specifically other than a blanket AD &gt; 10 which will give me both ref and alt alleles with AD &gt; 10.  Perhaps there is a bioawk solution or something more elaborate with coreutils which could successfully return sites with an alt AD count &gt; 10? " />
  <row Id="3162" PostHistoryTypeId="5" PostId="975" RevisionGUID="d8fb15cb-b735-46b2-b94f-3343f9294c23" CreationDate="2017-07-05T16:24:32.853" UserId="71" Comment="added 62 characters in body" Text="using [vcfilterjs][1]&#xD;&#xA;&#xD;&#xA;and the following script:&#xD;&#xA;&#xD;&#xA;    function accept(vc)&#xD;&#xA;    	{&#xD;&#xA;    	var i,j;&#xD;&#xA;    	for(i=0;i&lt; vc.getNSamples();++i)&#xD;&#xA;    		{&#xD;&#xA;    		var genotype = vc.getGenotype(i);&#xD;&#xA;    		if(!genotype.hasAD()) continue;&#xD;&#xA;    		var ad = genotype.getAD();&#xD;&#xA;    		/* loop over AD starting from '1' =  first ALT */&#xD;&#xA;    		for(j=1  ;j&lt; ad.length ;++j)&#xD;&#xA;    			{&#xD;&#xA;    			if(ad[j]&gt;10) return true;&#xD;&#xA;    			}&#xD;&#xA;    		}&#xD;&#xA;    	return false;&#xD;&#xA;    	}&#xD;&#xA;    accept(variant);&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;usage:&#xD;&#xA;&#xD;&#xA;    java -jar dist/vcffilterjs.jar -f script.js Test.vcf &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://lindenb.github.io/jvarkit/VCFFilterJS.html" />
  <row Id="3163" PostHistoryTypeId="5" PostId="987" RevisionGUID="8fd8413f-be62-4572-8415-d61339931fa1" CreationDate="2017-07-05T16:37:16.037" UserId="492" Comment="Add some explanation on how to add extra alternate alleles" Text="Bcftools version 1.3.1 seems to work for me.  This command only looks at the first and second alternate alleles. Extra alleles need to be added to the expression manually:&#xD;&#xA;&#xD;&#xA;    bgzip Test.vcf&#xD;&#xA;    bcftools index Test.vcf.gz&#xD;&#xA;    bcftools filter -i 'AD[1] &gt; 10 | AD[2] &gt; 10' Test.vcf.gz &#xD;&#xA;&#xD;&#xA;Output (without header):&#xD;&#xA;&#xD;&#xA;    6* 3	187451609	rs1880101	A	G	39794	PASS	AC=2;AF=1;AN=2;BaseQRankSum=1.859;ClippingRankSum=0;DB;DP=995;ExcessHet=3.0103;FS=0;MLEAC=2;MLEAF=1;MQ=60;MQRankSum=0;QD=24.56;ReadPosRankSum=0.406;SOR=8.234	GT:AD:DP:GQ:PL	1/1:1,988:989:99:39808,2949,0&#xD;&#xA;    7* 4	1803279	.	T	G	0	PASS	AC=0;AF=0;AN=2;BaseQRankSum=-6.652;ClippingRankSum=0;DP=245;ExcessHet=3.0103;FS=89.753;MLEAC=0;MLEAF=0;MQ=59.97;MQRankSum=0;ReadPosRankSum=-2.523;SOR=6.357	GT:AD:DP:GQ:PL	0/0:211,23:234:99:0,364,6739&#xD;&#xA;    8* 4	1803307	rs2305183	T	C	2486.6	PASS	AC=1;AF=0.5;AN=2;BaseQRankSum=-5.049;ClippingRankSum=0;DB;DP=215;ExcessHet=3.0103;FS=1.11;MLEAC=1;MLEAF=0.5;MQ=59.97;MQRankSum=0;QD=11.95;ReadPosRankSum=-0.045;SOR=0.809	GT:AD:DP:GQ:PL	0/1:116,92:208:99:2494,0,3673&#xD;&#xA;    12* 4	1803704	rs2234909	T	C	6676.6	PASS	AC=1;AF=0.5;AN=2;BaseQRankSum=-2.605;ClippingRankSum=0;DB;DP=456;ExcessHet=3.0103;FS=1.753;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=14.71;ReadPosRankSum=0.324;SOR=0.849	GT:AD:DP:GQ:PL	0/1:220,234:454:99:6684,0,6366&#xD;&#xA;    13* 4	1803824	rs2305184	C	G	2030.6	PASS	AC=1;AF=0.5;AN=2;BaseQRankSum=8.083;ClippingRankSum=0;DB;DP=124;ExcessHet=3.0103;FS=6.128;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=16.51;ReadPosRankSum=0.18;SOR=0.096	GT:AD:DP:GQ:PL	0/1:62,61:123:99:2038,0,1766&#xD;&#xA;    14* 4	1805296	rs3135883	G	A	3876.03	PASS	AC=2;AF=1;AN=2;DB;DP=110;ExcessHet=3.0103;FS=0;MLEAC=2;MLEAF=1;MQ=60;QD=29.22;SOR=9.401	GT:AD:DP:GQ:PL	1/1:0,109:109:99:3890,326,0&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3164" PostHistoryTypeId="5" PostId="997" RevisionGUID="018f0471-7a13-4ddd-96d2-3143c8dc11c4" CreationDate="2017-07-05T17:01:42.420" UserId="191" Comment="clarified the problem" Text="Sometimes it useful to perform a nucleotide protein coding gene sequence alignment based on codons, not on individual nucleotides. For example for further codon model analysis it is important to have full codons.&#xD;&#xA;&#xD;&#xA;A widely used approach here is to perform a protein sequence alignment first and then impose this alignment to the nucleotide sequences using [PAL2NAL][1], [CodonAlign][2] or something similar.&#xD;&#xA;&#xD;&#xA;This is how [transAlign][3] or [GUIDANCE][4] (in codon mode) work.&#xD;&#xA;&#xD;&#xA;The problem here is that you are discarding part of the information which could be potentially used for the sequence alignment. E.g. if you have slowly evolving low-complexity region adjacent to a quickly evolving one, the amino acid induced alignment could be wrong, while incorporating nucleotide sequence potentially allows to make the alignment more accurate.&#xD;&#xA;&#xD;&#xA;I'm aware of two programs which can do true codon alignment. First, [PRANK][5] has a dedicated codon model, but it is rather slow and using it is overkill for certain problems. Second, [Sequence Manipulation Suite][6] can perform codon alignments, but only for a pair of sequences; also it's javascript based, therefore it is hard to run it for a large number of sequences.&#xD;&#xA;&#xD;&#xA;Can you recommend any software for multiple codon sequence alignment? Preferably available for offline use.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.bork.embl.de/pal2nal/&#xD;&#xA;  [2]: http://wbiomed.curtin.edu.au/bioinf/CodonAlign.php&#xD;&#xA;  [3]: https://dx.doi.org/10.1186/1471-2105-6-156&#xD;&#xA;  [4]: http://guidance.tau.ac.il/ver2/&#xD;&#xA;  [5]: http://wasabiapp.org/software/prank/&#xD;&#xA;  [6]: http://www.bioinformatics.org/sms2/pairwise_align_codons.html" />
  <row Id="3165" PostHistoryTypeId="4" PostId="979" RevisionGUID="98413a68-2204-4f85-a79b-5190c087de02" CreationDate="2017-07-05T17:52:29.090" UserId="48" Comment="Improve title" Text="How to append numbers only on duplicates sequence names?" />
  <row Id="3166" PostHistoryTypeId="24" PostId="979" RevisionGUID="98413a68-2204-4f85-a79b-5190c087de02" CreationDate="2017-07-05T17:52:29.090" Comment="Proposed by 48 approved by 37 edit id of 231" />
  <row Id="3167" PostHistoryTypeId="5" PostId="996" RevisionGUID="8b56d876-4e34-4111-b587-b5c35f57ee06" CreationDate="2017-07-05T20:13:16.533" UserId="982" Comment="added 337 characters in body" Text="Extract desired gene sequences using [standalone blast][1] &#xD;&#xA;&#xD;&#xA;Simply provide a reference database with your desired output. &#xD;&#xA;Set up your command and away you go. You can set the search up with a for loop for a batch of sequences. Command may look like &#xD;&#xA;&#xD;&#xA;    for f in *.fasta; do&#xD;&#xA;       f=$(basename $f .fasta)&#xD;&#xA;       blastn \&#xD;&#xA;       -outfmt &quot;6 sseqid qseq %&quot; \&#xD;&#xA;       -query $f.fasta \&#xD;&#xA;       -subject reference.fna \&#xD;&#xA;       &gt; out/$f.fas&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;Watch the output as blast will spit out the gene as detected in + or - sense. If you want to only gather positive sense use `-strand`option. &#xD;&#xA;The default output I have here is tab output which requires a few sed commands to make into fasta.&#xD;&#xA;&#xD;&#xA;    sed -i \&#xD;&#xA;    -e 's/\s*$//g' \&#xD;&#xA;    -e 's/^/&gt;/g'  \&#xD;&#xA;    -e 's/\s\+/\n/g' \&#xD;&#xA;    *.fas &#xD;&#xA;&#xD;&#xA;Online alignment servers are an easy way to align small datasets e.g. [EBI][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi&#xD;&#xA;  [2]: http://www.ebi.ac.uk/Tools/msa/" />
  <row Id="3168" PostHistoryTypeId="2" PostId="1000" RevisionGUID="c112fc63-df7a-4f91-a8e9-4995cdb54d9c" CreationDate="2017-07-05T20:28:07.993" UserId="929" Text="I've used PropKa in the past to get isoelectric points. Pretty simple to use:&#xD;&#xA;&#xD;&#xA;https://github.com/jensengroup/propka-3.1" />
  <row Id="3169" PostHistoryTypeId="5" PostId="995" RevisionGUID="2ecff3dc-82b7-42f3-9cd1-54a47db93dd9" CreationDate="2017-07-05T20:28:38.417" UserId="581" Comment="added example code" Text="There are a lot of ways to do this. I suggest using Prokka/Roary to produce a core genome alignment. There's a useful tutorial on the [Roary](https://sanger-pathogens.github.io/Roary/) website:&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;for file in `ls`&#xD;&#xA;do&#xD;&#xA;    prokka --kingdom Bacteria --outdir ${file%%.*}  --genus Listeria --locustag ${file%%.*} $file&#xD;&#xA;    mv ${file%%.*}/PROKKA_07052017.gff GFF/${file%%.*}.gff # use current date&#xD;&#xA;done&#xD;&#xA;roary -f Alignment -e -n -v GFF/*.gff&#xD;&#xA;```" />
  <row Id="3170" PostHistoryTypeId="5" PostId="995" RevisionGUID="7e1c63d5-9b60-47e3-bf78-f678c2d6ead6" CreationDate="2017-07-05T20:33:40.160" UserId="581" Comment="added example codeforgot to mention the output fine" Text="There are a lot of ways to do this. I suggest using Prokka/Roary to produce a core genome alignment. There's a useful tutorial on the [Roary](https://sanger-pathogens.github.io/Roary/) website:&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;for file in `ls`&#xD;&#xA;do&#xD;&#xA;    prokka --kingdom Bacteria --outdir ${file%%.*}  --genus Listeria --locustag ${file%%.*} $file&#xD;&#xA;    mv ${file%%.*}/PROKKA_07052017.gff GFF/${file%%.*}.gff # use current date&#xD;&#xA;done&#xD;&#xA;roary -f Alignment -e -n -v GFF/*.gff&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;Alignment/core_gene_alignment.aln can be used as input for phylogenetic analyses" />
  <row Id="3171" PostHistoryTypeId="2" PostId="1001" RevisionGUID="b5501a1f-dc22-4064-a950-7fd3f76db593" CreationDate="2017-07-05T20:36:23.447" UserId="982" Text="I have a core genome dataset of approx 2530 genes for 149 taxa. I have run an unpartitioned phylogenetic analysis using iqtree. But I am unhappy with the resolution in some of the phylogeny. I want to check if partitioning the alignment  will improve resolution. My alignment is from roary output and MAFFT alignment.&#xD;&#xA;&#xD;&#xA;Ive tried partitioning by loci and using iqtree to determine a model of best fit for each but this is apparently a very long demanding process. [any info on your experience I would be grateful]&#xD;&#xA;&#xD;&#xA;Im also looking into using PartitionFinder but its not installing correctly on my system just yet. &#xD;&#xA;&#xD;&#xA;So while I wait for one of these to work I was wondering how others go about task? Is it worth the hassle for resolution? &#xD;&#xA;&#xD;&#xA;" />
  <row Id="3172" PostHistoryTypeId="1" PostId="1001" RevisionGUID="b5501a1f-dc22-4064-a950-7fd3f76db593" CreationDate="2017-07-05T20:36:23.447" UserId="982" Text="How to effectively partition core genome data for phylogenetic analysis" />
  <row Id="3173" PostHistoryTypeId="3" PostId="1001" RevisionGUID="b5501a1f-dc22-4064-a950-7fd3f76db593" CreationDate="2017-07-05T20:36:23.447" UserId="982" Text="&lt;phylogeny&gt;&lt;software-recommendation&gt;&lt;phylogenetics&gt;" />
  <row Id="3174" PostHistoryTypeId="5" PostId="995" RevisionGUID="2bf84fde-48bc-4b4c-a33c-aeb9fb29c981" CreationDate="2017-07-05T20:38:57.280" UserId="298" Comment="never do for file in ls. " Text="There are a lot of ways to do this. I suggest using Prokka/Roary to produce a core genome alignment. There's a useful tutorial on the [Roary](https://sanger-pathogens.github.io/Roary/) website:&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;for file in *&#xA;do&#xD;&#xA;    prokka --kingdom Bacteria --outdir ${file%%.*}  --genus Listeria --locustag ${file%%.*} $file&#xD;&#xA;    mv ${file%%.*}/PROKKA_07052017.gff GFF/${file%%.*}.gff # use current date&#xD;&#xA;done&#xD;&#xA;roary -f Alignment -e -n -v GFF/*.gff&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;Alignment/core_gene_alignment.aln can be used as input for phylogenetic analyses" />
  <row Id="3175" PostHistoryTypeId="5" PostId="995" RevisionGUID="b2589478-6062-4c13-9e2a-a5465df69ec2" CreationDate="2017-07-05T20:39:21.993" UserId="298" Comment="Made safe for weird file names" Text="There are a lot of ways to do this. I suggest using Prokka/Roary to produce a core genome alignment. There's a useful tutorial on the [Roary](https://sanger-pathogens.github.io/Roary/) website:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    for file in *&#xD;&#xA;    do&#xD;&#xA;        prokka --kingdom Bacteria --outdir &quot;${file%%.*}&quot;  --genus Listeria --locustag &quot;${file%%.*}&quot; &quot;$file&quot;&#xD;&#xA;        mv &quot;${file%%.*}&quot;/PROKKA_07052017.gff GFF/&quot;${file%%.*}&quot;.gff # use current date&#xD;&#xA;    done&#xD;&#xA;    roary -f Alignment -e -n -v GFF/*.gff&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Alignment/core_gene_alignment.aln can be used as input for phylogenetic analyses" />
  <row Id="3176" PostHistoryTypeId="2" PostId="1002" RevisionGUID="d02d6300-0c4f-4ff5-aeed-a99313a7a94b" CreationDate="2017-07-05T22:18:06.357" UserId="926" Text="I was wondering if there is a tutorial or a small code snippet to understand how to write bioinformatics pipeline using python, for example&#xD;&#xA;&#xD;&#xA;1. use a aligner (say hisat) &#xD;&#xA;2. get the output and process it using samtools &#xD;&#xA;&#xD;&#xA;I was able to use subprocess from python2.7 for this purpose using samtools but i am not able to link both the processes.i.e given path(which I can use argparse) for directory with fastq files output would be processed bam. &#xD;&#xA;&#xD;&#xA;sample code for samtools sam to bam :&#xD;&#xA;&#xD;&#xA;    import subprocess&#xD;&#xA;    subprocess.run(['samtools','view', '-bS',&#xD;&#xA;                    '../some.sam',&#xD;&#xA;                    '&gt;',&#xD;&#xA;                    '../some.bam'])&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3177" PostHistoryTypeId="1" PostId="1002" RevisionGUID="d02d6300-0c4f-4ff5-aeed-a99313a7a94b" CreationDate="2017-07-05T22:18:06.357" UserId="926" Text="using python to write bioinformatics pipelines tutorial" />
  <row Id="3178" PostHistoryTypeId="3" PostId="1002" RevisionGUID="d02d6300-0c4f-4ff5-aeed-a99313a7a94b" CreationDate="2017-07-05T22:18:06.357" UserId="926" Text="&lt;rna-seq&gt;&lt;samtools&gt;&lt;genomics&gt;" />
  <row Id="3179" PostHistoryTypeId="2" PostId="1003" RevisionGUID="5b2e1129-36d8-422f-b69f-50f82941bf45" CreationDate="2017-07-05T22:30:23.793" UserId="964" Text="BioPython has some good tools for processing reads and alignments.&#xD;&#xA;http://biopython.org/DIST/docs/tutorial/Tutorial.html&#xD;&#xA;&#xD;&#xA;There is a python library wrapping samtools so many of the samtools calls can be used directly as python objects and calls&#xD;&#xA;https://pysam.readthedocs.io/en/latest/&#xD;&#xA;&#xD;&#xA;I would use subprocess to call the aligner and specify the output to a bam file that you have named and then read that bam file with pysam to do the analysis that you are interested in.  &#xD;&#xA;&#xD;&#xA;If you provide more specifics about the analysis or aligners I can help you more but you are in the right track just need to connect the two calls together.&#xD;&#xA;&#xD;&#xA;example:&#xD;&#xA;&#xD;&#xA;    import subprocess&#xD;&#xA;    import pysam&#xD;&#xA;&#xD;&#xA;    subprocess.run([`hisat`, `some options`, `-o`, `some.bam`])&#xD;&#xA;&#xD;&#xA;    #get depth&#xD;&#xA;    depth = pysam.depth('some.bam')&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3180" PostHistoryTypeId="5" PostId="945" RevisionGUID="34be8057-75e0-4028-b2e6-7da7d4b7bed3" CreationDate="2017-07-05T23:16:42.950" UserId="73" Comment="Added pictures " Text="I'm working on a human genome project, trying to find variants that are unique to one person, but not to three other relatives of that person. Unfortunately, one of our regions of interest contains a long tandem repeat region (***30kb***, repeated 3 times in tandem). I'd like to be able to properly map these regions with WGS reads, but suspect that I'm coming up against a mapping preference problem with my current methods (Bowtie2 / HISAT2):&#xD;&#xA;&#xD;&#xA;1. Read A1 can potentially map to three different locations near the same locus in the genome (L1, L2, L3)&#xD;&#xA;2. L1 is most similar to A1, so is preferentially mapped. Because this similarity is greater than that of L2 or L3, the read is treated as a unique mapping&#xD;&#xA;3. Most other reads are similarly mapped to L1, possibly because it is the locus that has had the most correction applied to it in the reference genome&#xD;&#xA;4. An analysis of the mapping results indicates huge deletions at L2 and L3&#xD;&#xA;&#xD;&#xA;Is BWA any more resistant to this issue? If so, how can I get it to report randomly one of the three mappable loci (assuming they all appear in the results)?&#xD;&#xA;&#xD;&#xA;Are there any other ways I can deal with this issue of distributing mapped reads throughout tandem repeats?&#xD;&#xA;&#xD;&#xA;Update: here's what this region looks like in the 1000 genomes GBR population, using the low-coverage CRAM files:&#xD;&#xA;&#xD;&#xA;[![Common deletion in GBR on Chr20][1]][1]&#xD;&#xA;&#xD;&#xA;And here is what the same region looks like when I do a self-mapping with LAST. The long repetitive chunks are very obvious:&#xD;&#xA;&#xD;&#xA;[![Self-mapping][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/WA4Fk.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/UTN78.png" />
  <row Id="3181" PostHistoryTypeId="2" PostId="1004" RevisionGUID="4f9c0ce7-0e22-4c1e-bb01-d61711dae456" CreationDate="2017-07-06T00:25:58.437" UserId="73" Text="If you're just doing alignment and conversion to a sorted BAM file, there's no need to run it through python. A simple pipe on the Linux command line works just as well (and probably runs faster):&#xD;&#xA;&#xD;&#xA;    hisat2 -x genome.index -1 reads_R1.fq.gz -2 reads_R2.fq.gz | &#xD;&#xA;      samtools sort &gt; reads_vs_genome.bam" />
  <row Id="3182" PostHistoryTypeId="5" PostId="1003" RevisionGUID="73cd36fa-190b-4b2b-a7d2-4d519be9c515" CreationDate="2017-07-06T02:35:12.113" UserId="964" Comment="added 148 characters in body" Text="BioPython has some good tools for processing reads and alignments.&#xD;&#xA;http://biopython.org/DIST/docs/tutorial/Tutorial.html&#xD;&#xA;&#xD;&#xA;There is a python library wrapping samtools so many of the samtools calls can be used directly as python objects and calls&#xD;&#xA;https://pysam.readthedocs.io/en/latest/&#xD;&#xA;&#xD;&#xA;I would use subprocess to call the aligner and specify the output to a bam file that you have named and then read that bam file with pysam to do the analysis that you are interested in.  &#xD;&#xA;&#xD;&#xA;If you provide more specifics about the analysis or aligners I can help you more but you are in the right track just need to connect the two calls together.&#xD;&#xA;&#xD;&#xA;example:&#xD;&#xA;&#xD;&#xA;    import subprocess&#xD;&#xA;    import pysam&#xD;&#xA;    import os &#xD;&#xA;&#xD;&#xA;    for root, dirs, files in os.walk(rootPath):&#xD;&#xA;        for filename in fnmatch.filter(files, &quot;.fastq&quot;):&#xD;&#xA;&#xD;&#xA;            subprocess.run([`hisat`, `-f`, filename `-o`, `some.bam`])&#xD;&#xA;&#xD;&#xA;            #get depth&#xD;&#xA;            depth = pysam.depth('some.bam')&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3183" PostHistoryTypeId="5" PostId="1004" RevisionGUID="3a1c5342-72dd-4b58-8d0f-685a36070fbb" CreationDate="2017-07-06T02:51:18.897" UserId="73" Comment="deleted 1 character in body" Text="If you're just doing alignment and conversion to a sorted BAM file, there's no need to run it through python. A simple pipe on the unix command line works just as well (and probably runs faster):&#xD;&#xA;&#xD;&#xA;    hisat2 -x genome.index -1 reads_R1.fq.gz -2 reads_R2.fq.gz | &#xD;&#xA;      samtools sort &gt; reads_vs_genome.bam" />
  <row Id="3184" PostHistoryTypeId="2" PostId="1005" RevisionGUID="202a2934-9e3c-4c73-9ff8-5e25515e92ab" CreationDate="2017-07-06T07:35:34.347" UserId="294" Text="Answer from the authors: &#xD;&#xA;&#xD;&#xA;&gt; PASA ends up using gff3 format instead of sam format for uploading the&#xD;&#xA;&gt; alignments.  If somebody has the gmap gff3 output, then can be done to&#xD;&#xA;&gt; just upload that directly using the custom alignment importer, but not&#xD;&#xA;&gt; with sam. They suggest just having PASA rerun GMAP as part of its&#xD;&#xA;&gt; regular routine.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3185" PostHistoryTypeId="2" PostId="1006" RevisionGUID="100ca4bb-276a-47cc-a595-3b079a50d67c" CreationDate="2017-07-06T09:00:02.707" UserId="931" Text="Have a look at [luigi][1] for writing pipelines in python. You can found BioLuigi if you like :-P&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/spotify/luigi" />
  <row Id="3186" PostHistoryTypeId="2" PostId="1007" RevisionGUID="b614bb7d-973c-4b5d-8442-6f5a8ba88654" CreationDate="2017-07-06T09:03:18.220" UserId="931" Text="I have a list of about 50k 'Protein IDs' from Reactome. Is there a simple way to get all the corresponding 'Pathway IDs' for each protein? What is the best service to use? (I'm guessing I can use the Reactome API, but I don't necessarily want to hit that 50k times...)." />
  <row Id="3187" PostHistoryTypeId="1" PostId="1007" RevisionGUID="b614bb7d-973c-4b5d-8442-6f5a8ba88654" CreationDate="2017-07-06T09:03:18.220" UserId="931" Text="Convert Reactome Protein IDs to Pathway IDs?" />
  <row Id="3188" PostHistoryTypeId="3" PostId="1007" RevisionGUID="b614bb7d-973c-4b5d-8442-6f5a8ba88654" CreationDate="2017-07-06T09:03:18.220" UserId="931" Text="&lt;identifiers&gt;&lt;reactome&gt;" />
  <row Id="3189" PostHistoryTypeId="2" PostId="1008" RevisionGUID="90bee365-2fa9-4938-8210-234c527d07db" CreationDate="2017-07-06T09:12:57.450" UserId="377" Text="Taking a different tack from other answers, there's lots of tools for pipelines in Python. Note: there was a time when people would use &quot;pipeline&quot; to refer to a shell script. I'm talking about something more sophisticated that helps you decompose an analysis into parts and  runs it robustly.&#xD;&#xA;&#xD;&#xA; - **Snakemake** is my favourite. It's (nearly) pure Python and can generate reports.&#xD;&#xA; - **Nextflow** is growing in popularity and is pretty straightforward&#xD;&#xA; - **Ruffus** used to be reasonably popular, seemed fine when I used it&#xD;&#xA; - **Bpipe** is for bioinformatics&#xD;&#xA; - **Airflow** is a more industrial &quot;big&quot; solution but in wide use&#xD;&#xA; - See this [big list of pipeline systems][1]&#xD;&#xA;&#xD;&#xA;I'm sure you can find something there.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/pditommaso/awesome-pipeline" />
  <row Id="3190" PostHistoryTypeId="2" PostId="1009" RevisionGUID="d0679863-8c7e-467e-a3b8-4d7bd3e2fdb5" CreationDate="2017-07-06T09:53:12.087" UserId="191" Text="I personally didn't like using raw `subprocess` library for this. [`plumbum`][1] has a very nice syntax for this kind of problems.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    from plumbum import local&#xD;&#xA;&#xD;&#xA;    # load a command&#xD;&#xA;    samtools = local['samtools']&#xD;&#xA;    &#xD;&#xA;    # you can then easily redirect&#xD;&#xA;    (samtools['view', '-bS', '../some.sam'] &gt; '../some.bam')()&#xD;&#xA;    &#xD;&#xA;    # it is also easy to pipe &#xD;&#xA;    hisat2 = local['hisat2']&#xD;&#xA;    chain = hisat2['-x', 'genome.index', '-1', 'reads_R1.fq.gz', '-2', 'reads_R2.fq.gz'] | samtools['sort'] &gt; 'reads_vs_genome.bam'&#xD;&#xA;    &#xD;&#xA;    chain()&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://plumbum.readthedocs.io/en/latest/" />
  <row Id="3191" PostHistoryTypeId="5" PostId="1009" RevisionGUID="5d72326c-2597-42d5-8bb7-3cc85fa5e704" CreationDate="2017-07-06T11:16:07.240" UserId="191" Comment="agree with another answer" Text="I [agree][1] that using a specialized tool is probably a good idea.&#xD;&#xA;&#xD;&#xA;Nevertheless if you want to stick with Python, I suggest using [`plumbum`][2] instead of `subprocess`. It has a very nice syntax for this kind of problems.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-python --&gt;&#xD;&#xA;&#xD;&#xA;    from plumbum import local&#xD;&#xA;&#xD;&#xA;    # load a command&#xD;&#xA;    samtools = local['samtools']&#xD;&#xA;    &#xD;&#xA;    # you can then easily redirect&#xD;&#xA;    (samtools['view', '-bS', '../some.sam'] &gt; '../some.bam')()&#xD;&#xA;    &#xD;&#xA;    # it is also easy to pipe &#xD;&#xA;    hisat2 = local['hisat2']&#xD;&#xA;    chain = hisat2['-x', 'genome.index', '-1', 'reads_R1.fq.gz', '-2', 'reads_R2.fq.gz'] | samtools['sort'] &gt; 'reads_vs_genome.bam'&#xD;&#xA;    &#xD;&#xA;    chain()&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/1008/191&#xD;&#xA;  [2]: https://plumbum.readthedocs.io/en/latest/" />
  <row Id="3192" PostHistoryTypeId="6" PostId="1002" RevisionGUID="0139a6b6-91bc-46ab-b95e-4eaee4f76a38" CreationDate="2017-07-06T11:22:58.393" UserId="931" Comment="Added a tag." Text="&lt;rna-seq&gt;&lt;samtools&gt;&lt;python&gt;&lt;genomics&gt;" />
  <row Id="3193" PostHistoryTypeId="24" PostId="1002" RevisionGUID="0139a6b6-91bc-46ab-b95e-4eaee4f76a38" CreationDate="2017-07-06T11:22:58.393" Comment="Proposed by 931 approved by 191, 57 edit id of 233" />
  <row Id="3195" PostHistoryTypeId="5" PostId="1007" RevisionGUID="5760919c-abcb-41c5-b20b-7dc223fa35c2" CreationDate="2017-07-07T09:00:10.593" UserId="931" Comment="Added example IDs and expected output" Text="I have a list of about 50k 'Protein IDs' from Reactome. Is there a simple way to get all the corresponding 'Pathway IDs' for each protein? What is the best service to use? (I'm guessing I can use the Reactome API, but I don't necessarily want to hit that 50k times...).&#xD;&#xA;&#xD;&#xA;Protein IDs from reactome look like this:&#xD;&#xA;&#xD;&#xA;    R-HSA-49155&#xD;&#xA;    R-HSA-199420&#xD;&#xA;&#xD;&#xA;The corresponding Reactome 'Pathway IDs' for those Protein IDs would be:&#xD;&#xA;&#xD;&#xA;    R-HSA-49155  R-HSA-110331&#xD;&#xA;    R-HSA-49155  R-HSA-110330&#xD;&#xA;    R-HSA-49155  R-HSA-110357&#xD;&#xA;    R-HSA-199420 R-HSA-1660499&#xD;&#xA;    R-HSA-199420 R-HSA-202424&#xD;&#xA;    R-HSA-199420 R-HSA-199418&#xD;&#xA;" />
  <row Id="3196" PostHistoryTypeId="6" PostId="1007" RevisionGUID="5760919c-abcb-41c5-b20b-7dc223fa35c2" CreationDate="2017-07-07T09:00:10.593" UserId="931" Comment="Added example IDs and expected output" Text="&lt;conversion&gt;&lt;identifiers&gt;&lt;reactome&gt;" />
  <row Id="3197" PostHistoryTypeId="2" PostId="1010" RevisionGUID="5dcd6a53-6c94-48db-85c2-a5376193f148" CreationDate="2017-07-07T10:02:31.870" UserId="1075" Text="I have this read in my BAM file. It maps on chromosome I.&#xD;&#xA;&#xD;&#xA;I open this BAM file in IGV, and I can see the alignment on chromosome I.&#xD;&#xA;&#xD;&#xA;But when I open this file in R with Rsamtools:&#xD;&#xA;&#xD;&#xA;    bamContigsCel &lt;- Rsamtools::scanBam('output/alignment/pacbio/bwa/ref     /bristolAssemblySorted.bam', param = Rsamtools::ScanBamParam(what = Rsamtools::scanBamWhat(), flag = Rsamtools::scanBamFlag(isMinusStrand = FALSE), tag = bamTags))[[1]]&#xD;&#xA;&#xD;&#xA;I then check if the read maps to chromosome I in my R object but I cannot find it.&#xD;&#xA;&#xD;&#xA;    bamContigsCel$rname[bamContigsCel$qname == '000000F|arrow']&#xD;&#xA;    [1] II II IV&#xD;&#xA;    Levels: I II III IV MtDNA V X&#xD;&#xA;&#xD;&#xA;But if I look at the BAM file, it's there. Why isn't Rsamtools importing my read into R?&#xD;&#xA;&#xD;&#xA;    samtools view bristolAssemblySorted.bam | grep -n '000000F' | head  -c80&#xD;&#xA;    000000F|arrow   2064    I   336331  7   2926310H260M1774267H    *   0 0   GAAGCTGTCTAAACTTTGGC&#xD;&#xA;&#xD;&#xA;Thanks. Best, C.&#xD;&#xA;" />
  <row Id="3198" PostHistoryTypeId="1" PostId="1010" RevisionGUID="5dcd6a53-6c94-48db-85c2-a5376193f148" CreationDate="2017-07-07T10:02:31.870" UserId="1075" Text="scanBam from Rsamtools is not importing one of my reads into R" />
  <row Id="3199" PostHistoryTypeId="3" PostId="1010" RevisionGUID="5dcd6a53-6c94-48db-85c2-a5376193f148" CreationDate="2017-07-07T10:02:31.870" UserId="1075" Text="&lt;r&gt;&lt;bam&gt;&lt;samtools&gt;" />
  <row Id="3200" PostHistoryTypeId="5" PostId="1010" RevisionGUID="f4944a1a-ee44-4b72-90e2-aeac558fd3bd" CreationDate="2017-07-07T10:10:36.673" UserId="298" Comment="Replaced I with 1 for clarity (too easy to confuse with the pronoun &quot;I&quot;); removed thanks and salutations: they are implied here and we try to keep questions streamlined; Added $ to differentiate between the command and its output" Text="I have this read in my BAM file. It maps on chromosome 1.&#xD;&#xA;&#xD;&#xA;I open this BAM file in IGV, and I can see the alignment on chromosome 1.&#xD;&#xA;&#xD;&#xA;But when I open this file in R with Rsamtools:&#xD;&#xA;&#xD;&#xA;    bamContigsCel &lt;- Rsamtools::scanBam('output/alignment/pacbio/bwa/ref     /bristolAssemblySorted.bam', param = Rsamtools::ScanBamParam(what = Rsamtools::scanBamWhat(), flag = Rsamtools::scanBamFlag(isMinusStrand = FALSE), tag = bamTags))[[1]]&#xD;&#xA;&#xD;&#xA;I then check if the read maps to chromosome 1 in my R object but I cannot find it.&#xD;&#xA;&#xD;&#xA;    bamContigsCel$rname[bamContigsCel$qname == '000000F|arrow']&#xD;&#xA;    [1] II II IV&#xD;&#xA;    Levels: I II III IV MtDNA V X&#xD;&#xA;&#xD;&#xA;But if I look at the BAM file, it's there. Why isn't Rsamtools importing my read into R?&#xD;&#xA;&#xD;&#xA;    $ samtools view bristolAssemblySorted.bam | grep -n '000000F' | head  -c80&#xD;&#xA;    000000F|arrow   2064    I   336331  7   2926310H260M1774267H    *   0 0   GAAGCTGTCTAAACTTTGGC&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="3201" PostHistoryTypeId="2" PostId="1011" RevisionGUID="1ec9945d-16b1-40b9-b605-bd458d2aac45" CreationDate="2017-07-07T10:35:44.040" UserId="77" Text="Note the flag; that read is mapped in a reverse-complemented manner, so `isMinusStrand = FALSE` is filtering it out. I tested this by making a BAM file with only that read:&#xD;&#xA;&#xD;&#xA;    @SQ	SN:I	LN:1000000&#xD;&#xA;    000000F|arrow	2064	I	336331	7	2926310H260M1774267H	*	00	*	*&#xD;&#xA;&#xD;&#xA;Then in R:&#xD;&#xA;&#xD;&#xA;    &gt; library(Rsamtools)&#xD;&#xA;    &gt; blah = scanBam(&quot;foo.bam&quot;, param=ScanBamParam(what=scanBamWhat(), flag=scanBamFlag(isMinusStrand = T)))[[1]]&#xD;&#xA;    &gt; blah$qname&#xD;&#xA;    [1] &quot;000000F|arrow&quot;&#xD;&#xA;    &gt; blah = scanBam(&quot;foo.bam&quot;, param=ScanBamParam(what=scanBamWhat(), flag=scanBamFlag(isMinusStrand = F)))[[1]]&#xD;&#xA;    &gt; blah$qname&#xD;&#xA;    character(0)" />
  <row Id="3202" PostHistoryTypeId="5" PostId="1010" RevisionGUID="015d22d9-9c66-4dd3-97ba-889a664109b5" CreationDate="2017-07-07T11:46:03.383" UserId="77" Comment="added 62 characters in body" Text="I have this read in my BAM file. It maps on chromosome 1.&#xD;&#xA;&#xD;&#xA;I open this BAM file in IGV, and I can see the alignment on chromosome 1.&#xD;&#xA;&#xD;&#xA;But when I open this file in R with Rsamtools:&#xD;&#xA;&#xD;&#xA;    bamContigsCel &lt;- Rsamtools::scanBam('output/alignment/pacbio/bwa/ref     /bristolAssemblySorted.bam', param = Rsamtools::ScanBamParam(what = Rsamtools::scanBamWhat(), flag = Rsamtools::scanBamFlag(isMinusStrand = FALSE), tag = bamTags))[[1]]&#xD;&#xA;&#xD;&#xA;I then check if the read maps to chromosome 1 in my R object but I cannot find it.&#xD;&#xA;&#xD;&#xA;    bamContigsCel$rname[bamContigsCel$qname == '000000F|arrow']&#xD;&#xA;    [1] II II IV&#xD;&#xA;    Levels: I II III IV MtDNA V X&#xD;&#xA;&#xD;&#xA;But if I look at the BAM file, it's there. Why isn't Rsamtools importing my read into R?&#xD;&#xA;&#xD;&#xA;    $ samtools view bristolAssemblySorted.bam | grep -n '000000F' | head  -c80&#xD;&#xA;    000000F|arrow   2064    I   336331  7   2926310H260M1774267H    *   0 0   GAAGCTGTCTAAACTTTGGC&#xD;&#xA;&#xD;&#xA;Cross-posted on [biostars](https://www.biostars.org/p/261365/)&#xD;&#xA;" />
  <row Id="3203" PostHistoryTypeId="2" PostId="1012" RevisionGUID="2e79a877-bed2-4b23-a02c-204ae86528eb" CreationDate="2017-07-07T12:43:54.240" UserId="446" Text="I am dealing with a list of genes which have been selected from a gene enrichment analysis. In order to see what kind of genes are overrepresented, I ran [eggnog-mapper] (https://github.com/jhcepas/eggnog-mapper)! to do an orthology assignment of each gene against a bacterial database (provided in eggnog). &#xD;&#xA;&#xD;&#xA;After running eggnog you get a .csv file containing for each gene which is the closest ortholog, and a list of GO, KEGG and COG terms associated. &#xD;&#xA;&#xD;&#xA;From there, I would like to do a Fisher-test (also considering Bonferroni correction) to highlight COG categories overrepresented in our dataset compared to our reference genome. &#xD;&#xA;&#xD;&#xA;Finally my question: How do you deal with genes assigned to multiple COG categories (same would apply for GO terms). You count +1 to each of the COG categories assigned? Do you just choose one category per gene? &#xD;&#xA;&#xD;&#xA;Thanks in advance!&#xD;&#xA;&#xD;&#xA;Sergio." />
  <row Id="3204" PostHistoryTypeId="1" PostId="1012" RevisionGUID="2e79a877-bed2-4b23-a02c-204ae86528eb" CreationDate="2017-07-07T12:43:54.240" UserId="446" Text="COG Annotation - Dealing with genes assigned to two or more COG categories" />
  <row Id="3205" PostHistoryTypeId="3" PostId="1012" RevisionGUID="2e79a877-bed2-4b23-a02c-204ae86528eb" CreationDate="2017-07-07T12:43:54.240" UserId="446" Text="&lt;sequence-annotation&gt;" />
  <row Id="3206" PostHistoryTypeId="5" PostId="1012" RevisionGUID="1618225e-fea7-42bd-b1d9-ea03d204ba7d" CreationDate="2017-07-07T12:45:38.550" UserId="298" Comment="Fixed link formatting; removed thanks and salutations (they are implied here and we like to keep the questions streamlined)" Text="I am dealing with a list of genes which have been selected from a gene enrichment analysis. In order to see what kind of genes are overrepresented, I ran [eggnog-mapper](https://github.com/jhcepas/eggnog-mapper) to do an orthology assignment of each gene against a bacterial database (provided in eggnog). &#xD;&#xA;&#xD;&#xA;After running eggnog you get a .csv file containing for each gene which is the closest ortholog, and a list of GO, KEGG and COG terms associated. &#xD;&#xA;&#xD;&#xA;From there, I would like to do a Fisher-test (also considering Bonferroni correction) to highlight COG categories overrepresented in our dataset compared to our reference genome. &#xD;&#xA;&#xD;&#xA;Finally my question: How do you deal with genes assigned to multiple COG categories (same would apply for GO terms). You count +1 to each of the COG categories assigned? Do you just choose one category per gene? &#xD;&#xA;&#xD;&#xA;" />
  <row Id="3207" PostHistoryTypeId="2" PostId="1013" RevisionGUID="76a2bbab-4d9b-4252-aa09-970273a666a9" CreationDate="2017-07-07T12:48:31.647" UserId="298" Text="You add 1 to the count of each COG category. You are looking for over represented *COG categories* so you must count them all. Many genes will be assigned to multiple categories. In fact, a majority of human genes are given multiple GO cellular function annotations. I have worked considerably in this field and one of the surprises I found is how common protein multifunctionality is. But I digress. &#xD;&#xA;&#xD;&#xA;The point here is that you want to see whether any COG categories are over represented in your test set. Whether a gene is annotated to one or to a hundred categories is irrelevant. You are counting genes per COG, so you need to count each of the COGs assigned to a gene separately. " />
  <row Id="3208" PostHistoryTypeId="2" PostId="1014" RevisionGUID="99513f9f-61b0-432c-82ad-758b1d80a176" CreationDate="2017-07-07T12:52:14.280" UserId="1076" Text="I'm working with scATAC-Seq data on the K562 cell line, which is supposed to be derived from a female patient. [While following the scATAC-seq data analysis pipeline](https://www.nature.com/article-assets/npg/nature/journal/v523/n7561/extref/nature14590-s1.pdf), after performing bowtie alignment they recommend filtering out all reads aligned to mitochondrial DNA and on the Y chromosome.&#xD;&#xA;&#xD;&#xA;Out of curiosity, I decided to count how many reads aligned to chrY and found that it can be quite high - chrY has approximately 10% as many reads as a similarly-sized chromosome (chr19 in hg19).&#xD;&#xA;&#xD;&#xA;Is it normal to find reads mapping to chrY when the cell line is female-derived, or did I mess up somehow?" />
  <row Id="3209" PostHistoryTypeId="1" PostId="1014" RevisionGUID="99513f9f-61b0-432c-82ad-758b1d80a176" CreationDate="2017-07-07T12:52:14.280" UserId="1076" Text="Y Chromosome Aligned Reads in scATAC-seq data from a female-derived cell line?" />
  <row Id="3210" PostHistoryTypeId="3" PostId="1014" RevisionGUID="99513f9f-61b0-432c-82ad-758b1d80a176" CreationDate="2017-07-07T12:52:14.280" UserId="1076" Text="&lt;alignment&gt;&lt;bowtie2&gt;" />
  <row Id="3211" PostHistoryTypeId="2" PostId="1015" RevisionGUID="6b9f1c96-ca06-4619-a6f9-e65b43d6559a" CreationDate="2017-07-07T12:58:00.847" UserId="292" Text="There are homologous regions between X an Y chromosomes: https://en.wikipedia.org/wiki/Pseudoautosomal_region&#xD;&#xA;&#xD;&#xA;It is therefore normal to have some female-derived reads mapping in Y chromosome.&#xD;&#xA;&#xD;&#xA;You should probably check what proportion of such reads fall in other parts of the Y chromosome than pseudoautosomal regions." />
  <row Id="3212" PostHistoryTypeId="2" PostId="1016" RevisionGUID="49a6fe37-30fd-4a85-9a12-e4ee9b828b13" CreationDate="2017-07-07T19:10:32.453" UserId="704" Text="Does anyone know if there is a program/library/script in R or python that you give a list of protein peptides and a list of PTMs you want, like oxidation of methionine and acetylation of cysteine and as result to return you a matrix/list with the imported peptides and all possible locations of the selected modifications?&#xD;&#xA;&#xD;&#xA;Thank you." />
  <row Id="3213" PostHistoryTypeId="1" PostId="1016" RevisionGUID="49a6fe37-30fd-4a85-9a12-e4ee9b828b13" CreationDate="2017-07-07T19:10:32.453" UserId="704" Text="Software/library to produce a table with all possible PTM of peptide list" />
  <row Id="3214" PostHistoryTypeId="3" PostId="1016" RevisionGUID="49a6fe37-30fd-4a85-9a12-e4ee9b828b13" CreationDate="2017-07-07T19:10:32.453" UserId="704" Text="&lt;r&gt;&lt;proteins&gt;" />
  <row Id="3216" PostHistoryTypeId="5" PostId="1001" RevisionGUID="2f35908a-7c04-4e50-a598-eb10f2fa593e" CreationDate="2017-07-08T07:25:02.290" UserId="982" Comment="deleted 4 characters in body; edited title" Text="I have a core genome dataset of approx 2530 genes for 149 taxa. I have run an unpartitioned phylogenetic analysis using iqtree. But I am unhappy with the resolution in some of the phylogeny. I want to check if partitioning the alignment  will improve resolution. My alignment is from roary output and MAFFT alignment.&#xD;&#xA;&#xD;&#xA;Ive tried partitioning by loci and using iqtree (beta 1.6.4) to determine a model of best fit for each but this is apparently a very long demanding process. [any info on your experience I would be grateful]&#xD;&#xA;&#xD;&#xA;My next course of action is to remove uninformative sites. Id rather that this didnt chop up individual genes. Is it possible to observe the identity in each partition and remove genes which are 100% identical?&#xD;&#xA;" />
  <row Id="3217" PostHistoryTypeId="4" PostId="1001" RevisionGUID="2f35908a-7c04-4e50-a598-eb10f2fa593e" CreationDate="2017-07-08T07:25:02.290" UserId="982" Comment="deleted 4 characters in body; edited title" Text="How to optimise core genome data for phylogenetic analysis [partition vs remove uninformative sites]" />
  <row Id="3218" PostHistoryTypeId="5" PostId="1016" RevisionGUID="afe77bd5-20f4-450a-aa59-80e90fbf01ff" CreationDate="2017-07-08T11:43:12.400" UserId="96" Comment="clarify the language" Text="Does anyone know if there is a program/library/script in R or Python that takes as input a list of proteins/peptides and a list of post-translational modifications (PTMs; like oxidation of methionine and acetylation of cysteine), and as output returns a matrix/list with the imported peptides and all possible locations of the selected modifications?" />
  <row Id="3219" PostHistoryTypeId="4" PostId="1016" RevisionGUID="afe77bd5-20f4-450a-aa59-80e90fbf01ff" CreationDate="2017-07-08T11:43:12.400" UserId="96" Comment="clarify the language" Text="Software to produce a table of post-translational modifications from a peptide list" />
  <row Id="3220" PostHistoryTypeId="6" PostId="1016" RevisionGUID="afe77bd5-20f4-450a-aa59-80e90fbf01ff" CreationDate="2017-07-08T11:43:12.400" UserId="96" Comment="clarify the language" Text="&lt;r&gt;&lt;proteins&gt;&lt;python&gt;" />
  <row Id="3221" PostHistoryTypeId="24" PostId="1016" RevisionGUID="afe77bd5-20f4-450a-aa59-80e90fbf01ff" CreationDate="2017-07-08T11:43:12.400" Comment="Proposed by 96 approved by 57, 298 edit id of 235" />
  <row Id="3222" PostHistoryTypeId="2" PostId="1017" RevisionGUID="6bbd9fea-955f-41ec-861d-d68fd75cd901" CreationDate="2017-07-08T12:50:08.357" UserId="298" Text="I don't know of a tool that can do exactly what you want. However, there are various tools that can predict specific classes of post translational modifications. For example:&#xD;&#xA;&#xD;&#xA;1. Phosphorylation: [GPS][1] &#xD;&#xA;&#xD;&#xA; &gt;Computational prediction of phosphorylation sites with their cognate protein kinases (PKs) is greatly helpful for further experimental design. Although ~10 online predictors were developed, the PK classification and control of false positive rate (FPR) were not well addressed. Here we adopted a well-established rule to classify PKs into a hierarchical structure with four levels. &#xD;&#xA;&#xD;&#xA;2. Mitochondrial import: [Mitoprot][2]&#xD;&#xA;&#xD;&#xA; &gt; MitoProt calculates the N-terminal protein region that can support a Mitochondrial Targeting Sequence and the cleavage site. A complete description of the method to make the prediction is available in: M.G. Claros, P. Vincens. Computational method to predict mitochondrially imported proteins and their targeting sequences. Eur. J. Biochem. 241, 779-786 (1996). &#xD;&#xA;&#xD;&#xA;3. Acetylation: [&quot;PAIL: Prediction of Acetylation on Internal Lysines&quot;][3]&#xD;&#xA;&#xD;&#xA;   &gt; In this work, we present a novel online predictor for protein acetylation sites prediction of PAIL, Prediction of Acetylation on Internal Lysines. We have manually mined scientific literature to collect 249 experimentally verified acetylation sites of 92 distinct proteins. Then the BDM (Bayesian Discriminant Method) algorithm has been employed.&#xD;&#xA;&#xD;&#xA;You can use tools like the above and then combine the results to get the sort of matrix you are asking for. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://gps.biocuckoo.org/&#xD;&#xA;  [2]: http://ihg.gsf.de/ihg/mitoprot.html&#xD;&#xA;  [3]: http://bdmpail.biocuckoo.org/prediction.php" />
  <row Id="3223" PostHistoryTypeId="2" PostId="1018" RevisionGUID="ed36c649-9f49-43c6-88c7-99f3b2253cd7" CreationDate="2017-07-08T13:35:12.137" UserId="872" Text="I'm using velvet to align given reads of RNA to given CDs (coding areas and genes) of an organism so I can generate gene-expresion profiles. But after using `velvetg out-dir/ -alignment yes`, velvet produces `contig-alignment.psa` which is a file in a strange format. &#xD;&#xA;The file for every contig contains zero or more records of 7 integers that I cannot understand their meaning." />
  <row Id="3224" PostHistoryTypeId="1" PostId="1018" RevisionGUID="ed36c649-9f49-43c6-88c7-99f3b2253cd7" CreationDate="2017-07-08T13:35:12.137" UserId="872" Text="How to interpret `contig-alignment.psa` produced by velvet" />
  <row Id="3225" PostHistoryTypeId="3" PostId="1018" RevisionGUID="ed36c649-9f49-43c6-88c7-99f3b2253cd7" CreationDate="2017-07-08T13:35:12.137" UserId="872" Text="&lt;read-alignment&gt;&lt;rna-alignment&gt;" />
  <row Id="3226" PostHistoryTypeId="4" PostId="1018" RevisionGUID="5a4c0b76-b0bb-48c4-a3b9-61c203970fb7" CreationDate="2017-07-08T13:43:04.433" UserId="872" Comment="edited title" Text="How to interpret contig-alignment.psa produced by velvet" />
  <row Id="3227" PostHistoryTypeId="5" PostId="1018" RevisionGUID="5a4c0b76-b0bb-48c4-a3b9-61c203970fb7" CreationDate="2017-07-08T13:43:04.433" UserId="872" Comment="edited title" Text="I'm using velvet to align given reads of RNA to given CDs (i.e. coding areas and genes) of an organism, so I can generate gene-expresion profiles. But after using `velvetg out-dir/ -alignment yes`, velvet produces `contig-alignment.psa` which is a file in a strange format. &#xD;&#xA;The file for every contig contains zero or more records of 7 integers that I cannot understand their meaning." />
  <row Id="3228" PostHistoryTypeId="5" PostId="1018" RevisionGUID="b49caae4-6b56-44d4-9afb-1f5c5c9a91ed" CreationDate="2017-07-08T13:51:18.430" UserId="298" Comment="added 2 characters in body" Text="I'm using velvet to align given reads of RNA to given CDSs (i.e. coding areas and genes) of an organism, so I can generate gene-expression profiles. But after using `velvetg out-dir/ -alignment yes`, velvet produces `contig-alignment.psa` which is a file in a strange format. &#xD;&#xA;The file for every contig contains zero or more records of 7 integers, and I cannot understand their meaning." />
  <row Id="3229" PostHistoryTypeId="2" PostId="1019" RevisionGUID="23eaef7b-869d-4112-98b3-50ece00b1398" CreationDate="2017-07-08T14:24:13.377" UserId="872" Text="I'm trying to compute gene-expression profile for an organism. I have gene nucleotide sequences of the mentioned organism stored in a fasta file and a set of paired reads stored in two separate files with the same format. Now I want to compute coverage of every gene by the given reads that is on average how many reads cover a given segment of the sequence.&#xD;&#xA;&#xD;&#xA;Is there any tool for this specific purpose?&#xD;&#xA;I'm trying to use velvet sequencer but I have encountered a [few problems][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/1018/how-to-interpret-contig-alignment-psa-produced-by-velvet" />
  <row Id="3230" PostHistoryTypeId="1" PostId="1019" RevisionGUID="23eaef7b-869d-4112-98b3-50ece00b1398" CreationDate="2017-07-08T14:24:13.377" UserId="872" Text="How to compute gene expression for a set of rna reads?" />
  <row Id="3231" PostHistoryTypeId="3" PostId="1019" RevisionGUID="23eaef7b-869d-4112-98b3-50ece00b1398" CreationDate="2017-07-08T14:24:13.377" UserId="872" Text="&lt;rna-seq&gt;&lt;rna-alignment&gt;" />
  <row Id="4229" PostHistoryTypeId="5" PostId="1018" RevisionGUID="6623b2da-f4dc-445a-baa1-2ce61b596b6b" CreationDate="2017-07-08T14:53:23.463" UserId="298" Comment="added 455 characters in body" Text="I'm using velvet to align given reads of RNA to given CDSs (i.e. coding areas and genes) of an organism, so I can generate gene-expression profiles. But after using `velvetg out-dir/ -alignment yes`, velvet produces `contig-alignment.psa` which is a file in a strange format. &#xD;&#xA;The file for every contig contains zero or more records of 7 integers, and I cannot understand their meaning.&#xD;&#xA;&#xD;&#xA;The following is an excerpt of the file, and you can see the entire file [here][1]:&#xD;&#xA;&#xD;&#xA;    &gt;contig_1&#xD;&#xA;    1    193	1939	1939	6	194	2&#xD;&#xA;    195	244	1	1	6	22	71&#xD;&#xA;    &gt;contig_2&#xD;&#xA;    1	84	1	1	6	59	142&#xD;&#xA;    86	170	1935	1935	6	22	106&#xD;&#xA;    172	285	1935	1935	6	108	221&#xD;&#xA;    &gt;contig_3&#xD;&#xA;    1	98	2	2	6	1	98&#xD;&#xA;    100	321	2	2	6	100	321&#xD;&#xA;    334	415	1204	1204	6	1	82&#xD;&#xA;    &gt;contig_15&#xD;&#xA;    1	23	3	3	6	84	106&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://paste.ofcode.org/d9599cGH3aXnbdGqtWqrMs" />
  <row Id="4230" PostHistoryTypeId="6" PostId="1018" RevisionGUID="40878de5-d0dc-4df0-b524-c841a432e7c7" CreationDate="2017-07-08T15:44:04.793" UserId="298" Comment="edited tags" Text="&lt;rna-alignment&gt;&lt;read-alignment&gt;&lt;velvet&gt;" />
  <row Id="4231" PostHistoryTypeId="5" PostId="1019" RevisionGUID="43389330-21d1-4e31-864b-a3e78d350b9b" CreationDate="2017-07-08T16:07:49.300" UserId="298" Comment="added 12 characters in body; edited title" Text="I'm trying to compute a gene expression profile for an organism. I have gene nucleotide sequences of the mentioned organism stored in a fasta file and a set of paired reads stored in two separate files with the same format. Now I want to compute the coverage of every gene by the given reads. That is, on average how many reads cover a given segment of the sequence.&#xD;&#xA;&#xD;&#xA;Is there any tool for this specific purpose?&#xD;&#xA;I'm trying to use the velvet sequencer but I have encountered a [few problems][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/1018/how-to-interpret-contig-alignment-psa-produced-by-velvet" />
  <row Id="4232" PostHistoryTypeId="4" PostId="1019" RevisionGUID="43389330-21d1-4e31-864b-a3e78d350b9b" CreationDate="2017-07-08T16:07:49.300" UserId="298" Comment="added 12 characters in body; edited title" Text="How can I compute gene expression for a set of RNA reads?" />
  <row Id="4233" PostHistoryTypeId="2" PostId="2019" RevisionGUID="33f4bce7-4f24-4519-b2dd-e3f8bbce0f70" CreationDate="2017-07-08T16:22:43.643" UserId="77" Text="It's unclear if your paired-end reads are actually in fasta format, I'll presume that they're in fastq instead.&#xD;&#xA;&#xD;&#xA;The easiest tool to use is [salmon](http://salmon.readthedocs.io/en/latest/), which nicely deals with things like multimapping. If you're trying to judge the quality of an assembly, then I recommend having a look at [transrate](https://www.ncbi.nlm.nih.gov/pubmed/27252236), which uses some related methodology for assessing contig quality." />
  <row Id="4234" PostHistoryTypeId="5" PostId="1019" RevisionGUID="1e056a13-d40b-4216-8431-0e1e20ecec47" CreationDate="2017-07-08T19:08:23.327" UserId="872" Comment="added 7 characters in body" Text="I'm trying to compute a gene expression profile for an organism. I have gene nucleotide sequences of the mentioned organism stored in a fasta file and a set of paired reads stored in two separate files with the same fasta format. Now I want to compute the coverage of every gene by the given reads. That is, on average how many reads cover a given segment of the sequence.&#xD;&#xA;&#xD;&#xA;Is there any tool for this specific purpose?&#xD;&#xA;I'm trying to use the velvet sequencer but I have encountered a [few problems][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/1018/how-to-interpret-contig-alignment-psa-produced-by-velvet" />
  <row Id="4235" PostHistoryTypeId="2" PostId="2020" RevisionGUID="d06653cd-5365-455e-b190-6247a777f8c0" CreationDate="2017-07-08T19:17:51.123" UserId="35" Text="In addition to salmon that was already mentioned, you can also try [kallisto][1] or [RSEM][2], which are also fairly popular/respectable and will work with a transcriptome FASTA.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://pachterlab.github.io/kallisto/starting&#xD;&#xA;  [2]: https://github.com/bli25ucb/RSEM_tutorial" />
  <row Id="4236" PostHistoryTypeId="2" PostId="2021" RevisionGUID="fcdd34d6-c56f-4a59-9de6-5dc94e53551b" CreationDate="2017-07-08T21:50:31.803" UserId="73" Text="It's not possible to compute absolute expression from RNASeq reads if they are processed in the usual way, where a sequencer produces the same number of reads regardless of the input RNA amount. At best, RNASeq will give you an indication of [proportional expression](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004075) within a single sample. For this reason, relative expression (i.e. that used by differential expression tests) is easier to determine than absolute expression.&#xD;&#xA;&#xD;&#xA;The closest approximation to absolute expression is to generate an expression relative to the average expression of a set of housekeeping genes, but I don't think there's a universal set that has been decided on. Gene expression, even for common housekeeping genes, can vary depending on the environmental conditions of the cell. For example, GAPDK is involved in [immune cell activation](http://www.nature.com/nri/journal/v16/n9/full/nri.2016.70.html).&#xD;&#xA;&#xD;&#xA;However, as long as experimental conditions are similar, and you're not planning on looking for statistical significance, the proportional expression can still give qualitative insights into how cell populations behave in relation to other populations. DESeq2 provides a [variance-stabilising transformation function](https://rdrr.io/bioc/DESeq2/man/varianceStabilizingTransformation.html) that minimises variation for small-count genes, assuming that each sample has roughly the same total expression. I have found that I get better outcomes / comparisons from this transformation when carrying out a further adjustment to account for gene length (i.e. divide by the length of the longest transcript for each gene). See our [Th2 paper](http://jem.rupress.org/content/early/2016/12/01/jem.20160470#materials-methods), section &quot;Read mapping and differential expression analysis&quot; for more information. The &quot;transcripts-per-million&quot; values produced by [Kallisto](https://pachterlab.github.io/kallisto/about.html) and [Salmon](https://combine-lab.github.io/salmon/) provide measures similar to this.&#xD;&#xA;&#xD;&#xA;If, on the other hand, you were able to modify the experimental design, single cell sequencing (or &quot;known cell count&quot; sequencing) can be used for determining absolute expression: use a spiked-in transcript that is added in proportion to the *cell count*, so that results can be compared in proportion to the expression for that transcript." />
  <row Id="4237" PostHistoryTypeId="2" PostId="2022" RevisionGUID="3f788bd1-1261-495d-8912-2d07dff2b4a9" CreationDate="2017-07-09T03:06:16.293" UserId="1082" Text="I have been searching for a long time and the furthest I got is some database with the functional description for genes. Then, I have to parse these descriptions manually to figure out the association. Moreover, for some species, there are not even any functional descriptions. &#xD;&#xA;&#xD;&#xA;I need this information for mice, rice, and Arabidopsis Thaliana. Can anyone help me? &#xD;&#xA;&#xD;&#xA;" />
  <row Id="4238" PostHistoryTypeId="1" PostId="2022" RevisionGUID="3f788bd1-1261-495d-8912-2d07dff2b4a9" CreationDate="2017-07-09T03:06:16.293" UserId="1082" Text="Where can I find a database that has phenotype information together with associated SNPs?" />
  <row Id="4239" PostHistoryTypeId="3" PostId="2022" RevisionGUID="3f788bd1-1261-495d-8912-2d07dff2b4a9" CreationDate="2017-07-09T03:06:16.293" UserId="1082" Text="&lt;gene&gt;&lt;phylogenetics&gt;&lt;gwas&gt;" />
  <row Id="4240" PostHistoryTypeId="6" PostId="2022" RevisionGUID="81b4061a-d05b-4820-be46-92ea4e5b88c3" CreationDate="2017-07-09T16:27:02.363" UserId="57" Comment="deleted unrelated tag" Text="&lt;gene&gt;&lt;gwas&gt;" />
  <row Id="4241" PostHistoryTypeId="2" PostId="2023" RevisionGUID="f6fdc791-1dea-4869-b470-7c0950814448" CreationDate="2017-07-09T19:15:01.990" UserId="1024" Text="You can do this in [Hail][1]:&#xD;&#xA;&#xD;&#xA;    from hail import *&#xD;&#xA;    hc = HailContext()&#xD;&#xA;    (hc.import_vcf('test.vcf')&#xD;&#xA;     .filter_variants_expr('gs.exists(g =&gt; g.ad[1:].exists(d =&gt; d &gt; 10))')&#xD;&#xA;     .export_vcf('filtered.vcf'))&#xD;&#xA;&#xD;&#xA;This works with any number of samples and will keep the variants where at least one sample has a genotype with an alternate allele support by more than 10 reads.&#xD;&#xA;&#xD;&#xA;To verify we got the expected 6 variants:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; hc.import_vcf('filtered.vcf').count()&#xD;&#xA;    (1L, 6L)&#xD;&#xA;&#xD;&#xA;[`count`][2] returns the number of samples (1) and number of variants (6).&#xD;&#xA;&#xD;&#xA;Take a look at the [getting started page][3] or [tutorials][4] if you want to try it out!&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://hail.is&#xD;&#xA;  [2]: https://hail.is/hail/hail.VariantDataset.html#hail.VariantDataset.count&#xD;&#xA;  [3]: https://hail.is/hail/getting_started.html&#xD;&#xA;  [4]: https://hail.is/hail/tutorials-landing.html" />
  <row Id="4242" PostHistoryTypeId="5" PostId="1011" RevisionGUID="1f742236-f9d3-46da-9d69-2edd291bb401" CreationDate="2017-07-09T20:07:48.103" UserId="48" Comment="Minor correction to the code T == TRUE and F == FALSE" Text="Note the flag; that read is mapped in a reverse-complemented manner, so `isMinusStrand = FALSE` is filtering it out. I tested this by making a BAM file with only that read:&#xD;&#xA;&#xD;&#xA;    @SQ	SN:I	LN:1000000&#xD;&#xA;    000000F|arrow	2064	I	336331	7	2926310H260M1774267H	*	00	*	*&#xD;&#xA;&#xD;&#xA;Then in R:&#xD;&#xA;&#xD;&#xA;    &gt; library(Rsamtools)&#xD;&#xA;    &gt; blah = scanBam(&quot;foo.bam&quot;, param=ScanBamParam(what=scanBamWhat(), flag=scanBamFlag(isMinusStrand = TRUE)))[[1]]&#xD;&#xA;    &gt; blah$qname&#xD;&#xA;    [1] &quot;000000F|arrow&quot;&#xD;&#xA;    &gt; blah = scanBam(&quot;foo.bam&quot;, param=ScanBamParam(what=scanBamWhat(), flag=scanBamFlag(isMinusStrand = FALSE)))[[1]]&#xD;&#xA;    &gt; blah$qname&#xD;&#xA;    character(0)" />
  <row Id="4243" PostHistoryTypeId="24" PostId="1011" RevisionGUID="1f742236-f9d3-46da-9d69-2edd291bb401" CreationDate="2017-07-09T20:07:48.103" Comment="Proposed by 48 approved by 77 edit id of 234" />
  <row Id="4244" PostHistoryTypeId="2" PostId="2024" RevisionGUID="afc29859-3fc7-4866-82e5-9c589544d331" CreationDate="2017-07-09T21:31:41.673" UserId="734" Text="Are there researcher who apply data mining techniques on articles and paper such as google scholar in order to derive relations between genes and diseases?  " />
  <row Id="4245" PostHistoryTypeId="1" PostId="2024" RevisionGUID="afc29859-3fc7-4866-82e5-9c589544d331" CreationDate="2017-07-09T21:31:41.673" UserId="734" Text="Using data mining of papers in order to derive genomic connections" />
  <row Id="4246" PostHistoryTypeId="3" PostId="2024" RevisionGUID="afc29859-3fc7-4866-82e5-9c589544d331" CreationDate="2017-07-09T21:31:41.673" UserId="734" Text="&lt;gene&gt;&lt;genome&gt;&lt;cancer&gt;" />
  <row Id="4247" PostHistoryTypeId="5" PostId="2024" RevisionGUID="a7411f17-d3a7-4d99-a77c-e8594f9546d3" CreationDate="2017-07-09T21:46:23.243" UserId="77" Comment="Fix grammar a bit; edited tags" Text="Are there any examples of researchers data mining articles and paper (e.g., from pubmed or google scholar) in order to derive relations between genes and diseases?  " />
  <row Id="4248" PostHistoryTypeId="6" PostId="2024" RevisionGUID="a7411f17-d3a7-4d99-a77c-e8594f9546d3" CreationDate="2017-07-09T21:46:23.243" UserId="77" Comment="Fix grammar a bit; edited tags" Text="&lt;gene&gt;&lt;genome&gt;&lt;cancer&gt;&lt;data-mining&gt;" />
  <row Id="4249" PostHistoryTypeId="5" PostId="2024" RevisionGUID="68c42147-7473-4894-8659-c1982e56f2b8" CreationDate="2017-07-09T21:52:51.403" UserId="734" Comment="added 1 character in body" Text="Are there any examples of researchers data mining articles and paperד (e.g., from pubmed or google scholar) in order to derive relations between genes and diseases?  " />
  <row Id="4250" PostHistoryTypeId="5" PostId="2024" RevisionGUID="18b838b3-77d9-4606-bb41-c81a326b5a87" CreationDate="2017-07-09T21:59:22.660" UserId="77" Comment="edited body" Text="Are there any examples of researchers data mining articles and papers (e.g., from pubmed or google scholar) in order to derive relations between genes and diseases?  " />
  <row Id="4251" PostHistoryTypeId="2" PostId="2025" RevisionGUID="5526ace6-65ca-40ee-876b-b44a382c28df" CreationDate="2017-07-09T22:32:46.213" UserId="964" Text="There are many databases that have used publication scraping for oncogenic gene fusions. There are publications for the individual methods they used for their scraping and aggregation.&#xD;&#xA;&#xD;&#xA; - COSMIC - http://cancer.sanger.ac.uk/cosmic&#xD;&#xA; - TICdb - http://www.unav.es/genetica/TICdb/&#xD;&#xA; - ChimerDB - http://ercsb.ewha.ac.kr/fusiongene/ -&#xD;&#xA; https://www.ncbi.nlm.nih.gov/pubmed/27899563&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4252" PostHistoryTypeId="2" PostId="2026" RevisionGUID="592e6f3e-91a5-498c-b580-f50fed73c488" CreationDate="2017-07-09T22:50:16.190" UserId="73" Text="This existed as a closed silo, at least in 2015. Qiagen has a team of hired students and Post-Docs for collating research papers into their [Knowledge Base](https://www.qiagenbioinformatics.com/products/qiagen-knowledge-base/), an extensive database that is integrated into a few of their commercial products. Qiagen's claim is that by providing a consistently-structured and well-formatted database, the process of research discovery is accelerated. I've got a few notes from a seminar they gave us a couple of years ago:&#xD;&#xA;&#xD;&#xA;* The curation is better than a free / public database because free databases typically have wrong or inconsistent data&#xD;&#xA;* Their knowledge base is an army of hired MDs and PhDs, 5.3M &quot;findings&quot; as of Nov 05 2015&#xD;&#xA;* Scientific articles have unstructured data, so expert knowledge is required to process the data into a &quot;finding&quot; for their knowledge base&#xD;&#xA;&#xD;&#xA;I was already doubtful about their approach, and this doubt was magnified when they started getting into the details of their process. I started to lose trust and write my own thoughts about it:&#xD;&#xA;&#xD;&#xA;* The knowledge base they have created is private&#xD;&#xA;* The approach is to use a function (i.e. MDs/PhDs) to convert research articles into scientifically-validated associations. However, the function is effectively a black box; Qiagen allow the experts to decide for themselves how a particular article should be converted into a finding.&#xD;&#xA;* The findings in the knowledge base have no strength or likelihood.&#xD;&#xA;* The Experts were encouraged to only enter findings that were &quot;generally accepted by the scientific community. but it seemed like only a single expert was required to get a finding into the knowledge base (i.e. findings weren't curated by other people).&#xD;&#xA;* When using their software (e.g. IPA), p-values of 10^-15 and 10^-22 are reported; these numbers are meaningless.&#xD;&#xA;* The team didn't look at methylation, phosphorylation, etc.; plans are in place to hire additional experts to look at this at a later date.&#xD;&#xA;&#xD;&#xA;After listening to their seminar, I realised that Qiagen's idea of a general database that attempts to capture everything isn't going to work well. Experts will disagree about whether a particular association is valid, relevance (or correctness) will differ depending on the area of interest, and the established correctness of a piece of information can change over time.&#xD;&#xA;&#xD;&#xA;It would be possible to create something that might work by adding a lot more metadata to all the &quot;findings&quot; in such a database, but it would need to be a huge, public, collaborative effort. Something on the scale of SRA/ENA, with encouragement from the scientific community to enter details into the database prior to publications being accepted/indexed. If the people who wrote papers entered their findings in such a database, other researchers wouldn't need to be hired to try to understand papers and guess at the findings of published research." />
  <row Id="4253" PostHistoryTypeId="10" PostId="2024" RevisionGUID="38c1cd4d-e62b-4681-8841-4f1e07f30283" CreationDate="2017-07-09T23:00:13.187" UserId="298" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;}]}" />
  <row Id="4254" PostHistoryTypeId="2" PostId="2027" RevisionGUID="6d7417c3-6d38-4c45-8941-e7dcab1b5a04" CreationDate="2017-07-10T06:56:52.493" UserId="1085" Text="Few days ago, I am trying to find some databases to download  gwas dataset. I hope this site for 3000 rice will help : http://snp-seek.irri.org/. You can click [Genotype](http://snp-seek.irri.org/_snp.zul) in the index page for &quot;Query for SNPs from the 3000 genome project&quot;" />
  <row Id="4255" PostHistoryTypeId="2" PostId="2028" RevisionGUID="7de300ed-7d32-428c-9d0c-7c57b4989e49" CreationDate="2017-07-10T09:40:34.390" UserId="180" Text="MEDIPS is an established tool with functions for the quality control and analysis of data derived from immunoprecipitation (IP)-seq samples, like Methylation IP sequencing datasets.&#xD;&#xA;&#xD;&#xA;I would like to know if there are any other tools I should consider nowadays that could do a better job and hence replace MEDIPS for the analysis of MeDIP datasets." />
  <row Id="4256" PostHistoryTypeId="1" PostId="2028" RevisionGUID="7de300ed-7d32-428c-9d0c-7c57b4989e49" CreationDate="2017-07-10T09:40:34.390" UserId="180" Text="alternatives to MEDIPS to analyse MeDIP datasets" />
  <row Id="4257" PostHistoryTypeId="3" PostId="2028" RevisionGUID="7de300ed-7d32-428c-9d0c-7c57b4989e49" CreationDate="2017-07-10T09:40:34.390" UserId="180" Text="&lt;methylation&gt;&lt;medips&gt;&lt;medip-seq&gt;" />
  <row Id="4258" PostHistoryTypeId="5" PostId="1001" RevisionGUID="25d1d9ec-c729-4193-b96b-c81fff7c50a4" CreationDate="2017-07-10T11:19:15.403" UserId="982" Comment="added 163 characters in body" Text="I have a core genome dataset of approx 2530 genes for 149 taxa. I have run an unpartitioned phylogenetic analysis using iqtree. But I am unhappy with the resolution in some of the phylogeny. I want to check if partitioning the alignment  will improve resolution. My alignment is from roary output and MAFFT alignment.&#xD;&#xA;&#xD;&#xA;Ive tried partitioning by loci and using iqtree (beta 1.6.4) to determine a model of best fit for each but this is apparently a very long demanding process. What are other good methods/ means to partition DNA data?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;My next course of action is to remove uninformative sites. Id rather that this didnt chop up individual genes. Is it possible to observe the identity in each partition and remove genes which are 100% identical?&#xD;&#xA;iqtree does a quick analysis of each partition and reveals which are parsimony uninformative sites. Would I be just as well to delete these partitions?&#xD;&#xA;" />
  <row Id="4259" PostHistoryTypeId="2" PostId="2029" RevisionGUID="e5fb847c-1cdd-46ec-80ef-4fdfcb4aa02c" CreationDate="2017-07-10T12:02:01.193" UserId="1063" Text="I'm looking for a Linked Open Data approach to annotate a dataset with genomic and drugs information.&#xD;&#xA;&#xD;&#xA;According to the [Linked Open Data cloud][1], there are a lot of interconnected `RDF` vocabularies devoted to life sciences but, unfortunately, a lot of them seem out of date.&#xD;&#xA;&#xD;&#xA;I recently found Wikidata. &#xD;&#xA;&#xD;&#xA;&gt; Wikidata is a free and open knowledge base that can be read and edited&#xD;&#xA;&gt; by both humans and machines.&#xD;&#xA;&#xD;&#xA;It seems exactly what Linked Data should be and it seems continuously updated. At a first sight, it covers very well genomic and drugs knowledge.&#xD;&#xA;&#xD;&#xA;Does Anyone know if Wikidata is a reliable source for genomic and drugs annotation?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://lod-cloud.net/" />
  <row Id="4260" PostHistoryTypeId="1" PostId="2029" RevisionGUID="e5fb847c-1cdd-46ec-80ef-4fdfcb4aa02c" CreationDate="2017-07-10T12:02:01.193" UserId="1063" Text="Is Wikidata a reliable KB for genomic and drugs annotations?" />
  <row Id="4261" PostHistoryTypeId="3" PostId="2029" RevisionGUID="e5fb847c-1cdd-46ec-80ef-4fdfcb4aa02c" CreationDate="2017-07-10T12:02:01.193" UserId="1063" Text="&lt;annotation&gt;&lt;public-databases&gt;&lt;data-management&gt;" />
  <row Id="4262" PostHistoryTypeId="5" PostId="2029" RevisionGUID="38ac00fa-f256-448b-a053-61a69f2d250f" CreationDate="2017-07-10T12:08:06.710" UserId="298" Comment="added 38 characters in body" Text="I'm looking for a Linked Open Data approach to annotate a dataset with genomic and drugs information.&#xD;&#xA;&#xD;&#xA;According to the [Linked Open Data cloud][1], there are a lot of interconnected `RDF` vocabularies devoted to life sciences but, unfortunately, a lot of them seem out of date.&#xD;&#xA;&#xD;&#xA;I recently found [Wikidata][2]:&#xD;&#xA;&#xD;&#xA;&gt; Wikidata is a free and open knowledge base that can be read and edited&#xD;&#xA;&gt; by both humans and machines.&#xD;&#xA;&#xD;&#xA;It seems exactly what Linked Data should be and it seems continuously updated. At a first sight, it covers very well genomic and drugs knowledge.&#xD;&#xA;&#xD;&#xA;Does Anyone know if Wikidata is a reliable source for genomic and drugs annotation?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://lod-cloud.net/&#xD;&#xA;  [2]: https://www.wikidata.org/" />
  <row Id="4263" PostHistoryTypeId="50" PostId="627" RevisionGUID="aeb0aff1-82f9-45f6-abe4-a3059392bccf" CreationDate="2017-07-10T13:49:23.193" UserId="-1" />
  <row Id="4264" PostHistoryTypeId="2" PostId="2030" RevisionGUID="44018a5f-1740-4c50-85a5-d9ccd1fcc218" CreationDate="2017-07-10T16:25:32.420" UserId="57" Text="[International Mouse Phenotyping Consortium][1] is building a database of phenotypes and knock-outs of mouse. I believe that this database will be fairly complete (20000 knock-outs), but these are knock-outs, not SNPs... There are several mouse GWAS studies, but I am not aware of a database that would pull all the results together.&#xD;&#xA;&#xD;&#xA;*Arabidopsis* big GWAS project is called [1001 genomes project][2]. A lot more information about *Arabidopsis* from individual experiments is collected and maintained by [The Arabidopsis Information Resource][3], however this resource is subscription based, therefore what you need might not be available for free.&#xD;&#xA;&#xD;&#xA;As mentioned, SNPs of rice were identified and linked to different rice groups in [The 3,000 rice genomes project][4]. There is also [GWAS][5] done on 512 individuals, not sure if it is sufficient for reliable conclusions (even they claim that they found supersigificant links).&#xD;&#xA;&#xD;&#xA;Is this what you were searching for?&#xD;&#xA;&#xD;&#xA;  [1]: http://www.mousephenotype.org/&#xD;&#xA;  [2]: http://1001genomes.org/tools/&#xD;&#xA;  [3]: https://www.arabidopsis.org/&#xD;&#xA;  [4]: https://gigascience.biomedcentral.com/articles/10.1186/2047-217X-3-7&#xD;&#xA;  [5]: https://www.ncbi.nlm.nih.gov/pubmed/27322545" />
  <row Id="4265" PostHistoryTypeId="2" PostId="2031" RevisionGUID="4fa61a49-fc62-4e89-b09d-1f21cf2a9bfe" CreationDate="2017-07-10T17:05:03.750" UserId="1088" Text="I need to install DnaSp from the following link  http://www2.ub.es/dnasp/download.html&#xD;&#xA;&#xD;&#xA;but its not working on my mac. im using wine to install it using these instructions for wine http://www2.ub.es/dnasp/download.html&#xD;&#xA;&#xD;&#xA;it successfully installs but then i get this error&#xD;&#xA;&#xD;&#xA;Mollies-MacBook-Air:dnasp51001 molliepassacantando$ WINEPREFIX=~/.wine64 wine regsvr32.exe scrrun.dll mfc40.dll threed32.ocx&#xD;&#xA;regsvr32: Successfully registered DLL 'scrrun.dll'&#xD;&#xA;regsvr32: Successfully registered DLL 'mfc40.dll'&#xD;&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;regsvr32: Successfully registered DLL 'threed32.ocx'&#xD;&#xA;Mollies-MacBook-Air:dnasp51001 molliepassacantando$ winedevice.exe(30828,0x405ed000) malloc: *** error for object 0x401cea08: pointer being freed was not allocated&#xD;&#xA;*** set a breakpoint in malloc_error_break to debug&#xD;&#xA;&#xD;&#xA;and cannot run it. has anyone succesfuly installed this on a mac or know of an alternate program i could use?&#xD;&#xA;Thanks!" />
  <row Id="4266" PostHistoryTypeId="1" PostId="2031" RevisionGUID="4fa61a49-fc62-4e89-b09d-1f21cf2a9bfe" CreationDate="2017-07-10T17:05:03.750" UserId="1088" Text="How to get DnaSP on a Mac" />
  <row Id="4267" PostHistoryTypeId="3" PostId="2031" RevisionGUID="4fa61a49-fc62-4e89-b09d-1f21cf2a9bfe" CreationDate="2017-07-10T17:05:03.750" UserId="1088" Text="&lt;genome&gt;&lt;dna&gt;" />
  <row Id="4268" PostHistoryTypeId="5" PostId="2031" RevisionGUID="6fc95cb0-7b11-4992-a9ae-d3192adf0e1b" CreationDate="2017-07-10T17:09:02.527" UserId="57" Comment="language / formating" Text="I need to install [DnaSp][1],&#xD;&#xA;&#xD;&#xA;but its not working on my mac. I'm using wine to install it using [these instructions for wine][1]. &#xD;&#xA;&#xD;&#xA;it successfully installs but then I get this error&#xD;&#xA;&#xD;&#xA;    Mollies-MacBook-Air:dnasp51001 molliepassacantando$ WINEPREFIX=~/.wine64 wine regsvr32.exe scrrun.dll mfc40.dll threed32.ocx&#xD;&#xA;    regsvr32: Successfully registered DLL 'scrrun.dll'&#xD;&#xA;    regsvr32: Successfully registered DLL 'mfc40.dll'&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    regsvr32: Successfully registered DLL 'threed32.ocx'&#xD;&#xA;    Mollies-MacBook-Air:dnasp51001 molliepassacantando$ winedevice.exe(30828,0x405ed000) malloc: *** error for object 0x401cea08: pointer being freed was not allocated&#xD;&#xA;    *** set a breakpoint in malloc_error_break to debug&#xD;&#xA;&#xD;&#xA;and cannot run it. has anyone successfully installed DnaSp on a Mac or know of an alternate program I could use?&#xD;&#xA;&#xD;&#xA;  [1]: http://www2.ub.es/dnasp/download.html" />
  <row Id="4269" PostHistoryTypeId="4" PostId="2031" RevisionGUID="6fc95cb0-7b11-4992-a9ae-d3192adf0e1b" CreationDate="2017-07-10T17:09:02.527" UserId="57" Comment="language / formating" Text="How to install DnaSP on a Mac" />
  <row Id="4270" PostHistoryTypeId="6" PostId="2031" RevisionGUID="6fc95cb0-7b11-4992-a9ae-d3192adf0e1b" CreationDate="2017-07-10T17:09:02.527" UserId="57" Comment="language / formating" Text="&lt;genome&gt;&lt;dna&gt;&lt;dnasp&gt;" />
  <row Id="4271" PostHistoryTypeId="6" PostId="2031" RevisionGUID="0e883eb9-7e0b-4d87-90c4-9e91cf17febd" CreationDate="2017-07-10T17:13:04.340" UserId="298" Comment="edited tags" Text="&lt;genome&gt;&lt;dna&gt;&lt;dnasp&gt;&lt;software-installation&gt;" />
  <row Id="4272" PostHistoryTypeId="2" PostId="2032" RevisionGUID="bbe141a4-97b4-4391-9bb6-bc5ce3a2b109" CreationDate="2017-07-10T17:15:42.777" UserId="579" Text="The question is what you mean with &quot;reliable&quot;. For bacterial genome annotations there is one Wikidata based platform called [WikiGenomes][1] ([publications][2]).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://wikigenomes.org/&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/pubmed/28365742" />
  <row Id="4273" PostHistoryTypeId="5" PostId="2031" RevisionGUID="580091f5-cbff-402a-a4a1-9b475d53ad40" CreationDate="2017-07-10T19:20:42.660" UserId="1088" Comment="added 54 characters in body" Text="I need to install [DnaSp][1],&#xD;&#xA;&#xD;&#xA;but its not working on my mac. I'm using wine to install it using [these instructions for wine][1]. &#xD;&#xA;&#xD;&#xA;it successfully installs but then I get this error&#xD;&#xA;&#xD;&#xA;    Mollies-MacBook-Air:dnasp51001 molliepassacantando$ WINEPREFIX=~/.wine64 wine regsvr32.exe scrrun.dll mfc40.dll threed32.ocx&#xD;&#xA;    regsvr32: Successfully registered DLL 'scrrun.dll'&#xD;&#xA;    regsvr32: Successfully registered DLL 'mfc40.dll'&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    err:typelib:sltg_get_typelib_ref Unable to find reference&#xD;&#xA;    regsvr32: Successfully registered DLL 'threed32.ocx'&#xD;&#xA;    Mollies-MacBook-Air:dnasp51001 molliepassacantando$ winedevice.exe(30828,0x405ed000) malloc: *** error for object 0x401cea08: pointer being freed was not allocated&#xD;&#xA;    *** set a breakpoint in malloc_error_break to debug&#xD;&#xA;&#xD;&#xA;and cannot run it. has anyone successfully installed DnaSp on a Mac or know of an alternate program I could use? I need to be able to calculate haplotype frequencies.&#xD;&#xA;&#xD;&#xA;  [1]: http://www2.ub.es/dnasp/download.html" />
  <row Id="4274" PostHistoryTypeId="5" PostId="2027" RevisionGUID="c4311c8e-0f22-43fa-abb7-4f0a279f7d04" CreationDate="2017-07-10T19:57:06.570" UserId="73" Comment="Correct grammar / spelling" Text="A few days ago, I was trying to find some GWAS datasets to download. I hope this site for 3000 rice will help: http://snp-seek.irri.org/. You can click [Genotype](http://snp-seek.irri.org/_snp.zul) in the index page for &quot;Query for SNPs from the 3000 genome project&quot;" />
  <row Id="4275" PostHistoryTypeId="5" PostId="2029" RevisionGUID="77c86160-3bbd-4753-bff0-86a6c239037c" CreationDate="2017-07-10T22:59:23.797" UserId="1063" Comment="added 5 characters in body" Text="I'm looking for a Linked Open Data approach to annotate a dataset with human genome and drugs information.&#xD;&#xA;&#xD;&#xA;According to the [Linked Open Data cloud][1], there are a lot of interconnected `RDF` vocabularies devoted to life sciences but, unfortunately, a lot of them seem out of date.&#xD;&#xA;&#xD;&#xA;I recently found [Wikidata][2]:&#xD;&#xA;&#xD;&#xA;&gt; Wikidata is a free and open knowledge base that can be read and edited&#xD;&#xA;&gt; by both humans and machines.&#xD;&#xA;&#xD;&#xA;It seems exactly what Linked Data should be and it seems continuously updated. At a first sight, it covers very well genomic and drugs knowledge.&#xD;&#xA;&#xD;&#xA;Does Anyone know if Wikidata is a reliable source for genomic and drugs annotation?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://lod-cloud.net/&#xD;&#xA;  [2]: https://www.wikidata.org/" />
  <row Id="4276" PostHistoryTypeId="50" PostId="604" RevisionGUID="cb32e97f-1b31-4579-9ee4-dadf73a2212e" CreationDate="2017-07-10T23:52:16.907" UserId="-1" />
  <row Id="4277" PostHistoryTypeId="2" PostId="2033" RevisionGUID="a9c3e87e-5806-404d-9cba-a760e2e79b1d" CreationDate="2017-07-11T02:29:51.997" UserId="35" Text="I guess QSEA should be an obvious answer:&#xD;&#xA;&#xD;&#xA;&gt; QSEA (quantitative sequencing enrichment analysis) was developed as&#xD;&#xA;&gt; the successor of the MEDIPS package for analyzing data derived from&#xD;&#xA;&gt; methylated DNA immunoprecipitation (MeDIP) experiments followed by&#xD;&#xA;&gt; sequencing (MeDIP-seq).&#xD;&#xA;&#xD;&#xA;See http://bioconductor.org/packages/release/bioc/vignettes/qsea/inst/doc/qsea_tutorial.html&#xD;&#xA;" />
  <row Id="4278" PostHistoryTypeId="5" PostId="2027" RevisionGUID="64c9a5c7-98c5-4f08-89ad-33224bdbe242" CreationDate="2017-07-11T06:26:50.940" UserId="1085" Comment="added 345 characters in body" Text="A few days ago, I was trying to find some GWAS datasets to download. I hope this site for 3000 rice will help: http://snp-seek.irri.org/. You can click [Genotype](http://snp-seek.irri.org/_snp.zul) in the index page for &quot;Query for SNPs from the 3000 genome project&quot;. &#xD;&#xA;&#xD;&#xA;**UPDATE:** With the help of user manual, I can get IRIS ID from [Genotype](http://snp-seek.irri.org/_snp.zul). According to this [paper](https://www.ncbi.nlm.nih.gov/pubmed/12855438), iris id may associate with some phenotype information. But the website of IRIS is under maintenance, I can not get more information about phenotype.  &#xD;&#xA;&#xD;&#xA;" />
  <row Id="4279" PostHistoryTypeId="2" PostId="2034" RevisionGUID="352aa2d3-7f84-4467-b122-e5520a56b034" CreationDate="2017-07-11T07:03:13.770" UserId="1085" Text="I am trying to reorder scaffolds of a rice specie, but genetic map is not avaiable right now. Oryza sativa Japonica is a close relative of this rice specie. Mummer was used to do whole genome alignment, I am trying to reorder scaffolds according the mummer result. But I found it's to complicate to do this by myself, many scaffolds may align to multi Oryza sativa Japonica chromosomes or multi locus in same chromosome.  &#xD;&#xA;&#xD;&#xA;I hope someone could give me some softwares or hints for this task." />
  <row Id="4280" PostHistoryTypeId="1" PostId="2034" RevisionGUID="352aa2d3-7f84-4467-b122-e5520a56b034" CreationDate="2017-07-11T07:03:13.770" UserId="1085" Text="Without genetic map, reordering the scaffolds according a reference" />
  <row Id="4281" PostHistoryTypeId="3" PostId="2034" RevisionGUID="352aa2d3-7f84-4467-b122-e5520a56b034" CreationDate="2017-07-11T07:03:13.770" UserId="1085" Text="&lt;alignment&gt;" />
  <row Id="4282" PostHistoryTypeId="2" PostId="2035" RevisionGUID="81d4afef-cdac-403c-842b-b126bf3d6810" CreationDate="2017-07-11T07:17:43.397" UserId="982" Text="Have you tried [Mauve][1] alignment? Its pretty easy once you become familiar and has a GUI for further ease of use. Additionally there are a few online tutorials on how to re-order contigs/ scaffolds using this software. Heres [one I use][2] when in need.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://darlinglab.org/mauve/mauve.html&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2723005/" />
  <row Id="4283" PostHistoryTypeId="6" PostId="2034" RevisionGUID="66191697-ef4d-4ba7-bb96-f1326aea1360" CreationDate="2017-07-11T07:21:17.707" UserId="982" Comment="Include more tags for better visibility. " Text="&lt;alignment&gt;&lt;assembly&gt;&lt;software-recommendation&gt;&lt;scaffold&gt;" />
  <row Id="4284" PostHistoryTypeId="24" PostId="2034" RevisionGUID="66191697-ef4d-4ba7-bb96-f1326aea1360" CreationDate="2017-07-11T07:21:17.707" Comment="Proposed by 982 approved by 1085 edit id of 236" />
  <row Id="4285" PostHistoryTypeId="2" PostId="2036" RevisionGUID="cec6b48c-279f-4805-8c05-3216ed849d6a" CreationDate="2017-07-11T07:47:50.980" UserId="294" Text="Looking for tools to reconcile alignment file of experimental transcripts mapped to genome (SAM/BAM) with the reference transcriptome annotation (GTF) from Ensembl (organism: *D. melanogaster*). &#xD;&#xA;&#xD;&#xA;The aim would be to check which transcripts reported in the alignment file are also reported in the reference annotation.&#xD;&#xA;&#xD;&#xA;Any idea?" />
  <row Id="4286" PostHistoryTypeId="1" PostId="2036" RevisionGUID="cec6b48c-279f-4805-8c05-3216ed849d6a" CreationDate="2017-07-11T07:47:50.980" UserId="294" Text="tools to reconcile experimental transcripts with reference annotation" />
  <row Id="4287" PostHistoryTypeId="3" PostId="2036" RevisionGUID="cec6b48c-279f-4805-8c05-3216ed849d6a" CreationDate="2017-07-11T07:47:50.980" UserId="294" Text="&lt;alignment&gt;&lt;transcriptome&gt;" />
  <row Id="4288" PostHistoryTypeId="6" PostId="2036" RevisionGUID="886d9197-eaa8-4fe0-bb26-11c30850a8c0" CreationDate="2017-07-11T07:51:15.320" UserId="77" Comment="edited tags" Text="&lt;alignment&gt;&lt;transcriptome&gt;&lt;software-recommendation&gt;" />
  <row Id="4290" PostHistoryTypeId="5" PostId="2036" RevisionGUID="96110ced-b0ab-4fea-8211-ecfe09d0479f" CreationDate="2017-07-11T08:30:36.360" UserId="294" Comment="added 967 characters in body" Text="Looking for tools to reconcile alignment file of experimental transcripts mapped to genome (SAM/BAM) with the reference transcriptome annotation (GTF) from Ensembl (organism: *D. melanogaster*). &#xD;&#xA;&#xD;&#xA;The aim would be to check which transcripts reported in the alignment file are also reported in the reference annotation, generating a new annotation (e.g., GTF) including the new transcripts as seen in the alignment file (experimental transcripts to genome).&#xD;&#xA;&#xD;&#xA;Any idea?&#xD;&#xA;&#xD;&#xA;-- Edit: &#xD;&#xA;&#xD;&#xA;Up to now, I tried an in-house script that does the following: &#xD;&#xA;&#xD;&#xA; 1. generate a BED file from the BAM file with the exon coordinates (start/end) per transcript - using [BEDOPS v2p4p27][1]&#xD;&#xA; 2. convert the reference GTF to BED to retrieve the coordinates per annotated feature &#xD;&#xA; 3. compare the experimental vs the reference BED intervals&#xD;&#xA; 4. infer the suite of annotated exons per experimental transcript, thus, ideally, the isoform it corresponds to&#xD;&#xA;&#xD;&#xA;However, with this simple approach, I only get around 20% of the experimental transcripts that can be identified in all their length using reference annotation. This might be due either to a too simple approach (e.g. if the start/end of the exon in the experimental transcript is shifted by 1 base as compared to the reference, it won't be detected), or to a real biological signal (since I am also using data from long-read sequencing technologies).&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/bedops/bedops" />
  <row Id="4291" PostHistoryTypeId="2" PostId="2038" RevisionGUID="486db17a-5442-4d61-aef3-4a8c9b3291e7" CreationDate="2017-07-11T08:44:12.137" UserId="1092" Text="As per my answer to @_julien_roux on twitter:&#xD;&#xA;&#xD;&#xA;Trying to find novel transcripts within the context of an existing annotation is much less straightforward. You probably need to do a &quot;genome-guided assembly&quot; with Trinity and PASA:&#xD;&#xA;http://pasapipeline.github.io/#A_ComprehensiveTranscriptome&#xD;&#xA;&#xD;&#xA;We did something similar in much simpler organisms in our recent paper: &#xD;&#xA;http://dx.doi.org/10.1186/s12864-017-3505-0&#xD;&#xA;&#xD;&#xA;Your biggest problems are going to be alternative spliced gene models, the large amount of fragmentation inherent in de novo transcriptome assembly and largely underreported non-coding RNAs in most genomes. It was easier for us as the dictyostelids have few and small introns - fly is going to be a different ball-game. Be prepared to spend a lot of time optimising parameters before finding a result your happy with (maybe) ;)&#xD;&#xA;&#xD;&#xA;You may think the fly genome pretty good and complete, but be prepared to find lots of errors, corrections and novelty. We were very surprised at the amount of new annotation we were able to find in *D. discoideum* - an organism where every single gene had been manually curated!&#xD;&#xA;&#xD;&#xA;Good luck!" />
  <row Id="4292" PostHistoryTypeId="2" PostId="2039" RevisionGUID="fe77ad4e-f780-41c0-804d-d77e5331a479" CreationDate="2017-07-11T08:44:31.813" UserId="77" Text="I've never tried this myself, so I don't know how easy this is...&#xD;&#xA;&#xD;&#xA;One option would be to start with [GMAP](http://research-pub.gene.com/gmap/), which is meant to align whole transcripts against the genome. The really nice thing about this is that it can directly produce GFF3 files. You can then use that with your Ensembl GTF with `cuffcompare` or whatever the equivalent is in stringTie. You should then be able to get the information you want from the [transfrag codes](http://cole-trapnell-lab.github.io/cufflinks/cuffcompare/#transfrag-class-codes)." />
  <row Id="4293" PostHistoryTypeId="5" PostId="2034" RevisionGUID="a3e32225-5564-48a6-b794-bbb92a103495" CreationDate="2017-07-11T08:49:31.670" UserId="298" Comment="deleted 2 characters in body; edited title" Text="I am trying to reorder scaffolds of a rice species, but no genetic map is available right now. *Oryza sativa Japonica* is a close relative of this rice species. Mummer was used to do a whole genome alignment, and I am trying to reorder scaffolds according the mummer result. But I found it's too complicated to do this by myself, many scaffolds may align to multiple Oryza sativa Japonica chromosomes or multiple loci in the same chromosome.  &#xD;&#xA;&#xD;&#xA;What software can I use to handle this task?" />
  <row Id="4294" PostHistoryTypeId="4" PostId="2034" RevisionGUID="a3e32225-5564-48a6-b794-bbb92a103495" CreationDate="2017-07-11T08:49:31.670" UserId="298" Comment="deleted 2 characters in body; edited title" Text="Reordering scaffolds according to a reference without a genetic map" />
  <row Id="4296" PostHistoryTypeId="5" PostId="2038" RevisionGUID="5d748326-136b-45d7-bd67-f9678e510e0d" CreationDate="2017-07-11T08:52:17.217" UserId="298" Comment="added 3 characters in body" Text="As per my answer to @_julien_roux on twitter:&#xD;&#xA;&#xD;&#xA;Trying to find novel transcripts within the context of an existing annotation is much less straightforward. You probably need to do a &quot;genome-guided assembly&quot; with Trinity and PASA:&#xD;&#xA;http://pasapipeline.github.io/#A_ComprehensiveTranscriptome&#xD;&#xA;&#xD;&#xA;We did something similar in much simpler organisms in our recent paper: &#xD;&#xA;http://dx.doi.org/10.1186/s12864-017-3505-0&#xD;&#xA;&#xD;&#xA;Your biggest problems are going to be alternative spliced gene models, the large amount of fragmentation inherent in de novo transcriptome assembly and largely underreported non-coding RNAs in most genomes. It was easier for us as the dictyostelids have few and small introns - fly is going to be a different ball-game. Be prepared to spend a lot of time optimising parameters before finding a result you're happy with (maybe) ;)&#xD;&#xA;&#xD;&#xA;You may think the fly genome pretty good and complete, but be prepared to find lots of errors, corrections and novelty. We were very surprised at the amount of new annotations we were able to find in *D. discoideum* - an organism where every single gene had been manually curated!&#xD;&#xA;&#xD;&#xA;Good luck!" />
  <row Id="4297" PostHistoryTypeId="5" PostId="2038" RevisionGUID="644cc7f1-0646-4169-b21b-17bd25747fdb" CreationDate="2017-07-11T09:01:26.063" UserId="1092" Comment="&quot;new annotation&quot; is correct as originally written." Text="As per my answer to @_julien_roux on twitter:&#xD;&#xA;&#xD;&#xA;Trying to find novel transcripts within the context of an existing annotation is much less straightforward. You probably need to do a &quot;genome-guided assembly&quot; with Trinity and PASA:&#xD;&#xA;http://pasapipeline.github.io/#A_ComprehensiveTranscriptome&#xD;&#xA;&#xD;&#xA;We did something similar in much simpler organisms in our recent paper: &#xD;&#xA;http://dx.doi.org/10.1186/s12864-017-3505-0&#xD;&#xA;&#xD;&#xA;Your biggest problems are going to be alternative spliced gene models, the large amount of fragmentation inherent in de novo transcriptome assembly and largely underreported non-coding RNAs in most genomes. It was easier for us as the dictyostelids have few and small introns - fly is going to be a different ball-game. Be prepared to spend a lot of time optimising parameters before finding a result you're happy with (maybe) ;)&#xD;&#xA;&#xD;&#xA;You may think the fly genome pretty good and complete, but be prepared to find lots of errors, corrections and novelty. We were very surprised at the amount of new annotation we were able to find in *D. discoideum* - an organism where every single gene had been manually curated!&#xD;&#xA;&#xD;&#xA;Good luck!" />
  <row Id="4298" PostHistoryTypeId="2" PostId="2040" RevisionGUID="2e8697b7-acf2-442a-a412-81c08898e83e" CreationDate="2017-07-11T09:10:06.243" UserId="1092" Text="Have you looked at Ensembl? At least for mice, they have pretty rich data for SNPs e.g.&#xD;&#xA;http://www.ensembl.org/Mus_musculus/Variation/Explore?db=core;r=2:3225281-3226281;v=rs27096498;vdb=variation;vf=802773&#xD;&#xA;&#xD;&#xA;Assumes you have known dbSNP ids (or 'rs' numbers) for your SNPs. If not you could use their VEP tool to annotate them:&#xD;&#xA;http://www.ensembl.org/Homo_sapiens/Tools/VEP?db=core&#xD;&#xA;&#xD;&#xA;VEP is also available for rice and Arabidopsis:&#xD;&#xA;http://plants.ensembl.org/Oryza_sativa/Tools/VEP?db=core&#xD;&#xA;" />
  <row Id="4299" PostHistoryTypeId="2" PostId="2041" RevisionGUID="dc68f2bc-09ed-4251-95f5-6fa31e272a52" CreationDate="2017-07-11T09:18:11.997" UserId="1092" Text="By far my preferred option for doing this manually is PICR:&#xD;&#xA;http://www.ebi.ac.uk/Tools/picr/&#xD;&#xA;&#xD;&#xA;BTW it is not &quot;ridiculous&quot; to get different numbers of genes reported for a given set of proteins. For several reasons:&#xD;&#xA;&#xD;&#xA; 1. Uniprot IDs can disappear, merge or be split&#xD;&#xA; 2. not all uniprot and gene IDs have a 1-to-1 relationship&#xD;&#xA; 3. depending on species some gene symbols can be ambiguous or synonymous.&#xD;&#xA;" />
  <row Id="4300" PostHistoryTypeId="5" PostId="2035" RevisionGUID="486daf47-bec0-4cbc-8327-a997d6e5ca34" CreationDate="2017-07-11T09:55:52.823" UserId="982" Comment="added 311 characters in body" Text="Have you tried [Mauve][1] alignment? Its pretty easy once you become familiar and has a GUI for further ease of use. Additionally there are a few online tutorials on how to re-order contigs/ scaffolds using this software. Heres [one I use][2] when in need.&#xD;&#xA;&#xD;&#xA;Mauve contains a function called [Mauve Contig Mover (MCM)][3] which can be used to a) compare an assembly to a reference and/or b) rearrange contigs to improve assembly quality.   &#xD;&#xA;&#xD;&#xA;I use the tutorial as above which uses the GUI for Mauve. I&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://darlinglab.org/mauve/mauve.html&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2723005/&#xD;&#xA;  [3]: http://darlinglab.org/mauve/user-guide/reordering.html" />
  <row Id="4301" PostHistoryTypeId="10" PostId="46" RevisionGUID="0d8e515e-cc83-4588-b668-851657c1e8dc" CreationDate="2017-07-11T10:57:28.610" UserId="77" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:492,&quot;DisplayName&quot;:&quot;wkretzsch&quot;},{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;}]}" />
  <row Id="4305" PostHistoryTypeId="2" PostId="2043" RevisionGUID="a2b48276-e0c8-42bc-8259-8ac51999037d" CreationDate="2017-07-11T13:10:56.437" UserId="104" Text="BioPython's `.count()` methods, like Python's `str.count()`, perform a non-overlapping count, how can I do an overlapping one?&#xD;&#xA;&#xD;&#xA;For example, these code snippets `return 2`, but I want the answer `3`:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; from Bio.Seq import Seq&#xD;&#xA;    &gt;&gt;&gt; Seq('AAAA').count('AA')&#xD;&#xA;    2&#xD;&#xA;    &gt;&gt;&gt; 'AAAA'.count('AA')&#xD;&#xA;    2&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4306" PostHistoryTypeId="1" PostId="2043" RevisionGUID="a2b48276-e0c8-42bc-8259-8ac51999037d" CreationDate="2017-07-11T13:10:56.437" UserId="104" Text="How can I do an overlapping sequence count in Biopython?" />
  <row Id="4307" PostHistoryTypeId="3" PostId="2043" RevisionGUID="a2b48276-e0c8-42bc-8259-8ac51999037d" CreationDate="2017-07-11T13:10:56.437" UserId="104" Text="&lt;biopython&gt;&lt;python&gt;" />
  <row Id="4308" PostHistoryTypeId="2" PostId="2044" RevisionGUID="fa1bccc9-40d6-4f9c-b016-dea5d6abd0f2" CreationDate="2017-07-11T13:10:56.437" UserId="104" Text="For Biopython 1.70, there is a new `Seq.count_overlap()` method, which includes optional `start` and `end` arguments:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; from Bio.Seq import Seq&#xD;&#xA;    &gt;&gt;&gt; Seq('AAAA').count_overlap('AA')&#xD;&#xA;    3&#xD;&#xA;    &gt;&gt;&gt; Seq('AAAA').count_overlap('AA', 1, 4)&#xD;&#xA;    2&#xD;&#xA;&#xD;&#xA;This method is also implemented for the `MutableSeq` and `UnknownSeq` classes:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; from Bio.Seq import MutableSeq, UnknownSeq&#xD;&#xA;    &gt;&gt;&gt; MutableSeq('AAAA').count_overlap('AA')&#xD;&#xA;    3&#xD;&#xA;    &gt;&gt;&gt; UnknownSeq(4, character='A').count_overlap('AA')&#xD;&#xA;    3&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;Disclaimer: I co-contributed the `.count_overlap()` methods with Peter Cock, see [`97709cc`][1]&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/biopython/biopython/commit/97709cc7a4b8591e794b442796d41e4f7b855a0f&#xD;&#xA;" />
  <row Id="4310" PostHistoryTypeId="2" PostId="2045" RevisionGUID="bdf9be10-ed0b-49c7-a28b-28283c0c9949" CreationDate="2017-07-11T15:01:38.167" UserId="1092" Text="Short answer: you can't.&#xD;&#xA;&#xD;&#xA;Neural networks use positive and negative examples to add weights to the neural network architecture that is provided to it. Trying to deconvolve the meaning behind the weights is nigh-on impossible except for the simplest perceptron.&#xD;&#xA;&#xD;&#xA;This is common to many machine learning algorithms: they work very much like black boxes. One exception is decision trees. They will report what features are used to classify the positive vs negative datasets.&#xD;&#xA;&#xD;&#xA;However, as with all ML methods, you need to be very careful of your training datasets. This is especially true of motif searching and especially for negative datasets. It's quite easy to find a negative dataset which is completely inappropriate for learning and give you misleading accuracies.&#xD;&#xA;&#xD;&#xA;Like @user172818, I would try traditional methods as they can work well, again, if given appropriate data. The [MEME-suite][1] would be a good start.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://meme-suite.org/" />
  <row Id="4311" PostHistoryTypeId="2" PostId="2046" RevisionGUID="a211abab-d1d9-45e2-87fa-69e38371989b" CreationDate="2017-07-11T15:15:50.387" UserId="931" Text="I've been using [ATAC][1] to successfully align different versions of the rice genome assembly. There is a chance it will work for you between two closely related species (potato and tomato work well for example).&#xD;&#xA;&#xD;&#xA;It's not massively user friendly, but it's very fast, and it produces long, 'clean' alignments (i.e. almost all 1 to 1 alignments). The output format is a bit cryptic (zero-based start position and length instead of the more usual 1-based start and end positions), but as I said, it should basically output the order for you in about 30 minutes.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://kmer.sourceforge.net/wiki/index.php/Getting_Started_with_ATAC" />
  <row Id="4316" PostHistoryTypeId="33" PostId="1007" RevisionGUID="e89dd872-9fd1-4e9b-9b1a-2a461a650858" CreationDate="2017-07-11T15:28:03.450" UserId="931" Comment="2" />
  <row Id="4320" PostHistoryTypeId="5" PostId="1001" RevisionGUID="fd880838-ad51-4eb9-b84e-f93beb8c93f8" CreationDate="2017-07-11T16:47:59.913" UserId="982" Comment="added 141 characters in body" Text="I have a core genome dataset of approx 2530 genes for 149 taxa. I have run an unpartitioned phylogenetic analysis using iqtree. But I am unhappy with the resolution in some of the phylogeny. I want to check if partitioning the alignment  will improve resolution. My alignment is from roary output and MAFFT alignment.&#xD;&#xA;&#xD;&#xA;Ive tried partitioning by loci and using iqtree (beta 1.6.4) to determine a model of best fit for each but this is apparently a very long demanding process. What are other good methods/ means to partition DNA data?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;My next course of action is to remove uninformative sites. Id rather that this didnt chop up individual genes. Is it possible to observe the identity in each partition and remove genes which are 100% identical?&#xD;&#xA;iqtree does a quick analysis of each partition and reveals which are parsimony uninformative sites. Would I be just as well to delete these partitions?&#xD;&#xA;&#xD;&#xA;My current iqtree command is as follows:&#xD;&#xA;&#xD;&#xA;    iqtree -nt AUTO  -s core_gene_alignment.aln -spp loci.nex  -m TESTMERGE -rcluster-max 100&#xD;&#xA;" />
  <row Id="4321" PostHistoryTypeId="2" PostId="2048" RevisionGUID="4038834c-668c-48a5-a46d-7dca7f7c3cdb" CreationDate="2017-07-11T20:06:56.393" UserId="1025" Text="I'm trying to link GEOquery and minfi. &#xD;&#xA;&#xD;&#xA;Specifically I want to obtain beta values from the idat files on GEOquery. I was following this guide: https://kasperdanielhansen.github.io/genbioconductor/html/minfi.html, up until the preprocessing part. Meaning that I was able to obtain the RGset. Then I used my own preprocessing code to obtain the beta values. However, I cross checked them with the beta values that were on GEO and they weren't consistent.&#xD;&#xA;&#xD;&#xA;For example, the accession number I used was GSE68777. So I went to that study on GEO and clicked on the first sample: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM1681154. Then I scrolled down and clicked &quot;Download full table&quot; to download the samples and beta values in a text file. &#xD;&#xA;&#xD;&#xA;Then I typed head(beta) and chose the first sample. Then I did command F in the text file for that sample and it's value there wasn't the same as the value from the beta data table. Hopefully you can help find the error.&#xD;&#xA;&#xD;&#xA;Here is the code I'm using:&#xD;&#xA;&#xD;&#xA;    library(GEOquery)&#xD;&#xA;    library(minfi)&#xD;&#xA;    library(&quot;IlluminaHumanMethylation450kanno.ilmn12.hg19&quot;)&#xD;&#xA;    library(&quot;IlluminaHumanMethylation450kmanifest&quot;)&#xD;&#xA;    &#xD;&#xA;    ######## Code copied from 450k Guide ########&#xD;&#xA;&#xD;&#xA;    getGEOSuppFiles(&quot;GSE68777&quot;)&#xD;&#xA;    untar(&quot;GSE68777/GSE68777_RAW.tar&quot;, exdir = &quot;GSE68777/idat&quot;)&#xD;&#xA;    head(list.files(&quot;GSE68777/idat&quot;, pattern = &quot;idat&quot;))&#xD;&#xA;    idatFiles &lt;- list.files(&quot;GSE68777/idat&quot;, pattern = &quot;idat.gz$&quot;, full = TRUE)&#xD;&#xA;    sapply(idatFiles, gunzip, overwrite = TRUE)&#xD;&#xA;    rgSet &lt;- read.metharray.exp(&quot;GSE68777/idat&quot;)&#xD;&#xA;    geoMat &lt;- getGEO(&quot;GSE68777&quot;)&#xD;&#xA;    pD.all &lt;- pData(geoMat[[1]])&#xD;&#xA;    pD &lt;- pD.all[, c(&quot;title&quot;, &quot;geo_accession&quot;, &quot;characteristics_ch1.1&quot;, &quot;characteristics_ch1.2&quot;)]&#xD;&#xA;    names(pD)[c(3,4)] &lt;- c(&quot;group&quot;, &quot;sex&quot;)&#xD;&#xA;    pD$group &lt;- sub(&quot;^diagnosis: &quot;, &quot;&quot;, pD$group)&#xD;&#xA;    pD$sex &lt;- sub(&quot;^Sex: &quot;, &quot;&quot;, pD$sex)&#xD;&#xA;    sampleNames(rgSet) &lt;- sub(&quot;.*_5&quot;, &quot;5&quot;, sampleNames(rgSet))&#xD;&#xA;    rownames(pD) &lt;- pD$title&#xD;&#xA;    pD &lt;- pD[sampleNames(rgSet),]&#xD;&#xA;    pData(rgSet) &lt;- pD&#xD;&#xA;    &#xD;&#xA;    ######## My own preprocessing code ########&#xD;&#xA;    &#xD;&#xA;    rgSet &lt;- updateObject(rgSet)&#xD;&#xA;    &#xD;&#xA;    #RGChannelSet to MethylSet&#xD;&#xA;    MSet &lt;- preprocessIllumina(rgSet, bg.correct = TRUE, normalize = &quot;controls&quot;)&#xD;&#xA;    qc &lt;- getQC(MSet)&#xD;&#xA;    plotQC(qc)&#xD;&#xA;    &#xD;&#xA;    #MethylSet to GenomicMethylSet&#xD;&#xA;    RSet &lt;- ratioConvert(MSet, what = &quot;both&quot;, keepCN = TRUE)&#xD;&#xA;    RSet &lt;- updateObject(RSet)&#xD;&#xA;    &#xD;&#xA;    #GenomicMethylSet to GenomicRatioSet&#xD;&#xA;    GRset &lt;- mapToGenome(RSet)&#xD;&#xA;    snps &lt;- getSnpInfo(GRset)&#xD;&#xA;    GRset &lt;- addSnpInfo(GRset)&#xD;&#xA;    GRset &lt;- dropLociWithSnps(GRset, snps=c(&quot;SBE&quot;,&quot;CpG&quot;), maf=0)&#xD;&#xA;    GRset &lt;- keepSeqlevels(GRset, paste0('chr', 1:22), pruning.mode = &quot;coarse&quot;)&#xD;&#xA;    &#xD;&#xA;    beta &lt;- getBeta(GRset)" />
  <row Id="4322" PostHistoryTypeId="1" PostId="2048" RevisionGUID="4038834c-668c-48a5-a46d-7dca7f7c3cdb" CreationDate="2017-07-11T20:06:56.393" UserId="1025" Text="Minfi returning incorrect beta values" />
  <row Id="4323" PostHistoryTypeId="3" PostId="2048" RevisionGUID="4038834c-668c-48a5-a46d-7dca7f7c3cdb" CreationDate="2017-07-11T20:06:56.393" UserId="1025" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;normalization&gt;&lt;methylation&gt;&lt;data-mining&gt;" />
  <row Id="4324" PostHistoryTypeId="2" PostId="2049" RevisionGUID="7a77080a-fc29-407f-9edc-0e0b6718d631" CreationDate="2017-07-11T21:14:20.717" UserId="727" Text="my goal is to do a profile alignment of one protein sequence against the BLAST results for this protein, and to visualize the results.&#xD;&#xA;&#xD;&#xA;So far, I have a FASTA file of the protein, and a FASTA file with the sequences of the proteins from the BLAST results.&#xD;&#xA;&#xD;&#xA;I have downloaded clustalx and run a profile alignment using the single protein sequence as Profile 1 and the sequences from the BLAST results as Profile 2, and selected the option &quot;Align Sequences to Profile 1&quot;.&#xD;&#xA;&#xD;&#xA;I'm having difficulty interpreting and dealing with the results. Since the majority of the sequences aren't aligned, the results are a total mess, with a huge number of dashes added.&#xD;&#xA;&#xD;&#xA;My goal is to visualize the number of BLAST hits per amino acid in my protein of interest. &#xD;&#xA;&#xD;&#xA;ie: ![alignment](https://i.stack.imgur.com/L8jL5.jpg)&#xD;&#xA;&#xD;&#xA;I would like to have a graph with my protein on the x-axis and a plot like the regions of high conservation for the multiple alignments, except with the spikes corresponding to a high number of BLAST hits.&#xD;&#xA;&#xD;&#xA;Is there a better way to achieve this or to salvage the results from the profile alignment?&#xD;&#xA;&#xD;&#xA;Thanks." />
  <row Id="4325" PostHistoryTypeId="1" PostId="2049" RevisionGUID="7a77080a-fc29-407f-9edc-0e0b6718d631" CreationDate="2017-07-11T21:14:20.717" UserId="727" Text="Profile alignment: 90+ sequences against one sequence" />
  <row Id="4326" PostHistoryTypeId="3" PostId="2049" RevisionGUID="7a77080a-fc29-407f-9edc-0e0b6718d631" CreationDate="2017-07-11T21:14:20.717" UserId="727" Text="&lt;sequence-alignemnt&gt;" />
  <row Id="4327" PostHistoryTypeId="2" PostId="2050" RevisionGUID="752afd97-c422-472d-9a00-e044f8cd630a" CreationDate="2017-07-11T21:19:03.517" UserId="77" Text="If you don't mind hitting it 50k times and are OK with python3...&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    from urllib import request&#xD;&#xA;    import json&#xD;&#xA;&#xD;&#xA;    def getPathways(proteinID):&#xD;&#xA;        baseURL = 'http://reactome.org/ContentService/data/query'&#xD;&#xA;        PathwayIDs = set()&#xD;&#xA;        try:&#xD;&#xA;            response = request.urlopen('{}/{}'.format(baseURL, proteinID)).read().decode()&#xD;&#xA;            data = json.loads(response)&#xD;&#xA;            if 'consumedByEvent' in data:&#xD;&#xA;                for event in data['consumedByEvent']:&#xD;&#xA;                    PathwayIDs.add(event['stId'])&#xD;&#xA;            if 'producedByEvent' in data:&#xD;&#xA;                for event in data['producedByEvent']:&#xD;&#xA;                    PathwayIDs.add(event['stId'])&#xD;&#xA;        except:&#xD;&#xA;            pass&#xD;&#xA;        return PathwayIDs&#xD;&#xA;&#xD;&#xA;Usage would then be something like:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    l = ['R-HSA-49155', 'R-HSA-199420', '']&#xD;&#xA;    for rid in l:&#xD;&#xA;        ids = getPathways(rid)&#xD;&#xA;        for _ in ids:&#xD;&#xA;            print(&quot;{}\t{}&quot;.format(rid, _))&#xD;&#xA;&#xD;&#xA;Which would produce:&#xD;&#xA;&#xD;&#xA;    R-HSA-49155	R-HSA-110239&#xD;&#xA;    R-HSA-49155	R-HSA-110240&#xD;&#xA;    R-HSA-49155	R-HSA-110238&#xD;&#xA;    R-HSA-49155	R-HSA-110356&#xD;&#xA;    R-HSA-199420	R-HSA-8948800&#xD;&#xA;    R-HSA-199420	R-HSA-6807106&#xD;&#xA;    R-HSA-199420	R-HSA-6807206&#xD;&#xA;    R-HSA-199420	R-HSA-6807126&#xD;&#xA;    R-HSA-199420	R-HSA-8847968&#xD;&#xA;    R-HSA-199420	R-HSA-8850997&#xD;&#xA;    R-HSA-199420	R-HSA-2321904&#xD;&#xA;    R-HSA-199420	R-HSA-8948775&#xD;&#xA;    R-HSA-199420	R-HSA-8944497&#xD;&#xA;    R-HSA-199420	R-HSA-6807134&#xD;&#xA;    R-HSA-199420	R-HSA-8850945&#xD;&#xA;    R-HSA-199420	R-HSA-8873946&#xD;&#xA;&#xD;&#xA;Note that this will silently ignore invalid or missing IDs such as `''` (that's the `try` and `except` above. Note also that these are different pathway IDs than what you provided in your example. The main reason is that the protein IDs you showed are not always involved in the pathways IDs you showed (in my example, they always are)." />
  <row Id="4328" PostHistoryTypeId="2" PostId="2051" RevisionGUID="3e46cf10-9594-4383-a26b-5f00cf6f9c76" CreationDate="2017-07-11T22:01:38.863" UserId="240" Text="I'm looking at RRBS data from ENCODE, and to align the FASTQ files I've used Bismark with Bowtie 1. When I load the resulting BAM file into `R` with `Rsamtools` and `GenomicRanges`, I get something like this:&#xD;&#xA;&#xD;&#xA;    &gt; reads&#xD;&#xA;    GRanges object with 2 ranges and 5 metadata columns:&#xD;&#xA;          seqnames         ranges strand |                     seq                                   XM          XR          XG        NM&#xD;&#xA;             &lt;Rle&gt;      &lt;IRanges&gt;  &lt;Rle&gt; |          &lt;DNAStringSet&gt;                          &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;integer&gt;&#xD;&#xA;      [1]     chrM [16523, 16558]      - | ATAAAACCTA...CCCTTAAATA .....h........h.......z.............          CT          GA         3&#xD;&#xA;      [2]     chrM [16524, 16559]      + | TAAAGTTTAA...TTTTAAATAA .....hh.......hhh.h.z...hhhh........          CT          CT        11&#xD;&#xA;      -------&#xD;&#xA;      seqinfo: 25 sequences from an unspecified genome&#xD;&#xA;&#xD;&#xA;For the read on the - strand (which is aligned to the &quot;G-&gt;A&quot;-converted reference), how should I compare methylation calls to those on the + strand, since they don't line up?&#xD;&#xA;&#xD;&#xA;For example, the `z` in the methylation string of the - strand is one position ahead of the `z` in the + strand (which makes sense because of symmetric CpG methylation). But how should I determine whether these two methylation calls are &quot;essentially the same&quot; or not?" />
  <row Id="4329" PostHistoryTypeId="1" PostId="2051" RevisionGUID="3e46cf10-9594-4383-a26b-5f00cf6f9c76" CreationDate="2017-07-11T22:01:38.863" UserId="240" Text="How to interpret methylation calls from Bismark on opposite strands?" />
  <row Id="4330" PostHistoryTypeId="3" PostId="2051" RevisionGUID="3e46cf10-9594-4383-a26b-5f00cf6f9c76" CreationDate="2017-07-11T22:01:38.863" UserId="240" Text="&lt;r&gt;&lt;samtools&gt;&lt;methylation&gt;" />
  <row Id="4331" PostHistoryTypeId="2" PostId="2052" RevisionGUID="b95bc34e-b8bb-4053-affa-40b4f5801f8e" CreationDate="2017-07-11T22:22:01.657" UserId="77" Text="The strand the bismark reports is related to the strand from which the read originated, not necessarily how it's aligned. So, alignments on the + strand shouldn't have calls overlapping those on the - strand, since you can't have a C in the same place on the same strand. One should often see Z/z next to each other on opposite strand, like in your example, since these are CpG (so it should be a Z/z on the + strand and then a Z/z on the following base on the - strand). Thus, in your example you have two reads supporting unmethylation for the CpG as a whole (1 read for each of the Cs).&#xD;&#xA;&#xD;&#xA;The most confusing thing about BSseq is that reads (or paired-end reads) are only ever informative for a single strand. This is actually true for all sequencing with Illumina instruments (single-stranded fragments are loaded, after all), but since strands are almost always complementary we can usually ignore this fact.&#xD;&#xA;&#xD;&#xA;As an aside, you might find [MethylDackel](https://github.com/dpryan79/MethylDackel) useful (full disclosure, I wrote it). It'll be much faster at extracting methylation calls than bismark and supports nice things like excluding regions of bias methylation and likely variant positions." />
  <row Id="4332" PostHistoryTypeId="50" PostId="623" RevisionGUID="1fff4db3-3d8f-4103-8f98-7df37c16393a" CreationDate="2017-07-11T22:25:48.230" UserId="-1" />
  <row Id="4333" PostHistoryTypeId="2" PostId="2053" RevisionGUID="ddd9dee0-07ec-468e-821f-501f50a8367c" CreationDate="2017-07-12T03:18:23.897" UserId="1085" Text="I've encountered this problem before, and used python `re` module to solve this problem.&#xD;&#xA;&#xD;&#xA;    import re&#xD;&#xA;    all = re.findall(r'?=(AA)','AAAA')&#xD;&#xA;    counts = len(all)&#xD;&#xA;&#xD;&#xA;You can get more details in this [thread](https://stackoverflow.com/questions/11430863/how-to-find-overlapping-matches-with-a-regexp)" />
  <row Id="4334" PostHistoryTypeId="5" PostId="2053" RevisionGUID="f8ce8869-8444-4d50-928e-aeeb7e5921fc" CreationDate="2017-07-12T05:17:53.587" UserId="1085" Comment="added 2 characters in body" Text="I've encountered this problem before, and used python `re` module to solve this problem.&#xD;&#xA;&#xD;&#xA;    import re&#xD;&#xA;    all = re.findall(r'(?=(AA))','AAAA')&#xD;&#xA;    counts = len(all)&#xD;&#xA;&#xD;&#xA;You can get more details in this [thread](https://stackoverflow.com/questions/11430863/how-to-find-overlapping-matches-with-a-regexp)" />
  <row Id="4335" PostHistoryTypeId="2" PostId="2054" RevisionGUID="9ae61286-4b77-4a94-ab0c-0b9bf6559717" CreationDate="2017-07-12T08:14:19.597" UserId="73" Text="I have recently generated a genome-guided transcriptome with [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki), and would like to apply an additional filter to exclude transcripts that don't have good support from the RNASeq reads. This is with the goal of trying to reduce the initial dataset down to something a bit more manageable (I have about 300k transcripts covering 250Mb in total, but would prefer about 1/10 of that number).&#xD;&#xA;&#xD;&#xA;I see that Trinity has a [workflow using Bowtie2](https://github.com/trinityrnaseq/trinityrnaseq/wiki/RNA-Seq-Read-Representation-by-Trinity-Assembly) for evaluating the quality of assembled transcripts, but Bowtie2 is getting a bit old in the mapping world now. Are any of the super-fast transcript mappers (e.g. [Salmon](https://combine-lab.github.io/salmon/) or [Kallisto](https://pachterlab.github.io/kallisto/about)) appropriate to use for transcript filtering?" />
  <row Id="4336" PostHistoryTypeId="1" PostId="2054" RevisionGUID="9ae61286-4b77-4a94-ab0c-0b9bf6559717" CreationDate="2017-07-12T08:14:19.597" UserId="73" Text="Filter Trinity transcriptome based on RNASeq reads" />
  <row Id="4337" PostHistoryTypeId="3" PostId="2054" RevisionGUID="9ae61286-4b77-4a94-ab0c-0b9bf6559717" CreationDate="2017-07-12T08:14:19.597" UserId="73" Text="&lt;rna-seq&gt;&lt;read-mapping&gt;" />
  <row Id="4338" PostHistoryTypeId="2" PostId="2055" RevisionGUID="e9075e53-192a-4f0a-a917-656c8e1d8624" CreationDate="2017-07-12T08:41:00.213" UserId="73" Text="I used a combination of BUSCO and Salmon to filter transcripts based on their abundance in the RNASeq read dataset. The approach was roughly as follows:&#xD;&#xA;&#xD;&#xA; 1. Run BUSCO in short/transcript mode on the Trinity-generated sequences&#xD;&#xA; 2. Run Salmon on the Trinity-generated sequences, using the RNASeq reads that were used to generate the transcriptome&#xD;&#xA; 3. Merge the BUSCO full results table with the Salmon results table to determine distribution of the number of reads mapped for BUSCO genes&#xD;&#xA; 4. Choose an appropriate &quot;real&quot; read count threshold based on the read count distribution for BUSCO genes&#xD;&#xA; 5. Sub-select the transcripts based on this threshold, using the Salmon results table" />
  <row Id="4339" PostHistoryTypeId="5" PostId="2055" RevisionGUID="cc21c342-3f8a-42e4-9e41-e79faab6ceaf" CreationDate="2017-07-12T08:54:27.503" UserId="73" Comment="Added example scripts" Text="I used a combination of BUSCO and Salmon to filter transcripts based on their abundance in the RNASeq read dataset. The approach was roughly as follows:&#xD;&#xA;&#xD;&#xA; 1. Run BUSCO in short/transcript mode on the Trinity-generated sequences&#xD;&#xA; 2. Run Salmon on the Trinity-generated sequences, using the RNASeq reads that were used to generate the transcriptome&#xD;&#xA; 3. Merge the BUSCO full results table with the Salmon results table to determine distribution of the number of reads mapped for BUSCO genes&#xD;&#xA; 4. Choose an appropriate &quot;real&quot; read count threshold based on the read count distribution for BUSCO genes&#xD;&#xA; 5. Sub-select the transcripts based on this threshold, using the Salmon results table&#xD;&#xA;&#xD;&#xA;Step 1 -- BUSCO&#xD;&#xA;--&#xD;&#xA;    python ~/install/busco/BUSCO.py -i ../Trinity-GG.fasta -o BUSCO_Trinity-GG_nematodes -l ~/install/busco/nematoda_odb9 -m tran -c 10&#xD;&#xA;&#xD;&#xA;Step 2 -- Salmon&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;    ~/install/salmon/Salmon-0.8.2_linux_x86_64/bin/salmon index -t Trinity-GG.fasta -i Trinity-GG.fasta.sai&#xD;&#xA;    ~/install/salmon/Salmon-0.8.2_linux_x86_64/bin/salmon quant -i Trinity-GG.fasta.sai -1 reads-all_R1_trimmed.fastq.gz -2 reads-all_R2_trimmed.fastq.gz -p 10 -o quant/reads-all_quant -l A&#xD;&#xA;&#xD;&#xA;Step 3 -- Results merge (using R)&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/Rscript&#xD;&#xA;    busco.table &lt;-&#xD;&#xA;        read.delim(&quot;BUSCO/run_BUSCO_Trinity-GG_nematodes/full_table_BUSCO_T-BNOCFED_nematodes.tsv&quot;, comment.char=&quot;#&quot;,&#xD;&#xA;                   header=FALSE, stringsAsFactors = FALSE,&#xD;&#xA;                   col.names=c(&quot;Busco id&quot;,&quot;Status&quot;,&quot;Sequence&quot;,&quot;Score&quot;,&quot;Length&quot;));&#xD;&#xA;    busco.table &lt;- subset(busco.table, Status != &quot;Missing&quot;);&#xD;&#xA;    &#xD;&#xA;    count.table &lt;- read.delim(&quot;quant/reads-all_quant_ISR/quant.sf&quot;,&#xD;&#xA;                              stringsAsFactors=FALSE);&#xD;&#xA;    &#xD;&#xA;    busco.expr.df &lt;- merge(busco.table, count.table,&#xD;&#xA;                           by.x=&quot;Sequence&quot;, by.y=&quot;Name&quot;, all.x=TRUE);&#xD;&#xA;&#xD;&#xA;Step 4 -- Count thresholding (visual exploration)&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;    pdf(&quot;busco_NumReads.pdf&quot;, paper=&quot;a4r&quot;, width=11, height=8);&#xD;&#xA;    options(scipen=10);&#xD;&#xA;    busco.numReads &lt;- sort(tapply(busco.expr.df$NumReads, busco.expr.df$Busco.id,&#xD;&#xA;                                  function(x){&#xD;&#xA;                                      exp(mean(log(x[x&gt;0])))}), decreasing=TRUE);&#xD;&#xA;    par(mfrow=c(1,2));&#xD;&#xA;    plot(x=seq(0,1,length.out=length(busco.numReads)),&#xD;&#xA;         busco.numReads, log=&quot;y&quot;, ylab=&quot;Number of Mapped Reads&quot;,&#xD;&#xA;         yaxt=&quot;n&quot;,&#xD;&#xA;         xlab=&quot;Proportion of Complete BUSCO sequences&quot;);&#xD;&#xA;    axis(2, at=rep(c(1,2,5),each=6) * 10^(0:5), las=2, cex.axis=0.71);&#xD;&#xA;    abline(col=&quot;#00000020&quot;, h=rep(1:9,each=6) * 10^(0:5), lwd=0.1);&#xD;&#xA;    abline(col=&quot;#80808080&quot;, h=rep(1,each=6) * 10^(0:5), lwd=2);&#xD;&#xA;    rect(ybottom=10^(par(&quot;usr&quot;)[3]), ytop=10^(par(&quot;usr&quot;)[4]),&#xD;&#xA;         xleft=0.9, xright=par(&quot;usr&quot;)[2],&#xD;&#xA;         col=&quot;#80808020&quot;, border=NA);&#xD;&#xA;    plot(x=seq(0,1,length.out=length(busco.numReads)), xlim=c(0.9,1),&#xD;&#xA;         busco.numReads, log=&quot;y&quot;, ylab=&quot;Number of Mapped Reads&quot;, yaxt=&quot;n&quot;,&#xD;&#xA;         xlab=&quot;Proportion of Complete BUSCO sequences&quot;);&#xD;&#xA;    axis(2, at=rep(c(1,2,5),each=6) * 10^(0:5), las=2, cex.axis=0.71);&#xD;&#xA;    abline(col=&quot;#00000020&quot;, h=rep(1:9,each=6) * 10^(0:5), lwd=0.1);&#xD;&#xA;    abline(col=&quot;#80808080&quot;, h=rep(1,each=6) * 10^(0:5), lwd=2);&#xD;&#xA;    invisible(dev.off());&#xD;&#xA;&#xD;&#xA;[![choosing read count threshold from BUSCO genes][1]][1]&#xD;&#xA;&#xD;&#xA;Looks like a value of 50 will be suitable.&#xD;&#xA;&#xD;&#xA;Step 5 -- Sub-select the transcripts&#xD;&#xA;--&#xD;&#xA;&#xD;&#xA;    cat(file=&quot;HighCount_Transcripts_Num50.txt&quot;,&#xD;&#xA;        subset(count.table, NumReads &gt;= 50)$Name, sep=&quot;\n&quot;);&#xD;&#xA;&#xD;&#xA;Then filter using a suitable [FASTA selection utility](https://github.com/gringer/bioinfscripts/blob/master/fastx-fetch.pl):&#xD;&#xA;&#xD;&#xA;    pv Trinity-GG.fasta | ~/scripts/fastx-fetch.pl -i HighCount_Transcripts_Num50.txt &gt; HighCount50_Trinity-GG.fasta&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/7iw47.png&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4340" PostHistoryTypeId="2" PostId="2056" RevisionGUID="f014cb60-71ee-415b-8a58-e548a759b1c6" CreationDate="2017-07-12T09:36:21.243" UserId="180" Text="For DNA/RNA quantification machines like the Bioanalyzer or TapeStation, the DNA Integrity Number (DIN) or RNA Integrity Number (RIN) numbers are quoted as a measure of the fragmentation of the material.&#xD;&#xA;&#xD;&#xA;How is the DNA Integrity Number (DIN) calculated in Bioanalyzer/TapeStation? Is there any open-source software to parse/collate the data from the machines?" />
  <row Id="4341" PostHistoryTypeId="1" PostId="2056" RevisionGUID="f014cb60-71ee-415b-8a58-e548a759b1c6" CreationDate="2017-07-12T09:36:21.243" UserId="180" Text="how is the DNA Integrity Number (DIN) calculated in Bioanalyzer/TapeStation?" />
  <row Id="4342" PostHistoryTypeId="3" PostId="2056" RevisionGUID="f014cb60-71ee-415b-8a58-e548a759b1c6" CreationDate="2017-07-12T09:36:21.243" UserId="180" Text="&lt;dna&gt;&lt;quantification&gt;&lt;bioanalyzer&gt;" />
  <row Id="4343" PostHistoryTypeId="2" PostId="2057" RevisionGUID="9fe70b4e-2050-4f5b-82d3-2d61c70a5247" CreationDate="2017-07-12T09:38:45.987" UserId="180" Text="The best I could find by looking at github.com is:&#xD;&#xA;&#xD;&#xA;https://github.com/brianfleharty/Agilent-BioAnalyzer-RIN-R-code-for-Fly/blob/master/RNA_RIN.R" />
  <row Id="4344" PostHistoryTypeId="5" PostId="2057" RevisionGUID="efd14928-bc88-438d-b35e-6abb5054f218" CreationDate="2017-07-12T10:23:21.807" UserId="298" Comment="Please don't post links as answers. " Text="The best I could find by looking at github.com is:&#xD;&#xA;&#xD;&#xA;https://github.com/brianfleharty/Agilent-BioAnalyzer-RIN-R-code-for-Fly/blob/master/RNA_RIN.R&#xD;&#xA;&#xD;&#xA;------&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    #set the working directory&#xD;&#xA;    dir&lt;- &quot;H:\\B_RIN\\emd&quot;&#xD;&#xA;    setwd(dir)&#xD;&#xA;    myfiles&lt;-list.files()&#xD;&#xA;    which(substr(myfiles,69,79)==&quot;Results.csv&quot;)&#xD;&#xA;    results&lt;-myfiles[which(substr(myfiles,69,79)==&quot;Results.csv&quot;)]&#xD;&#xA;    n &lt;- which(substr(myfiles,69,79)==&quot;Results.csv&quot;)&#xD;&#xA;    targets &lt;- read.delim(myfiles[n[1]], sep=&quot;,&quot;,header=F,skip=14,as.is=T)&#xD;&#xA;    length(n)&#xD;&#xA;    targets2&lt;- read.delim(myfiles[n[1]], sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xD;&#xA;    str1&lt;-substr(targets2[1,2],1,67)&#xD;&#xA;    str1&#xD;&#xA;    str2&lt;-&quot;_S&quot;&#xD;&#xA;    str3&lt;-paste(str1,str2,sep=&quot;&quot;)&#xD;&#xA;    str3&#xD;&#xA;    chip_num&lt;-length(n)&#xD;&#xA;    samp_num_chip&lt;-length(grep(str3,myfiles))&#xD;&#xA;    samp_num&lt;-length(which(substr(myfiles,69,74)==&quot;Sample&quot;))&#xD;&#xA;    samples1&lt;- matrix(NA, ncol=chip_num, nrow=samp_num)&#xD;&#xA;    for(i in 1:chip_num)&#xD;&#xA;    {&#xD;&#xA;    #mod chip numb&#xD;&#xA;    targets2&lt;- read.delim(myfiles[n[i]], sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xD;&#xA;    str1&lt;-substr(targets2[1,2],1,67)&#xD;&#xA;    str2&lt;-&quot;_S&quot;&#xD;&#xA;    str3&lt;-paste(str1,str2,sep=&quot;&quot;)&#xD;&#xA;    samp_num_chip&lt;-length(grep(str3,myfiles))&#xD;&#xA;    #mod sample numb&#xD;&#xA;    str1&lt;-substr(targets2[1,2],1,67)&#xD;&#xA;    str4&lt;-&quot;_Sample&quot;&#xD;&#xA;    for(j in 1:samp_num_chip)&#xD;&#xA;    {&#xD;&#xA;    iii&lt;-j&#xD;&#xA;    end.st&lt;-&quot;.csv&quot;&#xD;&#xA;    str5&lt;-paste(str1,str4,iii,end.st,sep=&quot;&quot;)&#xD;&#xA;    samples1[j,i]&lt;-str5&#xD;&#xA;    }&#xD;&#xA;    }&#xD;&#xA;    samples1&#xD;&#xA;    grep(&quot;2100&quot;,samples1)&#xD;&#xA;    samples1&lt;-samples1[grep(&quot;2100&quot;,samples1)]&#xD;&#xA;    samples1&#xD;&#xA;    write.table(samples1,file=paste(dir,&quot;\\ba_lane.txt&quot;,sep=&quot;&quot;), sep=&quot;\t&quot;,col.names=NA)&#xD;&#xA;    #RIN Calculator#######refined peakfinder####################&#xD;&#xA;    #read bioanalyzer data into a matrix called dta&#xD;&#xA;    #since the total RNA and mRNA assays run differently&#xD;&#xA;    #skip more lines for the total RNA assay&#xD;&#xA;    ramp &lt;- colorRamp(c(&quot;darkmagenta&quot;,&quot;white&quot;))&#xD;&#xA;    elec &lt;- read.delim(&quot;ba_lane.txt&quot;, sep=&quot;\t&quot;,header=T,as.is=T)&#xD;&#xA;    qc.mat &lt;- matrix(NA, ncol=1, nrow=nrow(elec))&#xD;&#xA;    dta &lt;- matrix(NA, nrow=1060, ncol=nrow(elec))&#xD;&#xA;    for(i in 1:nrow(elec))&#xD;&#xA;    {&#xD;&#xA;    x &lt;- read.csv(as.character(elec[i,2]), header=F, skip=18, nrows=1060)&#xD;&#xA;    dta[,i] &lt;- x[,2]&#xD;&#xA;    }&#xD;&#xA;    time&lt;- x[,1]&#xD;&#xA;    #plot ladder and define peak threshold in HD&#xD;&#xA;    par(mfrow=c(3,4))&#xD;&#xA;    for(samples in 1:length(samples1))&#xD;&#xA;    #for(samples in 1:nrow(elec))&#xD;&#xA;    {&#xD;&#xA;    #which(dta[,lad] &gt; 5)&#xD;&#xA;    #draw some segments and store max fluor in max.peaks matrix&#xD;&#xA;    lad&lt;-samples&#xD;&#xA;    ti1&lt;-17&#xD;&#xA;    ti2&lt;-65&#xD;&#xA;    p.row&lt;-length(dta[which(time==ti1):which(time==ti1+1),lad])*(ti2-ti1+1)&#xD;&#xA;    peaks &lt;- matrix(NA, nrow=p.row, ncol=1)&#xD;&#xA;    max.peaks &lt;- matrix(NA, nrow=length(seq(ti1,ti2,.05)), ncol=1)&#xD;&#xA;    for (i in 1:length(seq(ti1,ti2,.05)))&#xD;&#xA;    {&#xD;&#xA;    ii&lt;-seq(ti1,ti2,.05)[i]&#xD;&#xA;    x0&lt;- ii&#xD;&#xA;    x0&lt;-round(x0,digits=3)&#xD;&#xA;    y0&lt;- ii&#xD;&#xA;    x&lt;- ii+.05&#xD;&#xA;    x&lt;-round(x,digits=3)&#xD;&#xA;    y&lt;- ii&#xD;&#xA;    #segments(x0,y0,x,y)&#xD;&#xA;    max.peaks[i]&lt;- max(dta[which(time==x0):which(time==x),lad])&#xD;&#xA;    }    &#xD;&#xA;    #human or mouse&#xD;&#xA;    cent&lt;-max(max.peaks)*.6&#xD;&#xA;    #flatworm&#xD;&#xA;    #cent&lt;-max(max.peaks)*.05&#xD;&#xA;    plot(time,dta[,lad],type=&quot;h&quot;,col=&quot;dodgerblue&quot;,xlab=&quot;seconds&quot;,ylab=&quot;Fluorescence Units&quot;,xlim=c(17,75),ylim=c(0,(max(max.peaks)*1.2)),main=elec[lad,1])&#xD;&#xA;    abline(h=cent,col=&quot;grey&quot;)&#xD;&#xA;    #define the middle of the segments&#xD;&#xA;    max.avg&lt;-seq(ti1+.025,ti2+.025,.05)&#xD;&#xA;    #plot the maximum value of each segment&#xD;&#xA;    max.peaks&#xD;&#xA;    lines(max.avg,max.peaks,type=&quot;b&quot;,col=&quot;deepskyblue&quot;)&#xD;&#xA;    #lines(max.avg,min.peaks,type=&quot;b&quot;,col=&quot;blue&quot;)&#xD;&#xA;    #find time associated with peaks &gt; 5 fu and fill seconds matrix with them&#xD;&#xA;    #12.5% of max fu incase chip is fu&#xD;&#xA;    p1&lt;-which(max.avg==38)&#xD;&#xA;    p2&lt;-which(max.avg==55)&#xD;&#xA;    sizes &lt;- which(max.peaks&gt;cent)&#xD;&#xA;    max.avg[sizes]&#xD;&#xA;    gt35&lt;-which(max.avg[sizes]&gt;35)&#xD;&#xA;    lt55&lt;-which(max.avg[sizes]&lt;55)&#xD;&#xA;    #as.numeric(duplicated(c(gt35,lt55)))&#xD;&#xA;    tfr&lt;-which(duplicated(c(gt35,lt55))==TRUE)&#xD;&#xA;    c(gt35,lt55)[tfr]&#xD;&#xA;    max.avg[sizes][c(gt35,lt55)[tfr]]&#xD;&#xA;    max.peaks[sizes][c(gt35,lt55)[tfr]]&#xD;&#xA;    points(max.avg[sizes+1][c(gt35,lt55)[tfr]],max.peaks[sizes+1][c(gt35,lt55)[tfr]],col=&quot;lawngreen&quot;,pch=19)&#xD;&#xA;    points(max.avg[sizes-1][c(gt35,lt55)[tfr]],max.peaks[sizes-1][c(gt35,lt55)[tfr]],col=&quot;lawngreen&quot;,pch=19)&#xD;&#xA;    points(max.avg[sizes][c(gt35,lt55)[tfr]],max.peaks[sizes][c(gt35,lt55)[tfr]],col=&quot;deeppink3&quot;,pch=19)&#xD;&#xA;    #colnms2&lt;-c(&quot;max.avg[sizes]&quot;,&quot;max.peaks[sizes]&quot;,&quot;max.avg[sizes+1]&quot;,&quot;max.peaks[sizes+1]&quot;,&quot;max.avg[sizes-1]&quot;)&#xD;&#xA;    max.avg[sizes]&#xD;&#xA;    max.peaks[sizes]&#xD;&#xA;    max.avg[sizes+1]&#xD;&#xA;    max.peaks[sizes+1]&#xD;&#xA;    max.avg[sizes-1]&#xD;&#xA;    max.peaks[sizes-1]&#xD;&#xA;    area&lt;-cbind(max.peaks[sizes-1][c(gt35,lt55)[tfr]],max.peaks[sizes][c(gt35,lt55)[tfr]],max.peaks[sizes+1][c(gt35,lt55)[tfr]])&#xD;&#xA;    time2&lt;-cbind(max.avg[sizes-1][c(gt35,lt55)[tfr]],max.avg[sizes][c(gt35,lt55)[tfr]],max.avg[sizes+1][c(gt35,lt55)[tfr]])&#xD;&#xA;    p1&lt;-area[which(area[,1]&lt;cent),1][1]&#xD;&#xA;    t1&lt;-time2[which(area[,1]&lt;cent),1][1]&#xD;&#xA;    p2&lt;-area[which(area[,1]&lt;cent),1][2]&#xD;&#xA;    t2&lt;-time2[which(area[,1]&lt;cent),1][2]&#xD;&#xA;    #area[which(area[,2]&lt;cent),2]&#xD;&#xA;    #area[which(area[,2]&lt;cent),2]&#xD;&#xA;    p3&lt;-area[which(area[,3]&lt;cent),3][1]&#xD;&#xA;    t3&lt;-time2[which(area[,3]&lt;cent),3][1]&#xD;&#xA;    p4&lt;-area[which(area[,3]&lt;cent),3][2]&#xD;&#xA;    t4&lt;-time2[which(area[,3]&lt;cent),3][2]&#xD;&#xA;    #abline(v=area[which(area[,3]&lt;cent),3])&#xD;&#xA;    #abline(h=11)&#xD;&#xA;    #abline(v=42.75)&#xD;&#xA;    v1&lt;-t1&#xD;&#xA;    v2&lt;-t2&#xD;&#xA;    v3&lt;-t3&#xD;&#xA;    v4&lt;-t4&#xD;&#xA;    abline(v=v1)&#xD;&#xA;    abline(v=v2)&#xD;&#xA;    abline(v=v3)&#xD;&#xA;    abline(v=v4)&#xD;&#xA;    #time[area[which(area[,1]&lt;cent),1][1]==dta[,lad]]&#xD;&#xA;    #time[area[which(area[,3]&lt;cent),3][1]==dta[,lad]]&#xD;&#xA;    #time[area[which(area[,1]&lt;cent),1][2]==dta[,lad]]&#xD;&#xA;    #time[area[which(area[,3]&lt;cent),3][2]==dta[,lad]]&#xD;&#xA;    #abline(v=time[area[which(area[,1]&lt;cent),1][1]==dta[,lad]])&#xD;&#xA;    #abline(v=time[area[which(area[,3]&lt;cent),3][1]==dta[,lad]])&#xD;&#xA;    #abline(v=time[area[which(area[,1]&lt;cent),1][2]==dta[,lad]])&#xD;&#xA;    #abline(v=time[area[which(area[,3]&lt;cent),3][2]==dta[,lad]])&#xD;&#xA;    #abline(h=area[which(area[,3]&lt;cent),3][1])&#xD;&#xA;    #abline(h=area[which(area[,3]&lt;cent),3][2]+3)&#xD;&#xA;    s18a&lt;-round(t1,digits=1)&#xD;&#xA;    s18b&lt;-round(t3,digits=1)&#xD;&#xA;    s28a&lt;-round(t2,digits=1)&#xD;&#xA;    s28b&lt;-round(t4,digits=1)&#xD;&#xA;    if(is.na(s18a)){s18a&lt;-30}&#xD;&#xA;    if(is.na(s18b)){s18b&lt;-35.05}&#xD;&#xA;    if(is.na(s28a)){s28a&lt;-30}&#xD;&#xA;    if(is.na(s28b)){s28b&lt;-30.05}&#xD;&#xA;    eta&lt;-which(time==s18a)&#xD;&#xA;    etb&lt;-which(time==s18b)&#xD;&#xA;    tea&lt;-which(time==s28a)&#xD;&#xA;    teb&lt;-which(time==s28b)&#xD;&#xA;    #dta[eta:etb,lad]&#xD;&#xA;    #dta[tea:teb,lad]&#xD;&#xA;    s18&lt;-sum(dta[eta:etb,lad])&#xD;&#xA;    s28&lt;-sum(dta[tea:teb,lad])&#xD;&#xA;    s28/s18&#xD;&#xA;    qc.mat[samples]&lt;-  (-1*exp((s28/s18)*-1)+1)*10&#xD;&#xA;    text(60,cent*.75,c(format(qc.mat[samples],digits=2),&quot;\n\nB-RIN&quot;))&#xD;&#xA;    }&#xD;&#xA;    qc.mat&#xD;&#xA;    ###end of ratio calc&#xD;&#xA;    ###&#xD;&#xA;    ###&#xD;&#xA;    #barplot(qc.mat,beside=T,ylim=c(0,11))&#xD;&#xA;    #set the working directory&#xD;&#xA;    #results &lt;- read.delim(&quot;results-files.txt&quot;, sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xD;&#xA;    #targets &lt;- read.delim(results[1,], sep=&quot;,&quot;,header=F,skip=14,as.is=T)&#xD;&#xA;    #targets[1:5,]&#xD;&#xA;    tbl&lt;-which(targets[,1]==&quot;Overall Results:&quot;)&#xD;&#xA;    which(targets[,1]==&quot;Sample Name&quot;)&#xD;&#xA;    length(which(targets[,1]==&quot;Sample Name&quot;))&#xD;&#xA;    samples&lt;-as.character(targets[which(targets[,1]==&quot;Sample Name&quot;),2])&#xD;&#xA;    #read in the targets file                               &#xD;&#xA;    colnms&lt;-as.character(targets[(tbl[1]+1):(tbl[1]+4),1])&#xD;&#xA;    mat1 &lt;- matrix(NA, ncol=length(colnms), nrow=length(samples)*length(results))&#xD;&#xA;    colnames(mat1)&lt;-colnms&#xD;&#xA;    rownames(mat1)&lt;-rep(samples,length(results))&#xD;&#xA;    for(i in 1:length(results)) &#xD;&#xA;    {&#xD;&#xA;    targets &lt;- read.delim(results[i], sep=&quot;,&quot;,header=F,skip=14,as.is=T)&#xD;&#xA;    targets[1:5,]&#xD;&#xA;    which(targets[,1]==&quot;Sample Name&quot;)&#xD;&#xA;    length(which(targets[,1]==&quot;Sample Name&quot;))&#xD;&#xA;    samples&lt;-as.character(targets[which(targets[,1]==&quot;Sample Name&quot;),2])&#xD;&#xA;    tbl&lt;-which(targets[,1]==&quot;Overall Results:&quot;)&#xD;&#xA;    length(which(targets[,1]==&quot;Overall Results:&quot;))&#xD;&#xA;    tbl[1]&#xD;&#xA;    tbl[1]&#xD;&#xA;    colnms&lt;-as.character(targets[(tbl[1]+1):(tbl[1]+4),1])&#xD;&#xA;    for (j in 1:length(samples))&#xD;&#xA;    {&#xD;&#xA;    mat1[((length(samples)*(i-1))+j),]&lt;-as.matrix(targets[(tbl[j]+1):(tbl[j]+4),2])&#xD;&#xA;    }&#xD;&#xA;    }&#xD;&#xA;    mat1&#xD;&#xA;    cbind(mat1,qc.mat)&#xD;&#xA;    colnames(qc.mat)&lt;-&quot;B-RIN&quot;&#xD;&#xA;    write.table(cbind(mat1,qc.mat),file=paste(dir,&quot;\\RIN_VS_B-RIN.txt&quot;,sep=&quot;&quot;), sep=&quot;\t&quot;,col.names=NA)&#xD;&#xA;    " />
  <row Id="4348" PostHistoryTypeId="5" PostId="2048" RevisionGUID="490991ba-1622-45d4-bbdb-13cd22a80100" CreationDate="2017-07-12T18:12:37.653" UserId="1025" Comment="deleted 708 characters in body" Text="I'm trying to link GEOquery and minfi. &#xD;&#xA;&#xD;&#xA;Specifically I want to obtain beta values from the idat files on GEOquery. I was following this guide: https://kasperdanielhansen.github.io/genbioconductor/html/minfi.html, up until the preprocessing part. Meaning that I was able to obtain the RGset. Then I used my own preprocessing code to obtain the beta values. However, I cross checked them with the beta values that were on GEO and they weren't consistent.&#xD;&#xA;&#xD;&#xA;For example, the accession number I used was GSE68777. So I went to that study on GEO and clicked on the first sample: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM1681154. Then I scrolled down and clicked &quot;Download full table&quot; to download the samples and beta values in a text file. &#xD;&#xA;&#xD;&#xA;Then I typed head(beta) and chose the first sample. Then I did command F in the text file for that sample and it's value there wasn't the same as the value from the beta data table. Hopefully you can help find the error.&#xD;&#xA;&#xD;&#xA;Here is the code I'm using:&#xD;&#xA;&#xD;&#xA;    library(GEOquery)&#xD;&#xA;    library(minfi)&#xD;&#xA;    library(&quot;IlluminaHumanMethylation450kanno.ilmn12.hg19&quot;)&#xD;&#xA;    library(&quot;IlluminaHumanMethylation450kmanifest&quot;)&#xD;&#xA;    &#xD;&#xA;    ######## Code copied from 450k Guide ########&#xD;&#xA;&#xD;&#xA;    getGEOSuppFiles(&quot;GSE68777&quot;)&#xD;&#xA;    untar(&quot;GSE68777/GSE68777_RAW.tar&quot;, exdir = &quot;GSE68777/idat&quot;)&#xD;&#xA;    head(list.files(&quot;GSE68777/idat&quot;, pattern = &quot;idat&quot;))&#xD;&#xA;    idatFiles &lt;- list.files(&quot;GSE68777/idat&quot;, pattern = &quot;idat.gz$&quot;, full = TRUE)&#xD;&#xA;    sapply(idatFiles, gunzip, overwrite = TRUE)&#xD;&#xA;    rgSet &lt;- read.metharray.exp(&quot;GSE68777/idat&quot;)&#xD;&#xA;    geoMat &lt;- getGEO(&quot;GSE68777&quot;)&#xD;&#xA;    pD.all &lt;- pData(geoMat[[1]])&#xD;&#xA;    pD &lt;- pD.all[, c(&quot;title&quot;, &quot;geo_accession&quot;, &quot;characteristics_ch1.1&quot;, &quot;characteristics_ch1.2&quot;)]&#xD;&#xA;    names(pD)[c(3,4)] &lt;- c(&quot;group&quot;, &quot;sex&quot;)&#xD;&#xA;    pD$group &lt;- sub(&quot;^diagnosis: &quot;, &quot;&quot;, pD$group)&#xD;&#xA;    pD$sex &lt;- sub(&quot;^Sex: &quot;, &quot;&quot;, pD$sex)&#xD;&#xA;    sampleNames(rgSet) &lt;- sub(&quot;.*_5&quot;, &quot;5&quot;, sampleNames(rgSet))&#xD;&#xA;    rownames(pD) &lt;- pD$title&#xD;&#xA;    pD &lt;- pD[sampleNames(rgSet),]&#xD;&#xA;    pData(rgSet) &lt;- pD&#xD;&#xA;    " />
  <row Id="4349" PostHistoryTypeId="2" PostId="2059" RevisionGUID="03853b3a-dc23-4be7-bb1b-b241235cbdc9" CreationDate="2017-07-13T08:24:21.383" UserId="367" Text="I am looking for advice on how to best approach a problem I am faced with. &#xD;&#xA;&#xD;&#xA;I have a dataset of numerous degradative biomarkers, clinical information and various other measures (from clinical trials). I would like to see how different biomarkers relate to each other, and effect their levels (high/low etc).&#xD;&#xA;&#xD;&#xA;I have thought about implementing something similar to a protein-protein interaction network, although this will of course not involve physical interactions, but more pathway interactions for further hypothesis driven research. &#xD;&#xA;&#xD;&#xA;I wanted to firstly: validate if this project actually makes sense and secondly: ask for some advice around how to best implement this. &#xD;&#xA;&#xD;&#xA;I have experience implementing neural/deep nets, directed and undirected networks (Bayes) in python, but not sure exactly how to go about this.&#xD;&#xA;&#xD;&#xA;Thanks!&#xD;&#xA;&#xD;&#xA; " />
  <row Id="4350" PostHistoryTypeId="1" PostId="2059" RevisionGUID="03853b3a-dc23-4be7-bb1b-b241235cbdc9" CreationDate="2017-07-13T08:24:21.383" UserId="367" Text="Building network for biomarker interaction analysis" />
  <row Id="4351" PostHistoryTypeId="3" PostId="2059" RevisionGUID="03853b3a-dc23-4be7-bb1b-b241235cbdc9" CreationDate="2017-07-13T08:24:21.383" UserId="367" Text="&lt;proteins&gt;&lt;networks&gt;&lt;pathway&gt;&lt;interactions&gt;" />
  <row Id="4352" PostHistoryTypeId="2" PostId="2060" RevisionGUID="d3f70a79-6b79-466c-8079-83aa0607753c" CreationDate="2017-07-13T08:35:41.727" UserId="1092" Text="I would recommend [Transrate](http://hibberdlab.com/transrate/index.html). It generates a score for each transcript based on many metrics (read their paper [here](http://dx.doi.org/10.1101/gr.196469.115) and, using that score, gives a predicted cut-off to produce the optimal transcript set." />
  <row Id="4353" PostHistoryTypeId="5" PostId="2060" RevisionGUID="578e6d91-1784-43ab-830d-87568edd1bfc" CreationDate="2017-07-13T08:44:04.673" UserId="1092" Comment="added 1 character in body" Text="I would recommend [Transrate](http://hibberdlab.com/transrate/index.html). It generates a score for each transcript based on many metrics (read their paper [here](http://dx.doi.org/10.1101/gr.196469.115)) and, using that score, gives a predicted cut-off to produce the optimal transcript set." />
  <row Id="4354" PostHistoryTypeId="2" PostId="2061" RevisionGUID="e57f582a-64b0-401e-ab9d-6a90c1c2dc21" CreationDate="2017-07-13T09:04:28.000" UserId="939" Text="On Reactome website they have at the download [page][1] mapping files (uniprot, ensembl, etc.), but unfortunately not for the protein IDs you are using (stable identifiers).&#xD;&#xA;&#xD;&#xA;I had contact with their helpdesk, and they sent me a file containing all protein IDs to the pathways. Exactly what you need. I have asked them if they wanted to put it on their download page as well, but not sure if they want to do this. Meanwhile you can ask for the file as well, or get it from my google [drive][2].&#xD;&#xA;&#xD;&#xA;I assume you know how to get your Protein IDs and pathways from this file, using e.g., R?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://reactome.org/pages/download-data/&#xD;&#xA;  [2]: https://drive.google.com/open?id=0ByVMqlqqt462enVaNkZZSU9iMjg" />
  <row Id="4355" PostHistoryTypeId="5" PostId="651" RevisionGUID="9a6cc25e-9652-4077-8a80-4b5cb9529eb7" CreationDate="2017-07-13T11:02:02.967" UserId="191" Comment="correct identifier VAL?" Text="**UPD**: As [suggested][1] by Llopis, maybe the correct identifier is VAL?&#xD;&#xA;&#xD;&#xA;I think there is no metabolite with such name in the database. I tried [their search](https://biocyc.org/ECOLI/search-query?type=COMPOUND&amp;name=Valine), the correct name seems to be `L-valine`. I expect that they use similar naming for other amino acids. If you cannot find something, you can use their compound search through the website. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/623/what-methods-should-i-use-from-pythoncyc-api-to-query-metabolites-in-biocyc-data/651?noredirect=1#comment3035_651" />
  <row Id="4356" PostHistoryTypeId="2" PostId="2062" RevisionGUID="74520455-194e-4bcb-8c51-af8e48affbbb" CreationDate="2017-07-13T12:22:24.687" UserId="252" Text="&#xD;&#xA;&#xD;&#xA;I would like to be able to batch download FASTA files from ENSEMBL. I normally would use the API to download them from the ENSEMBL gene IDs. However, the IDs aren't ones I know, only ones I would get from searching on the ENSEMBL website and collecting the IDs manually.&#xD;&#xA;&#xD;&#xA;Is there a way I can use the API to query search terms and collect all results that are returned?&#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="4357" PostHistoryTypeId="1" PostId="2062" RevisionGUID="74520455-194e-4bcb-8c51-af8e48affbbb" CreationDate="2017-07-13T12:22:24.687" UserId="252" Text="Is there a way to retrieve ENSEMBL IDs from a search query?" />
  <row Id="4358" PostHistoryTypeId="3" PostId="2062" RevisionGUID="74520455-194e-4bcb-8c51-af8e48affbbb" CreationDate="2017-07-13T12:22:24.687" UserId="252" Text="&lt;database&gt;&lt;reference-genome&gt;&lt;ensembl&gt;&lt;data-mining&gt;" />
  <row Id="4360" PostHistoryTypeId="2" PostId="2063" RevisionGUID="b79a4e08-0fe0-49c3-a8cd-1a2b3bc88d4b" CreationDate="2017-07-13T17:47:10.213" UserId="1092" Text="I would download the complete geneset FASTA file from [ensembl](http://www.ensembl.org/info/data/ftp/index.html) and then parse out the genes you want with a script.&#xD;&#xA;&#xD;&#xA;Alternatively, you could do similarly with API. Get all the gene names, select the ones you want, and then pull down each sequence." />
  <row Id="4364" PostHistoryTypeId="5" PostId="2062" RevisionGUID="032c6e14-f1c1-4882-97c8-e717f736e294" CreationDate="2017-07-14T08:59:31.373" UserId="252" Comment="added 391 characters in body" Text="&#xD;&#xA;I would like to be able to batch download FASTA files from ENSEMBL. I normally would use the API to download them from the ENSEMBL gene IDs. However, the IDs aren't ones I know, only ones I would get from searching on the ENSEMBL website and collecting the IDs manually.&#xD;&#xA;&#xD;&#xA;Is there a way I can use the API to query search terms and collect all results that are returned?&#xD;&#xA;&#xD;&#xA;So for example, if I use the ENSEMBL website to search &quot;HLA&quot; I could collect the list of IDs (ENSG00000204252 etc. etc.) manually. HLA works okay using the API to query gene IDs because there's only a few terms (A, B, C plus non-classical for class I) but I was wondering if there was a way to directly access the search query one the homepage programmatically, as I may need to do it for some messier examples." />
  <row Id="4365" PostHistoryTypeId="2" PostId="2065" RevisionGUID="560d3317-69a4-4741-aae5-1a22fe6b9264" CreationDate="2017-07-14T10:46:42.500" UserId="167" Text="I'm new to bioinformatics and Im starting a new microbial sequencing project and I'd like to record all of the qc data correctly. This is research and I'm not sure what I'll need later. Is there any consensus on useful qc metrics to record in a database for each sequencing run?" />
  <row Id="4366" PostHistoryTypeId="1" PostId="2065" RevisionGUID="560d3317-69a4-4741-aae5-1a22fe6b9264" CreationDate="2017-07-14T10:46:42.500" UserId="167" Text="What sequencing data metrics should I record?" />
  <row Id="4367" PostHistoryTypeId="3" PostId="2065" RevisionGUID="560d3317-69a4-4741-aae5-1a22fe6b9264" CreationDate="2017-07-14T10:46:42.500" UserId="167" Text="&lt;database&gt;&lt;ngs&gt;&lt;qc&gt;" />
  <row Id="4368" PostHistoryTypeId="2" PostId="2066" RevisionGUID="14d86c10-bd33-4885-bdab-8a0af29245d1" CreationDate="2017-07-14T10:54:33.113" UserId="48" Text="You can start by looking at the correlation (if data is non-parametric) of each variable with each variable of each data point you have. One thing that might change is the correlation between variables along the disease.&#xD;&#xA;&#xD;&#xA;Next you can determine which relationships are constant along the disease progression/clinical trial. Identify also the ones that don't show a constant relationship. To do so you can use WGCNA (although with just 10 variables is too low to expect a scale free network you can use to correlate the variables and group them by correlation profile)&#xD;&#xA;&#xD;&#xA;Then the question is which variables are more important, those who kept the same relationship or those which change? &#xD;&#xA;Use the eigengenes of those variables (separately by those that have a constant relationship and those which don't) to estimate the dependency with the parametric variables. Or you simply can do an ANOVA of all the variables with each non-parametric variable. " />
  <row Id="4369" PostHistoryTypeId="5" PostId="2065" RevisionGUID="c887e672-eb97-439c-b6f0-72bfd8cf2b1b" CreationDate="2017-07-14T10:55:05.473" UserId="167" Comment="added 87 characters in body" Text="I'm new to bioinformatics and Im starting a new microbial sequencing project and I'd like to record all of the qc data correctly. This is research and I'm not sure what I'll need later. Is there any consensus on useful qc metrics to record in a database for each sequencing run?&#xA;&#xA;No rnaseq, running FastQC, assembling with a defined reference for a single species. " />
  <row Id="4370" PostHistoryTypeId="2" PostId="2067" RevisionGUID="e41ce159-9719-483c-9bb8-67ba1961a4b0" CreationDate="2017-07-14T11:13:42.640" UserId="982" Text="I want to perform a genome comparison on a group of isolates. I want to look into two broad groups of taxa and compare the accessory genome in each group. I have been using prokka (v1.12) and roary (v3.8.2) to do this but it appears the accessory_binary_genes.fa file is actually an untrue representation.&#xD;&#xA;&#xD;&#xA;**Note:** gene_presence_absence.Rtab does contain all the full presence/ absence for accessory gene sets. Despite this Im still unhappy with the nomenclature of the gene groups [issue for another day] &#xD;&#xA;&#xD;&#xA;&gt; [\[github issue 335\]][1] Your best to ignore the accessory_binary_genes.fa file. It is just for creating a quick and dirty tree with FastTree. The file itself is filtered to remove very common and not common variation to speedup the tree generation, hence the difference in numbers.&#xD;&#xA;&gt; &#xD;&#xA;&gt; The top 5% and bottom 5% are excluded. It is truncated at 4000 genes.&#xD;&#xA;&#xD;&#xA;I've been looking into alternative pipelines and a new software [BPGA][2] looks promising. Does anyone have experience with this?&#xD;&#xA;&#xD;&#xA;I essentially want a tool which will give me the core and accessory gene sets.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/sanger-pathogens/Roary/issues/335&#xD;&#xA;  [2]: https://www.nature.com/articles/srep24373" />
  <row Id="4371" PostHistoryTypeId="1" PostId="2067" RevisionGUID="e41ce159-9719-483c-9bb8-67ba1961a4b0" CreationDate="2017-07-14T11:13:42.640" UserId="982" Text="What is the best software for bacterial core/pan genome pipeline?" />
  <row Id="4372" PostHistoryTypeId="3" PostId="2067" RevisionGUID="e41ce159-9719-483c-9bb8-67ba1961a4b0" CreationDate="2017-07-14T11:13:42.640" UserId="982" Text="&lt;genome&gt;&lt;software-recommendation&gt;" />
  <row Id="4373" PostHistoryTypeId="4" PostId="2067" RevisionGUID="f62f7be1-8286-45e5-8c01-4073af3dba02" CreationDate="2017-07-14T11:15:18.920" UserId="298" Comment="&quot;The best&quot; will almost always depend on opinion. I tried to make this more answerable. " Text="What tools can I use for a bacterial core/pan genome pipeline?" />
  <row Id="4374" PostHistoryTypeId="5" PostId="2059" RevisionGUID="0e3f4cf8-e5a2-4b47-8d7d-7ae4eb0fad25" CreationDate="2017-07-14T11:24:28.650" UserId="367" Comment="added 476 characters in body" Text="I am looking for advice on how to best approach a problem I am faced with. &#xD;&#xA;&#xD;&#xA;I have a dataset of numerous degradative biomarkers, clinical information and various other measures (from clinical trials). I would like to see how different biomarkers relate to each other, and effect their levels (high/low etc).&#xD;&#xA;&#xD;&#xA;I have thought about implementing something similar to a protein-protein interaction network, although this will of course not involve physical interactions, but more pathway interactions for further hypothesis driven research. &#xD;&#xA;&#xD;&#xA;I wanted to firstly: validate if this project actually makes sense and secondly: ask for some advice around how to best implement this. &#xD;&#xA;&#xD;&#xA;I have experience implementing neural/deep nets, directed and undirected networks (Bayes) in python, but not sure exactly how to go about this.&#xD;&#xA;&#xD;&#xA;Thanks!&#xD;&#xA;&#xD;&#xA;EDIT (from below): My goal is to gain an understanding of how these biomarker levels fluctuate in different disease profiles - and understand if there is a relationship between each of them (co/independent). The limitation is that I do not have access to genetic or environmental data so this will be a &quot;random effect&quot;. There are around 1000 marker samples and 10's of biomarkers mixed between clinical and biochemical. There are also numerous time points to be incorporated&#xD;&#xA; " />
  <row Id="4376" PostHistoryTypeId="2" PostId="2068" RevisionGUID="ca377393-0acb-4936-ae87-250047bd4dcc" CreationDate="2017-07-14T13:40:47.287" UserId="294" Text="I would like to convert a file in `gff3` format to a `gtf2.2` format.&#xD;&#xA;&#xD;&#xA;I unsuccessfully tried: &#xD;&#xA;&#xD;&#xA; 1. `gffread` utility in the `Cufflinks` package (`gffread input.gff3 -T -o output.gtf`): this results in an empty `output.gtf` file and an empty `log` file (used `Cufflinks` v.2.2.1) - I contacted the authors via their [Google group][1] but haven't heard of them yet &#xD;&#xA; 2. `gff3_to_gtf` utility in the `gt` package (`gt gff3_to_gtf input.gff3 -o output.gtf`): the output is not created and the log file is not informative - I contacted the authors via their [mailing list][2] but haven't heard of them yet &#xD;&#xA;&#xD;&#xA;The `gff3` file was created as output of [GMAP][3], and contains the transcripts as found by alignment to the reference (specifying option `-f gff3_match_cdna`).&#xD;&#xA;&#xD;&#xA;Here is the beginning of the gff3 file I tried to convert: &#xD;&#xA;&#xD;&#xA;    $ head r9_gmap_match-cdna.gff&#xD;&#xA;    ##gff-version   3&#xD;&#xA;    # Generated by GMAP version 2017-04-24 using call:  /home/aechchik/software/gmap-2017-04-24/bin/gmap.sse42 -d gmapidx -D /scratch/beegfs/monthly/aechchik/isoforms/ref/chromosomes/ -f gff3_match_cdna -n 0 -t 20 r9_2d.fasta&#xD;&#xA;    2L    gmapidx    cDNA_match    18442664    18443024    79    -    .ID=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1;Name=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d;Target=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d 141 489;Gap=M13 I10 M8 D3 M8 D3 M32 D1 M4 D2 M5 D3 M34 I3 M6 D1 M7 D1 M5 D1 M30 D2 M16 I1 M8 D3 M10 D1 M5 D1 M6 I1 M8 I1 M8 D1 M33 D3 M33 I1 M28 D3 M25&#xD;&#xA;    ###&#xD;&#xA;    3R    gmapidx    cDNA_match    15853880    15855465    96    +    .ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 80 1645;Gap=M46 D2 M12 D2 M157 D3 M4 D1 M50 I2 M3 I1 M12 I1 M68 I1 M66 I1 M53 D2 M47 D1 M16 D1 M35 D1 M155 D1 M28 D1 M166 D2 M47 D1 M8 D4 M69 D1 M28 D1 M5 D1 M12 D1 M202 I1 M115 D1 M61 I2 M7 D1 M7 I1 M36 D2 M41&#xD;&#xA;    3R    gmapidx    cDNA_match    15855529    15855742    97    +    .ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 1646 1856;Gap=M21 D1 M68 D1 M85 D1 M37&#xD;&#xA;    ###&#xD;&#xA;    X    gmapidx    cDNA_match    14837810    14838142    93    -    .ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 74 406;Gap=M13 D1 M182 I1 M14 I2 M56 I1 M30 D2 M21 D1 M13&#xD;&#xA;    X    gmapidx    cDNA_match    14837470    14837753    92    -    .ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 407 688;Gap=I2 M5 I1 M64 I1 M44 D1 M9 D5 M31 D3 M23 I1 M19 I1 M25 I1 M26 D1 M19 I1 M9&#xD;&#xA;    ###&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://groups.google.com/d/msg/tuxedo-tools-users/T-qE4P67-HU/O_up6Yh6BgAJ&#xD;&#xA;  [2]: http://gt-users@genometools.org&#xD;&#xA;  [3]: https://wiki.gacrc.uga.edu/wiki/Gmap" />
  <row Id="4377" PostHistoryTypeId="1" PostId="2068" RevisionGUID="ca377393-0acb-4936-ae87-250047bd4dcc" CreationDate="2017-07-14T13:40:47.287" UserId="294" Text="How to convert GFF3 to GTF2" />
  <row Id="4378" PostHistoryTypeId="3" PostId="2068" RevisionGUID="ca377393-0acb-4936-ae87-250047bd4dcc" CreationDate="2017-07-14T13:40:47.287" UserId="294" Text="&lt;software-recommendation&gt;&lt;format-conversion&gt;&lt;gff3&gt;&lt;gtf&gt;" />
  <row Id="4379" PostHistoryTypeId="2" PostId="2069" RevisionGUID="601484a3-69b8-4e75-a2ac-3b1f3d701698" CreationDate="2017-07-14T13:46:53.463" UserId="931" Text="The [GenomeTools][1] package has lots of small applications that let you massage GFF3. It may have a converter, or it may let you tweak your existing GFF3 so that gffread works&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://genometools.org/" />
  <row Id="4380" PostHistoryTypeId="5" PostId="2068" RevisionGUID="91ac0986-4807-4b70-9b1a-0ceb30e754f3" CreationDate="2017-07-14T14:23:03.737" UserId="294" Comment="added 5 characters in body" Text="I would like to convert a file in `gff3` format to a `gtf2.2` format.&#xD;&#xA;&#xD;&#xA;I unsuccessfully tried: &#xD;&#xA;&#xD;&#xA; 1. `gffread` utility in the `Cufflinks` package (`gffread input.gff3 -T -o output.gtf`): this results in an empty `output.gtf` file and an empty `log` file (used `Cufflinks` v.2.2.1) - I contacted the authors via their [Google group][1] but haven't heard of them yet &#xD;&#xA; 2. `gff3_to_gtf` utility in the `gt` package (`gt gff3_to_gtf input.gff3 -o output.gtf`): the output is not created and the log file is not informative - I contacted the authors via their [mailing list][2] but haven't heard of them yet &#xD;&#xA;&#xD;&#xA;The `gff3` file was created as output of [GMAP][3], and contains the transcripts as found by alignment to the reference (specifying option `-f gff3_match_cdna`).&#xD;&#xA;&#xD;&#xA;Here is the beginning of the gff3 file I tried to convert: &#xD;&#xA;&#xD;&#xA;    $ head r9_gmap_match-cdna.gff&#xD;&#xA;    ##gff-version   3&#xD;&#xA;    # Generated by GMAP version 2017-04-24 using call:  /home/aechchik/software/gmap-2017-04-24/bin/gmap.sse42 -d gmapidx -D /scratch/beegfs/monthly/aechchik/isoforms/ref/chromosomes/ -f gff3_match_cdna -n 0 -t 20 r9_2d.fasta&#xD;&#xA;    2L    gmapidx    cDNA_match    18442664    18443024    79    -    . ID=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1;Name=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d;Target=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d 141 489;Gap=M13 I10 M8 D3 M8 D3 M32 D1 M4 D2 M5 D3 M34 I3 M6 D1 M7 D1 M5 D1 M30 D2 M16 I1 M8 D3 M10 D1 M5 D1 M6 I1 M8 I1 M8 D1 M33 D3 M33 I1 M28 D3 M25&#xD;&#xA;    ###&#xD;&#xA;    3R    gmapidx    cDNA_match    15853880    15855465    96    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 80 1645;Gap=M46 D2 M12 D2 M157 D3 M4 D1 M50 I2 M3 I1 M12 I1 M68 I1 M66 I1 M53 D2 M47 D1 M16 D1 M35 D1 M155 D1 M28 D1 M166 D2 M47 D1 M8 D4 M69 D1 M28 D1 M5 D1 M12 D1 M202 I1 M115 D1 M61 I2 M7 D1 M7 I1 M36 D2 M41&#xD;&#xA;    3R    gmapidx    cDNA_match    15855529    15855742    97    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 1646 1856;Gap=M21 D1 M68 D1 M85 D1 M37&#xD;&#xA;    ###&#xD;&#xA;    X    gmapidx    cDNA_match    14837810    14838142    93    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 74 406;Gap=M13 D1 M182 I1 M14 I2 M56 I1 M30 D2 M21 D1 M13&#xD;&#xA;    X    gmapidx    cDNA_match    14837470    14837753    92    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 407 688;Gap=I2 M5 I1 M64 I1 M44 D1 M9 D5 M31 D3 M23 I1 M19 I1 M25 I1 M26 D1 M19 I1 M9&#xD;&#xA;    ###&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://groups.google.com/d/msg/tuxedo-tools-users/T-qE4P67-HU/O_up6Yh6BgAJ&#xD;&#xA;  [2]: http://gt-users@genometools.org&#xD;&#xA;  [3]: https://wiki.gacrc.uga.edu/wiki/Gmap" />
  <row Id="4381" PostHistoryTypeId="5" PostId="2068" RevisionGUID="058be6ac-e363-4772-8537-69b6a74ccaed" CreationDate="2017-07-14T14:37:26.373" UserId="294" Comment="added 533 characters in body" Text="I would like to convert a file in `gff3` format to a `gtf2.2` format.&#xD;&#xA;&#xD;&#xA;I unsuccessfully tried: &#xD;&#xA;&#xD;&#xA; 1. `gffread` utility in the [Cufflinks][1] package (`gffread input.gff3 -T -o output.gtf`): this results in an empty `output.gtf` file and an empty `log` file (used `Cufflinks` v.2.2.1) - I contacted the authors via their [Google group][2] but haven't heard of them yet &#xD;&#xA; 2. `gff3_to_gtf` utility in the [GenomeTools][3] (`gt`) package (`gt gff3_to_gtf input.gff3 -o output.gtf`): the output is not created and the log file is not informative - I contacted the authors via their [mailing list][4] but haven't heard of them yet &#xD;&#xA; 3. `GFF3_to_GTF` utility in the [FML][5] package (`./gff3_to_gtf_converter.pl input.gff3 output.gtf`): the output just contains a header (`##gff-version 2.5`) and the `log` is empty&#xD;&#xA;&#xD;&#xA;The `gff3` file was created as output of [GMAP][6], and contains the transcripts as found by alignment to the reference (specifying option `-f gff3_match_cdna`).&#xD;&#xA;&#xD;&#xA;Here is the beginning of the gff3 file I tried to convert: &#xD;&#xA;&#xD;&#xA;    $ head r9_gmap_match-cdna.gff&#xD;&#xA;    ##gff-version   3&#xD;&#xA;    # Generated by GMAP version 2017-04-24 using call:  /home/aechchik/software/gmap-2017-04-24/bin/gmap.sse42 -d gmapidx -D /scratch/beegfs/monthly/aechchik/isoforms/ref/chromosomes/ -f gff3_match_cdna -n 0 -t 20 r9_2d.fasta&#xD;&#xA;    2L    gmapidx    cDNA_match    18442664    18443024    79    -    . ID=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1;Name=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d;Target=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d 141 489;Gap=M13 I10 M8 D3 M8 D3 M32 D1 M4 D2 M5 D3 M34 I3 M6 D1 M7 D1 M5 D1 M30 D2 M16 I1 M8 D3 M10 D1 M5 D1 M6 I1 M8 I1 M8 D1 M33 D3 M33 I1 M28 D3 M25&#xD;&#xA;    ###&#xD;&#xA;    3R    gmapidx    cDNA_match    15853880    15855465    96    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 80 1645;Gap=M46 D2 M12 D2 M157 D3 M4 D1 M50 I2 M3 I1 M12 I1 M68 I1 M66 I1 M53 D2 M47 D1 M16 D1 M35 D1 M155 D1 M28 D1 M166 D2 M47 D1 M8 D4 M69 D1 M28 D1 M5 D1 M12 D1 M202 I1 M115 D1 M61 I2 M7 D1 M7 I1 M36 D2 M41&#xD;&#xA;    3R    gmapidx    cDNA_match    15855529    15855742    97    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 1646 1856;Gap=M21 D1 M68 D1 M85 D1 M37&#xD;&#xA;    ###&#xD;&#xA;    X    gmapidx    cDNA_match    14837810    14838142    93    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 74 406;Gap=M13 D1 M182 I1 M14 I2 M56 I1 M30 D2 M21 D1 M13&#xD;&#xA;    X    gmapidx    cDNA_match    14837470    14837753    92    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 407 688;Gap=I2 M5 I1 M64 I1 M44 D1 M9 D5 M31 D3 M23 I1 M19 I1 M25 I1 M26 D1 M19 I1 M9&#xD;&#xA;    ###&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cole-trapnell-lab.github.io/cufflinks/&#xD;&#xA;  [2]: https://groups.google.com/d/msg/tuxedo-tools-users/T-qE4P67-HU/O_up6Yh6BgAJ&#xD;&#xA;  [3]: http://genometools.org/&#xD;&#xA;  [4]: http://gt-users@genometools.org&#xD;&#xA;  [5]: https://testtoolshed.g2.bx.psu.edu/repository/display_tool?repository_id=afcb6456d8e300ed&amp;tool_config=database%2Fcommunity_files%2F000%2Frepo_21%2Ffml_gff_converter_programs%2Fgalaxy%2Fgff3_to_gtf.xml&amp;changeset_revision=4c459744cab1&#xD;&#xA;  [6]: https://wiki.gacrc.uga.edu/wiki/Gmap" />
  <row Id="4382" PostHistoryTypeId="5" PostId="987" RevisionGUID="537b7fa5-16c7-4270-ab8b-e9559de4a9cf" CreationDate="2017-07-14T15:18:35.790" UserId="492" Comment="Update answer with new changes to bcftools 1.5" Text="This now works with the development version of Bcftools v1.5 (commit 4f134df). Thanks to Petr Danecek for [adding the feature](https://github.com/samtools/bcftools/issues/639). I expect this feature to make its way into the next release of Bcftools:&#xD;&#xA;&#xD;&#xA;    git clone git://github.com/samtools/htslib.git&#xD;&#xA;    git clone git://github.com/samtools/bcftools.git&#xD;&#xA;    (cd bcftools; make)&#xD;&#xA;&#xD;&#xA;    bgzip Test.vcf&#xD;&#xA;    ./bcftools/bcftools index Test.vcf.gz&#xD;&#xA;    ./bcftools/bcftools filter -i 'AD[1-] &gt; 10' Test.vcf.gz &#xD;&#xA;&#xD;&#xA;Output without header (I have modified the second line to be tri-allelic to demonstrate the filtering works):&#xD;&#xA;&#xD;&#xA;    3	187451609	rs1880101	A	G	39794	PASS	AC=2;AF=1;AN=2;BaseQRankSum=1.859;ClippingRankSum=0;DB;DP=995;ExcessHet=3.0103;FS=0;MLEAC=2;MLEAF=1;MQ=60;MQRankSum=0;QD=24.56;ReadPosRankSum=0.406;SOR=8.234	GT:AD:DP:GQ:PL	1/1:1,988:989:99:39808,2949,0&#xD;&#xA;    4	1803279	.	T	G	0	PASS	AC=0;AF=0;AN=2;BaseQRankSum=-6.652;ClippingRankSum=0;DP=245;ExcessHet=3.0103;FS=89.753;MLEAC=0;MLEAF=0;MQ=59.97;MQRankSum=0;ReadPosRankSum=-2.523;SOR=6.357	GT:AD:DP:GQ:PL	0/0/0:211,3,34:234:99:0,364,6739&#xD;&#xA;    4	1803307	rs2305183	T	C	2486.6	PASS	AC=1;AF=0.5;AN=2;BaseQRankSum=-5.049;ClippingRankSum=0;DB;DP=215;ExcessHet=3.0103;FS=1.11;MLEAC=1;MLEAF=0.5;MQ=59.97;MQRankSum=0;QD=11.95;ReadPosRankSum=-0.045;SOR=0.809	GT:AD:DP:GQ:PL	0/1:116,92:208:99:2494,0,3673&#xD;&#xA;    4	1803704	rs2234909	T	C	6676.6	PASS	AC=1;AF=0.5;AN=2;BaseQRankSum=-2.605;ClippingRankSum=0;DB;DP=456;ExcessHet=3.0103;FS=1.753;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=14.71;ReadPosRankSum=0.324;SOR=0.849	GT:AD:DP:GQ:PL	0/1:220,234:454:99:6684,0,6366&#xD;&#xA;    4	1803824	rs2305184	C	G	2030.6	PASS	AC=1;AF=0.5;AN=2;BaseQRankSum=8.083;ClippingRankSum=0;DB;DP=124;ExcessHet=3.0103;FS=6.128;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=16.51;ReadPosRankSum=0.18;SOR=0.096	GT:AD:DP:GQ:PL	0/1:62,61:123:99:2038,0,1766&#xD;&#xA;    4	1805296	rs3135883	G	A	3876.03	PASS	AC=2;AF=1;AN=2;DB;DP=110;ExcessHet=3.0103;FS=0;MLEAC=2;MLEAF=1;MQ=60;QD=29.22;SOR=9.401	GT:AD:DP:GQ:PL	1/1:0,109:109:99:3890,326,0&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4383" PostHistoryTypeId="5" PostId="2068" RevisionGUID="8a68ab3b-520f-4236-bb3f-afcfab9eb337" CreationDate="2017-07-14T15:28:17.340" UserId="294" Comment="added 812 characters in body" Text="I would like to convert a file in `gff3` format to a `gtf2.2` format.&#xD;&#xA;&#xD;&#xA;The reason why I would like to do this is: I have a set of transcripts assembled by a bunch of different software (and using RNA-seq data from different sequencing technologies) and I would like to compare each of those sets with the reference transcriptome annotation for that species (*D. melanogaster*). I [already asked for the community advice][1] about how to proceed on that, but to run the suggested software ([TACO][2] and [cuffmerge][3]) I need to have a GTF file containing the experimental transcripts to compare to the reference transcripts (GTF).&#xD;&#xA;&#xD;&#xA;Up to now, I **unsuccessfully** tried: &#xD;&#xA;&#xD;&#xA; 1. `gffread` utility in the [Cufflinks][4] package (`gffread input.gff3 -T -o output.gtf`): this results in an empty `output.gtf` file and an empty `log` file (used `Cufflinks` v.2.2.1) - I contacted the authors via their [Google group][5] but haven't heard of them yet &#xD;&#xA; 2. `gff3_to_gtf` utility in the [GenomeTools][6] (`gt`) package (`gt gff3_to_gtf input.gff3 -o output.gtf`): the output is not created and the log file is not informative - I contacted the authors via their [mailing list][7] but haven't heard of them yet &#xD;&#xA; 3. `GFF3_to_GTF` utility in the [FML][8] package (`./gff3_to_gtf_converter.pl input.gff3 output.gtf`): the output just contains a header (`##gff-version 2.5`) and the `log` is empty&#xD;&#xA;&#xD;&#xA;The `gff3` file was created as output of [GMAP][9], and contains the transcripts as found by alignment to the reference (specifying option `-f gff3_match_cdna`).&#xD;&#xA;&#xD;&#xA;Here is the beginning of the gff3 file I tried to convert: &#xD;&#xA;&#xD;&#xA;    $ head r9_gmap_match-cdna.gff&#xD;&#xA;    ##gff-version   3&#xD;&#xA;    # Generated by GMAP version 2017-04-24 using call:  /home/aechchik/software/gmap-2017-04-24/bin/gmap.sse42 -d gmapidx -D /scratch/beegfs/monthly/aechchik/isoforms/ref/chromosomes/ -f gff3_match_cdna -n 0 -t 20 r9_2d.fasta&#xD;&#xA;    2L    gmapidx    cDNA_match    18442664    18443024    79    -    . ID=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1;Name=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d;Target=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d 141 489;Gap=M13 I10 M8 D3 M8 D3 M32 D1 M4 D2 M5 D3 M34 I3 M6 D1 M7 D1 M5 D1 M30 D2 M16 I1 M8 D3 M10 D1 M5 D1 M6 I1 M8 I1 M8 D1 M33 D3 M33 I1 M28 D3 M25&#xD;&#xA;    ###&#xD;&#xA;    3R    gmapidx    cDNA_match    15853880    15855465    96    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 80 1645;Gap=M46 D2 M12 D2 M157 D3 M4 D1 M50 I2 M3 I1 M12 I1 M68 I1 M66 I1 M53 D2 M47 D1 M16 D1 M35 D1 M155 D1 M28 D1 M166 D2 M47 D1 M8 D4 M69 D1 M28 D1 M5 D1 M12 D1 M202 I1 M115 D1 M61 I2 M7 D1 M7 I1 M36 D2 M41&#xD;&#xA;    3R    gmapidx    cDNA_match    15855529    15855742    97    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 1646 1856;Gap=M21 D1 M68 D1 M85 D1 M37&#xD;&#xA;    ###&#xD;&#xA;    X    gmapidx    cDNA_match    14837810    14838142    93    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 74 406;Gap=M13 D1 M182 I1 M14 I2 M56 I1 M30 D2 M21 D1 M13&#xD;&#xA;    X    gmapidx    cDNA_match    14837470    14837753    92    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 407 688;Gap=I2 M5 I1 M64 I1 M44 D1 M9 D5 M31 D3 M23 I1 M19 I1 M25 I1 M26 D1 M19 I1 M9&#xD;&#xA;    ###&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/2036/tools-to-reconcile-experimental-transcripts-with-reference-annotation&#xD;&#xA;  [2]: https://tacorna.github.io/&#xD;&#xA;  [3]: http://cole-trapnell-lab.github.io/cufflinks/cuffmerge/&#xD;&#xA;  [4]: http://cole-trapnell-lab.github.io/cufflinks/&#xD;&#xA;  [5]: https://groups.google.com/d/msg/tuxedo-tools-users/T-qE4P67-HU/O_up6Yh6BgAJ&#xD;&#xA;  [6]: http://genometools.org/&#xD;&#xA;  [7]: http://gt-users@genometools.org&#xD;&#xA;  [8]: https://testtoolshed.g2.bx.psu.edu/repository/display_tool?repository_id=afcb6456d8e300ed&amp;tool_config=database%2Fcommunity_files%2F000%2Frepo_21%2Ffml_gff_converter_programs%2Fgalaxy%2Fgff3_to_gtf.xml&amp;changeset_revision=4c459744cab1&#xD;&#xA;  [9]: https://wiki.gacrc.uga.edu/wiki/Gmap" />
  <row Id="4384" PostHistoryTypeId="2" PostId="2070" RevisionGUID="e34e08e7-81a8-4308-9e3a-9ea380e4c7a5" CreationDate="2017-07-14T16:12:50.580" UserId="29" Text="**Ensembl** contains this information: When you [go to the “phenotype” menu item of a given gene](http://www.ensembl.org/Homo_sapiens/Gene/Phenotype?db=core;g=ENSG00000164508;r=6:25726132-25726527;t=ENST00000297012#ALL_tablePanel), you will see a list of variants (potentially after clicking on “ALL associated variants”) with their associated genomic position.&#xD;&#xA;&#xD;&#xA;You can subtract the transcript’s start position from that position to find out the residue (of the translated sequence, and be careful to subtract the lengths of all preceding introns!). Unfortunately this seems to require some manual work.&#xD;&#xA;&#xD;&#xA;Alternatively you can [query the individual SNPs](http://www.ensembl.org/Homo_sapiens/Variation/Phenotype?db=core;g=ENSG00000164508;r=6:25726132-25726527;t=ENST00000297012;v=rs2275906;vdb=variation;vf=1677930), which will tell you their position in all transcripts (in this case: Leu 298/68/244):&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/zr2Q4.png" />
  <row Id="4385" PostHistoryTypeId="4" PostId="2049" RevisionGUID="a01bea3b-9d1a-43af-9b0b-54a4d80e1827" CreationDate="2017-07-14T16:21:15.993" UserId="727" Comment="edited title" Text="Profile of conservation between bacterial sequences and human protein" />
  <row Id="4386" PostHistoryTypeId="5" PostId="2049" RevisionGUID="be533971-5d0b-4891-baa5-84bc5d0002e2" CreationDate="2017-07-14T17:34:41.727" UserId="727" Comment="added 140 characters in body" Text="My goal is to make a conservation plot between bacterial sequences and a human protein.&#xD;&#xA;&#xD;&#xA;So far, I have a FASTA file of the protein, and a FASTA file with the sequences of the proteins from the BLAST results.&#xD;&#xA;&#xD;&#xA;In my initial attempts, I have tried to do this using msa software:&#xD;&#xA;&#xD;&#xA;I have downloaded clustalx and run a profile alignment using the single protein sequence as Profile 1 and the sequences from the BLAST results as Profile 2, and selected the option &quot;Align Sequences to Profile 1&quot;.&#xD;&#xA;&#xD;&#xA;I'm having difficulty interpreting and dealing with the results. Since the majority of the sequences aren't aligned, the results are a total mess, with a huge number of dashes added.&#xD;&#xA;&#xD;&#xA;My goal is to visualize the number of BLAST hits per amino acid in my protein of interest. &#xD;&#xA;&#xD;&#xA;ie: ![alignment](https://i.stack.imgur.com/L8jL5.jpg)&#xD;&#xA;&#xD;&#xA;I would like to have a graph with my protein on the x-axis and a plot like the regions of high conservation for the multiple alignments, except with the spikes corresponding to a high number of BLAST hits. This would allow me to identify regions of my protein that have higher sequence similarity to bacteria than others.&#xD;&#xA;&#xD;&#xA;Is there a better way to achieve this or to salvage the results from the profile alignment?&#xD;&#xA;&#xD;&#xA;Thanks." />
  <row Id="4387" PostHistoryTypeId="5" PostId="2051" RevisionGUID="3aef88ff-1b46-4fbc-a5ee-ce1ae839fca6" CreationDate="2017-07-14T21:06:46.883" UserId="57" Comment="expanded abbreveation I was not familiar with" Text="I'm looking at Reduced representation bisulfite sequencing (RRBS) data from ENCODE, and to align the FASTQ files I've used Bismark with Bowtie 1. When I load the resulting BAM file into `R` with `Rsamtools` and `GenomicRanges`, I get something like this:&#xD;&#xA;&#xD;&#xA;    &gt; reads&#xD;&#xA;    GRanges object with 2 ranges and 5 metadata columns:&#xD;&#xA;          seqnames         ranges strand |                     seq                                   XM          XR          XG        NM&#xD;&#xA;             &lt;Rle&gt;      &lt;IRanges&gt;  &lt;Rle&gt; |          &lt;DNAStringSet&gt;                          &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;integer&gt;&#xD;&#xA;      [1]     chrM [16523, 16558]      - | ATAAAACCTA...CCCTTAAATA .....h........h.......z.............          CT          GA         3&#xD;&#xA;      [2]     chrM [16524, 16559]      + | TAAAGTTTAA...TTTTAAATAA .....hh.......hhh.h.z...hhhh........          CT          CT        11&#xD;&#xA;      -------&#xD;&#xA;      seqinfo: 25 sequences from an unspecified genome&#xD;&#xA;&#xD;&#xA;For the read on the - strand (which is aligned to the &quot;G-&gt;A&quot;-converted reference), how should I compare methylation calls to those on the + strand, since they don't line up?&#xD;&#xA;&#xD;&#xA;For example, the `z` in the methylation string of the - strand is one position ahead of the `z` in the + strand (which makes sense because of symmetric CpG methylation). But how should I determine whether these two methylation calls are &quot;essentially the same&quot; or not?" />
  <row Id="4392" PostHistoryTypeId="2" PostId="2072" RevisionGUID="45958714-e6eb-4ae2-add7-1c4957e810e3" CreationDate="2017-07-15T11:30:13.013" UserId="77" Text="The following bit of python code should work:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    lastTranscript = [None, None, None, []]  # ID, chrom, strand, [(start, end, score), ...]&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    def getID(s):&#xD;&#xA;        &quot;&quot;&quot;Parse out the ID attribute&quot;&quot;&quot;&#xD;&#xA;        s = s.split(&quot;;&quot;)&#xD;&#xA;        for k in s:&#xD;&#xA;            if k.startswith(&quot;ID=&quot;):&#xD;&#xA;                return k[3:]&#xD;&#xA;        return None&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    def dumpLastTranscript():&#xD;&#xA;        &quot;&quot;&quot;Print the last transcript&quot;&quot;&quot;&#xD;&#xA;        bounds = sorted(lastTranscript[3])&#xD;&#xA;        print(&quot;{}\tgmapidx\tgene\t{}\t{}\t{}\t.\t.\tgene_id \&quot;{}\&quot;; transcript_id \&quot;{}\&quot;;&quot;.format(lastTranscript[1], bounds[0][0], bounds[-1][1], lastTranscript[2], lastTranscript[0], lastTranscript[0]))&#xD;&#xA;        print(&quot;{}\tgmapidx\ttranscript\t{}\t{}\t{}\t.\t.\tgene_id \&quot;{}\&quot;; transcript_id \&quot;{}\&quot;;&quot;.format(lastTranscript[1], bounds[0][0], bounds[-1][1], lastTranscript[2], lastTranscript[0], lastTranscript[0]))&#xD;&#xA;        for start, end, score in bounds:&#xD;&#xA;            print(&quot;{}\tgmapidx\texon\t{}\t{}\t{}\t{}\t.\tgene_id \&quot;{}\&quot;; transcript_id \&quot;{}\&quot;;&quot;.format(lastTranscript[1], start, end, score, lastTranscript[2], lastTranscript[0], lastTranscript[0]))&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    def handleLine(cols):&#xD;&#xA;        &quot;&quot;&quot;Handle a single line, appending the exon bounds to the previous if relevant&quot;&quot;&quot;&#xD;&#xA;        ID = getID(cols[8])&#xD;&#xA;        assert(ID is not None)&#xD;&#xA;        if lastTranscript[0] is not None and lastTranscript[0] != ID:&#xD;&#xA;            dumpLastTranscript()&#xD;&#xA;            lastTranscript[3] = []&#xD;&#xA;        lastTranscript[0] = ID&#xD;&#xA;        lastTranscript[1] = cols[0]&#xD;&#xA;        lastTranscript[2] = cols[6]&#xD;&#xA;        lastTranscript[3].append((int(cols[3]), int(cols[4]), cols[5]))&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    f = open(sys.argv[1])&#xD;&#xA;    for line in f:&#xD;&#xA;        if line.startswith(&quot;#&quot;):&#xD;&#xA;            continue&#xD;&#xA;        cols = line.strip().split(&quot;\t&quot;)&#xD;&#xA;        handleLine(cols)&#xD;&#xA;    &#xD;&#xA;    dumpLastTranscript()&#xD;&#xA;    f.close()&#xD;&#xA;&#xD;&#xA;If you saved that as `gff2gtf.py` and made it executable, the usage would be `./gff2gtf.py foo.gff3 &gt; foo.gtf`. With the example that you provided in your post, the result is:&#xD;&#xA;&#xD;&#xA;    2L	gmapidx	gene	18442664	18443024	-	.	.	gene_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;; transcript_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    2L	gmapidx	transcript	18442664	18443024	-	.	.	gene_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;; transcript_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    2L	gmapidx	exon	18442664	18443024	79	-	.	gene_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;; transcript_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    3R	gmapidx	gene	15853880	15855742	+	.	.	gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    3R	gmapidx	transcript	15853880	15855742	+	.	.	gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    3R	gmapidx	exon	15853880	15855465	96	+	.	gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    3R	gmapidx	exon	15855529	15855742	97	+	.	gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    X	gmapidx	gene	14837470	14838142	-	.	.	gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    X	gmapidx	transcript	14837470	14838142	-	.	.	gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    X	gmapidx	exon	14837470	14837753	92	-	.	gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    X	gmapidx	exon	14837810	14838142	93	-	.	gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xD;&#xA;&#xD;&#xA;That at least looks like correct gtf 2.2 format to me." />
  <row Id="4393" PostHistoryTypeId="2" PostId="2073" RevisionGUID="88ec9012-d9bf-473f-96d5-791a063103b2" CreationDate="2017-07-15T15:02:59.803" UserId="506" Text="I'm currently learning about GSEA in the hopes of using it in my analysis of differentially expressed genes , and I just had a few questions about the program, specifically about GSEAPreranked, which I need cleared up.&#xD;&#xA;&#xD;&#xA;1) On the ranked list needed for GSEA input, should the list include all genes, or only those that pass a certain threshold of significance (i.e. fold change higher than 2, p value less than 0.05, etc.)? Ideally I'd like to sort the genes by fold change alone as I don't trust my p values as much, so should I only include genes with high fold changes?&#xD;&#xA;&#xD;&#xA;2) I am comparing multiple conditions of disease with different treatments. Am I correct that GSEA only compares two conditions? If this is the case should I run GSEA for each control/treatment comparison? Would this be conventional?&#xD;&#xA;&#xD;&#xA;Thanks for the help!&#xD;&#xA;" />
  <row Id="4394" PostHistoryTypeId="1" PostId="2073" RevisionGUID="88ec9012-d9bf-473f-96d5-791a063103b2" CreationDate="2017-07-15T15:02:59.803" UserId="506" Text="Basic questions about GSEA" />
  <row Id="4395" PostHistoryTypeId="3" PostId="2073" RevisionGUID="88ec9012-d9bf-473f-96d5-791a063103b2" CreationDate="2017-07-15T15:02:59.803" UserId="506" Text="&lt;gene&gt;&lt;annotation&gt;&lt;rna&gt;" />
  <row Id="4397" PostHistoryTypeId="2" PostId="2074" RevisionGUID="22947263-6082-409e-9f46-224f10cf2c9b" CreationDate="2017-07-15T15:46:52.880" UserId="48" Text="You seem to refer to the GSEA provided by the [Broad institute][1], (there are other GSEA algorithms). &#xD;&#xA;&#xD;&#xA;1) You can provide whatever you wish, but if you want to know if those gene sets in which side of the ordered list are they, then provide all the list (of genes) you have.&#xD;&#xA;&#xD;&#xA;2) GSEA analyize if the order of a given list distributes in certain way the elements of another (unordered) list. In your case you would need to compute the GSEA for each treatment vs control comparison.&#xD;&#xA;&#xD;&#xA;  [1]: http://software.broadinstitute.org/gsea/index.jsp" />
  <row Id="4398" PostHistoryTypeId="50" PostId="756" RevisionGUID="6448cf02-3a76-45fe-ad9b-af92cd669a5e" CreationDate="2017-07-15T20:02:21.063" UserId="-1" />
  <row Id="4399" PostHistoryTypeId="6" PostId="2049" RevisionGUID="e0aab16d-f1db-4284-8d31-ed74635cbc47" CreationDate="2017-07-15T20:47:11.947" UserId="272" Comment="sequence-alignment tag misspelled" Text="&lt;sequence-alignment&gt;" />
  <row Id="4400" PostHistoryTypeId="24" PostId="2049" RevisionGUID="e0aab16d-f1db-4284-8d31-ed74635cbc47" CreationDate="2017-07-15T20:47:11.947" Comment="Proposed by 272 approved by 77 edit id of 237" />
  <row Id="4401" PostHistoryTypeId="6" PostId="997" RevisionGUID="610ee593-39a5-4d49-bf4c-e78c3edbd46e" CreationDate="2017-07-15T20:47:23.833" UserId="272" Comment="sequence-alignment tag misspelled" Text="&lt;codon&gt;&lt;sequence-alignment&gt;" />
  <row Id="4402" PostHistoryTypeId="24" PostId="997" RevisionGUID="610ee593-39a5-4d49-bf4c-e78c3edbd46e" CreationDate="2017-07-15T20:47:23.833" Comment="Proposed by 272 approved by 77 edit id of 238" />
  <row Id="4403" PostHistoryTypeId="2" PostId="2075" RevisionGUID="e68a6ec8-d518-428d-9a60-3b594a897d99" CreationDate="2017-07-16T13:37:45.500" UserId="929" Text="I'm not sure what you mean by it being a mess? That looks like a pretty good alignment to me, and it most certainly has aligned all the sequences. You are nearly always going to have gaps (see my MSA below, which are all paralogs from the *same genome* so they couldn't really be more closely related, and yet, there are gaps). That comes with its own set of problems mind you, as you need to decide how you're going to deal with them.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;You also don't necessarily have to do a profile alignment. Those sequences don't look massively divergent to me, so straight forward sequence alignment would probably give you more or less the same result.&#xD;&#xA;&#xD;&#xA;&gt;My goal is to visualize the number of BLAST hits per amino acid in my protein of interest.&#xD;&#xA;&#xD;&#xA;This doesn't really make sense. You should really be searching for the number of BLAST hits with a particular domain, if you want to infer homology.&#xD;&#xA;&#xD;&#xA;If you want a per-position visualisation of the conservation, you could look at the shannon entropy for each column and plot that. I wrote a script to do just that a little while ago: https://github.com/jrjhealey/bioinfo-tools/blob/master/Shannon.py&#xD;&#xA;&#xD;&#xA;Just beware it's not super well tested yet. Feed an MSA in with as many sequences as you want to analyse, but you'll have to have identified the sequences and done the alignment first.&#xD;&#xA;&#xD;&#xA;For example, given this MSA:&#xD;&#xA;&#xD;&#xA;        16    149&#xD;&#xA;    PAU_02775  MSTTPEQIAV EYPIPTYRFV VSLGDEQIPF NSVSGLDISH DVIEYKDGTG&#xD;&#xA;    PLT_01696  MSTTPEQIAV EYPIPTYRFV VSIGDEQIPF NSVSGLDISH DVIEYKDGTG&#xD;&#xA;    PAK_02606  MSTTPEQIAV EYPIPTYRFV VSIGDEQVPF NSVSGLDISH DVIEYKDGTG&#xD;&#xA;    PLT_01736  MSTTPEQIAV EYPIPTYRFV VSIGDEKVPF NSVSGLDISH DVIEYKDGTG&#xD;&#xA;    PAK_01896  MTTTT----V DYPIPAYRFV VSVGDEQIPF NNVSGLDITY DVIEYKDGTG&#xD;&#xA;    PAU_02074  MATTT----V DYPIPAYRFV VSVGDEQIPF NSVSGLDITY DVIEYKDGTG&#xD;&#xA;    PLT_02424  MSVTTEQIAV DYPIPTYRFV VSVGDEQIPF NNVSGLDITY DVIEYKDGTG&#xD;&#xA;    PLT_01716  MTITPEQIAV DYPIPAYRFV VSVGDEKIPF NNVSGLDVHY DVIEYKDGTG&#xD;&#xA;    PLT_01758  MAITPEQIAV EYPIPTYRFV VSVGDEQIPF NNVSGLDVHY DVIEYKDGIG&#xD;&#xA;    PAK_03203  MSTSTSQIAV EYPIPVYRFI VSIGDDQIPF NSVSGLDINY DTIEYRDGVG&#xD;&#xA;    PAU_03392  MSTSTSQIAV EYPIPVYRFI VSVGDEKIPF NSVSGLDISY DTIEYRDGVG&#xD;&#xA;    PAK_02014  MSITQEQIAA EYPIPSYRFM VSIGDVQVPF NSVSGLDRKY EVIEYKDGIG&#xD;&#xA;    PAU_02206  MSITQEQIAA EYPIPSYRFM VSIGDVQVPF NSVSGLDRKY EVIEYKDGIG&#xD;&#xA;    PAK_01787  MSTTADQIAV QYPIPTYRFV VTIGDEQMCF QSVSGLDISY DTIEYRDGVG&#xD;&#xA;    PAU_01961  MSTTADQIAV QYPIPTYRFV VTIGDEQMCF QSVSGLDISY DTIEYRDGVG&#xD;&#xA;    PLT_02568  MSTTVDQIAV QYPIPTYRFV VTVGDEQMSF QSVSGLDISY DTIEYRDGIG&#xD;&#xA;    &#xD;&#xA;               NYYKMPGQRQ AINISLRKGV FSGDTKLFDW INSIQLNQVE KKDISISLTN&#xD;&#xA;               NYYKMPGQRQ AINISLRKGV FSGDTKLFDW INSIQLNQVE KKDISISLTN&#xD;&#xA;               NYYKMPGQRQ AINISLRKGV FSGDTKLFDW INSIQLNQVE KKDISISLTN&#xD;&#xA;               NYYKMPGQRQ AINITLRKGV FSGDTKLFDW LNSIQLNQVE KKDISISLTN&#xD;&#xA;               NYYKMPGQRQ LINITLRKGV FPGDTKLFDW LNSIQLNQVE KKDVSISLTN&#xD;&#xA;               NYYKMPGQRQ LINITLRKGV FPGDTKLFDW LNSIQLNQVE KKDVSISLTN&#xD;&#xA;               NHYKMPGQRQ LINITLRKGV FPGDTKLFDW LNSIQLNQVE KKDVSISLTN&#xD;&#xA;               NYYKMPGQRQ SINITLRKGV FPGDTKLFDW INSIQLNQVE KKDIAISLTN&#xD;&#xA;               NYYKMPGQRQ SINITLRKGV FPGDTKLFDW INSIQLNQVE KKDIAISLTN&#xD;&#xA;               NWFKMPGQSQ LVNITLRKGV FPGKTELFDW INSIQLNQVE KKDITISLTN&#xD;&#xA;               NWFKMPGQSQ STNITLRKGV FPGKTELFDW INSIQLNQVE KKDITISLTN&#xD;&#xA;               NYYKMPGQIQ RVDITLRKGI FSGKNDLFNW INSIELNRVE KKDITISLTN&#xD;&#xA;               NYYKMPGQIQ RVDITLRKGI FSGKNDLFNW INSIELNRVE KKDITISLTN&#xD;&#xA;               NWLQMPGQRQ RPTITLKRGI FKGQSKLYDW INSISLNQIE KKDISISLTD&#xD;&#xA;               NWLQMPGQRQ RPTITLKRGI FKGQSKLYDW INSISLNQIE KKDISISLTD&#xD;&#xA;               NWLQMPGQRQ RPSITLKRGI FKGQSKLYDW INSISLNQIE KKDISISLTD&#xD;&#xA;    &#xD;&#xA;               EAGTEILMTW SVANAFPTSL TSPSFDATSN EVAVQEITLT ADRVTIQAA&#xD;&#xA;               EAGTEILMTW SVANAFPTSL ISPSFDATSN EVAVQEITLT ADRVTIQAA&#xD;&#xA;               EAGTEILMTW SVANAFPTSL TSPSFDATSN EVAVQEITLT ADRVTIQAA&#xD;&#xA;               EAGTEILMTW SVANAFPTSL TAPAFDATSN EVAVQEISLT ADRVTIQAA&#xD;&#xA;               ETGTEILMSW SVANAFPTSL TSPSFDATSN DIAVQEIKLT ADRVTIQAA&#xD;&#xA;               EVGTEILMTW SVANAFPTSL TSPSFDATSN DIAVQEIKLT ADRVTIQAA&#xD;&#xA;               EAGTEILMSW SVANAFPTSL TSPSFDATSN DIAVQEIKLT ADRVMIQAA&#xD;&#xA;               ETGSQILMTW NVANAFPTSF TSPSFDAASN DIAIQEIALV ADRVTIQAP&#xD;&#xA;               EAGTEILMTW NVANAFPTSF TSPSFDATSN EIAVQEIALT ADRVTIQAA&#xD;&#xA;               DAGTELLMTW NVSNAFPTSL TSPSFDATSN DIAVQEITLT ADRVIMQAV&#xD;&#xA;               DAGTELLMTW NVSNAFPTSL TSPSFDATSN DIAVQEITLM ADRVIMQAV&#xD;&#xA;               DTGSEVLMSW VVSNAFPSSL TAPSFDASSN EIAVQEISLV ADRVTIQVP&#xD;&#xA;               DTGSKVLMSW VVSNAFPSSL TAPSFDASSN EIAVQEISLV ADRVTIQVP&#xD;&#xA;               ETGSNLLITW NIANAFPEKL TAPSFDATSN EVAVQEMSLK ADRVTVEFH&#xD;&#xA;               ETGSNLLITW NIANAFPEKL TAPSFDATSN EVAVQEISLK ADRVTVEFH&#xD;&#xA;               ETGSNLLITW NIANAFPEKL TAPSFDATSN EVAVQEISLK ADRVTVEFH&#xD;&#xA;&#xD;&#xA;You'd get this plot:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&gt; I would like to have a graph with my protein on the x-axis and a plot&#xD;&#xA;&gt; like the regions of high conservation for the multiple alignments,&#xD;&#xA;&gt; except with the spikes corresponding to a high number of BLAST hits.&#xD;&#xA;&gt; This would allow me to identify regions of my protein that have higher&#xD;&#xA;&gt; sequence similarity to bacteria than others.&#xD;&#xA;&#xD;&#xA;I'm not sure your logic is quite right here though. Blast won't give you hits depending on a particular position. It's a local aligner, so it'll just return you hits where at least some part of your query matches at least some part of another.&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/zCFTz.png&#xD;&#xA;&#xD;&#xA;What you could do is take the logic in the script above, and just use a different metric. For example, perhaps you could count the proportion of sequences which have the most common amino acid at a given position within your MSA. That would be fairly crude though.&#xD;&#xA;&#xD;&#xA;As you say in the comments,&#xD;&#xA;&#xD;&#xA;&gt; My final goal is to produce a plot visualizing regions of high bacterial sequence similarity to my human protein of interest.&#xD;&#xA;&#xD;&#xA;your original alignment will show you this intrinsically, if only you include the sequences of all the BLAST hits in the first place. Thus your work flow will be:&#xD;&#xA;&#xD;&#xA;1. Blast sequence of interest.&#xD;&#xA;2. Download all/as many hits as you want (bear in mind the E-value/bitscore and the number of hits you get. It might only be a few dozen, in which case you can use the lot, but if not, just take all the hits below a certain cut-off.)&#xD;&#xA;3. Align all the sequences.&#xD;&#xA;4. Look at the column scores for the whole MSA. You can use whatever metric of conservation you like really. Might be as simple as proportion of sequences with the most common residue, or something more complex like Shannon entropy (though as you can see in the graph above Shannon entropy can be kinda noisy) etc." />
  <row Id="4404" PostHistoryTypeId="2" PostId="2076" RevisionGUID="212c4da0-f58f-4be6-bcff-ab409f3215da" CreationDate="2017-07-17T00:31:47.860" UserId="73" Text="I don't know of any transcript-to-transcript aligners that are able to do this, but [LAST](http://last.cbrc.jp/doc/lastal.html) can align transcript queries to protein reference sequences using a specified frameshift cost. Here's the specific documentation for that option:&#xD;&#xA;&#xD;&#xA;&gt; -F COST 	&#xD;&#xA;&gt; &#xD;&#xA;&gt; Align DNA queries to protein reference sequences, using the specified&#xD;&#xA;&gt; frameshift cost. A value of 15 seems to be reasonable. (As a special&#xD;&#xA;&gt; case, -F0 means DNA-versus-protein alignment without frameshifts,&#xD;&#xA;&gt; which is faster.) The output looks like this:&#xD;&#xA;&gt; &#xD;&#xA;&gt;     a score=108 s prot 2  40 + 649&#xD;&#xA;&gt;     FLLQAVKLQDP-STPHQIVPSP-VSDLIATHTLCPRMKYQDD s dna  8 117 + 999&#xD;&#xA;&gt;     FFLQ-IKLWDP\STPH*IVSSP/PSDLISAHTLCPRMKSQDN&#xD;&#xA;&gt; &#xD;&#xA;&gt; The \ indicates a forward shift by one nucleotide, and the / indicates&#xD;&#xA;&gt; a reverse shift by one nucleotide. The * indicates a stop codon. The&#xD;&#xA;&gt; same alignment in tabular format looks like this:&#xD;&#xA;&gt; &#xD;&#xA;&gt;     108 prot 2 40 + 649 dna 8 117 + 999 4,1:0,6,0:1,10,0:-1,19&#xD;&#xA;&gt; &#xD;&#xA;&gt; The &quot;-1&quot; indicates the reverse frameshift." />
  <row Id="4405" PostHistoryTypeId="5" PostId="2076" RevisionGUID="11edc309-4cff-483e-a140-1ae91cbe4f77" CreationDate="2017-07-17T00:41:12.627" UserId="73" Comment="added 283 characters in body" Text="I don't know of any transcript-to-transcript aligners that are able to do this, but [LAST](http://last.cbrc.jp/doc/lastal.html) can align transcript queries to protein reference sequences using a specified frameshift cost. Here's the specific documentation for that option:&#xD;&#xA;&#xD;&#xA;&gt; -F COST 	&#xD;&#xA;&gt; &#xD;&#xA;&gt; Align DNA queries to protein reference sequences, using the specified&#xD;&#xA;&gt; frameshift cost. A value of 15 seems to be reasonable. (As a special&#xD;&#xA;&gt; case, -F0 means DNA-versus-protein alignment without frameshifts,&#xD;&#xA;&gt; which is faster.) The output looks like this:&#xD;&#xA;&gt; &#xD;&#xA;&gt;     a score=108 s prot 2  40 + 649&#xD;&#xA;&gt;     FLLQAVKLQDP-STPHQIVPSP-VSDLIATHTLCPRMKYQDD s dna  8 117 + 999&#xD;&#xA;&gt;     FFLQ-IKLWDP\STPH*IVSSP/PSDLISAHTLCPRMKSQDN&#xD;&#xA;&gt; &#xD;&#xA;&gt; The \ indicates a forward shift by one nucleotide, and the / indicates&#xD;&#xA;&gt; a reverse shift by one nucleotide. The * indicates a stop codon. The&#xD;&#xA;&gt; same alignment in tabular format looks like this:&#xD;&#xA;&gt; &#xD;&#xA;&gt;     108 prot 2 40 + 649 dna 8 117 + 999 4,1:0,6,0:1,10,0:-1,19&#xD;&#xA;&gt; &#xD;&#xA;&gt; The &quot;-1&quot; indicates the reverse frameshift.&#xD;&#xA;&#xD;&#xA;I've sent an email to the [LAST mailing list](https://groups.google.com/forum/#!forum/last-align) about adding a frameshift penalty for transcript-to-transcript matching; I've been pleasantly surprised with the requested features that Martin Frith has added to LAST in the past. " />
  <row Id="4406" PostHistoryTypeId="2" PostId="2077" RevisionGUID="cc40e9d2-6356-4a16-8bb6-3ebbb84b68df" CreationDate="2017-07-17T02:10:55.023" UserId="73" Text="Note that DIN (DNA Integrity Number) and RIN (RNA Integrity Number) are different scores. I've had trouble in the past finding code or formulas for calculating both of these scores, which is very frustrating considering how frequently they are used in research papers and for NGS QC. The closest that I have been able to get to this are demonstrative graphs for various [RIN numbers](http://bmcmolbiol.biomedcentral.com/articles/10.1186/1471-2199-7-3) and [DIN numbers](http://www.agilent.com/cs/library/applications/5991-5258EN.pdf), with a description of the variables that are included in the model.&#xD;&#xA;&#xD;&#xA;The code that @719016 provided appears to attempt to calculate the RIN, not the DIN.&#xD;&#xA;&#xD;&#xA;Just in case it's helpful, here is an extract of that code, only including lines that seem to be directly involved in calculating the actual B_RIN values. This is not runnable, because it depends on particular input files, and it's still pretty unintelligible:&#xD;&#xA;&#xD;&#xA;    myfiles&lt;-list.files()&#xD;&#xA;    results&lt;-myfiles[which(substr(myfiles,69,79)==&quot;Results.csv&quot;)]&#xD;&#xA;    n &lt;- which(substr(myfiles,69,79)==&quot;Results.csv&quot;)&#xD;&#xA;    targets &lt;- read.delim(myfiles[n[1]], sep=&quot;,&quot;,header=F,skip=14,as.is=T)&#xD;&#xA;    targets2&lt;- read.delim(myfiles[n[1]], sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xD;&#xA;    str3&lt;-paste0(substr(targets2[1,2],1,67),&quot;_S&quot;)&#xD;&#xA;    chip_num&lt;-length(n)&#xD;&#xA;    samp_num_chip&lt;-length(grep(str3,myfiles))&#xD;&#xA;    samp_num&lt;-length(which(substr(myfiles,69,74)==&quot;Sample&quot;))&#xD;&#xA;    samples1&lt;- matrix(NA, ncol=chip_num, nrow=samp_num)&#xD;&#xA;    for(i in 1:chip_num)&#xD;&#xA;    {&#xD;&#xA;        ##mod chip numb&#xD;&#xA;        targets2&lt;- read.delim(myfiles[n[i]], sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xD;&#xA;        str3&lt;-paste0(substr(targets2[1,2],1,67),&quot;_S&quot;)&#xD;&#xA;        samp_num_chip&lt;-length(grep(str3,myfiles))&#xD;&#xA;        ##mod sample numb&#xD;&#xA;        str1&lt;-substr(targets2[1,2],1,67)&#xD;&#xA;        for(j in 1:samp_num_chip)&#xD;&#xA;        {&#xD;&#xA;            iii&lt;-j&#xD;&#xA;            str5&lt;-paste0(str1,&quot;_Sample&quot;,iii,&quot;.csv&quot;)&#xD;&#xA;            samples1[j,i]&lt;-str5&#xD;&#xA;        }&#xD;&#xA;    }&#xD;&#xA;    samples1&lt;-samples1[grep(&quot;2100&quot;,samples1)]&#xD;&#xA;    ##RIN Calculator#######refined peakfinder####################&#xD;&#xA;    ##read bioanalyzer data into a matrix called dta&#xD;&#xA;    ##since the total RNA and mRNA assays run differently&#xD;&#xA;    ##skip more lines for the total RNA assay&#xD;&#xA;    elec &lt;- read.delim(&quot;ba_lane.txt&quot;, sep=&quot;\t&quot;,header=T,as.is=T)&#xD;&#xA;    qc.mat &lt;- matrix(NA, ncol=1, nrow=nrow(elec))&#xD;&#xA;    dta &lt;- matrix(NA, nrow=1060, ncol=nrow(elec))&#xD;&#xA;    for(i in 1:nrow(elec))&#xD;&#xA;    {&#xD;&#xA;        x &lt;- read.csv(as.character(elec[i,2]), header=F, skip=18, nrows=1060)&#xD;&#xA;        dta[,i] &lt;- x[,2]&#xD;&#xA;    }&#xD;&#xA;    time&lt;- x[,1]&#xD;&#xA;    for(samples in 1:length(samples1))&#xD;&#xA;    {&#xD;&#xA;        ##store max fluor in max.peaks matrix&#xD;&#xA;        lad&lt;-samples&#xD;&#xA;        ti1&lt;-17&#xD;&#xA;        ti2&lt;-65&#xD;&#xA;        p.row&lt;-length(dta[which(time==ti1):which(time==ti1+1),lad])*(ti2-ti1+1)&#xD;&#xA;        peaks &lt;- matrix(NA, nrow=p.row, ncol=1)&#xD;&#xA;        max.peaks &lt;- matrix(NA, nrow=length(seq(ti1,ti2,.05)), ncol=1)&#xD;&#xA;        for (i in 1:length(seq(ti1,ti2,.05)))&#xD;&#xA;        {&#xD;&#xA;            ii&lt;-seq(ti1,ti2,.05)[i]&#xD;&#xA;            x0&lt;- ii&#xD;&#xA;            x0&lt;-round(x0,digits=3)&#xD;&#xA;            y0&lt;- ii&#xD;&#xA;            x&lt;- ii+.05&#xD;&#xA;            x&lt;-round(x,digits=3)&#xD;&#xA;            y&lt;- ii&#xD;&#xA;            max.peaks[i]&lt;- max(dta[which(time==x0):which(time==x),lad])&#xD;&#xA;        }    &#xD;&#xA;        ##human or mouse&#xD;&#xA;        cent&lt;-max(max.peaks)*.6&#xD;&#xA;        ##flatworm&#xD;&#xA;        ##cent&lt;-max(max.peaks)*.05&#xD;&#xA;        ##define the middle of the segments&#xD;&#xA;        max.avg&lt;-seq(ti1+.025,ti2+.025,.05)&#xD;&#xA;        ##find time associated with peaks &gt; 5 fu and fill seconds matrix with them&#xD;&#xA;        ##12.5% of max fu incase chip is fu&#xD;&#xA;        sizes &lt;- which(max.peaks&gt;cent)&#xD;&#xA;        gt35&lt;-which(max.avg[sizes]&gt;35)&#xD;&#xA;        lt55&lt;-which(max.avg[sizes]&lt;55)&#xD;&#xA;        tfr&lt;-which(duplicated(c(gt35,lt55))==TRUE)&#xD;&#xA;        area&lt;-cbind(max.peaks[sizes-1][c(gt35,lt55)[tfr]],&#xD;&#xA;                    max.peaks[sizes][c(gt35,lt55)[tfr]],&#xD;&#xA;                    max.peaks[sizes+1][c(gt35,lt55)[tfr]])&#xD;&#xA;        time2&lt;-cbind(max.avg[sizes-1][c(gt35,lt55)[tfr]],&#xD;&#xA;                     max.avg[sizes][c(gt35,lt55)[tfr]],&#xD;&#xA;                     max.avg[sizes+1][c(gt35,lt55)[tfr]])&#xD;&#xA;        t1&lt;-time2[which(area[,1]&lt;cent),1][1]&#xD;&#xA;        t2&lt;-time2[which(area[,1]&lt;cent),1][2]&#xD;&#xA;        t3&lt;-time2[which(area[,3]&lt;cent),3][1]&#xD;&#xA;        t4&lt;-time2[which(area[,3]&lt;cent),3][2]&#xD;&#xA;        s18a&lt;-ifelse(is.na(t1,-30,round(t1,digits=1)))&#xD;&#xA;        s18b&lt;-ifelse(is.na(t3,-35.05,round(t3,digits=1)))&#xD;&#xA;        s28a&lt;-ifelse(is.na(t2,-30,round(t2,digits=1)))&#xD;&#xA;        s28b&lt;-ifelse(is.na(t4,-30.05,round(t4,digits=1)))&#xD;&#xA;        eta&lt;-which(time==s18a)&#xD;&#xA;        etb&lt;-which(time==s18b)&#xD;&#xA;        tea&lt;-which(time==s28a)&#xD;&#xA;        teb&lt;-which(time==s28b)&#xD;&#xA;        s18&lt;-sum(dta[eta:etb,lad])&#xD;&#xA;        s28&lt;-sum(dta[tea:teb,lad])&#xD;&#xA;        qc.mat[samples]&lt;-  (-1*exp((s28/s18)*-1)+1)*10&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Mostly based on that last line, the RIN seems to be associated with the ratio of the 18s peak to the 28s peak.&#xD;&#xA;" />
  <row Id="4407" PostHistoryTypeId="36" PostId="2078" RevisionGUID="05a47b7f-f8ff-4bf3-b820-2c89e020ad9c" CreationDate="2017-07-17T04:47:41.360" UserId="-1" Comment="from https://biology.stackexchange.com/questions/62739/how-to-use-arlequin-via-the-command-line-for-fdist2" />
  <row Id="4408" PostHistoryTypeId="2" PostId="2078" RevisionGUID="260b247c-5571-4fed-ab49-8b3d92c21d6e" CreationDate="2017-07-16T20:23:10.543" UserId="888" Text="I am trying to use FDist2 (method to detect Fst outlier) via [Arlequin][1] (short explanation of this method in the manual at page 161 under `Detection of loci under selection from F-statistics`).&#xD;&#xA;&#xD;&#xA;I've got descent knowledge in computer science (incl. bash scripting) but I fail to understand how Arlequin works. Could you please help me with a very simple reproducible example on how to use FDist2 via Arlequin via the command line?&#xD;&#xA;&#xD;&#xA;------&#xD;&#xA;&#xD;&#xA;As the .zip documents comes with its pre-compiled version, I'll just note that I'm on Mac OS X but of course, a reproducible example with Linux would likely be equally helpful.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cmpg.unibe.ch/software/arlequin35/Arl35Downloads.html" />
  <row Id="4409" PostHistoryTypeId="1" PostId="2078" RevisionGUID="260b247c-5571-4fed-ab49-8b3d92c21d6e" CreationDate="2017-07-16T20:23:10.543" UserId="888" Text="How to use Arlequin via the command line for FDist2?" />
  <row Id="4410" PostHistoryTypeId="3" PostId="2078" RevisionGUID="260b247c-5571-4fed-ab49-8b3d92c21d6e" CreationDate="2017-07-16T20:23:10.543" UserId="888" Text="&lt;statistics&gt;" />
  <row Id="4411" PostHistoryTypeId="6" PostId="2078" RevisionGUID="d83c1cf9-6414-437e-a1fd-aa5637fd0130" CreationDate="2017-07-17T04:48:52.533" UserId="888" Comment="edited tags" Text="&lt;genome&gt;&lt;statistics&gt;&lt;positive-selection&gt;" />
  <row Id="4412" PostHistoryTypeId="2" PostId="2079" RevisionGUID="d91a697f-0c67-44ca-852b-b9d62051a4d5" CreationDate="2017-07-17T06:35:19.213" UserId="206" Text="Use the following R package for Gene Set Enrichment analysis of RNA-seq data.&#xD;&#xA;&#xD;&#xA;https://www.bioconductor.org/packages/release/bioc/html/SeqGSEA.html &#xD;&#xA;&#xD;&#xA;There is another R package recently published called &quot;Fast Gene Set Enrichment Analysis&quot; by Alexey Sergushichev.&#xD;&#xA;&#xD;&#xA;https://bioconductor.org/packages/release/bioc/html/fgsea.html" />
  <row Id="4413" PostHistoryTypeId="5" PostId="2072" RevisionGUID="ae149cdb-bb40-47b7-b423-8f51ef444c74" CreationDate="2017-07-17T07:29:29.317" UserId="77" Comment="edited body" Text="The following bit of python code should work:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    import sys&#xD;&#xA;    &#xD;&#xA;    lastTranscript = [None, None, None, []]  # ID, chrom, strand, [(start, end, score), ...]&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    def getID(s):&#xD;&#xA;        &quot;&quot;&quot;Parse out the ID attribute&quot;&quot;&quot;&#xD;&#xA;        s = s.split(&quot;;&quot;)&#xD;&#xA;        for k in s:&#xD;&#xA;            if k.startswith(&quot;ID=&quot;):&#xD;&#xA;                return k[3:]&#xD;&#xA;        return None&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    def dumpLastTranscript():&#xD;&#xA;        &quot;&quot;&quot;Print the last transcript&quot;&quot;&quot;&#xD;&#xA;        bounds = sorted(lastTranscript[3])&#xD;&#xA;        print(&quot;{}\tgmapidx\tgene\t{}\t{}\t.\t{}\t.\tgene_id \&quot;{}\&quot;; transcript_id \&quot;{}\&quot;;&quot;.format(lastTranscript[1], bounds[0][0], bounds[-1][1], lastTranscript[2], lastTranscript[0], lastTranscript[0]))&#xD;&#xA;        print(&quot;{}\tgmapidx\ttranscript\t{}\t{}\t.\t{}\t.\tgene_id \&quot;{}\&quot;; transcript_id \&quot;{}\&quot;;&quot;.format(lastTranscript[1], bounds[0][0], bounds[-1][1], lastTranscript[2], lastTranscript[0], lastTranscript[0]))&#xD;&#xA;        for start, end, score in bounds:&#xD;&#xA;            print(&quot;{}\tgmapidx\texon\t{}\t{}\t{}\t{}\t.\tgene_id \&quot;{}\&quot;; transcript_id \&quot;{}\&quot;;&quot;.format(lastTranscript[1], start, end, score, lastTranscript[2], lastTranscript[0], lastTranscript[0]))&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    def handleLine(cols):&#xD;&#xA;        &quot;&quot;&quot;Handle a single line, appending the exon bounds to the previous if relevant&quot;&quot;&quot;&#xD;&#xA;        ID = getID(cols[8])&#xD;&#xA;        assert(ID is not None)&#xD;&#xA;        if lastTranscript[0] is not None and lastTranscript[0] != ID:&#xD;&#xA;            dumpLastTranscript()&#xD;&#xA;            lastTranscript[3] = []&#xD;&#xA;        lastTranscript[0] = ID&#xD;&#xA;        lastTranscript[1] = cols[0]&#xD;&#xA;        lastTranscript[2] = cols[6]&#xD;&#xA;        lastTranscript[3].append((int(cols[3]), int(cols[4]), cols[5]))&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    f = open(sys.argv[1])&#xD;&#xA;    for line in f:&#xD;&#xA;        if line.startswith(&quot;#&quot;):&#xD;&#xA;            continue&#xD;&#xA;        cols = line.strip().split(&quot;\t&quot;)&#xD;&#xA;        handleLine(cols)&#xD;&#xA;    &#xD;&#xA;    dumpLastTranscript()&#xD;&#xA;    f.close()&#xD;&#xA;&#xD;&#xA;If you saved that as `gff2gtf.py` and made it executable, the usage would be `./gff2gtf.py foo.gff3 &gt; foo.gtf`. With the example that you provided in your post, the result is:&#xD;&#xA;&#xD;&#xA;    2L	gmapidx	gene	18442664	18443024	.	-	.	gene_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;; transcript_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    2L	gmapidx	transcript	18442664	18443024	.	-	.	gene_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;; transcript_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    2L	gmapidx	exon	18442664	18443024	79	-	.	gene_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;; transcript_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    3R	gmapidx	gene	15853880	15855742	.	+	.	gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    3R	gmapidx	transcript	15853880	15855742	.	+	.	gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    3R	gmapidx	exon	15853880	15855465	96	+	.	gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    3R	gmapidx	exon	15855529	15855742	97	+	.	gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    X	gmapidx	gene	14837470	14838142	.	-	.	gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    X	gmapidx	transcript	14837470	14838142	.	-	.	gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    X	gmapidx	exon	14837470	14837753	92	-	.	gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xD;&#xA;    X	gmapidx	exon	14837810	14838142	93	-	.	gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xD;&#xA;&#xD;&#xA;That at least looks like correct gtf 2.2 format to me." />
  <row Id="4414" PostHistoryTypeId="33" PostId="2067" RevisionGUID="6fbc3083-1c0c-4a93-93e8-310985afef6c" CreationDate="2017-07-17T07:40:52.730" UserId="982" Comment="3" />
  <row Id="4415" PostHistoryTypeId="5" PostId="2078" RevisionGUID="f30f8784-6c81-4e16-a134-3af1ac9480da" CreationDate="2017-07-17T09:35:10.773" UserId="298" Comment="deleted 9 characters in body; edited title" Text="I am trying to use FDist2 (method to detect Fst outlier) via [Arlequin][1] (short explanation of this method in the manual at page 161 under `Detection of loci under selection from F-statistics`).&#xD;&#xA;&#xD;&#xA;I've got a decent knowledge of programming (incl. bash scripting) but I fail to understand how Arlequin works. Could you please help me with a very simple reproducible example on how to use FDist2 via Arlequin via the command line?&#xD;&#xA;&#xD;&#xA;------&#xD;&#xA;&#xD;&#xA;As the .zip file comes with its pre-compiled version, I'll just note that I'm on Mac OS X but of course, a reproducible example with Linux would likely be equally helpful.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cmpg.unibe.ch/software/arlequin35/Arl35Downloads.html" />
  <row Id="4416" PostHistoryTypeId="4" PostId="2078" RevisionGUID="f30f8784-6c81-4e16-a134-3af1ac9480da" CreationDate="2017-07-17T09:35:10.773" UserId="298" Comment="deleted 9 characters in body; edited title" Text="How can I use Arlequin via the command line for FDist2?" />
  <row Id="4417" PostHistoryTypeId="2" PostId="2080" RevisionGUID="bc5a0c22-6bc5-4207-8498-9e2a87a471df" CreationDate="2017-07-17T11:21:39.053" UserId="446" Text="Roary also takes into account paralogs, so sometimes two core genes are split into different groups based on their neighbour genes and they end up with different nomenclature (group_*...). As suggested by Andrew Page in the github issue I would consider the gene_presence_absence.Rtab (this contains all the orthologous genes) and remove rows corresponding to vectors only containing 1s (core genes). In this way you will have a matrix of 1 and 0 corresponding to presence/absence of a particular accessory gene in your isolates. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4418" PostHistoryTypeId="2" PostId="2081" RevisionGUID="a89b66af-a65f-44bb-ac21-cff6f42f33d4" CreationDate="2017-07-17T12:22:15.753" UserId="180" Text="When doing Illumina 2x150bp sequencing of genomic DNA, and after aligning the reads to GRCh38, what percentage of the non-N fraction of the human genome is MAPQ=0? This is, what part corresponds to regions that can't be uniquely mapped with 2x150bp reads.&#xD;&#xA;&#xD;&#xA;And, how many genes are affected by MAPQ=0 regions?&#xD;&#xA;&#xD;&#xA;I presume the numbers will dance around depending on fragment sizes, read quality, etc. but I am happy with some starting numbers." />
  <row Id="4419" PostHistoryTypeId="1" PostId="2081" RevisionGUID="a89b66af-a65f-44bb-ac21-cff6f42f33d4" CreationDate="2017-07-17T12:22:15.753" UserId="180" Text="what percentage of the human genome is MAPQ=0?" />
  <row Id="4420" PostHistoryTypeId="3" PostId="2081" RevisionGUID="a89b66af-a65f-44bb-ac21-cff6f42f33d4" CreationDate="2017-07-17T12:22:15.753" UserId="180" Text="&lt;genome&gt;&lt;human&gt;&lt;illumina&gt;&lt;2x150bp&gt;&lt;mapq&gt;" />
  <row Id="4421" PostHistoryTypeId="5" PostId="2073" RevisionGUID="089e0d39-dcc7-47a4-934d-f768476d8866" CreationDate="2017-07-17T12:25:48.393" UserId="29" Comment="formatting; remove “thanks” etc., see https://bioinformatics.stackexchange.com/help/behavior" Text="I'm currently learning about GSEA in the hopes of using it in my analysis of differentially expressed genes , and I just had a few questions about the program, specifically about GSEAPreranked, which I need cleared up.&#xD;&#xA;&#xD;&#xA;1. On the ranked list needed for GSEA input, should the list include all genes, or only those that pass a certain threshold of significance (i.e. fold change higher than 2, p value less than 0.05, etc.)? Ideally I'd like to sort the genes by fold change alone as I don't trust my p values as much, so should I only include genes with high fold changes?&#xD;&#xA;&#xD;&#xA;2. I am comparing multiple conditions of disease with different treatments. Am I correct that GSEA only compares two conditions? If this is the case should I run GSEA for each control/treatment comparison? Would this be conventional?" />
  <row Id="4422" PostHistoryTypeId="2" PostId="2082" RevisionGUID="c243172d-4a8b-400c-b1f1-21f6d1541025" CreationDate="2017-07-17T14:19:35.680" UserId="1126" Text="LS-BSR should be able to give you what you're looking for:&#xD;&#xA;&#xD;&#xA;https://peerj.com/articles/332/&#xD;&#xA;&#xD;&#xA;https://github.com/jasonsahl/LS-BSR&#xD;&#xA;&#xD;&#xA;After you run the primary analysis, there is a simple documented workflow for splitting the pangenome into core and accessory, based on a user defined threshold. I'm the developer, so can help if you run into any problems.&#xD;&#xA;" />
  <row Id="4423" PostHistoryTypeId="5" PostId="2073" RevisionGUID="c8667e92-8dfd-4d09-b7f8-0d2d1a4bc4b5" CreationDate="2017-07-17T15:17:56.247" UserId="57" Comment="expanded abbreveation; added link" Text="I'm currently learning about Gene Set Enrichment Analysis ([GSEA][1]) in the hopes of using it in my analysis of differentially expressed genes, and I just had a few questions about the program, specifically about GSEAPreranked, which I need cleared up.&#xD;&#xA;&#xD;&#xA;1. On the ranked list needed for GSEA input, should the list include all genes, or only those that pass a certain threshold of significance (i.e. fold change higher than 2, p value less than 0.05, etc.)? Ideally I'd like to sort the genes by fold change alone as I don't trust my p values as much, so should I only include genes with high fold changes?&#xD;&#xA;&#xD;&#xA;2. I am comparing multiple conditions of disease with different treatments. Am I correct that GSEA only compares two conditions? If this is the case should I run GSEA for each control/treatment comparison? Would this be conventional?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://software.broadinstitute.org/gsea/index.jsp" />
  <row Id="4424" PostHistoryTypeId="2" PostId="2083" RevisionGUID="0f1d5371-ac4b-4015-9f31-c42caa72e2f0" CreationDate="2017-07-17T15:57:38.820" UserId="298" Text="I am trying to translate (lift over) bed files describing genomic regions from hg37 to hg38. I have tried both UCSC's [LiftOver][1] tool and [CrossMap][2] but saw that they give me different results. I therefore need a way of assessing how correct the results are. &#xD;&#xA;&#xD;&#xA;I decided to test by downloading a bed file of the RefSeq genes for hg37 and one for hg38 from UCSC. Then, I run `liftOver` to translate the hg37 file to hg39 coordinates and now I want to compare the results of the liftover to the bed file I downloaded for hg38.&#xD;&#xA;&#xD;&#xA;How can I usefully compare these files? I am hoping for a tool that can report how similar the two are. Ideally, I would like to see three things in the output:&#xD;&#xA;&#xD;&#xA;1. Matching regions: those that are shown with identical positions in the liftover results and the UCSC bed file. &#xD;&#xA;2. Non-matching regions: those whose coordinates do not match between the two files. &#xD;&#xA;3. (if possible) fuzzy matches: those that are not identical but pretty close (off by a few nucleotides).&#xD;&#xA;&#xD;&#xA;I thought it would be easy to do with a little awk script but it gets quite complicated because certain RefSeq genes are mapped to multiple positions (e.g. [MIR4454][3]). And the 3rd requirement (fuzzy) is not trivial to do in awk. I can live without that though, if a tool can give me the other two. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://genome.ucsc.edu/cgi-bin/hgLiftOver&#xD;&#xA;  [2]: http://crossmap.sourceforge.net/&#xD;&#xA;  [3]: http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=41553" />
  <row Id="4425" PostHistoryTypeId="1" PostId="2083" RevisionGUID="0f1d5371-ac4b-4015-9f31-c42caa72e2f0" CreationDate="2017-07-17T15:57:38.820" UserId="298" Text="How can I compare two bed files?" />
  <row Id="4426" PostHistoryTypeId="3" PostId="2083" RevisionGUID="0f1d5371-ac4b-4015-9f31-c42caa72e2f0" CreationDate="2017-07-17T15:57:38.820" UserId="298" Text="&lt;bed&gt;&lt;liftover&gt;" />
  <row Id="4427" PostHistoryTypeId="2" PostId="2084" RevisionGUID="e0bec872-9317-46eb-8881-1fe0775593e8" CreationDate="2017-07-17T18:09:10.553" UserId="96" Text="I wrote [ParsEval](http://aegean.readthedocs.io/en/stable/parseval.html) to handle these types of comparisons. ParsEval reports a variety of similarity statistics at both the nucleotide level and feature (exon) level. It doesn't explicitly support &quot;fuzzy&quot; matches, but it does report scaled values between 0.0 and 1.0 for similarity, so you could get a similar feel by definig a similarity cutoff for filtering.&#xD;&#xA;&#xD;&#xA;Caveat: ParsEval doesn't handle BED files, only GFF3." />
  <row Id="4428" PostHistoryTypeId="5" PostId="2048" RevisionGUID="cc32cdbf-702b-4c82-b78b-52cedbf6c500" CreationDate="2017-07-17T18:58:28.627" UserId="1025" Comment="added 98 characters in body" Text="UPDATE: I found the solution. I was using normalized values and GEO was using raw beta values.&#xD;&#xA;&#xD;&#xA;I'm trying to link GEOquery and minfi. &#xD;&#xA;&#xD;&#xA;Specifically I want to obtain beta values from the idat files on GEOquery. I was following this guide: https://kasperdanielhansen.github.io/genbioconductor/html/minfi.html, up until the preprocessing part. Meaning that I was able to obtain the RGset. Then I used my own preprocessing code to obtain the beta values. However, I cross checked them with the beta values that were on GEO and they weren't consistent.&#xD;&#xA;&#xD;&#xA;For example, the accession number I used was GSE68777. So I went to that study on GEO and clicked on the first sample: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM1681154. Then I scrolled down and clicked &quot;Download full table&quot; to download the samples and beta values in a text file. &#xD;&#xA;&#xD;&#xA;Then I typed head(beta) and chose the first sample. Then I did command F in the text file for that sample and it's value there wasn't the same as the value from the beta data table. Hopefully you can help find the error.&#xD;&#xA;&#xD;&#xA;Here is the code I'm using:&#xD;&#xA;&#xD;&#xA;    library(GEOquery)&#xD;&#xA;    library(minfi)&#xD;&#xA;    library(&quot;IlluminaHumanMethylation450kanno.ilmn12.hg19&quot;)&#xD;&#xA;    library(&quot;IlluminaHumanMethylation450kmanifest&quot;)&#xD;&#xA;    &#xD;&#xA;    ######## Code copied from 450k Guide ########&#xD;&#xA;&#xD;&#xA;    getGEOSuppFiles(&quot;GSE68777&quot;)&#xD;&#xA;    untar(&quot;GSE68777/GSE68777_RAW.tar&quot;, exdir = &quot;GSE68777/idat&quot;)&#xD;&#xA;    head(list.files(&quot;GSE68777/idat&quot;, pattern = &quot;idat&quot;))&#xD;&#xA;    idatFiles &lt;- list.files(&quot;GSE68777/idat&quot;, pattern = &quot;idat.gz$&quot;, full = TRUE)&#xD;&#xA;    sapply(idatFiles, gunzip, overwrite = TRUE)&#xD;&#xA;    rgSet &lt;- read.metharray.exp(&quot;GSE68777/idat&quot;)&#xD;&#xA;    geoMat &lt;- getGEO(&quot;GSE68777&quot;)&#xD;&#xA;    pD.all &lt;- pData(geoMat[[1]])&#xD;&#xA;    pD &lt;- pD.all[, c(&quot;title&quot;, &quot;geo_accession&quot;, &quot;characteristics_ch1.1&quot;, &quot;characteristics_ch1.2&quot;)]&#xD;&#xA;    names(pD)[c(3,4)] &lt;- c(&quot;group&quot;, &quot;sex&quot;)&#xD;&#xA;    pD$group &lt;- sub(&quot;^diagnosis: &quot;, &quot;&quot;, pD$group)&#xD;&#xA;    pD$sex &lt;- sub(&quot;^Sex: &quot;, &quot;&quot;, pD$sex)&#xD;&#xA;    sampleNames(rgSet) &lt;- sub(&quot;.*_5&quot;, &quot;5&quot;, sampleNames(rgSet))&#xD;&#xA;    rownames(pD) &lt;- pD$title&#xD;&#xA;    pD &lt;- pD[sampleNames(rgSet),]&#xD;&#xA;    pData(rgSet) &lt;- pD&#xD;&#xA;    " />
  <row Id="4429" PostHistoryTypeId="2" PostId="2085" RevisionGUID="bc51c3b2-3b37-4d9c-9c13-aadf9f846ae9" CreationDate="2017-07-17T19:06:47.940" UserId="776" Text="From a set operations viewpoint, consider your hg37 regions as a &quot;reference&quot; set and hg38 as a &quot;map&quot; set. &#xD;&#xA;&#xD;&#xA;If you have these sets in BED format, [BEDOPS][1] lets you do set operations on them with various overlap criteria.&#xD;&#xA;&#xD;&#xA;Specifically, you can use [BEDOPS `bedmap`][1] to map hg38 regions to hg37 regions. The `bedmap` tool lets you assign overlap criteria from as little as one base to as much as full, mutual overlap. &#xD;&#xA;&#xD;&#xA;Once you do mapping, you can do operations, like counting the number of regions that meet the overlap criteria between reference and map sets, and calculate percentages.&#xD;&#xA;&#xD;&#xA;For example, use `--exact` to get exactly-matching regions between hg37 and hg38:&#xD;&#xA;&#xD;&#xA;    $ bedmap --count --exact hg37.bed hg38.bed | awk '{s+=$1}END{print s;}' &gt; exact_counts.txt&#xD;&#xA;&#xD;&#xA;You can do fractional (&quot;fuzzy&quot;) overlaps, where (for example) at least 50% of an hg38 region has to overlap an hg37 region:&#xD;&#xA;&#xD;&#xA;    $ bedmap --count --fraction-map 0.5 hg37.bed hg38.bed | awk '{s+=$1}END{print s;}' &gt; fuzzy_0p5_counts.txt&#xD;&#xA;&#xD;&#xA;To get a count of no overlaps, build the inverse — or mask — of your hg38 regions, using Kent tools `fetchChromSizes` and BEDOPS `bedops --difference`:&#xD;&#xA;&#xD;&#xA;    $ fetchChromSizes hg38 | awk '{print $1&quot;\t0\t&quot;$2}' | sort-bed - &gt; hg38.bounds.bed&#xD;&#xA;    $ bedops --difference hg38.bounds.bed hg38.bed &gt; hg38.masked.bed&#xD;&#xA;&#xD;&#xA;The regions in `hg38.masked.bed` make up all of the genomic space in hg38, except for your lift-over regions.&#xD;&#xA;&#xD;&#xA;Apply `--count --fraction-ref 1.0` to count the number of reference (hg37) elements which completely overlap these masked hg38 regions:&#xD;&#xA;&#xD;&#xA;    $ bedmap --count --fraction-ref 1.0 hg37.bed hg38.masked.bed | awk '{s+=$1}END{print s;}' &gt; no_counts.txt&#xD;&#xA;&#xD;&#xA;The [documentation for `bedmap`][2] describes these overlap criteria and operations in more detail.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/index.html&#xD;&#xA;  [2]: http://bedops.readthedocs.io/en/latest/content/reference/statistics/bedmap.html#overlap-criteria" />
  <row Id="4430" PostHistoryTypeId="5" PostId="2085" RevisionGUID="139e1b60-b926-4c6e-826d-7f4c3a731781" CreationDate="2017-07-17T19:37:29.100" UserId="776" Comment="added 53 characters in body" Text="From a set operations viewpoint, consider your hg37 regions as a &quot;reference&quot; set and hg38 as a &quot;map&quot; set. &#xD;&#xA;&#xD;&#xA;If you have these sets in BED format, [BEDOPS][1] lets you do set operations on them with various overlap criteria.&#xD;&#xA;&#xD;&#xA;Specifically, you can use [BEDOPS `bedmap`][1] to map hg38 regions to hg37 regions. The `bedmap` tool lets you assign overlap criteria from as little as one base to as much as full, mutual overlap. &#xD;&#xA;&#xD;&#xA;Once you do mapping, you can do operations, like counting the number of regions that meet the overlap criteria between reference and map sets, and calculate percentages.&#xD;&#xA;&#xD;&#xA;For example, use `--exact` to get exactly-matching regions between hg37 and hg38:&#xD;&#xA;&#xD;&#xA;    $ bedmap --count --exact hg37.bed hg38.bed | awk '{s+=$1}END{print s;}' &gt; exact_counts.txt&#xD;&#xA;&#xD;&#xA;You can do fractional (&quot;fuzzy&quot;) overlaps, where (for example) at least 50% of an hg38 region has to overlap an hg37 region:&#xD;&#xA;&#xD;&#xA;    $ bedmap --count --fraction-map 0.5 hg37.bed hg38.bed | awk '{s+=$1}END{print s;}' &gt; fuzzy_0p5_counts.txt&#xD;&#xA;&#xD;&#xA;Getting a percentage out of a &quot;fuzzy&quot; result can be complicated by overlaps between hg38 regions that overlap a reference hg37 region. A more complicated procedure can be implemented with the `--echo-map` option and some command-line work to implement logic to deal with that case more stringently.&#xD;&#xA;&#xD;&#xA;To get a straight-up count of no overlaps, build the inverse — or mask — of your hg38 regions, using [Kent tools][2] `fetchChromSizes` and BEDOPS `bedops --difference`:&#xD;&#xA;&#xD;&#xA;    $ fetchChromSizes hg38 | awk '{print $1&quot;\t0\t&quot;$2}' | sort-bed - &gt; hg38.bounds.bed&#xD;&#xA;    $ bedops --difference hg38.bounds.bed hg38.bed &gt; hg38.masked.bed&#xD;&#xA;&#xD;&#xA;The regions in `hg38.masked.bed` make up all of the genomic space in hg38, except for your lift-over regions.&#xD;&#xA;&#xD;&#xA;Apply `--count --fraction-ref 1.0` to count the number of reference (hg37) elements which completely overlap these masked hg38 regions:&#xD;&#xA;&#xD;&#xA;    $ bedmap --count --fraction-ref 1.0 hg37.bed hg38.masked.bed | awk '{s+=$1}END{print s;}' &gt; no_counts.txt&#xD;&#xA;&#xD;&#xA;The [documentation for `bedmap`][3] describes these overlap criteria and operations in more detail.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/index.html&#xD;&#xA;  [2]: https://github.com/ENCODE-DCC/kentUtils&#xD;&#xA;  [3]: http://bedops.readthedocs.io/en/latest/content/reference/statistics/bedmap.html#overlap-criteria" />
  <row Id="4431" PostHistoryTypeId="2" PostId="2086" RevisionGUID="393efb86-2845-48c4-a7df-6076b5f61ce6" CreationDate="2017-07-17T23:46:37.833" UserId="1131" Text="Pathway-tools has a unique identifier (called a Frame-ID) for each metabolite in its database.  You can get the list of all compounds with this api call:&#xD;&#xA;&#xD;&#xA;`all_cpds = meta.get_class_all_instances('|Compounds|')`&#xD;&#xA;&#xD;&#xA;for each compound, you can also get its common name, or all its synonyms:&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;cpd_names = {}&#xD;&#xA;for cpd_frame_id in all_cpds:&#xD;&#xA;      cpd_names[cpd_frame_id] = meta.get_name_string(cpd_frame_id)&#xD;&#xA;```&#xD;&#xA;" />
  <row Id="4432" PostHistoryTypeId="2" PostId="2087" RevisionGUID="1261d204-0ee7-47df-a3cb-6c1881e46009" CreationDate="2017-07-18T13:01:54.007" UserId="1049" Text="An alternative way is using the Hmmer website, and do a hmmsearch with the Pfam accession, forex. (PF02910). Navigate to the domain tab and find the domain architecture that you are interested in and press view scores. That way, only the sequences you are interested in will be downloadable under the tab downloads. &#xD;&#xA;&#xD;&#xA;Step-by-Step guide:&#xD;&#xA;&#xD;&#xA;1- Go to http://www.ebi.ac.uk/Tools/hmmer/&#xD;&#xA;&#xD;&#xA;2- Search with hmmsearch http://www.ebi.ac.uk/Tools/hmmer/search/hmmsearch&#xD;&#xA;&#xD;&#xA;3- Use Accession search and type in PF02910&#xD;&#xA;&#xD;&#xA;4- Navigate to Domain&#xD;&#xA;&#xD;&#xA;5- To the right side of &#xD;&#xA;9742 sequences with domain architecture: FAD_binding_2, Succ_DH_flav_C &#xD;&#xA;&#xD;&#xA;Press View scores.&#xD;&#xA;&#xD;&#xA;6- It should show (Your results have been filtered) and Query matches (9742)&#xD;&#xA;&#xD;&#xA;7- Navigate to Download and download the sequences in the format of interest. &#xD;&#xA;&#xD;&#xA;8- Repeat for domain architectures of interest. &#xD;&#xA;&#xD;&#xA;I hope you find this helpful.&#xD;&#xA;&#xD;&#xA;Kind Regards,&#xD;&#xA;&#xD;&#xA;Sara&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4433" PostHistoryTypeId="2" PostId="2088" RevisionGUID="64b83a3e-42d1-4f9b-a6e1-a11603e8ccad" CreationDate="2017-07-18T13:26:44.693" UserId="292" Text="Suppose I have single-end RNA-seq data for which the reads in the fastq file are reversed with respect to the original extracted RNAs.&#xD;&#xA;&#xD;&#xA;Suppose I have the following workflow:&#xD;&#xA;&#xD;&#xA;1. I map the reads on the reference genome, sort and index the resulting sam output.&#xD;&#xA;2. I use the bam file to make a bigwig file showing reads mapping on the plus and minus strands of the chromosomes.&#xD;&#xA;3. I use the bam file and an annotation file to quantify the gene expression.&#xD;&#xA;&#xD;&#xA;Step 2. and 3. depend on step 1., but are not dependant on one another.&#xD;&#xA;&#xD;&#xA;I'm sure that strandedness needs to be taken into account no later than at step 3.&#xD;&#xA;&#xD;&#xA;The questions are the following:&#xD;&#xA;&#xD;&#xA;**Is it common to have mapping tools with an option to &quot;correct&quot; for strandedness ?**&#xD;&#xA;&#xD;&#xA;Assuming the bam file has been built ignoring library strandedness information, **is it good practice to correct for strandedness when building bigwig files?**&#xD;&#xA;&#xD;&#xA;Regarding this second point, I'm split between two attitudes:&#xD;&#xA;&#xD;&#xA;1. There should be no correction, the bigwig files are there to represent the &quot;raw&quot; mapping results.&#xD;&#xA;2. The bigwig files are there to give an idea of the origin and abundance of the originally extracted RNAs, so a strandedness correction should be used if necessary.&#xD;&#xA;&#xD;&#xA;Which one is the most common attitude?&#xD;&#xA;&#xD;&#xA;If the second one is preferred, then wouldn't it be &quot;safer&quot; / &quot;cleaner&quot; to handle strandedness as early as possible? Possibly at the mapping, or even using a pre-processing step?" />
  <row Id="4434" PostHistoryTypeId="1" PostId="2088" RevisionGUID="64b83a3e-42d1-4f9b-a6e1-a11603e8ccad" CreationDate="2017-07-18T13:26:44.693" UserId="292" Text="At what processing step should library strandedness type be taken into account?" />
  <row Id="4435" PostHistoryTypeId="3" PostId="2088" RevisionGUID="64b83a3e-42d1-4f9b-a6e1-a11603e8ccad" CreationDate="2017-07-18T13:26:44.693" UserId="292" Text="&lt;read-mapping&gt;&lt;strandedness&gt;" />
  <row Id="4436" PostHistoryTypeId="2" PostId="2089" RevisionGUID="a0922889-d6a6-4ac5-a8db-dfb1e5aea2b1" CreationDate="2017-07-18T15:07:12.493" UserId="294" Text="I usually take into account library strandness during the mapping to the reference step (but I never worked with bigwig files myself). &#xD;&#xA;&#xD;&#xA;Assuming RNA-seq data from Illumina, you can use [Hisat2][1] for alignment to the reference genome, and you have the option `--rna-strandness` to specify the strand-specificity information (default unstranded). This is particularly important when you wish to have XS strand attribute in your bam file for easier compatibility with downstream software (especially important in my experience if you are planning to use some utility from [Cufflinks][2] for your step 3).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://ccb.jhu.edu/software/hisat2/manual.shtml&#xD;&#xA;  [2]: http://cole-trapnell-lab.github.io/cufflinks/" />
  <row Id="4439" PostHistoryTypeId="2" PostId="2090" RevisionGUID="67dc0add-d440-4a7d-93f3-50afef6d8ea0" CreationDate="2017-07-18T15:34:49.860" UserId="373" Text="It seems that in snakemake the script specified after the `--jobscript` cannot be used properly if a `workdir:` is specified in the snakefile. &#xD;&#xA;The path of the script specified becomes relative to the workdir defined in the snakefile instead of being relative to the current working directory. I do not know if this is a feature or a bug !!&#xD;&#xA;&#xD;&#xA;Nevertheless, it becomes quite painful to workaround this problem. The only way I found was to parse the command line in the snakefile, for example:&#xD;&#xA;```&#xD;&#xA;snakemake -c &quot;qsub&quot; -j 30 --js ./sge.sh --latency-wait 30 -rp&#xD;&#xA;```&#xD;&#xA;in order to copy `./sge.sh` in the output directory defined by the keyword `workdir` in the snakefile.&#xD;&#xA;&#xD;&#xA;As a side effect, if you specify option for qsub on the command line, for example:&#xD;&#xA;```&#xD;&#xA;snakemake -c &quot;qsub -e ./logs/ -o ./logs/&quot; -j 30 --js ./sge.sh --latency-wait 30 -rp&#xD;&#xA;```&#xD;&#xA;the `logs` folder must be created under the workdir directory.&#xD;&#xA;&#xD;&#xA;Is there something I don't understand with the `--jobscript` option? Am I not using it the right way? Am I going against the best practice?" />
  <row Id="4440" PostHistoryTypeId="1" PostId="2090" RevisionGUID="67dc0add-d440-4a7d-93f3-50afef6d8ea0" CreationDate="2017-07-18T15:34:49.860" UserId="373" Text="combining the use of workdir and option --jobscript in snakemake" />
  <row Id="4441" PostHistoryTypeId="3" PostId="2090" RevisionGUID="67dc0add-d440-4a7d-93f3-50afef6d8ea0" CreationDate="2017-07-18T15:34:49.860" UserId="373" Text="&lt;snakemake&gt;" />
  <row Id="4442" PostHistoryTypeId="2" PostId="2091" RevisionGUID="7a3f4cad-b4ad-4419-b1f4-4a4bc0d191b7" CreationDate="2017-07-18T16:14:56.310" UserId="1092" Text="Firstly, right at the start if the experiment it's important that your RNA samples are processed with a strand-specific protocol (e.g. Illumina's TruSeq Stranded) in order to produce stranded libraries for sequencing. If the samples haven't been treated as such, then there's nothing you can do to correct it.&#xD;&#xA;&#xD;&#xA;Secondly, I'm not sure stranded protocols are compatible with single-end reads. The aligners I'm aware of use the direction of aligned read pairs to assess the strand from which the RNA fragment has come from. How that works with single-end reads, I don't know?&#xD;&#xA;&#xD;&#xA;Finally, to answer your specific question, essentially step 1 is where you need to start with strand-specific options and do so at all subsequent steps as appropriate." />
  <row Id="4443" PostHistoryTypeId="5" PostId="2068" RevisionGUID="c766b889-3d9e-46c4-ae98-c0f47a6bf125" CreationDate="2017-07-18T16:41:08.203" UserId="294" Comment="added 124 characters in body" Text="I would like to convert a file in `gff3` format to a `gtf2.2` format.&#xD;&#xA;&#xD;&#xA;The reason why I would like to do this is: I have a set of transcripts assembled by a bunch of different software (and using RNA-seq data from different sequencing technologies) and I would like to compare each of those sets with the reference transcriptome annotation for that species (*D. melanogaster*). I [already asked for the community advice][1] about how to proceed on that, but to run the suggested software ([TACO][2] and [cuffmerge][3]) I need to have a GTF file containing the experimental transcripts to compare to the reference transcripts (GTF).&#xD;&#xA;&#xD;&#xA;Up to now, I **unsuccessfully** tried: &#xD;&#xA;&#xD;&#xA; 1. `gffread` utility in the [Cufflinks][4] package (`gffread input.gff3 -T -o output.gtf`): this results in an empty `output.gtf` file and an empty `log` file (used `Cufflinks` v.2.2.1) - I contacted the authors via their [Google group][5] but haven't heard of them yet &#xD;&#xA; 2. `gff3_to_gtf` utility in the [GenomeTools][6] (`gt`) package (`gt gff3_to_gtf input.gff3 -o output.gtf`): the output is not created and the log file is not informative - I contacted the authors via their [mailing list][7] but haven't heard of them yet &#xD;&#xA; 3. `GFF3_to_GTF` utility in the [FML][8] package (`./gff3_to_gtf_converter.pl input.gff3 output.gtf`): the output just contains a header (`##gff-version 2.5`) and the `log` is empty&#xD;&#xA;&#xD;&#xA;The `gff3` file was created as output of [GMAP][9], and contains the transcripts as found by alignment to the reference (specifying option `-f gff3_match_cdna`). &#xD;&#xA;&#xD;&#xA;[Edit: what I found out a poteriori, is that this format is not a standard gff3, thus the conversion is not trivial...]&#xD;&#xA;&#xD;&#xA;Here is the beginning of the gff3 file I tried to convert: &#xD;&#xA;&#xD;&#xA;    $ head r9_gmap_match-cdna.gff&#xD;&#xA;    ##gff-version   3&#xD;&#xA;    # Generated by GMAP version 2017-04-24 using call:  /home/aechchik/software/gmap-2017-04-24/bin/gmap.sse42 -d gmapidx -D /scratch/beegfs/monthly/aechchik/isoforms/ref/chromosomes/ -f gff3_match_cdna -n 0 -t 20 r9_2d.fasta&#xD;&#xA;    2L    gmapidx    cDNA_match    18442664    18443024    79    -    . ID=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1;Name=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d;Target=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d 141 489;Gap=M13 I10 M8 D3 M8 D3 M32 D1 M4 D2 M5 D3 M34 I3 M6 D1 M7 D1 M5 D1 M30 D2 M16 I1 M8 D3 M10 D1 M5 D1 M6 I1 M8 I1 M8 D1 M33 D3 M33 I1 M28 D3 M25&#xD;&#xA;    ###&#xD;&#xA;    3R    gmapidx    cDNA_match    15853880    15855465    96    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 80 1645;Gap=M46 D2 M12 D2 M157 D3 M4 D1 M50 I2 M3 I1 M12 I1 M68 I1 M66 I1 M53 D2 M47 D1 M16 D1 M35 D1 M155 D1 M28 D1 M166 D2 M47 D1 M8 D4 M69 D1 M28 D1 M5 D1 M12 D1 M202 I1 M115 D1 M61 I2 M7 D1 M7 I1 M36 D2 M41&#xD;&#xA;    3R    gmapidx    cDNA_match    15855529    15855742    97    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 1646 1856;Gap=M21 D1 M68 D1 M85 D1 M37&#xD;&#xA;    ###&#xD;&#xA;    X    gmapidx    cDNA_match    14837810    14838142    93    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 74 406;Gap=M13 D1 M182 I1 M14 I2 M56 I1 M30 D2 M21 D1 M13&#xD;&#xA;    X    gmapidx    cDNA_match    14837470    14837753    92    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 407 688;Gap=I2 M5 I1 M64 I1 M44 D1 M9 D5 M31 D3 M23 I1 M19 I1 M25 I1 M26 D1 M19 I1 M9&#xD;&#xA;    ###&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/2036/tools-to-reconcile-experimental-transcripts-with-reference-annotation&#xD;&#xA;  [2]: https://tacorna.github.io/&#xD;&#xA;  [3]: http://cole-trapnell-lab.github.io/cufflinks/cuffmerge/&#xD;&#xA;  [4]: http://cole-trapnell-lab.github.io/cufflinks/&#xD;&#xA;  [5]: https://groups.google.com/d/msg/tuxedo-tools-users/T-qE4P67-HU/O_up6Yh6BgAJ&#xD;&#xA;  [6]: http://genometools.org/&#xD;&#xA;  [7]: http://gt-users@genometools.org&#xD;&#xA;  [8]: https://testtoolshed.g2.bx.psu.edu/repository/display_tool?repository_id=afcb6456d8e300ed&amp;tool_config=database%2Fcommunity_files%2F000%2Frepo_21%2Ffml_gff_converter_programs%2Fgalaxy%2Fgff3_to_gtf.xml&amp;changeset_revision=4c459744cab1&#xD;&#xA;  [9]: https://wiki.gacrc.uga.edu/wiki/Gmap" />
  <row Id="4444" PostHistoryTypeId="2" PostId="2092" RevisionGUID="69fabe1a-2cb7-489f-9e5e-60662c83aa88" CreationDate="2017-07-18T17:21:35.507" UserId="727" Text="I have found a blog post with a script that I would like to use for my current research project: [link](https://bcbio.wordpress.com/2009/02/07/automated-protein-conservation-display-from-blast-alignments/)&#xD;&#xA;&#xD;&#xA;The script is incredibly fast and produces a smooth conservation plot. In the blog post, the author mentions that it would be totally possible to change the remote blast function to a local database. Instead of making a local database, I have opted to try to change the function to still use a remote blast, but with different parameters.&#xD;&#xA;&#xD;&#xA;I found the following resource for learning about the NCBIWWW module: [link](http://biopython.org/DIST/docs/api/Bio.Blast.NCBIWWW-module.html)&#xD;&#xA;&#xD;&#xA;Using this, I rewrote the line in the script:&#xD;&#xA;&#xD;&#xA;    blast_handle = NCBIWWW.qblast(blast_method, &quot;nr&quot;, search_gi)&#xD;&#xA;&#xD;&#xA;to:&#xD;&#xA;&#xD;&#xA;    blast_handle = NCBIWWW.qblast(program=&quot;blastp&quot;, &#xD;&#xA;            							  database=&quot;refseq_protein&quot;, &#xD;&#xA;            							  sequence=search_gi, &#xD;&#xA;            							  entrez_query=&quot; txid2 [ORGN] OR txid4751 [ORGN] &quot;, &#xD;&#xA;            							  expect=20000.0, &#xD;&#xA;            							  alignments=10000, &#xD;&#xA;            							  descriptions=10000)&#xD;&#xA;&#xD;&#xA;Strangely, I get the exact same graph as an output even with the different parameters. Am I doing something about this search incorrectly?&#xD;&#xA;&#xD;&#xA;Thanks." />
  <row Id="4445" PostHistoryTypeId="1" PostId="2092" RevisionGUID="69fabe1a-2cb7-489f-9e5e-60662c83aa88" CreationDate="2017-07-18T17:21:35.507" UserId="727" Text="changing blast parameters using NCBIWWW module" />
  <row Id="4446" PostHistoryTypeId="3" PostId="2092" RevisionGUID="69fabe1a-2cb7-489f-9e5e-60662c83aa88" CreationDate="2017-07-18T17:21:35.507" UserId="727" Text="&lt;biopython&gt;&lt;blast&gt;&lt;python&gt;" />
  <row Id="4447" PostHistoryTypeId="2" PostId="2093" RevisionGUID="a49ff130-6a73-4c19-be69-8712dfdf9224" CreationDate="2017-07-18T17:59:44.230" UserId="77" Text="1. If your aligner has a strandedness option then go ahead and use it. The general idea here being that you'll preferentially align correctly to genes.&#xD;&#xA;2. Whether to bother with strandedness here depends on your goal. Are you interested in anti-sense transcription? Do you need to use the bigWig track to accurately quantify translational pausing or some other strand-specific phenomenon? If yes, then go ahead and make different bigWig files for each strand. If you just need the bigWig file for convenience in IGV or creating heatmaps (e.g., with deepTools) then you'll generally be fine without accounting for strand. For well-annotated species, the rate of anti-sense transcription is generally low and the rate of genes overlapping isn't absurdly high. Consequently, you can usually get away with using an unstranded bigWig.&#xD;&#xA;3. If you're using featureCounts you'll certainly want to use the appropriate strandedness setting.&#xD;&#xA;&#xD;&#xA;It's relatively common for RNAseq aligners/mappers to offer strandedness options. Certainly tophat2 and hisat(2) do, but also things like salmon.&#xD;&#xA;&#xD;&#xA;Again, for bigWig files the question becomes whether you really need two files to get the information you need. You're not going to delete the BAM files, so you can always make them if you need them." />
  <row Id="4448" PostHistoryTypeId="2" PostId="2094" RevisionGUID="c6d27c03-9d6b-46e7-9206-2f1a8bc26a10" CreationDate="2017-07-18T18:17:26.237" UserId="77" Text="To quote from the [snakemake documentation](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html#configure-working-directory)&#xD;&#xA;&#xD;&#xA;&gt; All paths in the snakefile are interpreted relative to the directory snakemake is executed in. This behaviour can be overridden by specifying a workdir in the snakefile:&#xD;&#xA;&#xD;&#xA;&gt; `workdir: &quot;path/to/workdir&quot;`&#xD;&#xA;&#xD;&#xA;&gt; Usually, it is preferred to only set the working directory via the command line, because above directive limits the portability of Snakemake workflows.&#xD;&#xA;&#xD;&#xA;It's not explicitly stated, but it's sort of implied that all relative paths then become relative to the working directory. I would expect that specifying an absolute path would get around this.&#xD;&#xA;&#xD;&#xA;As an aside, in my mind setting the working directory is usually only needed on clusters without a shared file system (presumably shared between the worker nodes, but not with the head node), since there you can't `cd $WORKDIR` before running snakemake." />
  <row Id="4449" PostHistoryTypeId="5" PostId="2094" RevisionGUID="9cf9cd72-ab02-43de-9488-d7234014e4fc" CreationDate="2017-07-18T18:44:51.447" UserId="77" Comment="added 63 characters in body" Text="To quote from the [snakemake documentation](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html#configure-working-directory)&#xD;&#xA;&#xD;&#xA;&gt; All paths in the snakefile are interpreted relative to the directory snakemake is executed in. This behaviour can be overridden by specifying a workdir in the snakefile:&#xD;&#xA;&#xD;&#xA;&gt; `workdir: &quot;path/to/workdir&quot;`&#xD;&#xA;&#xD;&#xA;&gt; Usually, it is preferred to only set the working directory via the command line, because above directive limits the portability of Snakemake workflows.&#xD;&#xA;&#xD;&#xA;It's not explicitly stated, but it's sort of implied that all relative paths then become relative to the working directory. I would expect that specifying an absolute path would get around this.&#xD;&#xA;&#xD;&#xA;As an aside, in my mind setting the working directory is usually only needed on clusters without a shared file system (presumably shared between the worker nodes, but not with the head node), since there you can't `cd $WORKDIR` before running snakemake. This is then normally done with your scheduler, in such cases." />
  <row Id="4450" PostHistoryTypeId="2" PostId="2095" RevisionGUID="62feda7a-c6ca-41a5-ac94-b6496425a6ed" CreationDate="2017-07-18T22:32:49.527" UserId="842" Text="I used prokka to create a database for some fasta files but I noticed a strange difference between the prokka and fasta files. Normally the seqid for the gff3 output for prokka has seqid's that match the sequence id's of the fasta files. The files I just produced with prokka have a different seqid than my fasta files. My fasta files look like this &#xD;&#xA;&#xD;&#xA;&gt; \&gt;NODE_108_length_645_cov_0.679537_ID_215&#xD;&#xA;&#xD;&#xA;However the prokka output gff3 file for the same fasta file uses a seqid like this&#xD;&#xA;&gt; gnl|X|JHIOGLGG_28&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;What would cause this difference? And do the last digits still correlate to the same seqid's even though the string before it has changed?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4451" PostHistoryTypeId="1" PostId="2095" RevisionGUID="62feda7a-c6ca-41a5-ac94-b6496425a6ed" CreationDate="2017-07-18T22:32:49.527" UserId="842" Text="Does Prokka or gff3 change the sequence / subject ID of fasta files?" />
  <row Id="4452" PostHistoryTypeId="3" PostId="2095" RevisionGUID="62feda7a-c6ca-41a5-ac94-b6496425a6ed" CreationDate="2017-07-18T22:32:49.527" UserId="842" Text="&lt;fasta&gt;&lt;gff3&gt;&lt;sequence-analysis&gt;" />
  <row Id="4453" PostHistoryTypeId="2" PostId="2096" RevisionGUID="ddcc5ebe-3cd0-47e5-b5cd-c8ae94e56bbc" CreationDate="2017-07-18T22:47:05.673" UserId="150" Text="Another quick Google search reveals the [Peptides R package][1].&#xD;&#xA;&#xD;&#xA;    library(Peptides)&#xD;&#xA;    s &lt;- &quot;RKTTLVPNTQTASPR&quot;&#xD;&#xA;    charge(s)&#xD;&#xA;&#xD;&#xA;    [1] 2.997683&#xD;&#xA;&#xD;&#xA;Optional arguments to `charge()` are `pH` (default = 7) and `pKscale` (choice of nine, default = &quot;Lehninger&quot;). See `?charge` for details.&#xD;&#xA;&#xD;&#xA;Another (non-R) option is the [EMBOSS suite program iep][2]. I highly recommend installing EMBOSS, it provides a suite of useful command-line tools with output that can be easily read and processed by other sofware, including R.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://cran.r-project.org/web/packages/Peptides/index.html&#xD;&#xA;  [2]: http://emboss.sourceforge.net/apps/release/6.6/emboss/apps/iep.html" />
  <row Id="4454" PostHistoryTypeId="5" PostId="2095" RevisionGUID="c823864d-49cd-472f-b4dd-afe095dc61c6" CreationDate="2017-07-18T23:24:55.483" UserId="298" Comment="added 9 characters in body" Text="I used prokka to create a database for some fasta files but I noticed a strange difference between the prokka and fasta files. Normally the seqid for the gff3 output for prokka has seqid's that match the sequence id's of the fasta files. The files I just produced with prokka have a different seqid than my fasta files. My fasta files look like this &#xD;&#xA;&#xD;&#xA;    &gt;NODE_108_length_645_cov_0.679537_ID_215&#xD;&#xA;&#xD;&#xA;However the prokka output gff3 file for the same fasta file uses a seqid like this&#xD;&#xA;    &#xD;&#xA;    gnl|X|JHIOGLGG_28&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;What would cause this difference? And do the last digits still correlate to the same seqid's even though the string before it has changed?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4455" PostHistoryTypeId="2" PostId="2097" RevisionGUID="2fc5f5fb-8aa0-4c03-a78b-293560e1e3a1" CreationDate="2017-07-19T03:09:25.747" UserId="1133" Text="I want to survival analysis using the subset of TCGA LUAD dataset, which identifier are located at https://xenabrowser.net/datapages/?host=https%3A%2F%2Ftcga.xenahubs.net&amp;dataset=TCGA.LUAD.sampleMap%2FLUAD_clinicalMatrix&amp;label=Phenotypes&amp;allIdentifiers=true .&#xD;&#xA;&#xD;&#xA;    &gt; head(LUAD_clinicalMatrix[,c(&quot;X_EVENT&quot;,&quot;X_OS_IND&quot;)])&#xD;&#xA;      X_EVENT X_OS_IND&#xD;&#xA;    1      NA       NA&#xD;&#xA;    2       0        0&#xD;&#xA;    3       1        1&#xD;&#xA;    4       0        0&#xD;&#xA;    5       0        0&#xD;&#xA;    6       0        0&#xD;&#xA;&#xD;&#xA;    &gt; all(LUAD_clinicalMatrix$X_EVENT==LUAD_clinicalMatrix$X_OS_UNIT)&#xD;&#xA;    [1] FALSE&#xD;&#xA;&#xD;&#xA;I want to know which identifier means the sample is alive or dead, and &quot;1&quot; means alive or dead?" />
  <row Id="4456" PostHistoryTypeId="1" PostId="2097" RevisionGUID="2fc5f5fb-8aa0-4c03-a78b-293560e1e3a1" CreationDate="2017-07-19T03:09:25.747" UserId="1133" Text="What does the &quot;X_OS_IND&quot; column mean?" />
  <row Id="4457" PostHistoryTypeId="3" PostId="2097" RevisionGUID="2fc5f5fb-8aa0-4c03-a78b-293560e1e3a1" CreationDate="2017-07-19T03:09:25.747" UserId="1133" Text="&lt;r&gt;&lt;file-formats&gt;" />
  <row Id="4458" PostHistoryTypeId="2" PostId="2098" RevisionGUID="52daa4f6-4870-4b0e-95f8-911eaef928b5" CreationDate="2017-07-19T04:55:16.090" UserId="1082" Text="So that I can start to practice GWAS and get ready to analyze human data. &#xD;&#xA;&#xD;&#xA;I can find several data sets (like [ZFIN][1]) that have the annotated genome information, but that's only one genome. Where can I download the raw data (like genotypes of 500 zebrafishes and corresponding phenotypes) to start my own analysis? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://zfin.org/downloads" />
  <row Id="4459" PostHistoryTypeId="1" PostId="2098" RevisionGUID="52daa4f6-4870-4b0e-95f8-911eaef928b5" CreationDate="2017-07-19T04:55:16.090" UserId="1082" Text="Where can I find a GWAS data set about genotypes and phenotypes for Zebrafish and rice?" />
  <row Id="4460" PostHistoryTypeId="3" PostId="2098" RevisionGUID="52daa4f6-4870-4b0e-95f8-911eaef928b5" CreationDate="2017-07-19T04:55:16.090" UserId="1082" Text="&lt;database&gt;&lt;gwas&gt;" />
  <row Id="4461" PostHistoryTypeId="2" PostId="2099" RevisionGUID="abb2b7c9-0b7c-4e10-863f-87fb879e1f66" CreationDate="2017-07-19T08:31:11.713" UserId="982" Text="Prokka is an annotation tool. It takes your contigs file with fasta headers `&gt;NODE_108...`and searchs for ORFs and their putative functions. Prokka produces submission ready output, the new fasta headers you see `&gt;JHIOGLGG_28...`are locus tags and gene description from the annotation and are not related to contig node data. Please see the [github information page][1] for more info.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/tseemann/prokka#output-files" />
  <row Id="4462" PostHistoryTypeId="34" PostId="1007" RevisionGUID="2c4786c5-af3c-4a2c-84a1-348b33cba9ec" CreationDate="2017-07-19T08:51:07.077" UserId="931" Comment="2" />
  <row Id="4464" PostHistoryTypeId="2" PostId="2100" RevisionGUID="5ad8ab2a-69fd-47aa-9344-3dc3b5592d4b" CreationDate="2017-07-19T08:55:52.523" UserId="931" Text="According to [this link][1], 1=death 0=censor and null=no data.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://groups.google.com/forum/#!topic/ucsc-cancer-genomics-browser/YvKnWZSsw1Q" />
  <row Id="4465" PostHistoryTypeId="2" PostId="2101" RevisionGUID="eda44835-b472-4b6a-872f-5e9b53d70558" CreationDate="2017-07-19T13:41:05.330" UserId="292" Text="I have surprisingly low counts when running `featureCounts` on some (single-end) RNA-seq data mapped on *C. elegans* genome using `hisat2`.&#xD;&#xA;&#xD;&#xA;To more easily show the problem, I generated a small subset of the bam file and of the annotation file I'm using. Here is what I can see when loading these two files on IGV:&#xD;&#xA;&#xD;&#xA;[![View of the his-16 -&gt; his-12 region in IGV][1]][1]&#xD;&#xA;&#xD;&#xA;The coverage for the &quot;his-11&quot; and &quot;his-15&quot; genes peak at around 3600. For &quot;his-12&quot; and &quot;his-16&quot;, it peaks at more than 1000&#xD;&#xA;&#xD;&#xA;Here are the content of my annotation file:&#xD;&#xA;&#xD;&#xA;    $ cat his_11-16.gtf &#xD;&#xA;    II	ensembl	transcript	13817759	13818143	.	-	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001890&quot;; gene_name &quot;his-16&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P8354&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.10&quot;; transcript_name &quot;ZK131.10&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS11539&quot;;&#xD;&#xA;    II	ensembl	transcript	13818371	13818739	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001889&quot;; gene_name &quot;his-15&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P8185&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.9&quot;; transcript_name &quot;ZK131.9&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS52436&quot;;&#xD;&#xA;    II	ensembl	transcript	13819626	13819937	.	-	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001888&quot;; gene_name &quot;his-14&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P21890&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.8&quot;; transcript_name &quot;ZK131.8&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS50308&quot;;&#xD;&#xA;    II	ensembl	transcript	13820210	13820620	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001887&quot;; gene_name &quot;his-13&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P2149&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.7&quot;; transcript_name &quot;ZK131.7&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS35106&quot;;&#xD;&#xA;    II	ensembl	transcript	13821198	13821581	.	-	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001886&quot;; gene_name &quot;his-12&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P26082&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.6&quot;; transcript_name &quot;ZK131.6&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS23693&quot;;&#xD;&#xA;    II	ensembl	transcript	13821778	13822314	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001885&quot;; gene_name &quot;his-11&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P10535&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.5&quot;; transcript_name &quot;ZK131.5&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS52792&quot;;&#xD;&#xA;&#xD;&#xA;The `featureCounts` run:&#xD;&#xA;&#xD;&#xA;    $ featureCounts -a his_11-16.gtf -o his_11-16_counts.txt -t transcript -g gene_name -O his_11-16_sorted.bam&#xD;&#xA;    &#xD;&#xA;            ==========     _____ _    _ ____  _____  ______          _____  &#xD;&#xA;            =====         / ____| |  | |  _ \|  __ \|  ____|   /\   |  __ \ &#xD;&#xA;              =====      | (___ | |  | | |_) | |__) | |__     /  \  | |  | |&#xD;&#xA;                ====      \___ \| |  | |  _ &lt;|  _  /|  __|   / /\ \ | |  | |&#xD;&#xA;                  ====    ____) | |__| | |_) | | \ \| |____ / ____ \| |__| |&#xD;&#xA;            ==========   |_____/ \____/|____/|_|  \_\______/_/    \_\_____/&#xD;&#xA;    	  v1.5.2&#xD;&#xA;    &#xD;&#xA;    //========================== featureCounts setting ===========================\\&#xD;&#xA;    ||                                                                            ||&#xD;&#xA;    ||             Input files : 1 BAM file                                       ||&#xD;&#xA;    ||                           S his_11-16_sorted.bam                           ||&#xD;&#xA;    ||                                                                            ||&#xD;&#xA;    ||             Output file : his_11-16_counts.txt                             ||&#xD;&#xA;    ||                 Summary : his_11-16_counts.txt.summary                     ||&#xD;&#xA;    ||              Annotation : his_11-16.gtf (GTF)                              ||&#xD;&#xA;    ||      Dir for temp files : ./                                               ||&#xD;&#xA;    ||                                                                            ||&#xD;&#xA;    ||                 Threads : 1                                                ||&#xD;&#xA;    ||                   Level : meta-feature level                               ||&#xD;&#xA;    ||              Paired-end : no                                               ||&#xD;&#xA;    ||         Strand specific : no                                               ||&#xD;&#xA;    ||      Multimapping reads : not counted                                      ||&#xD;&#xA;    || Multi-overlapping reads : counted                                          ||&#xD;&#xA;    ||   Min overlapping bases : 1                                                ||&#xD;&#xA;    ||                                                                            ||&#xD;&#xA;    \\===================== http://subread.sourceforge.net/ ======================//&#xD;&#xA;    &#xD;&#xA;    //================================= Running ==================================\\&#xD;&#xA;    ||                                                                            ||&#xD;&#xA;    || Load annotation file his_11-16.gtf ...                                     ||&#xD;&#xA;    ||    Features : 6                                                            ||&#xD;&#xA;    ||    Meta-features : 6                                                       ||&#xD;&#xA;    ||    Chromosomes/contigs : 1                                                 ||&#xD;&#xA;    ||                                                                            ||&#xD;&#xA;    || Process BAM file his_11-16_sorted.bam...                                   ||&#xD;&#xA;    ||    Single-end reads are included.                                          ||&#xD;&#xA;    ||    Assign reads to features...                                             ||&#xD;&#xA;    ||    Total reads : 35037                                                     ||&#xD;&#xA;    ||    Successfully assigned reads : 1849 (5.3%)                               ||&#xD;&#xA;    ||    Running time : 0.00 minutes                                             ||&#xD;&#xA;    ||                                                                            ||&#xD;&#xA;    ||                         Read assignment finished.                          ||&#xD;&#xA;    ||                                                                            ||&#xD;&#xA;    || Summary of counting results can be found in file &quot;his_11-16_counts.txt&quot;    ||&#xD;&#xA;    ||                                                                            ||&#xD;&#xA;    \\===================== http://subread.sourceforge.net/ ======================//&#xD;&#xA;    &#xD;&#xA;And the resulting counts file:&#xD;&#xA;&#xD;&#xA;    $ cat his_11-16_counts.txt&#xD;&#xA;    # Program:featureCounts v1.5.2; Command:&quot;featureCounts&quot; &quot;-a&quot; &quot;his_11-16.gtf&quot; &quot;-o&quot; &quot;his_11-16_counts.txt&quot; &quot;-t&quot; &quot;transcript&quot; &quot;-g&quot; &quot;gene_name&quot; &quot;-O&quot; &quot;his_11-16_sorted.bam&quot; &#xD;&#xA;    Geneid	Chr	Start	End	Strand	Length	his_11-16_sorted.bam&#xD;&#xA;    his-16	II	13817759	13818143	-	385	869&#xD;&#xA;    his-15	II	13818371	13818739	+	369	3&#xD;&#xA;    his-14	II	13819626	13819937	-	312	953&#xD;&#xA;    his-13	II	13820210	13820620	+	411	23&#xD;&#xA;    his-12	II	13821198	13821581	-	384	423&#xD;&#xA;    his-11	II	13821778	13822314	+	537	1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Why such a low assignment rate, why so little counts?**&#xD;&#xA;&#xD;&#xA;I see that `hisat2` decided to split an important proportion of the reads between orthologs, like &quot;his-16&quot; and &quot;his-12&quot;, but my understanding is that the default values for the `minOverlap` and `fracOverlap` parameters should ensure that such split reads will be counted:&#xD;&#xA;&#xD;&#xA;      --minOverlap &lt;int&gt;  Minimum number of overlapping bases in a read that is&#xD;&#xA;                          required for read assignment. 1 by default. Number of&#xD;&#xA;                          overlapping bases is counted from both reads if paired&#xD;&#xA;                          end. If a negative value is provided, then a gap of up&#xD;&#xA;                          to specified size will be allowed between read and the&#xD;&#xA;                          feature that the read is assigned to.&#xD;&#xA;    &#xD;&#xA;      --fracOverlap &lt;float&gt; Minimum fraction of overlapping bases in a read that is&#xD;&#xA;                          required for read assignment. Value should be within range&#xD;&#xA;                          [0,1]. 0 by default. Number of overlapping bases is&#xD;&#xA;                          counted from both reads if paired end. Both this option&#xD;&#xA;                          and '--minOverlap' option need to be satisfied for read&#xD;&#xA;                          assignment.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;What did I get wrong?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/iijez.png&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4466" PostHistoryTypeId="1" PostId="2101" RevisionGUID="eda44835-b472-4b6a-872f-5e9b53d70558" CreationDate="2017-07-19T13:41:05.330" UserId="292" Text="Counts obtained by featureCounts seem much less than observed coverage" />
  <row Id="4467" PostHistoryTypeId="3" PostId="2101" RevisionGUID="eda44835-b472-4b6a-872f-5e9b53d70558" CreationDate="2017-07-19T13:41:05.330" UserId="292" Text="&lt;read-mapping&gt;&lt;featurecounts&gt;&lt;coverage&gt;" />
  <row Id="4468" PostHistoryTypeId="2" PostId="2102" RevisionGUID="ca0fcb61-0143-45da-a3b5-29ab7bc0813d" CreationDate="2017-07-19T15:02:52.070" UserId="1140" Text="I would suggest you to run some simulation. You can use [wgsim][1] to simulate reads that are as much similar to what you want an answer for.&#xD;&#xA;I don't think there are faster methods.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/lh3/wgsim" />
  <row Id="4469" PostHistoryTypeId="5" PostId="2098" RevisionGUID="a57fe308-9ae0-444a-97e6-d8b639b462c3" CreationDate="2017-07-19T16:14:25.300" UserId="1082" Comment="added 548 characters in body" Text="My goal is to work on GWAS method development as a research project. However, human genome data are usually confidential because of the identification problem, so it's very hard to get them. (application takes a long time and I may not get it at the end.)&#xD;&#xA;&#xD;&#xA;Therefore, maybe I can try to play with the data of other species first. So I wonder where I can find some GWAS data for Zebrafish and rice. (The reason I am asking for these two is that I find researchers tend to perform genome analysis to mice, Arabidopsis Thaliana, zebrafish, and rice, and I already found where to download the GWAS data for the first two.)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I can find several data sets (like [ZFIN][1]) that have the annotated genome information, but that's only one genome. Where can I download the raw data (like genotypes of 500 zebrafishes and corresponding phenotypes) to start my own analysis? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://zfin.org/downloads" />
  <row Id="4470" PostHistoryTypeId="5" PostId="2078" RevisionGUID="4d2f2ffb-ec82-4629-a217-7c06dd8d723a" CreationDate="2017-07-19T17:50:18.107" UserId="888" Comment="added 17 characters in body" Text="I am trying to use FDist2 (method to detect Fst outlier) via [Arlequin][1] (short explanation of this method in the manual at page 161 under `Detection of loci under selection from F-statistics`).&#xD;&#xA;&#xD;&#xA;I've got a decent knowledge of programming (incl. bash scripting) but I fail to understand how Arlequin works. Could you please help me with a very simple reproducible example on how to use FDist2 via Arlequin via the command line?&#xD;&#xA;&#xD;&#xA;------&#xD;&#xA;&#xD;&#xA;As the .zip file comes with its pre-compiled version, I'll just note that I'm on Mac OS X but of course, a reproducible example with Linux (or at worst with Windows) would also be helpful.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cmpg.unibe.ch/software/arlequin35/Arl35Downloads.html" />
  <row Id="4471" PostHistoryTypeId="2" PostId="2103" RevisionGUID="a884cf8b-ac46-447d-99fd-ddd8f6be08d3" CreationDate="2017-07-19T19:20:48.830" UserId="37" Text="Mapping quality is determined by the repetitiveness of the genome, the sequencing error rate, insert size, the capability of the mapper and the nasty heuristics behind the mapper. MAPQ=0 to one mapper is not necessarily MAPQ=0 to another.&#xD;&#xA;&#xD;&#xA;That said, I get what you mean. You want to know the uniqueness/repetitiveness. It is still hard if you want to get a useful answer. For 150bp reads, each reference position is covered by 150 reads. What if 50 of them have no other exact hits elsewhere, but the rest 100 have? Is this position a repeat or not? In addition, what if the 50 are unique only because one mismatch? If there is a variant at that mismatch, the 50 would become repeats or mapped elsewhere.&#xD;&#xA;&#xD;&#xA;My preference is to say a position is &quot;unique&quot; under k-long reads if over k/2 reads overlapping the position have no other perfect or 1-mismatch/1-gap hits elsewhere in the genome. Under this definition, 79.3% of human genome are unique for 35bp reads. 92.4% for 75bp reads. I don't have the number for 150bp reads. I guess it will be around 95%. Empirically, 94-95% of human genome is callable with 100bp paired-end reads.&#xD;&#xA;&#xD;&#xA;As to other measurements, the most common one is the fraction of reads that has an exact hit elsewhere. You can use Fabio's method. It gives a good enough estimate once you simulate over 1 million reads. This fraction is around 86% for 35bp reads and 95% for 75bp reads, as I remember. The problem with this approach is this fraction is not very informative to variant calling due to the issues I talked about. Another way is to use RepeatMasker. It is worse. RepeatMasker masks 50% of human genome, but excludes segmental duplications where short reads can't be confidently mapped." />
  <row Id="4472" PostHistoryTypeId="2" PostId="2104" RevisionGUID="5a7a9336-b66a-4752-bbd8-d6a9e992c64e" CreationDate="2017-07-19T19:39:13.950" UserId="73" Text="I think it will be more productive for you if you choose one of the public human datasets and practise your GWAS methods on that. A couple of examples examples:&#xD;&#xA;&#xD;&#xA;* [1000 genomes](http://www.internationalgenome.org/data) -- use &quot;population&quot; as your trait of association. You'll find that you get association across the whole genome, so the analysis method would change from &quot;what bits are associated&quot; to &quot;what are the bits that are most consistently associated&quot;&#xD;&#xA;&#xD;&#xA;* NCI [Genomic Data Commons](https://portal.gdc.cancer.gov/repository) -- open access (and controlled access) data associated with cancer" />
  <row Id="4476" PostHistoryTypeId="2" PostId="2106" RevisionGUID="f9dee5fa-55fb-4693-8eea-b8a908882141" CreationDate="2017-07-19T20:49:38.947" UserId="926" Text="Could some one please help me with understanding how to generate the ribosomal interval list that is required to when using picard metrics this step&#xD;&#xA;&#xD;&#xA;    java -jar picard.jar CollectRnaSeqMetrics \&#xD;&#xA;          I=input.bam \&#xD;&#xA;          O=output.RNA_Metrics \&#xD;&#xA;          REF_FLAT=ref_flat.txt \&#xD;&#xA;          STRAND=SECOND_READ_TRANSCRIPTION_STRAND \&#xD;&#xA;          RIBOSOMAL_INTERVALS=ribosomal.interval_list&#xD;&#xA;&#xD;&#xA;I have a couple of bam files that were aligned using mm9 ensembl gtf as annotation, i did a lot of search and found this link Ribosomal Intervals For Collectrnaseqmetrics but this link or any other source does not explain how to produce or find ribo interval list from ensembl annotation&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4477" PostHistoryTypeId="1" PostId="2106" RevisionGUID="f9dee5fa-55fb-4693-8eea-b8a908882141" CreationDate="2017-07-19T20:49:38.947" UserId="926" Text="ribo interval list for picard ensembl mm9 gtf" />
  <row Id="4478" PostHistoryTypeId="3" PostId="2106" RevisionGUID="f9dee5fa-55fb-4693-8eea-b8a908882141" CreationDate="2017-07-19T20:49:38.947" UserId="926" Text="&lt;rna-seq&gt;" />
  <row Id="4479" PostHistoryTypeId="2" PostId="2107" RevisionGUID="b6ea7a54-99dd-4774-90dc-bfe1ea89003c" CreationDate="2017-07-19T21:08:28.883" UserId="77" Text="At least with Ensembl annotations, you can `grep rRNA foo.gtf &gt; rRNA.gtf` to get the annotated rRNAs. Note however, that the [45S cassette](https://www.ncbi.nlm.nih.gov/gene/100861531) isn't in the mouse reference genome, so you'll never have alignments to 18S, 5.8S, or 28S rRNA. I always get the feeling that the Collect*Metrics programs are made with human sequences in mind, since the cassette is included in the human reference genome.&#xD;&#xA;&#xD;&#xA;Anyway, while you can use the annotation and reference sequence to make picard happy, make sure to append the Rn45S sequence to your genome if you want to actually measure rRNA levels. In my experience this is only useful for rRNA-depleted samples, since the rates can vary wildly there." />
  <row Id="4480" PostHistoryTypeId="2" PostId="2108" RevisionGUID="664697a3-c34e-4822-b667-e1f7feb661de" CreationDate="2017-07-19T21:16:01.000" UserId="77" Text="Given the high level of multimapping in this region, you'll need to use the `-M --primary` options if you want to keep many of the alignments. I would be very hesitant to use these numbers as input for DESeq2 or similar programs, since it's fairly questionable whether one should fully trust the &quot;randomness&quot; of the aligner's assignments. I'm more comfortable using something like [salmon](http://salmon.readthedocs.io/en/latest/) for such cases." />
  <row Id="4481" PostHistoryTypeId="5" PostId="2107" RevisionGUID="926ebd7c-31e4-4bef-9333-8247595505c7" CreationDate="2017-07-19T21:20:24.753" UserId="77" Comment="added 482 characters in body" Text="At least with Ensembl annotations, you can `grep rRNA foo.gtf &gt; rRNA.gtf` to get the annotated rRNAs. Note however, that the [45S cassette](https://www.ncbi.nlm.nih.gov/gene/100861531) isn't in the mouse reference genome, so you'll never have alignments to 18S, 5.8S, or 28S rRNA. I always get the feeling that the Collect*Metrics programs are made with human sequences in mind, since the cassette is included in the human reference genome.&#xD;&#xA;&#xD;&#xA;Anyway, while you can use the annotation and reference sequence to make picard happy, make sure to append the Rn45S sequence to your genome if you want to actually measure rRNA levels. In my experience this is only useful for rRNA-depleted samples, since the rates can vary wildly there.&#xD;&#xA;&#xD;&#xA;**Edit**: To make a file compatible with picard's &quot;interval list&quot;, one can do the following with the GTF file:&#xD;&#xA;&#xD;&#xA;    grep rRNA foo.gtf | awk 'BEGIN{OFS=&quot;\t&quot;}{print $1, $4-1, $5}' &gt; foo.bed&#xD;&#xA;&#xD;&#xA;According to the [picard documentation](https://gatkforums.broadinstitute.org/gatk/discussion/1319/collected-faqs-about-interval-lists) a BED file can be used as input and the above `awk` code extracts the appropriate columns and switches to 0-based coordinates used in the BED format." />
  <row Id="4483" PostHistoryTypeId="5" PostId="2067" RevisionGUID="6cf9f090-8ea8-432c-84b4-d6bd51dbec31" CreationDate="2017-07-20T11:05:50.983" UserId="982" Comment="added 42 characters in body" Text="I want to perform a genome comparison on a group of isolates. I want to look into two broad groups of taxa and compare the accessory genome in each group. I have been using prokka (v1.12) and roary (v3.8.2) to do this but it appears the accessory_binary_genes.fa file is actually an untrue representation.&#xD;&#xA;&#xD;&#xA;**Note:** gene_presence_absence.Rtab does contain all the full presence/ absence for accessory gene sets. Despite this Im still unhappy with the nomenclature of the gene groups [issue for another day] &#xD;&#xA;&#xD;&#xA;&gt; [\[github issue 335\]][1] Your best to ignore the accessory_binary_genes.fa file. It is just for creating a quick and dirty tree with FastTree. The file itself is filtered to remove very common and not common variation to speedup the tree generation, hence the difference in numbers.&#xD;&#xA;&gt; &#xD;&#xA;&gt; The top 5% and bottom 5% are excluded. It is truncated at 4000 genes.&#xD;&#xA;&#xD;&#xA;I've been looking into alternative pipelines and a new software [BPGA][2] looks promising. Does anyone have experience with this?&#xD;&#xA;&#xD;&#xA;I essentially want a tool which will give me the core and accessory gene sets, without the noise from partial gene hits.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/sanger-pathogens/Roary/issues/335&#xD;&#xA;  [2]: https://www.nature.com/articles/srep24373" />
  <row Id="4484" PostHistoryTypeId="2" PostId="2109" RevisionGUID="d7132290-8c7a-44df-a2c5-ef75174f0a57" CreationDate="2017-07-20T12:38:01.380" UserId="734" Text="NCI-60 has additional [cell lines][1], but when I get results from [CellMiner][2] they are not included in there. &#xD;&#xA;&#xD;&#xA;Namely, the genes such as: 	`DLD-1`(Colon), `KM20L2`	(Colon), `M19-MEL` (Melanoma), don't show in the results. &#xD;&#xA;&#xD;&#xA;  [1]: https://dtp.cancer.gov/discovery_development/nci-60/cell_list.htm&#xD;&#xA;  [2]: https://discover.nci.nih.gov/cellminer/" />
  <row Id="4485" PostHistoryTypeId="1" PostId="2109" RevisionGUID="d7132290-8c7a-44df-a2c5-ef75174f0a57" CreationDate="2017-07-20T12:38:01.380" UserId="734" Text="Working with the additional cell lines in NCI-60" />
  <row Id="4486" PostHistoryTypeId="3" PostId="2109" RevisionGUID="d7132290-8c7a-44df-a2c5-ef75174f0a57" CreationDate="2017-07-20T12:38:01.380" UserId="734" Text="&lt;cancer&gt;&lt;cell-line&gt;" />
  <row Id="4487" PostHistoryTypeId="6" PostId="2081" RevisionGUID="6c5d3954-ee4d-4227-8b42-e0ab904e5f77" CreationDate="2017-07-20T12:56:04.567" UserId="57" Comment="tags" Text="&lt;genome&gt;&lt;read-mapping&gt;&lt;human&gt;&lt;illumina&gt;&lt;mapq&gt;" />
  <row Id="4488" PostHistoryTypeId="5" PostId="2109" RevisionGUID="bcdf1f17-c818-4803-97c8-1b7f2dd89158" CreationDate="2017-07-20T13:08:03.397" UserId="734" Comment="added 13 characters in body; added 82 characters in body" Text="NCI-60 has additional [cell lines][1], but when I get results from [CellMiner][2] they are not included in there. &#xD;&#xA;&#xD;&#xA;Namely, the genes such as: 	`DLD-1`(Colon), `KM20L2`	(Colon), `M19-MEL` (Melanoma), don't show in the results of CellMiner. &#xA;&#xA;Is there away to include these additional lines in the correlation calculations?&#xD;&#xA;&#xD;&#xA;  [1]: https://dtp.cancer.gov/discovery_development/nci-60/cell_list.htm&#xD;&#xA;  [2]: https://discover.nci.nih.gov/cellminer/" />
  <row Id="4489" PostHistoryTypeId="5" PostId="1001" RevisionGUID="c9db6b0a-e269-44da-b20f-9186e87326cf" CreationDate="2017-07-20T13:35:38.607" UserId="982" Comment="deleted 100 characters in body" Text="I have a core genome dataset of approx 2530 genes for 149 taxa. I have run an unpartitioned phylogenetic analysis using iqtree. But I am unhappy with the resolution in some of the phylogeny. I want to check if partitioning the alignment  will improve resolution. My alignment is from roary output and MAFFT alignment.&#xD;&#xA;&#xD;&#xA;Ive tried partitioning by loci and using iqtree (beta 1.6.4) to determine a model of best fit for each partition but this is apparently a very long demanding process as our HPC here keeps killing the analysis. What are other good methods/ means to partition DNA data?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;My next course of action is to remove uninformative sites. iqtree does a quick analysis of each partition and reveals which are parsimony uninformative sites. Would I be just as well to delete these partitions?&#xD;&#xA;&#xD;&#xA;My current iqtree command is as follows:&#xD;&#xA;&#xD;&#xA;    iqtree -nt AUTO  -s core_gene_alignment.aln -spp loci.nex  -m TESTMERGE -rcluster-max 100&#xD;&#xA;" />
  <row Id="4490" PostHistoryTypeId="2" PostId="2110" RevisionGUID="09f50c7d-73d3-4533-a22a-bbcd6739aa25" CreationDate="2017-07-20T14:51:07.630" UserId="475" Text="As has been said before, mappability to the 'human genome' depends on a number of factors, among these the reference version and type of reads, for which you are interested in GRCh38 and 2x150bp reads. Although I am not aware of numbers accounting for these particular reference and type of Illumina reads, the 1000 genomes project has provided the community with a similar and close estimation that you might be interested in considering regarding your inquiry.&#xD;&#xA;&#xD;&#xA;Similar to your question, the 1K project estimated 'the proportion of the human genome that is less accessible to short reads'. In these estimates the human genome is GRCh37 and the types of reads in question are mostly 2x Illumina with a mixture of lengths with the longest being up to 250bp. In these estimates each base in the human genome is considered (and marked) 'less accesible' according to these criteria:  &#xD;&#xA;&#xD;&#xA;L - depth of coverage is much lower than average&#xD;&#xA;&#xD;&#xA;H - depth of coverage is much higher than average&#xD;&#xA;&#xD;&#xA;Z - too many reads with zero mapping quality overlap this position&#xD;&#xA;&#xD;&#xA;Q - the average mapping quality at the position is too low&#xD;&#xA;&#xD;&#xA;Each of these criteria has &quot;standard&quot; and &quot;strict&quot; thresholds for a base to be considered - or not - in each category. You can read more in the link below: &#xD;&#xA;&#xD;&#xA;ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/accessible_genome_masks/README.accessible_genome_mask.20140520&#xD;&#xA;&#xD;&#xA;According to the strict thresholds, the human genome has about 16.8% of &quot;Z&quot; and 3.1% of &quot;Q&quot; bases, respectively. Considering the &quot;Z&quot; and &quot;Q&quot; criteria as a proxy for ~ mapq=0, about 19.9% of the human genome can not be uniquely mapped. &#xD;&#xA; " />
  <row Id="4491" PostHistoryTypeId="2" PostId="2111" RevisionGUID="606adf31-dde5-4dda-a643-ff4ce6b99d4c" CreationDate="2017-07-20T15:53:54.547" UserId="704" Text="I'm using the Peptides package in R to calculate the mass of peptides with the mw() method. The think is that i want to calculate them in different pH values and thus I wrote this little function to take in mind the pH value according this image &#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;My code is this:&#xD;&#xA;&#xD;&#xA;    phmw = function(pept_seq , unimod_location, pH ){&#xD;&#xA;      &#xD;&#xA;      monoisotopic_hydrogen_mass = 1.00794&#xD;&#xA;      &#xD;&#xA;      if(pH &lt; 7){&#xD;&#xA;        &#xD;&#xA;        # mol_weight = mol_weight_of_peptide + mol_weight_of_Hydrogen&#xD;&#xA;        mol_weight = mw(pept_seq, monoisotopic = T) + monoisotopic_hydrogen_mass&#xD;&#xA;        &#xD;&#xA;      }else if(pH &gt; 7){&#xD;&#xA;        &#xD;&#xA;        # mol_weight = mol_weight_of_peptide - mol_weight_of_Hydrogen&#xD;&#xA;        mol_weight = mw(pept_seq, monoisotopic = T) - monoisotopic_hydrogen_mass&#xD;&#xA;        &#xD;&#xA;      }else if(pH == 7){&#xD;&#xA;        &#xD;&#xA;        # mol_weight = mol_weight_of_peptide &#xD;&#xA;        mol_weight = mw(pept_seq, monoisotopic = T) &#xD;&#xA;      }&#xD;&#xA;      &#xD;&#xA;      return(mol_weight)&#xD;&#xA;      &#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Do you think that this approach is correct or not ?&#xD;&#xA;&#xD;&#xA;Thank you.&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/iA96t.jpg" />
  <row Id="4492" PostHistoryTypeId="1" PostId="2111" RevisionGUID="606adf31-dde5-4dda-a643-ff4ce6b99d4c" CreationDate="2017-07-20T15:53:54.547" UserId="704" Text="Influence of pH on protein peptide mass" />
  <row Id="4493" PostHistoryTypeId="3" PostId="2111" RevisionGUID="606adf31-dde5-4dda-a643-ff4ce6b99d4c" CreationDate="2017-07-20T15:53:54.547" UserId="704" Text="&lt;r&gt;&lt;proteins&gt;" />
  <row Id="4494" PostHistoryTypeId="5" PostId="2092" RevisionGUID="404cde06-7f30-492a-86d2-e9329fad801e" CreationDate="2017-07-20T21:46:00.317" UserId="727" Comment="added 823 characters in body" Text="I have found a blog post with a script that I would like to use for my current research project: [link](https://bcbio.wordpress.com/2009/02/07/automated-protein-conservation-display-from-blast-alignments/)&#xD;&#xA;&#xD;&#xA;The script is incredibly fast and produces a smooth conservation plot. In the blog post, the author mentions that it would be totally possible to change the remote blast function to a local database. Instead of making a local database, I have opted to try to change the function to still use a remote blast, but with different parameters.&#xD;&#xA;&#xD;&#xA;I found the following resource for learning about the NCBIWWW module: [link](http://biopython.org/DIST/docs/api/Bio.Blast.NCBIWWW-module.html)&#xD;&#xA;&#xD;&#xA;Using this, I rewrote the line in the script:&#xD;&#xA;&#xD;&#xA;    blast_handle = NCBIWWW.qblast(blast_method, &quot;nr&quot;, search_gi)&#xD;&#xA;&#xD;&#xA;to:&#xD;&#xA;&#xD;&#xA;    blast_handle = NCBIWWW.qblast(program=&quot;blastp&quot;, &#xD;&#xA;            							  database=&quot;refseq_protein&quot;, &#xD;&#xA;            							  sequence=search_gi, &#xD;&#xA;            							  entrez_query=&quot; txid2 [ORGN] OR txid4751 [ORGN] &quot;, &#xD;&#xA;            							  expect=20000.0, &#xD;&#xA;            							  alignments=10000, &#xD;&#xA;            							  descriptions=10000)&#xD;&#xA;&#xD;&#xA;Strangely, I get the exact same graph as an output even with the different parameters. Am I doing something about this search incorrectly?&#xD;&#xA;&#xD;&#xA;Thanks.&#xD;&#xA;&#xD;&#xA;Update:&#xD;&#xA;&#xD;&#xA;I got my BLAST to match the webpage. my current parameters are:&#xD;&#xA;&#xD;&#xA;    blast_handle = NCBIWWW.qblast(program=&quot;blastp&quot;, database=&quot;refseq_protein&quot;, sequence=&quot;P02686.3&quot;, word_size=6, expect=20000.0, hitlist_size=10000, gapcosts=&quot;11 1&quot;, matrix_name=&quot;BLOSUM62&quot;, filter=&quot;F&quot;, genetic_code=1, threshold=21, composition_based_statistics=2, alignments=10000, descriptions=10000, entrez_query=&quot; txid2 [ORGN] OR txid4751 [ORGN] &quot;)&#xD;&#xA;&#xD;&#xA;With this new blast_handle, the script no longer works. I get the following error codes:&#xD;&#xA;&#xD;&#xA;line 191, in &lt;module&gt;&#xD;&#xA;    main(sys.argv[1])&#xD;&#xA;&#xD;&#xA;line 27, in main&#xD;&#xA;    blast_rec = ncbi_manager.remote_blast(protein_gi, &quot;blastp&quot;)&#xD;&#xA;&#xD;&#xA;line 184, in remote_blast&#xD;&#xA;    return rec_it.next()&#xD;&#xA;&#xD;&#xA;line 575, in parse&#xD;&#xA;    raise ValueError(&quot;Your XML file was empty&quot;)&#xD;&#xA;&#xD;&#xA;ValueError: Your XML file was empty" />
  <row Id="4495" PostHistoryTypeId="2" PostId="2112" RevisionGUID="fbfb8f85-9366-407a-a866-94365f956c37" CreationDate="2017-07-20T23:10:02.150" UserId="888" Text="For MAC OSX, the executable found in `arlsumstat_macosx` (`arlsumstatmac_64bit`) seem to work appropriately. The directory for `arlsumstat_macosx` does not contain any examples but you can use example files from `arlecore_macosx/Example files_linux` using this syntax.&#xD;&#xA;&#xD;&#xA;The complication is that `arlsumstatmac_64bit` will look in the current directory for the file `arl_run.ars`. There is a file with the same name in `arlecore_macosx` but it is not compatible. So you have to be in the `arlsumstat_macosx`.&#xD;&#xA;&#xD;&#xA;In short, `cd` to `arlsumstat_macosx` and do&#xD;&#xA;&#xD;&#xA;    ./arlsumstatmac_64bit path/to/arlecore_macosx/Example\ files_linux/Freqncy/cohen.ars path/to/arlecore_macosx/Example\ files_linux/Freqncy/cohen.arp&#xD;&#xA;&#xD;&#xA;, where the `.arp` file contains the data and the `.ars` file contains the settings (the description of what you want Arlequin to do).&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4496" PostHistoryTypeId="5" PostId="2078" RevisionGUID="8fdc5df4-fca5-45ba-b09f-104e07c9b5ac" CreationDate="2017-07-20T23:10:33.560" UserId="888" Comment="deleted 221 characters in body; edited title" Text="I've got a decent knowledge of programming (incl. bash scripting) but I fail to understand how Arlequin works. Could you please help me with a very simple reproducible example on how to use Arlequin via the command line?&#xD;&#xA;&#xD;&#xA;As the .zip file comes with its pre-compiled version, I'll just note that I'm on Mac OS X but of course, a reproducible example with Linux (or at worst with Windows) would also be helpful.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cmpg.unibe.ch/software/arlequin35/Arl35Downloads.html" />
  <row Id="4497" PostHistoryTypeId="4" PostId="2078" RevisionGUID="8fdc5df4-fca5-45ba-b09f-104e07c9b5ac" CreationDate="2017-07-20T23:10:33.560" UserId="888" Comment="deleted 221 characters in body; edited title" Text="How can I use Arlequin via the command line?" />
  <row Id="4498" PostHistoryTypeId="2" PostId="2113" RevisionGUID="0ab0325a-2cac-4224-b27d-7087755f2c4a" CreationDate="2017-07-21T12:33:07.187" UserId="377" Text="A different sort of problem: even a small 'omics lab generates a lot of data, raw, intermediate and processed. What (software) solutions exist for managing this data, such that &quot;old&quot; data can be retrieved and checked or re-analysed, even after people have left the lab? Important points would be:&#xD;&#xA;&#xD;&#xA; - ease of installation&#xD;&#xA; - ease of putting data in, in an appropriately tagged / labeled fashion (it's no good if the repository is just a centralised bad mess)&#xD;&#xA; - useful search and exploration&#xD;&#xA; - security (i.e. restricted to members of the lab)&#xD;&#xA; - previews / summaries of the data&#xD;&#xA; - can accommodate any dataset&#xD;&#xA; - local, not SaaS&#xD;&#xA;&#xD;&#xA;It seems from my explorations of the topic that there's very little between the primitive (e.g. handrolled Access databases) and big industrial solutions. Things I have looked at include:&#xD;&#xA;&#xD;&#xA;- Dataverse: very popular, installation seems complex and unclear if uploading is that easy&#xD;&#xA;- DSpace: mostly for publications and documents&#xD;&#xA;- CKAN&#xD;&#xA;- OSF: used this for a while, integrates with a lot of services but uploading data seemed awkward" />
  <row Id="4499" PostHistoryTypeId="1" PostId="2113" RevisionGUID="0ab0325a-2cac-4224-b27d-7087755f2c4a" CreationDate="2017-07-21T12:33:07.187" UserId="377" Text="Solutions for managing data in a small bioinformatics / 'omics lab?" />
  <row Id="4500" PostHistoryTypeId="3" PostId="2113" RevisionGUID="0ab0325a-2cac-4224-b27d-7087755f2c4a" CreationDate="2017-07-21T12:33:07.187" UserId="377" Text="&lt;database&gt;&lt;data-management&gt;" />
  <row Id="4501" PostHistoryTypeId="2" PostId="2114" RevisionGUID="0daf9335-fb3c-412c-9330-e0436e53eb0f" CreationDate="2017-07-21T13:03:59.900" UserId="982" Text="I have a binary matrix of gene presence or absence which looks like: [roary output]&#xD;&#xA;&#xD;&#xA;    Gene	sample1	sample2	sample3	sample4&#xD;&#xA;    fliI	    1	    1     	1    	 1&#xD;&#xA;    patB_1    	1	    1	    1	     1&#xD;&#xA;    pgpA	    1	    1	    1        1&#xD;&#xA;    osmB	    1	    1	    1	     1&#xD;&#xA;    cspA	    1	    0	    1	     1&#xD;&#xA;&#xD;&#xA;How can I convert to fasta so it looks like and technically aligned:  &#xD;&#xA;&#xD;&#xA;    &gt;sample1&#xD;&#xA;    11111&#xD;&#xA;    &gt;sample2&#xD;&#xA;    11110&#xD;&#xA;&#xD;&#xA;Is it possible to do this in R? " />
  <row Id="4502" PostHistoryTypeId="1" PostId="2114" RevisionGUID="0daf9335-fb3c-412c-9330-e0436e53eb0f" CreationDate="2017-07-21T13:03:59.900" UserId="982" Text="How to convert a binary matrix of gene presence or absence into a fasta sequence" />
  <row Id="4503" PostHistoryTypeId="3" PostId="2114" RevisionGUID="0daf9335-fb3c-412c-9330-e0436e53eb0f" CreationDate="2017-07-21T13:03:59.900" UserId="982" Text="&lt;r&gt;&lt;fasta&gt;&lt;format-conversion&gt;" />
  <row Id="4504" PostHistoryTypeId="2" PostId="2115" RevisionGUID="d2b4e2b5-0795-440c-bd2d-ef47091de993" CreationDate="2017-07-21T13:20:17.730" UserId="77" Text="It's not a fasta file, but:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    &gt; m&#xD;&#xA;           sample1 sample2 sample3 sample4&#xD;&#xA;    fliI         1       1       1       1&#xD;&#xA;    patB_1       1       1       1       1&#xD;&#xA;    pgpA         1       1       1       1&#xD;&#xA;    osmB         1       1       1       1&#xD;&#xA;    cspA         1       0       1       1&#xD;&#xA;    &gt; # Collapse to labeled strings&#xD;&#xA;    &gt; blah = apply(m, 2, function(x) paste(x, collapse=''))&#xD;&#xA;    &gt; blah&#xD;&#xA;    sample1 sample2 sample3 sample4 &#xD;&#xA;    &quot;11111&quot; &quot;11110&quot; &quot;11111&quot; &quot;11111&quot; &#xD;&#xA;    &gt; # Write that to a file with the appropriate format&#xD;&#xA;    &gt; cat(paste(mapply(function(x, y) sprintf(&quot;&gt;%s\n%s\n&quot;, x, y), names(blah), blah), collapse=&quot;&quot;), file=&quot;some file.fa&quot;)&#xD;&#xA;&#xD;&#xA;Make sure to change `some file.fa`. I have no idea why you would want to do all of this, but this will give you the output you want:&#xD;&#xA;&#xD;&#xA;    $ cat blah.txt &#xD;&#xA;    &gt;sample1&#xD;&#xA;    11111&#xD;&#xA;    &gt;sample2&#xD;&#xA;    11110&#xD;&#xA;    &gt;sample3&#xD;&#xA;    11111&#xD;&#xA;    &gt;sample4&#xD;&#xA;    11111&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4505" PostHistoryTypeId="50" PostId="842" RevisionGUID="bbc06777-c36c-49d6-a12b-0e2fc52aebb3" CreationDate="2017-07-21T14:42:15.217" UserId="-1" />
  <row Id="4506" PostHistoryTypeId="2" PostId="2116" RevisionGUID="40b7c035-804b-4e41-8365-07fdbe17e762" CreationDate="2017-07-21T18:05:08.483" UserId="776" Text="One could also use `cut` and a pair of Python scripts:&#xD;&#xA;&#xD;&#xA;*transpose.py*&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python                                                                                                                                                                                                                                                                                                                       &#xD;&#xA;&#xD;&#xA;    import sys&#xD;&#xA;&#xD;&#xA;    for c in zip(*(l.split() for l in sys.stdin.readlines() if l.strip())):&#xD;&#xA;        sys.stdout.write(&quot;%s\n&quot; % ('\t'.join(c)))&#xD;&#xA;&#xD;&#xA;*mock_fasta.py*&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python                                                                                                                                                                                                                                                                                                                       &#xD;&#xA;&#xD;&#xA;    import sys&#xD;&#xA;&#xD;&#xA;    for e in (l.split() for l in sys.stdin.readlines() if l.strip()):&#xD;&#xA;        sys.stdout.write(&quot;&gt;%s\n%s\n&quot; % (e[0], ''.join(e[1:])))&#xD;&#xA;&#xD;&#xA;Then:&#xD;&#xA;&#xD;&#xA;    $ cut -f2- in.mtx | transpose.py | mock_fasta.py &gt; in.fa&#xD;&#xA;&#xD;&#xA;Which gives:&#xD;&#xA;&#xD;&#xA;    $ more in.fa&#xD;&#xA;    &gt;sample1&#xD;&#xA;    11111&#xD;&#xA;    &gt;sample2&#xD;&#xA;    11110&#xD;&#xA;    &gt;sample3&#xD;&#xA;    11111&#xD;&#xA;    &gt;sample4&#xD;&#xA;    11111" />
  <row Id="4507" PostHistoryTypeId="2" PostId="2117" RevisionGUID="eca957fe-3341-4f55-8f2d-c7b3785003af" CreationDate="2017-07-21T18:15:17.510" UserId="643" Text="I have identified a genomic region (CpG island) that is enriched for a characteristic of interest in the vast majority of my samples.&#xD;&#xA;&#xD;&#xA;After viewing it in the UCSC genome browser, I have confirmed that it does not overlap any annotated gene promoters, gene bodies, or retrotransposons, but am convinced that it is functionally interacting with something long range.&#xD;&#xA;&#xD;&#xA;Is there a publicly available visualization tool where I can input my genomic region and get a list of topologically associated domains from 3C or other chromatin-chromatin interaction data?" />
  <row Id="4508" PostHistoryTypeId="1" PostId="2117" RevisionGUID="eca957fe-3341-4f55-8f2d-c7b3785003af" CreationDate="2017-07-21T18:15:17.510" UserId="643" Text="Visualization tools for 3C/Hi-C long-range interaction data?" />
  <row Id="4509" PostHistoryTypeId="3" PostId="2117" RevisionGUID="eca957fe-3341-4f55-8f2d-c7b3785003af" CreationDate="2017-07-21T18:15:17.510" UserId="643" Text="&lt;software-recommendation&gt;&lt;visualization&gt;" />
  <row Id="4510" PostHistoryTypeId="2" PostId="2118" RevisionGUID="ab486d0c-1ef7-4e3d-88c8-53db85fc621f" CreationDate="2017-07-21T18:47:06.840" UserId="77" Text="Depending on the genome you want, you may be able to use the [chorogeneome navigator](http://chorogenome.ie-freiburg.mpg.de/), which is from some of my colleagues. This uses a number of public HiC datasets and allows you to view interactions." />
  <row Id="4511" PostHistoryTypeId="2" PostId="2119" RevisionGUID="68b8d322-9ed1-4c6f-8194-ea9f3deb4720" CreationDate="2017-07-21T19:45:06.567" UserId="926" Text="I have a problem here with my rna seq data:&#xD;&#xA;&#xD;&#xA;#Sequencing details:&#xD;&#xA;rRNA was removed, followed by cDNA preparation and generation of stranded libraries using the TruSeq Stranded Total RNA Sample Prep Kit. Sequencing performed on the HiSeq2500 platform (Illumina) to generate 2 × 125 bp paired-end reads&#xD;&#xA;&#xD;&#xA;#Alignment and preprocessing &#xD;&#xA;reads were aligned using tophat2(std aligner in pipeline at core,(--library-type fr-firststrand)) unfortunately the original unaligned files were purged and i only have access to this aligned file which i converted to fastq using following steps&#xD;&#xA;&#xD;&#xA; 1. samtools sort -n sample.bam -o sample_sorted.bam&#xD;&#xA; 2. bedtools bamtofastq -i sample_sorted.bam -fq sample_1.fq -fq2 sample_2.fq (get a lot of mate skipping errors here)&#xD;&#xA; 3. check for these reads for adapters and trim them and again use hisat2( with --rna-strandness RF)&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;#trimmed reads&#xD;&#xA;Reads were trimmed by passing parameters --&#xD;&#xA;adapters adapters.file --adapter-trim-end RIGHT --length-dist --threads 12 --adapter-min-overlap 7 --max-uncalled 250 --min-read-length 25 -- to FLEXBAR version 2.4&#xD;&#xA;&#xD;&#xA;##adapters for all samples using multiqc&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;##fastqc sequence length pre trimming [![enter image description here][2]][2]&#xD;&#xA;##fastqc sequence length post trimming &#xD;&#xA;[![enter image description here][3]][3]&#xD;&#xA;##rmats error&#xD;&#xA;Incorrect readLength. sample.bam has a read length of 114, while readLength param is 125&#xD;&#xA;&#xD;&#xA;##picard error on aligned bam from hisat2)&#xD;&#xA;ProcessExecutor	[2] &quot;Not creating insert size PDF as there are duplicated header names: All_Reads&quot;&#xD;&#xA;#snapshot of picard insertsize metrics file[![enter image description here][4]][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/IsVW7.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/fPONe.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/3nY5A.png&#xD;&#xA;  [4]: https://i.stack.imgur.com/lgeHk.png&#xD;&#xA;&#xD;&#xA;#Questions&#xD;&#xA;&#xD;&#xA; 1. Is my approach flawed at some step?&#xD;&#xA; 2. why do i get different read lengths error for each sample when i process it through rmats when i specify len as 125.&#xD;&#xA; 3. why is not histogram being produced when using picard insertsize metrics. &#xD;&#xA; 4. is normal to see the graph shift to the right after trimming?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4512" PostHistoryTypeId="1" PostId="2119" RevisionGUID="68b8d322-9ed1-4c6f-8194-ea9f3deb4720" CreationDate="2017-07-21T19:45:06.567" UserId="926" Text="insert size pre and post trimming" />
  <row Id="4513" PostHistoryTypeId="3" PostId="2119" RevisionGUID="68b8d322-9ed1-4c6f-8194-ea9f3deb4720" CreationDate="2017-07-21T19:45:06.567" UserId="926" Text="&lt;rna-seq&gt;&lt;rna-splicing&gt;&lt;trimming&gt;" />
  <row Id="4514" PostHistoryTypeId="5" PostId="2119" RevisionGUID="9d3b2464-2e8d-4d96-9a04-b3b57e317312" CreationDate="2017-07-21T19:54:13.983" UserId="926" Comment="deleted 1 character in body" Text="I have a problem here with my rna seq data:&#xD;&#xA;&#xD;&#xA;#Sequencing details:&#xD;&#xA;rRNA was removed, followed by cDNA preparation and generation of stranded libraries using the TruSeq Stranded Total RNA Sample Prep Kit. Sequencing performed on the HiSeq2500 platform (Illumina) to generate 2 × 125 bp paired-end reads&#xD;&#xA;&#xD;&#xA;#Alignment and preprocessing &#xD;&#xA;reads were aligned using tophat2(std aligner in pipeline at core,(--library-type fr-firststrand)) unfortunately the original unaligned files were purged and i only have access to this aligned file which i converted to fastq using following steps&#xD;&#xA;&#xD;&#xA; 1. samtools sort -n sample.bam -o sample_sorted.bam&#xD;&#xA; 2. bedtools bamtofastq -i sample_sorted.bam -fq sample_1.fq -fq2 sample_2.fq (get a lot of mate skipping errors here)&#xD;&#xA; 3. check for these reads for adapters and trim them and again use hisat2( with --rna-strandness RF)&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;#trimmed reads&#xD;&#xA;Reads were trimmed by passing parameters --&#xD;&#xA;adapters adapters.file --adapter-trim-end RIGHT --length-dist --threads 12 --adapter-min-overlap 7 --max-uncalled 250 --min-read-length 25 -- to FLEXBAR version 2.4&#xD;&#xA;&#xD;&#xA;##adapters for all samples using multiqc&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;##fastqc sequence length pre trimming [![enter image description here][2]][2]&#xD;&#xA;##fastqc sequence length post trimming &#xD;&#xA;[![enter image description here][3]][3]&#xD;&#xA;##rmats error&#xD;&#xA;Incorrect readLength. sample.bam has a read length of 114, while readLength param is 125&#xD;&#xA;&#xD;&#xA;##picard error on aligned bam from hisat2)&#xD;&#xA;ProcessExecutor	[2] &quot;Not creating insert size PDF as there are duplicated header names: All_Reads&quot;&#xD;&#xA;#snapshot of picard insertsize metrics file[![enter image description here][4]][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/IsVW7.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/fPONe.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/3nY5A.png&#xD;&#xA;  [4]: https://i.stack.imgur.com/lgeHk.png&#xD;&#xA;&#xD;&#xA;#Questions&#xD;&#xA;&#xD;&#xA; 1. Is my approach flawed at some step?&#xD;&#xA; 2. why do i get different read lengths error for each sample when i process it through rmats when i specify len as 125.&#xD;&#xA; 3. why is no histogram being produced when using picard insertsize metrics. &#xD;&#xA; 4. is normal to see the graph shift to the right after trimming?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4515" PostHistoryTypeId="5" PostId="2119" RevisionGUID="15da7e5c-2dfa-4336-ae60-cd1c57307fad" CreationDate="2017-07-21T20:07:29.230" UserId="926" Comment="added 171 characters in body" Text="I have a problem here with my rna seq data:&#xD;&#xA;&#xD;&#xA;#Sequencing details:&#xD;&#xA;rRNA was removed, followed by cDNA preparation and generation of stranded libraries using the TruSeq Stranded Total RNA Sample Prep Kit. Sequencing performed on the HiSeq2500 platform (Illumina) to generate 2 × 125 bp paired-end reads&#xD;&#xA;&#xD;&#xA;#Alignment and preprocessing &#xD;&#xA;reads were aligned using tophat2(std aligner in pipeline at core,(--library-type fr-firststrand)) unfortunately the original unaligned files were purged and i only have access to this aligned file which i converted to fastq using following steps&#xD;&#xA;&#xD;&#xA; 1. samtools sort -n sample.bam -o sample_sorted.bam&#xD;&#xA; 2. bedtools bamtofastq -i sample_sorted.bam -fq sample_1.fq -fq2 sample_2.fq (get a lot of mate skipping errors here)&#xD;&#xA; 3. check for these reads for adapters and trim them and again use hisat2( with --rna-strandness RF)&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;#trimmed reads&#xD;&#xA;Reads were trimmed by passing parameters --&#xD;&#xA;adapters adapters.file --adapter-trim-end RIGHT --length-dist --threads 12 --adapter-min-overlap 7 --max-uncalled 250 --min-read-length 25 -- to FLEXBAR version 2.4&#xD;&#xA;&#xD;&#xA;##adapters for all samples using multiqc&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;##fastqc sequence length pre trimming [![enter image description here][2]][2]&#xD;&#xA;##fastqc sequence length post trimming &#xD;&#xA;[![enter image description here][3]][3]&#xD;&#xA;##rmats error&#xD;&#xA;Incorrect readLength. sample.bam has a read length of 114, while readLength param is 125&#xD;&#xA;&#xD;&#xA;##picard error on aligned bam from hisat2)&#xD;&#xA;ProcessExecutor	[2] &quot;Not creating insert size PDF as there are duplicated header names: All_Reads&quot;&#xD;&#xA;#snapshot of picard insertsize metrics file[![enter image description here][4]][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;#Questions&#xD;&#xA;&#xD;&#xA; 1. Is my approach flawed at some step?&#xD;&#xA; 2. why do i get different read lengths error for each sample when i process it through rmats when i specify len as 125.&#xD;&#xA; 3. why is no histogram being produced when using picard insertsize metrics. &#xD;&#xA; 4. is normal to see the graph shift to the right after trimming?&#xD;&#xA;&#xD;&#xA;#insertsize for all samples (data from picard collectinsertsize complied usingmultiqc)[![enter image description here][5]][5]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/IsVW7.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/fPONe.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/3nY5A.png&#xD;&#xA;  [4]: https://i.stack.imgur.com/lgeHk.png&#xD;&#xA;  [5]: https://i.stack.imgur.com/l3A9I.png" />
  <row Id="4516" PostHistoryTypeId="2" PostId="2120" RevisionGUID="cba95901-d51e-4819-92c2-b2ffca78e038" CreationDate="2017-07-21T22:19:44.113" UserId="77" Text="1. It's not an unreasonable approach. rMATs is rather picky about its input, but it seems you've noticed that.&#xD;&#xA;2. rMATs can't handle trimmed reads. It also can't handle soft-clipped reads. You might as well not trim. You'll get a lower alignment rate, but your only other choice would be to either exclude the trimmed reads or trim everything down to the same length.&#xD;&#xA;3. You have a mixture of RF and FR alignments and the plotting script can't handle that.&#xD;&#xA;4. Assuming you mean the read length distribution, it's actually shifted to the left, not the right. You had a single length before and a long left tail after trimming." />
  <row Id="4517" PostHistoryTypeId="2" PostId="2121" RevisionGUID="d2dd6ba1-c85a-41b3-a3bc-8a6fbabeec45" CreationDate="2017-07-22T10:53:27.133" UserId="73" Text="A mixture of `apply`, `paste` and `cat` will do this:&#xD;&#xA;&#xD;&#xA;    &gt; data.mat&#xD;&#xA;           sample1 sample2 sample3 sample4&#xD;&#xA;    fliI         1       1       1       1&#xD;&#xA;    patB_1       1       1       1       1&#xD;&#xA;    pgpA         1       1       1       1&#xD;&#xA;    osmB         1       1       1       1&#xD;&#xA;    cspA         1       0       1       1&#xD;&#xA;    &gt; data.conv &lt;- apply(data.mat,2,paste,collapse=&quot;&quot;) # aggregate along 2nd dim&#xD;&#xA;    &gt; cat(paste0(&quot;&gt;&quot;, names(data.conv), &quot;\n&quot;, data.conv), sep=&quot;\n&quot;)&#xD;&#xA;    &gt;sample1&#xD;&#xA;    11111&#xD;&#xA;    &gt;sample2&#xD;&#xA;    11110&#xD;&#xA;    &gt;sample3&#xD;&#xA;    11111&#xD;&#xA;    &gt;sample4&#xD;&#xA;    11111&#xD;&#xA;&#xD;&#xA;If writing to a file is required, just add a `file` parameter to the `cat` command:&#xD;&#xA;&#xD;&#xA;    cat(paste0(&quot;&gt;&quot;, names(data.conv), &quot;\n&quot;, data.conv), &#xD;&#xA;        sep=&quot;\n&quot;, file=&quot;out_fastaLike.txt&quot;)" />
  <row Id="4518" PostHistoryTypeId="5" PostId="2076" RevisionGUID="527961ad-dc6e-43d2-b9bf-2befe4fc4a47" CreationDate="2017-07-22T11:05:35.870" UserId="73" Comment="added 229 characters in body" Text="I don't know of any transcript-to-transcript aligners that are able to do this, but [LAST](http://last.cbrc.jp/doc/lastal.html) can align transcript queries to protein reference sequences using a specified frameshift cost. Here's the specific documentation for that option:&#xD;&#xA;&#xD;&#xA;&gt; -F COST 	&#xD;&#xA;&gt; &#xD;&#xA;&gt; Align DNA queries to protein reference sequences, using the specified&#xD;&#xA;&gt; frameshift cost. A value of 15 seems to be reasonable. (As a special&#xD;&#xA;&gt; case, -F0 means DNA-versus-protein alignment without frameshifts,&#xD;&#xA;&gt; which is faster.) The output looks like this:&#xD;&#xA;&gt; &#xD;&#xA;&gt;     a score=108 s prot 2  40 + 649&#xD;&#xA;&gt;     FLLQAVKLQDP-STPHQIVPSP-VSDLIATHTLCPRMKYQDD s dna  8 117 + 999&#xD;&#xA;&gt;     FFLQ-IKLWDP\STPH*IVSSP/PSDLISAHTLCPRMKSQDN&#xD;&#xA;&gt; &#xD;&#xA;&gt; The \ indicates a forward shift by one nucleotide, and the / indicates&#xD;&#xA;&gt; a reverse shift by one nucleotide. The * indicates a stop codon. The&#xD;&#xA;&gt; same alignment in tabular format looks like this:&#xD;&#xA;&gt; &#xD;&#xA;&gt;     108 prot 2 40 + 649 dna 8 117 + 999 4,1:0,6,0:1,10,0:-1,19&#xD;&#xA;&gt; &#xD;&#xA;&gt; The &quot;-1&quot; indicates the reverse frameshift.&#xD;&#xA;&#xD;&#xA;I sent an email to the [LAST mailing list](https://groups.google.com/forum/#!forum/last-align) about adding a frameshift penalty for transcript-to-transcript matching; I've been pleasantly surprised with the requested features that Martin Frith has added to LAST in the past. Unfortunately, in this case the problem is too difficult to sort out due to all the possible combinations that could happen, so it's unlikely to be implemented in LAST in the forseeable future (unless someone else writes that code)." />
  <row Id="4519" PostHistoryTypeId="50" PostId="802" RevisionGUID="7d8c17d3-7e03-4546-ae94-22ebbf5193f0" CreationDate="2017-07-22T20:41:24.463" UserId="-1" />
  <row Id="4528" PostHistoryTypeId="2" PostId="2124" RevisionGUID="89e44415-e6f6-4d3c-aa3e-928893ee5ce7" CreationDate="2017-07-24T15:56:04.047" UserId="203" Text="From the same source as you reference yourself [v 3.3][1], I would argue that &#xD;&#xA;`REMARK 0` would be more accurate than `REMARK 250`.&#xD;&#xA;&#xD;&#xA;&gt; REMARK 0 (updated), Re-refinement notice&#xD;&#xA;&gt; REMARK 0 identifies entries in which a re-refinement has been performed using the data from an existing entry. This remark also describes the PDB code and the journal records for the original data set.&#xD;&#xA;&#xD;&#xA;While when you consider the definition of `REMARK 250`&#xD;&#xA;&#xD;&#xA;-- which follows directly:&#xD;&#xA;&#xD;&#xA; - `REMARK 205` (specific to Fiber diffraction experiment), &#xD;&#xA; - `REMARK 201`(and `REMARK 215/217` all specific to NMR experiment),&#xD;&#xA; - `REMARK 230` (neutron diffraction study), &#xD;&#xA; - `REMARK 240`(electron crystallography study) and &#xD;&#xA; - `REMARK 245`(and `REMARK 247`: specific to EM study)&#xD;&#xA;&#xD;&#xA;&gt;REMARK 250, Other Type of Experiment Details&#xD;&#xA;&#xD;&#xA;&gt;REMARKs specific to other kinds of studies, not listed above. &#xD;&#xA;REMARK 250 is mandatory if other than X-ray, NMR, neutron, or electron study. The format of the date in this remark is DD-MMM-YY. DD is the day of the month (a number 01 through 31), MMM is the English 3-letter abbreviation for the month, and YY is the year.&#xD;&#xA;&#xD;&#xA;and then `REMARK 265` is also about the crystallography experiment.&#xD;&#xA;&#xD;&#xA;Hence, I know that personally, I usually assume that whatever is annotated in `REMARK 250`is relevant to the first acquisition experiment to me.&#xD;&#xA;&#xD;&#xA;Another reason I am more inclined to think `REMARK 0`is more accurate, it is due that the definition of `REMARK 0` was __updated__ while `REMARK 6-99` are _are no longer for use of free text annotation_.&#xD;&#xA;&#xD;&#xA;BTW, I am not sure how [Pdb_extract][2] would do with the remarks when converting the PDB file to CIF files. It would probably worth perusing  [PDB to PDBx/mmCIF Data Item Correspondences][3] page.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.wwpdb.org/documentation/file-format-content/format33/remarks1.html &quot;v 3.3&quot;&#xD;&#xA;  [2]: http://pdb-extract.wwpdb.org/ &quot;Pdb_extract&quot;&#xD;&#xA;  [3]: http://mmcif.wwpdb.org/docs/pdb_to_pdbx_correspondences.html#XPLOR &quot;PDB to PDBx/mmCIF Data Item Correspondences&quot;" />
  <row Id="4529" PostHistoryTypeId="5" PostId="2124" RevisionGUID="a17833e2-1b04-4d1a-a9c9-7340c8590b56" CreationDate="2017-07-24T16:06:09.670" UserId="203" Comment="added 504 characters in body" Text="**EDIT:** &#xD;&#xA;&#xD;&#xA;Actually, after having a more thorough look into the documentation; `REMARK 3` might be the more relevant to what you need. Other tools &#xD;&#xA;&gt;REMARK 3 presents information on refinement program(s) used and related statistics. For non-diffraction studies, REMARK 3 is used to describe any refinement done, but its format is mostly free text.&#xD;&#xA;&#xD;&#xA;[More on `Remark 3`][4]&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;From the same source as you reference yourself [v 3.3][1], I would argue that &#xD;&#xA;`REMARK 0` would be more accurate than `REMARK 250`.&#xD;&#xA;&#xD;&#xA;&gt; REMARK 0 (updated), Re-refinement notice&#xD;&#xA;&gt; REMARK 0 identifies entries in which a re-refinement has been performed using the data from an existing entry. This remark also describes the PDB code and the journal records for the original data set.&#xD;&#xA;&#xD;&#xA;While when you consider the definition of `REMARK 250`&#xD;&#xA;&#xD;&#xA;-- which follows directly:&#xD;&#xA;&#xD;&#xA; - `REMARK 205` (specific to Fiber diffraction experiment), &#xD;&#xA; - `REMARK 201`(and `REMARK 215/217` all specific to NMR experiment),&#xD;&#xA; - `REMARK 230` (neutron diffraction study), &#xD;&#xA; - `REMARK 240`(electron crystallography study) and &#xD;&#xA; - `REMARK 245`(and `REMARK 247`: specific to EM study)&#xD;&#xA;&#xD;&#xA;&gt;REMARK 250, Other Type of Experiment Details&#xD;&#xA;&#xD;&#xA;&gt;REMARKs specific to other kinds of studies, not listed above. &#xD;&#xA;REMARK 250 is mandatory if other than X-ray, NMR, neutron, or electron study. The format of the date in this remark is DD-MMM-YY. DD is the day of the month (a number 01 through 31), MMM is the English 3-letter abbreviation for the month, and YY is the year.&#xD;&#xA;&#xD;&#xA;and then `REMARK 265` is also about the crystallography experiment.&#xD;&#xA;&#xD;&#xA;Hence, I know that personally, I usually assume that whatever is annotated in `REMARK 250`is relevant to the first acquisition experiment to me.&#xD;&#xA;&#xD;&#xA;Another reason I am more inclined to think `REMARK 0`is more accurate, it is due that the definition of `REMARK 0` was __updated__ while `REMARK 6-99` are _are no longer for use of free text annotation_.&#xD;&#xA;&#xD;&#xA;BTW, I am not sure how [Pdb_extract][2] would do with the remarks when converting the PDB file to CIF files. It would probably worth perusing  [PDB to PDBx/mmCIF Data Item Correspondences][3] page.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.wwpdb.org/documentation/file-format-content/format33/remarks1.html &quot;v 3.3&quot;&#xD;&#xA;  [2]: http://pdb-extract.wwpdb.org/ &quot;Pdb_extract&quot;&#xD;&#xA;  [3]: http://mmcif.wwpdb.org/docs/pdb_to_pdbx_correspondences.html#XPLOR &quot;PDB to PDBx/mmCIF Data Item Correspondences&quot;&#xD;&#xA;  [4]: http://www.wwpdb.org/documentation/file-format-content/format33/remark3.html &quot;More on `Remark 3`&quot;" />
  <row Id="4530" PostHistoryTypeId="5" PostId="2119" RevisionGUID="5726b42b-3fc2-4015-9b3e-793050005657" CreationDate="2017-07-24T16:27:41.447" UserId="29" Comment="formatting" Text="I have a problem here with my rna seq data:&#xD;&#xA;&#xD;&#xA;#Sequencing details:&#xD;&#xA;rRNA was removed, followed by cDNA preparation and generation of stranded libraries using the TruSeq Stranded Total RNA Sample Prep Kit. Sequencing performed on the HiSeq2500 platform (Illumina) to generate 2 × 125 bp paired-end reads&#xD;&#xA;&#xD;&#xA;#Alignment and preprocessing &#xD;&#xA;reads were aligned using tophat2(std aligner in pipeline at core (`--library-type fr-firststrand`), unfortunately the original unaligned files were purged and i only have access to this aligned file which i converted to fastq using following steps&#xD;&#xA;&#xD;&#xA; 1. `samtools sort -n sample.bam -o sample_sorted.bam`&#xD;&#xA; 2. `bedtools bamtofastq -i sample_sorted.bam -fq sample_1.fq -fq2 sample_2.fq` (get a lot of mate skipping errors here)&#xD;&#xA; 3. check for these reads for adapters and trim them and again use hisat2 (with `--rna-strandness RF`)&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;#trimmed reads&#xD;&#xA;Reads were trimmed by passing parameters `--adapters adapters.file --adapter-trim-end RIGHT --length-dist --threads 12 --adapter-min-overlap 7 --max-uncalled 250 --min-read-length 25 --` to FLEXBAR version 2.4&#xD;&#xA;&#xD;&#xA;##adapters for all samples using multiqc&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;##fastqc sequence length pre trimming&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;##fastqc sequence length post trimming &#xD;&#xA;[![enter image description here][3]][3]&#xD;&#xA;##rmats error&#xD;&#xA;&gt; Incorrect readLength. sample.bam has a read length of 114, while readLength param is 125&#xD;&#xA;&#xD;&#xA;##picard error on aligned bam from hisat2)&#xD;&#xA;&gt; ProcessExecutor	[2] &quot;Not creating insert size PDF as there are duplicated header names: All_Reads&quot;&#xD;&#xA;&#xD;&#xA;##snapshot of picard insertsize metrics file&#xD;&#xA;[![enter image description here][4]][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;#Questions&#xD;&#xA;&#xD;&#xA; 1. Is my approach flawed at some step?&#xD;&#xA; 2. why do i get different read lengths error for each sample when i process it through rmats when i specify len as 125?&#xD;&#xA; 3. why is no histogram being produced when using picard insertsize metrics?&#xD;&#xA; 4. is normal to see the graph shift to the right after trimming?&#xD;&#xA;&#xD;&#xA;#insertsize for all samples (data from picard collectinsertsize complied usingmultiqc)&#xD;&#xA;[![enter image description here][5]][5]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/IsVW7.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/fPONe.png&#xD;&#xA;  [3]: https://i.stack.imgur.com/3nY5A.png&#xD;&#xA;  [4]: https://i.stack.imgur.com/lgeHk.png&#xD;&#xA;  [5]: https://i.stack.imgur.com/l3A9I.png" />
  <row Id="4531" PostHistoryTypeId="2" PostId="2125" RevisionGUID="77e20438-5a22-40da-8cba-ddc6eaa28532" CreationDate="2017-07-24T19:52:43.587" UserId="1183" Text="Hm, I didn't know that the plugin existed so I wrote my own script to convert GP to minor allele dosage on github. Maybe someone else will find it useful :) https://github.com/7methylg/VCF-GP-to-DS" />
  <row Id="4542" PostHistoryTypeId="34" PostId="2067" RevisionGUID="947bcf96-7cbd-4d85-83a2-7d22f7cdf80d" CreationDate="2017-07-25T09:32:42.357" UserId="-1" Comment="3" />
  <row Id="4543" PostHistoryTypeId="2" PostId="2128" RevisionGUID="39b1a07a-bb0a-4d01-b112-5ecf78b71486" CreationDate="2017-07-25T09:34:43.760" UserId="1193" Text="I would like to gather proteins FASTA sequence from Entrez with python 2.7. I am looking for any proteins that have the keywords: &quot;terminase&quot; and &quot;large&quot; in their name. So far I got this code:&#xD;&#xA;&#xD;&#xA;    from Bio import Entrez&#xD;&#xA;    Entrez.email = &quot;example@example.org&quot;&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=&quot;terminase large&quot;, retmax=1000)&#xD;&#xA;    searchResult = Entrez.read(searchResultHandle)&#xD;&#xA;    ids = searchResult[&quot;IdList&quot;]&#xD;&#xA;    &#xD;&#xA;    handle = Entrez.efetch(db=&quot;protein&quot;, id=ids, rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xD;&#xA;    record = handle.read()&#xD;&#xA;    &#xD;&#xA;    out_handle = open('myfasta.fasta', 'w')&#xD;&#xA;    out_handle.write(record.rstrip('\n'))&#xD;&#xA;&#xD;&#xA;However, it can get me several terminases from various organisms, while I need only terminase form bacteriophages (specificly Viruses [taxid 10239], host: bacteria). I've managed to get the nuccore accession ids from NCBI of the viruses I am intersted in, but I don't know how to combine those two informations. The id file looks like this:&#xD;&#xA;&#xD;&#xA;    NC_001341&#xD;&#xA;    NC_001447&#xD;&#xA;    NC_028834&#xD;&#xA;    NC_023556&#xD;&#xA;    ...&#xD;&#xA;    &#xD;&#xA;Do I need to access every gb file of every ID and search for my desired protein in it?" />
  <row Id="4544" PostHistoryTypeId="1" PostId="2128" RevisionGUID="39b1a07a-bb0a-4d01-b112-5ecf78b71486" CreationDate="2017-07-25T09:34:43.760" UserId="1193" Text="Getting protein FASTA sequence based on keyword with python" />
  <row Id="4545" PostHistoryTypeId="3" PostId="2128" RevisionGUID="39b1a07a-bb0a-4d01-b112-5ecf78b71486" CreationDate="2017-07-25T09:34:43.760" UserId="1193" Text="&lt;biopython&gt;&lt;python&gt;" />
  <row Id="4549" PostHistoryTypeId="2" PostId="2130" RevisionGUID="8aaa23d0-ab3f-45a8-93f7-1d1e009583eb" CreationDate="2017-07-25T11:11:26.397" UserId="1191" Text="Can anyone explain me, why I don't find a specific protein with a blast that was took before from the NCBI refseq database?&#xD;&#xA;&#xD;&#xA;Specifically, I was trying to blast the protein with the accession number &quot;NP_420767&quot; and its sequence, respectively, however that protein does not show up in the results. It not only happens when the standard options are chosen, but also, when &quot;Reference proteins (refseq_protein)&quot; as database in the blast options is selected.&#xD;&#xA;&#xD;&#xA;I am really puzzled by that.. Shouldn't blast show the initial refseq entry in the results list as it is part of the refseq database and has the same sequence?" />
  <row Id="4550" PostHistoryTypeId="1" PostId="2130" RevisionGUID="8aaa23d0-ab3f-45a8-93f7-1d1e009583eb" CreationDate="2017-07-25T11:11:26.397" UserId="1191" Text="blasting a refseq protein does not show the protein in the result set" />
  <row Id="4551" PostHistoryTypeId="3" PostId="2130" RevisionGUID="8aaa23d0-ab3f-45a8-93f7-1d1e009583eb" CreationDate="2017-07-25T11:11:26.397" UserId="1191" Text="&lt;blast&gt;&lt;refseq&gt;" />
  <row Id="4552" PostHistoryTypeId="2" PostId="2131" RevisionGUID="24720c2d-f354-4643-8225-ead8e739e098" CreationDate="2017-07-25T11:26:33.690" UserId="1194" Text="&#xD;&#xA;&#xD;&#xA;Is there a way to filter bases in BAM files based on phred quallities through python's pysam ?&#xD;&#xA;&#xD;&#xA;I have a code here that&#xD;&#xA;&#xD;&#xA;   &#xD;&#xA;&#xD;&#xA; - Takes the nucleobases per position from a BAM file using pysam's pileup function&#xD;&#xA; - Saves it in ReverseList and ForwardList based on both strands (i.e forward and reverse), I want to reject those bases that have phred quality below 25 to be not stored in the ForwardList and ReverseList lists so that they are not used for further analysis.&#xD;&#xA;&#xD;&#xA;         samfile = pysam.Samfile( filename, &quot;rb&quot; )&#xD;&#xA;         ReverseList = [''] * lenref&#xD;&#xA;         ForwardList = [''] * lenref&#xD;&#xA;         for pileupcolumn in samfile.pileup() :&#xD;&#xA;             for pileupread in pileupcolumn.pileups:&#xD;&#xA;                 if (pileupread.alignment.mapping_quality &lt;= 15):&#xD;&#xA;                       continue      &#xD;&#xA;                 if not pileupread.is_del and not pileupread.is_refskip:&#xD;&#xA;                       if pileupread.alignment.is_reverse: #negative&#xD;&#xA;                              ReverseList[pileupcolumn.pos] += pileupread.alignment.query_sequence[pileupread.query_position]&#xD;&#xA;                       else:&#xD;&#xA;                              ForwardList[pileupcolumn.pos] += pileupread.alignment.query_sequence[pileupread.query_position]&#xD;&#xA;&#xD;&#xA;         samfile.close()&#xD;&#xA;&#xD;&#xA;Where lenref = 16569(length of mitochondrial genome) and filename is the name of BAM file. I want to filter based of phred qualities of bases.&#xD;&#xA;" />
  <row Id="4553" PostHistoryTypeId="1" PostId="2131" RevisionGUID="24720c2d-f354-4643-8225-ead8e739e098" CreationDate="2017-07-25T11:26:33.690" UserId="1194" Text="Filtering bases based on phred qualities with pysam" />
  <row Id="4554" PostHistoryTypeId="3" PostId="2131" RevisionGUID="24720c2d-f354-4643-8225-ead8e739e098" CreationDate="2017-07-25T11:26:33.690" UserId="1194" Text="&lt;bam&gt;&lt;python&gt;&lt;pysam&gt;" />
  <row Id="4555" PostHistoryTypeId="2" PostId="2132" RevisionGUID="3618be85-343c-4d53-be7f-4befc0b24b2e" CreationDate="2017-07-25T11:27:52.257" UserId="581" Text="NP_420767 is represented by the non-redundant refSeq protein WP_010919826, which has the same amino acid sequence. This is not very clearly annotated, but if you scroll down to the sequence in the GenPept entry for NP_420767, you'll see the following:&#xD;&#xA;&#xD;&#xA;    CONTIG      join(WP_010919826.1:1..799)&#xD;&#xA;&#xD;&#xA;See [here](https://www.ncbi.nlm.nih.gov/refseq/about/nonredundantproteins) for more info." />
  <row Id="4556" PostHistoryTypeId="10" PostId="2131" RevisionGUID="3aa0767b-80fa-4d57-9589-1f22c0d5b13e" CreationDate="2017-07-25T11:31:57.043" UserId="77" Comment="101" Text="{&quot;OriginalQuestionIds&quot;:[2127],&quot;Voters&quot;:[{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;}]}" />
  <row Id="4557" PostHistoryTypeId="11" PostId="2131" RevisionGUID="0ca37491-2b8b-436e-8e79-ec80017f126a" CreationDate="2017-07-25T11:42:05.820" UserId="77" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:77,&quot;DisplayName&quot;:&quot;Devon Ryan&quot;}]}" />
  <row Id="4559" PostHistoryTypeId="2" PostId="2133" RevisionGUID="d387e486-a8f2-4fc3-8027-483670cd1fd9" CreationDate="2017-07-25T11:42:57.597" UserId="77" Text="&#xD;&#xA;&#xD;&#xA;The PileupRead object as a query_position attribute, which you can use for this:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    for pileupcolumn in samfile.pileup() :&#xD;&#xA;        for pileupread in pileupcolumn.pileups:&#xD;&#xA;             if (pileupread.alignment.mapping_quality &lt;= 15):&#xD;&#xA;                 continue      &#xD;&#xA;             if not pileupread.is_del and not pileupread.is_refskip:&#xD;&#xA;                 if pileupread.alignment.query_qualities[pileupread.query_position] &lt; 10:&#xD;&#xA;                     # Skip entries with base phred scores &lt; 10&#xD;&#xA;                     continue&#xD;&#xA;                 if pileupread.alignment.is_reverse: #negative&#xD;&#xA;                       ReverseList[pileupcolumn.pos] += pileupread.alignment.query_sequence[pileupread.query_position]&#xD;&#xA;                 else:&#xD;&#xA;                       ForwardList[pileupcolumn.pos] += pileupread.alignment.query_sequence[pileupread.query_position]&#xD;&#xA;&#xD;&#xA;Note the 6th line, which implements a filter with a threshold of 10.&#xD;&#xA;" />
  <row Id="4560" PostHistoryTypeId="2" PostId="2134" RevisionGUID="3d0427ef-6af4-4792-9f87-1cdcf38f3188" CreationDate="2017-07-25T11:43:31.083" UserId="235" Text="I don't know of any prebuilt products, but I can describe how we managed this in my postdoc lab, and how I plan to manage it in my newly-started group. &#xD;&#xA;&#xD;&#xA;Rule 1: All work happens in the projects project directory on the central &#xD;&#xA;filestore, not on your desktop or laptop.&#xD;&#xA;&#xD;&#xA;Rule 2: Heavy computational work is done by the groups standard analysis pipelines. Interpretation is done in jupyter notebooks or Rmarkdown.&#xD;&#xA;&#xD;&#xA;A project has a directory on the group's filestore. That directory has a fixed structure:&#xD;&#xA;&#xD;&#xA;    proj001----raw_data             *&#xD;&#xA;           |&#xD;&#xA;            --external_datasets&#xD;&#xA;           |&#xD;&#xA;            --src                  *&#xD;&#xA;           |&#xD;&#xA;            --notebooks            *&#xD;&#xA;           |&#xD;&#xA;            --pipeline1&#xD;&#xA;           |&#xD;&#xA;            --pipeline2&#xD;&#xA;           | &#xD;&#xA;            -- etc&#xD;&#xA;     * link to a seperate, backed up filesystem.&#xD;&#xA;&#xD;&#xA;Pipelines are where heavy duty analysis happens and everyone uses the same standard pipelines, producing the same standard output files and database structure. &#xD;&#xA;&#xD;&#xA;So common pipelines might be the mapping pipeline, the readqc pipeline, the differential expression pipeline, the exome pipeline, the chip-seq pipeline etc...&#xD;&#xA;&#xD;&#xA;Pipelines have three important outputs: an automatically generated html report, an sqlite database and files in the export directory. &#xD;&#xA;&#xD;&#xA;When we archive a project, this is what we save, along with the pipeline's configuration file and log file. &#xD;&#xA;&#xD;&#xA;So if I know that Jane Bloggs did an RNAseq 5 years ago on a cell type i'm interested in, if I konw that that was project 5, I know that in the project 5 directory there will be a `diff_expression_pipeline` directory, and that it will contain an sqlite database called `csvdb` and that that will have a table called `refcoding_deseq_gene_diff` and that table will follow a known format. There will be bigwigs in the `export` directory. Or the BAMs will be in the `mapping_pipeline` directory. &#xD;&#xA;&#xD;&#xA;Of course the problem remains knowing that Jane Bloggs did this RNAseq and what the project was called. We use a Wiki for this, but its not ideal. " />
  <row Id="4562" PostHistoryTypeId="2" PostId="2135" RevisionGUID="282d3c16-fce5-4baf-a45c-e5f78018758c" CreationDate="2017-07-25T12:28:13.403" UserId="1193" Text="Found what I was looking for. In:&#xD;&#xA;&#xD;&#xA;    searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=&quot;terminase large&quot;, retmax=1000)&#xD;&#xA;&#xD;&#xA;I've added:&#xD;&#xA;&#xD;&#xA;    searchterm = &quot;(terminase large subunit AND viruses[Organism]) AND Caudovirales AND refseq[Filter]&quot;&#xD;&#xA;    searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=searchterm, retmax=6000)&#xD;&#xA;&#xD;&#xA;which norrowed down my searches to the desired viruses. Granted it's not filtered by host, but by a taxonomy group, but it is enough for my work.&#xD;&#xA;&#xD;&#xA;Thank you @Llopis for additional help" />
  <row Id="4563" PostHistoryTypeId="2" PostId="2136" RevisionGUID="48ffcc0a-0245-4204-9201-a2a74610093b" CreationDate="2017-07-25T13:07:46.147" UserId="1087" Text="I'm not sure what kinds of bioinformatics tasks you would like to perform, therefore it is difficult to give a good recommendation.&#xD;&#xA;&#xD;&#xA;If you're specifically working on statistical genetics, I can recommend [Hail](https://hail.is) [1]. Hail is an open-source tool for performing analyzing genetics data at the tens of terabyte scale. Most of Hail's users do their science in Jupyter notebooks that are backed by Google Cloud Platform Dataproc clusters. Hail permits you to perform a variety of statistical genetics tasks including:&#xD;&#xA;&#xD;&#xA; - filtering and aggregation for quality control&#xD;&#xA; - subsetting, linear regression, linear mixed model regression, and linear burden testing&#xD;&#xA; - utilities for computing various measures of relatedness&#xD;&#xA; - principal components analysis&#xD;&#xA; - variant splitting&#xD;&#xA; - import/export from a variety of formats including PLINK, VCF, and BGEN, and&#xD;&#xA; - a python API which enables the use of libraries like matplotlib for plotting analysis results&#xD;&#xA;&#xD;&#xA;To learn specifically about using Hail with the Google Cloud Platform and Jupyter notebooks, I strongly recommend [Liam's Hail forum post about his cloud-tools repository](http://discuss.hail.is/t/using-hail-with-jupyter-notebooks-on-google-cloud/196/2).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[1] Disclaimer: I work on Hail" />
  <row Id="4566" PostHistoryTypeId="5" PostId="2136" RevisionGUID="c255d9c5-cf1c-4cb2-86c3-82b3f68dc3cc" CreationDate="2017-07-25T15:20:14.457" UserId="1087" Comment="added example" Text="I'm not sure what kinds of bioinformatics tasks you would like to perform, therefore it is difficult to give a good recommendation.&#xD;&#xA;&#xD;&#xA;If you're specifically working on statistical genetics, I can recommend [Hail](https://hail.is) [1]. Hail is an open-source tool for performing analyzing genetics data at the tens of terabyte scale. Most of Hail's users do their science in Jupyter notebooks that are backed by Google Cloud Platform Dataproc clusters. Hail permits you to perform a variety of statistical genetics tasks including:&#xD;&#xA;&#xD;&#xA; - filtering and aggregation for quality control&#xD;&#xA; - subsetting, linear regression, linear mixed model regression, and linear burden testing&#xD;&#xA; - utilities for computing various measures of relatedness&#xD;&#xA; - principal components analysis&#xD;&#xA; - variant splitting&#xD;&#xA; - import/export from a variety of formats including PLINK, VCF, and BGEN, and&#xD;&#xA; - a python API which enables the use of libraries like matplotlib for plotting analysis results&#xD;&#xA;&#xD;&#xA;To learn specifically about using Hail with the Google Cloud Platform and Jupyter notebooks, I strongly recommend [Liam's Hail forum post about his cloud-tools repository](http://discuss.hail.is/t/using-hail-with-jupyter-notebooks-on-google-cloud/196/2).&#xD;&#xA;&#xD;&#xA;Here's an example, from the [Hail tutorial](https://hail.is/hail/tutorials/hail-overview.html#Quality-Control), of using Hail to perform some quality control and display a scatter plot of the first two principal components of the individuals:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    from hail import *&#xD;&#xA;    import matplotlib.pyplot as plt&#xD;&#xA;    import matplotlib.patches as mpatches&#xD;&#xA;    &#xD;&#xA;    hc = HailContext()&#xD;&#xA;    &#xD;&#xA;    table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample')&#xD;&#xA;    common_vds = (hc.read('data/1kg.vds')&#xD;&#xA;                  .annotate_samples_table(table, root='sa')&#xD;&#xA;                  .sample_qc()&#xD;&#xA;                  .filter_samples_expr('sa.qc.dpMean &gt;= 4 &amp;&amp; sa.qc.callRate &gt;= 0.97')&#xD;&#xA;                  .filter_genotypes('''let ab = g.ad[1] / g.ad.sum() in&#xD;&#xA;                             ((g.isHomRef &amp;&amp; ab &lt;= 0.1) ||&#xD;&#xA;                              (g.isHet &amp;&amp; ab &gt;= 0.25 &amp;&amp; ab &lt;= 0.75) ||&#xD;&#xA;                              (g.isHomVar &amp;&amp; ab &gt;= 0.9))''')&#xD;&#xA;                  .variant_qc()&#xD;&#xA;                  .filter_variants_expr('va.qc.AF &gt; 0.01')&#xD;&#xA;                  .ld_prune(memory_per_core=512, num_cores=4))&#xD;&#xA;    &#xD;&#xA;    pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen')&#xD;&#xA;    pca_table = pca.samples_table().to_pandas()&#xD;&#xA;    &#xD;&#xA;    colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}&#xD;&#xA;    plt.scatter(pca_table[&quot;sa.pca.PC1&quot;], pca_table[&quot;sa.pca.PC2&quot;],&#xD;&#xA;                c = pca_table[&quot;sa.SuperPopulation&quot;].map(colors),&#xD;&#xA;                alpha = .5)&#xD;&#xA;    plt.xlim(-0.6, 0.6)&#xD;&#xA;    plt.xlabel(&quot;PC1&quot;)&#xD;&#xA;    plt.ylabel(&quot;PC2&quot;)&#xD;&#xA;    legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]&#xD;&#xA;    plt.legend(handles=legend_entries, loc=2)&#xD;&#xA;    plt.show()&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[1] Disclaimer: I work on Hail" />
  <row Id="4567" PostHistoryTypeId="2" PostId="2137" RevisionGUID="d6e7ba96-19a4-4d34-b285-f1e207eaf52e" CreationDate="2017-07-25T15:56:54.430" UserId="292" Text="I have a gtf file from Ensembl, and I noticed that several &quot;transcript&quot; annotations have the exact same coordinates. See for instance the third and fourth transcripts (&quot;Y74C9A.2b.1&quot; and &quot;Y74C9A.2b.4&quot;) for this gene:&#xD;&#xA;&#xD;&#xA;    $ grep &quot;WBGene00022276&quot; genes.gtf | awk '$3 == &quot;transcript&quot; {print}'&#xD;&#xA;    I	ensembl	transcript	10413	16842	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P24752&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2a.2&quot;; transcript_name &quot;Y74C9A.2a.2&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS7921&quot;;&#xD;&#xA;    I	ensembl	transcript	11495	16793	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P22572&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2b.2&quot;; transcript_name &quot;Y74C9A.2b.2&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS9220&quot;;&#xD;&#xA;    I	ensembl	transcript	11499	16842	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P22572&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2b.1&quot;; transcript_name &quot;Y74C9A.2b.1&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS9215&quot;;&#xD;&#xA;    I	ensembl	transcript	11499	16842	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P22572&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2b.4&quot;; transcript_name &quot;Y74C9A.2b.4&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS9215&quot;;&#xD;&#xA;    I	ensembl	transcript	11505	16842	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P24752&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2a.3&quot;; transcript_name &quot;Y74C9A.2a.3&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS16820&quot;;&#xD;&#xA;    I	ensembl	transcript	11618	16842	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P24752&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2a.1&quot;; transcript_name &quot;Y74C9A.2a.1&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS43845&quot;;&#xD;&#xA;    I	ensembl	transcript	11623	16842	.	+	.	gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P22572&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2b.3&quot;; transcript_name &quot;Y74C9A.2b.3&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS49800&quot;;&#xD;&#xA;&#xD;&#xA;What does it mean? Do these two &quot;transcripts&quot; actually correspond to an exon, belonging to two different isoforms?" />
  <row Id="4568" PostHistoryTypeId="1" PostId="2137" RevisionGUID="d6e7ba96-19a4-4d34-b285-f1e207eaf52e" CreationDate="2017-07-25T15:56:54.430" UserId="292" Text="Same transcript coordinates in gtf file, different transcript ID" />
  <row Id="4569" PostHistoryTypeId="3" PostId="2137" RevisionGUID="d6e7ba96-19a4-4d34-b285-f1e207eaf52e" CreationDate="2017-07-25T15:56:54.430" UserId="292" Text="&lt;annotation&gt;&lt;gtf&gt;" />
  <row Id="4570" PostHistoryTypeId="2" PostId="2138" RevisionGUID="468b4a36-d642-4fdc-b5cc-531fdd6abb76" CreationDate="2017-07-25T16:48:03.920" UserId="235" Text="`transcript` objects cover the co-ordinates from the start of the first exon to the end of the last exon of a transcript (i.e. an isoform). If two different isoforms share the same first and last exons, but have a different set of internal exons, then their `transcript` entries will be the same, but the set of `exon` entires associated with each transcript will be different.&#xD;&#xA;&#xD;&#xA;For example, consider the two transcripts below:&#xD;&#xA;&#xD;&#xA;                          1         2&#xD;&#xA;                 12345678901234567890123 &#xD;&#xA;    transcript_1 |&gt;&gt;&gt;|----|&gt;&gt;|----|&gt;&gt;&gt;&gt;|&#xD;&#xA;&#xD;&#xA;    transcript_2 |&gt;&gt;|--|&gt;&gt;|------|&gt;&gt;&gt;&gt;&gt;|&#xD;&#xA;&#xD;&#xA;These two transcripts would give the following GTF entires&#xD;&#xA;&#xD;&#xA;    chr1   ensembl   transcript    1   23   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_1&quot;;&#xD;&#xA;    chr1   ensembl   exon    1   5   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_1&quot;;&#xD;&#xA;    chr1   ensembl   exon    10   13   .   +   .   gene_id &quot;gene_1&quot;; gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_1&quot;;   &#xD;&#xA;    chr1   ensembl   exon    18   23   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_1&quot;;   &#xD;&#xA;    chr1   ensembl   transcript    1   23   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_2&quot;;&#xD;&#xA;    chr1   ensembl   exon    1   4   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_2&quot;;&#xD;&#xA;    chr1   ensembl   exon    7   10   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_2&quot;;   &#xD;&#xA;    chr1   ensembl   exon    17   23   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_2&quot;;   " />
  <row Id="4571" PostHistoryTypeId="2" PostId="2139" RevisionGUID="a0f353e5-b0b3-49ff-9ec6-474b3f2a1b86" CreationDate="2017-07-25T17:07:37.253" UserId="298" Text="As Ian explained, these are different transcripts which happen to have the same start and end positions. You have no information on their exonic structure in that file. However, if you look them up at EnsEMBL, you will see:&#xD;&#xA;&#xD;&#xA;[Transcript Y74C9A.2b.1][1] :&#xD;&#xA;&#xD;&#xA;[![exonic structure of Y74C9A.2b.1][2]][2]&#xD;&#xA;&#xD;&#xA;    1	Y74C9A.2b.1.e1	11,499	11,561	-	-	63&#xD;&#xA;     	Intron 1-2	    11,562	11,617			56&#xD;&#xA;    2	Y74C9A.2b.1.e2	11,618	11,689	-	-	72&#xD;&#xA;     	Intron 2-3	    11,690	14,950			3,261&#xD;&#xA;    3	Y74C9A.2b.1.e3	14,951	15,160	-	1	210&#xD;&#xA;     	Intron 3-4	    15,161	16,472			1,312&#xD;&#xA;    4	Y74C9A.2a.1.e3	16,473	16,842	1	-	370&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;This has 4 exons, two of which are non-coding (UTR). The second starts at &#xD;&#xA;&#xD;&#xA;[Transcript Y74C9A.2b.4][3]&#xD;&#xA;&#xD;&#xA;[![exonic structure of Y74C9A.2b.4][4]][4]&#xD;&#xA;&#xD;&#xA;    1	Y74C9A.2b.4.e1	11,499	11,557	-	-	59&#xD;&#xA;     	Intron 1-2	    11,558	11,617			60&#xD;&#xA;    2	Y74C9A.2b.1.e2	11,618	11,689	-	-	72&#xD;&#xA;     	Intron 2-3	    11,690	14,950			3,261&#xD;&#xA;    3	Y74C9A.2b.1.e3	14,951	15,160	-	1	210&#xD;&#xA;     	Intron 3-4	    15,161	16,472			1,312&#xD;&#xA;    4	Y74C9A.2a.1.e3	16,473	16,842	1	-	370&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Granted, the two transcripts are very simlar, they both have 4 exons, the first 2 of which are non coding (UTR). However, the 1st exon of transcript &#xD;&#xA;Y74C9A.2b.1 is 63 nt long and ends at 11,561 while the 1st exon of transcript Y74C9A.2b.4 is 59 nts and ends at 11,557.&#xD;&#xA;&#xD;&#xA;So there you go: two (slightly) different transcripts. &#xD;&#xA;&#xD;&#xA;  [1]: http://May2017.archive.ensembl.org/Caenorhabditis_elegans/Transcript/Summary?db=core;g=WBGene00022276;r=I:11499-16842;t=Y74C9A.2b.1&#xD;&#xA;  [2]: https://i.stack.imgur.com/yYXlV.png&#xD;&#xA;  [3]: http://May2017.archive.ensembl.org/Caenorhabditis_elegans/Transcript/Summary?db=core;g=WBGene00022276;r=I:11499-16842;t=Y74C9A.2b.4&#xD;&#xA;  [4]: https://i.stack.imgur.com/uZbYI.png" />
  <row Id="4572" PostHistoryTypeId="5" PostId="2139" RevisionGUID="93501294-8bbe-4834-be3d-33568749ca2c" CreationDate="2017-07-25T17:14:56.770" UserId="298" Comment="added 67 characters in body" Text="As Ian [explained][1], these are different transcripts which happen to have the same start and end positions. You have no information on their exonic structure in that file. However, if you look them up at EnsEMBL, you will see:&#xD;&#xA;&#xD;&#xA;[Transcript Y74C9A.2b.1][2] :&#xD;&#xA;&#xD;&#xA;[![exonic structure of Y74C9A.2b.1][3]][3]&#xD;&#xA;&#xD;&#xA;    1	Y74C9A.2b.1.e1	11,499	11,561	-	-	63&#xD;&#xA;     	Intron 1-2	    11,562	11,617			56&#xD;&#xA;    2	Y74C9A.2b.1.e2	11,618	11,689	-	-	72&#xD;&#xA;     	Intron 2-3	    11,690	14,950			3,261&#xD;&#xA;    3	Y74C9A.2b.1.e3	14,951	15,160	-	1	210&#xD;&#xA;     	Intron 3-4	    15,161	16,472			1,312&#xD;&#xA;    4	Y74C9A.2a.1.e3	16,473	16,842	1	-	370&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;This has 4 exons, two of which are non-coding (UTR). The second starts at &#xD;&#xA;&#xD;&#xA;[Transcript Y74C9A.2b.4][4]&#xD;&#xA;&#xD;&#xA;[![exonic structure of Y74C9A.2b.4][5]][5]&#xD;&#xA;&#xD;&#xA;    1	Y74C9A.2b.4.e1	11,499	11,557	-	-	59&#xD;&#xA;     	Intron 1-2	    11,558	11,617			60&#xD;&#xA;    2	Y74C9A.2b.1.e2	11,618	11,689	-	-	72&#xD;&#xA;     	Intron 2-3	    11,690	14,950			3,261&#xD;&#xA;    3	Y74C9A.2b.1.e3	14,951	15,160	-	1	210&#xD;&#xA;     	Intron 3-4	    15,161	16,472			1,312&#xD;&#xA;    4	Y74C9A.2a.1.e3	16,473	16,842	1	-	370&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Granted, the two transcripts are very simlar, they both have 4 exons, the first 2 of which are non coding (UTR). However, the 1st exon of transcript &#xD;&#xA;Y74C9A.2b.1 is 63 nt long and ends at 11,561 while the 1st exon of transcript Y74C9A.2b.4 is 59 nts and ends at 11,557.&#xD;&#xA;&#xD;&#xA;So there you go: two (slightly) different transcripts. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/2138/298&#xD;&#xA;  [2]: http://May2017.archive.ensembl.org/Caenorhabditis_elegans/Transcript/Summary?db=core;g=WBGene00022276;r=I:11499-16842;t=Y74C9A.2b.1&#xD;&#xA;  [3]: https://i.stack.imgur.com/yYXlV.png&#xD;&#xA;  [4]: http://May2017.archive.ensembl.org/Caenorhabditis_elegans/Transcript/Summary?db=core;g=WBGene00022276;r=I:11499-16842;t=Y74C9A.2b.4&#xD;&#xA;  [5]: https://i.stack.imgur.com/uZbYI.png" />
  <row Id="4573" PostHistoryTypeId="2" PostId="2140" RevisionGUID="b126bf8f-414b-4631-966f-fa06d28b530b" CreationDate="2017-07-25T17:23:52.373" UserId="926" Text="We have an experimental design as seen below &#xD;&#xA;&#xD;&#xA;    sample	type	time&#xD;&#xA;    WT	Vehicle 	30 min&#xD;&#xA;    KO	Vehicle 	30 min&#xD;&#xA;    WT	Drug	30min&#xD;&#xA;    KO	Drug	30min&#xD;&#xA;    WT	Drug	1 hour&#xD;&#xA;    KO	Drug	1 hour&#xD;&#xA;    WT	Drug	2 hour&#xD;&#xA;    KO	Drug	2hour&#xD;&#xA;    WT	Drug	3 hour&#xD;&#xA;    KO	Drug	3hour&#xD;&#xA;&#xD;&#xA;concentration of drug was the same at each timepoint. We do not have any replicates and hence cannot use any of the differential expression statistical tools (asked this question at bioconductor where Dr. Love himself replied [link to question asked][1]) where he suggested a linear model. &#xD;&#xA;&#xD;&#xA;we were wondering if someone could help us with any such design (papers/previously performed analysis) what we would like to see is comparison/differences (significant) between these groups &#xD;&#xA;(veh vs 30min, veh vs 1 hour, beh vs 2 hours, veh vs 3 hours) &#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.bioconductor.org/p/98429/#98456" />
  <row Id="4574" PostHistoryTypeId="1" PostId="2140" RevisionGUID="b126bf8f-414b-4631-966f-fa06d28b530b" CreationDate="2017-07-25T17:23:52.373" UserId="926" Text="differential gene expression complex design no replicates" />
  <row Id="4575" PostHistoryTypeId="3" PostId="2140" RevisionGUID="b126bf8f-414b-4631-966f-fa06d28b530b" CreationDate="2017-07-25T17:23:52.373" UserId="926" Text="&lt;rna-seq&gt;&lt;deseq2&gt;&lt;differential-expression&gt;" />
  <row Id="4576" PostHistoryTypeId="5" PostId="2140" RevisionGUID="d2fb5ef5-1548-4229-8448-87fef260a5e4" CreationDate="2017-07-25T18:55:38.403" UserId="926" Comment="added 150 characters in body" Text="We have an experimental design as seen below &#xD;&#xA;&#xD;&#xA;Where we administered drug at 0 min for each mouse genotype and took them down at given below intervals. wt and ko mouse models  were administered only with the vehicle which would be our negative control. &#xD;&#xA;&#xD;&#xA;    sample	type	time&#xD;&#xA;    WT	Vehicle 	30 min&#xD;&#xA;    KO	Vehicle 	30 min&#xD;&#xA;    WT	Drug	30min&#xD;&#xA;    KO	Drug	30min&#xD;&#xA;    WT	Drug	1 hour&#xD;&#xA;    KO	Drug	1 hour&#xD;&#xA;    WT	Drug	2 hour&#xD;&#xA;    KO	Drug	2hour&#xD;&#xA;    WT	Drug	3 hour&#xD;&#xA;    KO	Drug	3hour&#xD;&#xA;&#xD;&#xA;We do not have any replicates and hence cannot use any of the differential expression statistical tools (asked this question at bioconductor where Dr. Love himself replied [link to question asked][1]) where he suggested a linear model. &#xD;&#xA;&#xD;&#xA;we were wondering if someone could help us with any such design (papers/previously performed analysis) what we would like to see is comparison/differences (significant) between these groups &#xD;&#xA;(veh vs 30min, veh vs 1 hour, beh vs 2 hours, veh vs 3 hours) &#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.bioconductor.org/p/98429/#98456" />
  <row Id="4577" PostHistoryTypeId="5" PostId="2140" RevisionGUID="87dc26f7-0444-4790-b004-e17c0d6c5ec7" CreationDate="2017-07-25T19:41:07.320" UserId="926" Comment="added 127 characters in body" Text="We have an experimental design as seen below &#xD;&#xA;&#xD;&#xA;Where we administered drug at 0 min for each mouse genotype and took them down at given below intervals. wt and ko mouse models  were administered only with the vehicle which would be our negative control.&#xD;&#xA;&#xD;&#xA;Performed rnaseq (total rna) truseq stranded protocol would like to see differences between each time point compared to the vehicle, used star aligner to align to mm9 , htseq for gene counts . &#xD;&#xA;&#xD;&#xA;    sample	type	time&#xD;&#xA;    WT	Vehicle 	30 min&#xD;&#xA;    KO	Vehicle 	30 min&#xD;&#xA;    WT	Drug	30min&#xD;&#xA;    KO	Drug	30min&#xD;&#xA;    WT	Drug	1 hour&#xD;&#xA;    KO	Drug	1 hour&#xD;&#xA;    WT	Drug	2 hour&#xD;&#xA;    KO	Drug	2hour&#xD;&#xA;    WT	Drug	3 hour&#xD;&#xA;    KO	Drug	3hour&#xD;&#xA;&#xD;&#xA;We do not have any replicates and hence cannot use any of the differential expression statistical tools (asked this question at bioconductor where Dr. Love himself replied [link to question asked][1]) where he suggested a linear model. &#xD;&#xA;&#xD;&#xA;we were wondering if someone could help us with any such design (papers/previously performed analysis) what we would like to see is comparison/differences (significant) between these groups &#xD;&#xA;(veh vs 30min, veh vs 1 hour, beh vs 2 hours, veh vs 3 hours) &#xD;&#xA;&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.bioconductor.org/p/98429/#98456" />
  <row Id="4578" PostHistoryTypeId="2" PostId="2141" RevisionGUID="24786031-fb6d-47cb-bda0-5f36f6260c7b" CreationDate="2017-07-25T22:01:40.587" UserId="824" Text="We have been organizing data on a &quot;per project&quot; basis, using a GitHub repository for each project. Each project ends up being a paper (written in R Markdown), so you could have multiple &quot;projects&quot; based on the same datasets. See this as an example:&#xD;&#xA;&#xD;&#xA;https://github.com/SchlossLab/new_project&#xD;&#xA;&#xD;&#xA;The analyses are all managed with a Makefile that goes from loading the sequence data from the SRA, to performing analyses, to rendering the final manuscript. This way it is always clear how things were done. Here is an example of what that can look like:&#xD;&#xA;&#xD;&#xA;https://github.com/SchlossLab/new_project/blob/master/Makefile&#xD;&#xA;&#xD;&#xA;As far as organizing the raw data and whatnot, I have just used a simple MySQL database hosted on MAMP (Mac version, Linux is LAMP, Windows is WAMP). It has a GUI and everything, and is pretty easy to use. Here is a resource for getting started on that:&#xD;&#xA;&#xD;&#xA;http://microbiology.github.io/PDFs/MySQL-Intro-Workshop-Hannigan.pdf&#xD;&#xA;&#xD;&#xA;All of this stuff is free to use, which is awesome! Hope this helps! :)" />
  <row Id="4579" PostHistoryTypeId="2" PostId="2142" RevisionGUID="e17f7fd0-cbcd-4685-abd5-8139b1ab0be4" CreationDate="2017-07-26T01:44:09.730" UserId="1196" Text="I am doing a research project involving calculating k-mer frequencies and I am wondering if there is any standard file format for storing k-mer counts." />
  <row Id="4580" PostHistoryTypeId="1" PostId="2142" RevisionGUID="e17f7fd0-cbcd-4685-abd5-8139b1ab0be4" CreationDate="2017-07-26T01:44:09.730" UserId="1196" Text="Is there a standard k-mer count file format?" />
  <row Id="4581" PostHistoryTypeId="3" PostId="2142" RevisionGUID="e17f7fd0-cbcd-4685-abd5-8139b1ab0be4" CreationDate="2017-07-26T01:44:09.730" UserId="1196" Text="&lt;file-formats&gt;&lt;k-mer&gt;&lt;dna&gt;" />
  <row Id="4582" PostHistoryTypeId="2" PostId="2143" RevisionGUID="01f44ad5-0bef-42ad-9653-82657057b2e6" CreationDate="2017-07-26T05:13:34.800" UserId="73" Text="Not as far as I am aware. The Ray assembler used to (and possibly still does) store the kmers as FASTA files where the header was the count of the sequence, which I thought was a pretty neat bastardisation of the FASTA file format.&#xD;&#xA;&#xD;&#xA;I think Jellyfish changed their format between v1 and v2, because they changed to doing counts based on bloom filters." />
  <row Id="4583" PostHistoryTypeId="2" PostId="2144" RevisionGUID="8319ddbd-265c-4c88-82c9-1b92113c52cd" CreationDate="2017-07-26T07:38:16.753" UserId="939" Text="Although it is not recommended to use no replicates, in the edgeR manual they give some advice on how to go on with no replicates design. See page 21 of user [guide][1]. You can e.g. estimate a BCV value. Of course it is still a trick and not sound statistics.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf" />
  <row Id="4584" PostHistoryTypeId="2" PostId="2145" RevisionGUID="a60bf00f-8f2b-4b33-9dda-5408fdb03cfc" CreationDate="2017-07-26T09:40:18.733" UserId="1021" Text="Low coverage MinION reads should be useful to close gaps and resolve repeats left by short-read assemblers. However, I haven't had any success with the software I know about. I'm aware of the following packages, either for scaffolding or closing gaps in short-read assemblies using long reads:&#xD;&#xA;&#xD;&#xA;+ [`LINKS`][5]&#xD;&#xA;+ [`OPERA-LG`][2]&#xD;&#xA;+ [`npScarf`][4]&#xD;&#xA;+ [`PBJelly`][1]&#xD;&#xA;+ [`SSPACE-longread`][3]&#xD;&#xA;&#xD;&#xA;I've tried `npScarf` and `LINKS`, and they ran successfully but didn't resolve the gaps in my assembly. I couldn't get `PBJelly` and `OPERA-LG` to run with the current versions of `BLASR` and `samtools`, and both packages seem not to be maintained. I have not tried `SSPACE-longread` because it's not open source.&#xD;&#xA;&#xD;&#xA;**What software can I use to fill gaps and resolve repeats in a short-read assembly with low-coverage Nanopore data?**&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;### More information&#xD;&#xA;&#xD;&#xA;I'm finishing a mitochondrial genome. I have a short read assembly of 17 kb with one gap, but molecular data suggests the genome may be around 32 kb.&#xD;&#xA;&#xD;&#xA;After mapping short reads against the short read assembly, `Pilon` identified a tandem repeat of 156 bp in an AT-rich region, but wasn't able to close the gap.&#xD;&#xA;&#xD;&#xA;I have 1.4 Gb of MinION rapid reads with an L50 of 8808 b. I mapped these reads against the short read assembly, and I can see reads that span the gap. I seem to have the information required to close the gap, but I don't know how to do it.&#xD;&#xA;&#xD;&#xA;The organism has a nuclear genome &gt; 600 Mb, so long read coverage is low. Despite this, I tried a *de novo* `Canu` assembly of the Nanopore reads, and I pulled out a 39 kb contig containing mitochondrial genes. Most genes on this contig are duplicated and fragmented, and `Pilon` wasn't able to improve it. &#xD;&#xA;&#xD;&#xA;Thanks for reading!&#xD;&#xA;&#xD;&#xA;  [1]: https://sourceforge.net/projects/pb-jelly&#xD;&#xA;  [2]: https://sourceforge.net/projects/operasf&#xD;&#xA;  [3]: https://www.baseclear.com/genomics/bioinformatics/basetools/SSPACE-longread&#xD;&#xA;  [4]: https://github.com/npScarf&#xD;&#xA;  [5]: https://github.com/warrenlr/LINKS/&#xD;&#xA;" />
  <row Id="4585" PostHistoryTypeId="1" PostId="2145" RevisionGUID="a60bf00f-8f2b-4b33-9dda-5408fdb03cfc" CreationDate="2017-07-26T09:40:18.733" UserId="1021" Text="How can I use Nanopore reads to close gaps or resolve repeats in a short-read assembly?" />
  <row Id="4586" PostHistoryTypeId="3" PostId="2145" RevisionGUID="a60bf00f-8f2b-4b33-9dda-5408fdb03cfc" CreationDate="2017-07-26T09:40:18.733" UserId="1021" Text="&lt;nanopore&gt;&lt;long-reads&gt;&lt;repeat-elements&gt;&lt;scaffold&gt;" />
  <row Id="4590" PostHistoryTypeId="2" PostId="2147" RevisionGUID="e7a1973a-a283-4dee-a0c4-032608109751" CreationDate="2017-07-26T10:25:29.833" UserId="123" Text="You might want to look into [Unicycler][1] (manuscript with more information [can be found here][2]); even though it is supposed to be used with bacterial genomes only, it might work well with such a small genome such as a mitochondria.&#xD;&#xA;&#xD;&#xA;[![enter image description here][3]][3]&#xD;&#xA;&#xD;&#xA;Beware that if you happen to have very long reads, you might end up with an assembly with multiple copies of the circular genome: you might want to look into [circlator][4] then.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/rrwick/Unicycler&#xD;&#xA;  [2]: http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005595&#xD;&#xA;  [3]: https://i.stack.imgur.com/wq02F.png&#xD;&#xA;  [4]: https://github.com/sanger-pathogens/circlator" />
  <row Id="4591" PostHistoryTypeId="5" PostId="2145" RevisionGUID="81bce709-f187-421b-a2a1-c0f8b8c1e5b1" CreationDate="2017-07-26T10:28:48.397" UserId="1021" Comment="added 6 characters in body" Text="Low coverage MinION reads should be useful to close gaps and resolve repeats left by short-read assemblers. However, I haven't had any success with the software I know about. I'm aware of the following packages, either for scaffolding or closing gaps in short-read assemblies using long reads:&#xD;&#xA;&#xD;&#xA;+ [`LINKS`][5]&#xD;&#xA;+ [`OPERA-LG`][2]&#xD;&#xA;+ [`npScarf`][4]&#xD;&#xA;+ [`PBJelly`][1]&#xD;&#xA;+ [`SSPACE-longread`][3]&#xD;&#xA;&#xD;&#xA;I've tried `npScarf` and `LINKS`, and they ran successfully but didn't resolve the gaps in my assembly. I couldn't get `PBJelly` and `OPERA-LG` to run with the current versions of `BLASR` and `samtools`, and both packages seem not to be maintained. I have not tried `SSPACE-longread` because it's not open source.&#xD;&#xA;&#xD;&#xA;**What software can I use to fill gaps and resolve repeats in a short-read assembly with low-coverage Nanopore data?**&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;### More information&#xD;&#xA;&#xD;&#xA;I'm finishing a mitochondrial genome. I have a short read assembly of 17 kb with one gap, but molecular data suggests the genome may be around 32 kb.&#xD;&#xA;&#xD;&#xA;After mapping short reads against the short read assembly, `Pilon` identified a tandem repeat of 156 bp in an AT-rich region, but wasn't able to close the gap.&#xD;&#xA;&#xD;&#xA;I have 1.4 Gb of MinION rapid reads with an L50 of 8808 b. I mapped these reads against the short read assembly, and I can see reads that span the gap. I seem to have the information required to close the gap, but I don't know how to do it.&#xD;&#xA;&#xD;&#xA;The organism has a nuclear genome &gt; 600 Mb, so long read coverage is low. Despite this, I tried a *de novo* `Canu` assembly of the Nanopore reads, and I pulled out a 39 kb contig containing mitochondrial genes. Most genes on this contig are duplicated and fragmented, and `Pilon` wasn't able to improve it. &#xD;&#xA;&#xD;&#xA;Thanks for reading!&#xD;&#xA;&#xD;&#xA;  [1]: https://sourceforge.net/projects/pb-jelly&#xD;&#xA;  [2]: https://sourceforge.net/projects/operasf&#xD;&#xA;  [3]: https://www.baseclear.com/genomics/bioinformatics/basetools/SSPACE-longread&#xD;&#xA;  [4]: https://github.com/mdcao/npScarf&#xD;&#xA;  [5]: https://github.com/warrenlr/LINKS/&#xD;&#xA;" />
  <row Id="4592" PostHistoryTypeId="5" PostId="2147" RevisionGUID="fe5d4fa8-bc19-4c11-bc09-8c6dd0e48713" CreationDate="2017-07-26T10:30:40.230" UserId="298" Comment="deleted 4 characters in body" Text="You might want to look into [Unicycler][1] (manuscript with more information [can be found here][2]); even though it is supposed to be used with bacterial genomes only, it might work well with a small genome such as a mitochondrion's.&#xD;&#xA;&#xD;&#xA;[![enter image description here][3]][3]&#xD;&#xA;&#xD;&#xA;Beware that if you happen to have very long reads, you might end up with an assembly with multiple copies of the circular genome: you might want to look into [circlator][4] then.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/rrwick/Unicycler&#xD;&#xA;  [2]: http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005595&#xD;&#xA;  [3]: https://i.stack.imgur.com/wq02F.png&#xD;&#xA;  [4]: https://github.com/sanger-pathogens/circlator" />
  <row Id="4593" PostHistoryTypeId="5" PostId="2145" RevisionGUID="cbc88eb9-1413-4a18-8395-38df12901922" CreationDate="2017-07-26T10:34:48.093" UserId="1021" Comment="added 6 characters in body" Text="Low coverage MinION reads should be useful to close gaps and resolve repeats left by short-read assemblers. However, I haven't had any success with the software I know about. I'm aware of the following packages, either for scaffolding or closing gaps in short-read assemblies using long reads:&#xD;&#xA;&#xD;&#xA;+ [`LINKS`][5]&#xD;&#xA;+ [`OPERA-LG`][2]&#xD;&#xA;+ [`npScarf`][4]&#xD;&#xA;+ [`PBJelly`][1]&#xD;&#xA;+ [`SSPACE-longread`][3]&#xD;&#xA;&#xD;&#xA;I've tried `npScarf` and `LINKS`, and they ran successfully but didn't resolve the gaps in my assembly. I couldn't get `PBJelly` and `OPERA-LG` to run with the current versions of `BLASR` and `samtools`, and both packages seem not to be maintained. I have not tried `SSPACE-longread` because it's not open source.&#xD;&#xA;&#xD;&#xA;**What software can I use to fill gaps and resolve repeats in a short-read assembly with low-coverage Nanopore data?**&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;### More information&#xD;&#xA;&#xD;&#xA;I'm finishing a mitochondrial genome. I have a 17 kb short read assembly with one gap. This was made from 2x150b paired-end reads from a TruSeq PCR-free library with a 400 b insert size. Molecular data suggests the genome may be around 32 kb.&#xD;&#xA;&#xD;&#xA;After mapping short reads against the short read assembly, `Pilon` identified a tandem repeat of 156 bp in an AT-rich region, but wasn't able to close the gap.&#xD;&#xA;&#xD;&#xA;I have 1.4 Gb of MinION rapid reads with an L50 of 8808 b. I mapped these reads against the short read assembly, and I can see reads that span the gap. I seem to have the information required to close the gap, but I don't know how to do it.&#xD;&#xA;&#xD;&#xA;The organism has a nuclear genome &gt; 600 Mb, so long read coverage is low. Despite this, I tried a *de novo* `Canu` assembly of the Nanopore reads, and I pulled out a 39 kb contig containing mitochondrial genes. Most genes on this contig are duplicated and fragmented, and `Pilon` wasn't able to improve it. &#xD;&#xA;&#xD;&#xA;Thanks for reading!&#xD;&#xA;&#xD;&#xA;  [1]: https://sourceforge.net/projects/pb-jelly&#xD;&#xA;  [2]: https://sourceforge.net/projects/operasf&#xD;&#xA;  [3]: https://www.baseclear.com/genomics/bioinformatics/basetools/SSPACE-longread&#xD;&#xA;  [4]: https://github.com/mdcao/npScarf&#xD;&#xA;  [5]: https://github.com/warrenlr/LINKS/&#xD;&#xA;" />
  <row Id="4596" PostHistoryTypeId="2" PostId="2148" RevisionGUID="e0f0b6a5-5b67-4bc9-a16f-d28b43779919" CreationDate="2017-07-26T11:42:11.777" UserId="235" Text="Mike Love is right. **If** the response you are looking for is linear in terms of change per minute, the most productive approach is likely to be fitting a linear model. You **might** get something out of this because the difference between successive time points represents multiple measurements of the rate of change over time. The biggest problem is that the time points aren't independent. You would need to fit a model that estimated that rate for both WT and KO and then test the difference in rates.&#xD;&#xA;&#xD;&#xA;I see three ways forward, in all cases you want to test the last term in the model `~time + genotype + time:genotype`&#xD;&#xA;&#xD;&#xA;1. Ideally you would like to fit a negative binomial linear model to the read counts. But that is going to be difficult as you have no replicates from which to assess the dispersion. Thus you could generate read counts using your favorite read count pipeline (STAR+featureCounts/Salmon/Kallisto) and take the approach in the edgeR guide noted by [b.nota][1] and then use edgeR to test the fitted model.&#xD;&#xA;&#xD;&#xA;2. You could use something like salmon or kallisto to estimate the TPMs assume that log TPMs are normally distributed and model them with plain `lm` in R, using something like `broom` to model each gene separately. &#xD;&#xA;&#xD;&#xA;3. You could use the Salmon or Kallisto bootstraps to estimate *technical* variation (but not biological) and then I suspect you could use Sleuth to fit the model.&#xD;&#xA;&#xD;&#xA;In any case these results should be treated with scepticism andmostly treated as hypothesis generating. You might find genes that you want to go back and confirm and you might see some patterns of interest if you look at some enrichments etc. which you can then design experiments to test. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/2144/235 &quot;b.nota&quot;" />
  <row Id="4597" PostHistoryTypeId="2" PostId="2149" RevisionGUID="f4bd7b40-7d1e-4d04-b96d-4bac4bc74386" CreationDate="2017-07-26T11:54:47.203" UserId="57" Text="### Question&#xD;&#xA;&#xD;&#xA;How can I extract reads from a `bam` file (produced by `bwa-mem`) to `fastq` given a list of reference sequences to filter out?&#xD;&#xA;&#xD;&#xA;### Potential difficulties&#xD;&#xA;&#xD;&#xA; - maintaining FR orientation of pair end reads (in `bam` all the sequences are reference sequences)&#xD;&#xA; - keeping R1 and R2 reads &#xD;&#xA; - keeping quality scores in the same encoding as original fastq (default illumina phred scores in my case)&#xD;&#xA; - bam can be (ana usually is) sorted by coordinates&#xD;&#xA;&#xD;&#xA;### Almost there solutions&#xD;&#xA;&#xD;&#xA;1. [Sujai's perl solution][1] in [blobology][2] does exact opposite - getting reads from list of references (so I could just reverse the list). The disadvantage is that the script outputs an interleaved `fq` file; requires unique names of mates, otherwise R1/R2 information is lost.&#xD;&#xA;&#xD;&#xA;2. samtools + grep them all from fastq files&#xD;&#xA;&#xD;&#xA;create a list of read names that do not map to filtered scaffolds. (cut will extract just read names, uniq will collapse pair end read names if they are the same). Then grep read names from fastq files and remove -- separator between hits&#xD;&#xA;&#xD;&#xA;    samtools view foo.bam | grep -vf list_of_scaffols_filter \&#xD;&#xA;      | cut -f 1 | uniq &gt; list_of_reads_to_keep&#xD;&#xA;&#xD;&#xA;    grep -A 3 -f list_of_reads_to_keep foo_R1.fq | grep -v &quot;^--$&quot; &gt; foo_R1_filtered_bash.fq&#xD;&#xA;    grep -A 3 -f list_of_reads_to_keep foo_R2.fq | grep -v &quot;^--$&quot; &gt; foo_R1_filtered_bash.fq&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;3. filter bam &amp; picard tools&#xD;&#xA;&#xD;&#xA;Or I could do just the filtering part and use [Picard-tools](http://broadinstitute.github.io/picard/command-line-overview.html) (Picard.SamToFastq), but as usual I am avoiding java as much as I can. I guess&#xD;&#xA;&#xD;&#xA;    samtools view foo.bam | grep -vf list_of_scaffols_filter \&#xD;&#xA;      | java -jar picard.jar SamToFastq INPUT=/dev/stdin \&#xD;&#xA;      FASTQ=foo_R1_filtered_bash.fq SECOND_END_FASTQ=foo_R2_filtered_bash.fq&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;The first solution does not really work for me since I do not want to rename all the reads in bam file and I want to keep the R1/R2 information (since R1 and R2 have different error profiles). Both solutions 2 and 3 I find bit clumsy and I am not sure if they are general, I might get some unexpected behaviour if one of reads is mapped second is not... They both relay on the same filtering step.&#xD;&#xA;&#xD;&#xA;I was wondering about some pysam solution. I guess it will be much slower, but at least it will be much clearer and perhaps more general. Something like in [Convert Bam File To Fasta File][3] - there is pysam solution for fasta (not fastq), almost there...&#xD;&#xA;&#xD;&#xA;### Story&#xD;&#xA;&#xD;&#xA;I have very fragmented reference genome. Some of scaffolds them are too small to works with, and some of them are contaminants (identified using blobtools).&#xD;&#xA;I want to separate reads that are mapping to different groups to separate contaminants, short scaffolds and scaffold that will be used for downstream analysis. The reason is that if we remap all the reads to filtered reference (0.7 - 0.8 of original genome), the most of them (0.95 - 0.99) will still find a place where they map, therefore there is 0.2 - 0.3 of misplaced reads that will obviously have to affect downstream analysis, like variant calling.&#xD;&#xA;&#xD;&#xA;This filtering idea is based logic that if the filtered duplicated genomic region will contain some small differences, they will attract their reads (and if I filter them I will improve variant calling) and if they will be exactly same, they will get reads assigned at random, so there is no harm in doing that.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/blaxterlab/blobology/blob/master/bowtie2_extract_reads_mapped_to_specific_contigs.pl&#xD;&#xA;  [2]: https://github.com/blaxterlab/blobology&#xD;&#xA;  [3]: https://www.biostars.org/p/6970/" />
  <row Id="4598" PostHistoryTypeId="1" PostId="2149" RevisionGUID="f4bd7b40-7d1e-4d04-b96d-4bac4bc74386" CreationDate="2017-07-26T11:54:47.203" UserId="57" Text="How to safely and efficiently convert subset of bam to fastq?" />
  <row Id="4599" PostHistoryTypeId="3" PostId="2149" RevisionGUID="f4bd7b40-7d1e-4d04-b96d-4bac4bc74386" CreationDate="2017-07-26T11:54:47.203" UserId="57" Text="&lt;bam&gt;&lt;fastq&gt;&lt;format-conversion&gt;&lt;filtering&gt;" />
  <row Id="4600" PostHistoryTypeId="2" PostId="2150" RevisionGUID="eb46644b-11ce-463e-b85e-0cf579435f5e" CreationDate="2017-07-26T13:13:32.230" UserId="77" Text="I'm not aware of any pre-made program to do this, so I [wrote one for you](https://github.com/dpryan79/Answers/tree/master/bioinfoSE_2149). This will take a BAM file with any ordering and produce properly ordered gzipped fastq files with the filtering as you requested. Internally, this iterates over all of the entries in the BAM file (ignoring secondary/supplemental entries and those where both mates map to your filter list), store the properly oriented sequence/quality/read name in a buffer, and then dumps that buffer entry to disk once the mate is found. This should be reasonably performant (hey, it's python, to don't expect too much), though if you happen to have indexed BAM files then one could think of ways to make this run faster.&#xD;&#xA;&#xD;&#xA;Do check the output, since I've only run one test." />
  <row Id="4601" PostHistoryTypeId="2" PostId="2151" RevisionGUID="b368a134-2378-4fd3-9948-461fb239d54e" CreationDate="2017-07-26T13:17:23.220" UserId="57" Text="Ok, I wrote a bit brute pysam / BioPython parser that uses index of bam to get proper order of read pair for R1 / R2 files and bitwise flag. It should not be too difficult to add more sophisticated filtering rules now.&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python3&#xD;&#xA;    # 1. arg - indexed bam file&#xD;&#xA;    # 2. arg - list of headers to filter&#xD;&#xA;    # 3. arg - name pattern for the output reads&#xD;&#xA;    &#xD;&#xA;    import os&#xD;&#xA;    import sys&#xD;&#xA;    import pysam&#xD;&#xA;    from Bio import SeqIO, Seq, SeqRecord&#xD;&#xA;    &#xD;&#xA;    samfile = pysam.AlignmentFile(sys.argv[1], &quot;rb&quot;)&#xD;&#xA;    header_set = set(line.strip() for line in open(sys.argv[2]))&#xD;&#xA;    base = sys.argv[3]&#xD;&#xA;    &#xD;&#xA;    out_R1 = base + 'R1_filtered.fq'&#xD;&#xA;    out_R2 = base + 'R2_filtered.fq'&#xD;&#xA;    &#xD;&#xA;    with open(out_R1, mode='w') as R1, open(out_R2, mode='w') as R2:&#xD;&#xA;    	for entry in samfile:&#xD;&#xA;    		if entry.is_read1 and not entry.reference_name in header_set:&#xD;&#xA;    			# pait pair&#xD;&#xA;    			entry_R2 = samfile.mate(entry)&#xD;&#xA;    			# do some sequence gymnastics with R1&#xD;&#xA;    			seq_R1 = Seq.Seq(entry.seq)&#xD;&#xA;    			if entry.is_reverse :&#xD;&#xA;    				seq_R1 = seq_R1.reverse_complement()&#xD;&#xA;    			# do some sequence gymnastics with R2&#xD;&#xA;    			seq_R2 = Seq.Seq(entry_R2.seq)&#xD;&#xA;    			if entry_R2.is_reverse :&#xD;&#xA;    				seq_R2 = seq_R2.reverse_complement()&#xD;&#xA;    			R1.write('@' + entry.qname + '\n' + str(seq_R1) + '\n+\n' + entry.qqual + '\n')&#xD;&#xA;    			R2.write('@' + entry_R2.qname + '\n' + str(seq_R2) + '\n+\n' + entry_R2.qqual + '\n')&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    samfile.close()&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;There are some ugly bits, please feel free to make it nicer.&#xD;&#xA;" />
  <row Id="4602" PostHistoryTypeId="5" PostId="2143" RevisionGUID="2558b928-5ad4-4f55-9395-3164e1c8a680" CreationDate="2017-07-26T13:25:26.233" UserId="73" Comment="added 17 characters in body" Text="Not as far as I am aware. The Ray assembler used to (and possibly still does) store the kmers as FASTA files where the header was the count of the sequence, which I thought was a pretty neat bastardisation of the FASTA file format.&#xD;&#xA;&#xD;&#xA;I think Jellyfish changed their format between v1 and v2 (both not FASTA), because they changed to doing counts based on bloom filters." />
  <row Id="4603" PostHistoryTypeId="2" PostId="2152" RevisionGUID="b54b2900-d885-439d-9fb5-ec7ade1472cc" CreationDate="2017-07-26T13:35:42.953" UserId="1199" Text="Let C be base coverage, R is the length of reads and K is the length of k-mer. Then k-mer coverage Ck can be computed as Ck = C*(R - K + 1)/R.&#xD;&#xA;&#xD;&#xA;Could someone please explain why is this equation valid? (I'm mostly confused as why it is divided by R).&#xD;&#xA;&#xD;&#xA;Source: [Velvet manual](http://www.ebi.ac.uk/~zerbino/velvet/Manual.pdf) Section 5.1" />
  <row Id="4604" PostHistoryTypeId="1" PostId="2152" RevisionGUID="b54b2900-d885-439d-9fb5-ec7ade1472cc" CreationDate="2017-07-26T13:35:42.953" UserId="1199" Text="Formula for k-mer coverage" />
  <row Id="4605" PostHistoryTypeId="3" PostId="2152" RevisionGUID="b54b2900-d885-439d-9fb5-ec7ade1472cc" CreationDate="2017-07-26T13:35:42.953" UserId="1199" Text="&lt;k-mer&gt;" />
  <row Id="4606" PostHistoryTypeId="5" PostId="2148" RevisionGUID="bc0153be-1f81-4b76-9fd7-cef6208d50d6" CreationDate="2017-07-26T14:28:53.783" UserId="235" Comment="added 169 characters in body" Text="Mike Love is right. **If** the response you are looking for is linear in terms of change per minute, the most productive approach is likely to be fitting a linear model. You **might** get something out of this because the difference between successive time points represents multiple measurements of the rate of change over time. The biggest problem is that the time points aren't independent. You would need to fit a model that estimated that rate for both WT and KO and then test the difference in rates.&#xD;&#xA;&#xD;&#xA;I see three ways forward, in all cases you want to test the last term in the model `~time + genotype + time:genotype`&#xD;&#xA;&#xD;&#xA;1. Ideally you would like to fit a negative binomial linear model to the read counts. But that is going to be difficult as you have no replicates from which to assess the dispersion. Thus you could generate read counts using your favorite read count pipeline (STAR+featureCounts/Salmon/Kallisto) and take the approach in the edgeR guide noted by [b.nota][1] and then use edgeR to test the fitted model.&#xD;&#xA;&#xD;&#xA;2. You could use something like salmon or kallisto to estimate the TPMs assume that log TPMs are normally distributed and model them with plain `lm` in R, using something like `broom` to model each gene separately. A tutorial by David Robinson, the author of `broom` on doing this sort of thing can be found [here][2]&#xD;&#xA;&#xD;&#xA;3. You could use the Salmon or Kallisto bootstraps to estimate *technical* variation (but not biological) and then I suspect you could use Sleuth to fit the model.&#xD;&#xA;&#xD;&#xA;In any case these results should be treated with scepticism andmostly treated as hypothesis generating. You might find genes that you want to go back and confirm and you might see some patterns of interest if you look at some enrichments etc. which you can then design experiments to test. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/a/2144/235 &quot;b.nota&quot;&#xD;&#xA;  [2]: http://varianceexplained.org/r/tidy-genomics-broom/ &quot;here&quot;" />
  <row Id="4607" PostHistoryTypeId="2" PostId="2153" RevisionGUID="13bc225d-1cbd-4988-ace3-357668259283" CreationDate="2017-07-26T15:20:57.187" UserId="1201" Text="I am interested in finding a database that takes a gene or protein name as input (possibly with the option to specify transcript) and gives information about the protein's functional domains in terms of either specific residue ranges, genomic coordinates, etc. Ideally, the database would include 3D conformational information as well as the functional domains. Does such a database exist? It seems that this information can be inferred for a given protein using various publications and references but I have yet to find a single collection containing these data all in one place." />
  <row Id="4608" PostHistoryTypeId="1" PostId="2153" RevisionGUID="13bc225d-1cbd-4988-ace3-357668259283" CreationDate="2017-07-26T15:20:57.187" UserId="1201" Text="Do any publicly available databases detail protein structure and functional domains?" />
  <row Id="4609" PostHistoryTypeId="3" PostId="2153" RevisionGUID="13bc225d-1cbd-4988-ace3-357668259283" CreationDate="2017-07-26T15:20:57.187" UserId="1201" Text="&lt;gene&gt;&lt;proteins&gt;&lt;protein-structure&gt;&lt;rna-structure&gt;" />
  <row Id="4610" PostHistoryTypeId="5" PostId="368" RevisionGUID="782d1793-728c-4bc0-9892-473cc139d440" CreationDate="2017-07-26T16:04:12.083" UserId="29" Comment="reformat using math layout" Text="The effective length is $\tilde{l}_i = l_i - \mu + 1$ (note the R code at the bottom of Harold's blog post), which in the case of $\mu &lt; l_i$ should be 1. Ideally, you'd use the mean fragment length mapped to the particular feature, rather than a global $\mu$, but that's a lot more work for likely 0 benefit.&#xD;&#xA;&#xD;&#xA;Regarding choosing a particular transcript, ideally one would use a method like salmon or kallisto (or RSEM if you have time to kill). Otherwise, your options are (A) choose the major isoform (if it's known in your tissue and condition) or (B) use a &quot;union gene model&quot; (sum the non-redundant exon lengths) or (C) take the median transcript length. None of those three options make much of a difference if you're comparing between samples, though they're all inferior to a salmon/kallisto/etc. metric.&#xD;&#xA;&#xD;&#xA;Why are salmon et al. better methods? They don't use arbitrary metrics that will be the same across samples to determine the feature length. Instead, they use expectation maximization (or similarish, since at least salmon doesn't actually use EM) to quantify individual isoform usage. The effective gene length in a sample is then the average of the transcript lengths after weighting for their relative expression (yes, one should remove $\mu$ in there). This can then vary between samples, which is quite useful if you have isoform switching between samples/groups in such a way that methods A-C above would miss (think of cases where the switch is to a smaller transcript with higher coverage over it...resulting in the coverage/length in methods A-C to be tamped down)." />
  <row Id="4611" PostHistoryTypeId="5" PostId="367" RevisionGUID="ff1a8804-e0a8-446a-842f-c80f252475d3" CreationDate="2017-07-26T16:06:30.160" UserId="298" Comment="Fixed formatting" Text="According to [this famous blog post](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/), the effective transcript length is:&#xD;&#xA;&#xD;&#xA;$\tilde{l}_i = l_i - \mu$&#xD;&#xA;&#xD;&#xA;where $l_i$ is the length of transcript and $\mu$ is the average fragment length. However, typically fragment length is about 300bp. What if when the transcript $l_i$ is smaller than 300? How do you compute the effective length in this case?&#xD;&#xA;&#xD;&#xA;A related question: when computing the FPKM of a gene, how to choose a transcript? Do we choose a &quot;canonical&quot; transcript (how?) or combine the signals from all transcripts to a gene-level FPKM?" />
  <row Id="4612" PostHistoryTypeId="5" PostId="289" RevisionGUID="f3060db8-fa54-49d4-baec-58bc9d041ed7" CreationDate="2017-07-26T16:07:05.550" UserId="298" Comment="deleted 2 characters in body" Text="Is there any resource (paper, blogpost, Github gist, etc.) describing the BWA-MEM algorithm for assigning mapping qualities? I vaguely remember that I have somewhere seen a formula for SE reads, which looked like&#xD;&#xA;&#xD;&#xA;$C * (s_1 - s_2) / s_1,$&#xD;&#xA;    &#xD;&#xA;where $s_1$ and $s_2$ denoted the alignment scores of two best alignments and `C` was some constant.&#xD;&#xA;&#xD;&#xA;I believe that a reimplementation of this algorithm in some scripting language could be very useful for the bioinfo community. For instance, I sometimes test various mapping methods and some of them tend to find good alignments, but fail in assigning appropriate qualities. Therefore, I would like to re-assign all the mapping qualities in a SAM file with the BWA-MEM algorithm.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Btw. This algorithm must already have been implemented outside BWA, see the BWA-MEM paper: &#xD;&#xA;&gt; GEM does not compute mapping quality. Its&#xD;&#xA;mapping quality is estimated with a BWA-like algorithm with suboptimal&#xD;&#xA;alignments available.&#xD;&#xA;&#xD;&#xA;Unfortunately, the [BWA-MEM paper repo](https://github.com/lh3/mem-paper/) contains only the resulting `.eval` files.&#xD;&#xA;&#xD;&#xA;**Update:** The question is *not* about the algorithm for computing alignment scores. Mapping qualities and alignment scores are two different things:&#xD;&#xA;&#xD;&#xA; * Alignment score quantifies the similarity between two sequences (e.g., a read and a reference sequence)&#xD;&#xA; * Mapping quality (MAQ) quantifies the probability that a read is aligned to a wrong position.&#xD;&#xA;&#xD;&#xA;Even alignments with high scores can have a very low mapping quality." />
  <row Id="4613" PostHistoryTypeId="5" PostId="54" RevisionGUID="a7257e90-5ba0-44c6-9eb3-ac4bd35a7c98" CreationDate="2017-07-26T16:08:55.733" UserId="298" Comment="Fixed formatting" Text="You may consider using [RUVSeq][1]. Here is an excerpt from the [2013 Nature Biotechnology publication][2]:&#xD;&#xA;&#xD;&#xA;&gt; We evaluate the performance of the External RNA Control Consortium (ERCC) spike-in controls and investigate the possibility of using them directly for normalization. We show that the spike-ins are not reliable enough to be used in standard global-scaling or regression-based normalization procedures. We propose a normalization strategy, called remove unwanted variation (RUV), that adjusts for nuisance technical effects by performing factor analysis on suitable sets of control genes (e.g., ERCC spike-ins) or samples (e.g., replicate libraries).&#xD;&#xA;&#xD;&#xA;RUVSeq essentially fits a generalized linear model (GLM) to the expression data, where your expression matrix `Y` is a `m` by `n` matrix, where `m` is the number of samples and `n` the number of genes. The model boils down to&#xD;&#xA;&#xD;&#xA;$Y = X*\beta + Z*\gamma + W*\alpha + \epsilon$&#xD;&#xA;&#xD;&#xA;where $X$ describes the conditions of interest (e.g., treatment vs. control), $Z$ describes observed covariates (e.g., gender) and $W$ describes unobserved covariates (e.g., batch, temperature, lab). $\beta$, $\gamma$ and $\alpha$ are parameter matrices which record the contribution of $X$, $Z$ and $W$, and $\epsilon$ is random noise. For subset of carefully selected genes (e.g., ERCC spike-ins, housekeeping genes, or technical replicates) we can assume that $X$ and $Z$ are zero, and find $W$ - the &quot;unwanted variation&quot; in your sample. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/html/RUVSeq.html&#xD;&#xA;  [2]: http://www.nature.com/nbt/journal/v32/n9/full/nbt.2931.html" />
  <row Id="4614" PostHistoryTypeId="5" PostId="65" RevisionGUID="17c87ff3-6881-476e-a12b-a42c1c9c3c94" CreationDate="2017-07-26T16:10:55.117" UserId="29" Comment="use math formatting" Text="At its easiest, you just store the *forward* ($F$) and *reverse* ($R$) hash value.&#xD;&#xA;&#xD;&#xA;You update the forward hash value by conventional means, e.g. bit-shifting the base value into its lower bits:&#xD;&#xA;&#xD;&#xA;$$&#xD;&#xA;F_{n + 1} = ((F_n \ll B \mid x) \mathop\&amp; M,&#xD;&#xA;$$&#xD;&#xA;&#xD;&#xA;$B$ is the bit size of the base encoding, $M$ is the word mask for a word of length $W$ bits, and can be omitted if the width of the hash is the width of the machine word; and the reverse hash value by doing the mathematical inverse, e.g. bit-shifting the value of the  reverse complement of the base into the upper bits:&#xD;&#xA;&#xD;&#xA;$$&#xD;&#xA;R_{n + 1} = ((R_n \gg B) \mid (\operatorname{compl}x \ll (W - B))) \mathop\&amp; M.&#xD;&#xA;$$&#xD;&#xA;&#xD;&#xA;And then you compute a *combined hash* value by xoring the two hashes, $H  = F \oplus R$." />
  <row Id="4615" PostHistoryTypeId="5" PostId="69" RevisionGUID="22770504-35e3-4813-98a5-bd2b242f2319" CreationDate="2017-07-26T16:13:26.200" UserId="29" Comment="add TMP equation" Text="First off,&#xD;&#xA;&#xD;&#xA;**Don’t use RPKMs**.&#xD;&#xA;&#xD;&#xA;[They are truly deprecated](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) because they’re confusing once it comes to paired-end reads. If anything, use *FPKM*s, which are mathematically the same but use a more correct name (do we count paired reads separately? No, we count *fragments*).&#xD;&#xA;&#xD;&#xA;Even better, [use TPM (= transcripts per million), or an appropriate cross-library normalisation method](http://rpubs.com/klmr/rnaseq-norm). TMP is defined as:&#xD;&#xA;&#xD;&#xA;$$&#xD;&#xA;\text{TPM}_\color{orchid}i =&#xD;&#xA;    {\color{dodgerblue}{\frac{x_\color{orchid}i}{{l_\text{eff}}_\color{orchid}i}}}&#xD;&#xA;    \cdot&#xD;&#xA;    \frac{1}{\sum_\color{tomato}j \color{dodgerblue}{\frac{x_\color{tomato}j}{{l_\text{eff}}_\color{tomato}j}}}&#xD;&#xA;    \cdot&#xD;&#xA;    \color{darkcyan}{10^6}&#xD;&#xA;$$&#xD;&#xA;&#xD;&#xA;where&#xD;&#xA;&#xD;&#xA;* $\color{orchid}i$: transcript index,&#xD;&#xA;* $x_i$: transcript raw count,&#xD;&#xA;* $\color{tomato}j$ iterates over all (known) transcripts,&#xD;&#xA;* $\color{dodgerblue}{\frac{x_k}{{l_\text{eff}}_k}}$: rate of fragment coverage per nucleobase ($l_\text{eff}$ being the [effective length](https://bioinformatics.stackexchange.com/q/367/29)),&#xD;&#xA;* $\color{darkcyan}{10^6}$: scaling factor (= “per millions”).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;That said, FPKM an be calculated in R as follows. Note that most of the calculation happens in log transformed number space, to avoid [numerical instability](https://en.wikipedia.org/wiki/Numerical_stability):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    fpkm = function (counts, effective_lengths) {&#xD;&#xA;        exp(log(counts) - log(effective_lengths) - log(sum(counts)) + log(1E9))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Here, the *effective length* is the transcript length minus the mean fragment length plus 1; that is, all the possible positions of an average fragment inside the transcript, which equals the number of all distinct fragments that can be sampled from a transcript.&#xD;&#xA;&#xD;&#xA;This function handles *one* library at a time. I ([and others](http://varianceexplained.org/r/tidy-genomics-biobroom/)) argue that this is the way functions should be written. If you want to apply the code to multiple libraries, nothing is easier using [‹dplyr›](http://dplyr.tidyverse.org/):&#xD;&#xA;&#xD;&#xA;    tidy_expression = tidy_expression %&gt;%&#xD;&#xA;        group_by(Sample) %&gt;%&#xD;&#xA;        mutate(FPKM = fpkm(Count, col_data$Lengths))&#xD;&#xA;&#xD;&#xA;However, the data in the question isn’t in tidy data format, so we first need to transform it accordingly using [‹tidyr›](http://tidyr.tidyverse.org/):&#xD;&#xA;&#xD;&#xA;    tidy_expression = expression %&gt;%&#xD;&#xA;        gather(Sample, Count)&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;This equation fails if all your counts are zero; instead of zeros you will get a vector of NaNs. You might want to account for that.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;And I mentioned that TPMs are superior, so here’s their function as well:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    tpm = function (counts, effective_lengths) {&#xD;&#xA;        rate = log(counts) - log(effective_lengths)&#xD;&#xA;        exp(rate - log(sum(exp(rate))) + log(1E6))&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4616" PostHistoryTypeId="5" PostId="795" RevisionGUID="c13d26a3-7f0f-4966-a30f-d7dda87b3398" CreationDate="2017-07-26T16:15:39.647" UserId="29" Comment="math formatting" Text="I have a [blog post][1] that describes the effective length (as well as these different relative abundance units).  The short explanation is that what people refer to as the &quot;effective length&quot; is actually the _expected_ effective length (i.e., the expectation, in a statistical sense, of the effective length).  The notion of effective length is actually a property of a transcript, fragment pair, and is equal to the number of potential starting locations for a fragment of this length on the given transcript.  If you take the average, over all fragments mapping to a transcript (potentially weighted by the conditional probability of this mapping), this quantity is the expected effective length of the transcript.  This is often approximated as simply $l_i - \mu$, or $l_i - \mu_{l_i}$ --- where $\mu_{l_i}$ is the mean of the _conditional_ fragment length distribution (conditioned on the fragment length being &lt; $l_i$ to account for exactly the issue that you raise).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://robpatro.com/blog/?p=235" />
  <row Id="4617" PostHistoryTypeId="2" PostId="2154" RevisionGUID="10199b5e-0d94-4d5e-855b-2cb929910785" CreationDate="2017-07-26T16:24:03.780" UserId="306" Text="Some of this information (at least some domains, active sites, etc) is available from UniProt: http://www.uniprot.org/&#xD;&#xA;&#xD;&#xA;If you want to download their whole database, you can search without specifying any terms and then click the Download button." />
  <row Id="4618" PostHistoryTypeId="5" PostId="87" RevisionGUID="e0970931-917d-45b9-84ea-6fd80cd5751e" CreationDate="2017-07-26T16:30:47.080" UserId="29" Comment="fix math syntax" Text="Rather than working on the base level, you could probably work on say gene level counts. [Kendall's tau](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient), an ordinal association metric, can then be used as an appropriate correlation measure.&#xD;&#xA;&#xD;&#xA;If $X$ and $Y$ are your iCLIP replicates, $i$ represents gene index and $(x_i, y_i)$ represents the number of RBP binding sites in $X$ and $Y$ respectively for the $i^{th}$ gene, Kendall's tau is defined as :&#xD;&#xA;&#xD;&#xA;$$&#xD;&#xA;\frac{\text{#(concordant pairs)} - \text{#(discordant pairs)}}{n(n-1)/2}&#xD;&#xA;$$&#xD;&#xA;&#xD;&#xA;Where any two pairs $(x_i, y_i)$ and $(x_j, y_j)$ are concordant if:&#xD;&#xA;&#xD;&#xA;- $x_i &gt; x_j$ AND $y_i &gt; y_j$&#xD;&#xA;&#xD;&#xA;OR&#xD;&#xA;&#xD;&#xA;- $x_i &lt; x_j$ AND $y_i &lt; y_j$&#xD;&#xA;&#xD;&#xA;Correspondingly they are discordant if:&#xD;&#xA;&#xD;&#xA;- $x_i &lt; x_j$ AND $y_i &gt; y_j$&#xD;&#xA;&#xD;&#xA;OR&#xD;&#xA;&#xD;&#xA;- $x_i &gt; x_j$ AND $y_i &lt; y_j$&#xD;&#xA;" />
  <row Id="4619" PostHistoryTypeId="5" PostId="2152" RevisionGUID="c03987b5-d387-457e-89a9-5122a67266e5" CreationDate="2017-07-26T16:32:17.023" UserId="29" Comment="math formatting" Text="Let $C$ be base coverage, $R$ is the length of reads and $K$ is the length of $k$-mer. Then $k$-mer coverage $C_k$ can be computed as $C_k = C\cdot(R - K + 1)/R$.&#xD;&#xA;&#xD;&#xA;Could someone please explain why is this equation valid (I'm mostly confused as why it is divided by $R$)?&#xD;&#xA;&#xD;&#xA;Source: [Velvet manual](http://www.ebi.ac.uk/~zerbino/velvet/Manual.pdf) Section 5.1" />
  <row Id="4621" PostHistoryTypeId="5" PostId="2154" RevisionGUID="06b4f5bc-cea7-421a-8317-6ce4f325a1cc" CreationDate="2017-07-26T17:12:17.537" UserId="29" Comment="linkify website name" Text="Some of this information (at least some domains, active sites, etc) is available from [UniProt](http://www.uniprot.org/).&#xD;&#xA;&#xD;&#xA;If you want to download their whole database, you can search without specifying any terms and then click the Download button." />
  <row Id="4622" PostHistoryTypeId="2" PostId="2156" RevisionGUID="195f5541-6f87-4693-a496-fc249e10ccf7" CreationDate="2017-07-26T17:21:23.227" UserId="1196" Text="I have been analyzing some virus DNA from the NCBI databases. The fasta sequences that I recieve from them have header lines that look like this:&#xD;&#xA;&#xD;&#xA;    &gt;gi|61393989|gb|AY848686.1|&#xD;&#xA;&#xD;&#xA;I think that the second number is the GenBank ID, but I have several questions:&#xD;&#xA;&#xD;&#xA;1) What does the &quot;gi&quot; in the first ID stand for?&#xD;&#xA;&#xD;&#xA;2) Is this a standard format for storing a series of IDs associated with sequences? That is, should I expect to see this format elsewhere and be prepared to parse fasta headers like this in code that I write?&#xD;&#xA;&#xD;&#xA;3) It seems that `AY848686` is also a valid GenBank ID, and I see the `.1` missing/present in GenBank IDs in various places. What does the `.1` mean at the end of GenBank IDs and why is it there/does it _need_ to be there?&#xD;&#xA;&#xD;&#xA;Thanks in advance for you answers." />
  <row Id="4623" PostHistoryTypeId="1" PostId="2156" RevisionGUID="195f5541-6f87-4693-a496-fc249e10ccf7" CreationDate="2017-07-26T17:21:23.227" UserId="1196" Text="Fasta Sequence Identifier format?" />
  <row Id="4624" PostHistoryTypeId="3" PostId="2156" RevisionGUID="195f5541-6f87-4693-a496-fc249e10ccf7" CreationDate="2017-07-26T17:21:23.227" UserId="1196" Text="&lt;fasta&gt;&lt;identifiers&gt;" />
  <row Id="4625" PostHistoryTypeId="2" PostId="2157" RevisionGUID="7d883206-917b-462e-a2ba-7f7299cb2ce7" CreationDate="2017-07-26T17:23:16.920" UserId="298" Text="[EnsEMBL][1] also has this. Search for your gene of interest, choose your transcript, go to the page of its protein product(s), and select &quot;Domains &amp; Features&quot; from the right-hand menu (using human p53 as an example):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    Domain source       Start End  Description                                            Accession InterPro  &#xD;&#xA;    PANTHER             3     331  -                                                      PTHR11447:SF6-         &#xD;&#xA;    Pfam                5     29   p53 transactivation domain                             PF08563   IPR013872 &#xD;&#xA;    PANTHER             3     331  p53 tumour suppressor family                           PTHR11447 IPR002117 &#xD;&#xA;    PRINTS              116   142  p53 tumour suppressor family                           PR00386   IPR002117 &#xD;&#xA;    PRINTS              158   179  p53 tumour suppressor family                           PR00386   IPR002117 &#xD;&#xA;    PRINTS              213   234  p53 tumour suppressor family                           PR00386   IPR002117 &#xD;&#xA;    PRINTS              236   258  p53 tumour suppressor family                           PR00386   IPR002117 &#xD;&#xA;    Prosite_patterns    237   249  p53 tumour suppressor family                           PS00348   IPR002117 &#xD;&#xA;    PRINTS              264   286  p53 tumour suppressor family                           PR00386   IPR002117 &#xD;&#xA;    Pfam                95    288  p53 DNA-binding domain                                 PF00870   IPR011615 &#xD;&#xA;    Superfamily         97    287  p53-like transcription factor DNA-binding              SSF49417  IPR008967 &#xD;&#xA;    Gene3D              94    297  p53/RUNT-type transcription factor DNA-binding domain  2.60.40.720IPR012346 &#xD;&#xA;&#xD;&#xA;        &#xD;&#xA;&#xD;&#xA;  [1]: http://www.ensembl.org&#xD;&#xA;  [2]: https://www.ncbi.nlm.nih.gov/&#xD;&#xA;" />
  <row Id="4626" PostHistoryTypeId="2" PostId="2158" RevisionGUID="18f18725-df45-4811-a8fd-4e8b88dad574" CreationDate="2017-07-26T17:25:01.930" UserId="96" Text="1. The `gi` is an abbreviation for &quot;Genbank identifier&quot;.&#xD;&#xA;2. This is a pretty standard convention used by data stored in NCBI databases. There used to be a pretty comprehensive description of the conventions used at NCBI (I wouldn't say it was a standard or specification, just convention) [here](ftp://ftp.ncbi.nih.gov/blast/documents/formatdb.html), but this page is no longer available it seems.&#xD;&#xA;3. The `AY848686` label is an accession value, and the appended number is a version number.&#xD;&#xA;&#xD;&#xA;See [this page](https://www.ncbi.nlm.nih.gov/Sitemap/sequenceIDs.html) for more info.&#xD;&#xA;&#xD;&#xA;The short answer is: these are all internal conventions. None of it *needs* to be there, except perhaps to ensure NCBI's internal tools work as expected." />
  <row Id="4627" PostHistoryTypeId="2" PostId="2159" RevisionGUID="e71652cf-fd6e-4e5a-be01-1ac16f90aed1" CreationDate="2017-07-26T17:31:21.397" UserId="298" Text="1. That is a GenBank gene ID and it ([apparently][1]) stands for &quot;GenInfo Identifier&quot;. The full GenBank ID is `gi|61393989`. &#xD;&#xA;&#xD;&#xA;2. Yes, this is the standard format for sequences provided by NCBI. However, you can have all sorts of extra details appended to that. They (NCBI) generally use `|` as a field separator, so you could try to write code that parse that, but I really wouldn't recommend it. The FASTA headers have traditionally been free. People can put anything they like there apart from a `\n`, so I really wouldn't put any effort into writing code to parse these headers. &#xD;&#xA;&#xD;&#xA; If you need a short name for the sequence, just take the first N characters or, if it matches `^..|.+?|`, take the characters until the second `|`. Seriously though, don't bother parsing this it is very rarely helpful and is almost certain to break as soon as you use a different database. &#xD;&#xA;&#xD;&#xA;3. The AY848686.1 is a GenBank accession. The `gb` indicates that it's a GenBank accession and the `.1` means it is the 1st version of that accession. &#xD;&#xA;&#xD;&#xA;As for why there are two ([source][1]):&#xD;&#xA;&#xD;&#xA;&gt; Unlike the gi number system, in which sequence identification numbers were not necessarily consistent across the databases (e.g., GenBank and EMBL could each assign their own gi number to a sequence), the new system is designed to ensure consistency. It is also designed to show a relationship between a sequence identification number and the accession number of the record in which it is found. In contrast, GI numbers are assigned consecutively and bear no resemblance to the accession number. Finally, the new system allows the assignment of alphanumeric protein IDs to proteins translations within nucleotide sequence records. The protein IDs contain three letters followed by five digits, a period, and a version number.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/Sitemap/sequenceIDs.html" />
  <row Id="4628" PostHistoryTypeId="5" PostId="2158" RevisionGUID="46613825-7627-4df6-9a83-110f053eefdc" CreationDate="2017-07-26T17:33:05.717" UserId="96" Comment="added 3865 characters in body" Text="1. The `gi` is an abbreviation for &quot;Genbank identifier&quot;.&#xD;&#xA;2. This is a pretty standard convention used by data stored in NCBI databases. There used to be a pretty comprehensive description of the conventions used at NCBI (I wouldn't say it was a standard or specification, just convention) [here](ftp://ftp.ncbi.nih.gov/blast/documents/formatdb.html), but this page is no longer available it seems.&#xD;&#xA;3. The `AY848686` label is an accession value, and the appended number is a version number.&#xD;&#xA;&#xD;&#xA;See [this page](https://www.ncbi.nlm.nih.gov/Sitemap/sequenceIDs.html) for more info.&#xD;&#xA;&#xD;&#xA;The short answer is: these are all internal conventions. None of it *needs* to be there, except perhaps to ensure NCBI's internal tools work as expected.&#xD;&#xA;&#xD;&#xA;------&#xD;&#xA;&#xD;&#xA;**UPDATE**: I found a non-official copy of the old docs [here](http://structure.usc.edu/blast/formatdb.html). Look for the section starting with &quot;FASTA Defline Format&quot;. I've copied it here to prevent further problems with bitrot in the future.&#xD;&#xA;&#xD;&#xA;    FASTA Defline Format&#xD;&#xA;    --------------------&#xD;&#xA;    The syntax of FASTA Deflines used by the NCBI BLAST server depends on&#xD;&#xA;    the database from which each sequence was obtained.  The table below lists&#xD;&#xA;    the identifiers for the databases from which the sequences were derived.&#xD;&#xA;     &#xD;&#xA;    &#xD;&#xA;      Database Name                         Identifier Syntax&#xD;&#xA;    &#xD;&#xA;      GenBank                               gb|accession|locus&#xD;&#xA;      EMBL Data Library                     emb|accession|locus&#xD;&#xA;      DDBJ, DNA Database of Japan           dbj|accession|locus&#xD;&#xA;      NBRF PIR                              pir||entry&#xD;&#xA;      Protein Research Foundation           prf||name&#xD;&#xA;      SWISS-PROT                            sp|accession|entry name&#xD;&#xA;      Brookhaven Protein Data Bank          pdb|entry|chain&#xD;&#xA;      Patents                               pat|country|number &#xD;&#xA;      GenInfo Backbone Id                   bbs|number &#xD;&#xA;      General database identifier           gnl|database|identifier&#xD;&#xA;      NCBI Reference Sequence               ref|accession|locus&#xD;&#xA;      Local Sequence identifier             lcl|identifier&#xD;&#xA;     &#xD;&#xA;    &#xD;&#xA;    For example, an identifier might be &quot;gb|M73307|AGMA13GT&quot;, where the &quot;gb&quot; tag&#xD;&#xA;    indicates that the identifier refers to a GenBank sequence, &quot;M73307&quot; is its&#xD;&#xA;    GenBank ACCESSION, and &quot;AGMA13GT&quot; is the GenBank LOCUS.  &#xD;&#xA;    &#xD;&#xA;    &quot;gi&quot; identifiers are being assigned by NCBI for all sequences contained&#xD;&#xA;    within NCBI's sequence databases.  The 'gi' identifier provides a uniform&#xD;&#xA;    and stable naming convention whereby a specific sequence is assigned&#xD;&#xA;    its unique gi identifier.  If a nucleotide or protein sequence changes,&#xD;&#xA;    however, a new gi identifier is assigned, even if the accession number&#xD;&#xA;    of the record remains unchanged. Thus gi identifiers provide a mechanism&#xD;&#xA;    for identifying the exact sequence that was used or retrieved in a&#xD;&#xA;    given search.&#xD;&#xA;    &#xD;&#xA;    For searches of the nr protein database where the sequences are derived&#xD;&#xA;    from conceptual translations of sequences from the nucleotide databases&#xD;&#xA;    the following syntax is used:&#xD;&#xA;    &#xD;&#xA;                         gi|gi_identifier&#xD;&#xA;    &#xD;&#xA;    An example would be:&#xD;&#xA;    &#xD;&#xA;            gi|451623           (U04987) env [Simian immunodeficiency...&quot;&#xD;&#xA;    &#xD;&#xA;    where '451623' is the gi identifier and the 'U04987' is the accession&#xD;&#xA;    number of the nucleotide sequence from which it was derived.&#xD;&#xA;    &#xD;&#xA;    Users are encouraged to use the '-I' option for Blast output which will&#xD;&#xA;    produce a header line with the gi identifier concatenated with the database&#xD;&#xA;    identifier of the database from which it was derived, for example, from a&#xD;&#xA;    nucleotide database:&#xD;&#xA;    &#xD;&#xA;            gi|176485|gb|M73307|AGMA13GT&#xD;&#xA;    &#xD;&#xA;    And similarly for protein databases: &#xD;&#xA;    &#xD;&#xA;            gi|129295|sp|P01013|OVAX_CHICK&#xD;&#xA;    &#xD;&#xA;    The gnl ('general') identifier allows databases not on the above list to&#xD;&#xA;    be identified with the same syntax.  An example here is the PID identifier:&#xD;&#xA;    &#xD;&#xA;            gnl|PID|e1632&#xD;&#xA;    &#xD;&#xA;    PID stands for Protein-ID; the &quot;e&quot; (in e1632) indicates that this ID&#xD;&#xA;    was issued by EMBL.  As mentioned above, use of the &quot;-I&quot; option&#xD;&#xA;    produces the NCBI gi (in addition to the PID) which users can also&#xD;&#xA;    retrieve on.&#xD;&#xA;    &#xD;&#xA;    The bar (&quot;|&quot;) separates different fields as listed in the above table.&#xD;&#xA;    In some cases a field is left empty, even though the original&#xD;&#xA;    specification called for including this field.  To make&#xD;&#xA;    these identifiers backwards-compatiable for older parsers&#xD;&#xA;    the empty field is denoted by an additional bar (&quot;||&quot;)." />
  <row Id="4629" PostHistoryTypeId="2" PostId="2160" RevisionGUID="f2b2686f-3d0a-4cb5-a0d9-11fe66d9a70f" CreationDate="2017-07-26T17:39:51.333" UserId="96" Text="Based on my experience reviewing k-mer counting software and as a core contributor to the khmer project, I can confidently say that there is no widely used standard format." />
  <row Id="4630" PostHistoryTypeId="2" PostId="2161" RevisionGUID="0f90a62a-236a-4531-ba11-6f364c529719" CreationDate="2017-07-26T17:47:55.083" UserId="1203" Text="if the data is in SRA, there is sra-stat utility that returns reads,bases and quality distribution. these are stored in the SRA file. use --quick to get the stored stats or --statistics to calculate additional values broken down per readgroup/barcode.&#xD;&#xA;sra-stat --quick SRR077487" />
  <row Id="4631" PostHistoryTypeId="2" PostId="2162" RevisionGUID="f9f1f186-4245-405f-bd5d-6bfe7769e6d4" CreationDate="2017-07-26T20:45:29.493" UserId="77" Text="$C_k$ is defined as the number of reads containing a k-mer. The fraction of a read available to contain a k-mer is $(R-K+1)/R$, which is the number of k-mers in the read divided by its length. That times the nucleotide coverage ($C$) is then the expected coverage of the k-mer." />
  <row Id="4632" PostHistoryTypeId="2" PostId="2163" RevisionGUID="442c0aed-e93f-4637-b569-3e8d12b0070e" CreationDate="2017-07-26T20:50:10.633" UserId="1194" Text="I have a code below:&#xD;&#xA;&#xD;&#xA;    def FilterReads(in_file, out_file):&#xD;&#xA;&#xD;&#xA;    def read_ok(read):&#xD;&#xA;        &quot;&quot;&quot;&#xD;&#xA;        read_ok - reject reads with a low quality (&lt;5) base call&#xD;&#xA;        read - a PySam AlignedRead object&#xD;&#xA;        returns: True if the read is ok&#xD;&#xA;        &quot;&quot;&quot;&#xD;&#xA;        if any([ord(c)-33 &lt; _BASE_QUAL_CUTOFF for c in list(read.qual)]):&#xD;&#xA;            return False&#xD;&#xA;        else:&#xD;&#xA;            return True&#xD;&#xA;&#xD;&#xA;    _BASE_QUAL_CUTOFF = 30&#xD;&#xA;&#xD;&#xA;    bam_in = pysam.Samfile(in_file, 'rb')&#xD;&#xA;    bam_out = pysam.Samfile(out_file, 'wb', template=bam_in)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    for read in bam_in.fetch():&#xD;&#xA;        if read_ok(read):&#xD;&#xA;            bam_out.write(read)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;This code works fine by &#xD;&#xA;&#xD;&#xA; - Rejecting reads having a base with phred quality score below five&#xD;&#xA; - But it first takes a BAM file &#xD;&#xA; - And then creates a filtered BAM file for further analysis but this takes a lot of time. &#xD;&#xA;&#xD;&#xA;So is there a way to reject these reads using pileup in pysam so that I may not have to create a file then sort it and again read it? &#xD;&#xA;Or can I modify [this code][1] to perform the same function&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioinformatics.stackexchange.com/questions/2131/filtering-bases-based-on-phred-qualities-with-pysam" />
  <row Id="4633" PostHistoryTypeId="1" PostId="2163" RevisionGUID="442c0aed-e93f-4637-b569-3e8d12b0070e" CreationDate="2017-07-26T20:50:10.633" UserId="1194" Text="Reject reads with low quality bases from a Bam file through pysam" />
  <row Id="4634" PostHistoryTypeId="3" PostId="2163" RevisionGUID="442c0aed-e93f-4637-b569-3e8d12b0070e" CreationDate="2017-07-26T20:50:10.633" UserId="1194" Text="&lt;bam&gt;&lt;ngs&gt;&lt;alignment&gt;&lt;python&gt;&lt;pysam&gt;" />
  <row Id="4635" PostHistoryTypeId="2" PostId="2164" RevisionGUID="9cd72bb2-74f7-4ed1-88c3-ab3176a12e5e" CreationDate="2017-07-26T22:03:37.427" UserId="727" Text="I'm attempting to visualize the results of my BLAST search in a way similar to the graphical display of the distribution of blast hits from the web BLAST.&#xD;&#xA;&#xD;&#xA;For example, from my BLAST search:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;except I would like the graph to contain information including information for the distribution of all 6139 hits across my query sequence.&#xD;&#xA;&#xD;&#xA;Since this is a large number of sequences to visualize a distribution, I am considering using a score of the number of hits on a certain region, and having an output more similar to:&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/zVWjr.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/HhPYg.png&#xD;&#xA;&#xD;&#xA;If I were to run my blast using the BLAST+ command line tool, and read the results into R, which parts of the output should I plot in order to recreate the graphical display of the distribution? Ideally I would like to make a reusable object that can make an equivalent plot for any BLAST input.&#xD;&#xA;&#xD;&#xA;Thanks." />
  <row Id="4636" PostHistoryTypeId="1" PostId="2164" RevisionGUID="9cd72bb2-74f7-4ed1-88c3-ab3176a12e5e" CreationDate="2017-07-26T22:03:37.427" UserId="727" Text="creating graph of distribution of blast hits on the query sequence" />
  <row Id="4637" PostHistoryTypeId="3" PostId="2164" RevisionGUID="9cd72bb2-74f7-4ed1-88c3-ab3176a12e5e" CreationDate="2017-07-26T22:03:37.427" UserId="727" Text="&lt;r&gt;&lt;blast&gt;" />
  <row Id="4638" PostHistoryTypeId="5" PostId="2124" RevisionGUID="0ba80f8c-c103-498d-b735-df7bcaf01ad2" CreationDate="2017-07-26T23:01:09.873" UserId="203" Comment="Completion of the unfinished sentence." Text="**EDIT:** &#xD;&#xA;&#xD;&#xA;Actually, after having a more thorough look into the documentation; `REMARK 3` might be the more relevant to what you need. Other tools are reported in that section. &#xD;&#xA;&gt;REMARK 3 presents information on refinement program(s) used and related statistics. For non-diffraction studies, REMARK 3 is used to describe any refinement done, but its format is mostly free text.&#xD;&#xA;&#xD;&#xA;[More on `Remark 3`][4]&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;From the same source as you reference yourself [v 3.3][1], I would argue that &#xD;&#xA;`REMARK 0` would be more accurate than `REMARK 250`.&#xD;&#xA;&#xD;&#xA;&gt; REMARK 0 (updated), Re-refinement notice&#xD;&#xA;&gt; REMARK 0 identifies entries in which a re-refinement has been performed using the data from an existing entry. This remark also describes the PDB code and the journal records for the original data set.&#xD;&#xA;&#xD;&#xA;While when you consider the definition of `REMARK 250`&#xD;&#xA;&#xD;&#xA;-- which follows directly:&#xD;&#xA;&#xD;&#xA; - `REMARK 205` (specific to Fiber diffraction experiment), &#xD;&#xA; - `REMARK 201`(and `REMARK 215/217` all specific to NMR experiment),&#xD;&#xA; - `REMARK 230` (neutron diffraction study), &#xD;&#xA; - `REMARK 240`(electron crystallography study) and &#xD;&#xA; - `REMARK 245`(and `REMARK 247`: specific to EM study)&#xD;&#xA;&#xD;&#xA;&gt;REMARK 250, Other Type of Experiment Details&#xD;&#xA;&#xD;&#xA;&gt;REMARKs specific to other kinds of studies, not listed above. &#xD;&#xA;REMARK 250 is mandatory if other than X-ray, NMR, neutron, or electron study. The format of the date in this remark is DD-MMM-YY. DD is the day of the month (a number 01 through 31), MMM is the English 3-letter abbreviation for the month, and YY is the year.&#xD;&#xA;&#xD;&#xA;and then `REMARK 265` is also about the crystallography experiment.&#xD;&#xA;&#xD;&#xA;Hence, I know that personally, I usually assume that whatever is annotated in `REMARK 250`is relevant to the first acquisition experiment to me.&#xD;&#xA;&#xD;&#xA;Another reason I am more inclined to think `REMARK 0`is more accurate, it is due that the definition of `REMARK 0` was __updated__ while `REMARK 6-99` are _are no longer for use of free text annotation_.&#xD;&#xA;&#xD;&#xA;BTW, I am not sure how [Pdb_extract][2] would do with the remarks when converting the PDB file to CIF files. It would probably worth perusing  [PDB to PDBx/mmCIF Data Item Correspondences][3] page.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.wwpdb.org/documentation/file-format-content/format33/remarks1.html &quot;v 3.3&quot;&#xD;&#xA;  [2]: http://pdb-extract.wwpdb.org/ &quot;Pdb_extract&quot;&#xD;&#xA;  [3]: http://mmcif.wwpdb.org/docs/pdb_to_pdbx_correspondences.html#XPLOR &quot;PDB to PDBx/mmCIF Data Item Correspondences&quot;&#xD;&#xA;  [4]: http://www.wwpdb.org/documentation/file-format-content/format33/remark3.html &quot;More on `Remark 3`&quot;" />
  <row Id="4639" PostHistoryTypeId="2" PostId="2165" RevisionGUID="23c26ab2-83d4-41fe-a1d0-e7e0191b4bbd" CreationDate="2017-07-27T02:14:57.477" UserId="506" Text="I have RNA seq data which I've quantified using Kallisto.  I'd like to use tximport to transform the read count data into input for EdgeR, following the R code supplied in the tximport documentation:&#xD;&#xA;&#xD;&#xA;    cts &lt;- txi$counts&#xD;&#xA;    normMat &lt;- txi$length&#xD;&#xA;    normMat &lt;- normMat/exp(rowMeans(log(normMat)))&#xD;&#xA;    library(edgeR)&#xD;&#xA;    o &lt;- log(calcNormFactors(cts/normMat)) + log(colSums(cts/normMat))&#xD;&#xA;    y &lt;- DGEList(cts)&#xD;&#xA;    y$offset &lt;- t(t(log(normMat)) + o)&#xD;&#xA;    # y is now ready for estimate dispersion functions see edgeR User's Guide&#xD;&#xA;&#xD;&#xA;I would ideally follow this up with EdgeR's exactTest.  However, I'd also like to remove genes with low read counts using code such as this:&#xD;&#xA;&#xD;&#xA;    filterCounts &lt;- function(counts) {&#xD;&#xA;      cpms &lt;- cpm(counts)&#xD;&#xA;      keep &lt;- rowSums(cpms &gt; 1) &gt;= 1&#xD;&#xA;      counts[keep,]&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;The only issue is that I can't tell whether I should remove the genes with low read counts before or after I normality the counts while importing with tximport in the first code.  I'm thinking that it would be best to do this before, editing out the abundance, counts, and length rows of the txi data frame which correspond to lowly expressed genes, but I woul dlik ethis to be confirmed before I proceed." />
  <row Id="4640" PostHistoryTypeId="1" PostId="2165" RevisionGUID="23c26ab2-83d4-41fe-a1d0-e7e0191b4bbd" CreationDate="2017-07-27T02:14:57.477" UserId="506" Text="When performing differential expression analysis, should genes with low read counts be removed before or after normalization?" />
  <row Id="4641" PostHistoryTypeId="3" PostId="2165" RevisionGUID="23c26ab2-83d4-41fe-a1d0-e7e0191b4bbd" CreationDate="2017-07-27T02:14:57.477" UserId="506" Text="&lt;rna-seq&gt;&lt;edger&gt;" />
  <row Id="4642" PostHistoryTypeId="6" PostId="2164" RevisionGUID="c4e0a8b2-2602-415e-a036-2d5fa04f0eb9" CreationDate="2017-07-27T03:45:52.203" UserId="96" Comment="Added visualization tag" Text="&lt;r&gt;&lt;blast&gt;&lt;visualization&gt;" />
  <row Id="4643" PostHistoryTypeId="6" PostId="884" RevisionGUID="3771332b-2e84-4231-9da2-ab3b07f925e2" CreationDate="2017-07-27T04:05:00.873" UserId="96" Comment="Added file-formats tag" Text="&lt;file-formats&gt;&lt;pdb&gt;" />
  <row Id="4644" PostHistoryTypeId="2" PostId="2166" RevisionGUID="6fb5361c-f148-4072-9f48-8657aa1ca3c2" CreationDate="2017-07-27T06:37:14.703" UserId="77" Text="The more genes you have the more robust the scaling factor is (among the reason why one doesn't normalize to ERCC spike-ins without a compelling reason), so I suppose in theory it's better to filter after determining the scale factor. Having said that, I'd be surprised if the results changed much either way. Unless you end up filtering out a LOT of genes the scaling factors should be fairly robust to the change." />
  <row Id="4647" PostHistoryTypeId="2" PostId="2167" RevisionGUID="06264f49-a5ac-46eb-b719-61f3c2acddb2" CreationDate="2017-07-27T08:55:09.647" UserId="939" Text="You probably want to include query start (qstart) and query end (qend) in your blast output.&#xD;&#xA;&#xD;&#xA;Something like this:&#xD;&#xA;&#xD;&#xA;    blastn -query your.fasta -out blast.out.txt -db your.db -outfmt '6 qseqid sseqid qstart qend length evalue'&#xD;&#xA;&#xD;&#xA;In R you can take the &quot;qstart:qend&quot; from each line for density plot.&#xD;&#xA;&#xD;&#xA; " />
  <row Id="4648" PostHistoryTypeId="6" PostId="2153" RevisionGUID="400141bf-ca10-4ea7-8606-a7ebd01abb71" CreationDate="2017-07-27T09:11:20.577" UserId="203" Comment="added the database tag" Text="&lt;database&gt;&lt;gene&gt;&lt;proteins&gt;&lt;protein-structure&gt;&lt;rna-structure&gt;" />
  <row Id="4649" PostHistoryTypeId="24" PostId="2153" RevisionGUID="400141bf-ca10-4ea7-8606-a7ebd01abb71" CreationDate="2017-07-27T09:11:20.577" Comment="Proposed by 203 approved by 96, 29 edit id of 241" />
  <row Id="4650" PostHistoryTypeId="2" PostId="2168" RevisionGUID="3e89e020-6779-4f2b-8cce-6a0511d802f4" CreationDate="2017-07-27T10:02:08.870" UserId="57" Text="I was still puzzled from the answers, so I tried to calculate with all the steps. I take this definition &quot;$C_k$ is the number of reads containing a k-mer.&quot; and corresponding definition for coverage ($C$): &quot;$C$ is the number of reads covering a base&quot;.&#xD;&#xA;&#xD;&#xA;Coverage is $C = \frac{T \cdot R}{L}$, where $T$ is total number of reads, $R$ is read length and $L$ is length of genome. Given the $C_k$ definition, $C_k = \frac{T (R - K + 1)}{L-K+1}$, where $R - K + 1$ is just number of kmers in a read, and $L-K+1$ is number of kmers in a genome (assuming all kmers are unique). Then,&#xD;&#xA;&#xD;&#xA;$$C_k = \frac{T (R - K + 1)}{L-K+1} = \frac{T (R - K + 1)}{L-K+1} \cdot \frac{R}{R} = \frac{R - K + 1}{R} \cdot \frac{T \cdot R}{L - K + 1}$$&#xD;&#xA;&#xD;&#xA;since $L &gt;&gt; K$, we can approximate $L - K + 1 \approx L$, then we reduce the expression to&#xD;&#xA;&#xD;&#xA;$$\frac{R - K + 1}{R} \cdot \frac{T \cdot R}{L} = \frac{R - K + 1}{R} \cdot C$$&#xD;&#xA;&#xD;&#xA;which is the formula for $C_k$." />
  <row Id="4651" PostHistoryTypeId="2" PostId="2169" RevisionGUID="3e9cf851-0771-4e6a-9657-f56d372a14a0" CreationDate="2017-07-27T10:52:01.277" UserId="872" Text="I am a student trying to analyze GEO2r datas for one of my courses.&#xD;&#xA;The IDs given in the output are different for different series. I need to convert all of them to a similar format.&#xD;&#xA;&#xD;&#xA;In this process I encountered the following type of ID which I don't know its origin:&#xD;&#xA;&#xD;&#xA;    ”ID&quot;	&quot;logFC&quot;&#xD;&#xA;    &quot;SP_v2 4634&quot;	&quot;-0.9897758&quot;&#xD;&#xA;    &quot;SP_v2 3382&quot;	&quot;-0.8391782&quot;&#xD;&#xA;    &quot;SP_v2 4210&quot;	&quot;-1.1693583&quot;&#xD;&#xA;    &quot;SP_v2 2117&quot;	&quot;-1.0504727&quot;&#xD;&#xA;    &quot;SP_v2 3488&quot;	&quot;-0.9756444&quot;&#xD;&#xA;    &quot;SP_v2 1128&quot;	&quot;-0.8289103&quot;&#xD;&#xA;    &quot;SP_v2 2735&quot;	&quot;-0.8629999&quot;&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;Each one of the rows represent a single gene.&#xD;&#xA;My question is that what is this ID?" />
  <row Id="4652" PostHistoryTypeId="1" PostId="2169" RevisionGUID="3e9cf851-0771-4e6a-9657-f56d372a14a0" CreationDate="2017-07-27T10:52:01.277" UserId="872" Text="What database do gene IDs starting with sp_v2 refer to?" />
  <row Id="4653" PostHistoryTypeId="3" PostId="2169" RevisionGUID="3e9cf851-0771-4e6a-9657-f56d372a14a0" CreationDate="2017-07-27T10:52:01.277" UserId="872" Text="&lt;gene&gt;" />
  <row Id="4654" PostHistoryTypeId="5" PostId="2168" RevisionGUID="56db69be-0cd1-4271-8d86-ab55164c6ce1" CreationDate="2017-07-27T10:54:57.303" UserId="57" Comment="small correciton" Text="I was still puzzled from the answers, so I tried to calculate with all the steps. I take this definition &quot;$C_k$ is the number of reads containing a k-mer.&quot; and corresponding definition for coverage ($C$): &quot;$C$ is the number of reads covering a base&quot;.&#xD;&#xA;&#xD;&#xA;Coverage is $C = \frac{T \cdot R}{L}$, where $T$ is total number of reads, $R$ is read length and $L$ is length of genome. Given the $C_k$ definition, $C_k = \frac{T (R - K + 1)}{L-K+1}$, where $R - K + 1$ is just number of kmers in a read, and $L-K+1$ is number of kmers in a genome. Then,&#xD;&#xA;&#xD;&#xA;$$C_k = \frac{T (R - K + 1)}{L-K+1} = \frac{T (R - K + 1)}{L-K+1} \cdot \frac{R}{R} = \frac{R - K + 1}{R} \cdot \frac{T \cdot R}{L - K + 1}$$&#xD;&#xA;&#xD;&#xA;since $L &gt;&gt; K$, we can approximate $L - K + 1 \approx L$, then we reduce the expression to&#xD;&#xA;&#xD;&#xA;$$\frac{R - K + 1}{R} \cdot \frac{T \cdot R}{L} = \frac{R - K + 1}{R} \cdot C$$&#xD;&#xA;&#xD;&#xA;which is the formula for $C_k$." />
  <row Id="4655" PostHistoryTypeId="2" PostId="2170" RevisionGUID="adabe033-2b5c-402e-a6f7-7da68390a599" CreationDate="2017-07-27T12:21:49.380" UserId="294" Text="Is there any extensive documentation or description for each class of Transfrag class codes as reported by the cuffcompare tool in the Cufflinks package?&#xD;&#xA;&#xD;&#xA;[Official doc][1] might not be the best. &#xD;&#xA;&#xD;&#xA;E.g. what does *contained* mean (class code: c)? Or, what is a *generic exonic overlap* (class code: o)?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cole-trapnell-lab.github.io/cufflinks/cuffcompare/#transfrag-class-codes" />
  <row Id="4656" PostHistoryTypeId="1" PostId="2170" RevisionGUID="adabe033-2b5c-402e-a6f7-7da68390a599" CreationDate="2017-07-27T12:21:49.380" UserId="294" Text="Documentation for Transfrag class codes (cuffcompare)" />
  <row Id="4657" PostHistoryTypeId="3" PostId="2170" RevisionGUID="adabe033-2b5c-402e-a6f7-7da68390a599" CreationDate="2017-07-27T12:21:49.380" UserId="294" Text="&lt;transcriptome&gt;" />
  <row Id="4658" PostHistoryTypeId="2" PostId="2171" RevisionGUID="be05a184-04bc-47a6-a718-8476988d9d21" CreationDate="2017-07-27T12:27:29.010" UserId="194" Text="This is possibly a wild goose chase, but a lot of searching led me to this sample: &#xD;&#xA;&#xD;&#xA;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM2536852&#xD;&#xA;&#xD;&#xA;Which appears to exhibit accessions of the right format. This in turn leads to the platform:&#xD;&#xA;&#xD;&#xA;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL22166&#xD;&#xA;&#xD;&#xA;Which is a custom spotted cDNA array, which the metadata seems to suggest is a human platform.&#xD;&#xA;&#xD;&#xA;The full annotation table gives gene symbols and Entrez IDs for most of the probes on the array:&#xD;&#xA;&#xD;&#xA;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?view=data&amp;acc=GPL22166&amp;id=53508&amp;db=GeoDb_blob144&#xD;&#xA;&#xD;&#xA;To take the example of the first line of your results:&#xD;&#xA;&#xD;&#xA;    ID	        ORF	    Entrez gene	SEQUENCE&#xD;&#xA;    SP_v2 4634	ETNK1	55500	    AAAGCAGCTTCATCTTTCAAAATTGATTTGCTCTGGTTTT&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The Entrez Gene record for that ID matches up:&#xD;&#xA;&#xD;&#xA;https://www.ncbi.nlm.nih.gov/gene/?term=55500&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4659" PostHistoryTypeId="2" PostId="2172" RevisionGUID="1df6f25b-959a-47c2-bbd1-c129318b0c7b" CreationDate="2017-07-27T12:47:53.153" UserId="77" Text="You can get a bit more information by [looking at the source code comments](https://github.com/cole-trapnell-lab/cufflinks/blob/b55bb214ad4abeb58a1435d8f7a7f0dd8bed8b76/src/gff_utils.cpp). [Contained](https://github.com/cole-trapnell-lab/cufflinks/blob/b55bb214ad4abeb58a1435d8f7a7f0dd8bed8b76/src/gff_utils.cpp#L140-L141) is for single exon transcripts that are primarily contained ([&gt;80% overlap](https://github.com/cole-trapnell-lab/cufflinks/blob/b55bb214ad4abeb58a1435d8f7a7f0dd8bed8b76/src/gff_utils.cpp#L145)) within an exon of an already annotated transcript. `Generic exonic overlap` is for cases where a transfrag exon overlaps that of an annotated transcript. The strand doesn't seem to matter in such cases." />
  <row Id="4661" PostHistoryTypeId="5" PostId="2169" RevisionGUID="86ddf9a1-3055-4ca3-828f-473e9f25e843" CreationDate="2017-07-27T13:03:53.463" UserId="298" Comment="added 31 characters in body; edited tags" Text="I am a student trying to analyze GEO2r datas for one of my courses.&#xD;&#xA;The IDs given in the output are different for different series. I need to convert all of them to a similar format.&#xD;&#xA;&#xD;&#xA;In this process I encountered the following type of ID which I don't know its origin:&#xD;&#xA;&#xD;&#xA;    ”ID&quot;	&quot;logFC&quot;&#xD;&#xA;    &quot;SP_v2 4634&quot;	&quot;-0.9897758&quot;&#xD;&#xA;    &quot;SP_v2 3382&quot;	&quot;-0.8391782&quot;&#xD;&#xA;    &quot;SP_v2 4210&quot;	&quot;-1.1693583&quot;&#xD;&#xA;    &quot;SP_v2 2117&quot;	&quot;-1.0504727&quot;&#xD;&#xA;    &quot;SP_v2 3488&quot;	&quot;-0.9756444&quot;&#xD;&#xA;    &quot;SP_v2 1128&quot;	&quot;-0.8289103&quot;&#xD;&#xA;    &quot;SP_v2 2735&quot;	&quot;-0.8629999&quot;&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;Each one of the rows represent a single gene.&#xD;&#xA;My question is that what is this ID? The GEO accession is GSE97750." />
  <row Id="4662" PostHistoryTypeId="6" PostId="2169" RevisionGUID="86ddf9a1-3055-4ca3-828f-473e9f25e843" CreationDate="2017-07-27T13:03:53.463" UserId="298" Comment="added 31 characters in body; edited tags" Text="&lt;identifiers&gt;" />
  <row Id="4665" PostHistoryTypeId="5" PostId="2165" RevisionGUID="2c14a8d8-7257-4889-a23a-1e29f070a103" CreationDate="2017-07-27T14:14:31.220" UserId="29" Comment="syntax highlighting" Text="I have RNA seq data which I've quantified using Kallisto.  I'd like to use tximport to transform the read count data into input for EdgeR, following the R code supplied in the tximport documentation:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    cts &lt;- txi$counts&#xD;&#xA;    normMat &lt;- txi$length&#xD;&#xA;    normMat &lt;- normMat/exp(rowMeans(log(normMat)))&#xD;&#xA;    library(edgeR)&#xD;&#xA;    o &lt;- log(calcNormFactors(cts/normMat)) + log(colSums(cts/normMat))&#xD;&#xA;    y &lt;- DGEList(cts)&#xD;&#xA;    y$offset &lt;- t(t(log(normMat)) + o)&#xD;&#xA;    # y is now ready for estimate dispersion functions see edgeR User's Guide&#xD;&#xA;&#xD;&#xA;I would ideally follow this up with EdgeR's exactTest.  However, I'd also like to remove genes with low read counts using code such as this:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    filterCounts &lt;- function(counts) {&#xD;&#xA;      cpms &lt;- cpm(counts)&#xD;&#xA;      keep &lt;- rowSums(cpms &gt; 1) &gt;= 1&#xD;&#xA;      counts[keep,]&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;The only issue is that I can't tell whether I should remove the genes with low read counts before or after I normality the counts while importing with tximport in the first code.  I'm thinking that it would be best to do this before, editing out the abundance, counts, and length rows of the txi data frame which correspond to lowly expressed genes, but I would like this to be confirmed before I proceed." />
  <row Id="4666" PostHistoryTypeId="5" PostId="598" RevisionGUID="8dbf5ca8-cc39-4462-8677-fe08485c941a" CreationDate="2017-07-27T19:35:04.413" UserId="776" Comment="added 120 characters in body" Text="I wrote a command-line k-mer counter called `kmer-counter` that will output results in a form that your Python script can consume: https://github.com/alexpreynolds/kmer-counter&#xD;&#xA;&#xD;&#xA;You can grab, build and install it like so:&#xD;&#xA;&#xD;&#xA;    $ git clone https://github.com/alexpreynolds/kmer-counter.git&#xD;&#xA;    $ cd kmer-counter&#xD;&#xA;    $ make&#xD;&#xA;    $ cp kmer-counter /usr/local/bin&#xD;&#xA;&#xD;&#xA;Once the binary is in your path, you might use it in Python like so:&#xD;&#xA;&#xD;&#xA;    k = 6&#xD;&#xA;    fastaFile = '/path/to/some/seqs.fa'&#xD;&#xA;    kmerCmd = 'kmer-counter --fasta --k=%d %s' % (k, fastaFile)&#xD;&#xA;    try:&#xD;&#xA;        output = subprocess.check_output(kmerCmd, shell=True)&#xD;&#xA;        result = {}&#xD;&#xA;        for line in output.splitlines():&#xD;&#xA;            (header, counts) = line.strip().split('\t')&#xD;&#xA;            header = header[1:]&#xD;&#xA;            kmers = dict((k,int(v)) for (k,v) in [d.split(':') for d in counts.split(' ')])&#xD;&#xA;            result[header] = kmers&#xD;&#xA;        sys.stdout.write(&quot;%s&quot; % (str(result)))&#xD;&#xA;    except subprocess.CalledProcessError as error:&#xD;&#xA;        sys.stderr.write(&quot;%s&quot; % (str(error)))&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Given example FASTA like this:&#xD;&#xA;&#xD;&#xA;    &gt;foo&#xD;&#xA;    TTAACG&#xD;&#xA;    &gt;bar&#xD;&#xA;    GTGGAAGTTCTTAGGGCATGGCAAAGAGTCAGAATTTGAC&#xD;&#xA;&#xD;&#xA;For k=6, you would get an iterable Python dictionary like this:&#xD;&#xA;&#xD;&#xA;    {'foo': {'TTAACG': 1, 'CGTTAA': 1}, 'bar': {'GTTCTT': 1, 'AGAACT': 1, 'GAGTCA': 1, 'ATGGCA': 1, 'GAACTT': 1, 'ATTCTG': 1, 'CTAAGA': 1, 'CTTCCA': 1, 'ATTTGA': 1, 'GGAAGT': 1, 'AGGGCA': 1, 'CCTAAG': 1, 'CTCTTT': 1, 'AATTTG': 1, 'TCTGAC': 1, 'TTTGCC': 1, 'CTTAGG': 1, 'TTTGAC': 1, 'GAAGTT': 1, 'CCCTAA': 1, 'AGAATT': 1, 'AGTCAG': 1, 'CTGACT': 1, 'TCTTAG': 1, 'CGTTAA': 1, 'GTGGAA': 1, 'TGCCAT': 1, 'ACTCTT': 1, 'GGGCAT': 1, 'TTAGGG': 1, 'CTTTGC': 1, 'TGGAAG': 1, 'GACTCT': 1, 'CATGCC': 1, 'GCAAAG': 1, 'AAATTC': 1, 'GTCAAA': 1, 'TGACTC': 1, 'TAGGGC': 1, 'AAGTTC': 1, 'ATGCCC': 1, 'TCAAAT': 1, 'CAAAGA': 1, 'AACTTC': 1, 'GTCAGA': 1, 'CAAATT': 1, 'TAAGAA': 1, 'CATGGC': 1, 'AAGAAC': 1, 'AAGAGT': 1, 'TCTTTG': 1, 'TTCCAC': 1, 'TGGCAA': 1, 'GGCAAA': 1, 'AGTTCT': 1, 'AGAGTC': 1, 'TCAGAA': 1, 'GAATTT': 1, 'AAAGAG': 1, 'TGCCCT': 1, 'CCATGC': 1, 'GGCATG': 1, 'TTGCCA': 1, 'CAGAAT': 1, 'AATTCT': 1, 'GCATGG': 1, 'ACTTCC': 1, 'TTCTTA': 1, 'GCCATG': 1, 'GCCCTA': 1, 'TTCTGA': 1}}&#xD;&#xA;&#xD;&#xA;You can use standard Python calls to manipulate this directory and get sums of counts per record, for sequence, etc. which seems to answer your question. Please feel free to clarify what you're looking for if this object representation is not clear.&#xD;&#xA;&#xD;&#xA;For a fully-fleshed out demonstration, see: https://github.com/alexpreynolds/kmer-counter/blob/master/test/kmer-test.py" />
  <row Id="4667" PostHistoryTypeId="5" PostId="598" RevisionGUID="3e3c8e4d-c701-431a-bed6-d8cf8edf7d0b" CreationDate="2017-07-27T20:08:44.917" UserId="776" Comment="added 8 characters in body" Text="I wrote a command-line k-mer counter called `kmer-counter` that will output results in a form that your Python script can consume: https://github.com/alexpreynolds/kmer-counter&#xD;&#xA;&#xD;&#xA;You can grab, build and install it like so:&#xD;&#xA;&#xD;&#xA;    $ git clone https://github.com/alexpreynolds/kmer-counter.git&#xD;&#xA;    $ cd kmer-counter&#xD;&#xA;    $ make&#xD;&#xA;    $ cp kmer-counter /usr/local/bin&#xD;&#xA;&#xD;&#xA;Once the binary is in your path, you might use it in Python like so:&#xD;&#xA;&#xD;&#xA;    k = 6&#xD;&#xA;    fastaFile = '/path/to/some/seqs.fa'&#xD;&#xA;    kmerCmd = 'kmer-counter --fasta --k=%d %s' % (k, fastaFile)&#xD;&#xA;    try:&#xD;&#xA;        output = subprocess.check_output(kmerCmd, shell=True)&#xD;&#xA;        result = {}&#xD;&#xA;        for line in output.splitlines():&#xD;&#xA;            (header, counts) = line.strip().split('\t')&#xD;&#xA;            header = header[1:]&#xD;&#xA;            kmers = dict((k,int(v)) for (k,v) in [d.split(':') for d in counts.split(' ')])&#xD;&#xA;            result[header] = kmers&#xD;&#xA;        sys.stdout.write(&quot;%s&quot; % (str(result)))&#xD;&#xA;    except subprocess.CalledProcessError as error:&#xD;&#xA;        sys.stderr.write(&quot;%s&quot; % (str(error)))&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;Given example FASTA like this:&#xD;&#xA;&#xD;&#xA;    &gt;foo&#xD;&#xA;    TTAACG&#xD;&#xA;    &gt;bar&#xD;&#xA;    GTGGAAGTTCTTAGGGCATGGCAAAGAGTCAGAATTTGAC&#xD;&#xA;&#xD;&#xA;For k=6, you would get an iterable Python dictionary like this:&#xD;&#xA;&#xD;&#xA;    {'foo': {'TTAACG': 1, 'CGTTAA': 1}, 'bar': {'GTTCTT': 1, 'AGAACT': 1, 'GAGTCA': 1, 'ATGGCA': 1, 'GAACTT': 1, 'ATTCTG': 1, 'CTAAGA': 1, 'CTTCCA': 1, 'ATTTGA': 1, 'GGAAGT': 1, 'AGGGCA': 1, 'CCTAAG': 1, 'CTCTTT': 1, 'AATTTG': 1, 'TCTGAC': 1, 'TTTGCC': 1, 'CTTAGG': 1, 'TTTGAC': 1, 'GAAGTT': 1, 'CCCTAA': 1, 'AGAATT': 1, 'AGTCAG': 1, 'CTGACT': 1, 'TCTTAG': 1, 'CGTTAA': 1, 'GTGGAA': 1, 'TGCCAT': 1, 'ACTCTT': 1, 'GGGCAT': 1, 'TTAGGG': 1, 'CTTTGC': 1, 'TGGAAG': 1, 'GACTCT': 1, 'CATGCC': 1, 'GCAAAG': 1, 'AAATTC': 1, 'GTCAAA': 1, 'TGACTC': 1, 'TAGGGC': 1, 'AAGTTC': 1, 'ATGCCC': 1, 'TCAAAT': 1, 'CAAAGA': 1, 'AACTTC': 1, 'GTCAGA': 1, 'CAAATT': 1, 'TAAGAA': 1, 'CATGGC': 1, 'AAGAAC': 1, 'AAGAGT': 1, 'TCTTTG': 1, 'TTCCAC': 1, 'TGGCAA': 1, 'GGCAAA': 1, 'AGTTCT': 1, 'AGAGTC': 1, 'TCAGAA': 1, 'GAATTT': 1, 'AAAGAG': 1, 'TGCCCT': 1, 'CCATGC': 1, 'GGCATG': 1, 'TTGCCA': 1, 'CAGAAT': 1, 'AATTCT': 1, 'GCATGG': 1, 'ACTTCC': 1, 'TTCTTA': 1, 'GCCATG': 1, 'GCCCTA': 1, 'TTCTGA': 1}}&#xD;&#xA;&#xD;&#xA;You can use standard Python calls to manipulate this dictionary object and get sums of counts per record, for sequence, etc. which seems to answer your question. Please feel free to clarify what you're looking for if this object representation is not clear.&#xD;&#xA;&#xD;&#xA;For a fully-fleshed out demonstration, see: https://github.com/alexpreynolds/kmer-counter/blob/master/test/kmer-test.py" />
  <row Id="4668" PostHistoryTypeId="50" PostId="765" RevisionGUID="937e124c-5e69-4310-a086-228f4c816e15" CreationDate="2017-07-27T21:31:05.903" UserId="-1" />
  <row Id="4669" PostHistoryTypeId="5" PostId="2143" RevisionGUID="40580af0-9c6c-4647-92e6-0f880dbced8c" CreationDate="2017-07-27T23:11:55.980" UserId="73" Comment="added 396 characters in body" Text="Not as far as I am aware. The Ray assembler used to (and possibly still does) store the kmers as FASTA files where the header was the count of the sequence, which I thought was a pretty neat bastardisation of the FASTA file format.&#xD;&#xA;&#xD;&#xA;[Jellyfish](http://www.genome.umd.edu/jellyfish.html) changed their format between v1 and v2 (both not FASTA), because they changed to doing counts based on bloom filters. Jellyfish2 has an optional two-pass method that sets up a bloom filter intermediate file to record kmers, and multiple different final reporting formats.&#xD;&#xA;&#xD;&#xA;[Khmer](http://khmer.readthedocs.io/en/v2.1.2/introduction.html) also uses bloom filters, but in a slightly different way. It also has been extended to be useful for partitioning and comparing datasets." />
  <row Id="4670" PostHistoryTypeId="5" PostId="2143" RevisionGUID="22be1768-be6c-459b-92e6-629bd47226db" CreationDate="2017-07-27T23:23:02.877" UserId="73" Comment="added 611 characters in body" Text="Not as far as I am aware. The Ray assembler used to (and possibly still does) store the kmers as FASTA files where the header was the count of the sequence, which I thought was a pretty neat bastardisation of the FASTA file format. It looks like this format is also used by Jellyfish when reporting kmer frequencies by the `dump` command:&#xD;&#xA;&#xD;&#xA;&gt; The dump subcommand outputs a list of all the k-mers in the file associated with their count. By default, the output is in FASTA format, where the header line contains the count of the k-mer and the sequence part is the sequence of the k-mer. This format has the advantage that the output contains the sequence of k-mers and can be directly fed into another program expecting the very common FASTA format. A more convenient column format (for human beings) is selected with the -c  switch.&#xD;&#xA;&#xD;&#xA;[Jellyfish](http://www.genome.umd.edu/jellyfish.html) changed their internal format between v1 and v2 (both not FASTA), because they changed to doing counts based on bloom filters. Jellyfish2 has an optional two-pass method that sets up a bloom filter intermediate file to record kmers, and multiple different final reporting formats.&#xD;&#xA;&#xD;&#xA;[Khmer](http://khmer.readthedocs.io/en/v2.1.2/introduction.html) also uses bloom filters, but in a slightly different way. It also has been extended to be useful for partitioning and comparing datasets." />
  <row Id="4671" PostHistoryTypeId="5" PostId="2143" RevisionGUID="f340c09f-8198-484f-b3ec-a2ad831895de" CreationDate="2017-07-28T01:33:46.580" UserId="73" Comment="added 58 characters in body" Text="Not as far as I am aware. The Ray assembler used to (and possibly still does) store the kmers as FASTA files where the header was the count of the sequence, which I thought was a pretty neat bastardisation of the FASTA file format. It looks like this format is also used by Jellyfish when reporting kmer frequencies by the `dump` command (but its default output format is a custom binary format):&#xD;&#xA;&#xD;&#xA;&gt; The dump subcommand outputs a list of all the k-mers in the file associated with their count. By default, the output is in FASTA format, where the header line contains the count of the k-mer and the sequence part is the sequence of the k-mer. This format has the advantage that the output contains the sequence of k-mers and can be directly fed into another program expecting the very common FASTA format. A more convenient column format (for human beings) is selected with the -c  switch.&#xD;&#xA;&#xD;&#xA;[Jellyfish](http://www.genome.umd.edu/jellyfish.html) changed their internal format between v1 and v2 (both not FASTA), because they changed to doing counts based on bloom filters. Jellyfish2 has an optional two-pass method that sets up a bloom filter intermediate file to record kmers, and multiple different final reporting formats.&#xD;&#xA;&#xD;&#xA;[Khmer](http://khmer.readthedocs.io/en/v2.1.2/introduction.html) also uses bloom filters, but in a slightly different way. It also has been extended to be useful for partitioning and comparing datasets." />
  <row Id="4672" PostHistoryTypeId="2" PostId="2173" RevisionGUID="c64a3170-86fb-4a6f-9992-61fed0fbcc36" CreationDate="2017-07-28T01:49:03.500" UserId="1218" Text="Take for instance, this hypothetical example:&#xD;&#xA;&#xD;&#xA;    bam &lt;- system.file(&quot;extdata&quot;,package = &quot;SomePackage&quot;,&quot;snps.bam&quot;)&#xD;&#xA;    reads &lt;- readGAlignments(bam)&#xD;&#xA;&#xD;&#xA;How do I display `reads` in a plot with [the second and fourth graph of this picture (mismatch and reads)](http://www.sthda.com/sthda/RDoc/figure/genomics/ggbio-tracks.png), and the [first graph of this picture?](https://image.slidesharecdn.com/di2011houston-121011145557-phpapp02/95/di-2011-houston-69-728.jpg?cb=1349967543)&#xD;&#xA;&#xD;&#xA;I've tried getting additional columns from readGAlignments, but it still seems to break.&#xD;&#xA;&#xD;&#xA;I've also tried using Gviz, but that doesn't support indels, which is crucial to my work." />
  <row Id="4673" PostHistoryTypeId="1" PostId="2173" RevisionGUID="c64a3170-86fb-4a6f-9992-61fed0fbcc36" CreationDate="2017-07-28T01:49:03.500" UserId="1218" Text="Using the ggbio package in R, how do you display a GenomicAlignments object as a mismatch plot and reads plot using autoplot?" />
  <row Id="4674" PostHistoryTypeId="3" PostId="2173" RevisionGUID="c64a3170-86fb-4a6f-9992-61fed0fbcc36" CreationDate="2017-07-28T01:49:03.500" UserId="1218" Text="&lt;r&gt;" />
  <row Id="4675" PostHistoryTypeId="5" PostId="2173" RevisionGUID="d3a85c24-ed29-4f90-a01d-01fa716e73db" CreationDate="2017-07-28T02:25:30.370" UserId="1218" Comment="added 225 characters in body" Text="Take for instance, this hypothetical example:&#xD;&#xA;&#xD;&#xA;    bam &lt;- system.file(&quot;extdata&quot;,package = &quot;SomePackage&quot;,&quot;snps.bam&quot;)&#xD;&#xA;    reads &lt;- readGAlignments(bam)&#xD;&#xA;&#xD;&#xA;How do I display `reads` in a plot with [the second and fourth graph of this picture (mismatch and reads)](http://www.sthda.com/sthda/RDoc/figure/genomics/ggbio-tracks.png), and the [first graph of this picture?](https://image.slidesharecdn.com/di2011houston-121011145557-phpapp02/95/di-2011-houston-69-728.jpg?cb=1349967543)&#xD;&#xA;&#xD;&#xA;I've tried getting additional columns from readGAlignments, but it still seems to break.&#xD;&#xA;&#xD;&#xA;I've also tried using Gviz, but that doesn't support indels, which is crucial to my work.&#xD;&#xA;&#xD;&#xA;Finally, is there a way to see individual mismatches on the reads like in the first graph of the second picture, or like in Gviz? I've looked at the varying graphs for ggbio and the karyogram seems like the closest thing." />
  <row Id="4676" PostHistoryTypeId="5" PostId="54" RevisionGUID="cd47095f-1b9f-44fd-b1c4-bfa7447be171" CreationDate="2017-07-28T02:29:08.227" UserId="163" Comment="Fix further math formatting" Text="You may consider using [RUVSeq][1]. Here is an excerpt from the [2013 Nature Biotechnology publication][2]:&#xD;&#xA;&#xD;&#xA;&gt; We evaluate the performance of the External RNA Control Consortium (ERCC) spike-in controls and investigate the possibility of using them directly for normalization. We show that the spike-ins are not reliable enough to be used in standard global-scaling or regression-based normalization procedures. We propose a normalization strategy, called remove unwanted variation (RUV), that adjusts for nuisance technical effects by performing factor analysis on suitable sets of control genes (e.g., ERCC spike-ins) or samples (e.g., replicate libraries).&#xD;&#xA;&#xD;&#xA;RUVSeq essentially fits a generalized linear model (GLM) to the expression data, where your expression matrix $Y$ is a $m$ by $n$ matrix, where $m$ is the number of samples and $n$ the number of genes. The model boils down to&#xD;&#xA;&#xD;&#xA;$Y = X*\beta + Z*\gamma + W*\alpha + \epsilon$&#xD;&#xA;&#xD;&#xA;where $X$ describes the conditions of interest (e.g., treatment vs. control), $Z$ describes observed covariates (e.g., gender) and $W$ describes unobserved covariates (e.g., batch, temperature, lab). $\beta$, $\gamma$ and $\alpha$ are parameter matrices which record the contribution of $X$, $Z$ and $W$, and $\epsilon$ is random noise. For subset of carefully selected genes (e.g., ERCC spike-ins, housekeeping genes, or technical replicates) we can assume that $X$ and $Z$ are zero, and find $W$ - the &quot;unwanted variation&quot; in your sample. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/bioc/html/RUVSeq.html&#xD;&#xA;  [2]: http://www.nature.com/nbt/journal/v32/n9/full/nbt.2931.html" />
  <row Id="4677" PostHistoryTypeId="2" PostId="2174" RevisionGUID="ffd5927a-37ed-4dda-b8d8-8b9863f989c2" CreationDate="2017-07-28T08:30:21.540" UserId="156" Text="I have a fastq file from minION (albacore) that contains information on the read ID and the start time of the read. I want to extract these two bits of information into a single csv file. &#xD;&#xA;&#xD;&#xA;I've been trying to figure out a grep/awk/sed solution, but I haven't been able to figure it out. &#xD;&#xA;&#xD;&#xA;E.g.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    @93a12f52-95e5-40c7-8c3e-70bf94ed0720 runid=17838b1d08f30a031bf60afabb146a8b0fba7486 read=12217 ch=492 start_time=2017-07-04T06:42:43Z&#xD;&#xA;    CTATTGTCCCCTGCCGCTGCCCCTCCTGCTACGCCCCACTGCCTCACCAGCCGTTACGGTCGCCCCCCATCGCATGCCTTTACACACACACTTCTTTACACATGCTATCTTCCC&#xD;&#xA;    +&#xD;&#xA;    &quot;&quot;*)&amp;$-.,-(#&quot;&amp;'$%''+16#&quot;$##&amp;)%%/&quot;+*(*(&amp;#&quot;&amp;'%1&quot;+)#)&quot;&quot;%$$#&amp;&amp;'1%&quot;'8&gt;MJ&lt;#'&amp;%'.2'.$(&amp;#'()'&amp;&amp;%'$('%&quot;%%%..$#&quot;&amp;&quot;#+&amp;,*$%&quot;#&quot;&#xD;&#xA;    @ff37e422-a25f-404c-8314-ef1733f9c30c runid=17838b1d08f30a031bf60afabb146a8b0fba7486 read=8432 ch=200 start_time=2017-07-04T06:56:41Z&#xD;&#xA;    CGATGGCCGTATGCTTTTGTTATGAAGCGAAAAGCTGCTCGCTTCTCTAGATAATAATGATGTGGCGAAAACGCTATGCGATTCGTTGACATACATGATGGCGGATTTATCTACCACTTTGTGGCATGCTTTTCTCGCCAGATAATGGAATGTTTCTCTGGCGGTAATGGATAGTATCAAATCTCACTAGCCCATTCTATAAGCGCATCCGCATGCACTAGTTCTTGATTCGATCGTCCTCTAGCATGTTCGAAGAAGATAGCATTCACTATCATCATCGCCTCAGGTAAGTTTATTCGGTTGGGGCGTGTGAAGGCAAACACCTTGTTGTCCAGTAAGTTTTCAGTTACTATAACTTAAAGTCGCACATGAATCTAGTCTCCTATTCCCCACCCATGATCCCACTCACACATTTCTACAGAGATGTGGTTAGAAATTTTCATATTAGGTCAGCTTTGACTCAATAAGACATAATTCTTCACTGAATGACTTTTTAAGAACCACCAGGACCAGAGAGAACCAAGAGAGTGGTACCTCTTAAAACACAATAAAGTGATTCAGCCTTAGCCATTGGATTCTGGAGGACCTTGAACCATGTGGGAAGCAGCTCAGGGTGGCCATGTACTATACTGGCGGGTAAGCTTCTGGAGTGCTAGGTTCTTTTTGTCTTTTCTTAAGCATTGCCGCCAGTTGATTGGGTTTTGAACATAAAATAATGCGCCACCAGCAATTCCAGATTTGTTCCTACGGGATAGATTTGTTCAGTTCTAGCATTATGCTTCACTAACCAGATGCGGGCCCTAAGTCCTTCACTTGGAATATTGGATTGGATCATGAGAATATTCTGTCTGAAGCTCGTCATTAATTTTGTTACAAAATAGAGCTTTTTGACTGGAAGTACCACCATACGTGTTCTCAAACTTCAGCATTTTTAGAACTTCCCACGGCATCTTGACCCTTTTCACAGCATGGATAGTCAGGCAGCAGTGAACTTTGTGACTCTTTAATGCCTTCACTTTTCTCTCAGTTTCCCCGCCTTGCGTTATCTTTACTCGTCTTGGGACTTTTATCCCAATGCCAGCCTTCTACCCTGAGACCTCAGTGGGTCATCATCCCAGCCCGGGACATCTCATCCCATCATTTATGGGCTGTTGTGTTTTTTTCAAAACCTAGCCCTCTCAGGAGGAGGAGGAGTGGGAGTCAGTTCAGTGAGGAGGATTAGGATGATCTGAAATGTAAGCACATATAAGCGAAGCACTTATTTTGGGTTGGGTCCTCACGGTGGACATAAGATCGCCTTATGTGTTTAGTAAGCCATTCCTAGCTCTCAATGGCGTGATTACATAGAAGCGTGAGGGATCAGTCCTATGGAAGACTAGGAAGTAAATGAACAAAATATATTAACCATAGAAGTCTCATGGGTCGCTGTAGCCAAAAGATTAACACTTTTGACTACATTGTGGTTTTAGGCATTGAAACAAAAACTTTGAGTCTCCTAAACAAATGAATGGAAAATAGTAGCGAACTTCGATTCCTAACATTAAATCTAGAAATAGCAAGTTAGTTTAAAGACTTTATTTAGCTTTGCTTGCTATAATGAAAACCTTGCCTCCCGGTCGGGGCCATTGTGCCTGAAGCTAGCTTATTGTCTCCTCGAGCTCCCAGCTTCAGCAACTCCTTTTGAAGCGTTTGTCTCAGCTTGGATCTTCAGCAGCTCTTGGTGGCTCTTTTGAGCTAGCTCCTCTGAGATCTTGTATTTGGTAGGTCGCTTAGTCATAGTACTTTTCTTTTAACACCCTTCAGCTCTACGATTACATTTGGTTTTGTGGATATCATAATGATTGATGTGAAGATACATTGTACATGTG&#xD;&#xA;    +&#xD;&#xA;    &quot;#&quot;&quot;#&quot;##&quot;'&quot;&amp;%&amp;?JP7+80)'&amp;*+&amp;&amp;(,.&gt;3&amp;*(#%(')*&amp;'*#(-$)&quot;&amp;,63;?844&amp;&amp;#$'3+.&gt;@9;-/259...&amp;:&quot;$&quot;/*&quot;#*#%(.&amp;%)+,/76/+'4B93:;70.1)'1:)*#&amp;()(+'03589/++&amp;)'$-:;@,B5(9+.JAU;7-+&amp;,1212.+329NPSDHRHH6),&amp;)%)+).-*$,&lt;1(.+&amp;$$2&amp;,-0+.&amp;'//&quot;4(%&quot;+#,/)&lt;56%*%*$&lt;4*/;DI+6&gt;((-%*).+--;2-'04-062&lt;@-&lt;&gt;6CP)#.0/+,#(*2+#&quot;$$-2'5(%%&amp;'5+)$$%8+0;&gt;.@9*(,((,*-2/49G9/1/3./.0CQ93E+/-D-'67J*)($-2+'*/&amp;*)+%&amp;'092&lt;C9/5()$&amp;%(.,+-'**)'2/.(//6&lt;59-5-)&quot;**40:A4+834)-(*+;K0-,&amp;)+%)(#-&amp;+&amp;&amp;))+$&gt;9.6&lt;?A;3'/85$/--..)/3DP99HJ9.&quot;2;I?@C/5%.4&lt;0+0+&amp;$-05C46208AFQE80&quot;%356+02*9I73//(-*/9706192*/)%)(&amp;'($1749C*KKA*/#&quot;/*)5($'#+7@=$%)/&amp;+%##*&amp;&amp;;.),#(4.,2/'-G03%&quot;&quot;#,(2)./,%%1;7#+%$'(03'5)+&amp;.%1)&amp;(%'#*,$3#''-3,),$&quot;),'+,,*$%'%$33ILADPPY@JW\O2-8'9[Y@--*.,&quot;&quot;$%$&quot;%#&quot;65',0/4(6.*43B1.#&amp;)+),(6BI729=9&amp;#$.)#,(,(/-286D&amp;),'C;P=)'-'#$)*91*$%(.3LH31,($/(+)'$$*#+'((('*)00:7C1E24(%+$&amp;-,$'++)&lt;)'**&quot;&amp;''&quot;(*(.14++$5&amp;*,%?.F1&lt;10&quot;&amp;%$,)&amp;&amp;*(&amp;$%$/.&quot;&quot;+3&quot;#&quot;#&quot;*%5*-&gt;87FQ&gt;AKY&lt;9855,&amp;(*@&gt;HAZRQ)'%$&quot;--&amp;/#%.+,/A9F8-1'+&quot;6--*34P-9;=997;7&lt;SUU8((-+.:784/%).&amp;)#*'(*/%7H(1JI-+&amp;+)&amp;9.,/N)&quot;%&quot;&amp;))22/&amp;,)%)'+*/17:8$''(*.*..(/.(%+*&amp;22(+)7GM99A@9)0*1%/470((&amp;((%(937'15084#,($&quot;&amp;'%&quot;'4.,+5&lt;7($*0+*291$*&amp;$%%('+/)'.1-(-.7=K/'%&quot;'*)03&lt;:/&lt;=1(&gt;.((-9-'+&gt;&amp;&amp;&amp;%';,)69V&lt;'1:&gt;FNXA+01:7*#&quot;$'$.%&amp;4BQT7/&amp;03++)'&amp;'*3E.+0(*6..*&lt;.)9*)))'%#(*.&amp;&amp;'&quot;*-IU=&amp;,-))-.,-&gt;'G--6..90067%(,0/*%9)&amp;#'&quot;&quot;#$,5H/$/#-/&amp;2(.0.+;&gt;9--7.3@+2(/4.*0&amp;,-&amp;'/*%0-**-#,/2&amp;-$*)'+#&quot;*)-*##&amp;$3):C:D6H+',./2?&lt;0&gt;)%#&amp;&amp;3::9;8$#%#('$#$',,,*-346@=.%&amp;&amp;(&amp;%('*+&amp;*&amp;1)'&amp;-..#'+2*)/6499=-2*,0,82,$)($:2%*##%$,.5=-5/((#)2./('($@=4,)%1+176:AHQT,(,)0(()@2'+(18G?($)(*'.)&amp;&amp;+*12SD7,18I2)()*+C:01ETZIPDUOB+.55K7B:A5)*/('')/-1=7E&lt;IGR=43@A68;K/:8.'--)BGEE5-0/5+,2''DXBLW7)#DHTC5AL82+--,&amp;#);+).,'3)/.336/&amp;'&amp;(,+8E346-/%1-)1*$+),@-&amp;-%'$+)5.&amp;&amp;%&amp;#(,2&amp;03&amp;/((,(-5%%)?7I4*').+/7+&amp;***845(*/.)(.0))&lt;?YK3=GHH2&amp;*)%$%#%KLY&lt;9&amp;&amp;+,8;/2*./;+))-)/03$&amp;*$+0'36&lt;J+5/+&amp;+(HGFFM57,59)-,0)/:**&quot;%$&amp;#$'&amp;$%#)*%33VX=@FOL00(''4IGIYOG0=1,,#(*$)-0CFM?8+31E8)(+)(%'&quot;'/&quot;&quot;%+3/,1;+7-*''&amp;-%98.$$/'&amp;)&amp;+&amp;(&amp;&amp;&quot;&#xD;&#xA;&#xD;&#xA;Should produce&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xD;&#xA;```" />
  <row Id="4678" PostHistoryTypeId="1" PostId="2174" RevisionGUID="ffd5927a-37ed-4dda-b8d8-8b9863f989c2" CreationDate="2017-07-28T08:30:21.540" UserId="156" Text="fastq to csv of read ID plus start times" />
  <row Id="4679" PostHistoryTypeId="3" PostId="2174" RevisionGUID="ffd5927a-37ed-4dda-b8d8-8b9863f989c2" CreationDate="2017-07-28T08:30:21.540" UserId="156" Text="&lt;fastq&gt;&lt;csv&gt;" />
  <row Id="4680" PostHistoryTypeId="2" PostId="2175" RevisionGUID="0240f88a-77c2-4fa3-acac-0243896f6431" CreationDate="2017-07-28T08:37:41.497" UserId="77" Text="    awk '{if(NR%4==1) print $1, $5}' file.fastq | sed -e &quot;s/ start_time=/, /g&quot; -e &quot;s/^@//g&quot;&#xD;&#xA;&#xD;&#xA;The `awk` command gets the first of every 4 lines, printing the first and fifth &quot;word&quot;. `sed` is then used to strip the initial `@` and replace ` start_time=` with `, `. The output on your example file is:&#xD;&#xA;&#xD;&#xA;    93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;    ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z" />
  <row Id="4681" PostHistoryTypeId="5" PostId="2174" RevisionGUID="4c8005fa-1e3d-4bbe-a5e0-a214f7c51057" CreationDate="2017-07-28T10:02:30.127" UserId="73" Comment="deleted 21 characters in body" Text="I have a fastq file from minION (albacore) that contains information on the read ID and the start time of the read. I want to extract these two bits of information into a single csv file. &#xD;&#xA;&#xD;&#xA;I've been trying to figure out a grep/awk/sed solution, but without success. &#xD;&#xA;&#xD;&#xA;E.g.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    @93a12f52-95e5-40c7-8c3e-70bf94ed0720 runid=17838b1d08f30a031bf60afabb146a8b0fba7486 read=12217 ch=492 start_time=2017-07-04T06:42:43Z&#xD;&#xA;    CTATTGTCCCCTGCCGCTGCCCCTCCTGCTACGCCCCACTGCCTCACCAGCCGTTACGGTCGCCCCCCATCGCATGCCTTTACACACACACTTCTTTACACATGCTATCTTCCC&#xD;&#xA;    +&#xD;&#xA;    &quot;&quot;*)&amp;$-.,-(#&quot;&amp;'$%''+16#&quot;$##&amp;)%%/&quot;+*(*(&amp;#&quot;&amp;'%1&quot;+)#)&quot;&quot;%$$#&amp;&amp;'1%&quot;'8&gt;MJ&lt;#'&amp;%'.2'.$(&amp;#'()'&amp;&amp;%'$('%&quot;%%%..$#&quot;&amp;&quot;#+&amp;,*$%&quot;#&quot;&#xD;&#xA;    @ff37e422-a25f-404c-8314-ef1733f9c30c runid=17838b1d08f30a031bf60afabb146a8b0fba7486 read=8432 ch=200 start_time=2017-07-04T06:56:41Z&#xD;&#xA;    CGATGGCCGTATGCTTTTGTTATGAAGCGAAAAGCTGCTCGCTTCTCTAGATAATAATGATGTGGCGAAAACGCTATGCGATTCGTTGACATACATGATGGCGGATTTATCTACCACTTTGTGGCATGCTTTTCTCGCCAGATAATGGAATGTTTCTCTGGCGGTAATGGATAGTATCAAATCTCACTAGCCCATTCTATAAGCGCATCCGCATGCACTAGTTCTTGATTCGATCGTCCTCTAGCATGTTCGAAGAAGATAGCATTCACTATCATCATCGCCTCAGGTAAGTTTATTCGGTTGGGGCGTGTGAAGGCAAACACCTTGTTGTCCAGTAAGTTTTCAGTTACTATAACTTAAAGTCGCACATGAATCTAGTCTCCTATTCCCCACCCATGATCCCACTCACACATTTCTACAGAGATGTGGTTAGAAATTTTCATATTAGGTCAGCTTTGACTCAATAAGACATAATTCTTCACTGAATGACTTTTTAAGAACCACCAGGACCAGAGAGAACCAAGAGAGTGGTACCTCTTAAAACACAATAAAGTGATTCAGCCTTAGCCATTGGATTCTGGAGGACCTTGAACCATGTGGGAAGCAGCTCAGGGTGGCCATGTACTATACTGGCGGGTAAGCTTCTGGAGTGCTAGGTTCTTTTTGTCTTTTCTTAAGCATTGCCGCCAGTTGATTGGGTTTTGAACATAAAATAATGCGCCACCAGCAATTCCAGATTTGTTCCTACGGGATAGATTTGTTCAGTTCTAGCATTATGCTTCACTAACCAGATGCGGGCCCTAAGTCCTTCACTTGGAATATTGGATTGGATCATGAGAATATTCTGTCTGAAGCTCGTCATTAATTTTGTTACAAAATAGAGCTTTTTGACTGGAAGTACCACCATACGTGTTCTCAAACTTCAGCATTTTTAGAACTTCCCACGGCATCTTGACCCTTTTCACAGCATGGATAGTCAGGCAGCAGTGAACTTTGTGACTCTTTAATGCCTTCACTTTTCTCTCAGTTTCCCCGCCTTGCGTTATCTTTACTCGTCTTGGGACTTTTATCCCAATGCCAGCCTTCTACCCTGAGACCTCAGTGGGTCATCATCCCAGCCCGGGACATCTCATCCCATCATTTATGGGCTGTTGTGTTTTTTTCAAAACCTAGCCCTCTCAGGAGGAGGAGGAGTGGGAGTCAGTTCAGTGAGGAGGATTAGGATGATCTGAAATGTAAGCACATATAAGCGAAGCACTTATTTTGGGTTGGGTCCTCACGGTGGACATAAGATCGCCTTATGTGTTTAGTAAGCCATTCCTAGCTCTCAATGGCGTGATTACATAGAAGCGTGAGGGATCAGTCCTATGGAAGACTAGGAAGTAAATGAACAAAATATATTAACCATAGAAGTCTCATGGGTCGCTGTAGCCAAAAGATTAACACTTTTGACTACATTGTGGTTTTAGGCATTGAAACAAAAACTTTGAGTCTCCTAAACAAATGAATGGAAAATAGTAGCGAACTTCGATTCCTAACATTAAATCTAGAAATAGCAAGTTAGTTTAAAGACTTTATTTAGCTTTGCTTGCTATAATGAAAACCTTGCCTCCCGGTCGGGGCCATTGTGCCTGAAGCTAGCTTATTGTCTCCTCGAGCTCCCAGCTTCAGCAACTCCTTTTGAAGCGTTTGTCTCAGCTTGGATCTTCAGCAGCTCTTGGTGGCTCTTTTGAGCTAGCTCCTCTGAGATCTTGTATTTGGTAGGTCGCTTAGTCATAGTACTTTTCTTTTAACACCCTTCAGCTCTACGATTACATTTGGTTTTGTGGATATCATAATGATTGATGTGAAGATACATTGTACATGTG&#xD;&#xA;    +&#xD;&#xA;    &quot;#&quot;&quot;#&quot;##&quot;'&quot;&amp;%&amp;?JP7+80)'&amp;*+&amp;&amp;(,.&gt;3&amp;*(#%(')*&amp;'*#(-$)&quot;&amp;,63;?844&amp;&amp;#$'3+.&gt;@9;-/259...&amp;:&quot;$&quot;/*&quot;#*#%(.&amp;%)+,/76/+'4B93:;70.1)'1:)*#&amp;()(+'03589/++&amp;)'$-:;@,B5(9+.JAU;7-+&amp;,1212.+329NPSDHRHH6),&amp;)%)+).-*$,&lt;1(.+&amp;$$2&amp;,-0+.&amp;'//&quot;4(%&quot;+#,/)&lt;56%*%*$&lt;4*/;DI+6&gt;((-%*).+--;2-'04-062&lt;@-&lt;&gt;6CP)#.0/+,#(*2+#&quot;$$-2'5(%%&amp;'5+)$$%8+0;&gt;.@9*(,((,*-2/49G9/1/3./.0CQ93E+/-D-'67J*)($-2+'*/&amp;*)+%&amp;'092&lt;C9/5()$&amp;%(.,+-'**)'2/.(//6&lt;59-5-)&quot;**40:A4+834)-(*+;K0-,&amp;)+%)(#-&amp;+&amp;&amp;))+$&gt;9.6&lt;?A;3'/85$/--..)/3DP99HJ9.&quot;2;I?@C/5%.4&lt;0+0+&amp;$-05C46208AFQE80&quot;%356+02*9I73//(-*/9706192*/)%)(&amp;'($1749C*KKA*/#&quot;/*)5($'#+7@=$%)/&amp;+%##*&amp;&amp;;.),#(4.,2/'-G03%&quot;&quot;#,(2)./,%%1;7#+%$'(03'5)+&amp;.%1)&amp;(%'#*,$3#''-3,),$&quot;),'+,,*$%'%$33ILADPPY@JW\O2-8'9[Y@--*.,&quot;&quot;$%$&quot;%#&quot;65',0/4(6.*43B1.#&amp;)+),(6BI729=9&amp;#$.)#,(,(/-286D&amp;),'C;P=)'-'#$)*91*$%(.3LH31,($/(+)'$$*#+'((('*)00:7C1E24(%+$&amp;-,$'++)&lt;)'**&quot;&amp;''&quot;(*(.14++$5&amp;*,%?.F1&lt;10&quot;&amp;%$,)&amp;&amp;*(&amp;$%$/.&quot;&quot;+3&quot;#&quot;#&quot;*%5*-&gt;87FQ&gt;AKY&lt;9855,&amp;(*@&gt;HAZRQ)'%$&quot;--&amp;/#%.+,/A9F8-1'+&quot;6--*34P-9;=997;7&lt;SUU8((-+.:784/%).&amp;)#*'(*/%7H(1JI-+&amp;+)&amp;9.,/N)&quot;%&quot;&amp;))22/&amp;,)%)'+*/17:8$''(*.*..(/.(%+*&amp;22(+)7GM99A@9)0*1%/470((&amp;((%(937'15084#,($&quot;&amp;'%&quot;'4.,+5&lt;7($*0+*291$*&amp;$%%('+/)'.1-(-.7=K/'%&quot;'*)03&lt;:/&lt;=1(&gt;.((-9-'+&gt;&amp;&amp;&amp;%';,)69V&lt;'1:&gt;FNXA+01:7*#&quot;$'$.%&amp;4BQT7/&amp;03++)'&amp;'*3E.+0(*6..*&lt;.)9*)))'%#(*.&amp;&amp;'&quot;*-IU=&amp;,-))-.,-&gt;'G--6..90067%(,0/*%9)&amp;#'&quot;&quot;#$,5H/$/#-/&amp;2(.0.+;&gt;9--7.3@+2(/4.*0&amp;,-&amp;'/*%0-**-#,/2&amp;-$*)'+#&quot;*)-*##&amp;$3):C:D6H+',./2?&lt;0&gt;)%#&amp;&amp;3::9;8$#%#('$#$',,,*-346@=.%&amp;&amp;(&amp;%('*+&amp;*&amp;1)'&amp;-..#'+2*)/6499=-2*,0,82,$)($:2%*##%$,.5=-5/((#)2./('($@=4,)%1+176:AHQT,(,)0(()@2'+(18G?($)(*'.)&amp;&amp;+*12SD7,18I2)()*+C:01ETZIPDUOB+.55K7B:A5)*/('')/-1=7E&lt;IGR=43@A68;K/:8.'--)BGEE5-0/5+,2''DXBLW7)#DHTC5AL82+--,&amp;#);+).,'3)/.336/&amp;'&amp;(,+8E346-/%1-)1*$+),@-&amp;-%'$+)5.&amp;&amp;%&amp;#(,2&amp;03&amp;/((,(-5%%)?7I4*').+/7+&amp;***845(*/.)(.0))&lt;?YK3=GHH2&amp;*)%$%#%KLY&lt;9&amp;&amp;+,8;/2*./;+))-)/03$&amp;*$+0'36&lt;J+5/+&amp;+(HGFFM57,59)-,0)/:**&quot;%$&amp;#$'&amp;$%#)*%33VX=@FOL00(''4IGIYOG0=1,,#(*$)-0CFM?8+31E8)(+)(%'&quot;'/&quot;&quot;%+3/,1;+7-*''&amp;-%98.$$/'&amp;)&amp;+&amp;(&amp;&amp;&quot;&#xD;&#xA;&#xD;&#xA;Should produce&#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xD;&#xA;```" />
  <row Id="4682" PostHistoryTypeId="6" PostId="52" RevisionGUID="4aa596ea-b5a6-4853-9124-cb1ec0892f60" CreationDate="2017-07-28T11:18:36.177" UserId="203" Comment="add normalization tag" Text="&lt;rna-seq&gt;&lt;transcriptome&gt;&lt;normalization&gt;" />
  <row Id="4683" PostHistoryTypeId="24" PostId="52" RevisionGUID="4aa596ea-b5a6-4853-9124-cb1ec0892f60" CreationDate="2017-07-28T11:18:36.177" Comment="Proposed by 203 approved by 77 edit id of 242" />
  <row Id="4684" PostHistoryTypeId="6" PostId="2165" RevisionGUID="a31f40a6-13a6-45ea-a31e-9b6cc6b2733c" CreationDate="2017-07-28T11:18:40.363" UserId="203" Comment="Add some tags" Text="&lt;rna-seq&gt;&lt;statistics&gt;&lt;edger&gt;&lt;best-practice&gt;&lt;preprocessing&gt;" />
  <row Id="4685" PostHistoryTypeId="24" PostId="2165" RevisionGUID="a31f40a6-13a6-45ea-a31e-9b6cc6b2733c" CreationDate="2017-07-28T11:18:40.363" Comment="Proposed by 203 approved by 77 edit id of 243" />
  <row Id="4686" PostHistoryTypeId="2" PostId="2176" RevisionGUID="20080e33-766e-4a33-a513-23841bdf21d4" CreationDate="2017-07-28T15:10:39.743" UserId="298" Text="Since the string `start_time` will only appear on the header line, or else you don't have a valid fastq file, you can simply do:&#xD;&#xA;&#xD;&#xA;    $ perl -ne '/^@(\S+).*start_time=(.*)/ &amp;&amp; print &quot;$1, $2\n&quot;' file.fastq &#xD;&#xA;    93a12f52-95e5-40c7-8c3e-70bf94ed0720,2017-07-04T06:42:43Z&#xD;&#xA;    ff37e422-a25f-404c-8314-ef1733f9c30c,2017-07-04T06:56:41Z&#xD;&#xA;        &#xD;&#xA;Alternatively, since you mentioned `awk` and `sed`:&#xD;&#xA;&#xD;&#xA;    $ awk -v OFS=&quot;, &quot; '/start_time/{print $1,$NF}' file.fastq | sed 's/start_time=//'&#xD;&#xA;    @93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;    @ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;Or, doing the whole thing in `awk`:&#xD;&#xA;&#xD;&#xA;    $ awk 'sub(/start_time=/,&quot;&quot;){print $1&quot;, &quot;$NF}' file.fastq &#xD;&#xA;    @93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;    @ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xD;&#xA;&#xD;&#xA;And if the `@` annoy you:&#xD;&#xA;&#xD;&#xA;    $ awk 'sub(/^@/,&quot;&quot;) &amp;&amp; sub(/ .*start_time=/,&quot;, &quot;)' file.fastq &#xD;&#xA;    93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;    ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xD;&#xA;&#xD;&#xA;  &#xD;&#xA;And in `sed`:&#xD;&#xA;&#xD;&#xA;    $ sed -n 's/^@\([^ ]*\).*start_time=\(.*\)/\1, \2/p' file.fastq &#xD;&#xA;    93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;    ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xD;&#xA;      &#xD;&#xA;Or, if your sed supports it:&#xD;&#xA;&#xD;&#xA;    $ sed -En 's/^@(\S+).*start_time=(.*)/\1, \2/p' file.fastq &#xD;&#xA;    93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;    ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Finally, since `grep` can't do replacements, to use it you would have to do something like:&#xD;&#xA;&#xD;&#xA;    $ grep -oP '^@\K\S+|start_time=\K.*' file.fastq | paste - - &#xD;&#xA;    93a12f52-95e5-40c7-8c3e-70bf94ed0720	2017-07-04T06:42:43Z&#xD;&#xA;    ff37e422-a25f-404c-8314-ef1733f9c30c	2017-07-04T06:56:41Z&#xD;&#xA;    &#xD;&#xA;And to get the commas:&#xD;&#xA;&#xD;&#xA;    $ grep -oP '^@\K\S+|start_time=\K.*' file.fastq | paste - - | sed 's/\t/, /'&#xD;&#xA;    93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;    ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    " />
  <row Id="4687" PostHistoryTypeId="5" PostId="2175" RevisionGUID="479d0c35-fa03-4505-be8a-40ffb337370f" CreationDate="2017-07-28T15:15:43.600" UserId="298" Comment="The g isn't needed. In the first case, there will only be one start_time= (i there are more, it breaks anyway) and in the second, you're anchoring the regex to the start of the line, so it's pointless." Text="    awk '{if(NR%4==1) print $1, $5}' file.fastq | sed -e &quot;s/ start_time=/, /&quot; -e &quot;s/^@//&quot;&#xD;&#xA;&#xD;&#xA;The `awk` command gets the first of every 4 lines, printing the first and fifth &quot;word&quot;. `sed` is then used to strip the initial `@` and replace ` start_time=` with `, `. The output on your example file is:&#xD;&#xA;&#xD;&#xA;    93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xD;&#xA;    ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z" />
  <row Id="4688" PostHistoryTypeId="2" PostId="2177" RevisionGUID="ec902ee8-165e-4f49-97ef-137df33b021f" CreationDate="2017-07-28T17:57:12.517" UserId="211" Text="GFF3 formats are tabular files with 9 fields per line, separated by tabs.First 8 fields share almost same data structure. But the 9th field varies a lot depending on the features. &#xD;&#xA;&#xD;&#xA;The **last field(9th field)** ***varies*** a lot from one gene prediction algorithm to another.&#xD;&#xA;&#xD;&#xA;Presently I am trying to build my own gff module that will parse one gff file and return one structured data structure( **python dictionary structure**). For that, I need a good understanding of the 9th field of the gff format.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So can any one please help me by providing various gff files." />
  <row Id="4689" PostHistoryTypeId="1" PostId="2177" RevisionGUID="ec902ee8-165e-4f49-97ef-137df33b021f" CreationDate="2017-07-28T17:57:12.517" UserId="211" Text="How 9th column of a gff file varies from one gene prediction algorithm to another?" />
  <row Id="4690" PostHistoryTypeId="3" PostId="2177" RevisionGUID="ec902ee8-165e-4f49-97ef-137df33b021f" CreationDate="2017-07-28T17:57:12.517" UserId="211" Text="&lt;gff3&gt;" />
  <row Id="4691" PostHistoryTypeId="5" PostId="2167" RevisionGUID="5f5c0871-5304-42bd-8645-5a97ae9f9269" CreationDate="2017-07-28T19:18:37.343" UserId="939" Comment="added 454 characters in body" Text="You probably want to include query start (qstart) and query end (qend) in your blast output.&#xD;&#xA;&#xD;&#xA;Something like this:&#xD;&#xA;&#xD;&#xA;    blastn -query your.fasta -out blast.out.txt -db your.db -outfmt '6 qseqid sseqid qstart qend length evalue'&#xD;&#xA;&#xD;&#xA;In R you can take the &quot;qstart:qend&quot; from each line for density plot.&#xD;&#xA;&#xD;&#xA;There are many ways in R to plot the densities of these start and end amino acids.&#xD;&#xA;&#xD;&#xA;Let me show an example with a small data frame:&#xD;&#xA;&#xD;&#xA;    qstart &lt;- c(200, 300, 250, 400, 500)&#xD;&#xA;    qend &lt;- c(300, 450, 400, 600, 650)&#xD;&#xA;    &#xD;&#xA;    df &lt;- as.data.frame(cbind(qstart,qend))&#xD;&#xA;    &#xD;&#xA;    aa &lt;- vector()&#xD;&#xA;    i=1&#xD;&#xA;    for(i in 1:5){&#xD;&#xA;    aa &lt;- append(aa, c(df[i,1]:df[i,2]))&#xD;&#xA;    i+1&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    hist(aa)&#xD;&#xA;    dens &lt;- density(aa)&#xD;&#xA;    plot(dens)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;" />
  <row Id="4692" PostHistoryTypeId="2" PostId="2178" RevisionGUID="41fefca2-cbc6-4f08-808f-00e9331a3d31" CreationDate="2017-07-28T19:18:45.167" UserId="96" Text="The first place to start is the [GFF3 specification](https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md). This is the official word on what is and is not allowed in a GFF3 file. For example, users can define arbitrary attribute keys, so long as they do not begin with an uppercase letter (these are reserved for &quot;official&quot; use).&#xD;&#xA;&#xD;&#xA;But your question doesn't seem to be about what is *allowed*, but what is *commonly used*. I have a question for you: which gene predictors are YOU using? Or are you using gene annotations produced by others?&#xD;&#xA;&#xD;&#xA;Here are a few examples.&#xD;&#xA;&#xD;&#xA;## NCBI RefSeq&#xD;&#xA;&#xD;&#xA;If you're using GFF3 from NCBI, chances are it looks something like this.&#xD;&#xA;&#xD;&#xA;    NC_007070.3     Gnomon  gene    329235  331223  .       +       .       ID=gene14;Dbxref=BEEBASE:GB42168,GeneID:551678;Name=LOC551678;gbkey=Gene;gene=LOC551678;gene_biotype=protein_coding&#xD;&#xA;    NC_007070.3     Gnomon  mRNA    329235  331223  .       +       .       ID=rna27;Parent=gene14;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;Name=XM_624067.4;gbkey=mRNA;gene=LOC551678;model_evidence=Supporting evidence includes similarity to: 66 ESTs%2C 24 Proteins%2C and 99%25 coverage of the annotated genomic feature by RNAseq alignments%2C including 113 samples with support for all annotated introns;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xD;&#xA;    NC_007070.3     Gnomon  exon    329235  329459  .       +       .       ID=id117;Parent=rna27;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;gbkey=mRNA;gene=LOC551678;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xD;&#xA;    NC_007070.3     Gnomon  exon    329850  330082  .       +       .       ID=id118;Parent=rna27;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;gbkey=mRNA;gene=LOC551678;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xD;&#xA;    NC_007070.3     Gnomon  exon    330166  330301  .       +       .       ID=id119;Parent=rna27;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;gbkey=mRNA;gene=LOC551678;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xD;&#xA;    NC_007070.3     Gnomon  exon    330376  331223  .       +       .       ID=id120;Parent=rna27;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;gbkey=mRNA;gene=LOC551678;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xD;&#xA;    NC_007070.3     Gnomon  CDS     329333  329459  .       +       0       ID=cds8;Parent=rna27;Dbxref=GeneID:551678,Genbank:XP_624070.1,BEEBASE:GB42168;Name=XP_624070.1;gbkey=CDS;gene=LOC551678;product=receptor expression-enhancing protein 5-like isoform X1;protein_id=XP_624070.1&#xD;&#xA;    NC_007070.3     Gnomon  CDS     329850  330082  .       +       2       ID=cds8;Parent=rna27;Dbxref=GeneID:551678,Genbank:XP_624070.1,BEEBASE:GB42168;Name=XP_624070.1;gbkey=CDS;gene=LOC551678;product=receptor expression-enhancing protein 5-like isoform X1;protein_id=XP_624070.1&#xD;&#xA;    NC_007070.3     Gnomon  CDS     330166  330301  .       +       0       ID=cds8;Parent=rna27;Dbxref=GeneID:551678,Genbank:XP_624070.1,BEEBASE:GB42168;Name=XP_624070.1;gbkey=CDS;gene=LOC551678;product=receptor expression-enhancing protein 5-like isoform X1;protein_id=XP_624070.1&#xD;&#xA;    NC_007070.3     Gnomon  CDS     330376  330416  .       +       2       ID=cds8;Parent=rna27;Dbxref=GeneID:551678,Genbank:XP_624070.1,BEEBASE:GB42168;Name=XP_624070.1;gbkey=CDS;gene=LOC551678;product=receptor expression-enhancing protein 5-like isoform X1;protein_id=XP_624070.1&#xD;&#xA;&#xD;&#xA;## MAKER&#xD;&#xA;&#xD;&#xA;The MAKER annotation workflow ([paper](http://dx.doi.org/10.1101/gr.6743907), [software](http://www.yandell-lab.org/software/maker.html)) is a pretty commonly used gene annotation tool, and produces GFF3 output like this.&#xD;&#xA;&#xD;&#xA;    scaffold_12     maker   gene    652527  655343  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959;Name=maker-scaffold_12-augustus-gene-0.959&#xD;&#xA;    scaffold_12     maker   mRNA    652527  655343  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1;Parent=maker-scaffold_12-augustus-gene-0.959;Name=maker-scaffold_12-augustus-gene-0.959-mRNA-1;_AED=0.24;_eAED=0.18;_QI=0|0|0|0.66|0.5|0.33|3|0|218&#xD;&#xA;    scaffold_12     maker   exon    652527  652817  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:exon:1203;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xD;&#xA;    scaffold_12     maker   exon    654877  655170  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:exon:1204;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xD;&#xA;    scaffold_12     maker   exon    655272  655343  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:exon:1205;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xD;&#xA;    scaffold_12     maker   CDS     652527  652817  .       +       0       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:cds;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xD;&#xA;    scaffold_12     maker   CDS     654877  655170  .       +       0       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:cds;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xD;&#xA;    scaffold_12     maker   CDS     655272  655343  .       +       0       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:cds;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xD;&#xA;    scaffold_12     maker   gene    941547  943897  .       +       .       ID=snap-scaffold_12-processed-gene-0.851;Name=snap-scaffold_12-processed-gene-0.851&#xD;&#xA;    scaffold_12     maker   mRNA    941547  943897  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1;Parent=snap-scaffold_12-processed-gene-0.851;Name=snap-scaffold_12-processed-gene-0.851-mRNA-1;_AED=0.22;_eAED=0.22;_QI=0|0|0.25|0.25|1|1|4|661|95&#xD;&#xA;    scaffold_12     maker   exon    941547  941631  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:exon:1206;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xD;&#xA;    scaffold_12     maker   exon    942343  942367  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:exon:1207;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xD;&#xA;    scaffold_12     maker   exon    942780  942920  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:exon:1208;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xD;&#xA;    scaffold_12     maker   exon    943200  943897  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:exon:1209;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xD;&#xA;    scaffold_12     maker   CDS     941547  941631  .       +       0       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:cds;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xD;&#xA;    scaffold_12     maker   CDS     942343  942367  .       +       2       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:cds;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xD;&#xA;    scaffold_12     maker   CDS     942780  942920  .       +       1       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:cds;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xD;&#xA;    scaffold_12     maker   CDS     943200  943236  .       +       1       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:cds;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xD;&#xA;    scaffold_12     maker   three_prime_UTR 943237  943897  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:three_prime_utr;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xD;&#xA;&#xD;&#xA;## GeneMark, SNAP, Augustus&#xD;&#xA;&#xD;&#xA;If we look at the output of several common *ab initio* gene prediction tools, none of them actually produces GFF3 by default. Here is some output from GeneMark ([paper](https://doi.org/10.1093/nar/26.4.1107), [software](http://exon.gatech.edu/GeneMark/)).&#xD;&#xA;&#xD;&#xA;    543   1   +  Initial         799051     799097         47          1 2 - -&#xD;&#xA;    543   2   +  Terminal        799236     799266         31          3 3 - -&#xD;&#xA;    &#xD;&#xA;    544   3   -  Terminal        802357     802514        158          3 2 - -&#xD;&#xA;    544   2   -  Internal        802607     802685         79          1 1 - -&#xD;&#xA;    544   1   -  Initial         802829     802843         15          3 1 - -&#xD;&#xA;&#xD;&#xA;Here is some output from SNAP ([paper](http://dx.doi.org/10.1186/1471-2105-5-59), [software](http://korflab.ucdavis.edu/software.html)).&#xD;&#xA;&#xD;&#xA;    Einit   8230    8239    +       9.329   0       1       0       scaffold_12-snap.4&#xD;&#xA;    Exon    8848    8869    +       2.484   2       2       2       scaffold_12-snap.4&#xD;&#xA;    Exon    10121   10208   +       15.302  1       0       2       scaffold_12-snap.4&#xD;&#xA;    Exon    11361   11420   +       5.969   0       0       2       scaffold_12-snap.4&#xD;&#xA;    Exon    11471   11535   +       0.921   0       2       1       scaffold_12-snap.4&#xD;&#xA;    Eterm   12169   12187   +       18.163  1       0       1       scaffold_12-snap.4&#xD;&#xA;    Einit   14569   14668   +       -6.918  0       1       0       scaffold_12-snap.5&#xD;&#xA;    Exon    15029   15203   +       -1.023  2       2       0       scaffold_12-snap.5&#xD;&#xA;    Exon    16171   16307   +       -9.230  1       1       1       scaffold_12-snap.5&#xD;&#xA;    Eterm   16667   16698   +       9.829   2       0       0       scaffold_12-snap.5&#xD;&#xA;    Einit   17809   17898   -       -5.390  0       0       0       scaffold_12-snap.6&#xD;&#xA;    Exon    17299   17350   -       6.978   0       1       2       scaffold_12-snap.6&#xD;&#xA;    Eterm   17029   17129   -       -2.857  2       0       0       scaffold_12-snap.6&#xD;&#xA;&#xD;&#xA;Both of these formats appear to be tab-delimited, but that's where the similarity with GFF3 ends.&#xD;&#xA;&#xD;&#xA;The output of Augustus ([paper](https://doi.org/10.1093/nar/gkl200), [software](http://bioinf.uni-greifswald.de/augustus/)) looks like GTF (a variant of the GFF format) once you lose all the lines beginning with a `#` symbol, but Augustus also has a GFF3 output mode.&#xD;&#xA;&#xD;&#xA;    # Predicted genes for sequence number 1 on both strands&#xD;&#xA;    # start gene g1&#xD;&#xA;    scaffold_12     AUGUSTUS        gene    2841    3400    1       +       .       g1&#xD;&#xA;    scaffold_12     AUGUSTUS        transcript      2841    3400    1       +       .       g1.t1&#xD;&#xA;    scaffold_12     AUGUSTUS        start_codon     2841    2843    .       +       0       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        intron  3027    3097    1       +       .       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        CDS     2841    3026    1       +       0       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        CDS     3098    3400    1       +       0       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        stop_codon      3398    3400    .       +       0       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xD;&#xA;    # protein sequence = [MAIKNAEHDLRVIVDAIEGLGLKVAPHKTEAMAFPASALCGRRGAAPPKIRLGGSSILVGSRSRWYISGHSENSSKSP&#xD;&#xA;    # RTEGKETTPLQQRDPLDAPLWVSGVVAHCCGGPEGQEGCPGLAAQGSDQGVLRIRDGLLCGYDGCGDHRPRPSDSSAGGGLCRP]&#xD;&#xA;    # end gene g1&#xD;&#xA;    ###&#xD;&#xA;    # start gene g2&#xD;&#xA;    scaffold_12     AUGUSTUS        gene    4712    15229   0.21    +       .       g2&#xD;&#xA;    scaffold_12     AUGUSTUS        transcript      4712    15229   0.21    +       .       g2.t1&#xD;&#xA;    scaffold_12     AUGUSTUS        start_codon     4712    4714    .       +       0       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        intron  4858    5591    0.74    +       .       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        intron  5686    15028   0.23    +       .       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        CDS     4712    4857    0.72    +       0       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        CDS     5592    5685    0.42    +       1       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        CDS     15029   15229   0.43    +       0       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xD;&#xA;    scaffold_12     AUGUSTUS        stop_codon      15227   15229   .       +       0       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xD;&#xA;    # protein sequence = [MGRNSHRSCCVVNCKITSAKSDCKFYKFPTAKWKINQRKMWVAAVKRQKYIKDEISHAETQTEITEVTGATKVNYANK&#xD;&#xA;    # KYICLLFVRTYVLRMLVDVALSNLRFSLFGIRKSLEIFGQSEKADQTRWRLPSCEMEWIESRKGKMRE]&#xD;&#xA;    # end gene g2&#xD;&#xA;    ###&#xD;&#xA;&#xD;&#xA;## Parsing attributes&#xD;&#xA;&#xD;&#xA;So...how should YOU handle attributes in GFF3's 9th column? That depends a lot on what you want to do with the data.&#xD;&#xA;&#xD;&#xA;The most important attributes are `ID` and `Parent`, which are used to define relationships between features and subfeatures. (These relationships implicitly define a directed acyclic graph of features, although most GFF3 parsers don't directly support traversal of this graph.)&#xD;&#xA;&#xD;&#xA;But not only do you have to handle the pre-defined attributes discussed in the GFF3 specification, you also have to be able to handle any number of arbitrary attributes whose keys you may not know beforehand.&#xD;&#xA;&#xD;&#xA;The simplest way to handle this would be to parse the attribute column into a dictionary of key/value pairs. Once it's in this form, it's trivial to see what attributes are there and how to access them.&#xD;&#xA;&#xD;&#xA;For example, if we have a feature with the following attributes...&#xD;&#xA;&#xD;&#xA;    ID=mRNA42;Parent=gene19;integrity=0.95;foo=bar&#xD;&#xA;&#xD;&#xA;...we would want it in a dictionary like so.&#xD;&#xA;&#xD;&#xA;    attributes = {&#xD;&#xA;        'ID': 'mrna42',&#xD;&#xA;        'Parent': 'gene19',&#xD;&#xA;        'integrity': '0.95',&#xD;&#xA;        'foo': 'bar',&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Python code to parse that might look something like this.&#xD;&#xA;&#xD;&#xA;    attributes = dict()&#xD;&#xA;    for keyvaluepair in attributestring.split(';'):&#xD;&#xA;        for key, value in keyvaluepair.split('='):&#xD;&#xA;            attributes[key] = value&#xD;&#xA;&#xD;&#xA;One thing to consider is that it's possible for an attribute to have multiple values (separated by commas). It's not commonly used, but it is valid and something that should be handled or at least checked for." />
  <row Id="4693" PostHistoryTypeId="6" PostId="2177" RevisionGUID="c9ad3b1f-783b-4b7e-90f5-318f0caa9676" CreationDate="2017-07-28T19:19:29.480" UserId="96" Comment="added new tags" Text="&lt;file-formats&gt;&lt;python&gt;&lt;gff3&gt;&lt;sequence-annotation&gt;" />
  <row Id="4694" PostHistoryTypeId="5" PostId="2173" RevisionGUID="41847e02-e655-443f-ae02-8a10f1e4ee29" CreationDate="2017-07-28T19:23:56.830" UserId="48" Comment="insert images in the question" Text="Take for instance, this hypothetical example:&#xD;&#xA;&#xD;&#xA;    bam &lt;- system.file(&quot;extdata&quot;,package = &quot;SomePackage&quot;,&quot;snps.bam&quot;)&#xD;&#xA;    reads &lt;- readGAlignments(bam)&#xD;&#xA;&#xD;&#xA;How do I display `reads` in a plot with the second and fourth graph of this picture (mismatch and reads) [![enter image description here][1]][1], and the first graph of this picture? [![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;I've tried getting additional columns from `readGAlignments`, but it still seems to break.&#xD;&#xA;&#xD;&#xA;I've also tried using Gviz, but that doesn't support indels, which is crucial to my work.&#xD;&#xA;&#xD;&#xA;Finally, is there a way to see individual mismatches on the reads like in the first graph of the second picture, or like in Gviz? I've looked at the varying graphs for ggbio and the karyogram seems like the closest thing.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/23dbd.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/ZFZAO.jpg" />
  <row Id="4695" PostHistoryTypeId="24" PostId="2173" RevisionGUID="41847e02-e655-443f-ae02-8a10f1e4ee29" CreationDate="2017-07-28T19:23:56.830" Comment="Proposed by 48 approved by 191, 96 edit id of 244" />
  <row Id="4696" PostHistoryTypeId="2" PostId="2179" RevisionGUID="fb4940f0-9a05-45b3-b06e-2e82ef88e577" CreationDate="2017-07-28T23:41:39.557" UserId="1224" Text="    use Bio::DB::GenBank;&#xD;&#xA;    use Bio::DB::Query::GenBank;&#xD;&#xA;    &#xD;&#xA;    $query = &quot;LEGK&quot;; &#xD;&#xA;    $query_obj = Bio::DB::Query::GenBank-&gt;new(-db =&gt; 'protein', &#xD;&#xA;                                              -query =&gt; $query );&#xD;&#xA;    &#xD;&#xA;    $gb_obj = Bio::DB::GenBank-&gt;new;&#xD;&#xA;    &#xD;&#xA;    $stream_obj = $gb_obj-&gt;get_Stream_by_query($query_obj);&#xD;&#xA;    &#xD;&#xA;    while ($seq_obj = $stream_obj-&gt;next_seq) {&#xD;&#xA;        # do something with the sequence object    &#xD;&#xA;        print &quot;&gt;$query&quot;,' ', $seq_obj-&gt;display_id, ' ', $seq_obj-&gt;desc,&quot;\n&quot;, $seq_obj-&gt;seq[,'\n';&#xD;&#xA;&#xD;&#xA;hey. How can i print first occurrence of protein sequence?" />
  <row Id="4697" PostHistoryTypeId="1" PostId="2179" RevisionGUID="fb4940f0-9a05-45b3-b06e-2e82ef88e577" CreationDate="2017-07-28T23:41:39.557" UserId="1224" Text="Bioperl - how can i print first result of search sequence per iteration?" />
  <row Id="4698" PostHistoryTypeId="3" PostId="2179" RevisionGUID="fb4940f0-9a05-45b3-b06e-2e82ef88e577" CreationDate="2017-07-28T23:41:39.557" UserId="1224" Text="&lt;database&gt;&lt;public-databases&gt;&lt;perl&gt;" />
  <row Id="4699" PostHistoryTypeId="2" PostId="2180" RevisionGUID="47cfea2f-03c1-4942-ae01-ad22708ef79f" CreationDate="2017-07-29T00:06:37.310" UserId="1226" Text="I'm very new to bioinformatics in general, and I'm trying to understand some basic concepts.&#xD;&#xA;&#xD;&#xA;I have RNAseq data, and bioinformatics people tell me that intensities cannot be compared across patients. So there are all of these pipelines to compare intensities to Z scores--are those as simple as just plugging data into a bioconductor package?&#xD;&#xA;&#xD;&#xA;It would be great to get some overview description of why Z values are important/why you can't compare intensities across patients, and/or some pointers towards any resources that I can read to learn more.&#xD;&#xA;&#xD;&#xA;Thanks much for your time!" />
  <row Id="4700" PostHistoryTypeId="1" PostId="2180" RevisionGUID="47cfea2f-03c1-4942-ae01-ad22708ef79f" CreationDate="2017-07-29T00:06:37.310" UserId="1226" Text="RNAseq: Z score, Intensity, and Resources" />
  <row Id="4701" PostHistoryTypeId="3" PostId="2180" RevisionGUID="47cfea2f-03c1-4942-ae01-ad22708ef79f" CreationDate="2017-07-29T00:06:37.310" UserId="1226" Text="&lt;rna-seq&gt;&lt;bioconductor&gt;&lt;python&gt;&lt;normalization&gt;" />
  <row Id="4702" PostHistoryTypeId="6" PostId="2173" RevisionGUID="37956e10-e772-419c-af10-e4260fcbca8a" CreationDate="2017-07-29T16:12:54.553" UserId="203" Comment="addition of tags (genome-browser added as Gviz is also mentioned)" Text="&lt;r&gt;&lt;visualization&gt;&lt;genome-browser&gt;" />
  <row Id="4703" PostHistoryTypeId="24" PostId="2173" RevisionGUID="37956e10-e772-419c-af10-e4260fcbca8a" CreationDate="2017-07-29T16:12:54.553" Comment="Proposed by 203 approved by 57, 77 edit id of 245" />
  <row Id="4704" PostHistoryTypeId="2" PostId="2181" RevisionGUID="4e6cd770-eab5-48bf-8921-28ae86dc1d50" CreationDate="2017-07-30T08:57:43.057" UserId="939" Text="It depends on what test or analysis you want to do, whether you need intensities (expression values) or z-scores.&#xD;&#xA;&#xD;&#xA;If you want to do statistical analysis, such as finding differentially expressed genes between groups of patients (e.g., with limma), you don't want to use z-scores. But you use normalized intensities (for microarrays) or start with raw counts for RNAseq. Then follow the user guide (limma has a great user guide, also for starters).&#xD;&#xA;&#xD;&#xA;For visualization in heatmaps or for other clustering (e.g., k-means, fuzzy) it is useful to use z-scores. Z-scores are a form of transformation (scaling), where every genes is sort of &quot;reset&quot; to the mean of all samples, using also the standard deviation. If you want to know exactly what a z-score is, a simple google search can tell you the details. &#xD;&#xA;&#xD;&#xA;In R you can use the scale function for z-score transformation. Be aware that the function works on columns though. Which means you have to transpose your matrix first if genes are in the rows, and then transpose them back after transformation." />
  <row Id="4705" PostHistoryTypeId="2" PostId="2182" RevisionGUID="1d0b9c68-4819-46ed-a09d-f5fdb9d339c8" CreationDate="2017-07-30T15:28:33.087" UserId="298" Text="In the simplest case, if you just want to stop after the first record was printed, you can just add `exit` (I also corrected the syntax errors you had and added `use strict` and `use warnings`; I suggest you get into the habit of using those two, they save you from a lot of grief in the long run):&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-perl --&gt;&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env perl&#xD;&#xA;    use Bio::DB::GenBank;&#xD;&#xA;    use Bio::DB::Query::GenBank;&#xD;&#xA;    use strict;&#xD;&#xA;    use warnings; &#xD;&#xA;    my $query = &quot;LEGK&quot;; &#xD;&#xA;    my $query_obj = Bio::DB::Query::GenBank-&gt;new(-db =&gt; 'protein', &#xD;&#xA;                                              -query =&gt; $query );&#xD;&#xA;    my $gb_obj = Bio::DB::GenBank-&gt;new;&#xD;&#xA;    my $stream_obj = $gb_obj-&gt;get_Stream_by_query($query_obj);&#xD;&#xA;    while (my $seq_obj = $stream_obj-&gt;next_seq) {&#xD;&#xA;    	print &quot;&gt;$query&quot;,' ', $seq_obj-&gt;display_id, ' ', $seq_obj-&gt;desc,&quot;\n&quot;, $seq_obj-&gt;seq, &quot;\n&quot;;&#xD;&#xA;    	exit;&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;If instead, you want to only process the first result for a list of multiple queries:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env perl&#xD;&#xA;    use Bio::DB::GenBank;&#xD;&#xA;    use Bio::DB::Query::GenBank;&#xD;&#xA;    use strict;&#xD;&#xA;    use warnings; &#xD;&#xA;    my @queries = qw(LEGK TP53 ACT1); &#xD;&#xA;    query:foreach my $query (@queries) {&#xD;&#xA;    	my $query_obj = Bio::DB::Query::GenBank-&gt;new(-db =&gt; 'protein', &#xD;&#xA;                                              -query =&gt; $query );&#xD;&#xA;    	my $gb_obj = Bio::DB::GenBank-&gt;new;&#xD;&#xA;    	my $stream_obj = $gb_obj-&gt;get_Stream_by_query($query_obj);&#xD;&#xA;    	while (my $seq_obj = $stream_obj-&gt;next_seq) {&#xD;&#xA;    		print &quot;&gt;$query&quot;,' ', $seq_obj-&gt;display_id, ' ', $seq_obj-&gt;desc,&quot;\n&quot;, $seq_obj-&gt;seq, &quot;\n&quot;;&#xD;&#xA;    		## Move to the next query&#xD;&#xA;    		next query;&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Or, to print only the first sequence if multiple quries have identical sequences:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env perl&#xD;&#xA;    use Bio::DB::GenBank;&#xD;&#xA;    use Bio::DB::Query::GenBank;&#xD;&#xA;    use strict;&#xD;&#xA;    use warnings; &#xD;&#xA;    my @queries = qw(LEGK TP53 ACT1); &#xD;&#xA;    foreach my $query (@queries) {&#xD;&#xA;    	my %seen;&#xD;&#xA;    	my $query_obj = Bio::DB::Query::GenBank-&gt;new(-db =&gt; 'protein', &#xD;&#xA;                                              -query =&gt; $query );&#xD;&#xA;    	my $gb_obj = Bio::DB::GenBank-&gt;new;&#xD;&#xA;    	my $stream_obj = $gb_obj-&gt;get_Stream_by_query($query_obj);&#xD;&#xA;    	while (my $seq_obj = $stream_obj-&gt;next_seq) {&#xD;&#xA;    		if ($seen{$seq_obj-&gt;seq}) {&#xD;&#xA;    			## Here, since we're not using an argument (next query), this will move&#xD;&#xA;    			## to the next iteration of the current loop and not the `foreach`. &#xD;&#xA;    			next;&#xD;&#xA;    		}&#xD;&#xA;    		print &quot;&gt;$query&quot;,' ', $seq_obj-&gt;display_id, ' ', $seq_obj-&gt;desc,&quot;\n&quot;, $seq_obj-&gt;seq, &quot;\n&quot;;&#xD;&#xA;    	}&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    " />
  <row Id="4706" PostHistoryTypeId="2" PostId="2183" RevisionGUID="17bd8a44-d2ba-428c-96e0-aac1cb28f985" CreationDate="2017-07-31T06:45:17.843" UserId="1230" Text="I have the following Chip-Seq data and could not found a description of it. Can you help me with it? More, I would like to find out the order of the nucleotides in binding regions; that is, what number they are.&#xD;&#xA;&#xD;&#xA;    @GWZHISEQ01:319:HA1NPADXX:1:1101:1182:2196 1:N:0:ACTGAT&#xD;&#xA;    GGAGGAAGTGTGTTGCTGCAGACAGTAGCAACCAGACCCACACTGCGCGTA&#xD;&#xA;    +&#xD;&#xA;    CCCFFFFFHHFHHIJJJJIJJIIIIHGJJJJJJJCGHIJJIJJFHIIJJII&#xD;&#xA;    @GWZHISEQ01:319:HA1NPADXX:1:1101:1272:2109 1:N:0:ACTGAT&#xD;&#xA;    NCCACGCTAGGCTCAGCTTGTCGGCCTGGCTAAGCAGTTGCGAAAGTGCGC&#xD;&#xA;    +&#xD;&#xA;    #1=DDDFFHHHHHJJIJJJJJJIGEHGGJIGHJFHGJIJJJJJJJ@CHCBG&#xD;&#xA;    @GWZHISEQ01:319:HA1NPADXX:1:1101:1418:2141 1:N:0:ACTGAT&#xD;&#xA;    GCACGCACTACCCAGAGATCATCCAAAGCCTGAAGCCACAGGGCGCACTCG&#xD;&#xA;    +&#xD;&#xA;    CCCFFFFFGHGHHJJJIIJJJJIIHGGGIJIIJJIIIJJJIJJJIJIJIJH&#xD;&#xA;    @GWZHISEQ01:319:HA1NPADXX:1:1101:1302:2160 1:N:0:ACTGAT&#xD;&#xA;    ACACTCTTTCCCTACACGACGCTCTTCCGAGATCGGAAGAGCACACGTCTG&#xD;&#xA;    +&#xD;&#xA;    CCCFFFFFHDHHHJJJJIJJJJIIIIIJIHJIJJJIIJHIIJJJJJJJJHH" />
  <row Id="4707" PostHistoryTypeId="1" PostId="2183" RevisionGUID="17bd8a44-d2ba-428c-96e0-aac1cb28f985" CreationDate="2017-07-31T06:45:17.843" UserId="1230" Text="Chip-Seq data description" />
  <row Id="4708" PostHistoryTypeId="3" PostId="2183" RevisionGUID="17bd8a44-d2ba-428c-96e0-aac1cb28f985" CreationDate="2017-07-31T06:45:17.843" UserId="1230" Text="&lt;chip-seq&gt;" />
  <row Id="4709" PostHistoryTypeId="2" PostId="2184" RevisionGUID="90d5d62c-4d5e-4ff7-8257-7ba00960fd30" CreationDate="2017-07-31T07:23:14.257" UserId="77" Text="That's a fastq file, you will want to align it to the genome, call peaks, and then use something like [MEME](http://meme-suite.org/index.html) to determine binding motifs." />
  <row Id="4710" PostHistoryTypeId="10" PostId="2183" RevisionGUID="16426867-453d-4837-ad28-9b0005bc7a63" CreationDate="2017-07-31T08:31:28.373" UserId="298" Comment="103" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:298,&quot;DisplayName&quot;:&quot;terdon&quot;}]}" />
  <row Id="4711" PostHistoryTypeId="2" PostId="2185" RevisionGUID="ed4aaba8-e908-481e-acde-9e72af624faa" CreationDate="2017-07-31T09:09:58.577" UserId="235" Text="In cancer genomics experiments often involve sequencing a large number of tumour samples, but few or no matched normals. This is partly due to financial constraints, but also due to ethical ones: it is easy to get ethical approval for tumour tissue, which is removed as a matter of course in a patient's care anyway, whereas it is hard to get approval for normal tissue, which would not normally be removed and would require an additional procedure.&#xD;&#xA;&#xD;&#xA;However, this presents a problem. Most of the procedures we use in standard gene expression analysis involve comparing one group to another after suitable normalisations of the data. But here we have no suitable comparison group. To get around this problem in cancer RNA-seq analysis we do something called outlier detection. That is, we assume that for a given gene, the majority of the patients have &quot;normal&quot; expression, and look for the few patients that don't look like the rest. We do this with the Z-score.&#xD;&#xA;&#xD;&#xA;To calculate the Z-score for an observation we subtract the mean of all observations and divide by the standard deviation. Thus the Z score of an observation is how many standard deviations an observation is from the mean of all observations - or how unusual it is. &#xD;&#xA;&#xD;&#xA;Thus for Gene A in Patient 1, we calculate how many standard deviations Gene A is from the mean of Gene A across all patients. A very big (or small/negative) value tells us that the expression of Gene A is unusual in patient 1 compared to the other patients. &#xD;&#xA;&#xD;&#xA;(NB: in some cancer studies they use the median and median absolute deviation rather than the mean and standard deviation to compute a &quot;robust Z-score&quot; as the data is unlikely to be normally distributed). " />
  <row Id="4712" PostHistoryTypeId="2" PostId="2186" RevisionGUID="edbc57a0-7cfa-45fb-8e75-85f768f0f3e3" CreationDate="2017-07-31T09:43:42.450" UserId="1193" Text="I have a problem: I've managed to download a massive fasta file of 1500 sequences, but now I want to split them into seperate fasta files based on the genus.&#xD;&#xA;&#xD;&#xA;I've mamanged to exctract the genus names of my organism with this:&#xD;&#xA;&#xD;&#xA;    outfile = open('species.txt', 'w')&#xD;&#xA;    with open('terminase_large.fasta') as fd:&#xD;&#xA;        for line in fd:&#xD;&#xA;            if line.startswith('&gt;'):&#xD;&#xA;                if '[' in line:&#xD;&#xA;                    name=line.split('[')[-1]&#xD;&#xA;                    name=name.split(' ', 1)[0]&#xD;&#xA;                    outfile.write(name[:] + &quot;\n&quot;)&#xD;&#xA;    outfile.close()&#xD;&#xA;&#xD;&#xA;And got to extract only the unique names with this:&#xD;&#xA;&#xD;&#xA;    lines_seen = set()&#xD;&#xA;    outfile = open('species2.txt', &quot;w&quot;)&#xD;&#xA;    for line in open(&quot;species.txt&quot;, &quot;r&quot;):&#xD;&#xA;        if line not in lines_seen:  # not a duplicate&#xD;&#xA;            outfile.write(line)&#xD;&#xA;            lines_seen.add(line)&#xD;&#xA;    outfile.close()&#xD;&#xA;&#xD;&#xA;(Can I merge those two scripts together?)&#xD;&#xA;&#xD;&#xA;Now, my genus names look like this:&#xD;&#xA;&#xD;&#xA;    Arthrobacter&#xD;&#xA;    Achromobacter&#xD;&#xA;    Delftia&#xD;&#xA;    ....&#xD;&#xA;&#xD;&#xA;I tried automating my script to get the Entrez data, but it gives me the 'Supplied id parameter is empty' message&#xD;&#xA;&#xD;&#xA;My code looks like this:&#xD;&#xA;&#xD;&#xA;    from Bio import Entrez&#xD;&#xA;    Entrez.email = &quot;example@example.org&quot;&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;species2.txt&quot;, &quot;r&quot;):&#xD;&#xA;        searchterm = &quot;(terminase large subunit AND viruses[Organism]) AND&quot; +line+ &quot;AND refseq[Filter]&quot;&#xD;&#xA;        searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=searchterm, retmax=1000)&#xD;&#xA;        searchResult = Entrez.read(searchResultHandle)&#xD;&#xA;        ids = searchResult[&quot;IdList&quot;]&#xD;&#xA;    &#xD;&#xA;        handle = Entrez.efetch(db=&quot;protein&quot;, id=ids, rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xD;&#xA;        record = handle.read()&#xD;&#xA;    &#xD;&#xA;        out_handle = open('terminase_large_'+str(line[:-1])+'.fasta', 'w')&#xD;&#xA;        out_handle.write(record.rstrip('\n'))&#xD;&#xA;&#xD;&#xA;Can someone help me with it?" />
  <row Id="4713" PostHistoryTypeId="1" PostId="2186" RevisionGUID="edbc57a0-7cfa-45fb-8e75-85f768f0f3e3" CreationDate="2017-07-31T09:43:42.450" UserId="1193" Text="Searching Entrez with every line of a file as a keyword" />
  <row Id="4714" PostHistoryTypeId="3" PostId="2186" RevisionGUID="edbc57a0-7cfa-45fb-8e75-85f768f0f3e3" CreationDate="2017-07-31T09:43:42.450" UserId="1193" Text="&lt;fasta&gt;&lt;python&gt;&lt;biopython&gt;" />
  <row Id="4715" PostHistoryTypeId="5" PostId="2186" RevisionGUID="2e349941-91bf-421e-ba95-af538dd89162" CreationDate="2017-07-31T09:54:39.217" UserId="1193" Comment="added the expamples of intput and output files" Text="I have a problem: I've managed to download a massive fasta file of 1500 sequences, but now I want to split them into seperate fasta files based on the genus.&#xD;&#xA;&#xD;&#xA;EDIT&#xD;&#xA;The fasta file looks like this:&#xD;&#xA;&#xD;&#xA;    &gt;YP_009300697.1 terminase large subunit [Arthrobacter phage Mudcat]&#xD;&#xA;    MGLSNTATPLYYGQF...&#xD;&#xA;    &#xD;&#xA;    &gt;YP_009208724.1 hypothetical protein ADP65_00072 [Achromobacter phage phiAxp-3]&#xD;&#xA;    MSNVLLKQELDEWL...&#xD;&#xA;    &#xD;&#xA;    &gt;YP_009148449.1 large terminase subunit [Delftia phage RG-2014]&#xD;&#xA;    MSEPRKLVKKTLD...&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;I would like to end up with something like this:&#xD;&#xA;&#xD;&#xA;    &gt;Mycobacterium phage JAMaL&#xD;&#xA;    MVRKKPPPELE...&#xD;&#xA;    &#xD;&#xA;    &gt;Mycobacterium phage Bruin&#xD;&#xA;    MEVCGYTLDDI...&#xD;&#xA;    &#xD;&#xA;    &gt;Mycobacterium phage Zaka&#xD;&#xA;    MSLDNHLPELA...&#xD;&#xA;&#xD;&#xA;So that the label on the alignment would have the name of the phage and not the protein's ID&#xD;&#xA;&#xD;&#xA;I've mamanged to exctract the genus names of my organism with this:&#xD;&#xA;&#xD;&#xA;    outfile = open('species.txt', 'w')&#xD;&#xA;    with open('terminase_large.fasta') as fd:&#xD;&#xA;        for line in fd:&#xD;&#xA;            if line.startswith('&gt;'):&#xD;&#xA;                if '[' in line:&#xD;&#xA;                    name=line.split('[')[-1]&#xD;&#xA;                    name=name.split(' ', 1)[0]&#xD;&#xA;                    outfile.write(name[:] + &quot;\n&quot;)&#xD;&#xA;    outfile.close()&#xD;&#xA;&#xD;&#xA;And got to extract only the unique names with this:&#xD;&#xA;&#xD;&#xA;    lines_seen = set()&#xD;&#xA;    outfile = open('species2.txt', &quot;w&quot;)&#xD;&#xA;    for line in open(&quot;species.txt&quot;, &quot;r&quot;):&#xD;&#xA;        if line not in lines_seen:  # not a duplicate&#xD;&#xA;            outfile.write(line)&#xD;&#xA;            lines_seen.add(line)&#xD;&#xA;    outfile.close()&#xD;&#xA;&#xD;&#xA;(Can I merge those two scripts together?)&#xD;&#xA;&#xD;&#xA;Now, my genus names look like this:&#xD;&#xA;&#xD;&#xA;    Arthrobacter&#xD;&#xA;    Achromobacter&#xD;&#xA;    Delftia&#xD;&#xA;    ....&#xD;&#xA;&#xD;&#xA;I tried automating my script to get the Entrez data, but it gives me the 'Supplied id parameter is empty' message&#xD;&#xA;&#xD;&#xA;My code looks like this:&#xD;&#xA;&#xD;&#xA;    from Bio import Entrez&#xD;&#xA;    Entrez.email = &quot;example@example.org&quot;&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;species2.txt&quot;, &quot;r&quot;):&#xD;&#xA;        searchterm = &quot;(terminase large subunit AND viruses[Organism]) AND&quot; +line+ &quot;AND refseq[Filter]&quot;&#xD;&#xA;        searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=searchterm, retmax=1000)&#xD;&#xA;        searchResult = Entrez.read(searchResultHandle)&#xD;&#xA;        ids = searchResult[&quot;IdList&quot;]&#xD;&#xA;    &#xD;&#xA;        handle = Entrez.efetch(db=&quot;protein&quot;, id=ids, rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xD;&#xA;        record = handle.read()&#xD;&#xA;    &#xD;&#xA;        out_handle = open('terminase_large_'+str(line[:-1])+'.fasta', 'w')&#xD;&#xA;        out_handle.write(record.rstrip('\n'))&#xD;&#xA;&#xD;&#xA;Can someone help me with it?" />
  <row Id="4716" PostHistoryTypeId="2" PostId="2187" RevisionGUID="7cb88f69-33a7-4bdc-a026-a9468ca9bf73" CreationDate="2017-07-31T10:35:37.210" UserId="298" Text="Splitting into multiple files and changing the IDs can be easily done:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    perl -pe 'if(/&gt;/){/\[(.*?)\]\s*$/; $_=&quot;&gt; $1\n&quot;}' file.fa | &#xD;&#xA;        awk '(/^&gt;/){name=$2} {print &gt;&gt; name&quot;.fa&quot;}'&#xD;&#xA;&#xD;&#xA;That assumes all your FASTA headers have `[foo bar baz]` as the last element of a line. It will create a file called `foo.fa` (the bacterium's name) with all sequences saved there. &#xD;&#xA;&#xD;&#xA;###Explanation&#xD;&#xA;&#xD;&#xA;* `perl -pe` : run the script given by `-e` on each line of the input file, and print the resulting line. &#xD;&#xA;* `if(/&gt;/)` : if this line starts with a `&gt;`. \&#xD;&#xA;* `/\[(.*?)\]\s*$/` : match an opening bracket (`\[`), then capture (that's what the parentheses do, they capture a pattern so we can refer to it as `$1`) everything until the first `]` (`.*?\]`)&#xD;&#xA;* `$_=&quot;&gt; $1\n&quot;` : the `$_` special variable in Perl is (in this case) the current line. So, `$_=foo` means &quot;make the current line read `foo`. Since the `-p` prints each input line, changing the value of `$_` means the changed value will be printed. So here, we are printing `&gt;`, whatever was in the square brackets (`$1`) and a newline character. &#xD;&#xA;&#xD;&#xA;The output of the perl command alone on your example input file is:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    $ perl -pe 'if(/&gt;/){/\[(.*)\]\s*$/; $_=&quot;&gt; $1\n&quot;}' file.fa&#xD;&#xA;    &gt; Arthrobacter phage Mudcat&#xD;&#xA;    MGLSNTATPLYYGQF...&#xD;&#xA;    &#xD;&#xA;    &gt; Achromobacter phage phiAxp-3&#xD;&#xA;    MSNVLLKQELDEWL...&#xD;&#xA;    &#xD;&#xA;    &gt; Delftia phage RG-2014&#xD;&#xA;    MSEPRKLVKKTLD...&#xD;&#xA;    &#xD;&#xA;So, we now pass it through `awk` which does:&#xD;&#xA;&#xD;&#xA;* `(/^&gt;/){a=$2}` : if this line starts with an `&gt;`, save the second field (the bacterial species) as the variable `name`. &#xD;&#xA;* `{print &gt;&gt; name&quot;.fa&quot;}` : print each line into a file whose name is the current value of the variable `name` with a `.fa.` extension. &#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;As for your script, you've got the right idea, but have a small bug. You forgot to remove the `\n` from your input file, so it looks for `Arthrobacter\n` instead of `Arthrobacter`. The golden rule of debugging is &quot;print all the things&quot;. If you add `print(&quot;Searchterm: &quot;,searchterm)`  you will see:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDArthrobacter  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDAchromobacter  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDDelftia  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;So, you need to remove the newline characters and add a space like so (I also made it a bit more &quot;pythonic&quot; and conforming to the Python syntax guidelines):&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import Entrez&#xD;&#xA;    Entrez.email = &quot;example@example.org&quot;&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;species2.txt&quot;, &quot;r&quot;):&#xD;&#xA;        line = line.rstrip()&#xD;&#xA;        searchterm = (&quot;(terminase large subunit AND viruses[Organism]) &quot; +&#xD;&#xA;                      &quot;AND %s AND refseq[Filter]&quot; % line)&#xD;&#xA;        print(&quot;Searchterm: &quot;, searchterm)&#xD;&#xA;        searchResultHandle = Entrez.esearch(db=&quot;protein&quot;,&#xD;&#xA;                                            term=searchterm, retmax=1000)&#xD;&#xA;        searchResult = Entrez.read(searchResultHandle)&#xD;&#xA;        ids = searchResult[&quot;IdList&quot;]&#xD;&#xA;    &#xD;&#xA;        handle = Entrez.efetch(db=&quot;protein&quot;, id=ids,&#xD;&#xA;                               rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xD;&#xA;        record = handle.read()&#xD;&#xA;    &#xD;&#xA;        out_handle = open('terminase_large_' + str(line[:-1]) + '.fasta', 'w')&#xD;&#xA;        out_handle.write(record.rstrip('\n'))&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;" />
  <row Id="4717" PostHistoryTypeId="5" PostId="2187" RevisionGUID="5584248f-759f-4454-ba83-8898a1282dfa" CreationDate="2017-07-31T11:17:45.677" UserId="298" Comment="added 566 characters in body" Text="Splitting into multiple files and changing the IDs can be easily done:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    perl -pe 'if(/&gt;/){/\[(.*?)\]\s*$/; $_=&quot;&gt; $1\n&quot;}' file.fa | &#xD;&#xA;        awk '(/^&gt;/){name=$2} {print &gt;&gt; name&quot;.fa&quot;}'&#xD;&#xA;&#xD;&#xA;That assumes all your FASTA headers have `[foo bar baz]` as the last element of a line. It will create a file called `foo.fa` (the bacterium's name) with all sequences saved there. &#xD;&#xA;&#xD;&#xA;###Explanation&#xD;&#xA;&#xD;&#xA;* `perl -pe` : run the script given by `-e` on each line of the input file, and print the resulting line. &#xD;&#xA;* `if(/&gt;/)` : if this line starts with a `&gt;`. \&#xD;&#xA;* `/\[(.*?)\]\s*$/` : match an opening bracket (`\[`), then capture (that's what the parentheses do, they capture a pattern so we can refer to it as `$1`) everything until the first `]` (`.*?\]`)&#xD;&#xA;* `$_=&quot;&gt; $1\n&quot;` : the `$_` special variable in Perl is (in this case) the current line. So, `$_=foo` means &quot;make the current line read `foo`. Since the `-p` prints each input line, changing the value of `$_` means the changed value will be printed. So here, we are printing `&gt;`, whatever was in the square brackets (`$1`) and a newline character. &#xD;&#xA;&#xD;&#xA;The output of the perl command alone on your example input file is:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    $ perl -pe 'if(/&gt;/){/\[(.*)\]\s*$/; $_=&quot;&gt; $1\n&quot;}' file.fa&#xD;&#xA;    &gt; Arthrobacter phage Mudcat&#xD;&#xA;    MGLSNTATPLYYGQF...&#xD;&#xA;    &#xD;&#xA;    &gt; Achromobacter phage phiAxp-3&#xD;&#xA;    MSNVLLKQELDEWL...&#xD;&#xA;    &#xD;&#xA;    &gt; Delftia phage RG-2014&#xD;&#xA;    MSEPRKLVKKTLD...&#xD;&#xA;    &#xD;&#xA;So, we now pass it through `awk` which does:&#xD;&#xA;&#xD;&#xA;* `(/^&gt;/){a=$2}` : if this line starts with an `&gt;`, save the second field (the bacterial species) as the variable `name`. &#xD;&#xA;* `{print &gt;&gt; name&quot;.fa&quot;}` : print each line into a file whose name is the current value of the variable `name` with a `.fa.` extension. &#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;If you prefer python scripts to the one-liner approach, you can do the same thing with:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    import re&#xD;&#xA;    outFile = None&#xD;&#xA;    for line in open(&quot;file.fa&quot;, &quot;r&quot;):&#xD;&#xA;        line = line.rstrip()&#xD;&#xA;        if line.startswith('&gt;'):&#xD;&#xA;            regex = re.compile('.*\[((.+?)\s+.*?)\].*')&#xD;&#xA;            matches = regex.search(line)&#xD;&#xA;            species = matches[2]&#xD;&#xA;            outFile = open(species + &quot;.fa&quot;, 'a')&#xD;&#xA;            outFile.write('&gt;%s\n' % matches[1])&#xD;&#xA;        else:&#xD;&#xA;            outFile.write(&quot;%s\n&quot; % line)&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;As for your script, you've got the right idea, but have a small bug. You forgot to remove the `\n` from your input file, so it looks for `Arthrobacter\n` instead of `Arthrobacter`. The golden rule of debugging is &quot;print all the things&quot;. If you add `print(&quot;Searchterm: &quot;,searchterm)`  you will see:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDArthrobacter  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDAchromobacter  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDDelftia  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;So, you need to remove the newline characters and add a space like so (I also made it a bit more &quot;pythonic&quot; and conforming to the Python syntax guidelines):&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import Entrez&#xD;&#xA;    Entrez.email = &quot;example@example.org&quot;&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;species2.txt&quot;, &quot;r&quot;):&#xD;&#xA;        line = line.rstrip()&#xD;&#xA;        searchterm = (&quot;(terminase large subunit AND viruses[Organism]) &quot; +&#xD;&#xA;                      &quot;AND %s AND refseq[Filter]&quot; % line)&#xD;&#xA;        print(&quot;Searchterm: &quot;, searchterm)&#xD;&#xA;        searchResultHandle = Entrez.esearch(db=&quot;protein&quot;,&#xD;&#xA;                                            term=searchterm, retmax=1000)&#xD;&#xA;        searchResult = Entrez.read(searchResultHandle)&#xD;&#xA;        ids = searchResult[&quot;IdList&quot;]&#xD;&#xA;    &#xD;&#xA;        handle = Entrez.efetch(db=&quot;protein&quot;, id=ids,&#xD;&#xA;                               rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xD;&#xA;        record = handle.read()&#xD;&#xA;    &#xD;&#xA;        out_handle = open('terminase_large_' + str(line[:-1]) + '.fasta', 'w')&#xD;&#xA;        out_handle.write(record.rstrip('\n'))&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;" />
  <row Id="4718" PostHistoryTypeId="5" PostId="2186" RevisionGUID="3f8f3c8c-8137-4d97-8efd-7e48f7880204" CreationDate="2017-07-31T11:51:43.040" UserId="1193" Comment="added 216 characters in body" Text="I have a problem: I've managed to download a massive fasta file of 1500 sequences, but now I want to split them into seperate fasta files based on the genus.&#xD;&#xA;&#xD;&#xA;EDIT&#xD;&#xA;The fasta file looks like this:&#xD;&#xA;&#xD;&#xA;    terminase_large.fasta&#xD;&#xA;    &gt;YP_009300697.1 terminase large subunit [Arthrobacter phage Mudcat]&#xD;&#xA;    MGLSNTATPLYYGQF...&#xD;&#xA;    &#xD;&#xA;    &gt;YP_009208724.1 hypothetical protein ADP65_00072 [Achromobacter phage phiAxp-3]&#xD;&#xA;    MSNVLLKQELDEWL...&#xD;&#xA;    &#xD;&#xA;    &gt;YP_009148449.1 large terminase subunit [Delftia phage RG-2014]&#xD;&#xA;    MSEPRKLVKKTLD...&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;I would like to end up with something like this:&#xD;&#xA;&#xD;&#xA;    mycobacterium_terminase.fasta&#xD;&#xA;    &gt;Mycobacterium phage JAMaL&#xD;&#xA;    MVRKKPPPELE...&#xD;&#xA;    &#xD;&#xA;    &gt;Mycobacterium phage Bruin&#xD;&#xA;    MEVCGYTLDDI...&#xD;&#xA;    &#xD;&#xA;    &gt;Mycobacterium phage Zaka&#xD;&#xA;    MSLDNHLPELA...&#xD;&#xA;&#xD;&#xA;    salmonella_terminase.fasta&#xD;&#xA;    &gt;Salmonella virus SETP7&#xD;&#xA;    MNVDITATEPQ...&#xD;&#xA;    &#xD;&#xA;    &gt;Salmonella virus SE2&#xD;&#xA;    MEGGSDSLIAM...&#xD;&#xA;    and so on...&#xD;&#xA;&#xD;&#xA;So that the label on the alignment would have the name of the phage and not the protein's ID&#xD;&#xA;&#xD;&#xA;I've mamanged to exctract the genus names of my organism with this:&#xD;&#xA;&#xD;&#xA;    outfile = open('species.txt', 'w')&#xD;&#xA;    with open('terminase_large.fasta') as fd:&#xD;&#xA;        for line in fd:&#xD;&#xA;            if line.startswith('&gt;'):&#xD;&#xA;                if '[' in line:&#xD;&#xA;                    name=line.split('[')[-1]&#xD;&#xA;                    name=name.split(' ', 1)[0]&#xD;&#xA;                    outfile.write(name[:] + &quot;\n&quot;)&#xD;&#xA;    outfile.close()&#xD;&#xA;&#xD;&#xA;And got to extract only the unique names with this:&#xD;&#xA;&#xD;&#xA;    lines_seen = set()&#xD;&#xA;    outfile = open('species2.txt', &quot;w&quot;)&#xD;&#xA;    for line in open(&quot;species.txt&quot;, &quot;r&quot;):&#xD;&#xA;        if line not in lines_seen:  # not a duplicate&#xD;&#xA;            outfile.write(line)&#xD;&#xA;            lines_seen.add(line)&#xD;&#xA;    outfile.close()&#xD;&#xA;&#xD;&#xA;(Can I merge those two scripts together?)&#xD;&#xA;&#xD;&#xA;Now, my genus names look like this:&#xD;&#xA;&#xD;&#xA;    Arthrobacter&#xD;&#xA;    Achromobacter&#xD;&#xA;    Delftia&#xD;&#xA;    ....&#xD;&#xA;&#xD;&#xA;I tried automating my script to get the Entrez data, but it gives me the 'Supplied id parameter is empty' message&#xD;&#xA;&#xD;&#xA;My code looks like this:&#xD;&#xA;&#xD;&#xA;    from Bio import Entrez&#xD;&#xA;    Entrez.email = &quot;example@example.org&quot;&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;species2.txt&quot;, &quot;r&quot;):&#xD;&#xA;        searchterm = &quot;(terminase large subunit AND viruses[Organism]) AND&quot; +line+ &quot;AND refseq[Filter]&quot;&#xD;&#xA;        searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=searchterm, retmax=1000)&#xD;&#xA;        searchResult = Entrez.read(searchResultHandle)&#xD;&#xA;        ids = searchResult[&quot;IdList&quot;]&#xD;&#xA;    &#xD;&#xA;        handle = Entrez.efetch(db=&quot;protein&quot;, id=ids, rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xD;&#xA;        record = handle.read()&#xD;&#xA;    &#xD;&#xA;        out_handle = open('terminase_large_'+str(line[:-1])+'.fasta', 'w')&#xD;&#xA;        out_handle.write(record.rstrip('\n'))&#xD;&#xA;&#xD;&#xA;Can someone help me with it?" />
  <row Id="4719" PostHistoryTypeId="50" PostId="266" RevisionGUID="344a4061-328d-4430-a21c-7b85d21604e0" CreationDate="2017-07-31T12:57:27.853" UserId="-1" />
  <row Id="4721" PostHistoryTypeId="2" PostId="2188" RevisionGUID="4ab6a1fe-991d-48e6-9de8-6f88b64c931f" CreationDate="2017-07-31T21:12:27.573" UserId="1234" Text="I also think pooling is the better option, followed by partitioning by coverage/taxonomy of contigs.&#xD;&#xA;&#xD;&#xA;Maybe check out [BlobTools](https://github.com/DRL/blobtools), which helps with filtering read-pairs by taxonomy of contigs they contribute to and also does nice visualisations of assemblies.&#xD;&#xA;&#xD;&#xA;[Workflow B](https://blobtools.readme.io/docs/the-blobtools-workflows) seems to be what you want.&#xD;&#xA;&#xD;&#xA;*Disclaimer: I am the developer of this tool.*" />
  <row Id="4722" PostHistoryTypeId="4" PostId="2186" RevisionGUID="23cdd4f0-8f15-4a63-aa8e-aab8536ef616" CreationDate="2017-08-01T05:57:48.453" UserId="73" Comment="changed title to fit question" Text="Subset FASTA file by species name" />
  <row Id="4723" PostHistoryTypeId="2" PostId="2189" RevisionGUID="306ac441-7755-4081-ae59-fff927673d4e" CreationDate="2017-08-01T10:33:20.660" UserId="-1" Text="" />
  <row Id="4724" PostHistoryTypeId="1" PostId="2189" RevisionGUID="306ac441-7755-4081-ae59-fff927673d4e" CreationDate="2017-08-01T10:33:20.660" UserId="-1" />
  <row Id="4725" PostHistoryTypeId="2" PostId="2190" RevisionGUID="e08715e2-efa3-4fc4-bed4-3f56f2082159" CreationDate="2017-08-01T10:33:20.660" UserId="-1" Text="" />
  <row Id="4726" PostHistoryTypeId="1" PostId="2190" RevisionGUID="e08715e2-efa3-4fc4-bed4-3f56f2082159" CreationDate="2017-08-01T10:33:20.660" UserId="-1" />
  <row Id="4727" PostHistoryTypeId="5" PostId="2189" RevisionGUID="3371f75c-3c47-40d6-b19b-47c64b29abed" CreationDate="2017-08-01T10:47:49.007" UserId="1140" Comment="added 189 characters in body" Text="RNA-seq (RNA sequencing) is a widely used approach to study gene expression. Questions should include this tag if they pertain to issues related to bioinformatics analysis of RNA-seq data. " />
  <row Id="4728" PostHistoryTypeId="24" PostId="2189" RevisionGUID="3371f75c-3c47-40d6-b19b-47c64b29abed" CreationDate="2017-08-01T10:47:49.007" Comment="Proposed by 1140 approved by 77 edit id of 246" />
  <row Id="4729" PostHistoryTypeId="5" PostId="2190" RevisionGUID="d967aa16-01dc-4f27-9c3b-42180ebc4e4c" CreationDate="2017-08-01T10:47:55.073" UserId="1140" Comment="added 189 characters in body" Text="RNA-seq (RNA sequencing) is a widely used approach to study gene expression. Questions should include this tag if they pertain to issues related to bioinformatics analysis of RNA-seq data. " />
  <row Id="4730" PostHistoryTypeId="24" PostId="2190" RevisionGUID="d967aa16-01dc-4f27-9c3b-42180ebc4e4c" CreationDate="2017-08-01T10:47:55.073" Comment="Proposed by 1140 approved by 77 edit id of 247" />
  <row Id="4736" PostHistoryTypeId="5" PostId="2179" RevisionGUID="f53cf26e-020b-4f12-9573-266050db658f" CreationDate="2017-08-01T12:57:03.253" UserId="48" Comment="polish the question a bit" Text="    use Bio::DB::GenBank;&#xD;&#xA;    use Bio::DB::Query::GenBank;&#xD;&#xA;    &#xD;&#xA;    $query = &quot;LEGK&quot;; &#xD;&#xA;    $query_obj = Bio::DB::Query::GenBank-&gt;new(-db =&gt; 'protein', &#xD;&#xA;                                              -query =&gt; $query );&#xD;&#xA;    &#xD;&#xA;    $gb_obj = Bio::DB::GenBank-&gt;new;&#xD;&#xA;    &#xD;&#xA;    $stream_obj = $gb_obj-&gt;get_Stream_by_query($query_obj);&#xD;&#xA;    &#xD;&#xA;    while ($seq_obj = $stream_obj-&gt;next_seq) {&#xD;&#xA;        # do something with the sequence object    &#xD;&#xA;        print &quot;&gt;$query&quot;,' ', $seq_obj-&gt;display_id, ' ', $seq_obj-&gt;desc,&quot;\n&quot;, $seq_obj-&gt;seq[,'\n';&#xD;&#xA;&#xD;&#xA;How can I print first occurrence of protein sequence?" />
  <row Id="4737" PostHistoryTypeId="24" PostId="2179" RevisionGUID="f53cf26e-020b-4f12-9573-266050db658f" CreationDate="2017-08-01T12:57:03.253" Comment="Proposed by 48 approved by 29, 77 edit id of 248" />
  <row Id="4740" PostHistoryTypeId="2" PostId="2192" RevisionGUID="5a4f617a-0331-4b32-8337-b5b7e5086800" CreationDate="2017-08-01T13:53:26.140" UserId="-1" Text="" />
  <row Id="4741" PostHistoryTypeId="1" PostId="2192" RevisionGUID="5a4f617a-0331-4b32-8337-b5b7e5086800" CreationDate="2017-08-01T13:53:26.140" UserId="-1" />
  <row Id="4742" PostHistoryTypeId="2" PostId="2193" RevisionGUID="c0c2ae56-56e8-479d-92e0-0ec167167952" CreationDate="2017-08-01T13:53:26.140" UserId="-1" Text="" />
  <row Id="4743" PostHistoryTypeId="1" PostId="2193" RevisionGUID="c0c2ae56-56e8-479d-92e0-0ec167167952" CreationDate="2017-08-01T13:53:26.140" UserId="-1" />
  <row Id="4744" PostHistoryTypeId="5" PostId="2193" RevisionGUID="1856eaa2-64d1-467e-a24f-62033cdbefcb" CreationDate="2017-08-01T14:02:54.490" UserId="1140" Comment="added 105 characters in body" Text="bwa (Burrows-Wheeler Aligner) is a software for aligning reads obtained with Next Generation Sequencing. " />
  <row Id="4745" PostHistoryTypeId="24" PostId="2193" RevisionGUID="1856eaa2-64d1-467e-a24f-62033cdbefcb" CreationDate="2017-08-01T14:02:54.490" Comment="Proposed by 1140 approved by 77 edit id of 250" />
  <row Id="4746" PostHistoryTypeId="5" PostId="2192" RevisionGUID="28e9812d-0f55-45e7-8823-5d4d5cec71c1" CreationDate="2017-08-01T14:02:58.737" UserId="1140" Comment="added 297 characters in body" Text="bwa (Burrows-Wheeler Aligner) is a software for aligning reads obtained with Next Generation Sequencing. Questions related to the functionalities of bwa should include this tag. Please consider specifying if you are using bwa aln or bwa mem. Fro more info: http://bio-bwa.sourceforge.net/bwa.shtml" />
  <row Id="4747" PostHistoryTypeId="24" PostId="2192" RevisionGUID="28e9812d-0f55-45e7-8823-5d4d5cec71c1" CreationDate="2017-08-01T14:02:58.737" Comment="Proposed by 1140 approved by 77 edit id of 249" />
  <row Id="4748" PostHistoryTypeId="2" PostId="2194" RevisionGUID="dbae8454-a138-4464-828f-8b3c9d2ee8a4" CreationDate="2017-08-01T16:24:34.650" UserId="29" Text="I’ve got an RNA-seq dataset with a large proportion of environmental RNA “contamination”. BLASTing random reads reveals that much of the data comes from bacterial, plant and viral RNA. My target organism only accounts for ~5% of the RNA-seq read data.&#xD;&#xA;&#xD;&#xA;I would like to get a more or less (more, if possible) comprehensive overview of what species are found in my sample — bacteria, plants, animals (?), but also viruses. How can I perform this?&#xD;&#xA;&#xD;&#xA;I have been unable to find a “standard” way of performing a metagenome screen. BLAST online services all seem to be severely rate limited (certainly unable to handle uploading ~80 M reads). Installing BLAST (or a similar tool) locally is of course not an issue, but I can’t find a comprehensive database that spans all phyla — the best I’ve been able to find are databases, such as [NCBI-NR](https://www.ncbi.nlm.nih.gov/refseq/about/nonredundantproteins/), that are restricted to single phyla or classes, or to bacteria.&#xD;&#xA;&#xD;&#xA;Ideally I’d like a workflow that I feed with my RNA-seq data, and get out a list of species with % of reads mapped." />
  <row Id="4749" PostHistoryTypeId="1" PostId="2194" RevisionGUID="dbae8454-a138-4464-828f-8b3c9d2ee8a4" CreationDate="2017-08-01T16:24:34.650" UserId="29" Text="How do I efficiently perform a metagenome screen of “all” species?" />
  <row Id="4750" PostHistoryTypeId="3" PostId="2194" RevisionGUID="dbae8454-a138-4464-828f-8b3c9d2ee8a4" CreationDate="2017-08-01T16:24:34.650" UserId="29" Text="&lt;rna-seq&gt;&lt;metagenome&gt;" />
  <row Id="4751" PostHistoryTypeId="2" PostId="2195" RevisionGUID="62e4693b-9366-40b9-8911-ba5a2de4b66e" CreationDate="2017-08-01T17:36:43.390" UserId="298" Text="I think you're just looking for `nr`. It is absolutely not limited to prokaryotes, far from it. It is, according to the blurb on the NCBI's blast page:&#xD;&#xA;&#xD;&#xA;&gt; The nucleotide collection consists of GenBank+EMBL+DDBJ+PDB+RefSeq sequences, but excludes EST, STS, GSS, WGS, TSA, patent sequences as well as phase 0, 1, and 2 HTGS sequences. The database is non-redundant. Identical sequences have been merged into one entry, while preserving the accession, GI, title and taxonomy information for each entry.&#xD;&#xA;&#xD;&#xA;The link you gave in your question doesn't describe the `nr` database, but the collection of RefSeq non-redundant sequences. The two are not the same thing at all. The nr databases contains sequences from all available species so it should be perfect for what you're after. &#xD;&#xA;&#xD;&#xA;Now, I admit I haven't tried this with millions of redas, but I did used to run many blast searches with several thousand sequences using the command line blast tools and the remote nr database. I just checked the [blast+ docs][1] now and see no mention of a rate limit. Which isn't to say there isn't one, for sure, but since each query is run sequentially when you give a multifasta file, there seems to reason to limit since the servers should be able to queue the jobs as needed. &#xD;&#xA;&#xD;&#xA;So, don't use the API or roll your own, just [download NCBI's blast client][2] and try that:&#xD;&#xA;&#xD;&#xA;    blastn -query query.fa -db nr -task blastn &#xD;&#xA;&#xD;&#xA;If that complains, you'll have to download the `nr` DB (well, since you want nucleotide blast, you probably want its nucleotide cousin, `nt`) which you can find here: ftp://ftp.ncbi.nlm.nih.gov/blast/db/. Just get all the `foo.nt*` files.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/books/NBK279690/&#xD;&#xA;  [2]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=Download" />
  <row Id="4752" PostHistoryTypeId="50" PostId="960" RevisionGUID="fe7be704-5f32-41b4-bdab-ee0ab537cddc" CreationDate="2017-08-01T21:19:10.073" UserId="-1" />
  <row Id="4753" PostHistoryTypeId="2" PostId="2196" RevisionGUID="2a91aff3-fdc0-42fd-8016-7b7ce7a4ce35" CreationDate="2017-08-01T21:54:51.507" UserId="73" Text="As you point out, BLAST is not going to work well for reads from a typical RNASeq experiment.&#xD;&#xA;&#xD;&#xA;From an RNASeq dataset, you might be able to get away with just a ribosomal screen using [Silva](https://www.arb-silva.de/projects/ssu-ref-nr/), because the ribosome is the most abundantly transcribed gene. This will usually be the case even if polyA selection or ribosomal depletion was carried out on a sample.&#xD;&#xA;&#xD;&#xA;However, bear in mind that there will be a lot of false results due to database incompleteness. I used [Kraken](http://ccb.jhu.edu/software/kraken/) when I did something like this a few years ago, and ended up with one sequence annotated as a fruit bat (where my subject was a water-living planarian flatworm). [Centrifuge](https://github.com/infphilo/centrifuge/) would be a better option now; the authors even claim that [they can store the entirety of nr in a 70 GB index](http://genome.cshlp.org/content/26/12/1721.full) with Centrifuge (which would be better / more comprehensive than just a ribosomal screen)." />
  <row Id="4754" PostHistoryTypeId="2" PostId="2197" RevisionGUID="a5983124-aaba-4ad6-8ed6-3ccb33e8da20" CreationDate="2017-08-01T22:16:10.137" UserId="73" Text="We have added ERCC spike-ins to all our RNASeq data, just in case other people might find it useful in the future. However, I have never used it in my own analyses because I can't think of a reasonable way that it could be used.&#xD;&#xA;&#xD;&#xA;The typical recommendation for ERCC is to add it in proportion to the input RNA amount, but that makes an assumption that total cell RNA counts are similar across different cells (which is demonstrably false by looking at single cell RNASeq results).&#xD;&#xA;&#xD;&#xA;I have yet to think of a situation where ERCC would provide better results than a &quot;housekeeping&quot; gene set sampled from the original reads." />
  <row Id="4755" PostHistoryTypeId="2" PostId="2198" RevisionGUID="bb6cc540-58cb-45f4-a9cf-0168d85dcf31" CreationDate="2017-08-01T22:25:13.990" UserId="73" Text="Albacore produces a `sequencing_summary.txt` file (actually TSV, not CSV) in the same directory as the `workspace` folder that might have the data that you want in it. Here are the fields present in that file:&#xD;&#xA;&#xD;&#xA; * filename&#xD;&#xA; * **read_id**&#xD;&#xA; * run_id&#xD;&#xA; * channel&#xD;&#xA; * **start_time**&#xD;&#xA; * duration&#xD;&#xA; * num_events&#xD;&#xA; * template_start&#xD;&#xA; * num_events_template&#xD;&#xA; * template_duration&#xD;&#xA; * num_called_template&#xD;&#xA; * sequence_length_template&#xD;&#xA; * mean_qscore_template&#xD;&#xA; * strand_score_template&#xD;&#xA; * complement_start&#xD;&#xA; * num_events_complement&#xD;&#xA; * complement_duration&#xD;&#xA; * num_called_complement&#xD;&#xA; * sequence_length_complement&#xD;&#xA; * mean_qscore_complement&#xD;&#xA; * strand_score_complement&#xD;&#xA; * sequence_length_2d&#xD;&#xA; * mean_qscore_2d" />
  <row Id="4756" PostHistoryTypeId="4" PostId="2174" RevisionGUID="a3388980-dcea-4a3f-8fbf-130a54afee73" CreationDate="2017-08-01T22:27:22.980" UserId="73" Comment="added nanopore to tag and title, restrucure title to fit question" Text="Extract nanopore read ID &amp; start times from fastq file" />
  <row Id="4757" PostHistoryTypeId="6" PostId="2174" RevisionGUID="a3388980-dcea-4a3f-8fbf-130a54afee73" CreationDate="2017-08-01T22:27:22.980" UserId="73" Comment="added nanopore to tag and title, restrucure title to fit question" Text="&lt;nanopore&gt;&lt;fastq&gt;&lt;csv&gt;" />
  <row Id="4758" PostHistoryTypeId="2" PostId="2199" RevisionGUID="d8c564a3-7b97-45e0-bfd0-2c05b94685c8" CreationDate="2017-08-02T05:27:40.667" UserId="315" Text="I have a question regarding the NGS (RNA-Seq) analysis.&#xD;&#xA;&#xD;&#xA;We have a time course data regarding a mouse model wt and mutant treated with a drug (10uM) and the taken down at different time points: 8 time points and did RNA-seq on these samples currently we have tpm values and raw gene counts from rna seq. Also we have only one sample for each time point. we would like to see for changes in genes across different time point but not sure statistical method should we use to determine this, should we consider for example a single time point wt vs mt, or see trends across wt and mt individually &#xD;&#xA;I know that there are some bioc packages for NGS analysis (edgeR,&#xD;&#xA;DESeq,etc). Is any package or method good for one-sample-per-time-point&#xD;&#xA;data?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Thank you." />
  <row Id="4759" PostHistoryTypeId="1" PostId="2199" RevisionGUID="d8c564a3-7b97-45e0-bfd0-2c05b94685c8" CreationDate="2017-08-02T05:27:40.667" UserId="315" Text="ngs time course experiment" />
  <row Id="4760" PostHistoryTypeId="3" PostId="2199" RevisionGUID="d8c564a3-7b97-45e0-bfd0-2c05b94685c8" CreationDate="2017-08-02T05:27:40.667" UserId="315" Text="&lt;rna-seq&gt;" />
  <row Id="4761" PostHistoryTypeId="6" PostId="2199" RevisionGUID="e3f33761-ac27-4595-806b-6c687d1b3e87" CreationDate="2017-08-02T08:56:11.040" UserId="982" Comment="Additional tags" Text="&lt;rna-seq&gt;&lt;software-recommendation&gt;&lt;visualization&gt;&lt;data&gt;" />
  <row Id="4762" PostHistoryTypeId="24" PostId="2199" RevisionGUID="e3f33761-ac27-4595-806b-6c687d1b3e87" CreationDate="2017-08-02T08:56:11.040" Comment="Proposed by 982 approved by 73, -1 edit id of 251" />
  <row Id="4763" PostHistoryTypeId="5" PostId="2199" RevisionGUID="6223c577-95fe-48fc-9dcb-01ef8f19db41" CreationDate="2017-08-02T08:56:11.040" UserId="57" Comment="Additional tags; deleted courtesy" Text="I have a question regarding the RNA-Seq analysis.&#xD;&#xA;&#xD;&#xA;We have a time course data regarding a mouse model wt and mutant treated with a drug (10uM) and the taken down at different time points: 8 time points and did RNA-seq on these samples currently we have tpm values and raw gene counts from rna seq. Also we have only one sample for each time point. we would like to see for changes in genes across different time point but not sure statistical method should we use to determine this, should we consider for example a single time point wt vs mt, or see trends across wt and mt individually &#xD;&#xA;I know that there are some bioc packages for NGS analysis (edgeR,&#xD;&#xA;DESeq,etc). Is any package or method good for one-sample-per-time-point&#xD;&#xA;data?" />
  <row Id="4764" PostHistoryTypeId="6" PostId="2199" RevisionGUID="6223c577-95fe-48fc-9dcb-01ef8f19db41" CreationDate="2017-08-02T08:56:11.040" UserId="57" Comment="Additional tags; deleted courtesy" Text="&lt;rna-seq&gt;&lt;software-recommendation&gt;&lt;statistics&gt;" />
  <row Id="4765" PostHistoryTypeId="10" PostId="2199" RevisionGUID="8ec6b38d-233f-46bf-b4eb-9d9fb6817025" CreationDate="2017-08-02T11:48:59.337" UserId="-1" Comment="101" Text="{&quot;OriginalQuestionIds&quot;:[2140],&quot;Voters&quot;:[{&quot;Id&quot;:235,&quot;DisplayName&quot;:&quot;Ian Sudbery&quot;},{&quot;Id&quot;:57,&quot;DisplayName&quot;:&quot;Kamil S Jaron&quot;},{&quot;Id&quot;:-1,&quot;DisplayName&quot;:&quot;Community&quot;,&quot;BindingReason&quot;:{&quot;DuplicateApprovedByAsker&quot;:&quot;&quot;}}]}" />
  <row Id="4767" PostHistoryTypeId="5" PostId="960" RevisionGUID="2a5a1edf-aa43-4b8e-960f-4c0be5a06a29" CreationDate="2017-08-02T15:43:07.487" UserId="240" Comment="Expanding PSSM initialization since I've never seen it before." Text="I have some protein sequences and I want to build a position-specific scoring matrix (PSSM) for them and then upload this PSSM to [NCBI PSI-BLAST](https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE=Proteins&amp;PROGRAM=blastp&amp;RUN_PSIBLAST=on). I used [CHAPS](http://fasta.bioch.virginia.edu/fasta_www2/chaps.cgi) program for this pupose but uploading the output PSSM gave me an error in NCBI PSI-BLAST. Do you know any tool or webserver for getting PSSMs for a group of protein sequences which then can work in NCBI PSI-BLAST?&#xD;&#xA;This is a part of my PSSM file: &#xD;&#xA;&#xD;&#xA;    PssmWithParameters ::= {&#xD;&#xA;    pssm {&#xD;&#xA;        isProtein TRUE ,&#xD;&#xA;        numRows 28 ,&#xD;&#xA;        numColumns 131 ,&#xD;&#xA;        byRow FALSE ,&#xD;&#xA;        query&#xD;&#xA;        seq {&#xD;&#xA;            id {&#xD;&#xA;                other {&#xD;&#xA;                    accession &quot;WP_000208753&quot; } } ,&#xD;&#xA;            inst {&#xD;&#xA;                repr raw ,&#xD;&#xA;                mol aa ,&#xD;&#xA;                length 131 ,&#xD;&#xA;                seq-data&#xD;&#xA;                ncbieaa &quot;MTTKRKPYVRPMTSTWWKKLPFYRFYMLREGTAVPAVWFSIELIFGLFALKNGPEAW&#xD;&#xA;    AGFIDFLQNPVIVIINLITLAAALLHTKTWFELAPKAANIIVKDEKMGPEPIIKSLWAVTVVATIVILFVALYW&quot; } } ,&#xD;&#xA;        intermediateData {&#xD;&#xA;            freqRatios {&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 564418841, 10, -10 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 11768571, 10, -9 } ,&#xD;&#xA;                { 185838265, 10, -10 } ,&#xD;&#xA;                { 31496547, 10, -9 } ,&#xD;&#xA;                { 3872857, 10, -8 } ,&#xD;&#xA;                { 291750464, 10, -10 } ,&#xD;&#xA;                { 128450763, 10, -10 } ,&#xD;&#xA;                { 759856221, 10, -10 } ,&#xD;&#xA;                { 359173937, 10, -10 } ,&#xD;&#xA;                { 179865517, 10, -9 } ,&#xD;&#xA;                { 14537895, 10, -8 } ,&#xD;&#xA;                { 212921456, 10, -10 } ,&#xD;&#xA;                { 220554141, 10, -10 } ,&#xD;&#xA;                { 368516324, 10, -10 } ,&#xD;&#xA;                { 319343525, 10, -10 } ,&#xD;&#xA;                { 426173953, 10, -10 } ,&#xD;&#xA;                { 463659523, 10, -10 } ,&#xD;&#xA;                { 817322186, 10, -10 } ,&#xD;&#xA;                { 811693964, 10, -11 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 227810064, 10, -10 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 768325726, 10, -10 } ,&#xD;&#xA;                { 0, 10, 0 } ,&#xD;&#xA;                { 1425562, 10, -8 } ,&#xD;&#xA;                { 372685285, 10, -10 } ,&#xD;&#xA;                { 466727421, 10, -10 } ,&#xD;&#xA;                { 185741084, 10, -10 } ,&#xD;&#xA;                { 427328646, 10, -10 } ,&#xD;&#xA;                { 122594965, 10, -10 } , &#xD;&#xA;        " />
  <row Id="4768" PostHistoryTypeId="24" PostId="960" RevisionGUID="2a5a1edf-aa43-4b8e-960f-4c0be5a06a29" CreationDate="2017-08-02T15:43:07.487" Comment="Proposed by 240 approved by 96, 57 edit id of 252" />
  <row Id="4769" PostHistoryTypeId="5" PostId="2136" RevisionGUID="5f929ff3-c4de-440e-b38d-b46c48703982" CreationDate="2017-08-02T17:37:40.247" UserId="1087" Comment="deleted 7 characters in body" Text="I'm not sure what kinds of bioinformatics tasks you would like to perform, therefore it is difficult to give a good recommendation.&#xD;&#xA;&#xD;&#xA;If you're specifically working on statistical genetics, I can recommend [Hail](https://hail.is) [1]. Hail is an open-source tool for ]]] analyzing genetics data at the tens of terabyte scale. Most of Hail's users do their science in Jupyter notebooks that are backed by Google Cloud Platform Dataproc clusters. Hail permits you to perform a variety of statistical genetics tasks including:&#xD;&#xA;&#xD;&#xA; - filtering and aggregation for quality control&#xD;&#xA; - subsetting, linear regression, linear mixed model regression, and linear burden testing&#xD;&#xA; - utilities for computing various measures of relatedness&#xD;&#xA; - principal components analysis&#xD;&#xA; - variant splitting&#xD;&#xA; - import/export from a variety of formats including PLINK, VCF, and BGEN, and&#xD;&#xA; - a python API which enables the use of libraries like matplotlib for plotting analysis results&#xD;&#xA;&#xD;&#xA;To learn specifically about using Hail with the Google Cloud Platform and Jupyter notebooks, I strongly recommend [Liam's Hail forum post about his cloud-tools repository](http://discuss.hail.is/t/using-hail-with-jupyter-notebooks-on-google-cloud/196/2).&#xD;&#xA;&#xD;&#xA;Here's an example, from the [Hail tutorial](https://hail.is/hail/tutorials/hail-overview.html#Quality-Control), of using Hail to perform some quality control and display a scatter plot of the first two principal components of the individuals:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    from hail import *&#xD;&#xA;    import matplotlib.pyplot as plt&#xD;&#xA;    import matplotlib.patches as mpatches&#xD;&#xA;    &#xD;&#xA;    hc = HailContext()&#xD;&#xA;    &#xD;&#xA;    table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample')&#xD;&#xA;    common_vds = (hc.read('data/1kg.vds')&#xD;&#xA;                  .annotate_samples_table(table, root='sa')&#xD;&#xA;                  .sample_qc()&#xD;&#xA;                  .filter_samples_expr('sa.qc.dpMean &gt;= 4 &amp;&amp; sa.qc.callRate &gt;= 0.97')&#xD;&#xA;                  .filter_genotypes('''let ab = g.ad[1] / g.ad.sum() in&#xD;&#xA;                             ((g.isHomRef &amp;&amp; ab &lt;= 0.1) ||&#xD;&#xA;                              (g.isHet &amp;&amp; ab &gt;= 0.25 &amp;&amp; ab &lt;= 0.75) ||&#xD;&#xA;                              (g.isHomVar &amp;&amp; ab &gt;= 0.9))''')&#xD;&#xA;                  .variant_qc()&#xD;&#xA;                  .filter_variants_expr('va.qc.AF &gt; 0.01')&#xD;&#xA;                  .ld_prune(memory_per_core=512, num_cores=4))&#xD;&#xA;    &#xD;&#xA;    pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen')&#xD;&#xA;    pca_table = pca.samples_table().to_pandas()&#xD;&#xA;    &#xD;&#xA;    colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}&#xD;&#xA;    plt.scatter(pca_table[&quot;sa.pca.PC1&quot;], pca_table[&quot;sa.pca.PC2&quot;],&#xD;&#xA;                c = pca_table[&quot;sa.SuperPopulation&quot;].map(colors),&#xD;&#xA;                alpha = .5)&#xD;&#xA;    plt.xlim(-0.6, 0.6)&#xD;&#xA;    plt.xlabel(&quot;PC1&quot;)&#xD;&#xA;    plt.ylabel(&quot;PC2&quot;)&#xD;&#xA;    legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]&#xD;&#xA;    plt.legend(handles=legend_entries, loc=2)&#xD;&#xA;    plt.show()&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[1] Disclaimer: I work on Hail" />
  <row Id="4770" PostHistoryTypeId="36" PostId="2200" RevisionGUID="d26dd79f-7fa6-4e08-bde4-366a1bf0d14e" CreationDate="2017-08-02T17:52:57.173" UserId="-1" Comment="from https://biology.stackexchange.com/questions/64326/biopython-resseq-doesnt-match-pdb-file" />
  <row Id="4771" PostHistoryTypeId="2" PostId="2200" RevisionGUID="9a7f972f-06cf-4022-99f2-28529f0ca08b" CreationDate="2017-08-02T17:24:24.340" UserId="1249" Text="I have a PDB file, and I need to extract its residue sequence numbers (resSeq's). Based on manual inspection of the first few lines of the PDB file (pasted below), I would think that resSeq's should begin with 22, 23. However, Biopython's PDB module suggests otherwise (output attached below as well). I wonder if it's a Biopython bug or if I have problems understanding the PDB format. &#xD;&#xA;&#xD;&#xA;ATOM      1  N   GLY A  22      78.171  89.858  59.231  1.00 21.24           N  &#xD;&#xA;ATOM      2  CA  GLY A  22      79.174  88.827  58.999  1.00 20.87           C  &#xD;&#xA;ATOM      3  C   GLY A  22      80.438  89.415  58.391  1.00 21.89           C  &#xD;&#xA;ATOM      4  O   GLY A  22      80.362  90.202  57.440  1.00 23.18           O  &#xD;&#xA;ATOM      5  N   LEU A  23      81.588  89.069  58.972  1.00 21.51           N  &#xD;&#xA;ATOM      6  CA  LEU A  23      82.895  89.555  58.527  1.00 20.80           C  &#xD;&#xA;ATOM      7  C   LEU A  23      83.288  89.020  57.162  1.00 22.41           C  &#xD;&#xA;ATOM      8  O   LEU A  23      82.889  87.923  56.788  1.00 22.93           O  &#xD;&#xA;ATOM      9  CB  LEU A  23      83.973  89.232  59.560  1.00 20.97           C  &#xD;&#xA;ATOM     10  CG  LEU A  23      84.225  87.818  60.062  1.00 13.32           C  &#xD;&#xA;ATOM     11  CD1 LEU A  23      85.448  87.888  60.939  1.00 15.24           C  &#xD;&#xA;ATOM     12  CD2 LEU A  23      83.035  87.258  60.829  1.00 12.21           C  &#xD;&#xA;&#xD;&#xA;    ...&#xD;&#xA;    for i in chain:&#xD;&#xA;        print i.get_full_id()&#xD;&#xA;&#xD;&#xA;    OUT:('pdb', 0, 'A', (' ', 2, ' '))&#xD;&#xA;        ('pdb', 0, 'A', (' ', 3, ' '))&#xD;&#xA;    ...&#xD;&#xA;" />
  <row Id="4772" PostHistoryTypeId="1" PostId="2200" RevisionGUID="9a7f972f-06cf-4022-99f2-28529f0ca08b" CreationDate="2017-08-02T17:24:24.340" UserId="1249" Text="Biopython: resseq doesn't match pdb file" />
  <row Id="4773" PostHistoryTypeId="3" PostId="2200" RevisionGUID="9a7f972f-06cf-4022-99f2-28529f0ca08b" CreationDate="2017-08-02T17:24:24.340" UserId="1249" Text="&lt;pdb&gt;&lt;biopython&gt;" />
  <row Id="4774" PostHistoryTypeId="5" PostId="186" RevisionGUID="3ae7558c-b298-447d-aaf1-8f7750c212eb" CreationDate="2017-08-02T18:25:20.317" UserId="1140" Comment="added 179 characters in body" Text="VCF, acronym for Variant Calling Format, is a text file type used to store information regarding genetic variants. Use this tag for questions related to the use of this file type." />
  <row Id="4775" PostHistoryTypeId="24" PostId="186" RevisionGUID="3ae7558c-b298-447d-aaf1-8f7750c212eb" CreationDate="2017-08-02T18:25:20.317" Comment="Proposed by 1140 approved by 77 edit id of 253" />
  <row Id="4776" PostHistoryTypeId="5" PostId="187" RevisionGUID="94959331-7a7d-4c48-b328-f1a6299b9ff4" CreationDate="2017-08-02T18:25:24.307" UserId="1140" Comment="added 114 characters in body" Text="VCF, acronym for Variant Calling Format, is a text file type used to store information regarding genetic variants." />
  <row Id="4777" PostHistoryTypeId="24" PostId="187" RevisionGUID="94959331-7a7d-4c48-b328-f1a6299b9ff4" CreationDate="2017-08-02T18:25:24.307" Comment="Proposed by 1140 approved by 77 edit id of 254" />
  <row Id="4778" PostHistoryTypeId="2" PostId="2201" RevisionGUID="bfa30781-655e-4feb-b0fc-abdd29724939" CreationDate="2017-08-02T20:14:06.153" UserId="1250" Text="This is a cross-post from Mathematics forum, but as know one had answered yet, I fIgured post here as well.&#xD;&#xA;&#xD;&#xA;I would like to describe a transform I used to rank my data points. I have recoded my variable columns from $10$ measurements with different range to $n$ (for each measurement) where $n \in \{0,0.5,1\}$.    &#xD;&#xA;&#xD;&#xA;For columns where measurements were expressed as categorical values I simply recoded the categorical value to either $0$, $0.5$ or $1$ and for continuous variables I recoded the lower quartile as $0$, interquartile as $0.5$ and upper quartile as $1$ and finally I summed the recoded values to produce a single score $\sum n_{i1,..i10} $ for each row. &#xD;&#xA;&#xD;&#xA;I am trying to write my method and I'd like to have it in mathematical notation. I was wondering if anyone could help me with this!  " />
  <row Id="4779" PostHistoryTypeId="1" PostId="2201" RevisionGUID="bfa30781-655e-4feb-b0fc-abdd29724939" CreationDate="2017-08-02T20:14:06.153" UserId="1250" Text="Mathematical notion for formulating a rank score" />
  <row Id="4780" PostHistoryTypeId="3" PostId="2201" RevisionGUID="bfa30781-655e-4feb-b0fc-abdd29724939" CreationDate="2017-08-02T20:14:06.153" UserId="1250" Text="&lt;modelling&gt;" />
  <row Id="4781" PostHistoryTypeId="5" PostId="2201" RevisionGUID="238ed6fd-8004-4075-b8b1-c76fc23b1f02" CreationDate="2017-08-02T20:21:24.400" UserId="1250" Comment="deleted 3 characters in body" Text="This is a cross-post from Mathematics forum. As no one has answered it yet, I fIgured post here as well.&#xD;&#xA;&#xD;&#xA;I would like to describe a transform I used to rank my data points. I have recoded my variable columns from $10$ measurements with different range to $n$ (for each measurement) where $n \in \{0,0.5,1\}$.    &#xD;&#xA;&#xD;&#xA;For columns where measurements were expressed as categorical values I simply recoded the categorical value to either $0$, $0.5$ or $1$ and for continuous variables I recoded the lower quartile as $0$, interquartile as $0.5$ and upper quartile as $1$ and finally I summed the recoded values to produce a single score $\sum n_{i1,..i10} $ for each row. &#xD;&#xA;&#xD;&#xA;I am trying to write my method and I'd like to have it in mathematical notation. I was wondering if anyone could help me with this!  " />
  <row Id="4782" PostHistoryTypeId="2" PostId="2202" RevisionGUID="4daa3e6d-1612-4899-86a4-11d8bf0d8b6c" CreationDate="2017-08-02T21:03:29.337" UserId="643" Text="I am interested in identifying indels in whole genome bisulfite sequencing data (76bp paired end). Currently, I do this by setting the -rfg and -rdg affine gap penalty scores for bowtie2 to more permissive values than the default 5+3N and mapping using the bisulfite sequencing alignment wrapper, bismark.&#xD;&#xA;&#xD;&#xA;My question is, what values of -rfg and -rdg will allow me to identify longest possible indels without sacrificing alignment quality? Is it better to set the affine penalty to zero with a high penalty for initially opening an indel (ex. 8+0N)? Or is it better to keep the initial penalty low and having a nonzero penalty for extension (ex. 2+1N)?" />
  <row Id="4783" PostHistoryTypeId="1" PostId="2202" RevisionGUID="4daa3e6d-1612-4899-86a4-11d8bf0d8b6c" CreationDate="2017-08-02T21:03:29.337" UserId="643" Text="Best way to detect long insertions in bisulfite sequencing data?" />
  <row Id="4784" PostHistoryTypeId="3" PostId="2202" RevisionGUID="4daa3e6d-1612-4899-86a4-11d8bf0d8b6c" CreationDate="2017-08-02T21:03:29.337" UserId="643" Text="&lt;ngs&gt;&lt;alignment&gt;&lt;read-mapping&gt;&lt;methylation&gt;&lt;indel&gt;" />
  <row Id="4785" PostHistoryTypeId="5" PostId="659" RevisionGUID="10aa6ea7-a0f2-4add-bd06-41376850aded" CreationDate="2017-08-03T06:09:39.177" UserId="240" Comment="Escaping $ character, since it caused accidental MathJax formatting" Text="I have trialed the free version of InsideDNA, and these were my notes:&#xD;&#xA;&#xD;&#xA; - Cost: \$225/month for a team of 5 with 50TB storage or \$45/month with 10TB storage for individuals (assuming 6 month package: https://insidedna.me/pricing).&#xD;&#xA; - Software installed: Around 600 bioinformatic tools available and standard command line tools; some popular tools missing (like [CD-HIT][1]), but should be possible to install on request.&#xD;&#xA; - Jobs: Maximum of 32 CPUs and 208 RAM per job submission. Test jobs generally worked, although a larger test job failed.&#xD;&#xA; - Other points: Command line was sometimes slow, `wget` queries were slow, and `scp` was blocked. However, these may be resolvable issues.&#xD;&#xA;&#xD;&#xA;Overall, I felt InsideDNA could be useful for groups without their own computational infrastructure and could be used for easily sharing resources between groups. The packages on offer seem not expensive, but I had a few issues, and I don't know how good their sys admin support would be.&#xD;&#xA;&#xD;&#xA;I have not used the Amazon service, so can't comment beyond the details on their website. Also there are a few alternative companies, such as [Genestack][2] and [DNAnexus][3], but I haven't directly tested them either.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://weizhongli-lab.org/cd-hit/&#xD;&#xA;  [2]: https://genestack.com/&#xD;&#xA;  [3]: https://www.dnanexus.com/" />
  <row Id="4786" PostHistoryTypeId="24" PostId="659" RevisionGUID="10aa6ea7-a0f2-4add-bd06-41376850aded" CreationDate="2017-08-03T06:09:39.177" Comment="Proposed by 240 approved by 57, 104 edit id of 255" />
  <row Id="4787" PostHistoryTypeId="5" PostId="2200" RevisionGUID="af5db6f2-4f9b-4f5f-9ce7-a2988d9770d8" CreationDate="2017-08-03T08:28:43.720" UserId="298" Comment="added 62 characters in body" Text="I have a PDB file, and I need to extract its residue sequence numbers (resSeq's). Based on manual inspection of the first few lines of the PDB file (pasted below), I would think that resSeq's should begin with 22, 23. However, Biopython's PDB module suggests otherwise (output attached below as well). I wonder if it's a Biopython bug or if I have problems understanding the PDB format. &#xD;&#xA;&#xD;&#xA;    ATOM      1  N   GLY A  22      78.171  89.858  59.231  1.00 21.24           N  &#xD;&#xA;    ATOM      2  CA  GLY A  22      79.174  88.827  58.999  1.00 20.87           C  &#xD;&#xA;    ATOM      3  C   GLY A  22      80.438  89.415  58.391  1.00 21.89           C  &#xD;&#xA;    ATOM      4  O   GLY A  22      80.362  90.202  57.440  1.00 23.18           O  &#xD;&#xA;    ATOM      5  N   LEU A  23      81.588  89.069  58.972  1.00 21.51           N  &#xD;&#xA;    ATOM      6  CA  LEU A  23      82.895  89.555  58.527  1.00 20.80           C  &#xD;&#xA;    ATOM      7  C   LEU A  23      83.288  89.020  57.162  1.00 22.41           C  &#xD;&#xA;    ATOM      8  O   LEU A  23      82.889  87.923  56.788  1.00 22.93           O  &#xD;&#xA;    ATOM      9  CB  LEU A  23      83.973  89.232  59.560  1.00 20.97           C  &#xD;&#xA;    ATOM     10  CG  LEU A  23      84.225  87.818  60.062  1.00 13.32           C  &#xD;&#xA;    ATOM     11  CD1 LEU A  23      85.448  87.888  60.939  1.00 15.24           C  &#xD;&#xA;    ATOM     12  CD2 LEU A  23      83.035  87.258  60.829  1.00 12.21           C  &#xD;&#xA;    &#xD;&#xA;&amp;nbsp;&#xD;&#xA;&#xD;&#xA;    ...&#xD;&#xA;    for i in chain:&#xD;&#xA;        print i.get_full_id()&#xD;&#xA;&#xD;&#xA;    OUT:('pdb', 0, 'A', (' ', 2, ' '))&#xD;&#xA;        ('pdb', 0, 'A', (' ', 3, ' '))&#xD;&#xA;    ...&#xD;&#xA;" />
  <row Id="4788" PostHistoryTypeId="2" PostId="2203" RevisionGUID="5a334a23-d18f-401a-80c3-9f08fc736e75" CreationDate="2017-08-03T08:54:48.673" UserId="446" Text="I want to incorporate in our Shiny app. the possibility to select different nucleotide sequences and do a pairwise sequence alignment in real-time. The user would define as input different nt. sequences and then a tree will pop up in the app. showing the similarity between them. Do you think it is feasible? Any recommendations and feedback are really appreciated :)" />
  <row Id="4789" PostHistoryTypeId="1" PostId="2203" RevisionGUID="5a334a23-d18f-401a-80c3-9f08fc736e75" CreationDate="2017-08-03T08:54:48.673" UserId="446" Text="Pairwise alignment with R" />
  <row Id="4790" PostHistoryTypeId="3" PostId="2203" RevisionGUID="5a334a23-d18f-401a-80c3-9f08fc736e75" CreationDate="2017-08-03T08:54:48.673" UserId="446" Text="&lt;r&gt;&lt;sequence-alignment&gt;" />
  <row Id="4791" PostHistoryTypeId="2" PostId="2204" RevisionGUID="6006e6d7-4290-4251-b7c3-d05185452cca" CreationDate="2017-08-03T09:29:03.723" UserId="1258" Text="Is the `MUMmer` suite capable of calculating reference sequence coverage statistics for all query sequences collectively? It would be possible to achieve by parsing the output of `nucmer` / `show-coords` / `show-tiling` but it seems like there should be a better way. I currently do this using a sensitive read mapper, `samtools depth` and some scripting.&#xD;&#xA;&#xD;&#xA;To clarify, I'd like to know the reference coverage achieved using all of the query sequences (i.e. the whole *de novo* assembly, in my case)." />
  <row Id="4792" PostHistoryTypeId="1" PostId="2204" RevisionGUID="6006e6d7-4290-4251-b7c3-d05185452cca" CreationDate="2017-08-03T09:29:03.723" UserId="1258" Text="How to calculate overall reference coverage with MUMmer" />
  <row Id="4793" PostHistoryTypeId="3" PostId="2204" RevisionGUID="6006e6d7-4290-4251-b7c3-d05185452cca" CreationDate="2017-08-03T09:29:03.723" UserId="1258" Text="&lt;assembly&gt;&lt;coverage&gt;" />
  <row Id="4794" PostHistoryTypeId="5" PostId="2203" RevisionGUID="1d6a737e-85aa-4466-a28a-8980bba28568" CreationDate="2017-08-03T09:29:59.613" UserId="29" Comment="deleted 1 character in body; edited title" Text="I want to incorporate in our Shiny app the possibility to select different nucleotide sequences and do a multiple sequence alignment in real-time. The user would define as input different nt. sequences and then a tree will pop up in the app. showing the similarity between them. Do you think it is feasible? Any recommendations and feedback are really appreciated :)" />
  <row Id="4795" PostHistoryTypeId="4" PostId="2203" RevisionGUID="1d6a737e-85aa-4466-a28a-8980bba28568" CreationDate="2017-08-03T09:29:59.613" UserId="29" Comment="deleted 1 character in body; edited title" Text="Multiple sequence alignment with R" />
  <row Id="4796" PostHistoryTypeId="5" PostId="2203" RevisionGUID="91ae775e-ee7c-4749-a2b1-dc8fa270e74c" CreationDate="2017-08-03T09:30:25.013" UserId="446" Comment="deleted 36 characters in body; edited title" Text="I want to incorporate in our Shiny app. the possibility to select different nucleotide sequences and do a multiple sequence alignment in real-time. The user would define as input different nt. sequences and then a tree will pop up in the app.. Do you think it is feasible? Any recommendations and feedback are really appreciated :)" />
  <row Id="4797" PostHistoryTypeId="4" PostId="2203" RevisionGUID="91ae775e-ee7c-4749-a2b1-dc8fa270e74c" CreationDate="2017-08-03T09:30:25.013" UserId="446" Comment="deleted 36 characters in body; edited title" Text="Multiple sequence alignment with R" />
  <row Id="4798" PostHistoryTypeId="5" PostId="2201" RevisionGUID="ea01f782-6de9-40fb-9cae-6a72834e0eda" CreationDate="2017-08-03T09:31:24.623" UserId="1250" Comment="added 58 characters in body" Text="This is a cross-post from [Mathematics forum][1]. As no one has answered it yet, I fIgured post here as well.&#xD;&#xA;&#xD;&#xA;I would like to describe a transform I used to rank my data points. I have recoded my variable columns from $10$ measurements with different range to $n$ (for each measurement) where $n \in \{0,0.5,1\}$.    &#xD;&#xA;&#xD;&#xA;For columns where measurements were expressed as categorical values I simply recoded the categorical value to either $0$, $0.5$ or $1$ and for continuous variables I recoded the lower quartile as $0$, interquartile as $0.5$ and upper quartile as $1$ and finally I summed the recoded values to produce a single score $\sum n_{i1,..i10} $ for each row. &#xD;&#xA;&#xD;&#xA;I am trying to write my method and I'd like to have it in mathematical notation. I was wondering if anyone could help me with this!  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://math.stackexchange.com/q/2380215" />
  <row Id="4801" PostHistoryTypeId="2" PostId="2206" RevisionGUID="31fe2f1f-0751-4a42-b143-8cadd4252d0d" CreationDate="2017-08-03T09:31:47.057" UserId="29" Text="The [‹msa› package on Bioconductor](https://bioconductor.org/packages/release/bioc/html/msa.html) does exactly that.&#xD;&#xA;&#xD;&#xA;It doesn’t hand the results on a silver platter, though: you’ll need to [read the vignette](https://bioconductor.org/packages/devel/bioc/vignettes/msa/inst/doc/msa.pdf) carefully to learn how to use it. But after that it’s pretty powerful." />
  <row Id="4802" PostHistoryTypeId="4" PostId="2204" RevisionGUID="0b812851-e759-42ec-8827-50937047386f" CreationDate="2017-08-03T11:18:30.613" UserId="298" Comment="How to is a declaration, not a question. It would be a good title for an article explaining how to do this, but not for a question asking how. " Text="How can I calculate overall reference coverage with MUMmer?" />
  <row Id="4803" PostHistoryTypeId="4" PostId="2204" RevisionGUID="110d0b5f-709b-4e31-8edd-d3bb1ab8acbb" CreationDate="2017-08-03T12:23:46.210" UserId="1258" Comment="edited title" Text="How to calculate overall reference coverage with MUMmer?" />
  <row Id="4804" PostHistoryTypeId="2" PostId="2207" RevisionGUID="eb109d19-f4ae-458d-8991-0b0012a6772d" CreationDate="2017-08-03T13:45:40.647" UserId="266" Text="Let's check it. I copied your file fragment into `a.pdb`.&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; from Bio import PDB&#xD;&#xA;    &gt;&gt;&gt; for res in PDB.PDBParser().get_structure('pdb', 'a.pdb')[0].get_residues():&#xD;&#xA;    ...   print(res.get_full_id())&#xD;&#xA;    ... &#xD;&#xA;    ('pdb', 0, 'A', (' ', 22, ' '))&#xD;&#xA;    ('pdb', 0, 'A', (' ', 23, ' '))&#xD;&#xA;&#xD;&#xA;So BioPython (ver. 1.66) gives 22 and 23 as expected.&#xD;&#xA;" />
  <row Id="4805" PostHistoryTypeId="2" PostId="2208" RevisionGUID="9c5370d6-d028-4823-a538-4b34742a905d" CreationDate="2017-08-03T14:10:09.633" UserId="-1" Text="" />
  <row Id="4806" PostHistoryTypeId="1" PostId="2208" RevisionGUID="9c5370d6-d028-4823-a538-4b34742a905d" CreationDate="2017-08-03T14:10:09.633" UserId="-1" />
  <row Id="4807" PostHistoryTypeId="2" PostId="2209" RevisionGUID="f9e8d5b1-74c7-4bb3-b593-5fe0c413455e" CreationDate="2017-08-03T14:10:09.633" UserId="-1" Text="" />
  <row Id="4808" PostHistoryTypeId="1" PostId="2209" RevisionGUID="f9e8d5b1-74c7-4bb3-b593-5fe0c413455e" CreationDate="2017-08-03T14:10:09.633" UserId="-1" />
  <row Id="4809" PostHistoryTypeId="2" PostId="2210" RevisionGUID="fa692e64-4105-41a0-a08c-33e872870d11" CreationDate="2017-08-03T15:23:45.557" UserId="180" Text="DNAnexus -- http://dnanexus.com&#xD;&#xA;&#xD;&#xA;BaseSpace -- http://basespace.illumina.com&#xD;&#xA;&#xD;&#xA;Seven Bridges Genomics -- http://www.sbgenomics.com&#xD;&#xA;&#xD;&#xA;Curoverse http://curoverse.com&#xD;&#xA;&#xD;&#xA;InsideDNA http://insidedna.me/&#xD;&#xA;" />
  <row Id="4810" PostHistoryTypeId="5" PostId="2209" RevisionGUID="3ca0c817-6c6b-4f64-9d2b-f1865dbbd184" CreationDate="2017-08-03T18:11:15.430" UserId="1140" Comment="added 63 characters in body" Text="python is a programming language, widely used in bioinformatics" />
  <row Id="4811" PostHistoryTypeId="24" PostId="2209" RevisionGUID="3ca0c817-6c6b-4f64-9d2b-f1865dbbd184" CreationDate="2017-08-03T18:11:15.430" Comment="Proposed by 1140 approved by 77 edit id of 257" />
  <row Id="4812" PostHistoryTypeId="5" PostId="2208" RevisionGUID="076005f0-e802-4075-8201-02aee26c7339" CreationDate="2017-08-03T18:11:21.103" UserId="1140" Comment="added 146 characters in body" Text="python (https://www.python.org/) is a programming language, widely used in bioinformatics. Use this tag for questions about programming in python." />
  <row Id="4813" PostHistoryTypeId="24" PostId="2208" RevisionGUID="076005f0-e802-4075-8201-02aee26c7339" CreationDate="2017-08-03T18:11:21.103" Comment="Proposed by 1140 approved by 77 edit id of 256" />
  <row Id="4814" PostHistoryTypeId="2" PostId="2211" RevisionGUID="3f68a763-c803-4f4a-b866-03e880dd78a4" CreationDate="2017-08-03T18:30:18.183" UserId="77" Text="You'd be best off by starting with `-rfg` and `-rdg` as is and reverting bismark's change of `--score-min` back to the default for bowtie2. That alone will allow for much longer indels. If that still doesn't suffice, then I'd play around more with `--score-min` before messing with the gap open/extend penalties. If you do need to play with those, then increase the gap open penalty and decrease the gap extension to 1. Do check that the resulting alignments aren't nonsense though!" />
  <row Id="4815" PostHistoryTypeId="2" PostId="2212" RevisionGUID="0002bb1f-ace0-4279-94cd-0622acaaaf51" CreationDate="2017-08-03T18:55:25.983" UserId="964" Text="I have had good success using the MEGAN software to detect and visualize contamination.  My input data was WGS not RNA-seq But I would check it out.  It can create a very nice weight phylogenetic tree for all the detected organisms.&#xD;&#xA;&#xD;&#xA;https://ab.inf.uni-tuebingen.de/software/megan6&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/ttT6l.png" />
  <row Id="4816" PostHistoryTypeId="2" PostId="2213" RevisionGUID="8bb26d50-0d1b-45e6-8fdd-7a48c4aff77c" CreationDate="2017-08-03T21:38:09.977" UserId="123" Text="You might want to look into [minihashes][1] as a more efficient way of clustering and comparing your sequences to known species and samples. [In the mash paper][2] the authors show how it would be feasible to create minihash sketches that you could search against from large metagenomics datasets (check the &quot;Clustering massive metagenomic datasets&quot; section in the results):&#xD;&#xA;&#xD;&#xA;&gt; For a large-scale test, samples from the Human Microbiome Project [36]&#xD;&#xA;&gt; (HMP) and Metagenomics of the Human Intestinal Tract [37] (MetaHIT)&#xD;&#xA;&gt; were combined to create a ~10 TB 888-sample dataset. Importantly, the&#xD;&#xA;&gt; size of a Mash sketch is independent of the input size, requiring only&#xD;&#xA;&gt; 70 MB to store the combined sketches (s = 10,000, k = 21) for these&#xD;&#xA;&gt; datasets.&#xD;&#xA;&#xD;&#xA;Moreover, the creators of [sourmash][3] (a tool using the same approach) provide [precomputed sketches for the whole of NCBI's RefSeq database][4] (includes bacterial, viral and fungal genomes). Creating a similar sketch for the whole of ENA/SRA (or a subset of it) could require some time but would allow for very fast and accurate searches.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/MinHash&#xD;&#xA;  [2]: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0997-x&#xD;&#xA;  [3]: https://sourmash.readthedocs.io/en/latest/&#xD;&#xA;  [4]: https://sourmash.readthedocs.io/en/latest/databases.html" />
  <row Id="4818" PostHistoryTypeId="5" PostId="2202" RevisionGUID="c09c7b55-945b-47b9-b412-48780af8c66d" CreationDate="2017-08-04T03:39:26.213" UserId="643" Comment="added 8 characters in body" Text="I am interested in identifying indels in whole genome bisulfite sequencing data (76bp paired end). Currently, I do this by setting the `-rfg` and `-rdg` affine gap penalty scores for bowtie2 to more permissive values than the default 5+3N and mapping using the bisulfite sequencing alignment wrapper, bismark.&#xD;&#xA;&#xD;&#xA;My question is, what values of `-rfg` and `-rdg` will allow me to identify longest possible indels without sacrificing alignment quality? Is it better to set the affine penalty to zero with a high penalty for initially opening an indel (ex. 8+0N)? Or is it better to keep the initial penalty low and having a nonzero penalty for extension (ex. 2+1N)?" />
  <row Id="4823" PostHistoryTypeId="5" PostId="2184" RevisionGUID="c95bd25b-7347-47d5-9799-e9f9b1acaf6f" CreationDate="2017-08-04T10:20:55.950" UserId="292" Comment="Added link to info about fastq" Text="That's a [fastq](https://en.wikipedia.org/wiki/FASTQ_format) file, you will want to align it to the genome, call peaks, and then use something like [MEME](http://meme-suite.org/index.html) to determine binding motifs." />
  <row Id="4824" PostHistoryTypeId="24" PostId="2184" RevisionGUID="c95bd25b-7347-47d5-9799-e9f9b1acaf6f" CreationDate="2017-08-04T10:20:55.950" Comment="Proposed by 292 approved by 298 edit id of 259" />
  <row Id="4825" PostHistoryTypeId="2" PostId="2215" RevisionGUID="4f2f84c9-abab-4908-bfa1-41184f6d115d" CreationDate="2017-08-04T13:17:53.547" UserId="678" Text="I have a set of RNA-seq samples from different experiments (Single and Paired End, depending on the experiment). I ran [FASTQC][1] in all the samples and found overrepresented adapter sequences:[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;I removed the adaptors (TruSeq adaptors) using [Cutadapt][3] (in addition, I removed low quality and N bases from the 3' end of the reads). After that, I ran again FASTQC and the output is the following (representative example) :&#xD;&#xA;[![enter image description here][4]][4]&#xD;&#xA;&#xD;&#xA;Does anyone know what is happening? Now I have an overrepresented sequence for which no sequence is provided. What does this mean? &#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/&#xD;&#xA;  [2]: https://i.stack.imgur.com/RlZlZ.png&#xD;&#xA;  [3]: http://cutadapt.readthedocs.io/en/stable/guide.html&#xD;&#xA;  [4]: https://i.stack.imgur.com/tpgVR.png" />
  <row Id="4826" PostHistoryTypeId="1" PostId="2215" RevisionGUID="4f2f84c9-abab-4908-bfa1-41184f6d115d" CreationDate="2017-08-04T13:17:53.547" UserId="678" Text="FASTQC overrepresented sequences after trimming" />
  <row Id="4827" PostHistoryTypeId="3" PostId="2215" RevisionGUID="4f2f84c9-abab-4908-bfa1-41184f6d115d" CreationDate="2017-08-04T13:17:53.547" UserId="678" Text="&lt;ngs&gt;&lt;fastq&gt;&lt;trimming&gt;" />
  <row Id="4828" PostHistoryTypeId="2" PostId="2216" RevisionGUID="6655d049-9cad-4903-9388-036d2b6ae5d3" CreationDate="2017-08-04T15:10:40.400" UserId="180" Text="A bit of a historical question on a number, 30 times coverage, that's become so familiar in the field: why do we sequence the human genome at 30x coverage?&#xD;&#xA;&#xD;&#xA;The question can be split into:&#xD;&#xA;- Who came up with the 30x value and why?&#xD;&#xA;- Does the value need to be updated into today's state-of-the-art techniques?" />
  <row Id="4829" PostHistoryTypeId="1" PostId="2216" RevisionGUID="6655d049-9cad-4903-9388-036d2b6ae5d3" CreationDate="2017-08-04T15:10:40.400" UserId="180" Text="why sequence the human genome at 30x coverage?" />
  <row Id="4830" PostHistoryTypeId="3" PostId="2216" RevisionGUID="6655d049-9cad-4903-9388-036d2b6ae5d3" CreationDate="2017-08-04T15:10:40.400" UserId="180" Text="&lt;genome&gt;&lt;sequencing&gt;&lt;dna&gt;&lt;next-generation&gt;" />
  <row Id="4831" PostHistoryTypeId="2" PostId="2217" RevisionGUID="ef499b2f-4c3a-405a-bdf8-1f33f096ee10" CreationDate="2017-08-04T15:46:51.593" UserId="74" Text="The earliest mention of the 30x paradigm I could find is in the original Illumina whole-genome sequencing paper: [Bentley, 2008](https://www.nature.com/nature/journal/v456/n7218/full/nature07517.html). Specifically, in Figure 5, they show that most SNPs have been found, and that there are few uncovered/uncalled bases by the time you reach 30x: [![30xSequencingDepth][1]][1]&#xD;&#xA;&#xD;&#xA;These days, 30x is still a common standard, but large-scale germline sequencing projects are often pushing down closer to 25x and finding it adequate. Every group doing this seriously has done power calculations based on specifics of their machines and prep (things like error rates and read lengths matter!).&#xD;&#xA;&#xD;&#xA;Cancer genomics is going in the other direction. When you have to contend with purity, ploidy, and subclonal populations, much more coverage than 30x is needed. Our group showed in [this 2015 paper](http://www.cell.com/cell-systems/abstract/S2405-4712(15)00113-1) that even 300x whole-genome coverage of a tumor was likely missing real rare variants in a tumor. &#xD;&#xA;&#xD;&#xA;On the whole, the sequence coverage you need really depends on what questions you're asking, and I'd recommend that anyone designing a sequencing experiment consult with both a sequencing expert and a statistician beforehand (and it's even better if those are the same person!)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/42eHQ.png" />
  <row Id="4833" PostHistoryTypeId="2" PostId="2218" RevisionGUID="7e71d9b0-ac83-49cb-aa3b-8ab454833c3f" CreationDate="2017-08-04T17:36:59.603" UserId="77" Text="As @AaronBerlin mentioned, you didn't remove reads that were completely trimmed. Next time use the `--minimum-length` option and set it to something reasonable, like 20. Alternatively, use &quot;Trim Galore!&quot;, which is a wrapper around cutadapt that has more reasonable defaults." />
  <row Id="4834" PostHistoryTypeId="2" PostId="2219" RevisionGUID="3b33a38b-0af3-45c2-b7bc-f017a6bd6000" CreationDate="2017-08-04T19:08:28.877" UserId="1258" Text="[Kindel][1] can infer consensus from low quality alignments of short reads to viral references, and extending it to work with single molecule reads and larger genomes is on my to-do list, though I imagine this will require some redesign. Presumably you're dealing with a bacterial or fungal genome in this case ? I do also have a basic C++ version, but it's a long way away from being user friendly. Anyway, it may be worth a look – feel free to get in touch with any issues you encounter.&#xD;&#xA;&#xD;&#xA;https://github.com/bede/kindel&#xD;&#xA;  &#xD;&#xA;   &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/bede/kindel" />
  <row Id="4835" PostHistoryTypeId="5" PostId="2219" RevisionGUID="093078ca-383b-41e7-a0f5-93dc6c8faab1" CreationDate="2017-08-04T19:24:34.243" UserId="1258" Comment="Pilon" Text="[Kindel][1] can infer consensus from low quality alignments of short reads to viral references, and extending it to work with single molecule reads and larger genomes is on my to-do list, though I imagine this will require some redesign. Presumably you're dealing with a bacterial or fungal genome in this case ? I do also have a basic C++ version, but it's a long way away from being user friendly. Anyway, it may be worth a look – feel free to get in touch with any issues you encounter.&#xD;&#xA;&#xD;&#xA;https://github.com/bede/kindel&#xD;&#xA;  &#xD;&#xA;&#xD;&#xA;EDIT: I'd use Pilon as mentioned above&#xD;&#xA;   &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/bede/kindel" />
  <row Id="4836" PostHistoryTypeId="5" PostId="2216" RevisionGUID="1f3486fd-3b3b-454a-9030-841cd890c1f7" CreationDate="2017-08-04T19:42:44.677" UserId="106" Comment="fixed list formatting" Text="A bit of a historical question on a number, 30 times coverage, that's become so familiar in the field: why do we sequence the human genome at 30x coverage?&#xD;&#xA;&#xD;&#xA;The question can be split into:&#xD;&#xA;&#xD;&#xA;- Who came up with the 30x value and why?&#xD;&#xA;- Does the value need to be updated into today's state-of-the-art techniques?" />
  <row Id="4837" PostHistoryTypeId="4" PostId="2216" RevisionGUID="1f3486fd-3b3b-454a-9030-841cd890c1f7" CreationDate="2017-08-04T19:42:44.677" UserId="106" Comment="fixed list formatting" Text="Why sequencing the human genome at 30x coverage?" />
  <row Id="4838" PostHistoryTypeId="24" PostId="2216" RevisionGUID="1f3486fd-3b3b-454a-9030-841cd890c1f7" CreationDate="2017-08-04T19:42:44.677" Comment="Proposed by 106 approved by -1 edit id of 261" />
  <row Id="4839" PostHistoryTypeId="5" PostId="2216" RevisionGUID="5ad22893-bc76-4eb3-a24b-b26462e13278" CreationDate="2017-08-04T19:42:44.677" UserId="96" Comment="fixed list formatting" Text="A bit of a historical question on a number, 30 times coverage, that's become so familiar in the field: why do we sequence the human genome at 30x coverage?&#xD;&#xA;&#xD;&#xA;My question has two parts:&#xD;&#xA;&#xD;&#xA;- Who came up with the 30x value and why?&#xD;&#xA;- Does the value need to be updated to reflect today's state-of-the-art?" />
  <row Id="4840" PostHistoryTypeId="4" PostId="2216" RevisionGUID="5ad22893-bc76-4eb3-a24b-b26462e13278" CreationDate="2017-08-04T19:42:44.677" UserId="96" Comment="fixed list formatting" Text="Why sequence the human genome at 30x coverage?" />
  <row Id="4841" PostHistoryTypeId="2" PostId="2220" RevisionGUID="1a052213-7534-4d6a-8396-1b6a923065bb" CreationDate="2017-08-04T20:51:38.387" UserId="1267" Text="30 times coverage is not unique to this problem, but number **30** has its empirical role in statistics:&#xD;&#xA;&#xD;&#xA;&gt; In statistical analysis, the rule of three states that if a certain event did not occur in a sample with n subjects, the interval from 0 to 3/n is a 95% confidence interval for the rate of occurrences in the population. **When n is greater than 30, this is a good approximation to results from more sensitive tests.**&#xD;&#xA;&#xD;&#xA;source: [Wikipedia: Rule of three (statistics)][1]&#xD;&#xA;&#xD;&#xA;Similarly, you can search for related questions like this one:&#xD;&#xA;&#xD;&#xA;* [What is the rationale behind the magic number 30 in statistics?][2]&#xD;&#xA;&#xD;&#xA;In line with this, I have seen data processing in other disciplines which required **n ≥ 30** for sufficient reliability of results.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Rule_of_three_(statistics)&#xD;&#xA;  [2]: https://www.researchgate.net/post/What_is_the_rationale_behind_the_magic_number_30_in_statistics" />
  <row Id="4849" PostHistoryTypeId="4" PostId="637" RevisionGUID="944df6e4-d590-463f-bc7a-ad929756a5f5" CreationDate="2017-08-05T18:04:19.953" UserId="734" Comment="edited title" Text="How to detect a mutation and predict its consequence?" />
  <row Id="4850" PostHistoryTypeId="2" PostId="2222" RevisionGUID="485b566a-9add-4aad-b09a-9d69efd74fc0" CreationDate="2017-08-06T03:27:17.950" UserId="37" Text="Solexa Inc. sequenced NA12878 chrX to ~30x in early 2007, which later became part of [Bentley (2008)](https://www.ncbi.nlm.nih.gov/pubmed/18987734). This, I believe, was the first time that 30x showed up. I don't recall they had a particular reason for that. Figure 5 in the published paper was just aftermath. It does not really explain why not 25x or 35x, given that the curves between 25x and 35x in that figure are about linear.&#xD;&#xA;&#xD;&#xA;In the abstract of [Ajay et al (2011)](https://www.ncbi.nlm.nih.gov/pubmed/21771779), the authors argued &quot;the current recommendation of ~30x coverage is not adequate&quot;. Nonetheless, the discussion section seems to suggest 50–60x would be necessary with GAIIx, but 35x was adequate with HiSeq2000 plus better recent chemistry. Overall, this paper provides a more thorough analysis. The data quality at that time is also closer to data we produce today.&#xD;&#xA;&#xD;&#xA;The required coverage is largely determined by two factors: read placement bias (e.g. GC bias) and base/mapping error rate. While GC bias has been reduced with the PCR-free protocol, base error rate has been going the downward since HiSeq2500. I guess 30x coverage would be necessary if you want to achieve the sensitivity with the older 30x data. Illumina, as a sequencing service provider, and our sequencing facility still insist on the 30x threshold." />
  <row Id="4851" PostHistoryTypeId="2" PostId="2223" RevisionGUID="04082f98-262b-4536-853d-3e33fd44f0e3" CreationDate="2017-08-06T19:26:16.787" UserId="146" Text="I have some software which takes fastas as the input. I need to include SNVs and InDels from a VCF into the reference hg38 and then use this.&#xD;&#xA;&#xD;&#xA;The problem is, I don't know of an algorithmically sound way to do this.&#xD;&#xA;&#xD;&#xA;(1) Are there any existing software packages which could do this efficiently? Is it easier to output a FASTA, or a bam (and then convert to a FASTA)? &#xD;&#xA;&#xD;&#xA;(2) What about if I wanted to do the same with a bedpe of germline structural variants? " />
  <row Id="4852" PostHistoryTypeId="1" PostId="2223" RevisionGUID="04082f98-262b-4536-853d-3e33fd44f0e3" CreationDate="2017-08-06T19:26:16.787" UserId="146" Text="How to manipulate a reference FASTA or bam to include variants from a VCF?" />
  <row Id="4853" PostHistoryTypeId="3" PostId="2223" RevisionGUID="04082f98-262b-4536-853d-3e33fd44f0e3" CreationDate="2017-08-06T19:26:16.787" UserId="146" Text="&lt;bam&gt;&lt;fasta&gt;&lt;vcf&gt;&lt;variants&gt;&lt;reference-genome&gt;" />
  <row Id="4854" PostHistoryTypeId="2" PostId="2224" RevisionGUID="70041637-e810-45c2-a17c-00b6f595951c" CreationDate="2017-08-06T19:47:22.410" UserId="59" Text="I'm trying to use Annovar to annotate some variants with their CADD and FATHMM scores.  I've downloaded the latest versions of the software and the databases but when I run it I get an error saying the index is out of date.&#xD;&#xA;&#xD;&#xA;    WARNING: Your index file annovar_latest/humandb/hg19_cadd.txt.idx is out of date and will not be used. ANNOVAR can still generate correct results without index file.&#xD;&#xA;&#xD;&#xA;It still runs without the index file but it is extremely slow. Does anyone know how to update the index file?" />
  <row Id="4855" PostHistoryTypeId="1" PostId="2224" RevisionGUID="70041637-e810-45c2-a17c-00b6f595951c" CreationDate="2017-08-06T19:47:22.410" UserId="59" Text="Annovar index out of date" />
  <row Id="4856" PostHistoryTypeId="3" PostId="2224" RevisionGUID="70041637-e810-45c2-a17c-00b6f595951c" CreationDate="2017-08-06T19:47:22.410" UserId="59" Text="&lt;annovar&gt;" />
  <row Id="4857" PostHistoryTypeId="2" PostId="2225" RevisionGUID="3192a28b-63fc-414a-8188-960c3f7ace2e" CreationDate="2017-08-06T20:36:58.513" UserId="77" Text="Your safest bet is to just [redownload the annotation](http://annovar.openbioinformatics.org/en/latest/user-guide/download/#additional-databases). I've not seen official documentation of the index used by annovar, but apparently it's just a text file with a single line header and chromosome-bin coordinates. Over on SEQanswers, [there's a thread discussing this issue](http://seqanswers.com/forums/showthread.php?t=23535) with a single [perl script](http://seqanswers.com/forums/attachment.php?attachmentid=3818&amp;d=1432814490) that allegedly recreates the index.&#xD;&#xA;" />
  <row Id="4858" PostHistoryTypeId="2" PostId="2226" RevisionGUID="9f260943-9a6e-4616-9e8d-7087e8b31830" CreationDate="2017-08-06T20:38:17.907" UserId="59" Text="GATK has a solution that might work for you.&#xD;&#xA;&#xD;&#xA;[FastaAlternateReferenceMaker](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_gatk_tools_walkers_fasta_FastaAlternateReferenceMaker.php)" />
  <row Id="4859" PostHistoryTypeId="2" PostId="2227" RevisionGUID="d5b92576-cd9a-49f9-9b01-0d032611d5a2" CreationDate="2017-08-06T21:23:14.093" UserId="73" Text="There's a `vcf2fq` [sub-program](https://github.com/lh3/samtools/blob/master/bcftools/vcfutils.pl) that was written as part of vcfutils to convert a VCF file into a fastq file given a reference sequence. Unfortunately this doesn't work properly with INDELs (it will just mask them, rather than actually converting them), so I wrote [a modification](https://github.com/gringer/bioinfscripts/blob/master/vcf2fq.pl) to implement INDEL correction as well:&#xD;&#xA;&#xD;&#xA;    ./vcf2fq.pl -f &lt;input.fasta&gt; &lt;all-site.vcf&gt; &gt; &lt;output.fastq&gt;&#xD;&#xA;" />
  <row Id="4860" PostHistoryTypeId="2" PostId="2228" RevisionGUID="2ad0b6d8-64ce-4024-877f-9c8825bc8561" CreationDate="2017-08-06T21:47:56.003" UserId="776" Text="You could [convert VCF to BED][1] via `vcf2bed --snvs` and `vcf2bed --insertions`, and then use `samtools faidx` by way of a [wrapper script][2] to convert BED to FASTA, e.g.:&#xD;&#xA;&#xD;&#xA;    $ vcf2bed --snvs &lt; variants.vcf | bed2faidxsta.pl &gt; snvs.fa&#xD;&#xA;    $ vcf2bed --insertions &lt; variants.vcf | bed2faidxsta.pl &gt; insertions.fa&#xD;&#xA;&#xD;&#xA;You need FASTA files for your reference genome, which have been indexed with `samtools faidx`, e.g., for `hg38`:&#xD;&#xA;&#xD;&#xA;    $ cd /foo/bar/baz&#xD;&#xA;    $ wget ftp://hgdownload.cse.ucsc.edu/goldenPath/hg38/chromosomes/*.fa.gz&#xD;&#xA;    $ for fn in `ls *.fa.gz`; do gunzip $fn; done&#xD;&#xA;    $ for fn in `ls *.fa`; do samtools faidx $fn; done&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/vcf2bed.html&#xD;&#xA;  [2]: https://gist.github.com/alexpreynolds/fa9b0f90e181e3b4f640" />
  <row Id="4861" PostHistoryTypeId="2" PostId="2229" RevisionGUID="38e685ca-7fd7-4bd7-8003-e38321bf46b1" CreationDate="2017-08-06T22:47:16.617" UserId="1280" Text="There are tonnes of them. On top of the excellent ones everybody mentioned&#xD;&#xA;&#xD;&#xA;1. iRods&#xD;&#xA;&#xD;&#xA;2. Arvados&#xD;&#xA;&#xD;&#xA;3. Galaxy" />
  <row Id="4862" PostHistoryTypeId="5" PostId="2228" RevisionGUID="3e390b2f-4195-4785-8390-836f720d8a07" CreationDate="2017-08-07T00:36:30.093" UserId="776" Comment="added 146 characters in body" Text="You could [convert VCF to BED][1] via `vcf2bed --snvs` and `vcf2bed --insertions`, and then use `samtools faidx` by way of a [wrapper script][2] to convert BED to FASTA, e.g.:&#xD;&#xA;&#xD;&#xA;    $ vcf2bed --snvs &lt; variants.vcf | bed2faidxsta.pl &gt; snvs.fa&#xD;&#xA;    $ vcf2bed --insertions &lt; variants.vcf | bed2faidxsta.pl &gt; insertions.fa&#xD;&#xA;&#xD;&#xA;You need FASTA files for your reference genome, which have been indexed with `samtools faidx`, e.g., for `hg38`:&#xD;&#xA;&#xD;&#xA;    $ cd /foo/bar/baz&#xD;&#xA;    $ wget ftp://hgdownload.cse.ucsc.edu/goldenPath/hg38/chromosomes/*.fa.gz&#xD;&#xA;    $ for fn in `ls *.fa.gz`; do gunzip $fn; done&#xD;&#xA;    $ for fn in `ls *.fa`; do samtools faidx $fn; done&#xD;&#xA;&#xD;&#xA;Once you have indexed FASTA files somewhere on your file system, you can pipe BED to the `bed2faidxsta.pl` script, to get out FASTA sequences.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/vcf2bed.html&#xD;&#xA;  [2]: https://gist.github.com/alexpreynolds/fa9b0f90e181e3b4f640" />
  <row Id="4863" PostHistoryTypeId="5" PostId="2216" RevisionGUID="e088b3b7-01c8-42cf-a63a-3384b4479db8" CreationDate="2017-08-07T08:26:14.137" UserId="180" Comment="added 91 characters in body" Text="A bit of a historical question on a number, 30 times coverage, that's become so familiar in the field: why do we sequence the human genome at 30x coverage?&#xD;&#xA;&#xD;&#xA;My question has two parts:&#xD;&#xA;&#xD;&#xA;- Who came up with the 30x value and why?&#xD;&#xA;- Does the value need to be updated to reflect today's state-of-the-art?&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/GGkq0.png" />
  <row Id="4864" PostHistoryTypeId="2" PostId="2230" RevisionGUID="54ea423d-0f79-4ddd-9644-4e6c0f123362" CreationDate="2017-08-07T09:44:43.227" UserId="1281" Text="Given a read ID, I want to edit a single basecall (e.g. the 12th base) for just that read from within a large FASTQ file containing millions of reads.&#xD;&#xA;&#xD;&#xA;example, i want to change the 12th base ('C') in read 31027 to a 'T':&#xD;&#xA;&#xD;&#xA;&lt;pre&gt;@70630 1:N:0:ATCACG&#xD;&#xA;GAAGGTCCATGGATAATACTCAATTTTCCACAACAGCTTTTGTACTCTAGATCATTGATATTTACCAAAAGTCACTCAAACTCATCCTATGCATAATTCTAGTCCACCAATCATGATATGATGGAGAACATGGTTGTAATCAGGAAGACAG&#xD;&#xA;+&#xD;&#xA;DDDDDHIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIHIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIGHHIIIIIIIIIIIIIIIIIIIIIIHIIHIHIIIHHIGIIIIHHHHIIH@&#xD;&#xA;@31027 1:N:0:ATCACG&#xD;&#xA;CAAAAGTCACTCAAACTCATCCTATGCATAATTCTAGTCCACCAATCATGATATGATGGAGAACATGGTTGTAATCAGGAAGACAGATAAAGCAGCAGACCAAAAGTAATCTGAGAAATTATATTTGAATCACTCAGATATACATCAAATA&#xD;&#xA;+&#xD;&#xA;DDBDDHC@GHFHHIIE@CEHHIHEFFCGFCH?HHEGFCCEHHCGH@HFHHHCHHIIIHEHIIHII@CECHIHIIIEECCDGEHFFHHHHEHHFHHGGIHHHDGHFHIIIGHHHHHHEHHIIHIIIGGHHHCHHHCHIIIHHIEH@GHIHIC&#xD;&#xA;@87319 1:N:0:ATCACG&#xD;&#xA;CAATTAAGCTTTGGCAACGGTGGTCAAGATGAGATGCATATGGAGATAATAACTAAAAGTCAATCGAGACTCATCGTATGCATATTTCTAGTCCATCGATCATGAAATGATAGGATAGCTAGAATGAAAAGTAAATTTCCAGAAGGTCCAT&#xD;&#xA;+&#xD;&#xA;DDDDDIIIIIIIIIIIIIHIHHIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIHHIIIIIIIIIIIIIIIIIHIIHIIIIIIIIIIIIHIIIIIIIIIHIHIH&#xD;&#xA;&lt;/pre&gt;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Of course I could stream through the entire FASTQ file rewriting everything to a new file until i get to the read of interest, do the edits to that read, write it out to the new file, and then keep going until the end. BioPython would work fine for this as would other approaches.&#xD;&#xA;&#xD;&#xA;However, is there an efficient way to do this without the full read/write stream? Could this be done with sed/awk ? What about an indexed fastq file?&#xD;&#xA;&#xD;&#xA;cheers&#xD;&#xA;David" />
  <row Id="4865" PostHistoryTypeId="1" PostId="2230" RevisionGUID="54ea423d-0f79-4ddd-9644-4e6c0f123362" CreationDate="2017-08-07T09:44:43.227" UserId="1281" Text="How can I edit a specific FASTQ read in place, given the read ID?" />
  <row Id="4866" PostHistoryTypeId="3" PostId="2230" RevisionGUID="54ea423d-0f79-4ddd-9644-4e6c0f123362" CreationDate="2017-08-07T09:44:43.227" UserId="1281" Text="&lt;ngs&gt;&lt;biopython&gt;&lt;fastq&gt;" />
  <row Id="4867" PostHistoryTypeId="2" PostId="2231" RevisionGUID="ad1f42aa-ffb8-44b1-9bc5-c07ee42dba81" CreationDate="2017-08-07T09:52:10.770" UserId="77" Text="You'll first need to determine the appropriate line number, which you can do with `grep -nw &quot;@31027&quot; foo.fastq`. After that, note that you can provide a line number to `sed`:&#xD;&#xA;&#xD;&#xA;    sed -i '123456s/CAAAAGTCACTCA/CAAAAGTCACTTA/' foo.fastq&#xD;&#xA;&#xD;&#xA;That will do the replacement only on line 123456 and edit the file in place (the `-i` option)." />
  <row Id="4869" PostHistoryTypeId="2" PostId="2232" RevisionGUID="7e671a2b-c10d-4e05-b8c3-c0d17e81c9f6" CreationDate="2017-08-07T11:06:35.623" UserId="-1" Text="" />
  <row Id="4870" PostHistoryTypeId="1" PostId="2232" RevisionGUID="7e671a2b-c10d-4e05-b8c3-c0d17e81c9f6" CreationDate="2017-08-07T11:06:35.623" UserId="-1" />
  <row Id="4871" PostHistoryTypeId="2" PostId="2233" RevisionGUID="dbc02355-0fe0-47ac-9f88-b623596cd887" CreationDate="2017-08-07T11:06:35.623" UserId="-1" Text="" />
  <row Id="4872" PostHistoryTypeId="1" PostId="2233" RevisionGUID="dbc02355-0fe0-47ac-9f88-b623596cd887" CreationDate="2017-08-07T11:06:35.623" UserId="-1" />
  <row Id="4873" PostHistoryTypeId="36" PostId="2234" RevisionGUID="34fc3921-abe4-4703-a568-96a7d8cef5cf" CreationDate="2017-08-07T11:26:19.750" UserId="-1" Comment="from https://biology.stackexchange.com/questions/64510/how-to-adapt-the-r-language-fgseal-function-to-perform-rapidgsea-computation-of" />
  <row Id="4874" PostHistoryTypeId="2" PostId="2234" RevisionGUID="0e266db7-cfe8-4d47-9892-9e3aadcc4d78" CreationDate="2017-08-06T20:33:05.040" UserId="1283" UserDisplayName="Frank" Text=" I wish to adapt the r language function fgseaL, https://github.com/ctlab/fgsea , to perform rapidGSEA, https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1244-x , computation of inter-class  deviation per gene and the subsequent gene rank sorting operation on 9 different phenotype labels as illustrated in the diagram immediately below:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/kXvmr.gif&#xD;&#xA;&#xD;&#xA;I thought of applying the R-language rank() function on the Expression Data Matrix D. If that is not correct, what sequence of R language commands should we&#xD;&#xA;apply to the Expression Data Matrix D to calculate a key value sorted deviation measure  across 8 labeled human leukemia groups and a healthy labeled normal control group prior to running fgseaL? Thank you for your consideration." />
  <row Id="4875" PostHistoryTypeId="1" PostId="2234" RevisionGUID="0e266db7-cfe8-4d47-9892-9e3aadcc4d78" CreationDate="2017-08-06T20:33:05.040" UserId="1283" UserDisplayName="Frank" Text="How to adapt the r language fgseaL function to perform rapidGSEA computation of gene ranks across 9 different phenotype labels?" />
  <row Id="4876" PostHistoryTypeId="3" PostId="2234" RevisionGUID="0e266db7-cfe8-4d47-9892-9e3aadcc4d78" CreationDate="2017-08-06T20:33:05.040" UserId="1283" UserDisplayName="Frank" Text="&lt;genomics&gt;" />
  <row Id="4877" PostHistoryTypeId="5" PostId="2232" RevisionGUID="e8938a80-8e3d-443b-8819-160d6ea8b769" CreationDate="2017-08-07T11:37:41.333" UserId="1140" Comment="added 175 characters in body" Text="FASTQ (https://en.wikipedia.org/wiki/FASTQ_format) is a file format use to store short reads and their quality values. Use this tag for questions related to fastq file format." />
  <row Id="4878" PostHistoryTypeId="24" PostId="2232" RevisionGUID="e8938a80-8e3d-443b-8819-160d6ea8b769" CreationDate="2017-08-07T11:37:41.333" Comment="Proposed by 1140 approved by 77 edit id of 262" />
  <row Id="4879" PostHistoryTypeId="5" PostId="2233" RevisionGUID="57efa441-0146-4972-bc29-9e46e6da02ca" CreationDate="2017-08-07T11:37:44.543" UserId="1140" Comment="added 72 characters in body" Text="FASTQ is a file format use to store short reads and their quality values" />
  <row Id="4880" PostHistoryTypeId="24" PostId="2233" RevisionGUID="57efa441-0146-4972-bc29-9e46e6da02ca" CreationDate="2017-08-07T11:37:44.543" Comment="Proposed by 1140 approved by 77 edit id of 263" />
  <row Id="4881" PostHistoryTypeId="5" PostId="2231" RevisionGUID="a60ae6ee-2df8-43fa-a693-a68c3c74e989" CreationDate="2017-08-07T11:48:56.240" UserId="298" Comment="The -m1 will cause grep to exit as soon as the first match is found. It should make it much faster" Text="You'll first need to determine the appropriate line number, which you can do with `grep -m1 -nw &quot;@31027&quot; foo.fastq`. After that, note that you can provide a line number to `sed`:&#xD;&#xA;&#xD;&#xA;    sed -i '123456s/CAAAAGTCACTCA/CAAAAGTCACTTA/' foo.fastq&#xD;&#xA;&#xD;&#xA;That will do the replacement only on line 123456 and edit the file in place (the `-i` option)." />
  <row Id="4882" PostHistoryTypeId="2" PostId="2235" RevisionGUID="69679a58-dc70-487e-934d-c04dc0612e14" CreationDate="2017-08-07T11:50:46.357" UserId="298" Text="If your fastq file is simple, if each of your reads only has a single line of DNA, you could do:&#xD;&#xA;&#xD;&#xA;    awk '{&#xD;&#xA;            if($1 == &quot;@31027&quot;){&#xD;&#xA;                a=NR+1&#xD;&#xA;            } &#xD;&#xA;            if(NR==a){&#xD;&#xA;                split($0,s,&quot;&quot;); &#xD;&#xA;                s[12]=&quot;T&quot;; &#xD;&#xA;                for(i in s){&#xD;&#xA;                    seq = sprintf(&quot;%s%s&quot;,seq,s[i]);&#xD;&#xA;                } &#xD;&#xA;                print seq&#xD;&#xA;            }&#xD;&#xA;            else{print}&#xD;&#xA;        }' file.fastq " />
  <row Id="4883" PostHistoryTypeId="5" PostId="2235" RevisionGUID="9903cb1b-6f66-4979-b334-27ff7746be11" CreationDate="2017-08-07T11:58:19.077" UserId="298" Comment="deleted 26 characters in body" Text="If your fastq file is simple, if each of your reads only has a single line of DNA, you could do:&#xD;&#xA;&#xD;&#xA;    awk '{&#xD;&#xA;            if($1 == &quot;@31027&quot;){&#xD;&#xA;                a=NR+1&#xD;&#xA;            } &#xD;&#xA;            if(NR==a){&#xD;&#xA;                split($0,s,&quot;&quot;); &#xD;&#xA;                s[12]=&quot;T&quot;; &#xD;&#xA;                for(i in s){&#xD;&#xA;                    seq = sprintf(&quot;%s%s&quot;,seq,s[i]);&#xD;&#xA;                } &#xD;&#xA;                $0=seq&#xD;&#xA;            }&#xD;&#xA;        }1;' file.fastq " />
  <row Id="4884" PostHistoryTypeId="2" PostId="2236" RevisionGUID="2412b636-fade-444c-9561-974f40a022f7" CreationDate="2017-08-07T12:14:39.873" UserId="77" Text="You rank the fit coefficient rather than the original score matrix. So, given a score matrix, `D`:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r --&gt;&#xD;&#xA;&#xD;&#xA;    D = matrix(c(22,20,9,8,46,22,18,10,3,18,3,29,2,1,5,45,43,47,17,5,14,44,21,36), byrow=T, ncol=6)&#xD;&#xA;    cl = c(0,0,0,1,1,1)&#xD;&#xA;    s = apply(m, 1, function(x) coef(lm(x~cl))[2]) # [1]&#xD;&#xA;    o = rank(s)&#xD;&#xA;    o = max(o) - o + 1 # [2]&#xD;&#xA;&#xD;&#xA;`o` is then the rank of each row.&#xD;&#xA;&#xD;&#xA;[1] This fits each row as a linear model of `cl` and extracts the `cl` coefficient.&#xD;&#xA;&#xD;&#xA;[2] This converts the ranking to be the same as shown in the figure. I don't know if this is important, but I would assume so given how GSEA methods tend to work." />
  <row Id="4886" PostHistoryTypeId="4" PostId="2234" RevisionGUID="4bfe1050-18f1-476e-af51-e70cb73b2e4c" CreationDate="2017-08-07T13:10:21.173" UserId="29" Comment="remove tags from title" Text="How to adapt the fgseaL function to perform rapidGSEA computation of gene ranks across 9 different phenotype labels?" />
  <row Id="4887" PostHistoryTypeId="6" PostId="2234" RevisionGUID="4bfe1050-18f1-476e-af51-e70cb73b2e4c" CreationDate="2017-08-07T13:10:21.173" UserId="29" Comment="remove tags from title" Text="&lt;r&gt;&lt;genomics&gt;&lt;gsea&gt;" />
  <row Id="4888" PostHistoryTypeId="5" PostId="2234" RevisionGUID="bb1928fd-1c70-4220-abd5-6b10fa49c8b8" CreationDate="2017-08-07T13:53:03.217" UserId="1283" Comment="added R language fgeaL source code to calculate a correlation matrix which needs explanation. " Text=" I wish to adapt the r language function fgseaL, https://github.com/ctlab/fgsea , to perform rapidGSEA, https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1244-x , computation of inter-class  deviation per gene and the subsequent gene rank sorting operation on 9 different phenotype labels as illustrated in the diagram immediately below:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/kXvmr.gif&#xD;&#xA;&#xD;&#xA;I thought of applying the R-language rank() function on the Expression Data Matrix D. If that is not correct, what sequence of R language commands should we&#xD;&#xA;apply to the Expression Data Matrix D to calculate a key value sorted deviation measure  across 8 labeled human leukemia groups and a healthy labeled normal control group prior to running fgseaL? &#xD;&#xA;&#xD;&#xA;I show below how fgseaL finds the correlation matrix between the R language variable , mat, which corresponds to the Expression Data Matrix D and the R language variable , labels , which is a vector of gene phenotype labels&#xD;&#xA;&#xD;&#xA;        tmatSc &lt;- scale(t(mat))&#xD;&#xA;        labelsSc &lt;- scale(labels)[, 1]&#xD;&#xA;    &#xD;&#xA;        minSize &lt;- max(minSize, 1)&#xD;&#xA;    &#xD;&#xA;        pathwaysFiltered &lt;- lapply(pathways, function(p) { as.vector(na.omit(fmatch(p, rownames(mat)))) })&#xD;&#xA;        pathwaysSizes &lt;- sapply(pathwaysFiltered, length)&#xD;&#xA;    &#xD;&#xA;        toKeep &lt;- which(minSize &lt;= pathwaysSizes &amp; pathwaysSizes &lt;= maxSize)&#xD;&#xA;        m &lt;- length(toKeep)&#xD;&#xA;    &#xD;&#xA;        if (m == 0) {&#xD;&#xA;            return(data.table(pathway=character(),&#xD;&#xA;                              pval=numeric(),&#xD;&#xA;                              padj=numeric(),&#xD;&#xA;                              ES=numeric(),&#xD;&#xA;                              NES=numeric(),&#xD;&#xA;                              nMoreExtreme=numeric(),&#xD;&#xA;                              size=integer(),&#xD;&#xA;                              leadingEdge=list()))&#xD;&#xA;        }&#xD;&#xA;    &#xD;&#xA;        pathwaysFiltered &lt;- pathwaysFiltered[toKeep]&#xD;&#xA;        pathwaysSizes &lt;- pathwaysSizes[toKeep]&#xD;&#xA;    &#xD;&#xA;        corRanks &lt;- var(tmatSc, labelsSc)[,1]&#xD;&#xA;        ranksOrder &lt;- order(corRanks, decreasing=T)&#xD;&#xA;        ranksOrderInv &lt;- invPerm(ranksOrder)&#xD;&#xA;        stats &lt;- corRanks[ranksOrder]&#xD;&#xA;    &#xD;&#xA;        pathwaysReordered &lt;- lapply(pathwaysFiltered, function(x) ranksOrderInv[x])&#xD;&#xA;    &#xD;&#xA;        gseaStatRes &lt;- do.call(rbind,&#xD;&#xA;                               lapply(pathwaysReordered, calcGseaStat,&#xD;&#xA;                                      stats=stats,&#xD;&#xA;                                      returnLeadingEdge=TRUE))&#xD;&#xA;&#xD;&#xA;Thank you for your consideration.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4889" PostHistoryTypeId="2" PostId="2237" RevisionGUID="335c952f-a7fd-4d01-83b7-94fc708d95d7" CreationDate="2017-08-07T14:57:32.910" UserId="298" Text="I am using the following command to get all refseq genes from UCSC:&#xD;&#xA;&#xD;&#xA;    /usr/bin/mysql --user=genomep --password=password --host=genome-mysql.cse.ucsc.edu \&#xD;&#xA;        -A -D hg38 -e 'select concat(t.name, &quot;.&quot;, i.version) name, \&#xD;&#xA;        k.locusLinkId as &quot;EntrezId&quot;, t.chrom, t.strand, t.txStart, \&#xD;&#xA;        t.txEnd, t.cdsStart, t.cdsEnd, t.exonCount, t.exonStarts, \&#xD;&#xA;        t.exonEnds, t.score, t.name2 from refGene t join hgFixed.gbCdnaInfo i \&#xD;&#xA;        on t.name = i.acc join hgFixed.refLink k on t.name = k.mrnaAcc'&#xD;&#xA;&#xD;&#xA;That returns data in the following format (showing the 1st 5 lines):&#xD;&#xA;&#xD;&#xA;    +-------------+-----------+-------+--------+---------+-------+----------+--------+-----------+--------------------+--------------------+-------+-----------+&#xD;&#xA;    | name        | EntrezId  | chrom | strand | txStart | txEnd | cdsStart | cdsEnd | exonCount | exonStarts         | exonEnds           | score | name2     |&#xD;&#xA;    +-------------+-----------+-------+--------+---------+-------+----------+--------+-----------+--------------------+--------------------+-------+-----------+&#xD;&#xA;    | NR_046018.2 | 100287102 | chr1  | +      |   11873 | 14409 |    14409 |  14409 |         3 | 11873,12612,13220, | 12227,12721,14409, |     0 | DDX11L1   |&#xD;&#xA;    | NR_106918.1 | 102466751 | chr1  | -      |   17368 | 17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-1 |&#xD;&#xA;    | NR_107062.1 | 102465909 | chr1  | -      |   17368 | 17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-2 |&#xD;&#xA;    | NR_107063.1 | 102465910 | chr1  | -      |   17368 | 17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-3 |&#xD;&#xA;    | NR_128720.1 | 103504738 | chr1  | -      |   17368 | 17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-4 |&#xD;&#xA;    +-------------+-----------+-------+--------+---------+-------+----------+--------+-----------+--------------------+--------------------+-------+-----------+&#xD;&#xA;&#xD;&#xA;I also want to find the accession of the canonical transcript of each of the genes returned by the command above. Those seem to be stored in the `knownCanonical` table:&#xD;&#xA;&#xD;&#xA;    $ /usr/bin/mysql --user=genomep --password=password --host=genome-mysql.cse.ucsc.edu -A -D hg38 -e 'select * from knownCanonical limit 5'&#xD;&#xA;    +-------+------------+-----------+-----------+------------+--------------------+&#xD;&#xA;    | chrom | chromStart | chromEnd  | clusterId | transcript | protein            |&#xD;&#xA;    +-------+------------+-----------+-----------+------------+--------------------+&#xD;&#xA;    | chrX  |  100628669 | 100636806 |         1 | uc004ega.3 | ENSG00000000003.14 |&#xD;&#xA;    | chrX  |  100584801 | 100599885 |         2 | uc004efy.5 | ENSG00000000005.5  |&#xD;&#xA;    | chr20 |   50934866 |  50958550 |         3 | uc002xvw.2 | ENSG00000000419.12 |&#xD;&#xA;    | chr1  |  169853073 | 169893959 |         4 | uc001ggs.5 | ENSG00000000457.13 |&#xD;&#xA;    | chr1  |  169795048 | 169854080 |         5 | uc001ggp.4 | ENSG00000000460.16 |&#xD;&#xA;    +-------+------------+-----------+-----------+------------+--------------------+&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;However, there seems to be no obvious way to link the `knownCanonical` table to the `refGene`, `hgFixed.gbCdnaInfo` and `hgFixed.refLink` tables used above. &#xD;&#xA;&#xD;&#xA;So, how can I modify my 1st query (or, if necessary write a new one) so that my results also include the accession of the gene's canonical transcript? " />
  <row Id="4890" PostHistoryTypeId="1" PostId="2237" RevisionGUID="335c952f-a7fd-4d01-83b7-94fc708d95d7" CreationDate="2017-08-07T14:57:32.910" UserId="298" Text="Get canonical transcript from UCSC" />
  <row Id="4891" PostHistoryTypeId="3" PostId="2237" RevisionGUID="335c952f-a7fd-4d01-83b7-94fc708d95d7" CreationDate="2017-08-07T14:57:32.910" UserId="298" Text="&lt;identifiers&gt;&lt;ucsc&gt;" />
  <row Id="4892" PostHistoryTypeId="2" PostId="2238" RevisionGUID="91c5fda5-dd13-4953-99fa-d328fd3e285c" CreationDate="2017-08-07T15:06:06.557" UserId="292" Text="## A (bio)awk-based solution&#xD;&#xA;&#xD;&#xA;### Generate the list of genus&#xD;&#xA;&#xD;&#xA;    bioawk -c fastx '{print $4}' terminase_large.fasta \&#xD;&#xA;        | sed -r 's/.*\[(\w+).*/\1/' \&#xD;&#xA;        | sort -u &gt; genus_names.txt&#xD;&#xA;&#xD;&#xA;Here is how it works:&#xD;&#xA;&#xD;&#xA;1. [`bioawk`](https://github.com/lh3/bioawk) is like awk but with extra parsing capabilities. `-c fastx` parses the fields in fasta or fastq records. `{print $comment}` will print the `$comment` field which contain the genus names you want to extract.&#xD;&#xA;&#xD;&#xA;2. The `sed` command captures (with parentheses) the word (`\w+`) that is just after the opening square bracket (which should be the genus name, if there is only one opening square bracket on the line), and substitutes the whole line (that is, everything before the bracket (`.*`), the bracket (`\[`), the word, and everything after the word (`.*` again)) with just the captured word (`\1`).&#xD;&#xA;&#xD;&#xA;3. All this is sorted, and the `-u` option of `sort` ensures there is only one occurrence of each genus name in the output, which we put in the `genus_names.txt` file.&#xD;&#xA;&#xD;&#xA;### Extract the fasta records for each genus&#xD;&#xA;&#xD;&#xA;    for genus in $(cat genus_names.txt)&#xD;&#xA;    do&#xD;&#xA;        bioawk -c fastx '{print}' terminase_large.fasta \&#xD;&#xA;            | grep ${genus} \&#xD;&#xA;            | awk -F &quot;\t&quot; '{print &quot;&gt;&quot;$1 $4&quot;\n&quot;$2}' &gt; ${genus}.fasta&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;Here is how it works:&#xD;&#xA;&#xD;&#xA;1. We loop on the content of the previously established list of genus names.&#xD;&#xA;&#xD;&#xA;2. For each value of `${genus}`, we parse again the fasta file with `bioawk`. This time, for a given fasta record, we print all the elements that have been parsed. They appear on one line, in the following order (according to `bioawk -c help`): `1:name 2:seq 3:qual 4:comment`&#xD;&#xA;&#xD;&#xA;3. We use `grep` to select the resulting lines that contain the genus name were currently dealing with.&#xD;&#xA;&#xD;&#xA;4. The lines that have been successfully selected are then parsed by awk and reformatted into fasta format. The `-F &quot;\t&quot;` is to indicate that the field delimiters are tabulations (which is how bioawk has written the records when we did `{print}`). The output is written in a file named using the current value of `${genus}`.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4893" PostHistoryTypeId="5" PostId="2238" RevisionGUID="d33ce9e6-b074-4eae-9da4-6a37fd372cdf" CreationDate="2017-08-07T15:20:23.450" UserId="292" Comment="Added header-reformatting in the output" Text="## A (bio)awk-based solution&#xD;&#xA;&#xD;&#xA;### Generate the list of genus&#xD;&#xA;&#xD;&#xA;    bioawk -c fastx '{print $comment}' terminase_large.fasta \&#xD;&#xA;        | sed -r 's/.*\[(\w+).*/\1/' \&#xD;&#xA;        | sort -u &gt; genus_names.txt&#xD;&#xA;&#xD;&#xA;Here is how it works:&#xD;&#xA;&#xD;&#xA;1. [`bioawk`](https://github.com/lh3/bioawk) is like awk but with extra parsing capabilities. `-c fastx` parses the fields in fasta or fastq records. `{print $comment}` will print the `$comment` field which contain the genus names you want to extract.&#xD;&#xA;&#xD;&#xA;2. The `sed` command captures (with parentheses) the word (`\w+`) that is just after the opening square bracket (which should be the genus name, if there is only one opening square bracket on the line), and substitutes the whole line (that is, everything before the bracket (`.*`), the bracket (`\[`), the word, and everything after the word (`.*` again)) with just the captured word (`\1`).&#xD;&#xA;&#xD;&#xA;3. All this is sorted, and the `-u` option of `sort` ensures there is only one occurrence of each genus name in the output, which we put in the `genus_names.txt` file.&#xD;&#xA;&#xD;&#xA;### Extract the fasta records for each genus&#xD;&#xA;&#xD;&#xA;    for genus in $(cat genus_names.txt)&#xD;&#xA;    do&#xD;&#xA;        bioawk -c fastx '{print}' terminase_large.fasta \&#xD;&#xA;            | grep ${genus} \&#xD;&#xA;            | awk -F &quot;\t&quot; '{print &quot;&gt;&quot;$4&quot;\n&quot;$2}' \&#xD;&#xA;            | sed -r 's/^&gt;[^\[]+\[(.*)\]/&gt;\1/' &gt; ${genus}.fasta&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;Here is how it works:&#xD;&#xA;&#xD;&#xA;1. We loop on the content of the previously established list of genus names.&#xD;&#xA;&#xD;&#xA;2. For each value of `${genus}`, we parse again the fasta file with `bioawk`. This time, for a given fasta record, we print all the elements that have been parsed. They appear on one line, in the following order (according to `bioawk -c help`): `1:name 2:seq 3:qual 4:comment`&#xD;&#xA;&#xD;&#xA;3. We use `grep` to select the resulting lines that contain the genus name were currently dealing with.&#xD;&#xA;&#xD;&#xA;4. The lines that have been successfully selected are then parsed by awk and reformatted into fasta format using only the comment part in the header. The `-F &quot;\t&quot;` is to indicate that the field delimiters are tabulations (which is how bioawk has written the records when we did `{print}`).&#xD;&#xA;&#xD;&#xA;5. We use `sed` to keep only the part inside the square brackets (again using a pattern capture between parentheses, that we use in the substitution part with `\1`), and the output is written in a file named using the current value of `${genus}`.&#xD;&#xA;&#xD;&#xA;More details on the last `sed` command:&#xD;&#xA;&#xD;&#xA;We match the whole header lines. They start with &quot;&gt;&quot; (`^&gt;`), then there are some non-&quot;[&quot; characters (`[^\[]+`), then a &quot;[&quot; (`\[`), then something we capture (`(.*)`), then a &quot;]&quot; (`\]`). We replace them with the captured part. The sequence lines are unaffected because they don't match the above regular expression." />
  <row Id="4894" PostHistoryTypeId="5" PostId="2228" RevisionGUID="09242494-3e7f-4860-ab0b-4a219bef7b5b" CreationDate="2017-08-07T16:41:45.307" UserId="776" Comment="added 99 characters in body" Text="You could [convert VCF to BED][1] via `vcf2bed --snvs`, `vcf2bed --insertions`, and `vcf2bed --deletions`, and then use `samtools faidx` by way of a [wrapper script][2] to convert BED to FASTA, e.g.:&#xD;&#xA;&#xD;&#xA;    $ vcf2bed --snvs &lt; variants.vcf | bed2faidxsta.pl &gt; snvs.fa&#xD;&#xA;    $ vcf2bed --insertions &lt; variants.vcf | bed2faidxsta.pl &gt; insertions.fa&#xD;&#xA;    $ vcf2bed --deletions &lt; variants.vcf | bed2faidxsta.pl &gt; deletions.fa&#xD;&#xA;&#xD;&#xA;You need FASTA files for your reference genome, which have been indexed with `samtools faidx`, e.g., for `hg38`:&#xD;&#xA;&#xD;&#xA;    $ cd /foo/bar/baz&#xD;&#xA;    $ wget ftp://hgdownload.cse.ucsc.edu/goldenPath/hg38/chromosomes/*.fa.gz&#xD;&#xA;    $ for fn in `ls *.fa.gz`; do gunzip $fn; done&#xD;&#xA;    $ for fn in `ls *.fa`; do samtools faidx $fn; done&#xD;&#xA;&#xD;&#xA;Once you have indexed FASTA files somewhere on your file system, you can pipe BED to the `bed2faidxsta.pl` script, to get out FASTA sequences.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/vcf2bed.html&#xD;&#xA;  [2]: https://gist.github.com/alexpreynolds/fa9b0f90e181e3b4f640" />
  <row Id="4895" PostHistoryTypeId="2" PostId="2239" RevisionGUID="42eacc25-5580-4771-9d78-c6c30ab30997" CreationDate="2017-08-07T21:02:46.693" UserId="77" Text="You can link them with the `kgXref` table, since `kgXref.refseq == refGene.name` and `kgXref.kgID == knownCanonical.transcript`. Since it seems that `knownCanonical.transcript` is what you want anyway, you don't even need to join on it:&#xD;&#xA;&#xD;&#xA;    mysql --user=genomep --password=password --host=genome-mysql.cse.ucsc.edu \&#xD;&#xA;    -A -D hg38 -e 'select concat(t.name, &quot;.&quot;, i.version) name, x.kgID, \&#xD;&#xA;    k.locusLinkId as &quot;EntrezId&quot;, t.chrom, t.strand, t.txStart, \&#xD;&#xA;    t.txEnd, t.cdsStart, t.cdsEnd, t.exonCount, t.exonStarts, \&#xD;&#xA;    t.exonEnds, t.score, t.name2 from refGene t join (hgFixed.gbCdnaInfo i, hgFixed.refLink k, kgXref x) \&#xD;&#xA;    on (t.name = i.acc and t.name = k.mrnaAcc and t.name = x.refseq) limit 10'&#xD;&#xA;&#xD;&#xA;The output is then:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    +-------------+------------+-----------+-------+--------+---------+--------+----------+--------+-----------+--------------------+--------------------+-------+------------+&#xD;&#xA;    | name        | kgID       | EntrezId  | chrom | strand | txStart | txEnd  | cdsStart | cdsEnd | exonCount | exonStarts         | exonEnds           | score | name2      |&#xD;&#xA;    +-------------+------------+-----------+-------+--------+---------+--------+----------+--------+-----------+--------------------+--------------------+-------+------------+&#xD;&#xA;    | NR_106918.1 | uc031tla.1 | 102466751 | chr1  | -      |   17368 |  17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-1  |&#xD;&#xA;    | NR_107062.1 | uc031tlm.1 | 102465909 | chr1  | -      |   17368 |  17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-2  |&#xD;&#xA;    | NR_107063.1 | uc032cta.1 | 102465910 | chr1  | -      |   17368 |  17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-3  |&#xD;&#xA;    | NR_128720.1 | uc032dmn.1 | 103504738 | chr1  | -      |   17368 |  17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-4  |&#xD;&#xA;    | NR_036051.1 | uc031tlb.1 | 100302278 | chr1  | +      |   30365 |  30503 |    30503 |  30503 |         1 | 30365,             | 30503,             |     0 | MIR1302-2  |&#xD;&#xA;    | NR_036266.1 | uc033cjs.1 | 100422831 | chr1  | +      |   30365 |  30503 |    30503 |  30503 |         1 | 30365,             | 30503,             |     0 | MIR1302-9  |&#xD;&#xA;    | NR_036267.1 | uc032csz.1 | 100422834 | chr1  | +      |   30365 |  30503 |    30503 |  30503 |         1 | 30365,             | 30503,             |     0 | MIR1302-10 |&#xD;&#xA;    | NR_036268.1 | uc032hiw.1 | 100422919 | chr1  | +      |   30365 |  30503 |    30503 |  30503 |         1 | 30365,             | 30503,             |     0 | MIR1302-11 |&#xD;&#xA;    | NR_026822.1 | uc001aak.4 |    654835 | chr1  | -      |   34610 |  36081 |    36081 |  36081 |         3 | 34610,35276,35720, | 35174,35481,36081, |     0 | FAM138C    |&#xD;&#xA;    | NR_106918.1 | uc031tla.1 | 102466751 | chr1  | -      |  187890 | 187958 |   187958 | 187958 |         1 | 187890,            | 187958,            |     0 | MIR6859-1  |&#xD;&#xA;    +-------------+------------+-----------+-------+--------+---------+--------+----------+--------+-----------+--------------------+--------------------+-------+------------+&#xD;&#xA;&#xD;&#xA;If you want the protein ID then join `knownCanonical` on `x.kgID`.&#xD;&#xA;    " />
  <row Id="4896" PostHistoryTypeId="2" PostId="2240" RevisionGUID="953a0353-40bc-44d8-a777-d7324eee45cb" CreationDate="2017-08-07T21:13:05.843" UserId="60" Text="Just for reference, there is a not very in depth, but first-hand reasoning (Jim Kent) given here about why bedGraphToBigWig does not support streaming http://genome.soe.ucsc.narkive.com/2S1Z3VpG/bedgraphtobigwig-reading-in-from-stdin" />
  <row Id="4898" PostHistoryTypeId="5" PostId="2223" RevisionGUID="a16a425a-7ea9-4113-a1c8-39b1c67c3162" CreationDate="2017-08-07T21:31:56.017" UserId="96" Comment="formatting fix" Text="I have some software which takes fastas as the input. I need to include SNVs and InDels from a VCF into the reference hg38 and then use this.&#xD;&#xA;&#xD;&#xA;The problem is, I don't know of an algorithmically sound way to do this.&#xD;&#xA;&#xD;&#xA;1. Are there any existing software packages which could do this efficiently? Is it easier to output a FASTA, or a bam (and then convert to a FASTA)? &#xD;&#xA;&#xD;&#xA;2. What about if I wanted to do the same with a bedpe of germline structural variants? " />
  <row Id="4899" PostHistoryTypeId="2" PostId="2241" RevisionGUID="e18d1dcb-d72f-4d45-9750-0fc48303077f" CreationDate="2017-08-07T23:00:43.070" UserId="823" Text="I am looking the secretome profile and the membrane receptor profile for a given cell type. &#xD;&#xA;&#xD;&#xA;In my specific case, this should be the secretome and outer membrane receptor profiles of dorsal root ganglion. &#xD;&#xA;&#xD;&#xA;What I've done in the past is taken proteomics datasets or RNAseq datasets and used this as a reference cell surface proteome or secretome. &#xD;&#xA;&#xD;&#xA;I wasn't sure what the best way to filter secreted proteins or membrane receptors were, so I used the Human Protein Atlas and this turned out to be a big headache. A possible alternative method might be to use DAVID, but there must be a more efficient method via the command line.&#xD;&#xA;&#xD;&#xA;My questions are these:&#xD;&#xA;&#xD;&#xA; -  Is there a &quot;consensus&quot; database containing protein expression&#xD;&#xA;   profiles for different tissue types in human?&#xD;&#xA; - If not, what is the best way to filter out the **secreted** and **transmembrane** proteins by gene symbol?" />
  <row Id="4900" PostHistoryTypeId="1" PostId="2241" RevisionGUID="e18d1dcb-d72f-4d45-9750-0fc48303077f" CreationDate="2017-08-07T23:00:43.070" UserId="823" Text="Secretome and membrane receptor profiles" />
  <row Id="4901" PostHistoryTypeId="3" PostId="2241" RevisionGUID="e18d1dcb-d72f-4d45-9750-0fc48303077f" CreationDate="2017-08-07T23:00:43.070" UserId="823" Text="&lt;public-databases&gt;&lt;networks&gt;" />
  <row Id="4902" PostHistoryTypeId="5" PostId="2234" RevisionGUID="94c2124c-277a-41b7-948b-0d9b54fa9d9a" CreationDate="2017-08-07T23:52:55.690" UserId="1283" Comment="added results of aparently good two group comparison to orginal question" Text=" I wish to adapt the r language function fgseaL, https://github.com/ctlab/fgsea , to perform rapidGSEA, https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1244-x , computation of inter-class  deviation per gene and the subsequent gene rank sorting operation on 9 different phenotype labels as illustrated in the diagram immediately below:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/kXvmr.gif&#xD;&#xA;&#xD;&#xA;I thought of applying the R-language rank() function on the Expression Data Matrix D. If that is not correct, what sequence of R language commands should we&#xD;&#xA;apply to the Expression Data Matrix D to calculate a key value sorted deviation measure  across 8 labeled human leukemia groups and a healthy labeled normal control group prior to running fgseaL? &#xD;&#xA;&#xD;&#xA;I show below how fgseaL finds the correlation matrix between the R language variable , mat, which corresponds to the Expression Data Matrix D and the R language variable , labels , which is a vector of gene phenotype labels&#xD;&#xA;&#xD;&#xA;        tmatSc &lt;- scale(t(mat))&#xD;&#xA;        labelsSc &lt;- scale(labels)[, 1]&#xD;&#xA;    &#xD;&#xA;        minSize &lt;- max(minSize, 1)&#xD;&#xA;    &#xD;&#xA;        pathwaysFiltered &lt;- lapply(pathways, function(p) { as.vector(na.omit(fmatch(p, rownames(mat)))) })&#xD;&#xA;        pathwaysSizes &lt;- sapply(pathwaysFiltered, length)&#xD;&#xA;    &#xD;&#xA;        toKeep &lt;- which(minSize &lt;= pathwaysSizes &amp; pathwaysSizes &lt;= maxSize)&#xD;&#xA;        m &lt;- length(toKeep)&#xD;&#xA;    &#xD;&#xA;        if (m == 0) {&#xD;&#xA;            return(data.table(pathway=character(),&#xD;&#xA;                              pval=numeric(),&#xD;&#xA;                              padj=numeric(),&#xD;&#xA;                              ES=numeric(),&#xD;&#xA;                              NES=numeric(),&#xD;&#xA;                              nMoreExtreme=numeric(),&#xD;&#xA;                              size=integer(),&#xD;&#xA;                              leadingEdge=list()))&#xD;&#xA;        }&#xD;&#xA;    &#xD;&#xA;        pathwaysFiltered &lt;- pathwaysFiltered[toKeep]&#xD;&#xA;        pathwaysSizes &lt;- pathwaysSizes[toKeep]&#xD;&#xA;    &#xD;&#xA;        corRanks &lt;- var(tmatSc, labelsSc)[,1]&#xD;&#xA;        ranksOrder &lt;- order(corRanks, decreasing=T)&#xD;&#xA;        ranksOrderInv &lt;- invPerm(ranksOrder)&#xD;&#xA;        stats &lt;- corRanks[ranksOrder]&#xD;&#xA;    &#xD;&#xA;        pathwaysReordered &lt;- lapply(pathwaysFiltered, function(x) ranksOrderInv[x])&#xD;&#xA;    &#xD;&#xA;        gseaStatRes &lt;- do.call(rbind,&#xD;&#xA;                               lapply(pathwaysReordered, calcGseaStat,&#xD;&#xA;                                      stats=stats,&#xD;&#xA;                                      returnLeadingEdge=TRUE))&#xD;&#xA;&#xD;&#xA;    correcttest &lt;- data.frame(names = row.names(normal))&#xD;&#xA;    correcttest &lt;- cbind(correcttest3, normal)&#xD;&#xA;    correcttest &lt;- cbind(correcttest3, ALL3m)&#xD;&#xA;    rownames(correcttest) &lt;- correcttest$names&#xD;&#xA;    correcttest$names &lt;- NULL&#xD;&#xA;    correctlabelnormal &lt;- rep(0:0, 73)&#xD;&#xA;    correctlabelALL3m &lt;- rep(1:1, 122)&#xD;&#xA;    correctlabel &lt;- as.vector(c(correctlabelnormal,correctlabelALL3m))&#xD;&#xA;    fgseaL(df,correcttest,correctlabel,nperm = 2000,minSize = 1, maxSize=50000)&#xD;&#xA;           pathway        pval        padj         ES       NES nMoreExtreme  size&#xD;&#xA;    1: Gene.Symbol 0.003940887 0.003940887 -0.2460126 -1.180009            3 45714&#xD;&#xA;                                     leadingEdge&#xD;&#xA;    1: AKIRIN2,LRRC20,HSPA5,HSPA5,DTWD2,ZFYVE28,&#xD;&#xA;&#xD;&#xA;Thank you for your consideration.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4903" PostHistoryTypeId="5" PostId="2234" RevisionGUID="f90062f4-1a1e-41fe-8b9d-a19dd7edb77c" CreationDate="2017-08-08T00:28:04.583" UserId="1283" Comment="added problematic r language example and apparently okay binary phentotype  labeled group results." Text=" I wish to adapt the r language function fgseaL, https://github.com/ctlab/fgsea , to perform rapidGSEA, https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1244-x , computation of inter-class  deviation per gene and the subsequent gene rank sorting operation on 9 different phenotype labels as illustrated in the diagram immediately below:&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/kXvmr.gif&#xD;&#xA;&#xD;&#xA;I thought of applying the R-language rank() function on the Expression Data Matrix D. If that is not correct, what sequence of R language commands should we&#xD;&#xA;apply to the Expression Data Matrix D to calculate a key value sorted deviation measure  across 8 labeled human leukemia groups and a healthy labeled normal control group prior to running fgseaL? &#xD;&#xA;&#xD;&#xA;I show below how fgseaL finds the correlation matrix between the R language variable , mat, which corresponds to the Expression Data Matrix D and the R language variable , labels , which is a vector of gene phenotype labels&#xD;&#xA;&#xD;&#xA;        tmatSc &lt;- scale(t(mat))&#xD;&#xA;        labelsSc &lt;- scale(labels)[, 1]&#xD;&#xA;    &#xD;&#xA;        minSize &lt;- max(minSize, 1)&#xD;&#xA;    &#xD;&#xA;        pathwaysFiltered &lt;- lapply(pathways, function(p) { as.vector(na.omit(fmatch(p, rownames(mat)))) })&#xD;&#xA;        pathwaysSizes &lt;- sapply(pathwaysFiltered, length)&#xD;&#xA;    &#xD;&#xA;        toKeep &lt;- which(minSize &lt;= pathwaysSizes &amp; pathwaysSizes &lt;= maxSize)&#xD;&#xA;        m &lt;- length(toKeep)&#xD;&#xA;    &#xD;&#xA;        if (m == 0) {&#xD;&#xA;            return(data.table(pathway=character(),&#xD;&#xA;                              pval=numeric(),&#xD;&#xA;                              padj=numeric(),&#xD;&#xA;                              ES=numeric(),&#xD;&#xA;                              NES=numeric(),&#xD;&#xA;                              nMoreExtreme=numeric(),&#xD;&#xA;                              size=integer(),&#xD;&#xA;                              leadingEdge=list()))&#xD;&#xA;        }&#xD;&#xA;    &#xD;&#xA;        pathwaysFiltered &lt;- pathwaysFiltered[toKeep]&#xD;&#xA;        pathwaysSizes &lt;- pathwaysSizes[toKeep]&#xD;&#xA;    &#xD;&#xA;        corRanks &lt;- var(tmatSc, labelsSc)[,1]&#xD;&#xA;        ranksOrder &lt;- order(corRanks, decreasing=T)&#xD;&#xA;        ranksOrderInv &lt;- invPerm(ranksOrder)&#xD;&#xA;        stats &lt;- corRanks[ranksOrder]&#xD;&#xA;    &#xD;&#xA;        pathwaysReordered &lt;- lapply(pathwaysFiltered, function(x) ranksOrderInv[x])&#xD;&#xA;    &#xD;&#xA;        gseaStatRes &lt;- do.call(rbind,&#xD;&#xA;                               lapply(pathwaysReordered, calcGseaStat,&#xD;&#xA;                                      stats=stats,&#xD;&#xA;                                      returnLeadingEdge=TRUE))&#xD;&#xA;&#xD;&#xA;    I found a problem with the algorithm shown immediately below.&#xD;&#xA;    correcttest &lt;- data.frame(names = row.names(normal))&#xD;&#xA;    correcttest &lt;- cbind(correcttest3, normal)&#xD;&#xA;    correcttest &lt;- cbind(correcttest3, ALL3m)&#xD;&#xA;    rownames(correcttest) &lt;- correcttest$names&#xD;&#xA;    correcttest$names &lt;- NULL&#xD;&#xA;    correctlabelnormal &lt;- rep(0:0, 73)&#xD;&#xA;    correctlabelALL3m &lt;- rep(1:1, 122)&#xD;&#xA;    correctlabel &lt;- as.vector(c(correctlabelnormal,correctlabelALL3m))&#xD;&#xA;    s &lt;- apply(correcttest, 1, function(x) coef(lm(x~correctlabel))[2])&#xD;&#xA;    o &lt;- rank(s)&#xD;&#xA;    o &lt;- max(o) - o + 1&#xD;&#xA;    res &lt;- fgseaL(df,o,correctlabel,nperm = 2000,minSize = 1, maxSize=50000)&#xD;&#xA;    empty data table (0 rows) of 8 columns:   pathway,pval,padj,ES,NES,nMoreExtreme,size&#xD;&#xA;&#xD;&#xA;    I found the binary phenotype labeled group fgseaL test results below looked satisfactory.    &#xD;&#xA;    correcttest &lt;- data.frame(names = row.names(normal))&#xD;&#xA;    correcttest &lt;- cbind(correcttest3, normal)&#xD;&#xA;    correcttest &lt;- cbind(correcttest3, ALL3m)&#xD;&#xA;    rownames(correcttest) &lt;- correcttest$names&#xD;&#xA;    correcttest$names &lt;- NULL&#xD;&#xA;    correctlabelnormal &lt;- rep(0:0, 73)&#xD;&#xA;    correctlabelALL3m &lt;- rep(1:1, 122)&#xD;&#xA;    correctlabel &lt;- as.vector(c(correctlabelnormal,correctlabelALL3m))&#xD;&#xA;    fgseaL(df,correcttest,correctlabel,nperm = 2000,minSize = 1, maxSize=50000)&#xD;&#xA;           pathway        pval        padj         ES       NES nMoreExtreme  size&#xD;&#xA;    1: Gene.Symbol 0.003940887 0.003940887 -0.2460126 -1.180009            3 45714&#xD;&#xA;                                     leadingEdge&#xD;&#xA;    1: AKIRIN2,LRRC20,HSPA5,HSPA5,DTWD2,ZFYVE28,&#xD;&#xA;&#xD;&#xA;Thank you for your consideration.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4905" PostHistoryTypeId="6" PostId="2237" RevisionGUID="0f3c92fc-ffa0-4a80-9a5a-00918c18ba57" CreationDate="2017-08-08T03:13:26.597" UserId="96" Comment="added tag" Text="&lt;public-databases&gt;&lt;identifiers&gt;&lt;ucsc&gt;" />
  <row Id="4906" PostHistoryTypeId="2" PostId="2242" RevisionGUID="4599f13f-a2c5-42ef-bdf4-88563c0bd42e" CreationDate="2017-08-08T09:59:08.317" UserId="294" Text="I would like to convert a [`BED`][1] format to [`GFF3`][2].&#xD;&#xA;&#xD;&#xA;The only useful tool that I could find via a [google search][3] seems to be [Galaxy][4], and I do not feel very comfortable with online tools. &#xD;&#xA;&#xD;&#xA;Does anyone knows about a command-line tool that can handle this conversion?&#xD;&#xA;&#xD;&#xA;  [1]: https://genome.ucsc.edu/FAQ/FAQformat#format1&#xD;&#xA;  [2]: http://www.ensembl.org/info/website/upload/gff3.html&#xD;&#xA;  [3]: https://www.google.ch/search?q=bed%20to%20gff3&#xD;&#xA;  [4]: https://galaxy.inf.ethz.ch/" />
  <row Id="4907" PostHistoryTypeId="1" PostId="2242" RevisionGUID="4599f13f-a2c5-42ef-bdf4-88563c0bd42e" CreationDate="2017-08-08T09:59:08.317" UserId="294" Text="How to convert BED to GFF3" />
  <row Id="4908" PostHistoryTypeId="3" PostId="2242" RevisionGUID="4599f13f-a2c5-42ef-bdf4-88563c0bd42e" CreationDate="2017-08-08T09:59:08.317" UserId="294" Text="&lt;software-recommendation&gt;&lt;bed&gt;&lt;format-conversion&gt;&lt;gff3&gt;" />
  <row Id="4909" PostHistoryTypeId="2" PostId="2243" RevisionGUID="d3abf7d5-1a9a-4d9d-bc9d-fdd98b9533ae" CreationDate="2017-08-08T11:02:02.630" UserId="235" Text="Just to point out that if you want to follow @Devon Ryan's answer for a different organism/assembly, that is not in his very useful linked resource, you can download NCBI to UCSC contig to chromosome number mappings from https://www.ncbi.nlm.nih.gov/assembly.&#xD;&#xA;&#xD;&#xA;To the site and search for your assembly name. At the bottom of the page is a box called &quot;Global assembly definition&quot; containing a link titled &quot;Download full sequence report&quot;.&#xD;&#xA;&#xD;&#xA;The downloaded file contains a table with:&#xD;&#xA;&#xD;&#xA;- Chromosome numbers in &quot;Sequence-Name&quot;/&quot;Assigned-molecule&quot;  &#xD;&#xA;- NCBI names in Refseq-Accn  &#xD;&#xA;- UCSC contig name in &quot;UCSC-style-name&quot;" />
  <row Id="4910" PostHistoryTypeId="5" PostId="2242" RevisionGUID="d1786955-a4d0-4535-8045-3d8bf8ec5df6" CreationDate="2017-08-08T11:07:54.597" UserId="294" Comment="added 1466 characters in body" Text="I would like to convert a [`BED`][1] format to [`GFF3`][2].&#xD;&#xA;&#xD;&#xA;The only useful tool that I could find via a [google search][3] seems to be [Galaxy][4], and I do not feel very comfortable with online tools. &#xD;&#xA;&#xD;&#xA;Does anyone knows about a command-line tool that can handle this conversion?&#xD;&#xA;&#xD;&#xA;Edit: here are some lines of my BED file: &#xD;&#xA;&#xD;&#xA;    $ head -4 last_minion-r7_sort.bed&#xD;&#xA;    211000022278137	175	211	8e5d0959-7cdb-49cf-9298-94ed3b2aedb5_Basecall_2D_000_2d	42	+&#xD;&#xA;    211000022279134	0	503	e8a9c6b8-bad2-4a7e-97d8-ca4acb34ff70_Basecall_2D_000_2d	69	-&#xD;&#xA;    211000022279134	24	353	e258783d-95a3-41f5-9ad5-bb12311dbaf4_Basecall_2D_000_2d	45	-&#xD;&#xA;    211000022279134	114	429	26601afb-581a-41df-b42b-b366148ea06f_Basecall_2D_000_2d	100	-&#xD;&#xA;&#xD;&#xA;The bed file thus has 6 columns as for now: chromosome, start coordinate, end coordinate, read name, score, strand. This file was obtained from conversion of `MAF` format (as output of alignment of RNA-seq reads to reference genome, using [`LAST`][5]) converted to `SAM` using [`maf-convert`][6], then to `BAM` using [`samtools`][7], finally to `BED` using [`bedtools`][8]. &#xD;&#xA;&#xD;&#xA;The aim of my conversion is basically to convert `SAM` -&gt; `GTF`, for [post-processing][9]. Since there is no straightforward way to do this, I am going through steps, the only way to do this in my knowledge is : `SAM` -&gt; `BAM` -&gt; `BED` -&gt; `GFF3` -&gt; `GTF` but for now I am stuck in the `BED` -&gt; `GFF3` part. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://genome.ucsc.edu/FAQ/FAQformat#format1&#xD;&#xA;  [2]: http://www.ensembl.org/info/website/upload/gff3.html&#xD;&#xA;  [3]: https://www.google.ch/search?q=bed%20to%20gff3&#xD;&#xA;  [4]: https://galaxy.inf.ethz.ch/&#xD;&#xA;  [5]: http://last.cbrc.jp/&#xD;&#xA;  [6]: http://last.cbrc.jp/doc/maf-convert.html&#xD;&#xA;  [7]: http://samtools.sourceforge.net/&#xD;&#xA;  [8]: http://bedtools.readthedocs.io/en/latest/content/bedtools-suite.html&#xD;&#xA;  [9]: https://bioinformatics.stackexchange.com/questions/2036/tools-to-reconcile-experimental-transcripts-with-reference-annotation" />
  <row Id="4911" PostHistoryTypeId="2" PostId="2244" RevisionGUID="e1a8c376-afd8-4578-b171-edab3ca29740" CreationDate="2017-08-08T11:42:33.840" UserId="235" Text="To answer the question as asked,  for people googling.&#xD;&#xA;In python:&#xD;&#xA;&#xD;&#xA;    #contigs.tsv contians chromosome names and lengths in two columns&#xD;&#xA;    for line in open(&quot;contigs.tsv&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;0&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;my_bed_file.bed&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;bed&quot;, &quot;interval&quot;, fields[1], fields[2], fields[4], fields[5], &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (fields[3], fields[0])&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4912" PostHistoryTypeId="5" PostId="2244" RevisionGUID="640a2d03-d1db-4fc8-a94e-2cb9e6f0dc42" CreationDate="2017-08-08T11:48:39.640" UserId="77" Comment="added 30 characters in body" Text="To answer the question as asked,  for people googling.&#xD;&#xA;In python:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    #contigs.tsv contians chromosome names and lengths in two columns&#xD;&#xA;    for line in open(&quot;contigs.tsv&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;0&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;my_bed_file.bed&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;bed&quot;, &quot;interval&quot;, fields[1], fields[2], fields[4], fields[5], &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (fields[3], fields[0])&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4913" PostHistoryTypeId="5" PostId="2244" RevisionGUID="3827f543-b46e-4ff5-a6b6-010679adad78" CreationDate="2017-08-08T11:50:07.840" UserId="235" Comment="added 941 characters in body" Text="To answer the question as asked,  for people googling.&#xD;&#xA;For BED6, in python:&#xD;&#xA;&#xD;&#xA;    #contigs.tsv contians chromosome names and lengths in two columns&#xD;&#xA;    for line in open(&quot;contigs.tsv&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;0&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;my_bed_file.bed&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;bed&quot;, &quot;interval&quot;, fields[1], fields[2], fields[4], fields[5], &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (fields[3], fields[0])&#xD;&#xA;&#xD;&#xA;For bed12, in python:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    #contigs.tsv contians chromosome names and lengths in two columns&#xD;&#xA;    for line in open(&quot;contigs.tsv&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;0&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;my_bed12.bed&quot;):&#xD;&#xA;    &#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       contig = fields[0]&#xD;&#xA;       start= fields[1]&#xD;&#xA;       end=fields[2]&#xD;&#xA;       name=fields[3]&#xD;&#xA;       score=fields[4]&#xD;&#xA;       strand=fields[5]&#xD;&#xA;       print contig, &quot;bed&quot;, &quot;interval&quot;, start, end, score, strand, &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (name, contig)&#xD;&#xA;    &#xD;&#xA;       block_starts = map(int,fields[11].split(&quot;,&quot;))&#xD;&#xA;       block_sizes = map(int, fields[10].split(&quot;,&quot;))&#xD;&#xA;       &#xD;&#xA;       for (block, (bstart, blen)) in enumerate(zip(block_starts, block_sizes)):&#xD;&#xA;          bend = start + bstart + blen&#xD;&#xA;          print contig, &quot;bed&quot;, &quot;block&quot;, str(start + bstart), str(bend), score, strand, &quot;.&quot;, &quot;ID=%s_%i;parent=%s&quot; %(name, block, name)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4914" PostHistoryTypeId="5" PostId="2244" RevisionGUID="d45c87e2-e176-467a-a154-2eaad0ca2bda" CreationDate="2017-08-08T11:52:28.133" UserId="77" Comment="added 58 characters in body" Text="To answer the question as asked,  for people googling.&#xD;&#xA;For BED6, in python:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    #contigs.tsv contians chromosome names and lengths in two columns&#xD;&#xA;    for line in open(&quot;contigs.tsv&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;0&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;my_bed_file.bed&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;bed&quot;, &quot;interval&quot;, fields[1], fields[2], fields[4], fields[5], &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (fields[3], fields[0])&#xD;&#xA;&#xD;&#xA;For bed12, in python:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    #contigs.tsv contians chromosome names and lengths in two columns&#xD;&#xA;    for line in open(&quot;contigs.tsv&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;0&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;my_bed12.bed&quot;):&#xD;&#xA;    &#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       contig = fields[0]&#xD;&#xA;       start= fields[1]&#xD;&#xA;       end=fields[2]&#xD;&#xA;       name=fields[3]&#xD;&#xA;       score=fields[4]&#xD;&#xA;       strand=fields[5]&#xD;&#xA;       print contig, &quot;bed&quot;, &quot;interval&quot;, start, end, score, strand, &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (name, contig)&#xD;&#xA;    &#xD;&#xA;       block_starts = map(int,fields[11].split(&quot;,&quot;))&#xD;&#xA;       block_sizes = map(int, fields[10].split(&quot;,&quot;))&#xD;&#xA;       &#xD;&#xA;       for (block, (bstart, blen)) in enumerate(zip(block_starts, block_sizes)):&#xD;&#xA;          bend = start + bstart + blen&#xD;&#xA;          print contig, &quot;bed&quot;, &quot;block&quot;, str(start + bstart), str(bend), score, strand, &quot;.&quot;, &quot;ID=%s_%i;parent=%s&quot; %(name, block, name)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4915" PostHistoryTypeId="2" PostId="2245" RevisionGUID="e9404dc4-7506-46c1-8c33-4ffd9ed134c4" CreationDate="2017-08-08T12:00:46.733" UserId="235" Text="To convert BAM to GTF, which is the best way to get a file to compare with cuffcompare:&#xD;&#xA;&#xD;&#xA;    import pysam&#xD;&#xA;    &#xD;&#xA;    bamfile=pysam.AlignmentFile(&quot;my_bam_file.bam&quot;)&#xD;&#xA;    &#xD;&#xA;    for alignment in bamfile.fetch():&#xD;&#xA;    &#xD;&#xA;        contig = bamfile.get_reference_name(alignment.reference_id)&#xD;&#xA;        name = alignment.query_name&#xD;&#xA;        &#xD;&#xA;        if alignment.is_reverse:&#xD;&#xA;            strand = &quot;-&quot;&#xD;&#xA;        else: &#xD;&#xA;            strand = &quot;+&quot;&#xD;&#xA;    &#xD;&#xA;        for start, end in alignment.getblocks():&#xD;&#xA;            print contig, &quot;BAM&quot;, &quot;exon&quot;, str(start), str(end), &quot;0&quot;, strand, &quot;.&quot;, 'gene_id &quot;%s&quot;; transcript_id &quot;%s&quot;;' % (name, name)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4916" PostHistoryTypeId="5" PostId="2245" RevisionGUID="48b7747f-bd77-424c-81c4-5df95ea77e73" CreationDate="2017-08-08T12:01:48.580" UserId="77" Comment="added 30 characters in body" Text="To convert BAM to GTF, which is the best way to get a file to compare with cuffcompare:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    import pysam&#xD;&#xA;    &#xD;&#xA;    bamfile=pysam.AlignmentFile(&quot;my_bam_file.bam&quot;)&#xD;&#xA;    &#xD;&#xA;    for alignment in bamfile.fetch():&#xD;&#xA;    &#xD;&#xA;        contig = bamfile.get_reference_name(alignment.reference_id)&#xD;&#xA;        name = alignment.query_name&#xD;&#xA;        &#xD;&#xA;        if alignment.is_reverse:&#xD;&#xA;            strand = &quot;-&quot;&#xD;&#xA;        else: &#xD;&#xA;            strand = &quot;+&quot;&#xD;&#xA;    &#xD;&#xA;        for start, end in alignment.getblocks():&#xD;&#xA;            print contig, &quot;BAM&quot;, &quot;exon&quot;, str(start), str(end), &quot;0&quot;, strand, &quot;.&quot;, 'gene_id &quot;%s&quot;; transcript_id &quot;%s&quot;;' % (name, name)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4917" PostHistoryTypeId="5" PostId="2245" RevisionGUID="16ca4458-e540-4470-8b54-3ff069d22375" CreationDate="2017-08-08T12:03:14.363" UserId="235" Comment="Fix differing co-ord systems - this is why one whould use a BED/GTF  module!!!" Text="To convert BAM to GTF, which is the best way to get a file to compare with cuffcompare:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    import pysam&#xD;&#xA;    &#xD;&#xA;    bamfile=pysam.AlignmentFile(&quot;my_bam_file.bam&quot;)&#xD;&#xA;    &#xD;&#xA;    for alignment in bamfile.fetch():&#xD;&#xA;    &#xD;&#xA;        contig = bamfile.get_reference_name(alignment.reference_id)&#xD;&#xA;        name = alignment.query_name&#xD;&#xA;        &#xD;&#xA;        if alignment.is_reverse:&#xD;&#xA;            strand = &quot;-&quot;&#xD;&#xA;        else: &#xD;&#xA;            strand = &quot;+&quot;&#xD;&#xA;    &#xD;&#xA;        for start, end in alignment.getblocks():&#xD;&#xA;            # note: BED is 0-based, half-open, GFF is 1-based, closed&#xD;&#xA;            print contig, &quot;BAM&quot;, &quot;exon&quot;, str(start + 1), str(end), &quot;0&quot;, strand, &quot;.&quot;, 'gene_id &quot;%s&quot;; transcript_id &quot;%s&quot;;' % (name, name)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;NB, this will also work with a SAM file as long as it is headered." />
  <row Id="4918" PostHistoryTypeId="5" PostId="2244" RevisionGUID="d81f02fc-173e-4755-946e-fb4a4a1b389a" CreationDate="2017-08-08T12:06:37.843" UserId="235" Comment="More 1-based shenanigans. I mean really, who makes their format 1-based!" Text="To answer the question as asked,  for people googling.&#xD;&#xA;For BED6, in python:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    #contigs.tsv contians chromosome names and lengths in two columns&#xD;&#xA;    for line in open(&quot;contigs.tsv&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;1&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;my_bed_file.bed&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;&#xD;&#xA;       # note: BED is 0-based, half-open, GFF is 1-based, closed&#xD;&#xA;       start = str(int(fields[1]) + 1)&#xD;&#xA;       print fields[0], &quot;bed&quot;, &quot;interval&quot;, fields[1], fields[2], fields[4], fields[5], &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (fields[3], fields[0])&#xD;&#xA;&#xD;&#xA;For bed12, in python:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    #contigs.tsv contians chromosome names and lengths in two columns&#xD;&#xA;    for line in open(&quot;contigs.tsv&quot;):&#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;1&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xD;&#xA;    &#xD;&#xA;    for line in open(&quot;my_bed12.bed&quot;):&#xD;&#xA;    &#xD;&#xA;       fields = line.strip().split(&quot;\t&quot;)&#xD;&#xA;       contig = fields[0]&#xD;&#xA;       # note: BED is 0-based, half-open, GFF is 1-based, closed&#xD;&#xA;       start= int(fields[1]) + 1)&#xD;&#xA;       end=fields[2]&#xD;&#xA;       name=fields[3]&#xD;&#xA;       score=fields[4]&#xD;&#xA;       strand=fields[5]&#xD;&#xA;       print contig, &quot;bed&quot;, &quot;interval&quot;, str(start), end, score, strand, &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (name, contig)&#xD;&#xA;    &#xD;&#xA;       block_starts = map(int,fields[11].split(&quot;,&quot;))&#xD;&#xA;       block_sizes = map(int, fields[10].split(&quot;,&quot;))&#xD;&#xA;       &#xD;&#xA;       for (block, (bstart, blen)) in enumerate(zip(block_starts, block_sizes)):&#xD;&#xA;          bend = start + bstart + blen&#xD;&#xA;          print contig, &quot;bed&quot;, &quot;block&quot;, str(start + bstart), str(bend), score, strand, &quot;.&quot;, &quot;ID=%s_%i;parent=%s&quot; %(name, block, name)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4919" PostHistoryTypeId="5" PostId="2242" RevisionGUID="6a646706-584d-42e3-abd2-0c89b8d840c9" CreationDate="2017-08-08T12:40:39.843" UserId="294" Comment="added 51 characters in body" Text="I would like to convert a [`BED`][1] format to [`GFF3`][2].&#xD;&#xA;&#xD;&#xA;The only useful tool that I could find via a [google search][3] seems to be [Galaxy][4], and I do not feel very comfortable with online tools, plus the webserver is currenlty under maintenance. &#xD;&#xA;&#xD;&#xA;Does anyone knows about a command-line tool that can handle this conversion?&#xD;&#xA;&#xD;&#xA;Edit: here are some lines of my BED file: &#xD;&#xA;&#xD;&#xA;    $ head -4 last_minion-r7_sort.bed&#xD;&#xA;    211000022278137	175	211	8e5d0959-7cdb-49cf-9298-94ed3b2aedb5_Basecall_2D_000_2d	42	+&#xD;&#xA;    211000022279134	0	503	e8a9c6b8-bad2-4a7e-97d8-ca4acb34ff70_Basecall_2D_000_2d	69	-&#xD;&#xA;    211000022279134	24	353	e258783d-95a3-41f5-9ad5-bb12311dbaf4_Basecall_2D_000_2d	45	-&#xD;&#xA;    211000022279134	114	429	26601afb-581a-41df-b42b-b366148ea06f_Basecall_2D_000_2d	100	-&#xD;&#xA;&#xD;&#xA;The bed file thus has 6 columns as for now: chromosome, start coordinate, end coordinate, read name, score, strand. This file was obtained from conversion of `MAF` format (as output of alignment of RNA-seq reads to reference genome, using [`LAST`][5]) converted to `SAM` using [`maf-convert`][6], then to `BAM` using [`samtools`][7], finally to `BED` using [`bedtools`][8]. &#xD;&#xA;&#xD;&#xA;The aim of my conversion is basically to convert `SAM` -&gt; `GTF`, for [post-processing][9]. Since there is no straightforward way to do this, I am going through steps, the only way to do this in my knowledge is : `SAM` -&gt; `BAM` -&gt; `BED` -&gt; `GFF3` -&gt; `GTF` but for now I am stuck in the `BED` -&gt; `GFF3` part. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://genome.ucsc.edu/FAQ/FAQformat#format1&#xD;&#xA;  [2]: http://www.ensembl.org/info/website/upload/gff3.html&#xD;&#xA;  [3]: https://www.google.ch/search?q=bed%20to%20gff3&#xD;&#xA;  [4]: https://galaxy.inf.ethz.ch/&#xD;&#xA;  [5]: http://last.cbrc.jp/&#xD;&#xA;  [6]: http://last.cbrc.jp/doc/maf-convert.html&#xD;&#xA;  [7]: http://samtools.sourceforge.net/&#xD;&#xA;  [8]: http://bedtools.readthedocs.io/en/latest/content/bedtools-suite.html&#xD;&#xA;  [9]: https://bioinformatics.stackexchange.com/questions/2036/tools-to-reconcile-experimental-transcripts-with-reference-annotation" />
  <row Id="4920" PostHistoryTypeId="5" PostId="2240" RevisionGUID="5781b0f8-f1b3-45c2-b730-a5c70ddad80c" CreationDate="2017-08-08T13:44:28.567" UserId="60" Comment="added 170 characters in body" Text="Just for reference, there is a not very in depth, but first-hand reasoning (Jim Kent) given here about why bedGraphToBigWig does not support streaming http://genome.soe.ucsc.narkive.com/2S1Z3VpG/bedgraphtobigwig-reading-in-from-stdin&#xD;&#xA;&#xD;&#xA;Edit: as noted wigToBigWig allows bedgraph streaming input via stdin but takes more memory than bedGraphToBigWig itself does&#xD;&#xA;&#xD;&#xA;    cat file.bedgraph | wigToBigWig stdin chrom.sizes output.bigwig" />
  <row Id="4921" PostHistoryTypeId="2" PostId="2246" RevisionGUID="04065f7a-f310-485c-a4ef-2833078d196c" CreationDate="2017-08-08T14:47:08.813" UserId="1286" Text="I'm currently developing a program that determines protein structure, and I'm in need of creating a kind of &quot;profile&quot; for each amino acid. I've tried searching over the past several months (here and there), but haven't been able to find any such library (the programming language doesn't matter). &#xD;&#xA;&#xD;&#xA;I'm specifically looking for something along the lines of an adjacency (bond) matrix for each atom within an amino acid, and in particular which atom within the amino acid attaches to the protein backbone." />
  <row Id="4922" PostHistoryTypeId="1" PostId="2246" RevisionGUID="04065f7a-f310-485c-a4ef-2833078d196c" CreationDate="2017-08-08T14:47:08.813" UserId="1286" Text="Looking for an amino acid library" />
  <row Id="4923" PostHistoryTypeId="3" PostId="2246" RevisionGUID="04065f7a-f310-485c-a4ef-2833078d196c" CreationDate="2017-08-08T14:47:08.813" UserId="1286" Text="&lt;protein-structure&gt;&lt;public-databases&gt;" />
  <row Id="4924" PostHistoryTypeId="2" PostId="2247" RevisionGUID="978ec7b0-fca7-428f-9f62-7fe0263edfa1" CreationDate="2017-08-08T15:45:48.750" UserId="48" Text="The amino acids attach to the protein backbone thorough the [peptide bond][1]. The carbon which links with other amino acids receive the name of C'.&#xD;&#xA;&#xD;&#xA;The bonds between atoms (of amino acids) are flexible, so the distance and the position (due to flexible angles between bonds) changes in each structure. Using PDB you can find the distances of the amino acids in known structures. Computing the distance between each atom of each aminoacid could be done with [Biopython][2]. You can [read the PDB][3] files and calculate the distance between atoms. Specifically you can look [here][4] for &quot;How do I measure distances?&quot;. Here I reproduce the example:&#xD;&#xA;&#xD;&#xA;&gt;     # Get some atoms&#xD;&#xA;&gt;     ca1 = residue1['CA']&#xD;&#xA;&gt;     ca2 = residue2['CA']&#xD;&#xA;&gt;     # Simply subtract the atoms to get their distance&#xD;&#xA;&gt;     distance = ca1 - ca2&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Peptide_bond&#xD;&#xA;  [2]: http://biopython.org&#xD;&#xA;  [3]: http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc151&#xD;&#xA;  [4]: http://biopython.org/wiki/The_Biopython_Structural_Bioinformatics_FAQ&#xD;&#xA;" />
  <row Id="4925" PostHistoryTypeId="5" PostId="2246" RevisionGUID="ad251d81-6341-427b-bd70-232cefdbe450" CreationDate="2017-08-08T15:47:18.567" UserId="1286" Comment="added 286 characters in body" Text="I'm currently developing a program that determines protein structure, and I'm in need of creating a kind of &quot;profile&quot; for each amino acid. I've tried searching over the past several months (here and there), but haven't been able to find any such library (the programming language doesn't matter). &#xD;&#xA;&#xD;&#xA;I'm specifically looking for something along the lines of an adjacency (bond) matrix for each atom within an amino acid, and in particular which atom within the amino acid attaches to the protein backbone.&#xD;&#xA;&#xD;&#xA;**EDIT**: I should also specify that in the adjacency (bond) matrix, I am only concerned with T/F values. Which is to say, I don't care for the bond length to be stored in the matrix, just the information of if two atoms are connected by a bond, denoted with a TRUE or FALSE value. " />
  <row Id="4929" PostHistoryTypeId="5" PostId="2246" RevisionGUID="978c732c-d1c2-4477-a441-3978a81e6024" CreationDate="2017-08-08T15:59:30.580" UserId="1286" Comment="added 392 characters in body" Text="I'm currently developing a program that determines protein structure, and I'm in need of creating a kind of &quot;profile&quot; for each amino acid. I've tried searching over the past several months (here and there), but haven't been able to find any such library (the programming language doesn't matter). &#xD;&#xA;&#xD;&#xA;I'm specifically looking for something along the lines of an adjacency (bond) matrix for each atom within an amino acid, and in particular which atom within the amino acid attaches to the protein backbone.&#xD;&#xA;&#xD;&#xA;**EDIT**: I should also specify that in the adjacency (bond) matrix, I am only concerned with T/F values. Which is to say, I don't care for the bond length to be stored in the matrix, just the information of if two atoms are connected by a bond, denoted with a TRUE or FALSE value. &#xD;&#xA;&#xD;&#xA;**EDIT**: If you're interested in seeing some details on how my algorithm works, I'm basing a great deal of my calculatory procedures on the research done by [Carlile Lavor][1]. See [here][2] for the publication I frequently reference.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scholar.google.com/citations?user=xu3AWpAAAAAJ&amp;hl=en&#xD;&#xA;  [2]: http://onlinelibrary.wiley.com/doi/10.1111/j.1475-3995.2007.00622.x/full" />
  <row Id="4930" PostHistoryTypeId="5" PostId="2246" RevisionGUID="80e247a7-93bd-4587-8dc3-47b47ddbf215" CreationDate="2017-08-08T16:00:09.153" UserId="298" Comment="Please don't add &quot;Edit&quot; to your questions, just edit them so that anyone who reads them for the first time can understand them." Text="I'm currently developing a program that determines protein structure, and I'm in need of creating a kind of &quot;profile&quot; for each amino acid. I've tried searching over the past several months (here and there), but haven't been able to find any such library (the programming language doesn't matter). &#xD;&#xA;&#xD;&#xA;I'm specifically looking for something along the lines of an adjacency (bond) matrix for each atom within an amino acid, and in particular which atom within the amino acid attaches to the protein backbone.&#xD;&#xA;&#xD;&#xA;I should also specify that in the adjacency (bond) matrix, I am only concerned with T/F values. Which is to say, I don't care for the bond length to be stored in the matrix, just the information of if two atoms are connected by a bond, denoted with a TRUE or FALSE value. &#xD;&#xA;&#xD;&#xA; If you're interested in seeing some details on how my algorithm works, I'm basing a great deal of my calculatory procedures on the research done by [Carlile Lavor][1]. See [here][2] for the publication I frequently reference.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scholar.google.com/citations?user=xu3AWpAAAAAJ&amp;hl=en&#xD;&#xA;  [2]: http://onlinelibrary.wiley.com/doi/10.1111/j.1475-3995.2007.00622.x/full" />
  <row Id="4938" PostHistoryTypeId="2" PostId="2250" RevisionGUID="a7ef29bf-528e-450d-9ba7-170ace63df87" CreationDate="2017-08-08T20:27:37.457" UserId="34" Text="Galaxy has API and API-consuming libraries (such as BioBlend) that will allow you to interactively script against it without opening the graphical interface at all.&#xD;&#xA;&#xD;&#xA;However you can also take almost any tool out of Galaxy and use it independently since everything is open source. The converter you mentioned is available as a Python script [here](https://github.com/galaxyproject/galaxy/blob/dev/tools/filters/bed_to_gff_converter.py) and the tool 'wrapper' which you can use to understand how to invoke the python script is [next to it](https://github.com/galaxyproject/galaxy/blob/dev/tools/filters/bed2gff.xml)." />
  <row Id="4939" PostHistoryTypeId="5" PostId="2238" RevisionGUID="91b2bd7a-541a-4985-aec1-89709a892a2e" CreationDate="2017-08-09T12:18:26.183" UserId="292" Comment="added 164 characters in body" Text="## A (bio)awk-based solution&#xD;&#xA;&#xD;&#xA;### Generate the list of genus&#xD;&#xA;&#xD;&#xA;    bioawk -c fastx '{print $comment}' terminase_large.fasta \&#xD;&#xA;        | sed -r 's/.*\[(\w+).*/\1/' \&#xD;&#xA;        | sort -u &gt; genus_names.txt&#xD;&#xA;&#xD;&#xA;Here is how it works:&#xD;&#xA;&#xD;&#xA;1. [`bioawk`](https://github.com/lh3/bioawk) is like awk but with extra parsing capabilities. `-c fastx` parses the fields in fasta or fastq records. `{print $comment}` will print the `$comment` field which contain the genus names you want to extract.&#xD;&#xA;&#xD;&#xA;2. The `sed` command captures (with parentheses) the word (`\w+`) that is just after the opening square bracket (which should be the genus name, if there is only one opening square bracket on the line), and substitutes the whole line (that is, everything before the bracket (`.*`), the bracket (`\[`), the word, and everything after the word (`.*` again)) with just the captured word (`\1`).&#xD;&#xA;&#xD;&#xA;3. All this is sorted, and the `-u` option of `sort` ensures there is only one occurrence of each genus name in the output, which we put in the `genus_names.txt` file.&#xD;&#xA;&#xD;&#xA;### Extract the fasta records for each genus&#xD;&#xA;&#xD;&#xA;    for genus in $(cat genus_names.txt)&#xD;&#xA;    do&#xD;&#xA;        bioawk -c fastx '{print}' terminase_large.fasta \&#xD;&#xA;            | grep ${genus} \&#xD;&#xA;            | awk -F &quot;\t&quot; '{print &quot;&gt;&quot;$4&quot;\n&quot;$2}' \&#xD;&#xA;            | sed -r 's/^&gt;[^\[]+\[(.*)\]/&gt;\1/' &gt; ${genus}.fasta&#xD;&#xA;    done&#xD;&#xA;&#xD;&#xA;Here is how it works:&#xD;&#xA;&#xD;&#xA;1. We loop on the content of the previously established list of genus names.&#xD;&#xA;&#xD;&#xA;2. For each value of `${genus}`, we parse again the fasta file with `bioawk`. This time, for a given fasta record, we print all the elements that have been parsed. They appear on one line, in the following order (according to `bioawk -c help`): `1:name 2:seq 3:qual 4:comment` (we actually only need `seq` and `comment` for the following steps, but there are less risks of making errors at the later `awk` step if we keep the four fields).&#xD;&#xA;&#xD;&#xA;3. We use `grep` to select the resulting lines that contain the genus name we're currently dealing with.&#xD;&#xA;&#xD;&#xA;4. The lines that have been successfully selected are then parsed by awk and reformatted into fasta format using only the comment part in the header. The `-F &quot;\t&quot;` is to indicate that the field delimiters are tabulations (which is how bioawk has written the records when we did `{print}`).&#xD;&#xA;&#xD;&#xA;5. We use `sed` to keep only the part inside the square brackets (again using a pattern capture between parentheses, that we use in the substitution part with `\1`), and the output is written in a file named using the current value of `${genus}`.&#xD;&#xA;&#xD;&#xA;More details on the last `sed` command:&#xD;&#xA;&#xD;&#xA;We match the whole header lines. They start with &quot;&gt;&quot; (`^&gt;`), then there are some non-&quot;[&quot; characters (`[^\[]+`), then a &quot;[&quot; (`\[`), then something we capture (`(.*)`), then a &quot;]&quot; (`\]`). We replace them with the captured part. The sequence lines are unaffected because they don't match the above regular expression." />
  <row Id="4940" PostHistoryTypeId="50" PostId="627" RevisionGUID="3e98e6f8-f31e-4268-85ed-f27348bf2a52" CreationDate="2017-08-09T14:17:51.377" UserId="-1" />
  <row Id="4941" PostHistoryTypeId="2" PostId="2251" RevisionGUID="cf616a3c-39de-4d64-9895-08328ffc1e44" CreationDate="2017-08-09T15:32:05.220" UserId="1075" Text="I am trying to install chromosomer (https://github.com/gtamazian/chromosomer) but I fail. Can anybody help me, please?&#xD;&#xA;&#xD;&#xA;&gt; pip install chromosomer&#xD;&#xA;&#xD;&#xA;Collecting chromosomer&#xD;&#xA;  Could not find a version that satisfies the requirement chromosomer (from versions: )&#xD;&#xA;No matching distribution found for chromosomer&#xD;&#xA;&#xD;&#xA;The output of 'pip' and 'python':&#xD;&#xA;&gt; pip&#xD;&#xA;&#xD;&#xA;Usage:   &#xD;&#xA;  pip &lt;command&gt; [options]&#xD;&#xA;&#xD;&#xA;Commands:&#xD;&#xA;  install                     Install packages.&#xD;&#xA;  download                    Download packages.&#xD;&#xA;  uninstall                   Uninstall packages.&#xD;&#xA;  freeze                      Output installed packages in requirements format.&#xD;&#xA;  list                        List installed packages.&#xD;&#xA;  show                        Show information about installed packages.&#xD;&#xA;  check                       Verify installed packages have compatible dependencies.&#xD;&#xA;  search                      Search PyPI for packages.&#xD;&#xA;  wheel                       Build wheels from your requirements.&#xD;&#xA;  hash                        Compute hashes of package archives.&#xD;&#xA;  completion                  A helper command used for command completion.&#xD;&#xA;  help                        Show help for commands.&#xD;&#xA;&#xD;&#xA;General Options:&#xD;&#xA;  -h, --help                  Show help.&#xD;&#xA;  --isolated                  Run pip in an isolated mode, ignoring&#xD;&#xA;                              environment variables and user configuration.&#xD;&#xA;  -v, --verbose               Give more output. Option is additive, and can&#xD;&#xA;                              be used up to 3 times.&#xD;&#xA;  -V, --version               Show version and exit.&#xD;&#xA;  -q, --quiet                 Give less output. Option is additive, and can&#xD;&#xA;                              be used up to 3 times (corresponding to&#xD;&#xA;                              WARNING, ERROR, and CRITICAL logging levels).&#xD;&#xA;  --log &lt;path&gt;                Path to a verbose appending log.&#xD;&#xA;  --proxy &lt;proxy&gt;             Specify a proxy in the form&#xD;&#xA;                              [user:passwd@]proxy.server:port.&#xD;&#xA;  --retries &lt;retries&gt;         Maximum number of retries each connection&#xD;&#xA;                              should attempt (default 5 times).&#xD;&#xA;  --timeout &lt;sec&gt;             Set the socket timeout (default 15 seconds).&#xD;&#xA;  --exists-action &lt;action&gt;    Default action when a path already exists:&#xD;&#xA;                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.&#xD;&#xA;  --trusted-host &lt;hostname&gt;   Mark this host as trusted, even though it does&#xD;&#xA;                              not have valid or any HTTPS.&#xD;&#xA;  --cert &lt;path&gt;               Path to alternate CA bundle.&#xD;&#xA;  --client-cert &lt;path&gt;        Path to SSL client certificate, a single file&#xD;&#xA;                              containing the private key and the certificate&#xD;&#xA;                              in PEM format.&#xD;&#xA;  --cache-dir &lt;dir&gt;           Store the cache data in &lt;dir&gt;.&#xD;&#xA;  --no-cache-dir              Disable the cache.&#xD;&#xA;  --disable-pip-version-check&#xD;&#xA;                              Don't periodically check PyPI to determine&#xD;&#xA;                              whether a new version of pip is available for&#xD;&#xA;                              download. Implied with --no-index.&#xD;&#xA;&gt; python&#xD;&#xA;&#xD;&#xA;Python 3.6.0 |Anaconda custom (x86_64)| (default, Dec 23 2016, 13:19:00) &#xD;&#xA;[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin&#xD;&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xD;&#xA;&#xD;&#xA;Thanks. Best, C." />
  <row Id="4942" PostHistoryTypeId="1" PostId="2251" RevisionGUID="cf616a3c-39de-4d64-9895-08328ffc1e44" CreationDate="2017-08-09T15:32:05.220" UserId="1075" Text="Cannot install chromosomer" />
  <row Id="4943" PostHistoryTypeId="3" PostId="2251" RevisionGUID="cf616a3c-39de-4d64-9895-08328ffc1e44" CreationDate="2017-08-09T15:32:05.220" UserId="1075" Text="&lt;scaffold&gt;" />
  <row Id="4944" PostHistoryTypeId="2" PostId="2252" RevisionGUID="4b605bb2-e585-49bb-b1a4-55b80efd3d52" CreationDate="2017-08-09T15:56:32.420" UserId="383" Text="Bioconductor makes this so easy. It does the coordinate conversion on import.&#xD;&#xA;&#xD;&#xA;    library(rtracklayer)&#xD;&#xA;    &#xD;&#xA;    ## import the bed file&#xD;&#xA;    bed.ranges &lt;- import.bed('regions.bed')&#xD;&#xA;    &#xD;&#xA;    ## export as a gff3 file&#xD;&#xA;    export.gff3(bed.ranges,'regions.gff3')&#xD;&#xA;&#xD;&#xA;And people wonder why R is so popular for bioinformatics...&#xD;&#xA;&#xD;&#xA;Also if needed, you could go straight from **BAM** file to **gff3** in R as well." />
  <row Id="4945" PostHistoryTypeId="5" PostId="2240" RevisionGUID="f944d53d-f0df-48f2-83d3-675756276869" CreationDate="2017-08-09T16:13:38.660" UserId="60" Comment="added 104 characters in body" Text="Just for reference, there is a not very in depth, but first-hand reasoning (Jim Kent) given here about why bedGraphToBigWig does not support streaming http://genome.soe.ucsc.narkive.com/2S1Z3VpG/bedgraphtobigwig-reading-in-from-stdin&#xD;&#xA;&#xD;&#xA;Edit: as noted wigToBigWig allows bedgraph streaming input via stdin but takes (possibly much) more memory than bedGraphToBigWig itself does&#xD;&#xA;&#xD;&#xA;    cat file.bedgraph | wigToBigWig stdin chrom.sizes output.bigwig&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Note: You can also use `wiggletools bin 10` for example to bin rows and downsample" />
  <row Id="4946" PostHistoryTypeId="5" PostId="2251" RevisionGUID="81436b2a-59f1-483a-8092-b80ed49805cb" CreationDate="2017-08-09T16:19:13.747" UserId="29" Comment="formatting, remove greetings" Text="I am trying to install [chromosomer](https://github.com/gtamazian/chromosomer) but I fail. Can anybody help me, please?&#xD;&#xA;&#xD;&#xA;&gt; pip install chromosomer&#xD;&#xA;&#xD;&#xA;Collecting chromosomer&#xD;&#xA;  Could not find a version that satisfies the requirement chromosomer (from versions: )&#xD;&#xA;No matching distribution found for chromosomer&#xD;&#xA;&#xD;&#xA;The output of 'pip' and 'python':&#xD;&#xA;&#xD;&#xA;    &gt; pip&#xD;&#xA;    &#xD;&#xA;    Usage:   &#xD;&#xA;      pip &lt;command&gt; [options]&#xD;&#xA;    &#xD;&#xA;    Commands:&#xD;&#xA;      install                     Install packages.&#xD;&#xA;      download                    Download packages.&#xD;&#xA;      uninstall                   Uninstall packages.&#xD;&#xA;      freeze                      Output installed packages in requirements format.&#xD;&#xA;      list                        List installed packages.&#xD;&#xA;      show                        Show information about installed packages.&#xD;&#xA;      check                       Verify installed packages have compatible dependencies.&#xD;&#xA;      search                      Search PyPI for packages.&#xD;&#xA;      wheel                       Build wheels from your requirements.&#xD;&#xA;      hash                        Compute hashes of package archives.&#xD;&#xA;      completion                  A helper command used for command completion.&#xD;&#xA;      help                        Show help for commands.&#xD;&#xA;    &#xD;&#xA;    General Options:&#xD;&#xA;      -h, --help                  Show help.&#xD;&#xA;      --isolated                  Run pip in an isolated mode, ignoring&#xD;&#xA;                                  environment variables and user configuration.&#xD;&#xA;      -v, --verbose               Give more output. Option is additive, and can&#xD;&#xA;                                  be used up to 3 times.&#xD;&#xA;      -V, --version               Show version and exit.&#xD;&#xA;      -q, --quiet                 Give less output. Option is additive, and can&#xD;&#xA;                                  be used up to 3 times (corresponding to&#xD;&#xA;                                  WARNING, ERROR, and CRITICAL logging levels).&#xD;&#xA;      --log &lt;path&gt;                Path to a verbose appending log.&#xD;&#xA;      --proxy &lt;proxy&gt;             Specify a proxy in the form&#xD;&#xA;                                  [user:passwd@]proxy.server:port.&#xD;&#xA;      --retries &lt;retries&gt;         Maximum number of retries each connection&#xD;&#xA;                                  should attempt (default 5 times).&#xD;&#xA;      --timeout &lt;sec&gt;             Set the socket timeout (default 15 seconds).&#xD;&#xA;      --exists-action &lt;action&gt;    Default action when a path already exists:&#xD;&#xA;                                  (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.&#xD;&#xA;      --trusted-host &lt;hostname&gt;   Mark this host as trusted, even though it does&#xD;&#xA;                                  not have valid or any HTTPS.&#xD;&#xA;      --cert &lt;path&gt;               Path to alternate CA bundle.&#xD;&#xA;      --client-cert &lt;path&gt;        Path to SSL client certificate, a single file&#xD;&#xA;                                  containing the private key and the certificate&#xD;&#xA;                                  in PEM format.&#xD;&#xA;      --cache-dir &lt;dir&gt;           Store the cache data in &lt;dir&gt;.&#xD;&#xA;      --no-cache-dir              Disable the cache.&#xD;&#xA;      --disable-pip-version-check&#xD;&#xA;                                  Don't periodically check PyPI to determine&#xD;&#xA;                                  whether a new version of pip is available for&#xD;&#xA;                                  download. Implied with --no-index.&#xD;&#xA;    &gt; python&#xD;&#xA;    &#xD;&#xA;    Python 3.6.0 |Anaconda custom (x86_64)| (default, Dec 23 2016, 13:19:00) &#xD;&#xA;    [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin&#xD;&#xA;    Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information." />
  <row Id="4947" PostHistoryTypeId="2" PostId="2253" RevisionGUID="797e4955-b9b7-4017-95b6-76cba2aae3d5" CreationDate="2017-08-09T16:21:04.900" UserId="29" Text="Chromosomer only exists for Python 2. You should thus be able to install it via&#xD;&#xA;&#xD;&#xA;    pip2 install chromosomer&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;**But:**&#xD;&#xA;&#xD;&#xA;Your Python installation is a bit screwed up: there are Python 2 and Python 3, which are unfortunately incompatible.&#xD;&#xA;&#xD;&#xA;On your system `python` and `pip` seem to be aliases for Python 3, which I’d strongly recommend against (lots of tools will break). Instead, they should alias Python 2. Python 3 should be invoked via `python3`/`pip3`." />
  <row Id="4948" PostHistoryTypeId="5" PostId="2251" RevisionGUID="3c8b8fb0-981f-4b0a-8362-1c5df455ff72" CreationDate="2017-08-09T16:23:24.010" UserId="29" Comment="remove" Text="I am trying to install [chromosomer](https://github.com/gtamazian/chromosomer) but I fail. Can anybody help me, please?&#xD;&#xA;&#xD;&#xA;&gt; pip install chromosomer&#xD;&#xA;&#xD;&#xA;Collecting chromosomer&#xD;&#xA;  Could not find a version that satisfies the requirement chromosomer (from versions: )&#xD;&#xA;No matching distribution found for chromosomer&#xD;&#xA;&#xD;&#xA;The output 'python':&#xD;&#xA;&#xD;&#xA;    &gt; python&#xD;&#xA;    &#xD;&#xA;    Python 3.6.0 |Anaconda custom (x86_64)| (default, Dec 23 2016, 13:19:00) &#xD;&#xA;    [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin&#xD;&#xA;    Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information." />
  <row Id="4949" PostHistoryTypeId="5" PostId="2251" RevisionGUID="bb5535c5-2a24-48df-8426-2f67c8356540" CreationDate="2017-08-09T16:39:29.187" UserId="298" Comment="added 24 characters in body; edited tags" Text="I am trying to install [chromosomer](https://github.com/gtamazian/chromosomer) but I fail. Can anybody help me, please?&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    $ pip install chromosomer&#xD;&#xA;    &#xD;&#xA;    Collecting chromosomer&#xD;&#xA;      Could not find a version that satisfies the requirement chromosomer (from versions: )&#xD;&#xA;    No matching distribution found for chromosomer&#xD;&#xA;    &#xD;&#xA;The output `python`:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;    &#xD;&#xA;    $ python&#xD;&#xA;    &#xD;&#xA;    Python 3.6.0 |Anaconda custom (x86_64)| (default, Dec 23 2016, 13:19:00) &#xD;&#xA;    [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin&#xD;&#xA;    Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information." />
  <row Id="4950" PostHistoryTypeId="6" PostId="2251" RevisionGUID="bb5535c5-2a24-48df-8426-2f67c8356540" CreationDate="2017-08-09T16:39:29.187" UserId="298" Comment="added 24 characters in body; edited tags" Text="&lt;python&gt;&lt;software-installation&gt;" />
  <row Id="4951" PostHistoryTypeId="2" PostId="2254" RevisionGUID="2de34676-1747-431e-a313-d9fe4f418014" CreationDate="2017-08-09T20:19:15.707" UserId="1302" Text="Here is how I was able to export to a standard gzipped FASTQ file using Biopython. Basically, instead of using `SeqIO.write`, I directly called the `.format` method of the SeqRecord object. The example code below imports a gzipped FASTQ file, removes reads that do not contain a G in positions 7, 8, and 9, and writes the results to a gzipped FASTQ file.&#xD;&#xA;&#xD;&#xA;    from Bio import SeqIO&#xD;&#xA;    import gzip&#xD;&#xA;&#xD;&#xA;    path_in = &quot;path/to/in.fastq.gz&quot;&#xD;&#xA;    path_out = &quot;path/to/out.fastq.gz&quot;&#xD;&#xA;    handle_in = gzip.open(path_in, &quot;rt&quot;)&#xD;&#xA;    handle_out = gzip.open(path_out, &quot;wt&quot;)&#xD;&#xA;&#xD;&#xA;    fq = SeqIO.parse(handle_in, &quot;fastq&quot;)&#xD;&#xA;    for read in fq:&#xD;&#xA;        # Only export reads that have a G in positions 7, 8,&#xD;&#xA;        # and 9&#xD;&#xA;        if read.seq[6:9] == &quot;GGG&quot;:&#xD;&#xA;            handle_out.write(read.format(&quot;fastq&quot;))&#xD;&#xA;&#xD;&#xA;    handle_in.close()&#xD;&#xA;    handle_out.close()&#xD;&#xA;&#xD;&#xA;Using the code example from the original question that converts a FASTA file to a FASTQ file, it would look something like the following:&#xD;&#xA;&#xD;&#xA;    fastq = gzip.open(&quot;path/to/out.fastq.gz&quot;, &quot;wt&quot;)&#xD;&#xA;    for record in SeqIO.parse(fasta, &quot;fasta&quot;):&#xD;&#xA;        fastq.write(record.format(&quot;fastq&quot;))&#xD;&#xA;    fastq.close()&#xD;&#xA;&#xD;&#xA;**Caveat:** This only works for Python 3. Python 3 differs from Python 2 in how it imports data from binary files ([source](http://python3porting.com/problems.html?highlight=decode#reading-from-files)). In fact, ideally you should be using Python 3.4 or greater since Biopython has compatibility issues with Python 3.3 ([source](https://github.com/biopython/biopython/pull/723))." />
  <row Id="4954" PostHistoryTypeId="2" PostId="2257" RevisionGUID="5b669e05-a445-439e-92c6-f1ce639bcf05" CreationDate="2017-08-09T23:26:19.593" UserId="727" Text="I'm running blast+ searches for a list of proteins using specific parameters.&#xD;&#xA;&#xD;&#xA;This is my search&#xD;&#xA;&#xD;&#xA;     blastp -query protein.fasta -remote -db refseq_protein -out &#xD;&#xA;    protein_refseq_blast.txt -outfmt '6 qseqid sseqid qstart qend length evalue' -&#xD;&#xA;    entrez_query 'txid2 [ORGN] or txid4751 [ORGN]' -evalue 200000 -max_target_seqs 10000&#xD;&#xA;&#xD;&#xA;For some proteins, this runs moderately quickly, ~5-10 mins. For other proteins, this command never stops running, even after an hour or so. I'm wondering if this variability in speed is due to any part of this choice in parameters, is simply variability in speed for the BLAST servers. Is it common for BLAST searches to take this long? Is there a way to run all of them in parallel using these parameters if this is the case?&#xD;&#xA;&#xD;&#xA;Thanks." />
  <row Id="4955" PostHistoryTypeId="1" PostId="2257" RevisionGUID="5b669e05-a445-439e-92c6-f1ce639bcf05" CreationDate="2017-08-09T23:26:19.593" UserId="727" Text="Major variability in speed of BLAST between proteins" />
  <row Id="4956" PostHistoryTypeId="3" PostId="2257" RevisionGUID="5b669e05-a445-439e-92c6-f1ce639bcf05" CreationDate="2017-08-09T23:26:19.593" UserId="727" Text="&lt;blast&gt;" />
  <row Id="4957" PostHistoryTypeId="50" PostId="604" RevisionGUID="9a6a058f-572b-49f6-a5f0-8c612a428116" CreationDate="2017-08-10T00:29:28.937" UserId="-1" />
  <row Id="4958" PostHistoryTypeId="2" PostId="2258" RevisionGUID="4feee92e-8326-491b-ae78-fd4fd8535a6b" CreationDate="2017-08-10T08:35:02.367" UserId="1148" Text="Try changing you `evalue` threshold to something like `0.001`. As I understand it, if you have a very high `evalue` such as yours, almost every hit will be considered `significant` and this might be responsible for longer search times. &#xD;&#xA;&#xD;&#xA;See the the section on E values [here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=FAQ" />
  <row Id="4959" PostHistoryTypeId="5" PostId="2258" RevisionGUID="c9ad6c0a-077e-449d-b442-957ad20ade5f" CreationDate="2017-08-10T08:41:57.097" UserId="1148" Comment="added 166 characters in body" Text="Try changing you `evalue` threshold to something like `0.001`.&#xD;&#xA;&#xD;&#xA;The `evalue` you have set is telling BLAST to consider sequences that are seen fewer than 200,000 times in the `refseq_protein` data base as significant. &#xD;&#xA;&#xD;&#xA;This means that almost every sequence in your query set will be considered as finding a match less than would be expected by chance, which I imagine is what's slowing done your search.&#xD;&#xA;&#xD;&#xA;See the the section on E values [here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=FAQ#expect" />
  <row Id="4960" PostHistoryTypeId="5" PostId="2258" RevisionGUID="251a13b6-20af-47d6-bb38-fe58c7d51215" CreationDate="2017-08-10T08:49:20.753" UserId="1148" Comment="deleted 5 characters in body" Text="Try changing you `evalue` threshold to something like `0.001`.&#xD;&#xA;&#xD;&#xA;The `evalue` you have set is telling BLAST to consider sequences that are seen fewer than 200,000 times in the `refseq_protein` data base as significant. &#xD;&#xA;&#xD;&#xA;This means that almost every sequence in your query set will be considered as finding a match less than would be expected by chance, which I imagine is what's slowing your search.&#xD;&#xA;&#xD;&#xA;See the the section on E values [here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=FAQ#expect" />
  <row Id="4961" PostHistoryTypeId="2" PostId="2259" RevisionGUID="fe26ef69-0fca-4775-a92d-624f335f2061" CreationDate="2017-08-10T08:52:33.817" UserId="982" Text="I'm looking to generate data based on the DNA composition of a region of my genomes (data is incomplete genomes from HiSeq runs). I'm looking for software which will give me GC content, GC skew, codon bias etc. I would like to use something which I can use with or plug into R. &#xD;&#xA;&#xD;&#xA;" />
  <row Id="4962" PostHistoryTypeId="1" PostId="2259" RevisionGUID="fe26ef69-0fca-4775-a92d-624f335f2061" CreationDate="2017-08-10T08:52:33.817" UserId="982" Text="Software recommendations - DNA composition" />
  <row Id="4963" PostHistoryTypeId="3" PostId="2259" RevisionGUID="fe26ef69-0fca-4775-a92d-624f335f2061" CreationDate="2017-08-10T08:52:33.817" UserId="982" Text="&lt;r&gt;&lt;software-recommendation&gt;&lt;dna&gt;" />
  <row Id="4964" PostHistoryTypeId="5" PostId="2258" RevisionGUID="8d16fabb-75d7-4ca2-8753-e1d7b4ddedc8" CreationDate="2017-08-10T09:17:59.260" UserId="298" Comment="Corrected wrong e-value description" Text="Try changing you `evalue` threshold to something like `0.001`.&#xD;&#xA;&#xD;&#xA;The `evalue` you have set is telling BLAST to report sequences which it would expect to find 200000 times in a database of this size purely by chance.&#xD;&#xA;&#xD;&#xA;This means that almost every sequence in your query set will be considered as finding a match less than would be expected by chance, which I imagine is what's slowing your search.&#xD;&#xA;&#xD;&#xA;See the the section on E values [here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=FAQ#expect" />
  <row Id="4965" PostHistoryTypeId="2" PostId="2260" RevisionGUID="2166c9f8-4f7e-49aa-b3d5-b6bf76bc07bf" CreationDate="2017-08-10T09:23:08.983" UserId="298" Text="The evalue associated with a blast HSP (High-scoring Segment Pair) is a measure of how often you would expect to find such an HSP in a database of this size by pure chance. So, setting the evalue to 200000 means you are interested in HSPs that would be expected to be found &lt;=200000 times by pure chance. In other words, absolutely everything. This evalue is pointless. &#xD;&#xA;&#xD;&#xA;Because your e-value is so high, BLAST will report absolutely anything and everything it finds, and the vast majority of your results will be meaningless and irrelevant. If you want a permissive e-value, you could use `1` or even `10` but 200000 is way too much. You usually only want negative e-values, cases where you wouldn't expect to find even one such HSP by chance. For highly dissimilar sequences, I would recommend you use 0.1 or something of that scale. &#xD;&#xA;&#xD;&#xA;Changing the evalue will speed things up considerably. That said, you should always expect different times for different queries since the time it takes to find hits will always depend on the number of hits that can be found. If your sequence can be aligned to many of the sequences in the database, it will take longer to fetch all results. " />
  <row Id="4966" PostHistoryTypeId="2" PostId="2261" RevisionGUID="8f586fad-8ef1-46a2-ab00-d1d74ba0d258" CreationDate="2017-08-10T09:58:50.333" UserId="939" Text="There are more R packages available that calculate GC content, for example Ape's `GC.content()` function.&#xD;&#xA;&#xD;&#xA;For codon usage bias, I think `cubfits` or `sscu` packages might be useful. " />
  <row Id="4967" PostHistoryTypeId="5" PostId="2258" RevisionGUID="6a3ddb1d-fc55-4033-b7fa-577dccc89e8d" CreationDate="2017-08-10T10:01:56.630" UserId="1148" Comment="added 644 characters in body" Text="Try changing you `evalue` threshold to something like `0.001`.&#xD;&#xA;&#xD;&#xA;If you blasted a sequence against a database containing 10,000 sequences and got a hit with an evalue of `1`, this would mean that, given the size of your database and length of protein, you would expect to find a match as good as (or better than) the hit returned once purely by chance. If the `evalue` threshold in your query was set to `0.01` (for example) then this hit would not be returned.&#xD;&#xA;&#xD;&#xA;If you got a hit with an `evalue` of 200,000 this would mean you would expect to see 200,000 matches with a similar score simply by chance. If your `evalue` threshold were set to 200000, then all of the hits with an evalue &lt;=200000 would be considered significant. If you set it to `0.001` then only those hits with `evalues` &lt;= `0.001` would be considered significant. &#xD;&#xA;&#xD;&#xA;This means that almost every sequence in your query set will be considered as finding a match less than would be expected by chance, which I imagine is what's slowing your search.&#xD;&#xA;&#xD;&#xA;See the the section on E values [here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=FAQ#expect&#xD;&#xA;" />
  <row Id="4968" PostHistoryTypeId="5" PostId="2260" RevisionGUID="7873b2a6-18b2-4ade-8017-21cfcb630650" CreationDate="2017-08-10T10:05:48.027" UserId="298" Comment="added 533 characters in body" Text="The evalue associated with a blast HSP (High-scoring Segment Pair) is a measure of how often you would expect to find such an HSP in a database of this size by pure chance. For instance, if you blast the sequence `ACTG` against the entirety of the NCBI's `nr` database, you would expect several hundred thousand hits by pure chance. That is, several hundred thousand sequences that can be aligned to your query but are in no way related to it (not actual homologs). Those hits don't actually mean anything, so you use the evalue to limit the results to those that are unlikely to be found by chance and, therefore, more likely to have biological meaning if hits are found. &#xD;&#xA;&#xD;&#xA;So, setting the evalue to 200000 means you are interested even in HSPs whose characteristics (score, length, etc) mean you would expect to find &lt;=200000 HSPs with such characteristics by pure chance. This evalue is pointless. &#xD;&#xA;&#xD;&#xA;Because your e-value is so high, BLAST will report absolutely anything and everything it finds, and the vast majority of your results will be meaningless and irrelevant. If you want a permissive e-value, you could use `1` or even `10` but 200000 is way too much. You usually only want negative e-values, cases where you wouldn't expect to find even one such HSP by chance. For highly dissimilar sequences, I would recommend you use 0.1 or something of that scale. &#xD;&#xA;&#xD;&#xA;Changing the evalue will speed things up considerably. That said, you should always expect different times for different queries since the time it takes to find hits will always depend on the number of hits that can be found. If your sequence can be aligned to many of the sequences in the database, it will take longer to fetch all results. " />
  <row Id="4969" PostHistoryTypeId="5" PostId="2216" RevisionGUID="f6177469-9028-4971-984a-93dd46d77333" CreationDate="2017-08-10T10:08:24.777" UserId="180" Comment="added 204 characters in body" Text="A bit of a historical question on a number, 30 times coverage, that's become so familiar in the field: why do we sequence the human genome at 30x coverage?&#xD;&#xA;&#xD;&#xA;My question has two parts:&#xD;&#xA;&#xD;&#xA;- Who came up with the 30x value and why?&#xD;&#xA;- Does the value need to be updated to reflect today's state-of-the-art?&#xD;&#xA;&#xD;&#xA;In summary, if the 30x value is a number that was based on the old Solexa GAIIx 2x35bp reads and error rates, and the current standard Illumina sequencing is 2x150bp, does the 30x value need updating?&#xD;&#xA;&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/GGkq0.png" />
  <row Id="4970" PostHistoryTypeId="5" PostId="2258" RevisionGUID="dbbc37d1-4a60-4176-8653-d58cf3dec428" CreationDate="2017-08-10T10:12:11.917" UserId="1148" Comment="added 13 characters in body" Text="Try changing you `evalue` threshold to something like `0.001`.&#xD;&#xA;&#xD;&#xA;If you blasted a sequence against a database containing 10,000 sequences and got a hit with an evalue of `1`, this would mean that, given the size of your database and length of protein, you would expect to find a match as good as (or better than) the hit returned once purely by chance. If the `evalue` threshold in your query was set to `0.01` (for example) then this hit would not be returned.&#xD;&#xA;&#xD;&#xA;If you got a hit with an `evalue` of 200,000 this would mean you would expect to see 200,000 matches with a similar score simply by chance. If your `evalue` threshold were set to 200000, then all of the hits with an evalue &lt;=200000 would be considered significant. If you set it to `0.001` then only those hits with `evalues` &lt;= `0.001` would be considered significant. &#xD;&#xA;&#xD;&#xA;This means that (depending on the size of the data base) almost every sequence in your query set will be considered as significant given your threshold, which will slow down your search hugely. &#xD;&#xA;&#xD;&#xA;See the the section on E values [here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=FAQ#expect" />
  <row Id="4971" PostHistoryTypeId="5" PostId="2261" RevisionGUID="d1608e57-1249-4365-bcb5-78af1a5d360c" CreationDate="2017-08-10T11:05:00.903" UserId="939" Comment="added 109 characters in body" Text="There are more R packages available that calculate GC content, for example Ape's `GC.content()` function.&#xD;&#xA;&#xD;&#xA;For example:&#xD;&#xA;&#xD;&#xA;    &gt; library(ape)&#xD;&#xA;    &gt; data(woodmouse)&#xD;&#xA;    &gt; GC.content(woodmouse)&#xD;&#xA;    [1] 0.3873347&#xD;&#xA;&#xD;&#xA;For codon usage bias, I think `cubfits` or `sscu` packages might be useful. " />
  <row Id="4972" PostHistoryTypeId="5" PostId="2259" RevisionGUID="4d1771ce-cd3d-4e35-a0cf-aeba92a6835a" CreationDate="2017-08-10T11:07:45.383" UserId="982" Comment="added 20 characters in body" Text="I'm looking to generate data based on the DNA composition of a region of my genomes (data is incomplete genomes from HiSeq runs). I'm looking for software which will give me sliding windows for GC content, GC skew, codon bias etc. I would like to use something which I can use with or plug into R. &#xD;&#xA;&#xD;&#xA;" />
  <row Id="4974" PostHistoryTypeId="5" PostId="2261" RevisionGUID="3a8c1bc0-53e7-4ddc-bbda-207eb5017f21" CreationDate="2017-08-10T11:24:02.910" UserId="939" Comment="added 193 characters in body" Text="There are more R packages available that calculate GC content, for example Ape's `GC.content()` function.&#xD;&#xA;&#xD;&#xA;For example:&#xD;&#xA;&#xD;&#xA;    &gt; library(ape)&#xD;&#xA;    &gt; data(woodmouse)&#xD;&#xA;    &gt; GC.content(woodmouse)&#xD;&#xA;    [1] 0.3873347&#xD;&#xA;&#xD;&#xA;With a sliding window is in `Biostrings` package.&#xD;&#xA;&#xD;&#xA;    &gt; library(Biostrings) &#xD;&#xA;    &gt; DNA &lt;- DNAString(&quot;ACTGAAACCGTGGCAGTTTGAC&quot;)&#xD;&#xA;    &gt; letterFrequencyInSlidingView(DNA, view.width=4,letters=&quot;CG&quot;)&#xD;&#xA;&#xD;&#xA;For codon usage bias, I think `cubfits` or `sscu` packages might be useful. " />
  <row Id="4975" PostHistoryTypeId="2" PostId="2262" RevisionGUID="c1f39ac7-78c4-465e-8a30-22494b46e849" CreationDate="2017-08-10T12:19:49.280" UserId="1305" Text="I believe all you need to do is to run dnadiff." />
  <row Id="4976" PostHistoryTypeId="2" PostId="2263" RevisionGUID="4808068b-5fab-4987-8f9b-2d6f96ba8af6" CreationDate="2017-08-10T12:24:36.120" UserId="180" Text="I noticed remote bigWig files (http URL) take sometimes about 1 minute to load on IGV. Once they loaded, jumping from one region to another can also take several seconds.&#xD;&#xA;&#xD;&#xA;My understanding is that bigWig files have an internal index for different chunks of different sizes. Can the index be modified so that the speed of loading on IGV is improved? Does this need to happen at the point of creation of the bigWig file, or can the index of an existing bigWig be updated to speed up loading?&#xD;&#xA;" />
  <row Id="4977" PostHistoryTypeId="1" PostId="2263" RevisionGUID="4808068b-5fab-4987-8f9b-2d6f96ba8af6" CreationDate="2017-08-10T12:24:36.120" UserId="180" Text="IGV faster loading remote bigWig files" />
  <row Id="4978" PostHistoryTypeId="3" PostId="2263" RevisionGUID="4808068b-5fab-4987-8f9b-2d6f96ba8af6" CreationDate="2017-08-10T12:24:36.120" UserId="180" Text="&lt;bigwig&gt;&lt;deeptools&gt;" />
  <row Id="4979" PostHistoryTypeId="2" PostId="2264" RevisionGUID="e2d9d8ca-c43e-40d6-bda2-0d2c1111474d" CreationDate="2017-08-10T12:33:11.853" UserId="77" Text="You would need to do this when creating the bigWig file. Note that I'm not aware of anything that allows manually setting the various zoom levels (libBigWig/pyBigWig will allow you to set a max number, but that's it).&#xD;&#xA;&#xD;&#xA;This then raises the question of whether IGV is using an appropriate zoom level to begin with. I've never looked at how it chooses which of the zoom levels to use, but I know it uses them since if you don't have any it will fail to load bigWig files.&#xD;&#xA;&#xD;&#xA;Honestly, though, if it's taking a minute to jump to a new region then I suspect that the network is just slow or the remote server is overtaxed. Changing the internal indexing isn't going to fix that." />
  <row Id="4980" PostHistoryTypeId="5" PostId="2259" RevisionGUID="7dccb1b7-c738-4563-8f7b-8b3a457286e7" CreationDate="2017-08-10T12:36:33.297" UserId="77" Comment="added 16 characters in body" Text="I'm looking to generate data based on the DNA composition of a region of my genomes (data is incomplete genomes from HiSeq runs in fasta format). I'm looking for software which will give me sliding windows for GC content, GC skew, codon bias etc. I would like to use something which I can use with or plug into R. &#xD;&#xA;&#xD;&#xA;" />
  <row Id="4981" PostHistoryTypeId="2" PostId="2265" RevisionGUID="c3fb8508-7146-4c25-b254-9a1842c0d49f" CreationDate="2017-08-10T12:46:40.243" UserId="292" Text="I generated a file starting with the following bed lines:&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	4220	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    I	4118	4358	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    I	4220	4223	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;I would like to merge them based on the name field (the 4-th column), taking the min for the start and the max for the end. Other fields are expected to be the same for all records having the same name.&#xD;&#xA;&#xD;&#xA;Expected result:&#xD;&#xA;&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I found a potential solution based on `bedtools groupby` here: https://www.biostars.org/p/145751/#145775&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&gt; Sample data:&#xD;&#xA;&#xD;&#xA;    cat genes.bed&#xD;&#xA;    chr14    49894259    49895806    ENSMUST00000053290    0.000000    ...&#xD;&#xA;    chr14    49894873    49894876    ENSMUST00000053290    0.000000    ...&#xD;&#xA;    chr14    49894876    49895800    ENSMUST00000053291    0.000000    ...&#xD;&#xA;    chr14    49895797    49895800    ENSMUST00000053291    0.000000    ...&#xD;&#xA;    chr14    49901908    49901941    ENSMUST00000053291    0.000000    ...&#xD;&#xA;&#xD;&#xA;&gt; Example output:&#xD;&#xA;&#xD;&#xA;    sort -k4,4 genes.bed \&#xD;&#xA;    | groupBy -g 1,4 -c 4,2,3 -o count,min,max \&#xD;&#xA;    | awk -v OFS='\t' '{print $1, $4, $5, $2, $3}'&#xD;&#xA;    &#xD;&#xA;    chr14    49894259    49895806    ENSMUST00000053290    2&#xD;&#xA;    chr14    49894876    49901941    ENSMUST00000053291    3&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;However:&#xD;&#xA;&#xD;&#xA;1. I don't understand the groupBy behaviour (Why `-g 1,4` and not just `-g 4`?, Why `-c 4,2,3` in this order and then rearrange things using `awk`?)&#xD;&#xA;&#xD;&#xA;2. This code doesn't work for me.&#xD;&#xA;&#xD;&#xA;Here is what happens when I try the solution given above:&#xD;&#xA;&#xD;&#xA;    $ head -3 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 1,4 -c 4,2,3 -o count,min,max | awk -v OFS='\t' '{print $1, $4, $5, $2, $3}'&#xD;&#xA;    3			3746	4220&#xD;&#xA;&#xD;&#xA;Here are attempt based on what I thought could work according to [the documentation](http://bedtools.readthedocs.io/en/latest/content/tools/groupby.html):&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 4 -c 1,2,3,4,5,6 -o first,min,max,distinct,first,first&#xD;&#xA;    I	3746	10230	&quot;WBGene00022277&quot;,&quot;WBGene00023193&quot;	.	-&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 4 -c 1,2,3,4,5,6 -o first,min,max,last,first,first&#xD;&#xA;    I	3746	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 4 -c 1,2,3,5,6 -o first,min,max,first,first&#xD;&#xA;    I	3746	10230	.	-&#xD;&#xA;&#xD;&#xA;I don't get why when I group based on the 4-th column, for which I have two distinct values, I cannot obtain two lines in the resulting output.&#xD;&#xA;&#xD;&#xA;I understand based on the comments on the documentation page that the documentation is not up-to-date. In particular, there is a `-full` option that is needed if one wants all fields to be outputted. Re-reading the solution mentioned above, I think I now understand the reason for the multiple columns for the `-g option` and for the `awk` rearrangement. Hence the following attempt.&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 1,4,5,6 -c 2,3 -o min,max -full&#xD;&#xA;    	I	3746	3909	&quot;WBGene00023193&quot;	.	-	3746	10230&#xD;&#xA;&#xD;&#xA;But this still doesn't give me two lines.&#xD;&#xA;&#xD;&#xA;Are there other tools that could do what I want efficiently?" />
  <row Id="4982" PostHistoryTypeId="1" PostId="2265" RevisionGUID="c3fb8508-7146-4c25-b254-9a1842c0d49f" CreationDate="2017-08-10T12:46:40.243" UserId="292" Text="Merging bed records based on name" />
  <row Id="4983" PostHistoryTypeId="3" PostId="2265" RevisionGUID="c3fb8508-7146-4c25-b254-9a1842c0d49f" CreationDate="2017-08-10T12:46:40.243" UserId="292" Text="&lt;bed&gt;&lt;bedtools&gt;" />
  <row Id="4984" PostHistoryTypeId="5" PostId="2262" RevisionGUID="3d2be9ad-a72d-4ab2-bf7e-183f890a13e8" CreationDate="2017-08-10T13:14:13.037" UserId="77" Comment="added 80 characters in body" Text="I believe all you need to do is to run `dnadiff` from MUMmer. That will run a comparison and output a number of useful metrics." />
  <row Id="4985" PostHistoryTypeId="5" PostId="2245" RevisionGUID="403778e5-b43c-41fa-aa3b-276ccd19566d" CreationDate="2017-08-10T13:39:09.303" UserId="235" Comment="added 59 characters in body" Text="To convert BAM to GTF, which is the best way to get a file to compare with cuffcompare:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    import pysam&#xD;&#xA;    &#xD;&#xA;    bamfile=pysam.AlignmentFile(&quot;my_bam_file.bam&quot;)&#xD;&#xA;    &#xD;&#xA;    for alignment in bamfile.fetch():&#xD;&#xA;    &#xD;&#xA;        if alignment.is_unmapped:&#xD;&#xA;            continue&#xD;&#xA;&#xD;&#xA;        contig = bamfile.get_reference_name(alignment.reference_id)&#xD;&#xA;        name = alignment.query_name&#xD;&#xA;        &#xD;&#xA;        if alignment.is_reverse:&#xD;&#xA;            strand = &quot;-&quot;&#xD;&#xA;        else: &#xD;&#xA;            strand = &quot;+&quot;&#xD;&#xA;    &#xD;&#xA;        for start, end in alignment.getblocks():&#xD;&#xA;            # note: BED is 0-based, half-open, GFF is 1-based, closed&#xD;&#xA;            print contig, &quot;BAM&quot;, &quot;exon&quot;, str(start + 1), str(end), &quot;0&quot;, strand, &quot;.&quot;, 'gene_id &quot;%s&quot;; transcript_id &quot;%s&quot;;' % (name, name)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;NB, this will also work with a SAM file as long as it is headered." />
  <row Id="4986" PostHistoryTypeId="2" PostId="2266" RevisionGUID="adf77e27-f2d5-4e49-b47a-f80795012bd3" CreationDate="2017-08-10T13:45:37.637" UserId="235" Text="You could do this with the [`CGAT` toolkit](https://github.com/CGATOxford/cgat):&#xD;&#xA;&#xD;&#xA;`cgat bed2bed --method=merge --merge-by-name -I bed_with_gene_ids.bed`&#xD;&#xA;&#xD;&#xA;Installing such a massive package might be overkill for this task though. " />
  <row Id="4987" PostHistoryTypeId="2" PostId="2267" RevisionGUID="a76aa190-fded-4722-b754-ab0988e2acca" CreationDate="2017-08-10T13:59:48.527" UserId="298" Text="If you're 100% sure that everything but the start and end positions will be the same for all lines sharing a name, you could just do it yourself. For example, in Perl:&#xD;&#xA;&#xD;&#xA;    $ perl -lane '$start{$F[3]}||=$F[1]; &#xD;&#xA;                  if( $F[1] &lt; $start{$F[3]} ){&#xD;&#xA;                     $start{$F[3]} = $F[1]&#xD;&#xA;                  } &#xD;&#xA;                  if( $F[2] &gt; $end{$F[3]} ){&#xD;&#xA;                     $end{$F[3]} = $F[2]&#xD;&#xA;                  } &#xD;&#xA;                  $chr{$F[3]} = $F[0]; &#xD;&#xA;                  $rest{$F[3]} = join &quot;\t&quot;, @F[4,$#F]; &#xD;&#xA;                  END{&#xD;&#xA;                     foreach $n (keys %chr){&#xD;&#xA;                      print &quot;$chr{$n}\t$start{$n}\t$end{$n}\t$n\t$rest{$n}&quot;&#xD;&#xA;                     }&#xD;&#xA;                  }' file.bed &#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    " />
  <row Id="4989" PostHistoryTypeId="5" PostId="2043" RevisionGUID="8adf2635-bc2a-4b58-9926-ed55734f8908" CreationDate="2017-08-10T14:41:41.410" UserId="104" Comment="changing to &quot;Biopython&quot; for consistancy, this is the case that they offically seem to use" Text="Biopython's `.count()` methods, like Python's `str.count()`, perform a non-overlapping count, how can I do an overlapping one?&#xD;&#xA;&#xD;&#xA;For example, these code snippets `return 2`, but I want the answer `3`:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; from Bio.Seq import Seq&#xD;&#xA;    &gt;&gt;&gt; Seq('AAAA').count('AA')&#xD;&#xA;    2&#xD;&#xA;    &gt;&gt;&gt; 'AAAA'.count('AA')&#xD;&#xA;    2&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4990" PostHistoryTypeId="2" PostId="2268" RevisionGUID="9543779f-b7bf-4fa1-87d7-8bae447f27f3" CreationDate="2017-08-10T15:10:04.033" UserId="939" Text="I don't think there is a database with expression data for your particular cell type. I am afraid your &quot;headache&quot; approach doesn't sound so bad...&#xD;&#xA;&#xD;&#xA;So get expression data e.g., from GEO for your cell type, and define which genes/proteins you consider as &quot;expressed&quot; (e.g., minimal RPKM value).&#xD;&#xA;&#xD;&#xA;Then, you can use GO annotation to find your genes involved with membrane or secretion (or what ever). You can use biomaRt in R for this.&#xD;&#xA;&#xD;&#xA;For example, you want to know all &quot;membrane&quot; genes.&#xD;&#xA;&#xD;&#xA;    &gt; library(biomaRt)&#xD;&#xA;    &gt; &#xD;&#xA;    &gt; ensembl = useMart(&quot;ensembl&quot;,dataset=&quot;hsapiens_gene_ensembl&quot;)&#xD;&#xA;    &gt; &#xD;&#xA;    &gt; membrane.genes &lt;- getBM(attributes=c('hgnc_symbol',&#xD;&#xA;    &gt; 'ensembl_transcript_id', 'go_id'),&#xD;&#xA;    &gt; filters = 'go', values = 'GO:0016020', mart = ensembl)&#xD;&#xA;&#xD;&#xA;And now you want to know it only from genes that are expressed in your ganglion cells (with dummy example of two genes).&#xD;&#xA;&#xD;&#xA;    &gt; gene.list &lt;- c(&quot;KIR2DL3&quot;, &quot;GPR1&quot;)&#xD;&#xA;    &gt; &#xD;&#xA;    &gt; membrane.genes.list &lt;- membrane.genes[membrane.genes$hgnc_symbol ==&#xD;&#xA;    &gt; gene.list,]&#xD;&#xA;    &gt; &#xD;&#xA;    &gt; membrane.only &lt;- membrane.genes.list[membrane.genes.list$go_id ==&#xD;&#xA;    &gt; 'GO:0016020',]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="4991" PostHistoryTypeId="2" PostId="2269" RevisionGUID="0f95829b-9302-47f1-80aa-68ee19024282" CreationDate="2017-08-10T16:25:38.047" UserId="1309" Text="I have some data I am working with, and I am curious if I am able to combine p-values from a paired t-test for CpG sites in the genome using Fisher's Method to get one p-value for each unique gene. Linked [here][1] is the Wikipedia page for Fisher's Method. I understand that an assumption of the method used is that each individual p-value being combined must be independent. I'm relatively new to biostatistics, so I'm curious if using CpGs from the same gene would violate this assumption.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Fisher%27s_method" />
  <row Id="4992" PostHistoryTypeId="1" PostId="2269" RevisionGUID="0f95829b-9302-47f1-80aa-68ee19024282" CreationDate="2017-08-10T16:25:38.047" UserId="1309" Text="Melt p-values for CpG sites mapping to the same gene" />
  <row Id="4993" PostHistoryTypeId="3" PostId="2269" RevisionGUID="0f95829b-9302-47f1-80aa-68ee19024282" CreationDate="2017-08-10T16:25:38.047" UserId="1309" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;gene&gt;&lt;methylation&gt;" />
  <row Id="4994" PostHistoryTypeId="2" PostId="2270" RevisionGUID="25982918-77b7-4a79-b8d6-fab190972ef3" CreationDate="2017-08-10T16:58:43.040" UserId="1310" Text="The point I always miss in the discussion about coverage is, that no one tells how it was calculated. Were duplicates removed? How do youcount overlapping paired read? As 2 or 1? Just to point out two things that have influence on the coverage. &#xA;&#xA;fin swimmer" />
  <row Id="4995" PostHistoryTypeId="2" PostId="2271" RevisionGUID="e5a32155-c8eb-4ec8-90c5-94f7ad44cb7c" CreationDate="2017-08-10T17:40:41.057" UserId="1076" Text="Although you don't mention it, I'm guessing you're using bedtools v2.26.0. Version 2.26.0 of groupBy has a bug in it, which you've encountered (it was fixed shortly after release, so you'll either have to use a version before the bug was introduced, or compile the current source code yourself from https://github.com/arq5x/bedtools2) &#xD;&#xA;&#xD;&#xA;v2.26.0: &#xD;&#xA;&#xD;&#xA;    local10:~/Documents/tmp$ cat asdf.bed &#xD;&#xA;    I	3746	3909	WBGene00023193	.	-&#xD;&#xA;    I	3746	3909	WBGene00023193	.	-&#xD;&#xA;    I	4118	4220	WBGene00022277	.	-&#xD;&#xA;    I	4118	4358	WBGene00022277	.	-&#xD;&#xA;    I	4118	10230	WBGene00022277	.	-&#xD;&#xA;    I	4220	4223	WBGene00022277	.	-&#xD;&#xA;    local10:~/Documents/tmp$ groupBy -i asdf.bed -g 4 -c 2,3 -o min,max &#xD;&#xA;    3746	10230 &#xD;&#xA;&#xD;&#xA;v2.26.0-125-g52db654 (I.E. compiling the source code from github): &#xD;&#xA;&#xD;&#xA;    local10:~/Documents/tmp$ bedtools2/bin/groupBy -i asdf.bed -g 4 -c 2,3 -o min,max&#xD;&#xA;    WBGene00023193	3746	3909&#xD;&#xA;    WBGene00022277	4118	10230&#xD;&#xA;&#xD;&#xA;------------------------------&#xD;&#xA;To answer your questions: &#xD;&#xA;&#xD;&#xA;1) You might notice that my output above gives the grouped columns first -- you'll have to reorder the output via awk in order to get it back in order.&#xD;&#xA;&#xD;&#xA;2) Version differences, as stated in the first part of my answer. &#xD;&#xA;&#xD;&#xA;(complete code to merge the bed files incoming)" />
  <row Id="4996" PostHistoryTypeId="5" PostId="2271" RevisionGUID="9342a538-a720-476b-bee8-b45a78414c85" CreationDate="2017-08-10T17:55:08.177" UserId="1076" Comment="Adding the code to merge the bed file" Text="Although you don't mention it, I'm guessing you're using bedtools v2.26.0. Version 2.26.0 of groupBy has a bug in it, which you've encountered (it was fixed shortly after release, so you'll either have to use a version before the bug was introduced, or compile the current source code yourself from https://github.com/arq5x/bedtools2) &#xD;&#xA;&#xD;&#xA;v2.26.0: &#xD;&#xA;&#xD;&#xA;    local10:~/Documents/tmp$ cat asdf.bed &#xD;&#xA;    I	3746	3909	WBGene00023193	.	-&#xD;&#xA;    I	3746	3909	WBGene00023193	.	-&#xD;&#xA;    I	4118	4220	WBGene00022277	.	-&#xD;&#xA;    I	4118	4358	WBGene00022277	.	-&#xD;&#xA;    I	4118	10230	WBGene00022277	.	-&#xD;&#xA;    I	4220	4223	WBGene00022277	.	-&#xD;&#xA;    local10:~/Documents/tmp$ groupBy -i asdf.bed -g 4 -c 2,3 -o min,max &#xD;&#xA;    3746	10230 &#xD;&#xA;&#xD;&#xA;v2.26.0-125-g52db654 (I.E. compiling the source code from github): &#xD;&#xA;&#xD;&#xA;    local10:~/Documents/tmp$ bedtools2/bin/groupBy -i asdf.bed -g 4 -c 2,3 -o min,max&#xD;&#xA;    WBGene00023193	3746	3909&#xD;&#xA;    WBGene00022277	4118	10230&#xD;&#xA;&#xD;&#xA;------------------------------&#xD;&#xA;To answer your questions: &#xD;&#xA;&#xD;&#xA;1) You might notice that my output above gives the grouped columns first; you'll have to reorder the output via awk in order to get it back in order. As for why they chose to group on both columns 1 and 4: if you have the same name on multiple chromosomes, you may want to treat them as separate features.&#xD;&#xA;&#xD;&#xA;2) Version differences, as stated in the first part of my answer. &#xD;&#xA;&#xD;&#xA;------------------------------&#xD;&#xA;To actually merge the file: &#xD;&#xA;You don't mention what you want to do with the rest of the columns, so I've just taken the first entry for each. Make sure to run this with a version other than v2.26.0:&#xD;&#xA;&#xD;&#xA;    groupBy -i asdf.bed -g 1,4 -c 2,3,1,5,6 -o min,max,first,first,first&#xD;&#xA;    | awk -v OFS='\t' '{print $4, $2, $3, $1, $5, $6}'" />
  <row Id="4997" PostHistoryTypeId="2" PostId="2272" RevisionGUID="744ece84-4ade-416a-83c2-48c1f6e02151" CreationDate="2017-08-10T18:11:16.970" UserId="77" Text="Methylation levels have high local correlation, so Fisher's method would be problematic. Having said that, you have no reason to use Fisher's method after a paired t-test. A paired t-test will give you a single p-value per gene, which is what you want. Do be sure to only include CpG with some minimal coverage in both group." />
  <row Id="4998" PostHistoryTypeId="5" PostId="2270" RevisionGUID="12a1c05f-7999-4442-927a-47a3fa8612ec" CreationDate="2017-08-10T18:12:40.087" UserId="77" Comment="deleted 8 characters in body" Text="The point I always miss in the discussion about coverage is, that no one tells how it was calculated. Were duplicates removed? How do you count overlapping paired-end reads? As 2 or 1? Just to point out two things that have influence on the coverage." />
  <row Id="4999" PostHistoryTypeId="2" PostId="2273" RevisionGUID="8e5a8dbc-d8e3-4c35-b5d4-f1b127d6206c" CreationDate="2017-08-10T18:48:21.233" UserId="776" Text="    $ cut -f4 in.bed | sort | uniq | awk '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $1; }' | paste -d &quot;\t&quot;  - - | sort-bed - &gt; answer.bed&#xD;&#xA;&#xD;&#xA;Given your sample input:&#xD;&#xA;&#xD;&#xA;    $ more in.bed&#xD;&#xA;    I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xD;&#xA;    I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xD;&#xA;    I   4118    4220    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4118    4358    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4220    4223    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;&#xD;&#xA;The `answer.bed` file:&#xD;&#xA;&#xD;&#xA;    $ more answer.bed&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;&#xD;&#xA;&#xD;&#xA;Sorting with `sort-bed` is useful at the end, so that you can pipe it or work with it with other BEDOPS tools, or other tools that now accept sorted BED input.&#xD;&#xA;&#xD;&#xA;Streaming is a pretty efficient way to do things, generally." />
  <row Id="5000" PostHistoryTypeId="5" PostId="2273" RevisionGUID="089b23d6-9ddb-4398-b893-bbf424eec2f1" CreationDate="2017-08-10T18:59:07.827" UserId="776" Comment="added 49 characters in body" Text="    $ cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq | awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }' | paste -d &quot;\t&quot; - - | sed 's/_/\t/g' | sort-bed - &gt; answer.bed&#xD;&#xA;&#xD;&#xA;Given your sample input:&#xD;&#xA;&#xD;&#xA;    $ more in.bed&#xD;&#xA;    I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xD;&#xA;    I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xD;&#xA;    I   4118    4220    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4118    4358    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4220    4223    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;&#xD;&#xA;The `answer.bed` file:&#xD;&#xA;&#xD;&#xA;    $ more answer.bed&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;Sorting with `sort-bed` is useful at the end, so that you can pipe it or work with it with other BEDOPS tools, or other tools that now accept sorted BED input.&#xD;&#xA;&#xD;&#xA;Streaming is a pretty efficient way to do things, generally." />
  <row Id="5001" PostHistoryTypeId="2" PostId="2274" RevisionGUID="523cf3f1-f4a6-4f4a-9e49-9484bef4027c" CreationDate="2017-08-10T22:22:01.717" UserId="146" Text="Let's say I have the genome `hg19` loaded into R via BSGenome&#xD;&#xA;                                                                                                                                                                                                                                                                                                                                                                                                       &#xD;&#xA;    library(&quot;BSgenome&quot;)    &#xD;&#xA;    hg19genome = getBSgenome('BSgenome.Hsapiens.UCSC.hg19', masked=FALSE)   &#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;I then have a list of SNPs loaded as a GRanges object, `gr`&#xD;&#xA;&#xD;&#xA;    library(GenomicRanges)&#xD;&#xA;    &#xD;&#xA;     &gt; gr&#xD;&#xA;     GRanges object with 212 ranges and 3 metadata columns:                                                                                                                                                                                                                                                                                                                                                &#xD;&#xA;            seqnames               ranges strand |     width     REF    ALT                                                                                                                                                                                                                                                                                                                             &#xD;&#xA;               &lt;Rle&gt;          v &lt;IRanges&gt;  &lt;Rle&gt; | &lt;numeric&gt;                                                                                                                                                                                                                                                                                                                        &#xD;&#xA;        [1]        1 [86099032, 86099032]      * |         1     C      T                                                                                                                                                                                                                                                                                                                  &#xD;&#xA;        [2]        1 [86099033, 86099033]      * |         1     C      A                                                                                                                                                                                                                                                                                                                   &#xD;&#xA;        [3]        1 [86099199, 86099199]      * |         1     G      A  &#xD;&#xA;      ....&#xD;&#xA;&#xD;&#xA;Is there a straightforward way to inject these SNPs into `hg19genome`?" />
  <row Id="5002" PostHistoryTypeId="1" PostId="2274" RevisionGUID="523cf3f1-f4a6-4f4a-9e49-9484bef4027c" CreationDate="2017-08-10T22:22:01.717" UserId="146" Text="Given a Genomic Ranges of SNPs, how to inject these SNPs in genome via BSGenome?" />
  <row Id="5003" PostHistoryTypeId="3" PostId="2274" RevisionGUID="523cf3f1-f4a6-4f4a-9e49-9484bef4027c" CreationDate="2017-08-10T22:22:01.717" UserId="146" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;biostrings&gt;" />
  <row Id="5004" PostHistoryTypeId="6" PostId="2274" RevisionGUID="08de04d9-bd7b-439f-bfa8-e1f5be938010" CreationDate="2017-08-10T22:37:31.110" UserId="146" Comment="edited tags" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;reference-genome&gt;&lt;snp&gt;&lt;biostrings&gt;" />
  <row Id="5005" PostHistoryTypeId="2" PostId="2275" RevisionGUID="06ccbbe5-5d41-4bf5-973a-18c69642b0ea" CreationDate="2017-08-10T22:44:40.640" UserId="156" Text="The problem: I have a VCF file, a reference genome, and a bunch of annotations for the reference (genes, repeat regions, etc.) as GFF or BED files.&#xD;&#xA;&#xD;&#xA;What I would like is a tool that takes all of this as input and outputs a tab- or comma-delimited table containing as much information as possible. Potential columns in the output include:&#xD;&#xA;&#xD;&#xA;1. Type of variant (SNV, structural, etc)&#xD;&#xA;2. Details of variant (e.g. reference base, variant base, coverage, position, etc)&#xD;&#xA;3. Annotations overlapping variant&#xD;&#xA;4. Annotations near variant (e.g. is it just upstream of a gene)&#xD;&#xA;5. If it appears in a coding region, does it change the amino acid.&#xD;&#xA;&#xD;&#xA;There are many tools that do *something like this*. But to the newbie (like me) it is not clear which tools are worth starting with. Since the majority of tools take a little effort to get working in the first place, my question is:&#xD;&#xA;&#xD;&#xA;What tool comes closest to doing what I have described, is known to reliable, and is likely to be maintained for the next few years." />
  <row Id="5006" PostHistoryTypeId="1" PostId="2275" RevisionGUID="06ccbbe5-5d41-4bf5-973a-18c69642b0ea" CreationDate="2017-08-10T22:44:40.640" UserId="156" Text="Tools to create annotated table of variants from VCF" />
  <row Id="5007" PostHistoryTypeId="3" PostId="2275" RevisionGUID="06ccbbe5-5d41-4bf5-973a-18c69642b0ea" CreationDate="2017-08-10T22:44:40.640" UserId="156" Text="&lt;annotation&gt;&lt;vcf&gt;" />
  <row Id="5013" PostHistoryTypeId="2" PostId="2277" RevisionGUID="124dac56-7feb-41cd-b1f6-213197fd5a9a" CreationDate="2017-08-11T06:50:40.807" UserId="1281" Text="I have used FEATnotator and I think it can provide all of the columns you would like to see. There are many output files generated, but the consolidated output has the following columns:&#xD;&#xA;&#xD;&#xA;- Chromosome     &#xD;&#xA;- Position       &#xD;&#xA;- Column_3     &#xD;&#xA;- Consensus_Allele &#xD;&#xA;- Annotation_Signature   &#xD;&#xA;- Reference_Base &#xD;&#xA;- Alternate_Base &#xD;&#xA;- Transition/Transversion SNP_Type       &#xD;&#xA;- Premature_STOP_Gained   STOP_Lost      &#xD;&#xA;- START_CODON    &#xD;&#xA;- STOP_CODON     &#xD;&#xA;- SPLICE_SITE &#xD;&#xA;- InterGenic     &#xD;&#xA;- Gene_Body      &#xD;&#xA;- Intron &#xD;&#xA;- Exon &#xD;&#xA;- Coding &#xD;&#xA;- UTR    &#xD;&#xA;- Transcription_Start_Site      &#xD;&#xA;- Nearest_gene   &#xD;&#xA;- Distance&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;here is some example output records from using a VCF, reference genome and GFF annotation file (sorry about the crappy formatting...there are a lot of fields!):&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;`Chr01   28177   T       G       NA      T     G TRANSVERSION    NA      NO      NO      NO      NO      NO      YES     NO      NO      NO      NO      NO      NO      Eucgr.A00211.v2.0       16054`&#xD;&#xA;&#xD;&#xA;`Chr01   44876   C       G       gene:Eucgr.A00211.v2.0   mRNA:Eucgr.A00211.1.v2.0[intron]       C       G       TRANSVERSION    NA      NO      NO      NO      NO      NO      NO      YES     YES     NO      NO      NO      NO      NA  NA`&#xD;&#xA;&#xD;&#xA;`Chr01   46819   A       G       gene:Eucgr.A00211.v2.0   mRNA:Eucgr.A00211.1.v2.0   CDS:Eucgr.A00211.1.v2.0.CDS.4[AAA - K =&gt; GAA - E (MISSENSE)]   exon:Eucgr.A00211.1.v2.0.exon.4      A       G       TRANSITION      MISSENSE        NO  NO       NO      NO      NO      NO      YES   NO        YES     YES     NO      NO      NA      NA`&#xD;&#xA;" />
  <row Id="5014" PostHistoryTypeId="2" PostId="2278" RevisionGUID="9b0ea714-0c53-492d-8393-02d7ef968022" CreationDate="2017-08-11T07:39:25.050" UserId="206" Text="snpEff is a great tool for annotating VCF files and you can add custom reference sequences.&#xD;&#xA;&#xD;&#xA;http://snpeff.sourceforge.net/&#xD;&#xA;&#xD;&#xA;Guide to add custom annotation files in snpEff&#xD;&#xA;&#xD;&#xA;https://gatkforums.broadinstitute.org/gatk/discussion/50/adding-genomic-annotations-using-snpeff-and-variantannotator&#xD;&#xA;&#xD;&#xA;There are a bunch of pre curated annotation datasets available in their database." />
  <row Id="5015" PostHistoryTypeId="2" PostId="2279" RevisionGUID="34142d33-27da-48d2-89eb-24fd8258b31e" CreationDate="2017-08-11T07:44:08.053" UserId="206" Text="I've annotated VCF files using snpEff and looking for a tool or script to parse the VCF file and clean up the file to make it interpretable for a biologist. " />
  <row Id="5016" PostHistoryTypeId="1" PostId="2279" RevisionGUID="34142d33-27da-48d2-89eb-24fd8258b31e" CreationDate="2017-08-11T07:44:08.053" UserId="206" Text="Tool or script to parse annotated VCF files" />
  <row Id="5017" PostHistoryTypeId="3" PostId="2279" RevisionGUID="34142d33-27da-48d2-89eb-24fd8258b31e" CreationDate="2017-08-11T07:44:08.053" UserId="206" Text="&lt;variant-calling&gt;&lt;snp&gt;" />
  <row Id="5020" PostHistoryTypeId="2" PostId="2280" RevisionGUID="64d1e871-cb33-4361-b9ce-88211459207d" CreationDate="2017-08-11T08:04:59.083" UserId="71" Text="(edit)you can filter the VCF annotations with **snpsift**, I've also written a **VcfFilterSequenceOntology** http://lindenb.github.io/jvarkit/VcfFilterSequenceOntology.html&#xD;&#xA;&#xD;&#xA;I've written vcf2table: http://lindenb.github.io/jvarkit/VcfToTable.html&#xD;&#xA;It decodes VEP and SNPeff annotations:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;chr1/10001/T (n 1)&#xD;&#xA;     Variant&#xD;&#xA;     +--------+--------------------+&#xD;&#xA;     | Key    | Value              |&#xD;&#xA;     +--------+--------------------+&#xD;&#xA;     | CHROM  | chr1               |&#xD;&#xA;     (....)&#xD;&#xA;     VEP&#xD;&#xA;     +--------------------------+------+----------------+------------+-----------------+--------+------------------+-----------------------------------------------+-------------+---------+-----------------+----------------------+&#xD;&#xA;     | PolyPhen                 | EXON | SIFT           | ALLELE_NUM | Gene            | SYMBOL | Protein_position | Consequence                                   | Amino_acids | Codons  | Feature         | BIOTYPE              |&#xD;&#xA;     +--------------------------+------+----------------+------------+-----------------+--------+------------------+-----------------------------------------------+-------------+---------+-----------------+----------------------+&#xD;&#xA;     | probably_damaging(0.956) | 8/9  | deleterious(0) | 1          | ENSG00000102967 | DHODH  | 346/395          | missense_variant                              | R/W         | Cgg/Tgg | ENST00000219240 | protein_coding       |&#xD;&#xA;     |                          | 3/4  |                | 1          | ENSG00000102967 | DHODH  |                  | non_coding_exon_variant&amp;nc_transcript_variant |             |         | ENST00000571392 | retained_intron      |&#xD;&#xA;     |                          |      |                | 1          | ENSG00000102967 | DHODH  |                  | downstream_gene_variant                       |             |         | ENST00000572003 | retained_intron      |&#xD;&#xA;     |                          |      |                | 1          | ENSG00000102967 | DHODH  |                  | downstream_gene_variant                       |             |         | ENST00000573843 | retained_intron      |&#xD;&#xA;     |                          |      |                | 1          | ENSG00000102967 | DHODH  |                  | downstream_gene_variant                       |             |         | ENST00000573922 | processed_transcript |&#xD;&#xA;     |                          |      |                | 1          | ENSG00000102967 | DHODH  | -/193            | intron_variant                                |             |         | ENST00000574309 | protein_coding       |&#xD;&#xA;     | probably_damaging(0.946) | 8/9  | deleterious(0) | 1          | ENSG00000102967 | DHODH  | 344/393          | missense_variant                              | R/W         | Cgg/Tgg | ENST00000572887 | protein_coding       |&#xD;&#xA;     +--------------------------+------+----------------+------------+-----------------+--------+------------------+-----------------------------------------------+-------------+---------+-----------------+----------------------+&#xD;&#xA;     Genotypes&#xD;&#xA;     +---------+------+-------+----+----+-----+---------+&#xD;&#xA;     | Sample  | Type | AD    | DP | GQ | GT  | PL      |&#xD;&#xA;     +---------+------+-------+----+----+-----+---------+&#xD;&#xA;     | M10475  | HET  | 10,2  | 15 | 10 | 0/1 | 25,0,10 |&#xD;&#xA;     | M10478  | HET  | 10,4  | 16 | 5  | 0/1 | 40,0,5  |&#xD;&#xA;     | M10500  | HET  | 10,10 | 21 | 7  | 0/1 | 111,0,7 |&#xD;&#xA;     | M128215 | HET  | 15,5  | 24 | 0  | 0/1 | 49,0,0  |&#xD;&#xA;     +---------+------+-------+----+----+-----+---------+&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5021" PostHistoryTypeId="2" PostId="2281" RevisionGUID="4b545882-b203-423f-8a59-aa0fb2c3a938" CreationDate="2017-08-11T13:12:29.063" UserId="383" Text="This seems relatively complicated given the structure of a BSGenome object.&#xD;&#xA;&#xD;&#xA;The creator of the package answered this question previously on the Bioconductor support forums:&#xD;&#xA;https://support.bioconductor.org/p/86665/#86757&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&gt; We don't provide an easy way to inject arbitrary SNPs in an arbitrary&#xD;&#xA;&gt; BSgenome at the moment. However, it should not be too hard to forge&#xD;&#xA;&gt; the BSgenome ... package. First you would need to&#xD;&#xA;&gt; compute the sequences of the mutated chromosomes (you can&#xD;&#xA;&gt; use replaceLetterAt for this), then write them to a 2bit file (put&#xD;&#xA;&gt; them in a DNAStringSet object and call rtracklayer::export on it),&#xD;&#xA;&gt; then use that 2bit file to forge the BSgenome ....&#xD;&#xA;&gt; package (see the BSgenomeForge vignette in the BSgenome package for&#xD;&#xA;&gt; how to do this).&#xD;&#xA;- Herve Pages&#xD;&#xA;&#xD;&#xA;Here is another similar question with more info about replaceLetterAt: https://support.bioconductor.org/p/26199/" />
  <row Id="5022" PostHistoryTypeId="5" PostId="2187" RevisionGUID="9cf74714-196a-4b2c-bc77-8d9da1b32d9a" CreationDate="2017-08-11T13:18:48.793" UserId="450" Comment="use `with` statement to open files" Text="Splitting into multiple files and changing the IDs can be easily done:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    perl -pe 'if(/&gt;/){/\[(.*?)\]\s*$/; $_=&quot;&gt; $1\n&quot;}' file.fa | &#xD;&#xA;        awk '(/^&gt;/){name=$2} {print &gt;&gt; name&quot;.fa&quot;}'&#xD;&#xA;&#xD;&#xA;That assumes all your FASTA headers have `[foo bar baz]` as the last element of a line. It will create a file called `foo.fa` (the bacterium's name) with all sequences saved there. &#xD;&#xA;&#xD;&#xA;###Explanation&#xD;&#xA;&#xD;&#xA;* `perl -pe` : run the script given by `-e` on each line of the input file, and print the resulting line. &#xD;&#xA;* `if(/&gt;/)` : if this line starts with a `&gt;`. \&#xD;&#xA;* `/\[(.*?)\]\s*$/` : match an opening bracket (`\[`), then capture (that's what the parentheses do, they capture a pattern so we can refer to it as `$1`) everything until the first `]` (`.*?\]`)&#xD;&#xA;* `$_=&quot;&gt; $1\n&quot;` : the `$_` special variable in Perl is (in this case) the current line. So, `$_=foo` means &quot;make the current line read `foo`. Since the `-p` prints each input line, changing the value of `$_` means the changed value will be printed. So here, we are printing `&gt;`, whatever was in the square brackets (`$1`) and a newline character. &#xD;&#xA;&#xD;&#xA;The output of the perl command alone on your example input file is:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    $ perl -pe 'if(/&gt;/){/\[(.*)\]\s*$/; $_=&quot;&gt; $1\n&quot;}' file.fa&#xD;&#xA;    &gt; Arthrobacter phage Mudcat&#xD;&#xA;    MGLSNTATPLYYGQF...&#xD;&#xA;    &#xD;&#xA;    &gt; Achromobacter phage phiAxp-3&#xD;&#xA;    MSNVLLKQELDEWL...&#xD;&#xA;    &#xD;&#xA;    &gt; Delftia phage RG-2014&#xD;&#xA;    MSEPRKLVKKTLD...&#xD;&#xA;    &#xD;&#xA;So, we now pass it through `awk` which does:&#xD;&#xA;&#xD;&#xA;* `(/^&gt;/){a=$2}` : if this line starts with an `&gt;`, save the second field (the bacterial species) as the variable `name`. &#xD;&#xA;* `{print &gt;&gt; name&quot;.fa&quot;}` : print each line into a file whose name is the current value of the variable `name` with a `.fa.` extension. &#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;If you prefer python scripts to the one-liner approach, you can do the same thing with:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    import re&#xD;&#xA;    outFile = None&#xD;&#xA;    for line in open(&quot;file.fa&quot;, &quot;r&quot;):&#xD;&#xA;        line = line.rstrip()&#xD;&#xA;        if line.startswith('&gt;'):&#xD;&#xA;            regex = re.compile('.*\[((.+?)\s+.*?)\].*')&#xD;&#xA;            matches = regex.search(line)&#xD;&#xA;            species = matches[2]&#xD;&#xA;            outFile = open(species + &quot;.fa&quot;, 'a')&#xD;&#xA;            outFile.write('&gt;%s\n' % matches[1])&#xD;&#xA;        else:&#xD;&#xA;            outFile.write(&quot;%s\n&quot; % line)&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;As for your script, you've got the right idea, but have a small bug. You forgot to remove the `\n` from your input file, so it looks for `Arthrobacter\n` instead of `Arthrobacter`. The golden rule of debugging is &quot;print all the things&quot;. If you add `print(&quot;Searchterm: &quot;,searchterm)`  you will see:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDArthrobacter  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDAchromobacter  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDDelftia  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;So, you need to remove the newline characters and add a space like so (I also made it a bit more &quot;pythonic&quot; and conforming to the Python syntax guidelines):&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import Entrez&#xD;&#xA;    Entrez.email = &quot;example@example.org&quot;&#xD;&#xA;    &#xD;&#xA;    with open(&quot;species2.txt&quot;, &quot;r&quot;) as in_handle:&#xD;&#xA;        for line in in_handle:&#xD;&#xA;            line = line.rstrip()&#xD;&#xA;            searchterm = (&quot;(terminase large subunit AND viruses[Organism]) &quot; +&#xD;&#xA;                          &quot;AND %s AND refseq[Filter]&quot; % line)&#xD;&#xA;            print(&quot;Searchterm: &quot;, searchterm)&#xD;&#xA;            searchResultHandle = Entrez.esearch(db=&quot;protein&quot;,&#xD;&#xA;                                                term=searchterm, retmax=1000)&#xD;&#xA;            searchResult = Entrez.read(searchResultHandle)&#xD;&#xA;            ids = searchResult[&quot;IdList&quot;]&#xD;&#xA;    &#xD;&#xA;            handle = Entrez.efetch(db=&quot;protein&quot;, id=ids,&#xD;&#xA;                                   rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xD;&#xA;            record = handle.read()&#xD;&#xA;&#xD;&#xA;            with open('terminase_large_' + str(line[:-1]) + '.fasta', 'w') as out_handle:&#xD;&#xA;                out_handle.write(record.rstrip('\n'))&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;" />
  <row Id="5023" PostHistoryTypeId="24" PostId="2187" RevisionGUID="9cf74714-196a-4b2c-bc77-8d9da1b32d9a" CreationDate="2017-08-11T13:18:48.793" Comment="Proposed by 450 approved by 298 edit id of 265" />
  <row Id="5024" PostHistoryTypeId="5" PostId="2187" RevisionGUID="bf026f4c-aaf1-4d1f-9c4a-7100a6159fc9" CreationDate="2017-08-11T13:19:48.687" UserId="450" Comment="use `with` statement to open files" Text="Splitting into multiple files and changing the IDs can be easily done:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    perl -pe 'if(/&gt;/){/\[(.*?)\]\s*$/; $_=&quot;&gt; $1\n&quot;}' file.fa | &#xD;&#xA;        awk '(/^&gt;/){name=$2} {print &gt;&gt; name&quot;.fa&quot;}'&#xD;&#xA;&#xD;&#xA;That assumes all your FASTA headers have `[foo bar baz]` as the last element of a line. It will create a file called `foo.fa` (the bacterium's name) with all sequences saved there. &#xD;&#xA;&#xD;&#xA;###Explanation&#xD;&#xA;&#xD;&#xA;* `perl -pe` : run the script given by `-e` on each line of the input file, and print the resulting line. &#xD;&#xA;* `if(/&gt;/)` : if this line starts with a `&gt;`. \&#xD;&#xA;* `/\[(.*?)\]\s*$/` : match an opening bracket (`\[`), then capture (that's what the parentheses do, they capture a pattern so we can refer to it as `$1`) everything until the first `]` (`.*?\]`)&#xD;&#xA;* `$_=&quot;&gt; $1\n&quot;` : the `$_` special variable in Perl is (in this case) the current line. So, `$_=foo` means &quot;make the current line read `foo`. Since the `-p` prints each input line, changing the value of `$_` means the changed value will be printed. So here, we are printing `&gt;`, whatever was in the square brackets (`$1`) and a newline character. &#xD;&#xA;&#xD;&#xA;The output of the perl command alone on your example input file is:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    $ perl -pe 'if(/&gt;/){/\[(.*)\]\s*$/; $_=&quot;&gt; $1\n&quot;}' file.fa&#xD;&#xA;    &gt; Arthrobacter phage Mudcat&#xD;&#xA;    MGLSNTATPLYYGQF...&#xD;&#xA;    &#xD;&#xA;    &gt; Achromobacter phage phiAxp-3&#xD;&#xA;    MSNVLLKQELDEWL...&#xD;&#xA;    &#xD;&#xA;    &gt; Delftia phage RG-2014&#xD;&#xA;    MSEPRKLVKKTLD...&#xD;&#xA;    &#xD;&#xA;So, we now pass it through `awk` which does:&#xD;&#xA;&#xD;&#xA;* `(/^&gt;/){a=$2}` : if this line starts with an `&gt;`, save the second field (the bacterial species) as the variable `name`. &#xD;&#xA;* `{print &gt;&gt; name&quot;.fa&quot;}` : print each line into a file whose name is the current value of the variable `name` with a `.fa.` extension. &#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;If you prefer python scripts to the one-liner approach, you can do the same thing with:&#xD;&#xA;&#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    import re&#xD;&#xA;    outFile = None&#xD;&#xA;    with open(&quot;file.fa&quot;, &quot;r&quot;) as inFile, open(species + &quot;.fa&quot;, 'a') as outFile:&#xD;&#xA;        for line in inFile:&#xD;&#xA;            line = line.rstrip()&#xD;&#xA;            if line.startswith('&gt;'):&#xD;&#xA;                regex = re.compile('.*\[((.+?)\s+.*?)\].*')&#xD;&#xA;                matches = regex.search(line)&#xD;&#xA;                species = matches[2]&#xD;&#xA;                outFile.write('&gt;%s\n' % matches[1])&#xD;&#xA;            else:&#xD;&#xA;                outFile.write(&quot;%s\n&quot; % line)&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;As for your script, you've got the right idea, but have a small bug. You forgot to remove the `\n` from your input file, so it looks for `Arthrobacter\n` instead of `Arthrobacter`. The golden rule of debugging is &quot;print all the things&quot;. If you add `print(&quot;Searchterm: &quot;,searchterm)`  you will see:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-none --&gt;&#xD;&#xA;&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDArthrobacter  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDAchromobacter  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    Searchterm:  (terminase large subunit AND viruses[Organism]) ANDDelftia  &#xD;&#xA;    AND refseq[Filter]&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;So, you need to remove the newline characters and add a space like so (I also made it a bit more &quot;pythonic&quot; and conforming to the Python syntax guidelines):&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;    #!/usr/bin/env python&#xD;&#xA;    from Bio import Entrez&#xD;&#xA;    Entrez.email = &quot;example@example.org&quot;&#xD;&#xA;    &#xD;&#xA;    with open(&quot;species2.txt&quot;, &quot;r&quot;) as in_handle:&#xD;&#xA;        for line in in_handle:&#xD;&#xA;            line = line.rstrip()&#xD;&#xA;            searchterm = (&quot;(terminase large subunit AND viruses[Organism]) &quot; +&#xD;&#xA;                          &quot;AND %s AND refseq[Filter]&quot; % line)&#xD;&#xA;            print(&quot;Searchterm: &quot;, searchterm)&#xD;&#xA;            searchResultHandle = Entrez.esearch(db=&quot;protein&quot;,&#xD;&#xA;                                                term=searchterm, retmax=1000)&#xD;&#xA;            searchResult = Entrez.read(searchResultHandle)&#xD;&#xA;            ids = searchResult[&quot;IdList&quot;]&#xD;&#xA;    &#xD;&#xA;            handle = Entrez.efetch(db=&quot;protein&quot;, id=ids,&#xD;&#xA;                                   rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xD;&#xA;            record = handle.read()&#xD;&#xA;&#xD;&#xA;            with open('terminase_large_' + str(line[:-1]) + '.fasta', 'w') as out_handle:&#xD;&#xA;                out_handle.write(record.rstrip('\n'))&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;" />
  <row Id="5025" PostHistoryTypeId="24" PostId="2187" RevisionGUID="bf026f4c-aaf1-4d1f-9c4a-7100a6159fc9" CreationDate="2017-08-11T13:19:48.687" Comment="Proposed by 450 approved by 298 edit id of 266" />
  <row Id="5027" PostHistoryTypeId="5" PostId="2277" RevisionGUID="c56a8bc5-a473-457e-9a94-8836ac2315ac" CreationDate="2017-08-11T13:28:51.170" UserId="298" Comment="Fixed formatting" Text="I have used FEATnotator and I think it can provide all of the columns you would like to see. There are many output files generated, but the consolidated output has the following columns:&#xD;&#xA;&#xD;&#xA;- Chromosome     &#xD;&#xA;- Position       &#xD;&#xA;- Column_3     &#xD;&#xA;- Consensus_Allele &#xD;&#xA;- Annotation_Signature   &#xD;&#xA;- Reference_Base &#xD;&#xA;- Alternate_Base &#xD;&#xA;- Transition/Transversion SNP_Type       &#xD;&#xA;- Premature_STOP_Gained   STOP_Lost      &#xD;&#xA;- START_CODON    &#xD;&#xA;- STOP_CODON     &#xD;&#xA;- SPLICE_SITE &#xD;&#xA;- InterGenic     &#xD;&#xA;- Gene_Body      &#xD;&#xA;- Intron &#xD;&#xA;- Exon &#xD;&#xA;- Coding &#xD;&#xA;- UTR    &#xD;&#xA;- Transcription_Start_Site      &#xD;&#xA;- Nearest_gene   &#xD;&#xA;- Distance&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;here is some example output records from using a VCF, reference genome and GFF annotation file (sorry about the crappy formatting...there are a lot of fields!):&#xD;&#xA;&#xD;&#xA;    Chr01   28177   T       G       NA      T     G TRANSVERSION    NA      NO      NO      NO      NO      NO      YES     NO      NO      NO      NO      NO      NO      Eucgr.A00211.v2.0       16054&#xD;&#xA;    Chr01   44876   C       G       gene:Eucgr.A00211.v2.0   mRNA:Eucgr.A00211.1.v2.0[intron]       C       G       TRANSVERSION    NA      NO      NO      NO      NO      NO      NO      YES     YES     NO      NO      NO      NO      NA  NA&#xD;&#xA;    Chr01   46819   A       G       gene:Eucgr.A00211.v2.0   mRNA:Eucgr.A00211.1.v2.0   CDS:Eucgr.A00211.1.v2.0.CDS.4[AAA - K =&gt; GAA - E (MISSENSE)]   exon:Eucgr.A00211.1.v2.0.exon.4      A       G       TRANSITION      MISSENSE        NO  NO       NO      NO      NO      NO      YES   NO        YES     YES     NO      NO      NA      NA&#xD;&#xA;" />
  <row Id="5029" PostHistoryTypeId="5" PostId="2265" RevisionGUID="44105f8e-48a0-46a0-816a-e43fc2cf3e3b" CreationDate="2017-08-11T15:18:50.827" UserId="292" Comment="Added bedtools based solution" Text="I generated a file starting with the following bed lines:&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	4220	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    I	4118	4358	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    I	4220	4223	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;I would like to merge them based on the name field (the 4-th column), taking the min for the start and the max for the end. Other fields are expected to be the same for all records having the same name.&#xD;&#xA;&#xD;&#xA;Expected result:&#xD;&#xA;&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I found a potential solution based on `bedtools groupby` here: https://www.biostars.org/p/145751/#145775&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&gt; Sample data:&#xD;&#xA;&#xD;&#xA;    cat genes.bed&#xD;&#xA;    chr14    49894259    49895806    ENSMUST00000053290    0.000000    ...&#xD;&#xA;    chr14    49894873    49894876    ENSMUST00000053290    0.000000    ...&#xD;&#xA;    chr14    49894876    49895800    ENSMUST00000053291    0.000000    ...&#xD;&#xA;    chr14    49895797    49895800    ENSMUST00000053291    0.000000    ...&#xD;&#xA;    chr14    49901908    49901941    ENSMUST00000053291    0.000000    ...&#xD;&#xA;&#xD;&#xA;&gt; Example output:&#xD;&#xA;&#xD;&#xA;    sort -k4,4 genes.bed \&#xD;&#xA;    | groupBy -g 1,4 -c 4,2,3 -o count,min,max \&#xD;&#xA;    | awk -v OFS='\t' '{print $1, $4, $5, $2, $3}'&#xD;&#xA;    &#xD;&#xA;    chr14    49894259    49895806    ENSMUST00000053290    2&#xD;&#xA;    chr14    49894876    49901941    ENSMUST00000053291    3&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;However:&#xD;&#xA;&#xD;&#xA;1. I don't understand the groupBy behaviour (Why `-g 1,4` and not just `-g 4`?, Why `-c 4,2,3` in this order and then rearrange things using `awk`?)&#xD;&#xA;&#xD;&#xA;2. This code doesn't work for me.&#xD;&#xA;&#xD;&#xA;Here is what happens when I try the solution given above:&#xD;&#xA;&#xD;&#xA;    $ head -3 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 1,4 -c 4,2,3 -o count,min,max | awk -v OFS='\t' '{print $1, $4, $5, $2, $3}'&#xD;&#xA;    3			3746	4220&#xD;&#xA;&#xD;&#xA;Here are attempt based on what I thought could work according to [the documentation](http://bedtools.readthedocs.io/en/latest/content/tools/groupby.html):&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 4 -c 1,2,3,4,5,6 -o first,min,max,distinct,first,first&#xD;&#xA;    I	3746	10230	&quot;WBGene00022277&quot;,&quot;WBGene00023193&quot;	.	-&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 4 -c 1,2,3,4,5,6 -o first,min,max,last,first,first&#xD;&#xA;    I	3746	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 4 -c 1,2,3,5,6 -o first,min,max,first,first&#xD;&#xA;    I	3746	10230	.	-&#xD;&#xA;&#xD;&#xA;I don't get why when I group based on the 4-th column, for which I have two distinct values, I cannot obtain two lines in the resulting output.&#xD;&#xA;&#xD;&#xA;I understand based on the comments on the documentation page that the documentation is not up-to-date. In particular, there is a `-full` option that is needed if one wants all fields to be outputted. Re-reading the solution mentioned above, I think I now understand the reason for the multiple columns for the `-g option` and for the `awk` rearrangement. Hence the following attempt.&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 1,4,5,6 -c 2,3 -o min,max -full&#xD;&#xA;    	I	3746	3909	&quot;WBGene00023193&quot;	.	-	3746	10230&#xD;&#xA;&#xD;&#xA;But this still doesn't give me two lines.&#xD;&#xA;&#xD;&#xA;Are there other tools that could do what I want efficiently?&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;### Edit: Solution&#xD;&#xA;&#xD;&#xA;According to [this answer](https://bioinformatics.stackexchange.com/a/2271/292), the problem with bedtools is that there is a bug in the latest release (2.26.0 as of august 2017). In order to have a functional `bedtools groupby`, one needs to get the development version from github.&#xD;&#xA;&#xD;&#xA;With the [github version of bedtools](https://github.com/arq5x/bedtools2/commit/52db65490b196f95bcd141a20c79ed36d9989496), I can now get the expected result as follows:&#xD;&#xA;&#xD;&#xA;    $ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 1,4,5,6 -c 2,3 -o min,max | awk -v OFS=&quot;\t&quot; '{print $1,$5,$6,$2,$3,$4}'&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;I include fields 1, 5 and 6 in `-g` (besides field 4) in order to have them printed out. In my bed file, they should be the same for a given value of field 4. The `awk` part is needed because one has apparently not total control on the output order: the `-g` fields come before the `-c` fields." />
  <row Id="5030" PostHistoryTypeId="5" PostId="2273" RevisionGUID="0e7b586c-81b5-48c5-82f1-8846a9f45376" CreationDate="2017-08-11T16:46:08.213" UserId="776" Comment="added 1728 characters in body" Text="    $ cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq | awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }' | paste -d &quot;\t&quot; - - | sed 's/_/\t/g' | sort-bed - &gt; answer.bed&#xD;&#xA;&#xD;&#xA;Given your sample input:&#xD;&#xA;&#xD;&#xA;    $ more in.bed&#xD;&#xA;    I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xD;&#xA;    I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xD;&#xA;    I   4118    4220    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4118    4358    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4220    4223    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;&#xD;&#xA;The `answer.bed` file:&#xD;&#xA;&#xD;&#xA;    $ more answer.bed&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;Sorting with `sort-bed` is useful at the end, so that you can pipe it or work with it with other BEDOPS tools, or other tools that now accept sorted BED input.&#xD;&#xA;&#xD;&#xA;Streaming is a pretty efficient way to do things, generally.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;**How this works**&#xD;&#xA;&#xD;&#xA;Here's the pipeline again:&#xD;&#xA;&#xD;&#xA;    $ cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq | awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }' | paste -d &quot;\t&quot; - - | sed 's/_/\t/g' | sort-bed - &gt; answer.bed&#xD;&#xA;&#xD;&#xA;We start by cutting columns 4 through 6 (id, score and strand), replacing tabs with underscores, sorting and removing duplicates:&#xD;&#xA;&#xD;&#xA;    cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq&#xD;&#xA;&#xD;&#xA;What we get out of this is a sorted list of &quot;needles&quot; — one for each ID-score-strand combination: an *ID-needle* — that we can use to `grep` or filter the original BED file.&#xD;&#xA;&#xD;&#xA;This list is piped to `awk` which, for each ID-needle, runs `grep` against the original BED file and pipes the subset to `bedops --merge -`, which merges overlapping intervals. The `system` command prints the merged interval on one line. The following `print $0` statement prints the needle on the next line:&#xD;&#xA;&#xD;&#xA;    awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }'&#xD;&#xA;&#xD;&#xA;We take the pair of alternating lines and re-linearize them with `paste`. This result contains four columns: the three columns of each merged interval, and the ID-needle. &#xD;&#xA;&#xD;&#xA;We then use `sed` to replace underscores with tabs, so that we turn the ID-needle back into three, tab-separated ID-score-strand columns:&#xD;&#xA;&#xD;&#xA;    paste -d &quot;\t&quot; - - | sed 's/_/\t/g'&#xD;&#xA;&#xD;&#xA;The output is now a six-column BED file, but it is ordered by the sort order we applied on ID-needles further up the pipeline, which we don't want. What we really want is BED that is sorted per BEDOPS `sort-bed`, so that we can do more set operations and get a correct result. So we pipe this to `sort-bed -` to write a sorted file to `answer.bed`:&#xD;&#xA;&#xD;&#xA;    sort-bed - &gt; answer.bed" />
  <row Id="5031" PostHistoryTypeId="5" PostId="2273" RevisionGUID="d8dfadc7-5a0d-4125-8262-59e9bc0fedf3" CreationDate="2017-08-11T16:51:49.820" UserId="776" Comment="added 1728 characters in body" Text="    $ cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq | awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }' | paste -d &quot;\t&quot; - - | sed 's/_/\t/g' | sort-bed - &gt; answer.bed&#xD;&#xA;&#xD;&#xA;Given your sample input:&#xD;&#xA;&#xD;&#xA;    $ more in.bed&#xD;&#xA;    I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xD;&#xA;    I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xD;&#xA;    I   4118    4220    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4118    4358    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xD;&#xA;    I   4220    4223    &quot;WBGene00022277&quot;    .   -&#xD;&#xA;&#xD;&#xA;The `answer.bed` file:&#xD;&#xA;&#xD;&#xA;    $ more answer.bed&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;Sorting with `sort-bed` is useful at the end, so that you can pipe it or work with it with other BEDOPS tools, or other tools that now accept sorted BED input.&#xD;&#xA;&#xD;&#xA;Streaming is a pretty efficient way to do things, generally.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;**How this works**&#xD;&#xA;&#xD;&#xA;Here's the pipeline again:&#xD;&#xA;&#xD;&#xA;    $ cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq | awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }' | paste -d &quot;\t&quot; - - | sed 's/_/\t/g' | sort-bed - &gt; answer.bed&#xD;&#xA;&#xD;&#xA;We start by cutting columns 4 through 6 (id, score and strand), replacing tabs with underscores, sorting and removing duplicates:&#xD;&#xA;&#xD;&#xA;    cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq&#xD;&#xA;&#xD;&#xA;What we get out of this is a sorted list of &quot;needles&quot; — one for each ID-score-strand combination: an *ID-needle* — that we can use to `grep` or filter the original BED file.&#xD;&#xA;&#xD;&#xA;This list is piped to `awk` which, for each ID-needle, runs `grep` against the original BED file and pipes the subset to `bedops --merge -`, which merges overlapping intervals. &#xD;&#xA;&#xD;&#xA;Note that merging only works for overlapping intervals. *Merging is not necessarily the same as returning a min-max pair, and this pipeline will break if there are intervals that do not overlap.* But you could modify the `awk` statement to process the input intervals and return the minimum and maximum interval coordinates, if that is really what you want, by tracking the min and max values over all intervals that come into `awk`, and printing a final interval with an `END` block.&#xD;&#xA;&#xD;&#xA;The `system` command prints the merged interval on one line. The following `print $0` statement prints the needle on the next line:&#xD;&#xA;&#xD;&#xA;    awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }'&#xD;&#xA;&#xD;&#xA;We take each pair of alternating lines and re-linearize them with `paste`. This result now contains four columns: the three columns of each merged interval, and the ID-needle. &#xD;&#xA;&#xD;&#xA;We then use `sed` to replace underscores with tabs, so that we turn the ID-needle back into three, tab-separated ID-score-strand columns:&#xD;&#xA;&#xD;&#xA;    paste -d &quot;\t&quot; - - | sed 's/_/\t/g'&#xD;&#xA;&#xD;&#xA;The output is now a six-column BED file, but it is ordered by the sort order we applied on ID-needles further up the pipeline, which we don't want. What we really want is BED that is sorted per BEDOPS `sort-bed`, so that we can do more set operations and get a correct result. So we pipe this to `sort-bed -` to write a sorted file to `answer.bed`:&#xD;&#xA;&#xD;&#xA;    sort-bed - &gt; answer.bed" />
  <row Id="5032" PostHistoryTypeId="5" PostId="2271" RevisionGUID="11b429be-7310-42d4-83bb-4c3145ceb3db" CreationDate="2017-08-11T17:36:04.583" UserId="1076" Comment="Adding improvements suggested by Devon Ryan" Text="Although you don't mention it, I'm guessing you're using bedtools v2.26.0. Version 2.26.0 of groupBy has a bug in it, which you've encountered (it was fixed shortly after release, so you'll either have to use a version before the bug was introduced, or compile the current source code yourself from https://github.com/arq5x/bedtools2) &#xD;&#xA;&#xD;&#xA;v2.26.0: &#xD;&#xA;&#xD;&#xA;    local10:~/Documents/tmp$ cat asdf.bed &#xD;&#xA;    I	3746	3909	WBGene00023193	.	-&#xD;&#xA;    I	3746	3909	WBGene00023193	.	-&#xD;&#xA;    I	4118	4220	WBGene00022277	.	-&#xD;&#xA;    I	4118	4358	WBGene00022277	.	-&#xD;&#xA;    I	4118	10230	WBGene00022277	.	-&#xD;&#xA;    I	4220	4223	WBGene00022277	.	-&#xD;&#xA;    local10:~/Documents/tmp$ groupBy -i asdf.bed -g 4 -c 2,3 -o min,max &#xD;&#xA;    3746	10230 &#xD;&#xA;&#xD;&#xA;v2.26.0-125-g52db654 (I.E. compiling the source code from github): &#xD;&#xA;&#xD;&#xA;    local10:~/Documents/tmp$ bedtools2/bin/groupBy -i asdf.bed -g 4 -c 2,3 -o min,max&#xD;&#xA;    WBGene00023193	3746	3909&#xD;&#xA;    WBGene00022277	4118	10230&#xD;&#xA;&#xD;&#xA;------------------------------&#xD;&#xA;To answer your questions: &#xD;&#xA;&#xD;&#xA;1) You might notice that my output above gives the grouped columns first; you'll have to reorder the output via awk in order to get it back in order. As for why they chose to group on both columns 1 and 4: if you have the same name on multiple chromosomes, you may want to treat them as separate features.&#xD;&#xA;&#xD;&#xA;2) Version differences, as stated in the first part of my answer. &#xD;&#xA;&#xD;&#xA;------------------------------&#xD;&#xA;To actually merge the file: &#xD;&#xA;&#xD;&#xA;Make sure to run this with a version other than v2.26.0 (As Devon Ryan writes in the comments, you may want to add column 6 to `-g` to make it strand-specific):&#xD;&#xA;&#xD;&#xA;    ./bedtools2/bin/groupBy -i asdf.bed -g 1,4 -c 2,3,5,6 -o min,max,first,first \&#xD;&#xA;         | awk -v OFS='\t' '{print $1, $3, $4, $2, $5, $6}'&#xD;&#xA;    I	3746	3909	WBGene00023193	.	-&#xD;&#xA;    I	4118	10230	WBGene00022277	.	-&#xD;&#xA;" />
  <row Id="5033" PostHistoryTypeId="2" PostId="2282" RevisionGUID="090caf5c-dc14-40ed-8fe8-9d59653f9f90" CreationDate="2017-08-11T18:47:12.110" UserId="206" Text="After reading some of the forum posts in Biostar and SeqAnswers I find it very confusing whether to filter out the duplicate reads from aligned files or not. As far I understand it's very difficult to distinguish between highly expressed genes and duplicate reads and we may lose important information during the filtration process.&#xD;&#xA;&#xD;&#xA;So, is it really necessary to remove the duplicates in differential expression analysis using RNA-seq data?" />
  <row Id="5034" PostHistoryTypeId="1" PostId="2282" RevisionGUID="090caf5c-dc14-40ed-8fe8-9d59653f9f90" CreationDate="2017-08-11T18:47:12.110" UserId="206" Text="Removing PCR duplicates in RNA-seq Analysis" />
  <row Id="5035" PostHistoryTypeId="3" PostId="2282" RevisionGUID="090caf5c-dc14-40ed-8fe8-9d59653f9f90" CreationDate="2017-08-11T18:47:12.110" UserId="206" Text="&lt;rna-seq&gt;&lt;differential-expression&gt;" />
  <row Id="5036" PostHistoryTypeId="2" PostId="2283" RevisionGUID="ffe91bea-9c0d-4be4-b485-0d3db1546aaf" CreationDate="2017-08-11T21:08:06.510" UserId="315" Text="Generally you should just leave them as is. One does remove/mark duplicates in DNA seq. &#xD;&#xA;&#xD;&#xA;For further read check this [Nature paper ][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.nature.com/articles/srep25533" />
  <row Id="5037" PostHistoryTypeId="2" PostId="2284" RevisionGUID="c140f9cc-6b7b-4841-ad49-2faaf2bcae81" CreationDate="2017-08-12T21:00:41.833" UserId="1322" Text="I am the developer of Uberon and I would be happy to help you with what you need, either from Uberon, or from the FMA.&#xD;&#xA;&#xD;&#xA;You mentioned you need something simpler than FMA. There are a variety of tools for creating custom subsets. Additionally, some ontologies provide ready-made subsets for particular purposes.&#xD;&#xA;&#xD;&#xA;For example, we make a subset called 'basic.obo' from Uberon:&#xD;&#xA;http://uberon.github.io/downloads.html#subsets&#xD;&#xA;&#xD;&#xA;You can examine this directly in either oboedit or Protege5 (note Protege5 parses obo so you don't need to convert).&#xD;&#xA;&#xD;&#xA;From your question, you didn't indicate you needed programmatic access, but if you do there are a variety of options, as well as more programmer-friendly JSON exports.&#xD;&#xA;&#xD;&#xA;Most ontologies have a number of different options for web-browsing, you can see some of the options for Uberon here:&#xD;&#xA;http://obofoundry.org/ontology/uberon.html&#xD;&#xA;&#xD;&#xA;You mentioned annotating disease sites. There may be a tools (either desktop or web-based) for helping with this - e.g. providing autocomplete over a desired subset of an ontology.&#xD;&#xA;&#xD;&#xA;You may also want to ask your question on a mailing list like obo-discuss. Unfortunately many ontology developers don't read stackexchange, but most are always keen to hear of community requirements, to help them provide better and more useful products." />
  <row Id="5038" PostHistoryTypeId="2" PostId="2285" RevisionGUID="ebfeb92a-332e-4968-86f2-9dfa25add677" CreationDate="2017-08-12T21:23:24.850" UserId="-1" Text="" />
  <row Id="5039" PostHistoryTypeId="1" PostId="2285" RevisionGUID="ebfeb92a-332e-4968-86f2-9dfa25add677" CreationDate="2017-08-12T21:23:24.850" UserId="-1" />
  <row Id="5040" PostHistoryTypeId="2" PostId="2286" RevisionGUID="2758c324-68f9-4a9a-899a-8d298d15b9a6" CreationDate="2017-08-12T21:23:24.850" UserId="-1" Text="" />
  <row Id="5041" PostHistoryTypeId="1" PostId="2286" RevisionGUID="2758c324-68f9-4a9a-899a-8d298d15b9a6" CreationDate="2017-08-12T21:23:24.850" UserId="-1" />
  <row Id="5042" PostHistoryTypeId="2" PostId="2287" RevisionGUID="9ca399a0-f90c-4532-8453-f08f0cb20628" CreationDate="2017-08-12T21:25:50.737" UserId="-1" Text="" />
  <row Id="5043" PostHistoryTypeId="1" PostId="2287" RevisionGUID="9ca399a0-f90c-4532-8453-f08f0cb20628" CreationDate="2017-08-12T21:25:50.737" UserId="-1" />
  <row Id="5044" PostHistoryTypeId="2" PostId="2288" RevisionGUID="e9e63db2-9593-4c66-a3ac-803dfeb73b40" CreationDate="2017-08-12T21:25:50.737" UserId="-1" Text="" />
  <row Id="5045" PostHistoryTypeId="1" PostId="2288" RevisionGUID="e9e63db2-9593-4c66-a3ac-803dfeb73b40" CreationDate="2017-08-12T21:25:50.737" UserId="-1" />
  <row Id="5046" PostHistoryTypeId="2" PostId="2289" RevisionGUID="238d258b-23b5-45e3-ae81-fac10efc0e2a" CreationDate="2017-08-12T22:46:42.073" UserId="136" Text="I'd like to know how common certain mutation *types* are in public data sets like the 1000 Genomes, ExAC, and ESP6500. Specifically, I'd like to know the distribution of stop-gains, stop-losses, frameshift, and other mutation types. For example, what is the median number of stop-gains observed across individuals in the ExAC population? I'd like to have the full distribution, but median values would work.&#xD;&#xA;&#xD;&#xA;To be clear, I am *not* looking for allele frequencies.&#xD;&#xA;&#xD;&#xA;I didn't find this kind of summary data when I looked at the respective websites. Is there a good source for these summary data?" />
  <row Id="5047" PostHistoryTypeId="1" PostId="2289" RevisionGUID="238d258b-23b5-45e3-ae81-fac10efc0e2a" CreationDate="2017-08-12T22:46:42.073" UserId="136" Text="Where can I find summary data for how common certain mutation *types* are?" />
  <row Id="5048" PostHistoryTypeId="3" PostId="2289" RevisionGUID="238d258b-23b5-45e3-ae81-fac10efc0e2a" CreationDate="2017-08-12T22:46:42.073" UserId="136" Text="&lt;public-databases&gt;&lt;1000-genomes&gt;&lt;exac&gt;&lt;esp6500&gt;" />
  <row Id="5049" PostHistoryTypeId="5" PostId="2288" RevisionGUID="73cacb9c-a92e-4d1f-9129-64a2b5723509" CreationDate="2017-08-13T07:13:57.537" UserId="1322" Comment="added 257 characters in body" Text="The Gene Ontology (GO) is a collection of annotations of genes from multiple species to terms describing the functional role of those genes. The terms are organized in a graph structure that allows for propagation of information. See http://geneontology.org" />
  <row Id="5050" PostHistoryTypeId="24" PostId="2288" RevisionGUID="73cacb9c-a92e-4d1f-9129-64a2b5723509" CreationDate="2017-08-13T07:13:57.537" Comment="Proposed by 1322 approved by 77 edit id of 268" />
  <row Id="5051" PostHistoryTypeId="5" PostId="2286" RevisionGUID="333f5a30-bc39-4a65-b43e-c9246c1ed596" CreationDate="2017-08-13T07:14:05.840" UserId="1322" Comment="added 371 characters in body" Text="Ontologies are collections of terms organized in a graph that can be used to annotate biological data, for example, tagging genes or variants with terms from disease or phenotype ontologies. The most well known bio ontology is the Gene Ontology (GO), used for functional annotation of genes. Sites such as the OBO Foundry and Bioportal can be used to find bio-ontologies." />
  <row Id="5052" PostHistoryTypeId="24" PostId="2286" RevisionGUID="333f5a30-bc39-4a65-b43e-c9246c1ed596" CreationDate="2017-08-13T07:14:05.840" Comment="Proposed by 1322 approved by 77 edit id of 267" />
  <row Id="5053" PostHistoryTypeId="2" PostId="2290" RevisionGUID="a8713663-7c58-446c-ae96-6f0069dcdc40" CreationDate="2017-08-13T21:44:02.207" UserId="1283" Text="  This weekend, I  installed the R language packages rapidGSEA and the Broad Institute GSEA , the Nvidia CUDA Toolkit 6.5-- Custom installation option  and MINGW makefiles on my son's Windows 8.1 notebook computer. &#xD;&#xA;  This step was very tricky to accomplish because I had to maneuver around the &quot;NVIDIA Installer cannot continue. The NVIDIA graphics driver is not compatible with this version of Windows&quot; installation error message which prevents the NVIDIA compilers from being installed correctly.&#xD;&#xA;  My objective is to run rapidGSEA without the CUDA enhancemnets since I am not using a computer with a NVIDIA GPU.&#xD;&#xA;  I did this step because rapidGSEA and Broad Institute GSEA have a significantly richer set of gene set enrichment analytics than fgseaL as shown below:&#xD;&#xA;&#xD;&#xA;The GSEA method takes five default arguments and three optional arguments&#xD;&#xA;GSEA &lt;- function(exprsData, labelList, geneSets, numPermutations, metricString,&#xD;&#xA;                 dumpFileName=&quot;&quot;, checkInput=TRUE, doublePrecision=FALSE) {...}&#xD;&#xA;&#xD;&#xA;exprsData, labelList and geneSets refer to the data obtained in the previous section. numPermutations denotes the number of permutations in the resampling test, metricString denotes the local ranking measure (one of the following):&#xD;&#xA;&#xD;&#xA;    naive_diff_of_classes&#xD;&#xA;    naive_ratio_of_classes&#xD;&#xA;    naive_log2_ratio_of_classes&#xD;&#xA;    stable_diff_of_classes&#xD;&#xA;    stable_ratio_of_classes&#xD;&#xA;    stable_log2_ratio_of_classes&#xD;&#xA;    onepass_signal2noise&#xD;&#xA;    onepass_t_test&#xD;&#xA;    twopass_signal2noise&#xD;&#xA;    twopass_t_test&#xD;&#xA;    stable_signal2noise&#xD;&#xA;    stable_t_test&#xD;&#xA;    overkill_signal2noise&#xD;&#xA;    overkill_t_test" />
  <row Id="5054" PostHistoryTypeId="5" PostId="2290" RevisionGUID="7799cb7c-f442-405a-8fb4-7a6335743491" CreationDate="2017-08-13T21:52:33.080" UserId="1283" Comment="corrected misspelling" Text="  This weekend, I  installed the R language packages rapidGSEA and the Broad Institute GSEA , the Nvidia CUDA Toolkit 6.5-- Custom installation option  and MINGW makefiles on my son's Windows 8.1 notebook computer. &#xD;&#xA;  This step was very tricky to accomplish because I had to maneuver around the &quot;NVIDIA Installer cannot continue. The NVIDIA graphics driver is not compatible with this version of Windows&quot; installation error message which prevents the NVIDIA compilers from being installed correctly.&#xD;&#xA;  My objective is to run rapidGSEA without the CUDA enhancements since I am not using a computer with a NVIDIA GPU.&#xD;&#xA;  I did this step because rapidGSEA and Broad Institute GSEA have a significantly richer set of gene set enrichment analytics than fgseaL as shown below:&#xD;&#xA;&#xD;&#xA;The GSEA method takes five default arguments and three optional arguments&#xD;&#xA;GSEA &lt;- function(exprsData, labelList, geneSets, numPermutations, metricString,&#xD;&#xA;                 dumpFileName=&quot;&quot;, checkInput=TRUE, doublePrecision=FALSE) {...}&#xD;&#xA;&#xD;&#xA;exprsData, labelList and geneSets refer to the data obtained in the previous section. numPermutations denotes the number of permutations in the resampling test, metricString denotes the local ranking measure (one of the following):&#xD;&#xA;&#xD;&#xA;    naive_diff_of_classes&#xD;&#xA;    naive_ratio_of_classes&#xD;&#xA;    naive_log2_ratio_of_classes&#xD;&#xA;    stable_diff_of_classes&#xD;&#xA;    stable_ratio_of_classes&#xD;&#xA;    stable_log2_ratio_of_classes&#xD;&#xA;    onepass_signal2noise&#xD;&#xA;    onepass_t_test&#xD;&#xA;    twopass_signal2noise&#xD;&#xA;    twopass_t_test&#xD;&#xA;    stable_signal2noise&#xD;&#xA;    stable_t_test&#xD;&#xA;    overkill_signal2noise&#xD;&#xA;    overkill_t_test" />
  <row Id="5055" PostHistoryTypeId="2" PostId="2291" RevisionGUID="cc934433-534f-4bab-8226-d32887c41b2c" CreationDate="2017-08-13T22:27:07.477" UserId="1024" Text="You can do this easily with [Hail][1].  Hail primarily uses BED files to annotate genetic datasets (see the last [annotate_variants_table][2] example), but you can manipulate BED files using Hail's general facilities for manipulating delimited text files.  For example:&#xD;&#xA;&#xD;&#xA;    $ cat genes.bed&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	3746	3909	&quot;WBGene00023193&quot;	.	-&#xD;&#xA;    I	4118	4220	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    I	4118	4358	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    I	4118	10230	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;    I	4220	4223	&quot;WBGene00022277&quot;	.	-&#xD;&#xA;&#xD;&#xA;The Hail script (python code):&#xD;&#xA;&#xD;&#xA;    from hail import *&#xD;&#xA;    hc = HailContext()&#xD;&#xA;    (hc&#xD;&#xA;     .import_table('genes.bed', impute=True, no_header=True)&#xD;&#xA;     .aggregate_by_key('f0 = f0, f3 = f3',&#xD;&#xA;        'f1 = f1.min(), f2 = f2.max(), f4 = &quot;.&quot;, f5 = &quot;-&quot;')&#xD;&#xA;     .select(['f0', 'f1', 'f2', 'f3', 'f4', 'f5'])&#xD;&#xA;     .export('genes_merged.bed', header=False))&#xD;&#xA;&#xD;&#xA;The result:&#xD;&#xA;&#xD;&#xA;    $ cat genes_merged.bed &#xD;&#xA;    I	3746	3909	WBGene00023193	.	-&#xD;&#xA;    I	4118	10230	WBGene00022277	.	-&#xD;&#xA;&#xD;&#xA;I aggregate over chrom and name so this solution won't merge entries on different chromosomes. The `select` is necessary to reorder the fields because `aggregate_by_key` places the keys being aggregated over first.&#xD;&#xA;&#xD;&#xA;Disclosure: I work on Hail.&#xD;&#xA;&#xD;&#xA;  [1]: https://hail.is&#xD;&#xA;  [2]: https://hail.is/docs/stable/hail.VariantDataset.html#hail.VariantDataset.annotate_variants_table" />
  <row Id="5056" PostHistoryTypeId="2" PostId="2292" RevisionGUID="ad333244-3414-407e-81c4-9ea9648778f0" CreationDate="2017-08-14T04:20:37.817" UserId="1024" Text="[Hail][1] might be an option for you.&#xD;&#xA;&#xD;&#xA;It is actively developed by a growing team at the Broad.  It is rigorously tested (continuous integration, continuous deployment, bug reports get regression tests, blah blah blah).&#xD;&#xA;&#xD;&#xA;It was designed to solve this problem (among others).  It can import a variety of formats, including VCF, TSV, UCSC BED, JSON and interval files.  (We don't have explicit support for GFF, but we can probably handle them with general facilities.  If not, get in touch and we'll add support.)  It can call out to VEP (and soon Nirvana, Illumina's VEP rewrite).  It has general facilities to transform, filter, clean and query data.&#xD;&#xA;&#xD;&#xA;What's more, we've curated a large collection of annotation resources (currently ~22 databases of annotations + VEP), hosted in a public bucket in Google cloud, and built an interactive query builder to select which resources you want to use.  Get in touch if you'd like us to add additional resources.  You can read more about it [here][2].&#xD;&#xA;&#xD;&#xA;Disclaimer: I work on Hail.&#xD;&#xA;&#xD;&#xA;  [1]: https://hail.is&#xD;&#xA;  [2]: https://hail.is/docs/stable/annotationdb.html" />
  <row Id="5057" PostHistoryTypeId="2" PostId="2293" RevisionGUID="577f55ca-3b07-497a-8344-eeabd5cfb6ff" CreationDate="2017-08-14T08:33:43.183" UserId="180" Text="Is the mappability of the centromeres in the GRCh38 genome reference similar to each other?&#xD;&#xA;&#xD;&#xA;As far as I can remember when GRCh38 came out, the sequence of the centromeres was determined by a combination of sequencing data and software prediction.&#xD;&#xA;&#xD;&#xA;Given the way the sequence of the centromeres has been determined, should we expect Illumina 2x150bp (or shorter, 2x75bp) reads to map relatively equally to all centromere sequences?" />
  <row Id="5058" PostHistoryTypeId="1" PostId="2293" RevisionGUID="577f55ca-3b07-497a-8344-eeabd5cfb6ff" CreationDate="2017-08-14T08:33:43.183" UserId="180" Text="GRCh38 centromeres mappability" />
  <row Id="5059" PostHistoryTypeId="3" PostId="2293" RevisionGUID="577f55ca-3b07-497a-8344-eeabd5cfb6ff" CreationDate="2017-08-14T08:33:43.183" UserId="180" Text="&lt;illumina&gt;&lt;grch38&gt;&lt;centromere&gt;&lt;mappability&gt;" />
  <row Id="5060" PostHistoryTypeId="2" PostId="2294" RevisionGUID="d4afa78b-da2f-40fb-8137-0080c7e3e753" CreationDate="2017-08-14T08:53:07.083" UserId="149" Text="You can use python's [`re`](https://docs.python.org/2/library/re.html#module-re) module. This also allows for getting the indices of those matches, which could be handy down the track.&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; import re&#xD;&#xA;    &gt;&gt;&gt; matches = re.finditer(r'(?=(AA))', 'AAAA')&#xD;&#xA;    &gt;&gt;&gt; indices = [match.span(1) for match in matches]&#xD;&#xA;    &gt;&gt;&gt; indices&#xD;&#xA;    [(0, 2), (1, 3), (2, 4)]&#xD;&#xA;    &gt;&gt;&gt; num_matches = len(indices)&#xD;&#xA;    &gt;&gt;&gt; num_matches&#xD;&#xA;    3&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5061" PostHistoryTypeId="5" PostId="2294" RevisionGUID="071d2795-7fe4-4d80-a58c-c88f24d7815b" CreationDate="2017-08-14T08:58:49.933" UserId="149" Comment="added 43 characters in body" Text="You can use `finditer` from python's [`re`](https://docs.python.org/2/library/re.html#module-re) module. The advantage of this approach is it allows for getting the indices of those matches, which could be handy down the track.&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; import re&#xD;&#xA;    &gt;&gt;&gt; matches = re.finditer(r'(?=(AA))', 'AAAA')&#xD;&#xA;    &gt;&gt;&gt; indices = [match.span(1) for match in matches]&#xD;&#xA;    &gt;&gt;&gt; indices&#xD;&#xA;    [(0, 2), (1, 3), (2, 4)]&#xD;&#xA;    &gt;&gt;&gt; num_matches = len(indices)&#xD;&#xA;    &gt;&gt;&gt; num_matches&#xD;&#xA;    3&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5062" PostHistoryTypeId="5" PostId="644" RevisionGUID="dc89f728-3f6b-4b78-8067-9681628ed38b" CreationDate="2017-08-14T09:51:31.817" UserId="48" Comment="minor change, compressing a link to improve readability" Text="I'll follow up to the [great answer](https://bioinformatics.stackexchange.com/a/638/292) from [Kamil S Jaron](https://bioinformatics.stackexchange.com/users/57/kamil-s-jaron):&#xD;&#xA;&#xD;&#xA;Regarding predicting what the variant (&quot;mutation&quot; is a very loaded term) will do, there are a variety of tools. Chief among these are [annovar](http://annovar.openbioinformatics.org/en/latest/) and [VEP](http://www.ensembl.org/info/docs/tools/vep/index.html). The general idea behind these is to classify the variants according to their overlap with genes, which codons they change (if any), how big that change is (e.g., changes in charge are more likely detrimental) and so on. One could also consider conservation, since if a position is highly conserved then changes in it are more likely to be detrimental.&#xD;&#xA;&#xD;&#xA;If you really want to predict how a variant will change a protein's function then that usually requires prior knowledge about the proteins in question. Eventually someone will use machine learning to cull the literature and provide good predictions, but I haven't seen that yet." />
  <row Id="5063" PostHistoryTypeId="24" PostId="644" RevisionGUID="dc89f728-3f6b-4b78-8067-9681628ed38b" CreationDate="2017-08-14T09:51:31.817" Comment="Proposed by 48 approved by 77 edit id of 269" />
  <row Id="5064" PostHistoryTypeId="2" PostId="2295" RevisionGUID="0e32a1e3-9538-4da4-ba2f-09d79aae5dd1" CreationDate="2017-08-14T10:33:13.743" UserId="235" Text="For normal RNA-seq PCR duplicates are normally kept in, but the duplication rate can be used as a quality control: The higher the duplication rate, the lower the quality. For expression analysis, it is probably best to discard high duplication rate samples, rather than deduplicate them. &#xD;&#xA;&#xD;&#xA;In general, the smaller the amount of RNA input into the library prepartion the worst the duplication. Many protocols for very low input quantities (such as single cell) include random barcodes called UMIs (Unique Molecular Identifiers). These allow PCR duplicates to be distinguished from genuinely independent molecules that just happen to have the sample mapping position. " />
  <row Id="5068" PostHistoryTypeId="2" PostId="2297" RevisionGUID="0472e24c-2278-4d5d-b70f-e96ee8867237" CreationDate="2017-08-14T10:49:09.487" UserId="73" Text="I doubt it, unless you are asking if mapping would be similarly bad for all centromeres. Here are some repetitive structures (probably not centromeric) that I've found in the nanopore reads for &quot;human&quot; sample NA12878, produced by the [nanopore-WGS consortium](https://github.com/nanopore-wgs-consortium/NA12878):&#xD;&#xA;&#xD;&#xA;[![Repetitive human reads #1][1]][1]&#xD;&#xA;&#xD;&#xA;These structures are consistent in that they repeat lots of times, but the internal patterns can be quite different. Here are a few more:&#xD;&#xA;&#xD;&#xA;[![Repetitive human reads #2][2]][2]&#xD;&#xA;&#xD;&#xA;Given that centromeres need to be uniquely linkable to a single chromosome, it would make sense to me if the centromere internal structure is unique to each chromosome.&#xD;&#xA;&#xD;&#xA;It's possible to have highly-repetitive structures which don't map to each other. While I haven't dug too deeply into the human reads, I've looked at the 5 most-compressible regions in an assembled rodent parasite (*Nippostrongylus brasiliensis*) genome, and found no internal similarities between them:&#xD;&#xA;&#xD;&#xA;[![Nippo most-compressible regions][3]][3]&#xD;&#xA;&#xD;&#xA;One of the issues with assembly from Illumina reads is that these highly-repetitive regions get collapsed into a single repeat (or at best a region up to twice the fragment length). With internal repeat units that have identity of over 98%, assembling the true sequence is very difficult, even with an exact knowledge of paired read separation. Even if this were possible, it can be impossible to correctly place a read because multiple internal units could be identical (or similarly different) to the sequenced read.&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/2D3cq.jpg&#xD;&#xA;  [2]: https://i.stack.imgur.com/SU65U.jpg&#xD;&#xA;  [3]: https://i.stack.imgur.com/GtvQz.jpg" />
  <row Id="5071" PostHistoryTypeId="2" PostId="2298" RevisionGUID="2c74ff08-61cd-41b4-b1da-04a6562bd1dc" CreationDate="2017-08-14T19:51:21.827" UserId="926" Text="What the difference between TPM and CPM when dealing with RNA seq data?&#xD;&#xA;&#xD;&#xA;What metrics would you use if you have to perform some down stream analysis other than Differential expression for eg. &#xD;&#xA;&#xD;&#xA;Clustering analysis using Hclust function and then plotting heat map to find differences in terms of expression levels, correlation and pca&#xD;&#xA;&#xD;&#xA;Is it wrong to use TPM for such analysis, if yes then when does one use TPM versus CPM. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5072" PostHistoryTypeId="1" PostId="2298" RevisionGUID="2c74ff08-61cd-41b4-b1da-04a6562bd1dc" CreationDate="2017-08-14T19:51:21.827" UserId="926" Text="Difference between CPM and TPM and which one for downstream analysis?" />
  <row Id="5073" PostHistoryTypeId="3" PostId="2298" RevisionGUID="2c74ff08-61cd-41b4-b1da-04a6562bd1dc" CreationDate="2017-08-14T19:51:21.827" UserId="926" Text="&lt;rna-seq&gt;" />
  <row Id="5074" PostHistoryTypeId="2" PostId="2299" RevisionGUID="308dffb7-e8d3-4b03-a8ad-4f724eb36841" CreationDate="2017-08-14T21:15:27.507" UserId="77" Text="You can find the various equations in [this oft-cited blog post from Harold Pimental](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/). Anyway, CPM is basically depth-normalized counts whereas TPM is length normalized (and then normalized by the length-normalized values of the other genes).&#xD;&#xA;&#xD;&#xA;If one has to choose between those two choices one typically chooses TPM for most things, since generally the length normalization is handy. Realistically, you probably want `log(TPM)` since otherwise noise in your most highly expressed genes end up driving everything." />
  <row Id="5076" PostHistoryTypeId="2" PostId="2300" RevisionGUID="27f73557-7dee-4f1e-af40-9bc77d2a19b8" CreationDate="2017-08-15T07:11:42.977" UserId="1331" Text="I am getting peptides using biomaRt library in R for the gene 'BRCA1'. 27 different aminoacid sequences are returned, and 12 of those have an asterisk (*) in the end, while 15 do not. What does that mean? I know that asterisk is used to indicate stop codon. So, does that mean the peptides without an asterisk do not have a stop codon? Is that possible at all? Are all of those valid peptides or not?&#xD;&#xA;&#xD;&#xA;Below is the R code I used: &#xD;&#xA;&#xD;&#xA;    library(biomaRt);&#xD;&#xA;    mart = useMart('ensembl', dataset='hsapiens_gene_ensembl');&#xD;&#xA;    seq = getSequence(id='BRCA1', type='hgnc_symbol', seqType='peptide', mart = mart);&#xD;&#xA;&#xD;&#xA;`MDLSALRVEEVQNVINAMQKILECPICLELIKEPVSTKCDHIFCKVLLCCPSWSTVVRS*` is an example sequence with an asterisk in the end. `MDLSALRVEEVQNVINAMQKILECPICLELIKEPVSTKCDHIFCKSLQESTRFSQLVEEL‌​LKIICAFQLDTGLEYANSYN‌​FAKKENNSPEHLKDEVSI` is an example without an asterisk." />
  <row Id="5077" PostHistoryTypeId="1" PostId="2300" RevisionGUID="27f73557-7dee-4f1e-af40-9bc77d2a19b8" CreationDate="2017-08-15T07:11:42.977" UserId="1331" Text="Why some of the gene peptides returned by biomaRt do not have an asterisk in the end?" />
  <row Id="5078" PostHistoryTypeId="3" PostId="2300" RevisionGUID="27f73557-7dee-4f1e-af40-9bc77d2a19b8" CreationDate="2017-08-15T07:11:42.977" UserId="1331" Text="&lt;gene&gt;&lt;proteins&gt;&lt;biomart&gt;" />
  <row Id="5079" PostHistoryTypeId="2" PostId="2301" RevisionGUID="904c8eec-ff8b-43f9-8522-8166187b6d52" CreationDate="2017-08-15T07:25:59.873" UserId="77" Text="If you [look at the original data at Ensembl](http://www.ensembl.org/Homo_sapiens/Transcript/Summary?g=ENSG00000012048;r=17:43093586-43125364;t=ENST00000477152) you'll notice that most of these are labeled &quot;CDS 3' incomplete&quot; and have a TSL (transcript support level) of 1, which is as low as it goes. It seems likely that that is simply an incomplete annotation. I'm not surprised that there are a bunch of these for BRCA1, since there was a long time when there were a LOT of legal issues surrounding working on it." />
  <row Id="5080" PostHistoryTypeId="2" PostId="2302" RevisionGUID="fb3bdb96-7bbd-4633-bfc7-794558324de7" CreationDate="2017-08-15T08:15:32.233" UserId="108" Text="Say I have reads that overlap some genes that produce small RNAs, but I want only those reads that start at exactly the TSS of the loci. In other words, reads whose 5' end match the 5' end of a genomic feature. &#xD;&#xA;&#xD;&#xA;```&#xD;&#xA;   5'....3'&#xD;&#xA;...+++++++... Gene&#xD;&#xA;...0000000... R1&#xD;&#xA;...00000000.. R2&#xD;&#xA;..000000000.. R3&#xD;&#xA;..00000000... R4&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;The output should be R1 and R2.&#xD;&#xA;&#xD;&#xA;I believe this is a fairly common bioinformatics operation but somehow it doesn't seem to be an option in the general use tools I looked at (py-bedtools, samtools, bedops). &#xD;&#xA;&#xD;&#xA;A solution I thought of would be:&#xD;&#xA;&#xD;&#xA;1. reduce the gene coordinates to its 5' end, and &#xD;&#xA;2. using bedtools `bedClosest` to annotate the distance between reads and genes, and finally &#xD;&#xA;3. select reads that overlap (d=0) and write them to a (bam) file.&#xD;&#xA;&#xD;&#xA;This involves quite a bit of wrangling data with `pybedtools` or `awk/bash`, and I wonder is there is a more elegant solution to this. Solutions in `R/Bioconductor` are also welcome.&#xD;&#xA;" />
  <row Id="5081" PostHistoryTypeId="1" PostId="2302" RevisionGUID="fb3bdb96-7bbd-4633-bfc7-794558324de7" CreationDate="2017-08-15T08:15:32.233" UserId="108" Text="How to filter intervals (reads or genomic coordinates) that have the exact same 5' or 3' ends?" />
  <row Id="5082" PostHistoryTypeId="3" PostId="2302" RevisionGUID="fb3bdb96-7bbd-4633-bfc7-794558324de7" CreationDate="2017-08-15T08:15:32.233" UserId="108" Text="&lt;bedtools&gt;" />
  <row Id="5083" PostHistoryTypeId="2" PostId="2303" RevisionGUID="dce6510d-ad09-405c-9485-883f99361eef" CreationDate="2017-08-15T08:33:27.510" UserId="77" Text="Presuming you have deepTools installed:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    #/!usr/bin/env python&#xD;&#xA;    from deeptoolsintervals.parse import GTF&#xD;&#xA;    import pysam&#xD;&#xA;&#xD;&#xA;    anno = GTF([&quot;some/gtf/or/bed/file&quot;, &quot;or/more/than/one/if you like/&quot;])&#xD;&#xA;    bam = pysam.AlignmentFile(&quot;something.bam&quot;)&#xD;&#xA;    for read in bam:&#xD;&#xA;        s = read.reference_start&#xD;&#xA;        if read.is_reverse:&#xD;&#xA;            s = read.reference_end&#xD;&#xA;        e = s + 1&#xD;&#xA;        overlaps = anno.findOverlaps(read.reference_name s, e, matchType=4)&#xD;&#xA;        # For a GTF, parse the results to see if this is the correct type.&#xD;&#xA;        # For a BED3/BED6 file there will be no exons, so anything other than&#xD;&#xA;        # None indicates an appropriate overlap&#xD;&#xA;&#xD;&#xA;        do something&#xD;&#xA;&#xD;&#xA;`deeptoolsintervals` in the GTF/BED parsing and custom interval tree library used by deepTools. `anno` holds the interval tree and the associated information (gene/transcript name or ID, exons, strand, etc.). the `matchType=4` is the pivotal part since that specifies that only overlaps having the exact same start position should be included. Assuming you have a stranded library, you might instead do something like switch between `matchType=4` (the same start position) and `matchType=5` (the same end position) depending on the orientation of the alignment and the the `strand=` of the gene.&#xD;&#xA;&#xD;&#xA;Note that you'll need to make a `set()` to hold read names to find the mates (that or jump around the file to get them, which is probably slower)." />
  <row Id="5088" PostHistoryTypeId="5" PostId="2301" RevisionGUID="bea3b2f8-df1a-497e-9140-b952c8c3c661" CreationDate="2017-08-15T09:11:38.693" UserId="77" Comment="added 7 characters in body" Text="If you [look at the original data at Ensembl](http://www.ensembl.org/Homo_sapiens/Transcript/Summary?g=ENSG00000012048;r=17:43093586-43125364;t=ENST00000477152) you'll notice that most of these are labeled &quot;CDS 3' incomplete&quot; &lt;s&gt;and have a TSL (transcript support level) of 1, which is as low as it goes&lt;/s&gt;. It seems likely that that is simply an incomplete annotation. I'm not surprised that there are a bunch of these for BRCA1, since there was a long time when there were a LOT of legal issues surrounding working on it." />
  <row Id="5089" PostHistoryTypeId="2" PostId="2305" RevisionGUID="23c64fd5-b0b6-41a5-a81b-da855986a7cd" CreationDate="2017-08-15T09:45:02.523" UserId="29" Text="Neither CPM nor TPM are well suited here, because neither performs robust cross-sample normalisation (see the blog post Devon linked to).&#xD;&#xA;&#xD;&#xA;DESeq2 provides two robust log-space normalisation methods for downstream analysis, the *regularised log* (`rlog`), and the *variance stabilising transformation* (`vst`). The [DESeq2 vignette explains how to use these](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#data-transformations-and-visualization) for things like hclust.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;On a more general note, CPM does not account for transcript length differences, while TPM does. If the choice is between TPM and CPM I would therefore use TPM. However, if you are only comparing the same transcripts across experiments, the transcript length is actually invariant so it doesn’t matter (but CPM is still not a good cross-experiment normalisation)." />
  <row Id="5090" PostHistoryTypeId="2" PostId="2306" RevisionGUID="76f2052d-0497-40fd-87f5-f06ad7c8c943" CreationDate="2017-08-15T10:46:20.913" UserId="294" Text="`fast5` is a variant of [`HDF5`][1] the native format in which raw data from Oxford Nanopore MinION basecalling are provided. You can easily extract the reads in fast5 format into a standard fastq format, using for example [`poretools`][2]. &#xD;&#xA;&#xD;&#xA;Say I have aligned these reads in `fastq` format to an external reference genome, resulting in a `SAM` file. Say I have then taken a subset of the  `SAM` file, according to the [bitwise flag][3], to include only the reads that map to the reference. With the read ID, I can then grep them out from the file containing the reads in `fastq` format, generating a subset file in `fastq` format containing only the IDs that have mapped to the reference.  &#xD;&#xA;&#xD;&#xA;Now my question is, can we subset reads from the `fast5` archive according to the list of mapping reads as taken from the file with reads in `fastq` format? This is for educational purposes, so that we have a smaller starting archive, and the `fast5` -&gt; `fastq` extraction takes less cpu time. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.hdfgroup.org/HDF5/whatishdf5.html&#xD;&#xA;  [2]: https://poretools.readthedocs.io/en/latest/&#xD;&#xA;  [3]: https://samtools.github.io/hts-specs/SAMv1.pdf" />
  <row Id="5091" PostHistoryTypeId="1" PostId="2306" RevisionGUID="76f2052d-0497-40fd-87f5-f06ad7c8c943" CreationDate="2017-08-15T10:46:20.913" UserId="294" Text="How to convert fastq to fast5" />
  <row Id="5092" PostHistoryTypeId="3" PostId="2306" RevisionGUID="76f2052d-0497-40fd-87f5-f06ad7c8c943" CreationDate="2017-08-15T10:46:20.913" UserId="294" Text="&lt;nanopore&gt;&lt;software-recommendation&gt;&lt;fastq&gt;&lt;format-conversion&gt;&lt;minion&gt;" />
  <row Id="5096" PostHistoryTypeId="2" PostId="2308" RevisionGUID="3043541a-c644-4169-8c89-354308d6c024" CreationDate="2017-08-15T16:34:21.093" UserId="1334" Text="I'm comparing the results that I obtain when doing a DE analysis with the Wald test and the likelihood-ratio test. One the thing that I've noticed is that there are many genes with 'beta' close to zero that are considered differentially expressed between the conditions.&#xD;&#xA;&#xD;&#xA;I know that the likelihood-ratio test does't use the beta values to calculate the p-values, but I find it strange that transcripts with similar expression values between the conditions are being considered differentially expressed.&#xD;&#xA;&#xD;&#xA;**Volcano plot (Wald test q-values):**&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;**Volcano plot (likelihood-ratio test q-values):**&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/ATPtp.png&#xD;&#xA;  [2]: https://i.stack.imgur.com/hovqx.png" />
  <row Id="5097" PostHistoryTypeId="1" PostId="2308" RevisionGUID="3043541a-c644-4169-8c89-354308d6c024" CreationDate="2017-08-15T16:34:21.093" UserId="1334" Text="Question: Transcripts with beta close to zero are considered differentially expressed in a likelihood-ratio test in sleuth" />
  <row Id="5098" PostHistoryTypeId="3" PostId="2308" RevisionGUID="3043541a-c644-4169-8c89-354308d6c024" CreationDate="2017-08-15T16:34:21.093" UserId="1334" Text="&lt;rna-seq&gt;&lt;differential-expression&gt;" />
  <row Id="5099" PostHistoryTypeId="5" PostId="2136" RevisionGUID="56e768d1-3feb-4dd8-ac91-b557221fe8f6" CreationDate="2017-08-15T17:50:38.403" UserId="1087" Comment="deleted 4 characters in body" Text="I'm not sure what kinds of bioinformatics tasks you would like to perform, therefore it is difficult to give a good recommendation.&#xD;&#xA;&#xD;&#xA;If you're specifically working on statistical genetics, I can recommend [Hail](https://hail.is) [1]. Hail is an open-source tool for analyzing genetics data at the tens of terabyte scale. Most of Hail's users do their science in Jupyter notebooks that are backed by Google Cloud Platform Dataproc clusters. Hail permits you to perform a variety of statistical genetics tasks including:&#xD;&#xA;&#xD;&#xA; - filtering and aggregation for quality control&#xD;&#xA; - subsetting, linear regression, linear mixed model regression, and linear burden testing&#xD;&#xA; - utilities for computing various measures of relatedness&#xD;&#xA; - principal components analysis&#xD;&#xA; - variant splitting&#xD;&#xA; - import/export from a variety of formats including PLINK, VCF, and BGEN, and&#xD;&#xA; - a python API which enables the use of libraries like matplotlib for plotting analysis results&#xD;&#xA;&#xD;&#xA;To learn specifically about using Hail with the Google Cloud Platform and Jupyter notebooks, I strongly recommend [Liam's Hail forum post about his cloud-tools repository](http://discuss.hail.is/t/using-hail-with-jupyter-notebooks-on-google-cloud/196/2).&#xD;&#xA;&#xD;&#xA;Here's an example, from the [Hail tutorial](https://hail.is/hail/tutorials/hail-overview.html#Quality-Control), of using Hail to perform some quality control and display a scatter plot of the first two principal components of the individuals:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-py --&gt;&#xD;&#xA;&#xD;&#xA;    from hail import *&#xD;&#xA;    import matplotlib.pyplot as plt&#xD;&#xA;    import matplotlib.patches as mpatches&#xD;&#xA;    &#xD;&#xA;    hc = HailContext()&#xD;&#xA;    &#xD;&#xA;    table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample')&#xD;&#xA;    common_vds = (hc.read('data/1kg.vds')&#xD;&#xA;                  .annotate_samples_table(table, root='sa')&#xD;&#xA;                  .sample_qc()&#xD;&#xA;                  .filter_samples_expr('sa.qc.dpMean &gt;= 4 &amp;&amp; sa.qc.callRate &gt;= 0.97')&#xD;&#xA;                  .filter_genotypes('''let ab = g.ad[1] / g.ad.sum() in&#xD;&#xA;                             ((g.isHomRef &amp;&amp; ab &lt;= 0.1) ||&#xD;&#xA;                              (g.isHet &amp;&amp; ab &gt;= 0.25 &amp;&amp; ab &lt;= 0.75) ||&#xD;&#xA;                              (g.isHomVar &amp;&amp; ab &gt;= 0.9))''')&#xD;&#xA;                  .variant_qc()&#xD;&#xA;                  .filter_variants_expr('va.qc.AF &gt; 0.01')&#xD;&#xA;                  .ld_prune(memory_per_core=512, num_cores=4))&#xD;&#xA;    &#xD;&#xA;    pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen')&#xD;&#xA;    pca_table = pca.samples_table().to_pandas()&#xD;&#xA;    &#xD;&#xA;    colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}&#xD;&#xA;    plt.scatter(pca_table[&quot;sa.pca.PC1&quot;], pca_table[&quot;sa.pca.PC2&quot;],&#xD;&#xA;                c = pca_table[&quot;sa.SuperPopulation&quot;].map(colors),&#xD;&#xA;                alpha = .5)&#xD;&#xA;    plt.xlim(-0.6, 0.6)&#xD;&#xA;    plt.xlabel(&quot;PC1&quot;)&#xD;&#xA;    plt.ylabel(&quot;PC2&quot;)&#xD;&#xA;    legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]&#xD;&#xA;    plt.legend(handles=legend_entries, loc=2)&#xD;&#xA;    plt.show()&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[1] Disclaimer: I work on Hail" />
  <row Id="5100" PostHistoryTypeId="2" PostId="2309" RevisionGUID="12c829d2-61a0-4d02-861b-4c5bd6dcb1e8" CreationDate="2017-08-15T18:44:00.690" UserId="73" Text="I do this very frequently, using the read file name to identify FAST5 files associated with particular reads. If a FASTQ record includes the channel number and read number (and preferably the runID as well), then I use that information to find the associated FAST5 read." />
  <row Id="5101" PostHistoryTypeId="2" PostId="2310" RevisionGUID="2dddbd57-a35d-4fe6-bc85-9679e156bc9c" CreationDate="2017-08-16T01:27:43.727" UserId="823" Text="In my RNAseq dataset of differentiated stem-cell lines, some samples have far fewer significantly differentially expressed genes than others. QC shows that this is because there are way fewer reads for one sample than the others.&#xD;&#xA;&#xD;&#xA;Can gene co-expression networks be used to infer differential expression? For example, if certain genes just miss the cutoff q-value cutoff  If so, what papers have done so? " />
  <row Id="5102" PostHistoryTypeId="1" PostId="2310" RevisionGUID="2dddbd57-a35d-4fe6-bc85-9679e156bc9c" CreationDate="2017-08-16T01:27:43.727" UserId="823" Text="Can gene co-expression networks be used to help identify differentially expressed genes?" />
  <row Id="5103" PostHistoryTypeId="3" PostId="2310" RevisionGUID="2dddbd57-a35d-4fe6-bc85-9679e156bc9c" CreationDate="2017-08-16T01:27:43.727" UserId="823" Text="&lt;rna-seq&gt;&lt;differential-expression&gt;&lt;networks&gt;" />
  <row Id="5104" PostHistoryTypeId="2" PostId="2311" RevisionGUID="3e0a2cfa-17b8-46f6-bf7a-fb9e3fb29f29" CreationDate="2017-08-16T04:30:09.763" UserId="149" Text="This is something I have been meaning to get around to for a while, so thanks for the prompt.&#xD;&#xA;&#xD;&#xA;I have [created a python script](https://github.com/mbhall88/fast5_in_fastq) to do what (I think) you're after. Yes, another Nanopore script. Because the world doesn't have enough of them.&#xD;&#xA;&#xD;&#xA;As you have mentioned this is for educational purposes I have added a tonne of comments to the code too so I think you shouldn't have any issues following it.&#xD;&#xA;&#xD;&#xA;The docs on the [GitHub repo](https://github.com/mbhall88/fast5_in_fastq) have all the info, but for those reading along at home&#xD;&#xA;&#xD;&#xA;    git clone https://github.com/mbhall88/fast5_in_fastq.git&#xD;&#xA;    cd fast5_in_fastq&#xD;&#xA;    ./fast5_in_fastq -i &lt;fast5_dir&gt; -f &lt;in.fastq&gt; -o &lt;out.txt&gt;&#xD;&#xA;&#xD;&#xA;What it does is read in `&lt;in.fastq&gt;` and extract the read id from each header. It then goes through all the fast5 files under `&lt;fast_dir&gt;` and checks whether their read id is in the set of read ids from `&lt;in.fastq&gt;`. If it is, the path to the file is written to it's own line in `&lt;out.txt&gt;`.&#xD;&#xA;&#xD;&#xA;If no output (`-o`) is given, it will write the output to stdout.&#xD;&#xA;&#xD;&#xA;So if you wanted to pipe these paths into another program, you could do something like&#xD;&#xA;&#xD;&#xA;    mkdir subset_dir&#xD;&#xA;    ./fast5_in_fastq -i &lt;/path/to/fast5s/&gt; -f &lt;in.fastq&gt; | xargs cp -t subset_dir/&#xD;&#xA;&#xD;&#xA;The above example would copy the fast5 files that are found in your fastq to `subset_dir/`.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5105" PostHistoryTypeId="6" PostId="2306" RevisionGUID="98f334cc-6e89-471e-94bd-6ea763810403" CreationDate="2017-08-16T05:40:06.967" UserId="294" Comment="edited tags" Text="&lt;nanopore&gt;&lt;software-recommendation&gt;&lt;fastq&gt;&lt;format-conversion&gt;&lt;fast5&gt;" />
  <row Id="5106" PostHistoryTypeId="2" PostId="2312" RevisionGUID="2616d33e-7807-435a-b9a4-455b3bdab5b9" CreationDate="2017-08-16T06:41:26.923" UserId="77" Text="While you can use networks to find differentially expressed genes (see the [WGCNA](https://labs.genetics.ucla.edu/horvath/CoexpressionNetwork/Rpackages/WGCNA/) package, which does this) in my experience this ends up largely matching what you'd get using a traditional package with a looser threshold for significance. Given the time savings of traditional packages, there's rarely any gain to using networks (it won't hurt to try, just note that it'll take some time). If some very interesting genes are just above your significance threshold then change your threshold. You need to validate your findings in some way anyway, so your p-value threshold is partly just a way to protect you from wasting your time (but if background knowledge suggests that your time wouldn't be wasted...)." />
  <row Id="5107" PostHistoryTypeId="5" PostId="2309" RevisionGUID="660bf2b6-a5ca-44a7-8a9e-6ae46eddc503" CreationDate="2017-08-16T07:38:17.043" UserId="73" Comment="added 452 characters in body" Text="I do this very frequently, using the read file name to identify FAST5 files associated with particular reads. If a FASTQ record includes the channel number and read number (and preferably the runID as well), then I use that information to find the associated FAST5 read.&#xD;&#xA;&#xD;&#xA;If reads are called with Albacore, then the `sequencing_summary.txt` file has additional information. The first column is the exact name of the file that was called, the second column is the read ID of the FASTQ sequence, and the third column is the run ID associated with that read ID. This is much easier to work with, but does require calling reads with Albacore (which, admittedly, is what seems to be producing the best results at the moment)." />
  <row Id="5108" PostHistoryTypeId="5" PostId="2311" RevisionGUID="af50b285-3a2c-4a6c-b748-491efdefead1" CreationDate="2017-08-16T08:23:40.657" UserId="149" Comment="added 284 characters in body" Text="This is something I have been meaning to get around to for a while, so thanks for the prompt.&#xD;&#xA;&#xD;&#xA;I have [created a python script](https://github.com/mbhall88/fast5_in_ref) to do what (I think) you're after. Yes, another Nanopore script. Because the world doesn't have enough of them.&#xD;&#xA;&#xD;&#xA;As you have mentioned this is for educational purposes I have added a tonne of comments to the code too so I think you shouldn't have any issues following it.&#xD;&#xA;&#xD;&#xA;The docs on the [GitHub repo](https://github.com/mbhall88/fast5_in_ref) have all the info, but for those reading along at home&#xD;&#xA;&#xD;&#xA;    git clone https://github.com/mbhall88/fast5_in_ref.git&#xD;&#xA;    cd fast5_in_ref&#xD;&#xA;    ./fast5_in_ref -i &lt;fast5_dir&gt; -r &lt;in.fastq|in.bam|in.sam&gt; -o &lt;out.txt&gt;&#xD;&#xA;&#xD;&#xA;What it does is read in `&lt;in.fastq|in.bam|in.sam&gt;` and extract the read id from each header. It then goes through all the fast5 files under `&lt;fast_dir&gt;` and checks whether their read id is in the set of read ids from `&lt;in.fastq|in.bam|in.sam&gt;`. If it is, the path to the file is written to it's own line in `&lt;out.txt&gt;`.&#xD;&#xA;&#xD;&#xA;If no output (`-o`) is given, it will write the output to stdout.&#xD;&#xA;&#xD;&#xA;So if you wanted to pipe these paths into another program, you could do something like&#xD;&#xA;&#xD;&#xA;    mkdir subset_dir&#xD;&#xA;    ./fast5_in_ref -i &lt;fast5_dir&gt; -r &lt;in.fastq|in.bam|in.sam&gt; | xargs cp -t subset_dir/&#xD;&#xA;&#xD;&#xA;The above example would copy the fast5 files that are found in your fastq/BAM/SAM to `subset_dir/`.&#xD;&#xA;&#xD;&#xA;**EDIT**:&#xD;&#xA;I have now added compatibility for BAM and SAM files (was only fastq originally). These are potentially even more useful for this application than fastq. As a result I have updated all examples, plus the code and links on GitHub" />
  <row Id="5109" PostHistoryTypeId="2" PostId="2313" RevisionGUID="3a302cbe-a897-4826-85e3-182bebf493a3" CreationDate="2017-08-16T08:27:56.623" UserId="206" Text="Co-expression network will give you an idea about the genes having similar expression patterns and the nodes will be decided on the basis of correlation scores and nothing much you will get from differential gene expression perspective.&#xD;&#xA;&#xD;&#xA;I would suggest you to try some other models for differential expression analysis like [baySeq][2] , [DESeq][3], [edgeR][4], [NOIseq][5]. Depending upon the model you use for analysis you might get some extra significant DEG. Also, try different significance cutoff and see how it's changing the analysis result to get the threshold that suits the data best.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://labs.genetics.ucla.edu/horvath/CoexpressionNetwork/Rpackages/WGCNA/&#xD;&#xA;  [2]: http://bioconductor.org/packages/release/bioc/html/baySeq.html&#xD;&#xA;  [3]: http://bioconductor.org/packages/release/bioc/html/DESeq2.html&#xD;&#xA;  [4]: https://bioconductor.org/packages/release/bioc/html/edgeR.html&#xD;&#xA;  [5]: https://www.bioconductor.org/packages/release/bioc/html/NOISeq.html" />
  <row Id="5110" PostHistoryTypeId="2" PostId="2314" RevisionGUID="51b92d89-ea1c-47df-a6fd-540f1145b191" CreationDate="2017-08-16T08:49:48.813" UserId="1332" Text="If it's 3' incomplete that means the evidence used to create it was a fragment. Here's the evidence used to construct BRCA1-214 ENST00000477152.5, a 3' incomplete:&#xD;&#xA;http://www.ensembl.org/Homo_sapiens/Transcript/SupportingEvidence?db=core;g=ENSG00000012048;r=17:43093586-43125364;t=ENST00000477152&#xD;&#xA;&#xD;&#xA;You can see that there's a full length cDNA from EMBL, AK307553.1, which was used to create this model:&#xD;&#xA;http://www.ebi.ac.uk/ena/data/view/AK307553.1&#xD;&#xA;&#xD;&#xA;The sequence was mapped against the genome sequence to create the transcript model. When searching for the translation, it was found that the open reading frame was started, but there was no stop codon. This suggests that the cDNA AK307553.1 is actually a fragment, that the mRNA was broken or cleaved before it was reverse-transcribed sequenced. We display it in Ensembl in the hope that this will lead someone to identify the full length transcript that it represents.&#xD;&#xA;&#xD;&#xA;Many people choose not to work with these incomplete transcripts. You can do this by filtering by Gencode Basic in biomaRt." />
  <row Id="5111" PostHistoryTypeId="5" PostId="2306" RevisionGUID="d3c2bd4a-0ca3-4e1e-8d82-44fca9835dfc" CreationDate="2017-08-16T08:52:58.910" UserId="149" Comment="Basecalling can be output in fastq alone. fast5 is designed first and foremost for raw data. But basecalling data can be added to a fast5 file." Text="`fast5` is a variant of [`HDF5`][1] the native format in which raw data from Oxford Nanopore MinION are provided. You can easily extract the reads in fast5 format into a standard fastq format, using for example [`poretools`][2]. &#xD;&#xA;&#xD;&#xA;Say I have aligned these reads in `fastq` format to an external reference genome, resulting in a `SAM` file. Say I have then taken a subset of the  `SAM` file, according to the [bitwise flag][3], to include only the reads that map to the reference. With the read ID, I can then grep them out from the file containing the reads in `fastq` format, generating a subset file in `fastq` format containing only the IDs that have mapped to the reference.  &#xD;&#xA;&#xD;&#xA;Now my question is, can we subset reads from the `fast5` archive according to the list of mapping reads as taken from the file with reads in `fastq` format? This is for educational purposes, so that we have a smaller starting archive, and the `fast5` -&gt; `fastq` extraction takes less cpu time. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://support.hdfgroup.org/HDF5/whatishdf5.html&#xD;&#xA;  [2]: https://poretools.readthedocs.io/en/latest/&#xD;&#xA;  [3]: https://samtools.github.io/hts-specs/SAMv1.pdf" />
  <row Id="5112" PostHistoryTypeId="24" PostId="2306" RevisionGUID="d3c2bd4a-0ca3-4e1e-8d82-44fca9835dfc" CreationDate="2017-08-16T08:52:58.910" Comment="Proposed by 149 approved by 77 edit id of 270" />
  <row Id="5113" PostHistoryTypeId="5" PostId="2311" RevisionGUID="08f15af9-c062-4b6f-a43a-63b9b551a882" CreationDate="2017-08-16T09:40:20.417" UserId="149" Comment="added 25 characters in body" Text="This is something I have been meaning to get around to for a while, so thanks for the prompt.&#xD;&#xA;&#xD;&#xA;I have [created a python script](https://github.com/mbhall88/fast5_in_ref) to do what (I think) you're after. Yes, another Nanopore script. Because the world doesn't have enough of them.&#xD;&#xA;&#xD;&#xA;As you have mentioned this is for educational purposes I have added a tonne of comments to the code too so I think you shouldn't have any issues following it.&#xD;&#xA;&#xD;&#xA;The docs on the [GitHub repo](https://github.com/mbhall88/fast5_in_ref) have all the info, but for those reading along at home&#xD;&#xA;&#xD;&#xA;    git clone https://github.com/mbhall88/fast5_in_ref.git&#xD;&#xA;    cd fast5_in_ref &amp;&amp; chmod +x fast5_in_ref&#xD;&#xA;    ./fast5_in_ref -i &lt;fast5_dir&gt; -r &lt;in.fastq|in.bam|in.sam&gt; -o &lt;out.txt&gt;&#xD;&#xA;&#xD;&#xA;What it does is read in `&lt;in.fastq|in.bam|in.sam&gt;` and extract the read id from each header. It then goes through all the fast5 files under `&lt;fast_dir&gt;` and checks whether their read id is in the set of read ids from `&lt;in.fastq|in.bam|in.sam&gt;`. If it is, the path to the file is written to it's own line in `&lt;out.txt&gt;`.&#xD;&#xA;&#xD;&#xA;If no output (`-o`) is given, it will write the output to stdout.&#xD;&#xA;&#xD;&#xA;So if you wanted to pipe these paths into another program, you could do something like&#xD;&#xA;&#xD;&#xA;    mkdir subset_dir&#xD;&#xA;    ./fast5_in_ref -i &lt;fast5_dir&gt; -r &lt;in.fastq|in.bam|in.sam&gt; | xargs cp -t subset_dir/&#xD;&#xA;&#xD;&#xA;The above example would copy the fast5 files that are found in your fastq/BAM/SAM to `subset_dir/`.&#xD;&#xA;&#xD;&#xA;**EDIT**:&#xD;&#xA;I have now added compatibility for BAM and SAM files (was only fastq originally). These are potentially even more useful for this application than fastq. As a result I have updated all examples, plus the code and links on GitHub" />
  <row Id="5118" PostHistoryTypeId="2" PostId="2316" RevisionGUID="198d2dae-0818-478c-a121-6a0bdbf4cce5" CreationDate="2017-08-17T02:33:01.107" UserId="163" Text="I am running a slow downstream analysis on a large set of nanopore reads (approx 3 million), and would like to split them into smaller chunks, run the analysis in massively parallel, and then recombine. Originally I just split the FASTQ into chunks, re-aligned each chunk, and then merged the output, but here I would like to use an existing alignment so I can compare results with existing analyses (i.e. the alignments must be the same).&#xD;&#xA;&#xD;&#xA;How can I efficiently split a FASTQ file and a BAM file giving the alignment of the FASTA file into chunks, ensuring that all of the reads in FASTQ chunk 1 are in BAM chunk 1, vice versa and so on?&#xD;&#xA;&#xD;&#xA;My FASTQ is approximately 45GB and my BAM is 33GB, so I would prefer to avoid storing one of the two files in memory if possible." />
  <row Id="5119" PostHistoryTypeId="1" PostId="2316" RevisionGUID="198d2dae-0818-478c-a121-6a0bdbf4cce5" CreationDate="2017-08-17T02:33:01.107" UserId="163" Text="Split FASTQ and matching BAM into matching chunks" />
  <row Id="5120" PostHistoryTypeId="3" PostId="2316" RevisionGUID="198d2dae-0818-478c-a121-6a0bdbf4cce5" CreationDate="2017-08-17T02:33:01.107" UserId="163" Text="&lt;bam&gt;&lt;fasta&gt;&lt;nanopore&gt;&lt;fastq&gt;&lt;subset&gt;" />
  <row Id="5121" PostHistoryTypeId="5" PostId="2316" RevisionGUID="f83d4f4c-7f21-42c8-8528-4096e879e8a4" CreationDate="2017-08-17T03:41:07.107" UserId="163" Comment="fix typo" Text="I am running a slow downstream analysis on a large set of nanopore reads (approx 3 million), and would like to split them into smaller chunks, run the analysis in massively parallel, and then recombine. Originally I just split the FASTQ into chunks, re-aligned each chunk, and then merged the output, but here I would like to use an existing alignment so I can compare results with existing analyses (i.e. the alignments must be the same).&#xD;&#xA;&#xD;&#xA;How can I efficiently split a FASTQ file and a BAM file giving the alignment of the FASTA file into chunks, ensuring that all of the reads in FASTQ chunk 1 are in BAM chunk 1, vice versa and so on?&#xD;&#xA;&#xD;&#xA;My FASTQ is approximately 45GB and my BAM is 33GB, so I would prefer to avoid storing one of the two files in memory if possible.&#xD;&#xA;&#xD;&#xA;EDIT: Here's some pseudocode of exactly what I'm trying to do:&#xD;&#xA;&#xD;&#xA;    # input: in.bam, in.fastq, chunk_size&#xD;&#xA;    i &lt;- 0&#xD;&#xA;    for fastq_read in in.fastq:&#xD;&#xA;        bam_read &lt;- extract fastq_read.read_name from in.bam&#xD;&#xA;        n &lt;- i modulo chunk_size&#xD;&#xA;        write fastq_read to out.n.fastq&#xD;&#xA;        write bam_read to out.n.bam&#xD;&#xA;        i &lt;- i + 1&#xD;&#xA;&#xD;&#xA;I could swap the above to iterate through the bam file and fetch from the fastq if that's easier." />
  <row Id="5122" PostHistoryTypeId="2" PostId="2317" RevisionGUID="6d813d3d-7f4d-4dbc-8f39-c9d71315cce6" CreationDate="2017-08-17T07:33:10.510" UserId="149" Text="Hmm, it's hard to think of a super efficient way of doing this (assuming the files aren't ordered the same - if they are then this whole answer is basically redundant). And also assuming the read ids for both files aren't a perfect intersection.&#xD;&#xA;&#xD;&#xA;Off the top of my head you probably want to build a set of read ids for the fastq file and another for the bam. Some python code to get started on that:&#xD;&#xA;&#xD;&#xA;    import pysam&#xD;&#xA;    import itertools&#xD;&#xA;    &#xD;&#xA;    def get_read_id_fastq(ref_path):&#xD;&#xA;        &quot;&quot;&quot;Extracts the read ids from a fastq file.&quot;&quot;&quot;&#xD;&#xA;        read_ids = set()&#xD;&#xA;        with open(ref_path, 'r') as ref:&#xD;&#xA;            for line in ref:&#xD;&#xA;                if line.startswith('@'):  # i.e if line is header&#xD;&#xA;                    # split the line on spaces, take the first element, remove @&#xD;&#xA;                    read_id = line.split(' ')[0].replace('@', '')&#xD;&#xA;                    read_ids.add(read_id)&#xD;&#xA;    &#xD;&#xA;        return read_ids&#xD;&#xA;    &#xD;&#xA;    def get_read_id_bam(ref_path):&#xD;&#xA;        &quot;&quot;&quot;Extract the read ids from a BAM file.&quot;&quot;&quot;&#xD;&#xA;    &#xD;&#xA;        read_ids = set()&#xD;&#xA;        with pysam.AlignmentFile(ref_path, 'r', ignore_truncation=True) as ref:&#xD;&#xA;            for read in ref:&#xD;&#xA;                # query_name is the query template name&#xD;&#xA;                read_ids.add(read.query_name)&#xD;&#xA;    &#xD;&#xA;        return read_ids&#xD;&#xA;    &#xD;&#xA;    fastq_ids = get_read_id_fastq(fastq_path)&#xD;&#xA;    bam_ids = get_read_id_bam(bam_path)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Then take the intersection of these two sets and that's your common read ids. &#xD;&#xA;&#xD;&#xA;    common_ids = fastq_ids &amp; bam_ids&#xD;&#xA;&#xD;&#xA;The next part will be a bit more involved. You will have to iterate through each file one at a time. I would suggest that for the first one you iterate through, create a running dictionary with the read id that was written as the key and the 'chuck number' it was written to as the value. You could create a cycle for your chunk size to manage this effectively&#xD;&#xA;&#xD;&#xA;    chuck_cycle = itertools.chain(*zip(range(chunk_size)*len(common_ids)))&#xD;&#xA;    write_idx = {}&#xD;&#xA;&#xD;&#xA;The next part will probably require you to put in some hard-fought times getting it to work. I'll put in some rough pseudo-code to give you an idea.&#xD;&#xA;&#xD;&#xA;    for line in file:&#xD;&#xA;        read_id = # get line read_id&#xD;&#xA;        if read_id in common_ids:&#xD;&#xA;            chunk_num = chunk_cycle.next()&#xD;&#xA;            write_idx[read_id] = chunk_num&#xD;&#xA;            file_to_write_to = 'out.{}.bam'.format(chunk_num)&#xD;&#xA;            # open this file or write to it if already open&#xD;&#xA;&#xD;&#xA;When you go to do the next file when you find a read id is in the common set, you look up it's value in `write_idx` and this will give you the chunk number to write to. &#xD;&#xA;&#xD;&#xA;The reads wont be in the exact same order between the two files, but you could sort later if you needed it (not sure it would matter?).&#xD;&#xA;&#xD;&#xA;Hope this helps. Sorry I couldn't give you more, but this should give you a head start hopefully.&#xD;&#xA;" />
  <row Id="5124" PostHistoryTypeId="5" PostId="2317" RevisionGUID="2284c9a0-2999-4f6f-8476-1140da71b4cc" CreationDate="2017-08-17T13:00:35.147" UserId="149" Comment="edited body" Text="Hmm, it's hard to think of a super efficient way of doing this (assuming the files aren't ordered the same - if they are then this whole answer is basically redundant). And also assuming the read ids for both files aren't a perfect intersection.&#xD;&#xA;&#xD;&#xA;Off the top of my head you probably want to build a set of read ids for the fastq file and another for the bam. Some python code to get started on that:&#xD;&#xA;&#xD;&#xA;    import pysam&#xD;&#xA;    import itertools&#xD;&#xA;    &#xD;&#xA;    def get_read_id_fastq(ref_path):&#xD;&#xA;        &quot;&quot;&quot;Extracts the read ids from a fastq file.&quot;&quot;&quot;&#xD;&#xA;        read_ids = set()&#xD;&#xA;        with open(ref_path, 'r') as ref:&#xD;&#xA;            for line in ref:&#xD;&#xA;                if line.startswith('@'):  # i.e if line is header&#xD;&#xA;                    # split the line on spaces, take the first element, remove @&#xD;&#xA;                    read_id = line.split(' ')[0].replace('@', '')&#xD;&#xA;                    read_ids.add(read_id)&#xD;&#xA;    &#xD;&#xA;        return read_ids&#xD;&#xA;    &#xD;&#xA;    def get_read_id_bam(ref_path):&#xD;&#xA;        &quot;&quot;&quot;Extract the read ids from a BAM file.&quot;&quot;&quot;&#xD;&#xA;    &#xD;&#xA;        read_ids = set()&#xD;&#xA;        with pysam.AlignmentFile(ref_path, 'r', ignore_truncation=True) as ref:&#xD;&#xA;            for read in ref:&#xD;&#xA;                # query_name is the query template name&#xD;&#xA;                read_ids.add(read.query_name)&#xD;&#xA;    &#xD;&#xA;        return read_ids&#xD;&#xA;    &#xD;&#xA;    fastq_ids = get_read_id_fastq(fastq_path)&#xD;&#xA;    bam_ids = get_read_id_bam(bam_path)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Then take the intersection of these two sets and that's your common read ids. &#xD;&#xA;&#xD;&#xA;    common_ids = fastq_ids &amp; bam_ids&#xD;&#xA;&#xD;&#xA;The next part will be a bit more involved. You will have to iterate through each file one at a time. I would suggest that for the first one you iterate through, create a running dictionary with the read id that was written as the key and the 'chuck number' it was written to as the value. You could create a cycle for your chunk size to manage this effectively&#xD;&#xA;&#xD;&#xA;    chunk_cycle = itertools.chain(*zip(range(chunk_size)*len(common_ids)))&#xD;&#xA;    write_idx = {}&#xD;&#xA;&#xD;&#xA;The next part will probably require you to put in some hard-fought times getting it to work. I'll put in some rough pseudo-code to give you an idea.&#xD;&#xA;&#xD;&#xA;    for line in file:&#xD;&#xA;        read_id = # get line read_id&#xD;&#xA;        if read_id in common_ids:&#xD;&#xA;            chunk_num = chunk_cycle.next()&#xD;&#xA;            write_idx[read_id] = chunk_num&#xD;&#xA;            file_to_write_to = 'out.{}.bam'.format(chunk_num)&#xD;&#xA;            # open this file or write to it if already open&#xD;&#xA;&#xD;&#xA;When you go to do the next file when you find a read id is in the common set, you look up it's value in `write_idx` and this will give you the chunk number to write to. &#xD;&#xA;&#xD;&#xA;The reads wont be in the exact same order between the two files, but you could sort later if you needed it (not sure it would matter?).&#xD;&#xA;&#xD;&#xA;Hope this helps. Sorry I couldn't give you more, but this should give you a head start hopefully.&#xD;&#xA;" />
  <row Id="5125" PostHistoryTypeId="2" PostId="2318" RevisionGUID="281180df-7d9f-4d78-aecf-d13ef590c583" CreationDate="2017-08-17T13:10:12.177" UserId="939" Text="I recently installed Ubuntu 16.04 (because I was still using 12.04). But it seems my bedtools scripts don't work properly anymore. I can't figure out how to use the new bedtools for my old ways. What I want to do is get the number of reads from a bam file, per interval from a bed file. It was very simple with my old version of Ubuntu and bedtools:&#xD;&#xA;&#xD;&#xA;    bedtools coverage -abam file.bam -b All_peaks.bed &gt; file.cov.txt&#xD;&#xA;&#xD;&#xA;But this doesn't seem to work anymore. I used to get small files as results (~3MB), but now huge files are created (~4GB) including read names??&#xD;&#xA;&#xD;&#xA;I am using bedtools v2.25 now.&#xD;&#xA;" />
  <row Id="5126" PostHistoryTypeId="1" PostId="2318" RevisionGUID="281180df-7d9f-4d78-aecf-d13ef590c583" CreationDate="2017-08-17T13:10:12.177" UserId="939" Text="How to count reads in bam per bed interval with bedtools" />
  <row Id="5127" PostHistoryTypeId="3" PostId="2318" RevisionGUID="281180df-7d9f-4d78-aecf-d13ef590c583" CreationDate="2017-08-17T13:10:12.177" UserId="939" Text="&lt;bam&gt;&lt;bedtools&gt;" />
  <row Id="5128" PostHistoryTypeId="2" PostId="2319" RevisionGUID="eeccd7b0-c4ed-478f-8052-152546329d09" CreationDate="2017-08-17T13:21:05.450" UserId="77" Text="The order of `-a` and `-b` switched at some point. You want:&#xD;&#xA;&#xD;&#xA;    bedtools coverage -a All_peaks.bed -b file.bam &gt; file.cov.txt&#xD;&#xA;&#xD;&#xA;For reference, this is the end of the help output in version 2.25:&#xD;&#xA;&#xD;&#xA;    Default Output:  &#xD;&#xA;	     After each entry in A, reports: &#xD;&#xA;    	   1) The number of features in B that overlapped the A interval.&#xD;&#xA;    	   2) The number of bases in A that had non-zero coverage.&#xD;&#xA;    	   3) The length of the entry in A.&#xD;&#xA;    	   4) The fraction of bases in A that had non-zero coverage.&#xD;&#xA;&#xD;&#xA;And this is the equivalent output from version 2.19:&#xD;&#xA;&#xD;&#xA;    Default Output:  &#xD;&#xA;    	 After each entry in B, reports: &#xD;&#xA;    	   1) The number of features in A that overlapped the B interval.&#xD;&#xA;    	   2) The number of bases in B that had non-zero coverage.&#xD;&#xA;    	   3) The length of the entry in B.&#xD;&#xA;    	   4) The fraction of bases in B that had non-zero coverage.&#xD;&#xA;&#xD;&#xA;**Update**: The change in behavior happened in version 2.24:&#xD;&#xA;&#xD;&#xA;&gt; We have changed the behavior of the coverage tool such that it is consistent with the other tools. Specifically, coverage is now&#xD;&#xA;computed for the intervals in the A file based on the overlaps with the B file, rather than vice versa." />
  <row Id="5132" PostHistoryTypeId="5" PostId="385" RevisionGUID="eeaa701e-97f8-41b2-8b98-a5c9f07a525e" CreationDate="2017-08-17T14:05:01.363" UserId="29" Comment="deleted 15 characters in body" Text="Let’s start with what they have in common: All three formats store&#xD;&#xA;&#xD;&#xA;1. sequence data, and&#xD;&#xA;2. sequence metadata.&#xD;&#xA;&#xD;&#xA;Furthermore, all three formats are text-based.&#xD;&#xA;&#xD;&#xA;However, beyond that all three formats are different and serve different purposes.&#xD;&#xA;&#xD;&#xA;Let’s start with the simplest format:&#xD;&#xA;&#xD;&#xA;### FASTA&#xD;&#xA;&#xD;&#xA;FASTA stores a variable number of sequence records, and for each record it stores the sequence itself, and a sequence ID. Each record starts with a header line whose first character is `&gt;`, followed by the sequence ID. The next lines of a record contain the actual sequence.&#xD;&#xA;&#xD;&#xA;The [Wikipedia artice](https://en.wikipedia.org/wiki/FASTA_format) gives several examples for peptide sequences, but since FASTQ and SAM are used exclusively (?) for nucleotide sequences, here’s a nucleotide example:&#xD;&#xA;&#xD;&#xA;    &gt;Mus_musculus_tRNA-Ala-AGC-1-1 (chr13.trna34-AlaAGC)&#xD;&#xA;    GGGGGTGTAGCTCAGTGGTAGAGCGCGTGCTTAGCATGCACGAGGcCCTGGGTTCGATCC&#xD;&#xA;    CCAGCACCTCCA&#xD;&#xA;    &gt;Mus_musculus_tRNA-Ala-AGC-10-1 (chr13.trna457-AlaAGC)&#xD;&#xA;    GGGGGATTAGCTCAAATGGTAGAGCGCTCGCTTAGCATGCAAGAGGtAGTGGGATCGATG&#xD;&#xA;    CCCACATCCTCCA&#xD;&#xA;&#xD;&#xA;The ID can be in any arbitrary format, although [several conventions exist](https://en.wikipedia.org/wiki/FASTA_format#Sequence_identifiers).&#xD;&#xA;&#xD;&#xA;In the context of nucleotide sequences, FASTA is mostly used to store reference data; that is, data extracted from a curated database; the above is adapted from [GtRNAdb](http://gtrnadb.ucsc.edu/) (a database of tRNA sequences).&#xD;&#xA;&#xD;&#xA;### FASTQ&#xD;&#xA;&#xD;&#xA;FASTQ was conceived to solve a specific problem of FASTA files: when sequencing, the confidence in a given [base call](https://biology.stackexchange.com/a/1873/166) (that is, the identity of a nucleotide) varies. This is expressed in the [Phred quality score](https://en.wikipedia.org/wiki/Phred_quality_score). FASTA had no standardised way of encoding this. By contrast, a FASTQ record [contains a sequence of quality scores](https://en.wikipedia.org/wiki/FASTQ_format#Encoding) for each nucleotide.&#xD;&#xA;&#xD;&#xA;A FASTQ record has the following format:&#xD;&#xA;&#xD;&#xA;1. A line starting with `@`, containing the sequence ID.&#xD;&#xA;2. One or more lines that contain the sequence.&#xD;&#xA;3. A new line starting with the character `+`, and being either empty or repeating the sequence ID.&#xD;&#xA;4. One or more lines that contain the quality scores.&#xD;&#xA;&#xD;&#xA;Here’s an example of a FASTQ file with two records:&#xD;&#xA;&#xD;&#xA;    @071112_SLXA-EAS1_s_7:5:1:817:345&#xD;&#xA;    GGGTGATGGCCGCTGCCGATGGCGTC&#xD;&#xA;    AAATCCCACC&#xD;&#xA;    +&#xD;&#xA;    IIIIIIIIIIIIIIIIIIIIIIIIII&#xD;&#xA;    IIII9IG9IC&#xD;&#xA;    @071112_SLXA-EAS1_s_7:5:1:801:338&#xD;&#xA;    GTTCAGGGATACGACGTTTGTATTTTAAGAATCTGA&#xD;&#xA;    +&#xD;&#xA;    IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII6IBI&#xD;&#xA;&#xD;&#xA;FASTQ files are mostly used to store short-read data from high-throughput sequencing experiments. As a consequence, the sequence and quality scores are usually put into a single line each, and indeed many tools assume that each record in a FASTQ file is exactly four lines long, even though this isn’t guaranteed.&#xD;&#xA;&#xD;&#xA;As for FASTA, the format of the sequence ID isn’t standardised, but different producers of FASTQ use [fixed notations that follow strict conventions](https://en.wikipedia.org/wiki/FASTQ_format#Illumina_sequence_identifiers).&#xD;&#xA;&#xD;&#xA;### SAM&#xD;&#xA;&#xD;&#xA;SAM files are so complex that a [complete description](https://samtools.github.io/hts-specs/SAMv1.pdf) &lt;sup&gt;[PDF]&lt;/sup&gt; takes 15 pages. So here’s the short version.&#xD;&#xA;&#xD;&#xA;The original purpose of SAM files is to store mapping information for sequences from high-throughput sequencing. As a consequence, a SAM record needs to store more than just the sequence and its quality, it also needs to store information about where and how a sequence maps into the reference.&#xD;&#xA;&#xD;&#xA;Unlike the previous formats, SAM is tab-based, and each record, consisting of either 11 or 12 fields, fills exactly one line. Here’s an example (tabs replaced by fixed-width spacing):&#xD;&#xA;&#xD;&#xA;    r001  99  chr1  7 30  17M         =  37  39  TTAGATAAAGGATACTG   IIIIIIIIIIIIIIIII&#xD;&#xA;    r002  0   chrX  9 30  3S6M1P1I4M  *  0   0   AAAAGATAAGGATA      IIIIIIIIII6IBI    NM:i:1&#xD;&#xA;&#xD;&#xA;For a description of the individual fields, refer to the documentation. The relevant bit is this: SAM can express exactly the same information as FASTQ, plus, as mentioned, the mapping information. However, SAM is also used to store read data *without* mapping information.&#xD;&#xA;&#xD;&#xA;In addition to sequence records, SAM files can also contain a *header*, which stores information about the reference that the sequences were mapped to, and the tool used to create the SAM file. Header information precede the sequence records, and consist of lines starting with `@`.&#xD;&#xA;&#xD;&#xA;SAM itself is almost never used as a storage format; instead, files are stored in BAM format, which is a compact binary representation of SAM. It stores the same information, just more efficiently, and in conjunction with a [search index](https://en.wikipedia.org/wiki/Database_index), allows fast retrieval of individual records from the middle of the file (= fast [random access](https://en.wikipedia.org/wiki/Random_access)). BAM files are also much more compact than compressed FASTQ or FASTA files.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;The above implies a *hierarchy* in what the formats can store: FASTA ⊂ FASTQ ⊂ SAM.&#xD;&#xA;&#xD;&#xA;In a typical high-throughput analysis workflow, you will encounter all three file types:&#xD;&#xA;&#xD;&#xA;1. FASTA to store the reference genome/transcriptome that the sequence fragments will be mapped to.&#xD;&#xA;2. FASTQ to store the sequence fragments before mapping.&#xD;&#xA;3. SAM/BAM to store the sequence fragments after mapping." />
  <row Id="5137" PostHistoryTypeId="2" PostId="2321" RevisionGUID="3d368579-cfc5-4ad6-87f0-cbf997221ec9" CreationDate="2017-08-17T16:42:31.383" UserId="823" Text="I have not found any work which investigates assessment of differences in levels of secreted proteins by taking advantage of differential expression of the genes which mediate the secretory pathway.&#xD;&#xA;&#xD;&#xA;For example, suppose the gene whose product binds specifically to a certain signal peptide is down-regulated. I would assume that this implies a down-regulation in all the levels of secreted proteins to which it binds.&#xD;&#xA;&#xD;&#xA;Is such an analysis possible to date? If so, where? &#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="5138" PostHistoryTypeId="1" PostId="2321" RevisionGUID="3d368579-cfc5-4ad6-87f0-cbf997221ec9" CreationDate="2017-08-17T16:42:31.383" UserId="823" Text="Using signal peptide and the expression levels of signal recognition particle in secretome analysis" />
  <row Id="5139" PostHistoryTypeId="3" PostId="2321" RevisionGUID="3d368579-cfc5-4ad6-87f0-cbf997221ec9" CreationDate="2017-08-17T16:42:31.383" UserId="823" Text="&lt;proteins&gt;&lt;differential-expression&gt;&lt;networks&gt;&lt;interactions&gt;" />
  <row Id="5140" PostHistoryTypeId="2" PostId="2322" RevisionGUID="2673e745-512b-4e81-942c-73fc35f4b073" CreationDate="2017-08-17T19:47:04.523" UserId="1250" Text="Here is my solution to the problem. I am posting it here in case anyone else come up with the same idea but did not know how to formulate it in mathematical notation!&#xD;&#xA;[![enter image description here][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/Jah8y.png" />
  <row Id="5141" PostHistoryTypeId="2" PostId="2323" RevisionGUID="5f6abd9c-3333-490d-9729-672b24a23117" CreationDate="2017-08-18T00:23:15.570" UserId="1343" Text="I'm trying to confirm that the sequence of a novel gene is derived by exon shuffling between several different genes. I have the promoter sequence, gene sequence, and mRNA (with defined exon/intro boundaries). I've tried performing several database searches with the sequences, but each search produces a set of different unrelated hits. How would one go about using this info to confirm the exon shuffling hypothesis? Any tips are appreciated. Thanks!" />
  <row Id="5142" PostHistoryTypeId="1" PostId="2323" RevisionGUID="5f6abd9c-3333-490d-9729-672b24a23117" CreationDate="2017-08-18T00:23:15.570" UserId="1343" Text="How to confirm exon shuffling in a gene?" />
  <row Id="5143" PostHistoryTypeId="3" PostId="2323" RevisionGUID="5f6abd9c-3333-490d-9729-672b24a23117" CreationDate="2017-08-18T00:23:15.570" UserId="1343" Text="&lt;sequencing&gt;&lt;sequence-analysis&gt;" />
  <row Id="5145" PostHistoryTypeId="2" PostId="2324" RevisionGUID="fd59ea97-4979-49e0-82e3-dd9c075905e4" CreationDate="2017-08-18T03:12:20.163" UserId="506" Text="I'm currently working with data from a Luminex multiplex assay. In this assay, the concentrations of 17 different analyte proteins were identified in 12 groups in triplicate. One of these 17 groups was used as the control, and the log2 fold changes were calculated for the analyte concentration of each sample in each group using the average control concentration for that analyte.&#xD;&#xA;&#xD;&#xA;However, now I would like to calculate a p-value for the identified fold changes if possible.&#xD;&#xA;&#xD;&#xA;My current preliminary idea is to perform the t test for each group compared with the control group for each analyte, but I don't think that my group sizes are large enough for this. My statistical knowledge is lacking, so if there is a better way of calculating these p-values I don't know about it.&#xD;&#xA;&#xD;&#xA;Is there a better way of obtaining these p-values?" />
  <row Id="5146" PostHistoryTypeId="1" PostId="2324" RevisionGUID="fd59ea97-4979-49e0-82e3-dd9c075905e4" CreationDate="2017-08-18T03:12:20.163" UserId="506" Text="How to calculate p-values for fold changes?" />
  <row Id="5147" PostHistoryTypeId="3" PostId="2324" RevisionGUID="fd59ea97-4979-49e0-82e3-dd9c075905e4" CreationDate="2017-08-18T03:12:20.163" UserId="506" Text="&lt;statistics&gt;" />
  <row Id="5148" PostHistoryTypeId="6" PostId="2323" RevisionGUID="989b1fbb-4499-44e1-bf5a-90d854806879" CreationDate="2017-08-18T04:32:55.503" UserId="96" Comment="added tag" Text="&lt;sequencing&gt;&lt;sequence-analysis&gt;&lt;molecular-genetics&gt;" />
  <row Id="5150" PostHistoryTypeId="6" PostId="14" RevisionGUID="4631ccd2-1a09-44e7-8ad2-c5f57dd6e37c" CreationDate="2017-08-18T04:33:47.617" UserId="96" Comment="added tag" Text="&lt;file-formats&gt;&lt;fasta&gt;&lt;fastq&gt;&lt;sam&gt;" />
  <row Id="5151" PostHistoryTypeId="6" PostId="2282" RevisionGUID="f6bc7321-6969-4940-a104-4c11269f410e" CreationDate="2017-08-18T04:38:32.127" UserId="96" Comment="added tag" Text="&lt;rna-seq&gt;&lt;differential-expression&gt;&lt;quality-control&gt;" />
  <row Id="5153" PostHistoryTypeId="4" PostId="2308" RevisionGUID="bf850435-08f3-48f6-8d18-7be3937a770a" CreationDate="2017-08-18T04:41:53.457" UserId="96" Comment="edited title" Text="Sleuth: transcripts with beta close to 0 are considered differentially expressed in a likelihood-ratio test" />
  <row Id="5154" PostHistoryTypeId="2" PostId="2325" RevisionGUID="3f6b6115-5336-4565-8b14-d4fff63632e5" CreationDate="2017-08-18T04:54:39.033" UserId="37" Text="&gt; should we expect Illumina 2x150bp (or shorter, 2x75bp) reads to map relatively equally to all centromere sequences?&#xD;&#xA;&#xD;&#xA;No. It has long been established that different chromosomes are associated with different centromeric sequences. It is sometimes possible to tell which chr a read is originated from based on its sequence.&#xD;&#xA;&#xD;&#xA;The GRCh38 centromeres are trickier. As I remember, the centromeric sequences were computationally generated with a Markov chain (or something alike) modeled after Venter's genome. GRC can distinguish most chromosomes, but not all. Some alpha arrays are placed onto 2 or 4 chromosomes. The original GRCh38 keeps all 4 copies. When you download GRCh38 for mapping purposes, only one copy is retained; additional copies are hard masked along with PARs on chrY.&#xD;&#xA;&#xD;&#xA;If you want to know more, see [this paper](http://genome.cshlp.org/content/early/2017/04/07/gr.213611.116.abstract) and [this](https://www.ncbi.nlm.nih.gov/pubmed/24501022)." />
  <row Id="5155" PostHistoryTypeId="2" PostId="2326" RevisionGUID="0fc7a911-aa04-47dd-b3a9-4ac78daa1be6" CreationDate="2017-08-18T05:08:01.100" UserId="206" Text="So to get a p value you must have one null hypothesis like the fold changes are significant or not. Then you test the hypothesis against the data and calculate the p value (likeliness or unlikeliness) for a given threshold.&#xD;&#xA;&#xD;&#xA;In your case, t test is good enough to calculate the p-value.&#xD;&#xA;&#xD;&#xA;http://willett.ece.wisc.edu/wp-uploads/2016/01/05b-TandP.pdf" />
  <row Id="5158" PostHistoryTypeId="2" PostId="2327" RevisionGUID="8a6c1a66-cc62-4baa-b769-f017326e2b1b" CreationDate="2017-08-18T06:51:59.027" UserId="77" Text="Your null hypothesis would be that the fold-changes are 0, so you can either do the T-test accordingly or simply do away with the fold changes and perform the T-test between the raw values in the group (this is preferable to performing a T-test of one group vs. 0, since it allows you to assess the expected variability around 0). Note, however, that 3 samples is pretty much the bare minimum needed for any statistics, so your power will be terrible and your estimation of variance will likely be pretty inaccurate. Consequently, take any p-values with an appropriate grain of salt.&#xD;&#xA;&#xD;&#xA;To my knowledge there aren't any good alternatives for your situation without having a good expected background distribution." />
  <row Id="5159" PostHistoryTypeId="2" PostId="2328" RevisionGUID="90ffb2ac-a325-48a0-b24c-b1da9e27e397" CreationDate="2017-08-18T08:58:29.290" UserId="1092" Text="You can't calculate a p-value on the fold-change values, you need to use the  concentrations in triplicate thus giving a measure of the variance for the t-test to use. &#xD;&#xA;&#xD;&#xA;t-test assumes your data are normally distributed, if they aren't you're going to get spurious p-values. If you aren't sure a non-parametric test like Wilcoxon is better. It will be less sensitive although with only 3 replicates your experiment is low powered anyway." />
  <row Id="5160" PostHistoryTypeId="2" PostId="2329" RevisionGUID="c175aeb4-d17a-45a0-bb7a-57be2059e91c" CreationDate="2017-08-18T10:13:12.797" UserId="123" Text="I'm trying to understand the magnitude of batch effects in my RNA-seq samples, and I was wondering which expression units are more suitable to draw a PCA. I'm thinking of either `counts` or `TPM`, but things like [`rlog` or `vst`][1] could work too.&#xD;&#xA;&#xD;&#xA;Additionally, I'm wondering whether either of this units should be log-transformed first, to avoid high-abundance transcripts driving the PCA.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#data-transformations-and-visualization" />
  <row Id="5161" PostHistoryTypeId="1" PostId="2329" RevisionGUID="c175aeb4-d17a-45a0-bb7a-57be2059e91c" CreationDate="2017-08-18T10:13:12.797" UserId="123" Text="Which measure should be used in a PCA or RNA-seq data? TPM or counts?" />
  <row Id="5162" PostHistoryTypeId="3" PostId="2329" RevisionGUID="c175aeb4-d17a-45a0-bb7a-57be2059e91c" CreationDate="2017-08-18T10:13:12.797" UserId="123" Text="&lt;rna-seq&gt;" />
  <row Id="5163" PostHistoryTypeId="2" PostId="2330" RevisionGUID="3126ba0e-b7bf-4158-b4a2-5992d4cb1e9c" CreationDate="2017-08-18T10:17:53.487" UserId="77" Text="tldr: log transform counts and TPMs, but rlog/vst are preferred&#xD;&#xA;&#xD;&#xA;TPM should be log transformed to get more useful results. If you're using DESeq2 already (given the reference to `rlog` and `vst`, this seems likely), then please go ahead and use `rlog` or `vst`. That will give you more reasonable results than raw counts. If you're stuck with counts for some reason, then first use normalized counts so they're at least a bit more comparable and then log transform them so your highly expressed genes aren't driving everything." />
  <row Id="5164" PostHistoryTypeId="2" PostId="2331" RevisionGUID="96c19979-c155-4cda-a432-47565abe1867" CreationDate="2017-08-18T10:56:05.363" UserId="235" Text="The key point here is whether or not the values are approximately normally distributed and whether any transformations can be applied to make them so. &#xD;&#xA;&#xD;&#xA;For a t-test, the most important thing is that there is no relationship between the mean and the variance. &#xD;&#xA;&#xD;&#xA;Its also correct that you don't want to divide by the mean of the controls. You want to retain all the information. &#xD;&#xA;&#xD;&#xA;So here is how i'd start, first calculate the mean and standard deviation of each of your 18 groups (17 treatments + control), and plot this on a scatter plot (i.e. mean on X, SD on Y).&#xD;&#xA;&#xD;&#xA; Is there a relationship between the two?&#xD;&#xA;&#xD;&#xA;Repeat the exercise with log transformed values. Is the relationship more or less.&#xD;&#xA;&#xD;&#xA;Pick the set of values that has the weakest mean-SD relationship. I'm guessing the log transformed might well be better.&#xD;&#xA;&#xD;&#xA;If X = (x_1, x_2, x_3) is your controls and Y_i = (y_(i,1), y_(i,2), y_(i,3)) are the treated samples from treatment i, to test the difference between treatment and control, do a t-test on X and Y (not `T/mean(c)`)&#xD;&#xA;&#xD;&#xA;for example in R&#xD;&#xA;&#xD;&#xA;    X = c(x_1, x_2, x_3)&#xD;&#xA;    Y_1 = c(y_11, y_12, y_13)&#xD;&#xA;    t.test (X, Y_1)&#xD;&#xA;&#xD;&#xA;or in excel is if the three values for X are in column A and the three values for Y_1 in column B&#xD;&#xA;&#xD;&#xA;    =T.TEST(A1:A3,B1:B3,2,3)&#xD;&#xA;&#xD;&#xA;If you are going to do 17 of these, remember to do a multiple testing correction by multiplying the resulting p-values by 17. &#xD;&#xA;&#xD;&#xA;If you want to compare treatment 1 (Y_1) vs treatment 2 (Y_2) do the comparison directly, not taking the control into account:&#xD;&#xA;&#xD;&#xA;    t.test(Y_1, Y_2)&#xD;&#xA;&#xD;&#xA;Remember that if you use log transformed values, you calulate log fold changes using `mean(Y)-mean(X)` rather than `mean(Y)/mean(X)`&#xD;&#xA;" />
  <row Id="5165" PostHistoryTypeId="5" PostId="2330" RevisionGUID="32e28c96-2b75-45af-8595-953fd51a8933" CreationDate="2017-08-18T10:57:03.527" UserId="77" Comment="added 311 characters in body" Text="tldr: log transform counts and TPMs, but rlog/vst are preferred&#xD;&#xA;&#xD;&#xA;TPM should be log transformed to get more useful results. If you're using DESeq2 already (given the reference to `rlog` and `vst`, this seems likely), then please go ahead and use `rlog` or `vst`. That will give you more reasonable results than raw counts. If you're stuck with counts for some reason, then first use normalized counts so they're at least a bit more comparable and then log transform them so your highly expressed genes aren't driving everything.&#xD;&#xA;&#xD;&#xA;**Edit**: As an aside, if you know what the batch effect is (e.g., library prep. date), it's sometimes convenient to include that in your model. You can then assess the genes that are actually changed due to that, which is sometimes handy to know (e.g., which genes might be more/less prone to degradation)." />
  <row Id="5166" PostHistoryTypeId="2" PostId="2332" RevisionGUID="c63dfef9-36d0-421b-8181-bc9fb4a3d09d" CreationDate="2017-08-18T11:13:10.100" UserId="73" Text="PCA assumes that the input data is approximately normally distributed on each dimension. It would be a good idea to do some initial data quality checks to verify that this is the case (and transform the data appropriately if not), or at least verify that the data is approximately normally distributed in the aggregate.&#xD;&#xA;&#xD;&#xA;For looking at Illumina RNASeq data, what worked best for me (i.e. produced the most normal-looking data) was the following steps:&#xD;&#xA;&#xD;&#xA; 1. Removing genes that had low raw counts in all samples&#xD;&#xA; 2. Using DESeq's variance-stabilized transform (which transforms counts into a log-like distribution)&#xD;&#xA; 3. Further normalising the VST values by dividing by the longest transcript length within each gene (which I call VSTPk)&#xD;&#xA;&#xD;&#xA;These steps are stated in a bit more detail in our Th2 paper that was published at the end of last year:&#xD;&#xA;&#xD;&#xA;http://jem.rupress.org/content/early/2016/12/01/jem.20160470#materials-methods" />
  <row Id="5167" PostHistoryTypeId="2" PostId="2333" RevisionGUID="c24ed5c5-46fc-49b7-a4f6-d5691dd35482" CreationDate="2017-08-18T12:00:33.640" UserId="1226" Text="I'm pretty new to all of this--forgive me if this is a simple question.&#xD;&#xA;&#xD;&#xA;When I download illumina counts from GEO (like the supplementary file here:https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi for example). Can I do comparisons directly on that file? Is there some normalization procedure I should go though?&#xD;&#xA;&#xD;&#xA;Thanks!" />
  <row Id="5168" PostHistoryTypeId="1" PostId="2333" RevisionGUID="c24ed5c5-46fc-49b7-a4f6-d5691dd35482" CreationDate="2017-08-18T12:00:33.640" UserId="1226" Text="Analyzing Illumina Counts" />
  <row Id="5169" PostHistoryTypeId="3" PostId="2333" RevisionGUID="c24ed5c5-46fc-49b7-a4f6-d5691dd35482" CreationDate="2017-08-18T12:00:33.640" UserId="1226" Text="&lt;rna-seq&gt;&lt;normalization&gt;&lt;illumina&gt;" />
  <row Id="5170" PostHistoryTypeId="5" PostId="2333" RevisionGUID="ef60ec9f-3d4c-4269-82c4-f25ab5194a7d" CreationDate="2017-08-18T12:05:03.827" UserId="1226" Comment="added 12 characters in body" Text="I'm pretty new to all of this--forgive me if this is a simple question.&#xD;&#xA;&#xD;&#xA;When I download illumina counts from GEO (like the supplementary file in &#xD;&#xA;GSE89225). Can I do comparisons directly on that file? Is there some normalization procedure I should go through?&#xD;&#xA;&#xD;&#xA;Thanks!&#xD;&#xA;" />
  <row Id="5171" PostHistoryTypeId="2" PostId="2334" RevisionGUID="db0c4401-48c8-47a4-a313-9d6b34456bed" CreationDate="2017-08-18T12:09:02.667" UserId="77" Text="The counts files for GSE89225 is the output of HTSeq-count as a large matrix. Unless you are developing a differential expression package yourself you should not attempt to directly use this. Rather, you should load it into R and use packages such as DESeq2, edgeR, or limma (those are the most popular ones).&#xD;&#xA;&#xD;&#xA;For convenience, in DESeq2 you would want the `DESeqDatasetFromMatrix()` function after loading this." />
  <row Id="5172" PostHistoryTypeId="5" PostId="2322" RevisionGUID="af74b69b-a206-4305-876d-6c6ba0cac839" CreationDate="2017-08-18T13:28:36.283" UserId="48" Comment="add correct image description" Text="Here is my solution to the problem. I am posting it here in case anyone else come up with the same idea but did not know how to formulate it in mathematical notation!&#xD;&#xA;[![mathematical representation of the rank sum][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/Jah8y.png" />
  <row Id="5173" PostHistoryTypeId="24" PostId="2322" RevisionGUID="af74b69b-a206-4305-876d-6c6ba0cac839" CreationDate="2017-08-18T13:28:36.283" Comment="Proposed by 48 approved by -1 edit id of 271" />
  <row Id="5174" PostHistoryTypeId="5" PostId="2322" RevisionGUID="543370db-391c-4b0d-83ab-056b39d6eaf6" CreationDate="2017-08-18T13:28:36.283" UserId="1250" Comment="add correct image description" Text="Here is my solution to the problem. I am posting it here in case anyone else come up with the same idea but did not know how to formulate it in mathematical notation!&#xD;&#xA;[![mathematical representation of the rank sum][1]][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://i.stack.imgur.com/k4IcO.png" />
  <row Id="5175" PostHistoryTypeId="4" PostId="2201" RevisionGUID="111afa2d-433a-461e-83bf-fd814e93b5fb" CreationDate="2017-08-18T13:29:16.580" UserId="77" Comment="edited title" Text="Mathematical notation for formulating a rank score" />
  <row Id="5176" PostHistoryTypeId="5" PostId="2322" RevisionGUID="5be21438-fbfd-44e3-a7e5-ea1a61f7a990" CreationDate="2017-08-18T17:39:08.600" UserId="29" Comment="replace image with mathematical notation" Text="Here is my solution to the problem. I am posting it here in case anyone else come up with the same idea but did not know how to formulate it in mathematical notation!&#xD;&#xA;&#xD;&#xA;&gt; $$ \psi_i = \sum_{i=1}^9 S_i $$&#xD;&#xA;&gt;&#xD;&#xA;&gt; where $S_i$ is the score function and defined as:&#xD;&#xA;&gt;&#xD;&#xA;&gt; $$&#xD;&#xA;S_i = \cases{&#xD;&#xA;  0 &amp; \text{if $\theta(x_i) &lt; Q_1$ or $\theta(x_i) = $ benign/neutral} \\&#xD;&#xA;  0.5 &amp; \text{if $Q_1 \leq \theta(x_i) &lt; Q_3$ or $\theta(x_i) = $ possibly damaging/uncertain} \\&#xD;&#xA;  1 &amp; \text{if $\theta(x_i) \geq Q_3$ or $\theta(x_i) = $ damaging}&#xD;&#xA;}&#xD;&#xA;$$&#xD;&#xA;&gt;&#xD;&#xA;&gt; The $\theta(x_i)$ is the pathogenicity or conservation score for variant $x$ as defined by model $i$ and $Q$ denotes the quartile range for scores from M-CAP, CADD, GERP and Phylop models." />
  <row Id="5178" PostHistoryTypeId="50" PostId="2098" RevisionGUID="2700af97-c2d5-4b3f-bc89-27bf5965b494" CreationDate="2017-08-18T19:42:09.680" UserId="-1" />
  <row Id="5179" PostHistoryTypeId="5" PostId="2332" RevisionGUID="521f5daf-1b0f-4838-8dd5-87d8098e9c19" CreationDate="2017-08-18T21:13:03.153" UserId="73" Comment="added 3 characters in body" Text="PCA works best when the input data is approximately normally distributed on each dimension. It would be a good idea to do some initial data quality checks to verify that this is the case (and transform the data appropriately if not), or at least verify that the data is approximately normally distributed in the aggregate.&#xD;&#xA;&#xD;&#xA;For looking at Illumina RNASeq data, what worked best for me (i.e. produced the most normal-looking data) was the following steps:&#xD;&#xA;&#xD;&#xA; 1. Removing genes that had low raw counts in all samples&#xD;&#xA; 2. Using DESeq's variance-stabilized transform (which transforms counts into a log-like distribution)&#xD;&#xA; 3. Further normalising the VST values by dividing by the longest transcript length within each gene (which I call VSTPk)&#xD;&#xA;&#xD;&#xA;These steps are stated in a bit more detail in our Th2 paper that was published at the end of last year:&#xD;&#xA;&#xD;&#xA;http://jem.rupress.org/content/early/2016/12/01/jem.20160470#materials-methods" />
  <row Id="5180" PostHistoryTypeId="2" PostId="2335" RevisionGUID="2cfeab58-ac01-44a2-9bee-88619b2e3964" CreationDate="2017-08-19T01:05:10.987" UserId="167" Text="Is there a tool that can scan fastq files without assembling them for a custom list of user defined snps?" />
  <row Id="5181" PostHistoryTypeId="1" PostId="2335" RevisionGUID="2cfeab58-ac01-44a2-9bee-88619b2e3964" CreationDate="2017-08-19T01:05:10.987" UserId="167" Text="Custom snp list finder?" />
  <row Id="5182" PostHistoryTypeId="3" PostId="2335" RevisionGUID="2cfeab58-ac01-44a2-9bee-88619b2e3964" CreationDate="2017-08-19T01:05:10.987" UserId="167" Text="&lt;fastq&gt;&lt;vcf&gt;&lt;snp&gt;" />
  <row Id="5183" PostHistoryTypeId="2" PostId="2336" RevisionGUID="ae323375-f24c-417c-868d-d4fca7e47b5d" CreationDate="2017-08-19T06:34:42.700" UserId="294" Text="Try this pipeline: &#xD;&#xA;&#xD;&#xA;1. Clean raw fastq files (trimmomatic)&#xD;&#xA;2. Align clean fastq files to the reference genome (dna-to-dna aligner of your choice)&#xD;&#xA;3. Convert the alignment sam file to bam (samtools)&#xD;&#xA;4. Call SNPs from bam file (samtools)&#xD;&#xA;&#xD;&#xA;Here's a start tutorial on this: http://ged.msu.edu/angus/tutorials-2013/snp_tutorial.html" />
  <row Id="5187" PostHistoryTypeId="5" PostId="2331" RevisionGUID="66309e92-120f-426d-ac1b-7d0027f68537" CreationDate="2017-08-19T17:55:22.827" UserId="48" Comment="add latex notation, minor grammar correction" Text="The key point here is whether or not the values are approximately normally distributed and whether any transformations can be applied to make them so. &#xD;&#xA;&#xD;&#xA;For a t-test, the most important thing is that there is no relationship between the mean and the variance. &#xD;&#xA;&#xD;&#xA;Its also correct that you don't want to divide by the mean of the controls. You want to retain all the information. &#xD;&#xA;&#xD;&#xA;So here is how i'd start, first calculate the mean and standard deviation of each of your 18 groups (17 treatments + control), and plot this on a scatter plot (i.e. mean on X, SD on Y).&#xD;&#xA;&#xD;&#xA; Is there a relationship between the two?&#xD;&#xA;&#xD;&#xA;Repeat the exercise with log transformed values. Is the relationship more or less.&#xD;&#xA;&#xD;&#xA;Pick the set of values that has the weakest mean-SD relationship. I'm guessing the log transformed might well be better.&#xD;&#xA;&#xD;&#xA;If $X = (x_1, x_2, x_3)$ is your controls and $Y_i = (y_{i,1}, y_{i,2}, y_{i,3})$ are the treated samples from treatment i, to test the difference between treatment and control, do a t-test on X and Y (not `T/mean(c)`)&#xD;&#xA;&#xD;&#xA;for example in R&#xD;&#xA;&#xD;&#xA;    X = c(x_1, x_2, x_3)&#xD;&#xA;    Y_1 = c(y_11, y_12, y_13)&#xD;&#xA;    t.test (X, Y_1)&#xD;&#xA;&#xD;&#xA;or in excel is if the three values for X are in column A and the three values for $Y_1$ in column B&#xD;&#xA;&#xD;&#xA;    =T.TEST(A1:A3,B1:B3,2,3)&#xD;&#xA;&#xD;&#xA;If you are going to do 17 of these, remember to do a multiple testing correction by multiplying the resulting p-values by 17. &#xD;&#xA;&#xD;&#xA;If you want to compare treatment 1 ($Y_1$) vs treatment 2 ($Y_2$) do the comparison directly, not taking the control into account:&#xD;&#xA;&#xD;&#xA;    t.test(Y_1, Y_2)&#xD;&#xA;&#xD;&#xA;Remember that if you use log transformed values, you calculate log fold changes using `mean(Y)-mean(X)` rather than `mean(Y)/mean(X)`&#xD;&#xA;" />
  <row Id="5188" PostHistoryTypeId="24" PostId="2331" RevisionGUID="66309e92-120f-426d-ac1b-7d0027f68537" CreationDate="2017-08-19T17:55:22.827" Comment="Proposed by 48 approved by 77 edit id of 272" />
  <row Id="5194" PostHistoryTypeId="2" PostId="2338" RevisionGUID="b6828ac5-2e52-4f69-b6bf-593b5c9f529c" CreationDate="2017-08-19T19:58:06.747" UserId="1250" Text="We all have used freemix score at some stage to check contamination or swaps in our sequencing experiment, but can anyone once for all explain how the score is calculated? " />
  <row Id="5195" PostHistoryTypeId="1" PostId="2338" RevisionGUID="b6828ac5-2e52-4f69-b6bf-593b5c9f529c" CreationDate="2017-08-19T19:58:06.747" UserId="1250" Text="VerifyBamID freemix score" />
  <row Id="5196" PostHistoryTypeId="3" PostId="2338" RevisionGUID="b6828ac5-2e52-4f69-b6bf-593b5c9f529c" CreationDate="2017-08-19T19:58:06.747" UserId="1250" Text="&lt;bam&gt;&lt;ngs&gt;" />
  <row Id="5197" PostHistoryTypeId="2" PostId="2339" RevisionGUID="25634f4e-2de7-4686-aeaa-bfe8891f5dff" CreationDate="2017-08-20T11:15:18.257" UserId="1249" Text="What are the relative advantages / disadvantages of the TM (template modelling) and GDT (global distance test, total score) scores, used in protein structure prediction competitions, such as [CASP](https://www.google.ru/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwir2PqO1-XVAhVHPhQKHcFGBF0QFggmMAA&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FCASP&amp;usg=AFQjCNENea3DBFw-bFeW2BRImlsjhtnxDA)? When should I choose one over the other? " />
  <row Id="5198" PostHistoryTypeId="1" PostId="2339" RevisionGUID="25634f4e-2de7-4686-aeaa-bfe8891f5dff" CreationDate="2017-08-20T11:15:18.257" UserId="1249" Text="Differences between TM and GDT-TS scores for structure comparison" />
  <row Id="5199" PostHistoryTypeId="3" PostId="2339" RevisionGUID="25634f4e-2de7-4686-aeaa-bfe8891f5dff" CreationDate="2017-08-20T11:15:18.257" UserId="1249" Text="&lt;protein-structure&gt;&lt;statistics&gt;&lt;modelling&gt;" />
  <row Id="5200" PostHistoryTypeId="50" PostId="842" RevisionGUID="a038e72a-c4c0-4a85-8da9-287c772e76e8" CreationDate="2017-08-20T14:57:35.197" UserId="-1" />
  <row Id="5201" PostHistoryTypeId="2" PostId="2340" RevisionGUID="f1a22810-5b52-478b-bdf0-f4caaf032b9c" CreationDate="2017-08-20T22:16:29.723" UserId="123" Text="You could use [ARIBA][1], which will map you reads to the regions of your interest (more on that in a second) and then assemble/call variants.&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;The tool is mostly used to find known variants related to AMR (antimicrobial resistance) from a set of databases, but [according to the project's wiki][3] you can build your own database with your region of interest.&#xD;&#xA;&#xD;&#xA;    Additionally, reference sequences can be either of:&#xD;&#xA;    &#xD;&#xA;        Presence/absence sequences. ARIBA will look for these sequences in the input reads and report any that it finds, and also any variants between these sequences and the input reads.&#xD;&#xA;    &#xD;&#xA;        Variants only sequences. These should have known variant details specified in the metadata file (see below). ARIBA reports only when it finds at least one of the given variants in each of these these sequences. If a sample has one of these sequences, but does not have one of the supplied variants, then it is not reported. If you supply a variants only sequence, but no variant, then the sequence will be removed during sanity checks.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.biorxiv.org/content/early/2017/04/07/118000&#xD;&#xA;  [2]: https://i.stack.imgur.com/ZHRVk.jpg&#xD;&#xA;  [3]: https://github.com/sanger-pathogens/ariba/wiki/Task%3A-prepareref#user-provided-data---fasta-and-metadata&#xD;&#xA;" />
  <row Id="5202" PostHistoryTypeId="2" PostId="2341" RevisionGUID="80eef0c9-c463-412c-92a8-3d5159bdbded" CreationDate="2017-08-21T09:45:33.583" UserId="982" Text="I would like to extract all the CDS from a batch of genomes. I have found a perl script from [BioStars][1] but this does not seem to work for me. I would preferably like to have a script/ method which will use the locus tag as a header. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/46281/" />
  <row Id="5203" PostHistoryTypeId="1" PostId="2341" RevisionGUID="80eef0c9-c463-412c-92a8-3d5159bdbded" CreationDate="2017-08-21T09:45:33.583" UserId="982" Text="How to extract / convert gff3 CDS sequences to multifasta" />
  <row Id="5204" PostHistoryTypeId="3" PostId="2341" RevisionGUID="80eef0c9-c463-412c-92a8-3d5159bdbded" CreationDate="2017-08-21T09:45:33.583" UserId="982" Text="&lt;fasta&gt;&lt;software-recommendation&gt;&lt;format-conversion&gt;&lt;gff3&gt;" />
  <row Id="5205" PostHistoryTypeId="5" PostId="2341" RevisionGUID="d46beaad-e8e0-49d8-8f11-653793cbe2b5" CreationDate="2017-08-21T09:51:14.530" UserId="982" Comment="added 524 characters in body" Text="I would like to extract all the CDS from a batch of genomes. I have found a perl script from [BioStars][1] but this does not seem to work for me. I would preferably like to have a script/ method which will use the locus tag as a header.&#xD;&#xA;&#xD;&#xA;**Example**&#xD;&#xA;&#xD;&#xA;My gff files are from prokka output&#xD;&#xA;&#xD;&#xA;    ATCC0000	Prodigal:2.6	CDS	243	434	.	+	0	ID=FKKLIMLP_00001;Parent=FKKLIMLP_00001_gene;inference=ab initio prediction:Prodigal:2.6;locus_tag=FKKLIMLP_00001;product=hypothetical protein&#xD;&#xA;    ATCC0000	prokka	gene	243	434	.	+	.	ID=FKKLIMLP_00001_gene;locus_tag=FKKLIMLP_00001&#xD;&#xA;    ATCC0000	Prodigal:2.6	CDS	1727	2131	.	-	0	ID=FKKLIMLP_00002;Parent=FKKLIMLP_00002_gene;inference=ab initio prediction:Prodigal:2.6;locus_tag=FKKLIMLP_00002;product=hypothetical protein&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.biostars.org/p/46281/" />
  <row Id="5206" PostHistoryTypeId="2" PostId="2342" RevisionGUID="68ffe3ce-62ed-4a56-ac43-f93c21473591" CreationDate="2017-08-21T09:57:25.883" UserId="123" Text="Using python and [this GFF parser][1] that mimics [Biopython's SeqIO parsers][2]:&#xD;&#xA;&#xD;&#xA;    from BCBio import GFF&#xD;&#xA;    &#xD;&#xA;    # Read the gff&#xD;&#xA;    for seq in GFF.parse('my_file.gff'):&#xD;&#xA;        # only focus on the CDSs&#xD;&#xA;        for feat in filter(lambda x: x.type == 'CDS',&#xD;&#xA;                           seq.features):&#xD;&#xA;            # extract the locus tag&#xD;&#xA;            locus_tag = feat.qualifiers.get('locus_tag',&#xD;&#xA;                                            ['unspecified'])[0]&#xD;&#xA;            # extract the sequence&#xD;&#xA;            dna_seq = seq = str(feat.extract(seq).seq)&#xD;&#xA;            # simply print the sequence in fasta format&#xD;&#xA;            print('&gt;%s\n%s' % (locus_tag, dna_seq))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/chapmanb/bcbb/tree/master/gff&#xD;&#xA;  [2]: http://biopython.org/wiki/SeqIO" />
  <row Id="5207" PostHistoryTypeId="2" PostId="2343" RevisionGUID="f1d71b00-275f-49c2-96f4-594ec2060402" CreationDate="2017-08-21T10:36:56.920" UserId="1355" Text="I have a lot of protein clusters. I want to perform an enrichment analysis of their functional annotations, against reference datasets or list of genes i shape.&#xD;&#xA;&#xD;&#xA;Initially, i used [DAVID](https://david.ncifcrf.gov), with the following drawbacks:&#xD;&#xA;&#xD;&#xA;- since i didn't find any other way to use it, i'm limited by the webinterface and its limitations (number of genes, url size, number of request/day).&#xD;&#xA;- automation over the web interface is a hacky script.&#xD;&#xA;&#xD;&#xA;The team behind seems to offer a way to run DAVID in-house (allowing, hopefully, to relax the limitations), but i can't find any way to download the database from their website.&#xD;&#xA;&#xD;&#xA;Is there any alternative way to get enrichment analysis over proteins in a reliable and automated way ?" />
  <row Id="5208" PostHistoryTypeId="1" PostId="2343" RevisionGUID="f1d71b00-275f-49c2-96f4-594ec2060402" CreationDate="2017-08-21T10:36:56.920" UserId="1355" Text="How to run enrichment analysis of protein functional annotation?" />
  <row Id="5209" PostHistoryTypeId="3" PostId="2343" RevisionGUID="f1d71b00-275f-49c2-96f4-594ec2060402" CreationDate="2017-08-21T10:36:56.920" UserId="1355" Text="&lt;database&gt;&lt;public-databases&gt;&lt;annotation&gt;&lt;data-download&gt;" />
  <row Id="5210" PostHistoryTypeId="5" PostId="2343" RevisionGUID="f87239b1-e821-4f9d-8b5f-6bf62d04142a" CreationDate="2017-08-21T11:23:31.280" UserId="77" Comment="added 1 character in body; edited tags" Text="I have a lot of protein clusters. I want to perform an enrichment analysis of their functional annotations, against reference datasets or list of genes I select.&#xD;&#xA;&#xD;&#xA;Initially, I used [DAVID](https://david.ncifcrf.gov), with the following drawbacks:&#xD;&#xA;&#xD;&#xA;- since I didn't find any other way to use it, I'm limited by the web interface and its limitations (number of genes, url size, number of request/day).&#xD;&#xA;- automation over the web interface is a hacky script.&#xD;&#xA;&#xD;&#xA;The team behind seems to offer a way to run DAVID in-house (allowing, hopefully, to relax the limitations), but I can't find any way to download the database from their website.&#xD;&#xA;&#xD;&#xA;Is there any alternative way to get enrichment analysis over proteins in a reliable and automated way?" />
  <row Id="5211" PostHistoryTypeId="6" PostId="2343" RevisionGUID="f87239b1-e821-4f9d-8b5f-6bf62d04142a" CreationDate="2017-08-21T11:23:31.280" UserId="77" Comment="added 1 character in body; edited tags" Text="&lt;go&gt;&lt;go-enrichment&gt;" />
  <row Id="5212" PostHistoryTypeId="2" PostId="2344" RevisionGUID="a209494d-4c23-4c8b-a121-5c1064ba5d24" CreationDate="2017-08-21T11:23:34.903" UserId="294" Text="The [gffread utility][1] in Cufflinks package might be interesting for you. To generate a multi-fasta file with nucleotide sequences from your GFF file, then you can try: &#xD;&#xA;&#xD;&#xA;    gffread -w output_transcripts.fasta -g reference_genome.fa input_transcripts.gff&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://ccb.jhu.edu/software/stringtie/gff.shtml#gffread" />
  <row Id="5213" PostHistoryTypeId="4" PostId="2335" RevisionGUID="e9136cdb-3cc1-490b-aa7d-9c7b8247698b" CreationDate="2017-08-21T11:23:44.720" UserId="123" Comment="Better question title" Text="Is there a way to quickly verify the presence of some SNPs in Fastq files?" />
  <row Id="5214" PostHistoryTypeId="24" PostId="2335" RevisionGUID="e9136cdb-3cc1-490b-aa7d-9c7b8247698b" CreationDate="2017-08-21T11:23:44.720" Comment="Proposed by 123 approved by 77 edit id of 273" />
  <row Id="5215" PostHistoryTypeId="2" PostId="2345" RevisionGUID="bcbe7246-e446-4bd7-9ef9-b18a05ce1e52" CreationDate="2017-08-21T11:26:13.097" UserId="939" Text="I would recommend R instead of sticking to websites.&#xD;&#xA;&#xD;&#xA;There are many tools for enrichment analysis. I like to use [GOseq][1], which is initially made for RNAseq data, but can also be used for proteins (I have used it for proteomics). You can use the length of the protein instead of transcript length (to correct for length bias), or you can exclude the length information by just using the hypergeometric method.&#xD;&#xA;&#xD;&#xA;See [manual][2] for code.&#xD;&#xA;&#xD;&#xA;Subsequently you can use [gogadget][3], a tool I made for visualization of GOseq results with a heatmap or cytoscape network.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioconductor.org/packages/release/bioc/html/goseq.html&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/bioc/vignettes/goseq/inst/doc/goseq.pdf&#xD;&#xA;  [3]: https://sourceforge.net/projects/gogadget/" />
  <row Id="5216" PostHistoryTypeId="2" PostId="2346" RevisionGUID="cfe39d3f-6734-4cdd-9dd0-232759bd4129" CreationDate="2017-08-21T12:05:47.623" UserId="235" Text="It is a well reported fact that GO analysis of RNAseq results is affected by a number of biases, including length bias and expression level bias. &#xD;&#xA;&#xD;&#xA;The `bioconductor` package [`goseq`][1] allows you to correct for these biases. &#xD;&#xA;&#xD;&#xA;By default it corrects for length bias, but you can also get it to do read count bias. Using read counts to do the correction is attractive because in theory it should account for both sources of bias ($read counts\approx expression \times length$).&#xD;&#xA;&#xD;&#xA;I'm doing an enrichment analysis were I have tried both options (length and read counts) and get very different answers. If I run a binomial regression on expression and length vs probability of being differential, I can see that both are *independently* important.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r ---&gt;&#xD;&#xA;&#xD;&#xA;    &gt; model &lt;- glm(sig ~  expression + log(length), data=retained_genes,  family=binomial(link=&quot;logit&quot;))&#xD;&#xA;    &gt; print(anova(model, test=&quot;Chisq&quot;))&#xD;&#xA;&#xD;&#xA;    Analysis of Deviance Table&#xD;&#xA;    &#xD;&#xA;    Model: binomial, link: logit&#xD;&#xA;    &#xD;&#xA;    Response: sig&#xD;&#xA;    &#xD;&#xA;    Terms added sequentially (first to last)&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;                Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    &#xD;&#xA;    NULL                         6676     4507.1              &#xD;&#xA;    expression   1   115.00      6675     4392.1 &lt; 2.2e-16 ***&#xD;&#xA;    log(length)  1   102.55      6674     4289.5 &lt; 2.2e-16 ***&#xD;&#xA;    ---&#xD;&#xA;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#xD;&#xA;&#xD;&#xA;So I'm know unsure what to do, should I use the analysis corrected for length or read count. Or perhaps take only terms significant in both? Or only in one?&#xD;&#xA;&#xD;&#xA;  [1]: https://bioconductor.org/packages/release/bioc/html/goseq.html" />
  <row Id="5217" PostHistoryTypeId="1" PostId="2346" RevisionGUID="cfe39d3f-6734-4cdd-9dd0-232759bd4129" CreationDate="2017-08-21T12:05:47.623" UserId="235" Text="Correct for gene length or read counts in GO enrichment analysis" />
  <row Id="5218" PostHistoryTypeId="3" PostId="2346" RevisionGUID="cfe39d3f-6734-4cdd-9dd0-232759bd4129" CreationDate="2017-08-21T12:05:47.623" UserId="235" Text="&lt;rna-seq&gt;&lt;differential-expression&gt;&lt;go&gt;" />
  <row Id="5223" PostHistoryTypeId="2" PostId="2348" RevisionGUID="29062d33-db17-44f2-85d6-2e6044a110d3" CreationDate="2017-08-21T13:21:22.433" UserId="939" Text="I am not sure if it makes sense to use read counts as bias instead of gene length (and I certainly wouldn't expect the same results). &#xD;&#xA;&#xD;&#xA;Do you use total read counts of all your samples (library size)? &#xD;&#xA;&#xD;&#xA;The correction for gene length is a pure technical one, the longer a gene the more reads will align (and higher read count genes are easier significant, since they are way above the noise threshold). If you use read counts, you also have a biological factor (expression) in there, which (I think) is the stuff you test with statistics (e.g., with edgeR) and thus not the bias you want to correct for. " />
  <row Id="5224" PostHistoryTypeId="2" PostId="2349" RevisionGUID="b35b6246-2c2e-41e9-b470-46936e6da1e6" CreationDate="2017-08-21T14:55:53.037" UserId="1236" Text="I am helping a colleague to interpret protein multiple sequence alignment results. My colleague used a third-party proprietary solution (I know...) to calculate the alignment, and one of the resulting tables is a pairwise distance matrix with identity % in the top triangle and **identity plus similarity** in the bottom triangle. &#xD;&#xA;&#xD;&#xA;I am struggling to interpret identity plus similarity, despite the fact I found the metric in several papers. " />
  <row Id="5225" PostHistoryTypeId="1" PostId="2349" RevisionGUID="b35b6246-2c2e-41e9-b470-46936e6da1e6" CreationDate="2017-08-21T14:55:53.037" UserId="1236" Text="Identity plus Similarity Interpretation" />
  <row Id="5226" PostHistoryTypeId="3" PostId="2349" RevisionGUID="b35b6246-2c2e-41e9-b470-46936e6da1e6" CreationDate="2017-08-21T14:55:53.037" UserId="1236" Text="&lt;proteins&gt;&lt;sequence-alignment&gt;" />
  <row Id="5227" PostHistoryTypeId="2" PostId="2350" RevisionGUID="5dec7171-455e-419a-bcda-d8bf557a3d8b" CreationDate="2017-08-21T15:43:53.230" UserId="48" Text="My understanding of the subject is that the bias of the gene length (and other bias) should be taken care when analyzing the expression and **before** enrichment analysis. The enrichment analysis should be done once the corrections has been performed. Because as the abstract of the [GOseq paper states][1]:&#xD;&#xA;&#xD;&#xA;&gt;GO analysis is widely used ... but standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly expressed transcripts.&#xD;&#xA;&#xD;&#xA;So first, take care of the differential expression bias by length and then use the GO to reduce complexity. How you take care of the bias in the RNA-seq data is another question. But the [cqn][2] package of Bioconductor can correct the expression by gene length and GC content. However, this correction might hurt the differential tool used (see [this discussion][3] in Bioconductor), so it might be better in some cases to use GOSeq. &#xD;&#xA;&#xD;&#xA;Now, to the question itself:&#xD;&#xA;&#xD;&#xA;&gt; So I'm know unsure what to do, should I use the analysis corrected for length or read count. Or perhaps take only terms significant in both? Or only in one?&#xD;&#xA;&#xD;&#xA;Use whatever correction method that yields better differentially expressed genes (DEG. If you find that the correction for length improves the accuracy of the predictions of DEG better than correcting by length and GC, then use that one. &#xD;&#xA;&#xD;&#xA;Another option to obtain accurate GO terms, then you could use other testing procedures which don't rely uniquely in the Fisher test, such as those that take into account the structure of the GO graph. [TopGO][4] use this approach (note that it is a bit difficult to work with this package),this will reduce the role of the gene length bias (and probably other bias) in the resulting significant GO.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-2-r14&#xD;&#xA;  [2]: http://bioconductor.org/packages/cqn&#xD;&#xA;  [3]: https://support.bioconductor.org/p/95683/#95713&#xD;&#xA;  [4]: http://bioconductor.org/packages/topGO" />
  <row Id="5228" PostHistoryTypeId="5" PostId="2346" RevisionGUID="15b31767-18d7-41c0-ba08-fd019b4e1e8c" CreationDate="2017-08-21T15:51:45.610" UserId="235" Comment="added 113 characters in body" Text="It is a well reported fact that GO analysis of RNAseq results is affected by a number of biases, including length bias and expression level bias. &#xD;&#xA;&#xD;&#xA;The `bioconductor` package [`goseq`][1] allows you to correct for these biases. &#xD;&#xA;&#xD;&#xA;By default it corrects for length bias, but you can also get it to do read count bias. Using read counts to do the correction is attractive because in theory it should account for both sources of bias ($read counts\approx expression \times length$).&#xD;&#xA;&#xD;&#xA;I'm doing an enrichment analysis were I have tried both options (length and read counts) and get very different answers. If I run a binomial regression on expression and length vs probability of being differential, I can see that both are *independently* important.&#xD;&#xA;&#xD;&#xA;&lt;!-- language: lang-r ---&gt;&#xD;&#xA;&#xD;&#xA;    &gt; model &lt;- glm(sig ~  expression + log(length), data=retained_genes,  family=binomial(link=&quot;logit&quot;))&#xD;&#xA;    &gt; print(anova(model, test=&quot;Chisq&quot;))&#xD;&#xA;&#xD;&#xA;    Analysis of Deviance Table&#xD;&#xA;    &#xD;&#xA;    Model: binomial, link: logit&#xD;&#xA;    &#xD;&#xA;    Response: sig&#xD;&#xA;    &#xD;&#xA;    Terms added sequentially (first to last)&#xD;&#xA;&#xD;&#xA;                           Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    &#xD;&#xA;    NULL                                    6676     4507.1              &#xD;&#xA;    expression              1  114.998      6675     4392.1 &lt; 2.2e-16 ***&#xD;&#xA;    log(length)             1  102.553      6674     4289.5 &lt; 2.2e-16 ***&#xD;&#xA;    expression:log(length)  1   34.094      6673     4255.4 5.252e-09 ***&#xD;&#xA;    ---&#xD;&#xA;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#xD;&#xA;&#xD;&#xA;So I'm know unsure what to do, should I use the analysis corrected for length or read count. Or perhaps take only terms significant in both? Or only in one?&#xD;&#xA;&#xD;&#xA;  [1]: https://bioconductor.org/packages/release/bioc/html/goseq.html&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5231" PostHistoryTypeId="50" PostId="802" RevisionGUID="dae4159f-9f0b-4b4f-986a-639d3719eaf4" CreationDate="2017-08-21T21:28:25.223" UserId="-1" />
  <row Id="5232" PostHistoryTypeId="2" PostId="2351" RevisionGUID="aa6d80b2-e849-427a-a020-31e7adb2f0a1" CreationDate="2017-08-21T23:06:08.790" UserId="400" Text="I guess you already know what identity means. But for completeness, it is the % of pairwise alignment position which have an identical amino acid residue. For the identity+similarity it is important to know that amino acids can be grouped according to their physicochemical properties e.g. charge, size etc. While there are standard groupings for amino acids it may be that the proprietary algorithm uses different groupings. Either way, % identity+similarity would imply the number of pairwise alignment positions which contain either the same amino acid or amino acids from the same group (however they define groups)." />
  <row Id="5233" PostHistoryTypeId="2" PostId="2352" RevisionGUID="04979095-0901-40b1-8dc1-3c1b907127ef" CreationDate="2017-08-22T06:29:21.313" UserId="1331" Text="I am using plink to check the association between a phenotype and the SNPs of a gene. plink says the phenotype is significantly associated with a SNP on that gene, and when I check the SNP alleles, for 6 out of 49 samples, the nucleotides are 'GA', and for the rest, they are 'AG', so only the order is different. I am not a biologist, but my basic genetics knowledge says that those two are semantically the same. However, obviously, plink does not agree with me and it associates this SNP with a small p-value since 'GA' samples have a higher value for the phenotype than 'AG' samples. I am very new to SNP analysis, any help is appreciated." />
  <row Id="5234" PostHistoryTypeId="1" PostId="2352" RevisionGUID="04979095-0901-40b1-8dc1-3c1b907127ef" CreationDate="2017-08-22T06:29:21.313" UserId="1331" Text="Is there any difference between SNPs 'AG' and 'GA' in association analyses?" />
  <row Id="5235" PostHistoryTypeId="3" PostId="2352" RevisionGUID="04979095-0901-40b1-8dc1-3c1b907127ef" CreationDate="2017-08-22T06:29:21.313" UserId="1331" Text="&lt;snp&gt;" />
  <row Id="5236" PostHistoryTypeId="2" PostId="2353" RevisionGUID="95c4d264-6502-4c35-8667-aa479c39e114" CreationDate="2017-08-22T06:40:57.690" UserId="77" Text="Is &quot;user5054&quot; the same as &quot;user5504&quot;? No? Exactly. Not only does order matter, it's incredibly vitally important. `AG` and `GA` are completely and totally different from each other. If this is in a coding region, then the resulting amino acid is undoubtedly changed (fun fact, the [only exception](https://en.wikipedia.org/wiki/DNA_codon_table) is `TAG` and `TGA`). If it's at a [splice site](https://en.wikipedia.org/wiki/RNA_splicing) then splicing is likely altered (AG is part of a splice acceptor site). If it's at the binding site of something then it wouldn't be surprised if it didn't bind any more." />
  <row Id="5237" PostHistoryTypeId="2" PostId="2354" RevisionGUID="649d9c81-f1e1-4613-973f-a49a7d617afa" CreationDate="2017-08-22T07:53:00.347" UserId="1332" Text="Could you please show us the context in which this appears, as you seem to be interpreting this differently to Devon.&#xD;&#xA;&#xD;&#xA;If it's appearing as you say, GA and AG, then Devon is right, this usually means that the sequence goes ###AG### or ###GA###, which are two very different sequences.&#xD;&#xA;&#xD;&#xA;If, however, as you're implying, the sequences are actually ###G### or ###A###, then semantically, the two ways of writing it have the same outcome. Normally, however, this is written as A/G or G/A.&#xD;&#xA;&#xD;&#xA;Alternatively, you're looking at genotypes, which would usually appear as A|G and G|A. In this case, both individuals are heterozygotes, and in terms of the individual SNP they are the same. However the side of the pipe is relevant as this tells you which homologous chromosome it is on. This allows you to calculate LD with respect to neighbouring SNPs, and to determine the actual sequences of the proteins in that individual." />
  <row Id="5238" PostHistoryTypeId="2" PostId="2355" RevisionGUID="96d24f1c-8b3e-4e0d-8f58-b34a475d3b70" CreationDate="2017-08-22T09:19:27.233" UserId="73" Text="Most association analyses are carried out at a single SNP level, so AG and GA are likely to indicate a heterozygous genotype at a particular location. However, the precise notation matters.&#xD;&#xA;&#xD;&#xA;As @Emily_Ensembl has alluded to, for VCF files, A/G indicates unphased SNPs (order unknown, and shouldn't be considered in analyses), whereas A|G indicates phased SNPs (order matters).&#xD;&#xA;&#xD;&#xA;It is possible to carry out association analyses by combining multiple variants into a single haplotype, in which case the order would also matter. It's unlikely that this is the case, because such haplotype variant analyses are substantially more complicated to carry out, and will typically be paired with information about SNP linkage (statistic D´ or r²)." />
  <row Id="5239" PostHistoryTypeId="5" PostId="2343" RevisionGUID="09bf9bf2-c934-40d2-908b-a5164b771275" CreationDate="2017-08-22T09:52:44.567" UserId="1355" Comment="question" Text="I have a lot of protein clusters. I want to perform an enrichment analysis of their functional annotations, against reference datasets or list of genes I select.&#xD;&#xA;&#xD;&#xA;More precisely: a method yields cluster of proteins. I want to decide, for each cluster (which is a set of proteins identifiers), if it holds meaning regarding the proteins it contains, by comparing the annotations found in the cluster and the annotations found in common or specific datasets.&#xD;&#xA;&#xD;&#xA;Initially, I used [DAVID](https://david.ncifcrf.gov), with the following drawbacks:&#xD;&#xA;&#xD;&#xA;- since I didn't find any other way to use it, I'm limited by the web interface and its limitations (number of genes, url size, number of request/day).&#xD;&#xA;- automation over the web interface is a hacky script.&#xD;&#xA;&#xD;&#xA;The team behind seems to offer a way to run DAVID in-house (allowing, hopefully, to relax the limitations), but I can't find any way to download the database from their website.&#xD;&#xA;&#xD;&#xA;What are the alternative ways to get enrichment analysis over proteins in a reliable and automated way ?" />
  <row Id="5240" PostHistoryTypeId="6" PostId="2352" RevisionGUID="62f3dd18-9348-4ec6-a6d7-06e66a17963e" CreationDate="2017-08-22T09:55:58.653" UserId="123" Comment="Adding the gwas tag" Text="&lt;snp&gt;&lt;gwas&gt;" />
  <row Id="5241" PostHistoryTypeId="24" PostId="2352" RevisionGUID="62f3dd18-9348-4ec6-a6d7-06e66a17963e" CreationDate="2017-08-22T09:55:58.653" Comment="Proposed by 123 approved by 77 edit id of 274" />
  <row Id="5242" PostHistoryTypeId="2" PostId="2356" RevisionGUID="674736d7-fe39-4f3e-9347-92a4d78144fa" CreationDate="2017-08-22T10:11:37.243" UserId="208" Text="I have multiple libraries of **10x Chromium single-cell RNA-seq** data, which I'd like to combine. One option is using `cellranger aggr` which by default does a depth normalization:&#xD;&#xA;&#xD;&#xA;`mapped`: (default) Subsample reads from higher-depth libraries until they all have an equal number of confidently mapped reads per cell. `raw`: Subsample reads from higher-depth libraries until they all have an equal number of total (i.e. raw, mapping-independent) reads per cell.&#xD;&#xA;`none`: Do not normalize at all.&#xD;&#xA;&#xD;&#xA;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate&#xD;&#xA;&#xD;&#xA;The new version of `Seurat` (2.0) provides another way via the `MergeSeurat()` (or `AddSamples()`) functions.&#xD;&#xA;&#xD;&#xA;http://satijalab.org/seurat/merge_vignette.html&#xD;&#xA;&#xD;&#xA;These have the arguments:&#xD;&#xA;&#xD;&#xA;    do.logNormalize whether to normalize the expression data per cell and transform to log space.&#xD;&#xA;    total.expr      scale factor in the log normalization &#xD;&#xA;    do.scale        In object@scale.data, perform row-scaling (gene-based z-score) &#xD;&#xA;    do.center       In object@scale.data, perform row-centering (gene-based centering)&#xD;&#xA;&#xD;&#xA;I'm wondering how do these methods compare? What are the pros/cons etc? What do we need to consider when we combine samples?&#xD;&#xA;" />
  <row Id="5243" PostHistoryTypeId="1" PostId="2356" RevisionGUID="674736d7-fe39-4f3e-9347-92a4d78144fa" CreationDate="2017-08-22T10:11:37.243" UserId="208" Text="Cellranger aggr and Seurat merge difference?" />
  <row Id="5244" PostHistoryTypeId="3" PostId="2356" RevisionGUID="674736d7-fe39-4f3e-9347-92a4d78144fa" CreationDate="2017-08-22T10:11:37.243" UserId="208" Text="&lt;rna-seq&gt;&lt;seurat&gt;&lt;10x&gt;&lt;single-cell&gt;" />
  <row Id="5246" PostHistoryTypeId="5" PostId="2343" RevisionGUID="673a6177-05c6-4568-87d8-5cbc5b194703" CreationDate="2017-08-22T10:30:08.333" UserId="1355" Comment="added 141 characters in body" Text="I have a lot of protein clusters. I want to perform an enrichment analysis of their functional annotations, against reference datasets or list of genes I select.&#xD;&#xA;&#xD;&#xA;More precisely: a method yields cluster of proteins. I want to decide, for each cluster (which is a set of proteins identifiers), if it holds meaning regarding the proteins it contains, by comparing the annotations found in the cluster and the annotations found in common or specific datasets.&#xD;&#xA;&#xD;&#xA;Initially, I used [DAVID](https://david.ncifcrf.gov), which compute the GO annotations from the protein list, then perform the enrichment analysis against common datasets.&#xD;&#xA;&#xD;&#xA;However, DAVID suffer of the following drawbacks:&#xD;&#xA;&#xD;&#xA;- since I didn't find any other way to use it, I'm limited by the web interface and its limitations (number of genes, url size, number of request/day).&#xD;&#xA;- automation over the web interface is a hacky script.&#xD;&#xA;&#xD;&#xA;The team behind seems to offer a way to run DAVID in-house (allowing, hopefully, to relax the limitations), but I can't find any way to download the database from their website.&#xD;&#xA;&#xD;&#xA;What are the alternative ways to get enrichment analysis over proteins in a reliable and automated way ?" />
  <row Id="5251" PostHistoryTypeId="5" PostId="2345" RevisionGUID="99a120d4-4a87-4a9d-96cc-fa0a94ba0bf8" CreationDate="2017-08-22T10:43:13.063" UserId="939" Comment="added 231 characters in body" Text="I would recommend R instead of sticking to websites.&#xD;&#xA;&#xD;&#xA;There are many tools for enrichment analysis. I like to use [GOseq][1], which is initially made for RNAseq data, but can also be used for proteins (I have used it for proteomics). You can use the length of the protein instead of transcript length (to correct for length bias), or you can exclude the length information by just using the hypergeometric method.&#xD;&#xA;&#xD;&#xA;See [manual][2] for code.&#xD;&#xA;&#xD;&#xA;Subsequently you can use [gogadget][3], a tool I made for visualization of GOseq results with a heatmap or cytoscape network.&#xD;&#xA;&#xD;&#xA;Additionally, you might wanna take a look at [clusterProfiler][4]. With this package you can do enrichment analysis for many clusters in parallel.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bioconductor.org/packages/release/bioc/html/goseq.html&#xD;&#xA;  [2]: https://bioconductor.org/packages/release/bioc/vignettes/goseq/inst/doc/goseq.pdf&#xD;&#xA;  [3]: https://sourceforge.net/projects/gogadget/&#xD;&#xA;  [4]: https://bioconductor.org/packages/release/bioc/html/clusterProfiler.html" />
  <row Id="5252" PostHistoryTypeId="2" PostId="2358" RevisionGUID="4d7974f7-95d8-4b3f-a7cd-7b7f290ab52a" CreationDate="2017-08-22T14:49:00.440" UserId="1283" Text="  I finally found another answer to my question. Please read this great article in May 12 2017 BioMed Central (BMC) Bioinformatics article titled &lt;Ranking metrics in gene set enrichment analysis: do they matter?&gt; with the URL[ https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1674-0]&#xD;&#xA;&#xD;&#xA;Also, please read this blog , &quot;Diving into Genetics and Genomics: Gene Set Enrichment Analysis (GSEA) explained.&quot; with the URL [http://crazyhottommy.blogspot.com/2016/08/gene-set-enrichment-analysis-gsea.html]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1674-0.&#xD;&#xA;  [2]: http://crazyhottommy.blogspot.com/2016/08/gene-set-enrichment-analysis-gsea.html&#xD;&#xA;&#xD;&#xA;After reading these two articles, my choice for the best rapidGSEA local ranking measure is Minimum Significant Difference(i.e. MSD) because it has the best overall false positive rate(i.e FPR) for larger sample sizes.&#xD;&#xA;&#xD;&#xA;  Finally, it is important to realize that fgseaL's phenotype labeling cannot be emulated by GSEA with any of it's sixteen possible ranking metrics." />
  <row Id="5253" PostHistoryTypeId="2" PostId="2359" RevisionGUID="99ce1c51-ffc9-4ec6-aec5-489a7cbdbbe3" CreationDate="2017-08-22T16:39:42.897" UserId="294" Text="I would like to modify some reference transcripts from Ensembl (*D. melanogaster*) to introduce a controlled rate of random errors in the sequences. The idea would be to introduce random base substitutions in these sequences, no indels for now, because I would like to keep the transcript sequence length as it is in the reference. &#xD;&#xA;&#xD;&#xA;The rate of error per transcript will be determined according to an error profile computed from an external set of RNA-seq reads (e.g., generated with ONT MinION)&#xD;&#xA;&#xD;&#xA;The aim of this modification would be to establish a rough benchmark of the performances of aligners to use over transcripts from spliced reads (rna-to-genome), aka with more than one exon.&#xD;&#xA;&#xD;&#xA;Any idea of which software would be best for this purpose?" />
  <row Id="5254" PostHistoryTypeId="1" PostId="2359" RevisionGUID="99ce1c51-ffc9-4ec6-aec5-489a7cbdbbe3" CreationDate="2017-08-22T16:39:42.897" UserId="294" Text="Introduce errors in reference transcripts according to external dataset error model" />
  <row Id="5255" PostHistoryTypeId="3" PostId="2359" RevisionGUID="99ce1c51-ffc9-4ec6-aec5-489a7cbdbbe3" CreationDate="2017-08-22T16:39:42.897" UserId="294" Text="&lt;rna-seq&gt;&lt;software-recommendation&gt;&lt;nanopore&gt;&lt;simulated-data&gt;" />
  <row Id="5256" PostHistoryTypeId="5" PostId="2358" RevisionGUID="aaf14894-6bd6-426f-acfa-75e21933e95c" CreationDate="2017-08-22T17:29:39.863" UserId="77" Comment="deleted 203 characters in body" Text="  I finally found another answer to my question. Please read this great article in May 12 2017 BioMed Central (BMC) Bioinformatics article titled [Ranking metrics in gene set enrichment analysis: do they matter?](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1674-0).&#xD;&#xA;&#xD;&#xA;Also, please read this blog , [Diving into Genetics and Genomics: Gene Set Enrichment Analysis (GSEA) explained](http://crazyhottommy.blogspot.com/2016/08/gene-set-enrichment-analysis-gsea.html).&#xD;&#xA;&#xD;&#xA;After reading these two articles, my choice for the best rapidGSEA local ranking measure is Minimum Significant Difference (i.e., MSD), because it has the best overall false positive rate (i.e., FPR) for larger sample sizes.&#xD;&#xA;&#xD;&#xA;Finally, it is important to realize that fgseaL's phenotype labelling cannot be emulated by GSEA with any of it's sixteen possible ranking metrics." />
  <row Id="5257" PostHistoryTypeId="2" PostId="2360" RevisionGUID="150dc3ba-8077-4324-8142-c5832e469273" CreationDate="2017-08-22T17:39:19.703" UserId="77" Text="It sounds like what you're really looking for is a read simulator. A cursory search turns up [NanoSim](http://www.biorxiv.org/content/early/2016/03/18/044545), which is designed to simulate reads from a MinION. This has the benefit of at least having been [used in some of the published literature](https://gigascience.biomedcentral.com/articles/10.1186/s13742-016-0140-7), which is always a nice sign.&#xD;&#xA;&#xD;&#xA;You may also find [this review article](http://www.nature.com/nrg/journal/v17/n8/full/nrg.2016.57.html) on read simulators useful. It doesn't specifically mention NanoSim, but it should prove to be a useful review of the general concepts anyway if you need to read up on them." />
  <row Id="5258" PostHistoryTypeId="2" PostId="2361" RevisionGUID="777e49d6-6386-4db5-b067-12b91b125489" CreationDate="2017-08-22T19:20:21.920" UserId="1334" Text="I have data, obtained from a single metagenomic DNA sample, that consists of two MiSeq FASTQ files (R1 and R2) that I merged using [PEAR][1].&#xD;&#xA;&#xD;&#xA;Now I want to estimate the abundances of the bacteria taxa to generate a figure like this one:&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;Figure from: *Panosyan, Hovik, and Nils‐Kåre Birkeland. &quot;Microbial diversity in an Armenian geothermal spring assessed by molecular and culture‐based methods.&quot; Journal of basic microbiology 54.11 (2014): 1240-1250.*&#xD;&#xA;&#xD;&#xA;The problem is that there wasn't a step of amplification of the 16S region as the goal of the sequencing was to discover new genes. I've already isolated 16S reads from my sample using [SortMeRNA][3], but it seems like softwares that do OTU picking, taxonomic assignment and diversity analyses (such as mothur and QIIME) require that all the reads come from the same region of the 16S gene.&#xD;&#xA;&#xD;&#xA;Is there a way of using these 16S reads that I've filtered using SortMeRNA in a diversity analysis using mothur/QUIIME?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btt593&#xD;&#xA;  [2]: https://i.stack.imgur.com/FlJxD.jpg&#xD;&#xA;  [3]: http://bioinfo.lifl.fr/RNA/sortmerna/" />
  <row Id="5259" PostHistoryTypeId="1" PostId="2361" RevisionGUID="777e49d6-6386-4db5-b067-12b91b125489" CreationDate="2017-08-22T19:20:21.920" UserId="1334" Text="Microbial diversity analysis using whole-genome metagenomic data" />
  <row Id="5260" PostHistoryTypeId="3" PostId="2361" RevisionGUID="777e49d6-6386-4db5-b067-12b91b125489" CreationDate="2017-08-22T19:20:21.920" UserId="1334" Text="&lt;metagenome&gt;&lt;taxonomy&gt;" />
  <row Id="5261" PostHistoryTypeId="2" PostId="2362" RevisionGUID="3b21bb2e-56b4-4721-bf25-91ad82b03750" CreationDate="2017-08-22T19:43:37.373" UserId="73" Text="Do any of the answers for [this question](https://bioinformatics.stackexchange.com/questions/202/tools-for-simulating-oxford-nanopore-reads/222) help? Karel Brinda has mentioned a few read simulators in [the answer to that question](https://bioinformatics.stackexchange.com/a/273/73), and has [a thesis](http://brinda.cz/publications/brinda_phd.pdf) with more information.&#xD;&#xA;&#xD;&#xA;Excluding INDEL errors doesn't sound like a good idea; length can still be preserved even if doing that, it just needs an adjustment at the end of the sequence. Note that if you're trying to model nanopore reads, what you're really modelling is the base-caller, rather than the sequencer. I mention this in more detail in [my answer](https://bioinformatics.stackexchange.com/a/226/73).&#xD;&#xA;&#xD;&#xA;In most cases where errors are modelled, I find it better to use publicly-available data instead. Especially for nanopore data, there are unmodelled systematic errors in the base-callers and sequencer that can't be simulated using any programs (because they are unmodelled). The following paper would be a good place to start for cDNA sequences, which looks at single-cell data from mouse (C57Bl/6) B1a cells:&#xD;&#xA;&#xD;&#xA;http://www.biorxiv.org/content/early/2017/04/13/126847&#xD;&#xA;&#xD;&#xA;Illumina and ONT reads for that study can be found in SRA under accession number [SRP082530](https://trace.ncbi.nlm.nih.gov/Traces/sra/?study=SRP082530)." />
  <row Id="5262" PostHistoryTypeId="5" PostId="2362" RevisionGUID="aba7ff9a-31b1-4c8f-9f6d-23a826a4703a" CreationDate="2017-08-22T19:52:49.440" UserId="73" Comment="added 501 characters in body" Text="Do any of the answers for [this question](https://bioinformatics.stackexchange.com/questions/202/tools-for-simulating-oxford-nanopore-reads/222) help? Karel Brinda has mentioned a few read simulators in [the answer to that question](https://bioinformatics.stackexchange.com/a/273/73), and has [a thesis](http://brinda.cz/publications/brinda_phd.pdf) with more information.&#xD;&#xA;&#xD;&#xA;Excluding INDEL errors doesn't sound like a good idea; length can still be preserved even if doing that, it just needs an adjustment at the end of the sequence. Note that if you're trying to model nanopore reads, what you're really modelling is the base-caller, rather than the sequencer. I mention this in more detail in [my answer](https://bioinformatics.stackexchange.com/a/226/73).&#xD;&#xA;&#xD;&#xA;In most cases where errors are modelled, I find it better to use publicly-available data instead. Especially for nanopore data, there are unmodelled systematic errors in the base-callers and sequencer that can't be simulated using any programs (because they are unmodelled). The following paper would be a good place to start for cDNA sequences, which looks at single-cell data from mouse (C57Bl/6) B1a cells:&#xD;&#xA;&#xD;&#xA;http://www.biorxiv.org/content/early/2017/04/13/126847&#xD;&#xA;&#xD;&#xA;Illumina and ONT reads for that study can be found in SRA under accession number [SRP082530](https://trace.ncbi.nlm.nih.gov/Traces/sra/?study=SRP082530).&#xD;&#xA;&#xD;&#xA;I don't know of any recent *D. melanogaster* studies that have been done using nanopore. There's always the option of spending $1000 on a [purchase of a MinION](https://store.nanoporetech.com/minion/rna/sets) with an RNA starter kit to do the study yourself. Here's an older targeted gene study, but bear in mind that it was using an R7.3 flow cell, so errors rates will be much higher than what is currently available:&#xD;&#xA;&#xD;&#xA;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0777-z" />
  <row Id="5263" PostHistoryTypeId="5" PostId="2361" RevisionGUID="495d1d9a-6417-4d2d-9c5c-e80b4da611d4" CreationDate="2017-08-22T20:23:18.003" UserId="1334" Comment="deleted 1 character in body" Text="I have data, obtained from a single metagenomic DNA sample, that consists of two MiSeq FASTQ files (R1 and R2) that I merged using [PEAR][1].&#xD;&#xA;&#xD;&#xA;Now I want to estimate the abundances of the bacteria taxa to generate a figure like this one:&#xD;&#xA;&#xD;&#xA;[![enter image description here][2]][2]&#xD;&#xA;&#xD;&#xA;Figure from: *Panosyan, Hovik, and Nils‐Kåre Birkeland. &quot;Microbial diversity in an Armenian geothermal spring assessed by molecular and culture‐based methods.&quot; Journal of basic microbiology 54.11 (2014): 1240-1250.*&#xD;&#xA;&#xD;&#xA;The problem is that there wasn't a step of amplification of the 16S region as the goal of the sequencing was to discover new genes. I've already isolated 16S reads from my sample using [SortMeRNA][3], but it seems like softwares that do OTU picking, taxonomic assignment and diversity analyses (such as mothur and QIIME) require that all the reads come from the same region of the 16S gene.&#xD;&#xA;&#xD;&#xA;Is there a way of using these 16S reads that I've filtered using SortMeRNA in a diversity analysis using mothur/QIIME?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btt593&#xD;&#xA;  [2]: https://i.stack.imgur.com/FlJxD.jpg&#xD;&#xA;  [3]: http://bioinfo.lifl.fr/RNA/sortmerna/" />
  <row Id="5264" PostHistoryTypeId="2" PostId="2363" RevisionGUID="e556f610-0cf5-4d25-9336-ff5d36340092" CreationDate="2017-08-22T20:51:08.273" UserId="146" Text="I am constructing a bit of software which pipes the outputs of the bam file via `samtools view` into a script for parsing. My goal is to (somehow) make this process more efficient, and faster than `samtools view`. &#xD;&#xA;&#xD;&#xA;I am only using 3-4 fields in the bam. Naturally, if I only wanted those fields, I could do something like `samtools view file1.bam | cut -f ##`. &#xD;&#xA;&#xD;&#xA;However, I'm trying to be faster than `samtools view`. &#xD;&#xA;&#xD;&#xA;(1) Given the bam file format, is it theoretically possible to output the bam faster than samtools view by skipping all unnecessary fields? &#xD;&#xA;&#xD;&#xA;(2) Is there software available to achieve (1), or would one need to hack samtools? " />
  <row Id="5265" PostHistoryTypeId="1" PostId="2363" RevisionGUID="e556f610-0cf5-4d25-9336-ff5d36340092" CreationDate="2017-08-22T20:51:08.273" UserId="146" Text="Is there a way to output a bam faster than `samtools view` by skipping fields?" />
  <row Id="5266" PostHistoryTypeId="3" PostId="2363" RevisionGUID="e556f610-0cf5-4d25-9336-ff5d36340092" CreationDate="2017-08-22T20:51:08.273" UserId="146" Text="&lt;bam&gt;&lt;samtools&gt;&lt;sam&gt;" />
  <row Id="5267" PostHistoryTypeId="5" PostId="550" RevisionGUID="9bb810d4-6770-4a2a-bc71-a479e0268b50" CreationDate="2017-08-22T21:01:56.533" UserId="-1" Comment="Added FluDB as a possible name for IRD" Text="There area few different influenza virus database resources:&#xD;&#xA;&#xD;&#xA;- The [Influenza Research Database (IRD)](http://www.fludb.org) (a.k.a FluDB - based upon URL)&#xD;&#xA;    - A NIAID Bioinformatics Resource Center or BRC which highly curates the data brought in and integrates it with numerous other relevant data types&#xD;&#xA;- The [NCBI Influenza Virus Resource](https://www.ncbi.nlm.nih.gov/genomes/FLU/Database/nph-select.cgi?go=database)&#xD;&#xA;    - A sub-project of the NCBI with data curated over and above the GenBank data that is part of the NCBI&#xD;&#xA;- The [GISAID EpiFlu Database](http://platform.gisaid.org/)&#xD;&#xA;    - A database of sequences from the Global Initiative on Sharing All Influenza Data. Has unique data from many countries but requires user agree to a data sharing policy.&#xD;&#xA;- The [OpenFluDB](http://openflu.vital-it.ch/browse.php)&#xD;&#xA;    - Former GISAID database that contains some sequence data that GenBank does not have.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;For those who also may be interested in other virus databases, there are:&#xD;&#xA;&#xD;&#xA;- [Virus Pathogen Resource (VIPR)](https://www.viprbrc.org)&#xD;&#xA;    - A companion portal to the IRD, which hosts curated and integrated data for most other NIAID A-C virus pathogens including (but not limited to) Ebola, Zika, Dengue, Enterovirus, and Hepatitis C&#xD;&#xA;- [LANL HIV database](https://www.hiv.lanl.gov/)&#xD;&#xA;   - Los Alamos National Laboratory HIV database with HIV data and many useful tools for all virus bioinformatics&#xD;&#xA;- [PaVE: Papilloma virus genome database](https://pave.niaid.nih.gov) (from quintik comment)&#xD;&#xA;   - NIAID developed and maintained Papilloma virus bioinformatics portal&#xD;&#xA;&#xD;&#xA;Disclaimer: I used to work for the IRD / VIPR and currently work for NIAID." />
  <row Id="5268" PostHistoryTypeId="24" PostId="550" RevisionGUID="9bb810d4-6770-4a2a-bc71-a479e0268b50" CreationDate="2017-08-22T21:01:56.533" Comment="Proposed by 73.86.32.35|ebeffb82-7801-4a75-9bc8-061a07f12c3b approved by 714 edit id of 275" />
  <row Id="5269" PostHistoryTypeId="2" PostId="2364" RevisionGUID="b15cc5eb-39ea-449b-973e-325f7b3dd721" CreationDate="2017-08-23T02:13:50.693" UserId="964" Text="I like `bedtools getfasta`.  My typical option set is `bedtools getfasta -fi &lt;reference&gt; -bed &lt;gff_file&gt; -name -s`.  Be aware of the `-s` to make sure you are pulling the correct strand.  I like bedtools because it is a versatile tool overall for handling bed, gff and vcf file manipulations. &#xD;&#xA;&#xD;&#xA;    # bedtools getfasta&#xD;&#xA;    &#xD;&#xA;    Tool:    bedtools getfasta (aka fastaFromBed)&#xD;&#xA;    Version: v2.26.0&#xD;&#xA;    Summary: Extract DNA sequences from a fasta file based on feature coordinates.&#xD;&#xA;    &#xD;&#xA;    Usage:   bedtools getfasta [OPTIONS] -fi &lt;fasta&gt; -bed &lt;bed/gff/vcf&gt;&#xD;&#xA;    &#xD;&#xA;    Options: &#xD;&#xA;    	-fi	Input FASTA file&#xD;&#xA;    	-bed	BED/GFF/VCF file of ranges to extract from -fi&#xD;&#xA;    	-name	Use the name field for the FASTA header&#xD;&#xA;    	-split	given BED12 fmt., extract and concatenate the sequencesfrom the BED &quot;blocks&quot; (e.g., exons)&#xD;&#xA;    	-tab	Write output in TAB delimited format.&#xD;&#xA;    		- Default is FASTA format.&#xD;&#xA;    &#xD;&#xA;    	-s	Force strandedness. If the feature occupies the antisense,&#xD;&#xA;    		strand, the sequence will be reverse complemented.&#xD;&#xA;    		- By default, strand information is ignored.&#xD;&#xA;    &#xD;&#xA;    	-fullHeader	Use full fasta header.&#xD;&#xA;    		- By default, only the word before the first space or tab is used.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5270" PostHistoryTypeId="2" PostId="2365" RevisionGUID="4dcfc95a-61d2-4cd0-abc2-467ee65902d0" CreationDate="2017-08-23T03:14:30.797" UserId="73" Text="The BAM file format is not a text-based format. It has a specific binary structure, specified in reasonable detail in the [SAM file format specification](https://samtools.github.io/hts-specs/SAMv1.pdf). Whenever this information is displayed on a screen as text, it needs to be converted from the binary format to a text format, which takes a bit of time and processing power.&#xD;&#xA;&#xD;&#xA;As this question suggests, if only a specific field from alignments is needed (or fields), then it will probably be better to extract just those fields and do any necessary conversion only on those fields. While this *can* be done by writing a BAM parser from scratch, many developers have already written software libraries to process BAM files in this way.&#xD;&#xA;&#xD;&#xA;Devon Ryan has suggested [htslib](http://www.htslib.org/), which is a C library written by a group that includes the people who wrote the SAM/BAM/CRAM file format specifications. There's also [pysam](https://github.com/pysam-developers/pysam), which is a python wrapper around htslib.&#xD;&#xA;&#xD;&#xA;The particular tool that is used will depend on your familiarity with programming and the specific thing that you want to do. If you just want to &quot;output a BAM file&quot;, then cat is the quickest:&#xD;&#xA;&#xD;&#xA;    cat file1.bam&#xD;&#xA;&#xD;&#xA;... but you probably don't want to do that, because it seems like you want to process a text-based representation with a script. Because you haven't specified which fields you are interested in, it's not possible to suggest the best thing to use. In the end, I expect that an &quot;efficient&quot; solution to your problem would involve `htslib` in some form. This is not really hacking *samtools*, it's using the backend of samtools to process BAM data.&#xD;&#xA;&#xD;&#xA;However, it's useful to also think about the time cost of coding. What application do you have which means that the text-based processing method is not sufficient? It takes time to write code, and a lot more time to debug that code to make sure it's doing the right thing in all situations. If this is for a one-off thing, then `samtools view` output fed into your script may be the quickest solution." />
  <row Id="5271" PostHistoryTypeId="2" PostId="2366" RevisionGUID="9851b180-c442-417c-9bd3-be60e66ade50" CreationDate="2017-08-23T04:12:35.133" UserId="96" Text="The `xtractore` program from the [AEGeAn Toolkit](http://brendelgroup.github.io/AEGeAn/) was designed for this type of use case. Just set `--type=CDS`.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    $ xtractore -h&#xD;&#xA;    &#xD;&#xA;    xtractore: extract sequences corresponding to annotated features from the&#xD;&#xA;               given sequence file&#xD;&#xA;    &#xD;&#xA;    Usage: xtractore [options] features.gff3 sequences.fasta&#xD;&#xA;      Options:&#xD;&#xA;        -d|--debug            print debugging output&#xD;&#xA;        -h|--help             print this help message and exit&#xD;&#xA;        -i|--idfile: FILE     file containing a list of feature IDs (1 per line&#xD;&#xA;                              with no spaces); if provided, only features with&#xD;&#xA;                              IDs in this file will be extracted&#xD;&#xA;        -o|--outfile: FILE    file to which output sequences will be written;&#xD;&#xA;                              default is terminal (stdout)&#xD;&#xA;        -t|--type: STRING     feature type to extract; can be used multiple&#xD;&#xA;                              times to extract features of multiple types&#xD;&#xA;        -v|--version          print version number and exit&#xD;&#xA;        -V|--verbose          print verbose warning and error messages&#xD;&#xA;        -w|--width: INT       width of each line of sequence in the Fasta&#xD;&#xA;                              output; default is 80; set to 0 for no&#xD;&#xA;                              formatting&#xD;&#xA;&#xD;&#xA;It looks like you're processing prokaryotic genomes, but this program works also on eukaryotic genomes where CDSs often have introns interspersed." />
  <row Id="5272" PostHistoryTypeId="2" PostId="2367" RevisionGUID="5a0cb2b7-fbdf-4ac2-a0a8-952174c02786" CreationDate="2017-08-23T06:18:51.097" UserId="1364" Text="I want to use Canu to correct my nanopore long read (version: MinION R9.5), but I am not quite sure how to set the correctErrorRate. Whether I should follow the Canu manual (Nanopore R7 2D and Nanopore R9 1D Increase the maximum allowed difference in overlaps from the default of 14.4% to 22.5% with correctedErrorRate=0.225), or you guys have a better option? &#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;" />
  <row Id="5273" PostHistoryTypeId="1" PostId="2367" RevisionGUID="5a0cb2b7-fbdf-4ac2-a0a8-952174c02786" CreationDate="2017-08-23T06:18:51.097" UserId="1364" Text="Error rate setting in Canu error correction" />
  <row Id="5274" PostHistoryTypeId="3" PostId="2367" RevisionGUID="5a0cb2b7-fbdf-4ac2-a0a8-952174c02786" CreationDate="2017-08-23T06:18:51.097" UserId="1364" Text="&lt;canu&gt;" />
  <row Id="5275" PostHistoryTypeId="6" PostId="2367" RevisionGUID="a96b4bd1-fb4d-4f58-9ab4-0d94857d70a1" CreationDate="2017-08-23T09:22:12.157" UserId="294" Comment="added nanopore tag" Text="&lt;nanopore&gt;&lt;canu&gt;" />
  <row Id="5276" PostHistoryTypeId="24" PostId="2367" RevisionGUID="a96b4bd1-fb4d-4f58-9ab4-0d94857d70a1" CreationDate="2017-08-23T09:22:12.157" Comment="Proposed by 294 approved by 77 edit id of 276" />
  <row Id="5277" PostHistoryTypeId="2" PostId="2368" RevisionGUID="03c085de-a520-42f7-a039-39c8269a497d" CreationDate="2017-08-23T10:49:18.397" UserId="48" Text="DAVID depend on a couple of databases from the Gene Ontology Consortium, Reactome, KEGG,... most of them are accessible via Bioconductor. To perform an enrichment analysis you can have a look at the tutorial of the several [packages in Bioconductor][1] that do this. &#xD;&#xA;&#xD;&#xA;Some of the most important for analyzing enrichment in GO terms are [topGO][2], [goseq][3], [GOstats][4]. I would also recommend [GOSemSim][5] if you want to compare between GO  to focus on a specific GO terms.&#xD;&#xA;&#xD;&#xA;Other important packages are the [fgsea][6] to test any kind of gene set (which is similar to the one hosted by the Broad Institute), [gsva][7] for enrichment analysis by sample, [limma][8] has some functions for functional enrichment too. [Piano][9], [GSCA][10], [SPIA][11] are also worth mentioning.&#xD;&#xA;&#xD;&#xA;Bioconductor has &quot;standard&quot; data sets of the expressions of some cells, like [airway][12] and [ALL][13] frequently used in vignettes. They are not reference data sets because there isn't a reference expression for a cell of an organisms. It depends on the type of cell, the experiment, the conditions...&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bioconductor.org/packages/release/BiocViews.html#___GeneSetEnrichment&#xD;&#xA;  [2]: http://bioconductor.org/packages/topGO&#xD;&#xA;  [3]: http://bioconductor.org/packages/goseq&#xD;&#xA;  [4]: http://bioconductor.org/packages/GOstats&#xD;&#xA;  [5]: http://bioconductor.org/packages/GOSemSim&#xD;&#xA;  [6]: http://bioconductor.org/packages/fgsea&#xD;&#xA;  [7]: http://bioconductor.org/packages/gsva&#xD;&#xA;  [8]: http://bioconductor.org/packages/limma&#xD;&#xA;  [9]: http://bioconductor.org/packages/piano&#xD;&#xA;  [10]: http://bioconductor.org/packages/GSCA&#xD;&#xA;  [11]: http://bioconductor.org/packages/SPIA&#xD;&#xA;  [12]: http://bioconductor.org/packages/airway&#xD;&#xA;  [13]: http://bioconductor.org/packages/ALL" />
  <row Id="5278" PostHistoryTypeId="5" PostId="2363" RevisionGUID="7239dbbb-ea7d-4902-b82b-58c2f305b329" CreationDate="2017-08-23T12:43:23.493" UserId="37" Comment="added 12 characters in body; edited title" Text="I am constructing a bit of software which pipes the outputs of the bam file via `samtools view` into a script for parsing. My goal is to (somehow) make this process more efficient, and faster than `samtools view`. &#xD;&#xA;&#xD;&#xA;I am only using 3-4 fields in the bam. Naturally, if I only wanted those fields, I could do something like `samtools view file1.bam | cut -f ##`. &#xD;&#xA;&#xD;&#xA;However, I'm trying to be faster than this approach. More specifically: &#xD;&#xA;&#xD;&#xA;(1) Given the bam file format, is it theoretically possible to output several requested SAM fields faster than `samtools view|cut -f`? &#xD;&#xA;&#xD;&#xA;(2) Is there software available to achieve (1), or would one need to hack samtools? " />
  <row Id="5279" PostHistoryTypeId="4" PostId="2363" RevisionGUID="7239dbbb-ea7d-4902-b82b-58c2f305b329" CreationDate="2017-08-23T12:43:23.493" UserId="37" Comment="added 12 characters in body; edited title" Text="Is there a way to retrieve several SAM fields faster than `samtools view | cut -f`?" />
  <row Id="5280" PostHistoryTypeId="5" PostId="2358" RevisionGUID="83bd8965-4444-4571-a350-95d1c5da3bf6" CreationDate="2017-08-23T16:16:10.837" UserId="1283" Comment="corrected error about phenotype labeling in my second answer." Text="  I finally found another answer to my question. Please read this great article in May 12 2017 BioMed Central (BMC) Bioinformatics article titled [Ranking metrics in gene set enrichment analysis: do they matter?](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1674-0).&#xD;&#xA;&#xD;&#xA;Also, please read this blog , [Diving into Genetics and Genomics: Gene Set Enrichment Analysis (GSEA) explained](http://crazyhottommy.blogspot.com/2016/08/gene-set-enrichment-analysis-gsea.html).&#xD;&#xA;&#xD;&#xA;After reading these two articles, my choice for the best rapidGSEA local ranking measure is Minimum Significant Difference (i.e., MSD), because it has the best overall false positive rate (i.e., FPR) for larger sample sizes.&#xD;&#xA;&#xD;&#xA;Finally, it is important to realize that fgseaL's phenotype labeling can hypothetically be emulated by GSEA with either one of it's sixteen possible ranking metrics or a custom ranking metric." />
  <row Id="5286" PostHistoryTypeId="2" PostId="2370" RevisionGUID="79e41625-1ca3-4336-9309-bef58c57cd2a" CreationDate="2017-08-24T00:01:39.523" UserId="35" Text="It's generally a good idea to trust the &quot;official&quot; suggestions. You can also adjust the error rate based on coverage according to [parameter reference][1]:&#xD;&#xA;&#xD;&#xA;&gt; For low coverage datasets (less than 30X), we recommend increasing&#xD;&#xA;&gt; correctedErrorRate slightly, by 1% or so.&#xD;&#xA;&gt; &#xD;&#xA;&gt; For high-coverage datasets (more than 60X), we recommend decreasing&#xD;&#xA;&gt; correctedErrorRate slighly, by 1% or so.&#xD;&#xA;&gt; &#xD;&#xA;&gt; Raising the correctedErrorRate will increase run time. Likewise,&#xD;&#xA;&gt; decreasing correctedErrorRate will decrease run time, at the risk of&#xD;&#xA;&gt; missing overlaps and fracturing the assembly.&#xD;&#xA;&#xD;&#xA;That being said, the sequencing quality can vary from between libraries or flow cells. The error rate is not a constant.&#xD;&#xA;&#xD;&#xA;According to the developer ([full thread][2]):&#xD;&#xA;&#xD;&#xA;&gt; if you want to assemble 1D data or have a bad run, it is possible you&#xD;&#xA;&gt; would need to increase the error rate. You could run with a high error&#xD;&#xA;&gt; rate (0.1) and look at the distribution of overlap error rates in the&#xD;&#xA;&gt; unitig step to look for a peak in the distribution. If you have a near&#xD;&#xA;&gt; neighbor you could also map the corrected reads to it and estimate the&#xD;&#xA;&gt; residual error that way.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://canu.readthedocs.io/en/latest/parameter-reference.html&#xD;&#xA;  [2]: https://github.com/marbl/canu/issues/109" />
  <row Id="5287" PostHistoryTypeId="2" PostId="2371" RevisionGUID="98c5dd5d-8320-4a75-8b52-542608481975" CreationDate="2017-08-24T00:15:42.657" UserId="35" Text="Cell Ranger aggregate subsamples reads (unless you select `none`), so you will end up with less total reads in samples that have more initially. The output is still raw counts, but you will have more or less per cell.&#xD;&#xA;&#xD;&#xA;Seurat just merges the raw counts matrices and normalizes those.&#xD;&#xA;" />
  <row Id="5288" PostHistoryTypeId="2" PostId="2372" RevisionGUID="b0511904-b064-4b9c-98d0-fb6cd44e450a" CreationDate="2017-08-24T13:36:56.130" UserId="294" Text="The executable `fastq-sim` in [DNemulator][1] package is able to modify a set of input sequences in `fasta` format according to an external set of quality scores reported in a `fastq` file. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cbrc3.cbrc.jp/~martin/dnemulator/README.html" />
  <row Id="5289" PostHistoryTypeId="2" PostId="2373" RevisionGUID="50d3153a-5c4b-4f93-b5a0-a537a82b83d2" CreationDate="2017-08-24T13:42:57.060" UserId="522" Text="I'm looking for a way to identify low complexity regions and other repeats in the genome of *Escherichia coli*. I found that RepeatMasker may be used for example when drafting genomes of prokaryotes ([e.g.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3587949/)). But RepeatMasker works on a limited dataset of species, neither of them being prokaryotes. By default, when running RepeatMasker, if no species is specified, it will compare with homo sapiens data. &#xD;&#xA;&#xD;&#xA;This seems rather unadequate, but the most relevent alternative, [PRAP](https://sites.google.com/site/prapsoftware/), requires a &quot;dead&quot; tool (VisCoSe, by Michael Spitzer).&#xD;&#xA;&#xD;&#xA; 1. Is it still wise to to use RepeatMasker on *Escherichia coli*?&#xD;&#xA; 2.  If yes, which settings would maximise relevance ?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5290" PostHistoryTypeId="1" PostId="2373" RevisionGUID="50d3153a-5c4b-4f93-b5a0-a537a82b83d2" CreationDate="2017-08-24T13:42:57.060" UserId="522" Text="Is it wise to use RepeatMasker on prokaryotes?" />
  <row Id="5291" PostHistoryTypeId="3" PostId="2373" RevisionGUID="50d3153a-5c4b-4f93-b5a0-a537a82b83d2" CreationDate="2017-08-24T13:42:57.060" UserId="522" Text="&lt;repeat-elements&gt;" />
  <row Id="5292" PostHistoryTypeId="2" PostId="2374" RevisionGUID="10ebf8d8-862e-4db6-88b0-0a746e6b9381" CreationDate="2017-08-24T13:53:40.627" UserId="123" Text="If I understood correctly your question, you want to mask those regions in a (FASTA?) genome. I think you could identify those regions using [mummer][1] and mask them using [bedtools][2].&#xD;&#xA;&#xD;&#xA;    # align genome against itself&#xD;&#xA;    nucmer --maxmatch --nosimplify genome.fasta genome.fasta&#xD;&#xA;&#xD;&#xA;    # select repeats and convert the corrdinates to bed format&#xD;&#xA;    show-coords -r -T out.delta -H | awk '{if ($1 != $3 &amp;&amp; $2 != $4) print $0}' | awk '{print $8&quot;\t&quot;$1&quot;\t&quot;$2}' &gt; repeats.bed&#xD;&#xA;&#xD;&#xA;    # mask those bases with bedtools&#xD;&#xA;    bedtools maskfasta -fi genome.fasta -bed repeats.bed -fo masked.fasta&#xD;&#xA;&#xD;&#xA;Have a look at [nucmer][3] and [bedtools maskfasta][4] options to fine-tune your analysis.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://mummer.sourceforge.net/&#xD;&#xA;  [2]: http://bedtools.readthedocs.io/en/latest/&#xD;&#xA;  [3]: http://mummer.sourceforge.net/manual/#identifyingrepeats&#xD;&#xA;  [4]: http://bedtools.readthedocs.io/en/latest/content/tools/maskfasta.html" />
  <row Id="5293" PostHistoryTypeId="5" PostId="2367" RevisionGUID="6fa00bd0-cbee-4bc8-a244-c805080ceeb3" CreationDate="2017-08-24T15:42:12.300" UserId="96" Comment="minor fixes, retag" Text="I want to use Canu to correct my nanopore long read (version: MinION R9.5), but I am not quite sure how to set the correctErrorRate. Should I follow the Canu manual (Nanopore R7 2D and Nanopore R9 1D Increase the maximum allowed difference in overlaps from the default of 14.4% to 22.5% with correctedErrorRate=0.225), or you guys have a better option? &#xD;&#xA;" />
  <row Id="5294" PostHistoryTypeId="6" PostId="2367" RevisionGUID="6fa00bd0-cbee-4bc8-a244-c805080ceeb3" CreationDate="2017-08-24T15:42:12.300" UserId="96" Comment="minor fixes, retag" Text="&lt;nanopore&gt;&lt;canu&gt;&lt;quality-control&gt;" />
  <row Id="5296" PostHistoryTypeId="2" PostId="2375" RevisionGUID="87015c1f-f585-4230-bb1f-09c848e3a575" CreationDate="2017-08-24T16:38:17.097" UserId="123" Text="I'm trying to run a differential gene expression analysis using DESeq2, with counts coming from kallisto. I have imported them using tximport and I'm creating the `DESeqDataSet` (dds) using the `DESeqDataSetFromMatrix` function.&#xD;&#xA;&#xD;&#xA;    &gt; dds &lt;- DESeqDataSetFromMatrix(counts,&#xD;&#xA;                                    samples,&#xD;&#xA;                                    ~ strain + batch)&#xD;&#xA;&#xD;&#xA;And I get the following error, expected given my experimental design:&#xD;&#xA;&#xD;&#xA;    Error in checkFullRank(modelMatrix) :&#xD;&#xA;      the model matrix is not full rank, so the model cannot be fit as specified.&#xD;&#xA;      One or more variables or interaction terms in the design formula are linear&#xD;&#xA;      combinations of the others and must be removed.&#xD;&#xA;&#xD;&#xA;Now, I know that I can just remove one column from the design matrix to make it work, but is there a way to supply my own design matrix to DESeq2? The following code raises an error:&#xD;&#xA;&#xD;&#xA;    &gt; design &lt;- model.matrix(~strain+batch, s2c)&#xD;&#xA;    &gt; design = design[, -9] # removing a column to avoid full-rank&#xD;&#xA;    &gt; dds &lt;- DESeqDataSetFromMatrix(counts, s2c, design=design)&#xD;&#xA;    converting counts to integer mode&#xD;&#xA;    Error: $ operator is invalid for atomic vectors&#xD;&#xA;&#xD;&#xA;Is there a way to provide my own `model.matrix`?&#xD;&#xA;&#xD;&#xA;p.s. the modified model works in sleuth, but I would like to use DESeq2 for this particular analysis." />
  <row Id="5297" PostHistoryTypeId="1" PostId="2375" RevisionGUID="87015c1f-f585-4230-bb1f-09c848e3a575" CreationDate="2017-08-24T16:38:17.097" UserId="123" Text="Is it possible to create a DESeqDataSet with a user-provided design matrix?" />
  <row Id="5298" PostHistoryTypeId="3" PostId="2375" RevisionGUID="87015c1f-f585-4230-bb1f-09c848e3a575" CreationDate="2017-08-24T16:38:17.097" UserId="123" Text="&lt;rna-seq&gt;&lt;deseq2&gt;" />
  <row Id="5299" PostHistoryTypeId="2" PostId="2376" RevisionGUID="dad40082-90a4-40ee-8f2e-99fa22030857" CreationDate="2017-08-24T17:32:11.120" UserId="77" Text="Provide rank sufficient design to `DESeqDataSetFromMatrix` and then use your custom model matrix in `DESeq`. In essence:&#xD;&#xA;&#xD;&#xA;    dds = DESeqDataSetFromMatric(counts, s2c, design=~batch)&#xD;&#xA;    design &lt;- model.matrix(~strain+batch, s2c)&#xD;&#xA;    design = design[, -9]&#xD;&#xA;    DESeq(dds, full=design)&#xD;&#xA;&#xD;&#xA;See [this thread](https://support.bioconductor.org/p/64480/) on the bioconductor site for details." />
  <row Id="5301" PostHistoryTypeId="2" PostId="2377" RevisionGUID="9803f1cc-a416-48e7-b4ba-94371e591b1c" CreationDate="2017-08-24T21:07:58.983" UserId="711" Text="I'm having a difficulty in grasping the general purpose and concept of indel calling.&#xD;&#xA;&#xD;&#xA;What exactly is this process? " />
  <row Id="5302" PostHistoryTypeId="1" PostId="2377" RevisionGUID="9803f1cc-a416-48e7-b4ba-94371e591b1c" CreationDate="2017-08-24T21:07:58.983" UserId="711" Text="What is Indel Calling and what is its purpose?" />
  <row Id="5303" PostHistoryTypeId="3" PostId="2377" RevisionGUID="9803f1cc-a416-48e7-b4ba-94371e591b1c" CreationDate="2017-08-24T21:07:58.983" UserId="711" Text="&lt;genome&gt;&lt;indel&gt;" />
  <row Id="5304" PostHistoryTypeId="2" PostId="2378" RevisionGUID="ceeb2117-8a11-4b3e-8167-92c1edc62edd" CreationDate="2017-08-24T21:26:20.367" UserId="1375" Text="Given an experiment consisting of an input (baseline RNA) and IP (pulldown to find RNAs bound to certain protein of interest)... Is a DE analysis performed over the RNA-seq data from the samples (lets say with EdgeR or DESEQ2) suitable to reveal the preferentially bound RNAs? What other software tools would you recommend?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5305" PostHistoryTypeId="1" PostId="2378" RevisionGUID="ceeb2117-8a11-4b3e-8167-92c1edc62edd" CreationDate="2017-08-24T21:26:20.367" UserId="1375" Text="RIP-seq analysis?" />
  <row Id="5306" PostHistoryTypeId="3" PostId="2378" RevisionGUID="ceeb2117-8a11-4b3e-8167-92c1edc62edd" CreationDate="2017-08-24T21:26:20.367" UserId="1375" Text="&lt;rna-seq&gt;&lt;software-recommendation&gt;" />
  <row Id="5307" PostHistoryTypeId="2" PostId="2379" RevisionGUID="3efaa25e-475c-4b12-bed9-4becc1be99ca" CreationDate="2017-08-24T22:44:21.977" UserId="96" Text="Insertions and deletions (indels) are one type among many different types of *genetic variation*, such as single nucleotide variants (SNVs), copy number variants (CNVs), and structural variants (SVs).&#xD;&#xA;&#xD;&#xA;The goal of indel calling, like the goal of any variant calling, is to identify genetic variants that can subsequently be associated with important phenotypes, esp. disease. For example, if 60% of patients with disease XYZ have an indel in the promoter region of gene 123, then that is information of extreme interest and value in research and in clinical care.&#xD;&#xA;&#xD;&#xA;Genome-wide association studies (GWAS) have been trying to correlate SNVs to disease and other phenotypes for years. Much less work has been done with indels, but their discovery and analysis remains an area of intense interest.&#xD;&#xA;&#xD;&#xA;As far as the *process* of indel calling, large indels can usually be found  by mapping paired reads and looking for large discrepancies between the expected distance between pairs and the observed distance.&#xD;&#xA;&#xD;&#xA;&gt; Huh, my average insert size is 400bp, but the aligned read pairs flanking this area are 1200bp apart. Must be an 800bp insertion in there!&#xD;&#xA;&#xD;&#xA;Smaller indels are much more difficult to detect, since they are harder to distinguish from the noise (i.e. the variation in length of sequenced fragments)." />
  <row Id="5308" PostHistoryTypeId="6" PostId="2377" RevisionGUID="8ca3d37b-e2a2-4a2c-b641-1b823a175b7e" CreationDate="2017-08-24T22:44:55.177" UserId="96" Comment="retag" Text="&lt;genome&gt;&lt;variant-calling&gt;&lt;indel&gt;" />
  <row Id="5309" PostHistoryTypeId="5" PostId="2379" RevisionGUID="4691cbda-4ba5-46e5-9a1d-393748bead94" CreationDate="2017-08-24T22:50:32.847" UserId="96" Comment="added 141 characters in body" Text="Insertions and deletions (indels) are one type among many different types of *genetic variation*, such as single nucleotide variants (SNVs), copy number variants (CNVs), and structural variants (SVs). I'll assume here that you know how indels are defined, but are simple trying to understand the importance of discovering and analyzing them.&#xD;&#xA;&#xD;&#xA;The goal of indel calling, like the goal of any variant calling, is to identify genetic variants that can subsequently be associated with important phenotypes, esp. disease. For example, if 60% of patients with disease XYZ have an indel in the promoter region of gene 123, then that is information of extreme interest and value in research and in clinical care.&#xD;&#xA;&#xD;&#xA;Genome-wide association studies (GWAS) have been trying to correlate SNVs to disease and other phenotypes for years. Much less work has been done with indels, but their discovery and analysis remains an area of intense interest.&#xD;&#xA;&#xD;&#xA;As far as the *process* of indel calling, large indels can usually be found  by mapping paired reads and looking for large discrepancies in the expected distance between pairs and the observed distance.&#xD;&#xA;&#xD;&#xA;&gt; Huh, my average insert size is 400bp, but the aligned read pairs flanking this area are 1200bp apart. Must be an 800bp insertion in there!&#xD;&#xA;&#xD;&#xA;Smaller indels are much more difficult to detect, since they are harder to distinguish from noise (i.e. the variation in length of sequenced fragments)." />
  <row Id="5310" PostHistoryTypeId="6" PostId="2373" RevisionGUID="60e577a5-d62e-4809-b100-0d0ad1e56272" CreationDate="2017-08-24T23:29:23.597" UserId="123" Comment="Added some more tags" Text="&lt;genome&gt;&lt;repeat-elements&gt;&lt;sequence-analysis&gt;" />
  <row Id="5311" PostHistoryTypeId="24" PostId="2373" RevisionGUID="60e577a5-d62e-4809-b100-0d0ad1e56272" CreationDate="2017-08-24T23:29:23.597" Comment="Proposed by 123 approved by 96, -1 edit id of 277" />
  <row Id="5312" PostHistoryTypeId="5" PostId="2373" RevisionGUID="90ae90f0-1250-4fd4-9c12-af63b13f2068" CreationDate="2017-08-24T23:29:23.597" UserId="73" Comment="Added some more tags" Text="I'm looking for a way to identify low complexity regions and other repeats in the genome of *Escherichia coli*. I found that RepeatMasker may be used for example when drafting genomes of prokaryotes ([*E. coli* example](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3587949/)). But RepeatMasker works on a limited dataset of species, neither of them being prokaryotes. By default, when running RepeatMasker, if no species is specified, it will compare with homo sapiens data. &#xD;&#xA;&#xD;&#xA;This seems rather inadequate, but the most relevent alternative, [PRAP](https://sites.google.com/site/prapsoftware/), requires a &quot;dead&quot; tool (VisCoSe, by Michael Spitzer).&#xD;&#xA;&#xD;&#xA; 1. Is it still wise to to use RepeatMasker on *Escherichia coli*?&#xD;&#xA; 2.  If yes, which settings would maximise relevance ?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="5313" PostHistoryTypeId="2" PostId="2380" RevisionGUID="082758cf-d0aa-48db-a5c2-351ca229015e" CreationDate="2017-08-25T06:54:34.863" UserId="77" Text="You can try doing standard differential expression, but I worry that the between-sample normalization will work poorly. Personally, I would do peak calling instead, followed by diffBind. You have a few tools to choose from when it comes to this. In the past, I've rolled my own methods for this using MACS2 and genomic alignments (I then converted those to bedGraph files where entries are transcripts, so peak calling is then in transcript-coordinates).&#xD;&#xA;&#xD;&#xA;It's probably easier, though, to use premade software. There are a number of packages out there for RIP-seq. These include [piranha](https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/bts569), [RIPSeeker](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3632129/) and [ASPeak](https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btt428). Particularly if you're just starting out, you'd be well served with using one of those tools." />
  <row Id="5314" PostHistoryTypeId="2" PostId="2381" RevisionGUID="735eba5e-f324-49e6-92fa-1d7a8953f700" CreationDate="2017-08-25T11:15:54.753" UserId="235" Text="You need to be careful of terminology. To me, a RIP-seq experiment involves a pull down, followed by a RNAseq library prep. Thus, the whole transcript is captured, not just the binding site of the protein (as in CLIP-Seq, HITS-CLIP, PAR-CLIP, iCLIP or eCLIP). Thus &quot;peak-callers&quot; whether they be designed for calling protein-DNA or protein-RNA binding sites are not suitable as the signal will not be punctuate or peaky. Of the methods mentioned by @Devon Ryan, only [RIPSeeker][1] seems setup to deal with this sort of data. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;In some of the papers that talk about calling RNA-protein interactions they lump RIP-seq in with the CLIP techniques (see piranha and ASPeak papers referenced by @Devon Ryan). They appear to be talking about a technique where the RNA is fragmented before it is pulled down. Thus you would capture peaky binding sites (actually, the protocol would be remarkably similar to CLIP). &#xD;&#xA;&#xD;&#xA;You should work out which of these you have. &#xD;&#xA;&#xD;&#xA;As for using DESeq/EdgeR etc, in the RIPSeeker paper they say:&#xD;&#xA;&#xD;&#xA;&gt; Furthermore, programs for de novo transcript assembly followed by&#xD;&#xA;&gt; differential expression (DE) analysis, such as the Cufflinks/Cuffdiff&#xD;&#xA;&gt; suite (15,16), and for DE on a set of known transcripts, such as DESeq&#xD;&#xA;&gt; (17), may appear applicable to RIP-seq analysis. Unlike peak-calling&#xD;&#xA;&gt; strategy, however, the transcript-based methods assume the full&#xD;&#xA;&gt; transcriptome being sequenced at a fairly deep coverage (as usually&#xD;&#xA;&gt; the case in RNA-seq) and thus may be sensitive to background noise&#xD;&#xA;&gt; typical to the IP-based protocols, which is due to both the&#xD;&#xA;&gt; non-specific RNA interactions with a protein of interest and the&#xD;&#xA;&gt; non-specific RNA input from the pull-down of the (mutant) control&#xD;&#xA;&gt; (Supplementary Figures S3 and S4).&#xD;&#xA;&#xD;&#xA;I don't know to what extent RIPSeeker performs better than the naive DESeq approach as its not included as one of the comparators in the paper. &#xD;&#xA;&#xD;&#xA;  [1]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3632129/" />
  <row Id="5315" PostHistoryTypeId="2" PostId="2382" RevisionGUID="4013d064-1519-49af-9f79-fc8aeef1def3" CreationDate="2017-08-25T11:31:05.573" UserId="37" Text="There are two types of INDELs: short indels and long indels. Some put the threshold at 50bp; others choose 1000bp. Short and long indels are called differently.&#xD;&#xA;&#xD;&#xA;&lt;50bp short indels are called from read alignment directly. Modern indel callers essentially build a multi-alignment of reads and call an indel with enough read supports. Short indels may break open reading frames.&#xD;&#xA;&#xD;&#xA;For Illumina reads, long indels are usually called with split alignment or read pair alignment because mainstream aligners are unable to directly align through a long indel. Long indels are a type of structural variation. They may affect gene structures in a more dramatic way (e.g. delete a whole exon; screw up transcription by transposon insertion; create pseudogenes).&#xD;&#xA;&#xD;&#xA;Ultimately, both short and long indels are types of genetic variants. Calling them helps to understand how genetics shapes phenotypes." />
  <row Id="5316" PostHistoryTypeId="2" PostId="2383" RevisionGUID="e78081d3-937a-429b-81f1-40abd475932a" CreationDate="2017-08-25T11:42:24.700" UserId="37" Text="[This preprint](http://www.biorxiv.org/content/early/2017/04/11/126656) uses pbsim to simulate ONT RNA-seq reads for fruit fly. It is probably worth reading if you want to do the same thing.&#xD;&#xA;&#xD;&#xA;You should include INDEL errors. Those are what make RNA-seq alignment challenging. For the benchmark purpose, adding INDELs does not increase the complexity at all. You can parse splice junctions on the reference from CIGAR and compare them to the annotation. You don't need to worry about the base-level alignment.&#xD;&#xA;&#xD;&#xA;In addition, there are [public real ONT data](https://www.nature.com/articles/ncomms16027) (AC:SRP082530) for [SIRV spike-in control](https://www.lexogen.com/sirvs/) and mouse B cells. You don't actually need simulation." />
  <row Id="5317" PostHistoryTypeId="2" PostId="2384" RevisionGUID="4251d53a-68ed-4487-8a0c-69f99387c457" CreationDate="2017-08-25T11:46:38.820" UserId="1377" Text="I have 3224 Ensembl id's as rownames in a dataframe &quot;G&quot;. To convert Ensembl ids into Genesymbols I used biomart like following.&#xD;&#xA;&#xD;&#xA;    library('biomaRt')&#xD;&#xA;    mart &lt;- useDataset(&quot;hsapiens_gene_ensembl&quot;, useMart(&quot;ensembl&quot;))&#xD;&#xA;    genes &lt;- rownames(G)&#xD;&#xA;    G &lt;-G[,-6]&#xD;&#xA;    G_list &lt;- getBM(filters= &quot;ensembl_gene_id&quot;, attributes= c(&quot;ensembl_gene_id&quot;,&quot;hgnc_symbol&quot;),values=genes,mart= mart)&#xD;&#xA;Now in G_list I can see only 3200 ensembl ids showing Genesymbols / No Gene_symbols. Why the other 24 ensembl ids are not seen in G_list? If there are no gene_symbol for those 24 ensembl ids it should atleast show &quot;-&quot;&#xD;&#xA;&#xD;&#xA;what is the problem here?" />
  <row Id="5318" PostHistoryTypeId="1" PostId="2384" RevisionGUID="4251d53a-68ed-4487-8a0c-69f99387c457" CreationDate="2017-08-25T11:46:38.820" UserId="1377" Text="Ensembl id to GeneSymbol with biomart" />
  <row Id="5319" PostHistoryTypeId="3" PostId="2384" RevisionGUID="4251d53a-68ed-4487-8a0c-69f99387c457" CreationDate="2017-08-25T11:46:38.820" UserId="1377" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;biomart&gt;" />
  <row Id="5320" PostHistoryTypeId="6" PostId="2384" RevisionGUID="973d593b-806e-4e4f-be0b-2050ca329e90" CreationDate="2017-08-25T11:59:11.217" UserId="77" Comment="edited tags" Text="&lt;r&gt;&lt;bioconductor&gt;&lt;biomart&gt;&lt;ensembl&gt;" />
  <row Id="5321" PostHistoryTypeId="5" PostId="2384" RevisionGUID="b968d12f-d9ee-4eaa-b9a4-b81bd71fd19b" CreationDate="2017-08-25T12:07:08.273" UserId="77" Comment="added 192 characters in body" Text="I have 3224 Ensembl id's as rownames in a dataframe &quot;G&quot;. To convert Ensembl ids into Genesymbols I used biomart like following.&#xD;&#xA;&#xD;&#xA;    library('biomaRt')&#xD;&#xA;    mart &lt;- useDataset(&quot;hsapiens_gene_ensembl&quot;, useMart(&quot;ensembl&quot;))&#xD;&#xA;    genes &lt;- rownames(G)&#xD;&#xA;    G &lt;-G[,-6]&#xD;&#xA;    G_list &lt;- getBM(filters= &quot;ensembl_gene_id&quot;, attributes= c(&quot;ensembl_gene_id&quot;,&quot;hgnc_symbol&quot;),values=genes,mart= mart)&#xD;&#xA;Now in G_list I can see only 3200 ensembl ids showing Genesymbols / No Gene_symbols. Why the other 24 ensembl ids are not seen in G_list? If there are no gene_symbol for those 24 ensembl ids it should atleast show &quot;-&quot;&#xD;&#xA;&#xD;&#xA;Examples of problematic IDs are: ENSG00000257061, ENSG00000255778, ENSG00000267268. These are not at all shown in G_list (biomaRt). So, I gave them in biodbnet, which seems to handle them.&#xD;&#xA;&#xD;&#xA;what is the problem here?" />
  <row Id="5322" PostHistoryTypeId="2" PostId="2385" RevisionGUID="3a7c4805-e81c-4b9d-bb30-2bfa0db0c4bb" CreationDate="2017-08-25T12:10:40.503" UserId="77" Text="It looks like you were using an old annotation. The problematic IDs you posted existed in the GRCh37 annotations, but don't in the most recent GRCh38 annotation. For that reason they were excluded. The IDs that have `-` as symbols don't have associated symbols, but are present in the database." />
  <row Id="5323" PostHistoryTypeId="2" PostId="2386" RevisionGUID="b50daa02-f84d-42d8-b993-7d457eed86a2" CreationDate="2017-08-25T12:17:12.407" UserId="37" Text="I modified your original question: as you are extracting 4 fields, you are not outputting BAM. The answer to the modified question is: yes, you can write a C program with htslib (or with bamtools, bioD, bioGo or rust-bio). Formatting an entire SAM is fairly expensive. You can see this by comparing `samtools view aln.bam &gt; /dev/null` and `samtools view -u aln.bam &gt; /dev/null`. With a C program, you can select fields to output. This will give you a noticeable performance boost, depending on the fields you extract.&#xD;&#xA;&#xD;&#xA;In general, if you really care about performance, don't use script. The time you save from optimizing `samtools view|cut -f` will be quickly neutralized by the inefficiency of most scripting languages." />
  <row Id="5324" PostHistoryTypeId="5" PostId="2383" RevisionGUID="9c1a7658-09eb-4cb1-8075-6c4e69994ef5" CreationDate="2017-08-25T12:23:33.937" UserId="37" Comment="added 112 characters in body" Text="[This preprint](http://www.biorxiv.org/content/early/2017/04/11/126656) uses pbsim to simulate ONT RNA-seq reads for fruit fly. It is probably worth reading if you want to do the same thing.&#xD;&#xA;&#xD;&#xA;You should include INDEL errors. Those are what make RNA-seq alignment challenging. For the benchmark purpose, adding INDELs does not increase the complexity at all. You can parse splice junctions on the reference from CIGAR and compare them to the annotation. You don't need to worry about the base-level alignment.&#xD;&#xA;&#xD;&#xA;In addition, there are [public real ONT data](https://www.nature.com/articles/ncomms16027) (AC:SRP082530) for [SIRV spike-in control](https://www.lexogen.com/sirvs/) and mouse B cells. You don't actually need simulation.&#xD;&#xA;&#xD;&#xA;PS: just noticed that you are an author of the first preprint I cited. I would use real data for evaluation." />
  <row Id="5325" PostHistoryTypeId="5" PostId="2385" RevisionGUID="c06e2cad-ea82-4e15-9cfb-3094fed835ea" CreationDate="2017-08-25T13:03:35.240" UserId="77" Comment="added 172 characters in body" Text="It looks like you were using an old annotation. The problematic IDs you posted existed in the GRCh37 annotations, but don't in the most recent GRCh38 annotation. For that reason they were excluded. The IDs that have `-` as symbols don't have associated symbols, but are present in the database.&#xD;&#xA;&#xD;&#xA;To use an archived version in biomart:&#xD;&#xA;&#xD;&#xA;    mart = useDataset(&quot;hsapiens_gene_ensembl&quot;, useEnsembl(biomart=&quot;ensembl&quot;, version=84))&#xD;&#xA;&#xD;&#xA;That's an example for release 84." />
  <row Id="5326" PostHistoryTypeId="2" PostId="2387" RevisionGUID="6507db72-7392-400e-b487-24a2c7510592" CreationDate="2017-08-25T14:27:47.847" UserId="272" Text="Another library you can use for this purpose is the [htsjdk][1], which is written in java. Using htsjdk with java is analogous to using htslib with C; the BAM format is already handled by the library and you can manipulate fields in your own code. The same basic analysis applies to java as C; you do not need to convert the BAM file to text and then parse it. Generally, a program implemented in java will be slower than C but still significantly faster than scripting. You do get language features of java: portability, array bounds checking, garbage collection, etc. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/samtools/htsjdk" />
  <row Id="5327" PostHistoryTypeId="5" PostId="2379" RevisionGUID="862e7313-2f6f-4142-b38f-3d425940a8cd" CreationDate="2017-08-25T15:58:42.050" UserId="96" Comment="insertion --&gt; deletion" Text="Insertions and deletions (indels) are one type among many different types of *genetic variation*, such as single nucleotide variants (SNVs), copy number variants (CNVs), and structural variants (SVs). I'll assume here that you know how indels are defined, but are simple trying to understand the importance of discovering and analyzing them.&#xD;&#xA;&#xD;&#xA;The goal of indel calling, like the goal of any variant calling, is to identify genetic variants that can subsequently be associated with important phenotypes, esp. disease. For example, if 60% of patients with disease XYZ have an indel in the promoter region of gene 123, then that is information of extreme interest and value in research and in clinical care.&#xD;&#xA;&#xD;&#xA;Genome-wide association studies (GWAS) have been trying to correlate SNVs to disease and other phenotypes for years. Much less work has been done with indels, but their discovery and analysis remains an area of intense interest.&#xD;&#xA;&#xD;&#xA;As far as the *process* of indel calling, large indels can usually be found  by mapping paired reads and looking for large discrepancies in the expected distance between pairs and the observed distance.&#xD;&#xA;&#xD;&#xA;&gt; Huh, my average insert size is 400bp, but the aligned read pairs flanking this area are 1200bp apart. Must be an 800bp deletion in there!&#xD;&#xA;&#xD;&#xA;Smaller indels are much more difficult to detect, since they are harder to distinguish from noise (i.e. the variation in length of sequenced fragments)." />
  <row Id="5328" PostHistoryTypeId="2" PostId="2388" RevisionGUID="5932217f-66c9-4f3e-a11c-566283b43233" CreationDate="2017-08-25T21:30:44.297" UserId="1302" Text="Here's my current understanding of the freemix metric output by verifyBamID. The freemix score is a measure of contamination that is calculated without knowing the genotypes of the individuals. In the paper it is equation (2), and the authors refer to it as the &quot;sequence-only&quot; method. However, in addition to the sequencing reads, it requires some knowledge of SNPs and their allele frequency from the population. They demonstrate in the paper that using the SNPs and allele frequency estimates from a different population actually reduces the estimated level of contamination.&#xD;&#xA;&#xD;&#xA;As for intuition for what equation (2) is measuring, imagine what would happen to the number of heterozygous sites detected if DNA from two different individuals was sequenced together. The total number of heterozygous sites would increase because it would count all sites where the two individuals are homozygous for opposite alleles or at least one of the two individuals is heterozygous. This would lead to an increased fraction of heterozygous sites over that expected from [Hardy-Weinberg Equilibrium](https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle) (i.e. 2pq).&#xD;&#xA;&#xD;&#xA;Here's the [explanation of freemix](https://groups.google.com/d/msg/verifybamid/oNMOfEZDwE4/AbVP_D-mbBEJ) directly from the author, Hyun Min Kang:&#xD;&#xA;&#xD;&#xA;&gt; The key idea of FREEMIX estimate is to use excessive heterozygosity to estimate the level of contamination. Especially for common SNPs, you will observe higher fraction of heterozygous alleles than 2*p*(1-p), and it turns out that you can quantify the contamination very well if you know the population allele frequency already. If you do not have accurate population allele frequency information, than it would be harder to estimate FREEMIX parameters using verifyBamID.&#xD;&#xA;&#xD;&#xA;Additional sources:&#xD;&#xA;&#xD;&#xA;* [Verifying Sample Identities - Implementation](http://genome.sph.umich.edu/wiki/Verifying_Sample_Identities_-_Implementation)&#xD;&#xA;* [Interpreting output files](http://genome.sph.umich.edu/wiki/VerifyBamID#Interpreting_output_files)&#xD;&#xA;* [verifyBamID Google Group](https://groups.google.com/forum/#!forum/verifybamid)&#xD;&#xA;" />
  <row Id="5330" PostHistoryTypeId="2" PostId="2389" RevisionGUID="c5e43c62-8117-40f1-90aa-24034fa22916" CreationDate="2017-08-26T18:22:30.873" UserId="1384" Text="How are you using altmetrics, have they made any difference to impact of your publications? &#xD;&#xA;&#xD;&#xA;[‘Altmetrics’! Can you afford to ignore it?][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bjsm.bmj.com/content/early/2017/08/22/bjsports-2017-098258" />
  <row Id="5331" PostHistoryTypeId="1" PostId="2389" RevisionGUID="c5e43c62-8117-40f1-90aa-24034fa22916" CreationDate="2017-08-26T18:22:30.873" UserId="1384" Text="Are altmetrics any use?" />
  <row Id="5332" PostHistoryTypeId="3" PostId="2389" RevisionGUID="c5e43c62-8117-40f1-90aa-24034fa22916" CreationDate="2017-08-26T18:22:30.873" UserId="1384" Text="&lt;file-formats&gt;" />
  <row Id="5333" PostHistoryTypeId="2" PostId="2390" RevisionGUID="58f4b7b9-53e6-4f7f-a545-96846dcd9fa0" CreationDate="2017-08-26T21:14:09.057" UserId="1385" Text="In my greedy search with pseudocounts algorithm in my bioinformatics course, I did not follow the pseudocode since I wanted to solve the problem in my own way. Unfortunately, although my answer is correct, the answer we are supposed to give is the first correct answer, not the last.&#xD;&#xA;&#xD;&#xA;The problem works as such: Given t strings, you make a profile matrix from the first k-mer in the first string, and then you look at the next string and using that profile matrix, you find the best k-mer match, and then you combine that k-mer with the one from the first string to make a new profile matrix, etc. You eventually do this for all k-mer windows in your first string of DNA. The way I did it was by making a probability profile matrix from the best k-mer groups in each case, and then multiplying the greatest probabilities in each column. This gives me a score, and if the next score is higher (thus, more probable) that group of k-mers is used. Below is my code:&#xD;&#xA;&#xD;&#xA;     def highest_probability(string, k, matrix):&#xD;&#xA;        score = 1&#xD;&#xA;        temp_string = &quot;&quot;&#xD;&#xA;        c = 0&#xD;&#xA;        best_string = &quot;&quot;&#xD;&#xA;        temp = 0&#xD;&#xA;        while c+k &lt; len(string):&#xD;&#xA;            for i in range(c, c+k):&#xD;&#xA;                if string[i] == 'A':&#xD;&#xA;                    score *= matrix[i-c]['A']&#xD;&#xA;                    temp_string += 'A'&#xD;&#xA;                elif string[i] == 'C':&#xD;&#xA;                    score *= matrix[i-c]['C']&#xD;&#xA;                    temp_string += 'C'&#xD;&#xA;                elif string[i] == 'G':&#xD;&#xA;                    score *= matrix[i-c]['G']&#xD;&#xA;                    temp_string += 'G'&#xD;&#xA;                elif string[i] == 'T':&#xD;&#xA;                    score *= matrix[i-c]['T']&#xD;&#xA;                    temp_string += 'T'&#xD;&#xA;                if i == c+k-1:&#xD;&#xA;                    if score &gt;= temp:&#xD;&#xA;                        temp = score&#xD;&#xA;                        best_string = temp_string&#xD;&#xA;            temp_string=&quot;&quot;&#xD;&#xA;            score=1&#xD;&#xA;            c+=1&#xD;&#xA;        print(best_string)&#xD;&#xA;        return best_string&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    def GreedyMotifSearch(DNA, k, t):&#xD;&#xA;        best_motifs = [i[0:k] for i in DNA]&#xD;&#xA;        score=0&#xD;&#xA;        c=0&#xD;&#xA;        while c+k&lt;len(DNA[0]):&#xD;&#xA;            k_mer = DNA[0][c:c+k]&#xD;&#xA;            motif=[k_mer]&#xD;&#xA;            matrix = make_matrix(k_mer, matrix=[], i=0)&#xD;&#xA;            for i in range(1, t):&#xD;&#xA;                compare_string = DNA[i]&#xD;&#xA;                good_motif = highest_probability(compare_string, k, matrix)&#xD;&#xA;                motif.append(good_motif)&#xD;&#xA;                matrix = make_matrix(good_motif, matrix, i)&#xD;&#xA;            temp_score=1&#xD;&#xA;            for dictionary in matrix:&#xD;&#xA;                temp_score *= max(dictionary.values())&#xD;&#xA;            print(temp_score)&#xD;&#xA;            if temp_score &gt; score:&#xD;&#xA;                score = temp_score&#xD;&#xA;                best_motifs = motif&#xD;&#xA;            c+=1&#xD;&#xA;        return best_motifs&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    def make_matrix(string, matrix, i):&#xD;&#xA;        if len(matrix)==0:&#xD;&#xA;            for ch in string:&#xD;&#xA;                new_dict = {'A':0, 'C':0, 'G':0, 'T':0}&#xD;&#xA;                new_dict[ch] = 1&#xD;&#xA;                matrix += [new_dict]&#xD;&#xA;            print(matrix)&#xD;&#xA;            return matrix&#xD;&#xA;        elif len(matrix)==3:&#xD;&#xA;            for j in range(len(string)):&#xD;&#xA;                matrix[j][string[j]] += 1&#xD;&#xA;            Sum = [sum(matrix[0].values()),sum(matrix[1].values()),sum(matrix[2].values())]&#xD;&#xA;            for columns in range(len(matrix)):&#xD;&#xA;                for keys in matrix[columns]:&#xD;&#xA;                    matrix[columns][keys] /= Sum[columns]&#xD;&#xA;            print(matrix)&#xD;&#xA;            return matrix &#xD;&#xA;            &#xD;&#xA;    DNA=['GGCGTTCAGGCA',&#xD;&#xA;    'AAGAATCAGTCA',&#xD;&#xA;    'CAAGGAGTTCGC',&#xD;&#xA;    'CACGTCAATCAC',&#xD;&#xA;    'CAATAATATTCG']&#xD;&#xA;    k=3&#xD;&#xA;    t=5&#xD;&#xA;    print(GreedyMotifSearch(DNA, k, t))  &#xD;&#xA;&#xD;&#xA;      &#xD;&#xA;      &#xD;&#xA;&#xD;&#xA;The output gives me ['TTC', 'GTC', 'TTC', 'GTC', 'TTC'] which is equally correct to the answer given in the problem output which is TTC,ATC,TTC,ATC,TTC. Unfortunately, my answer is the last correct occurrence. I tried adjusting my highest_probability function with if score is greater than temp instead of greater or equal, but that gives me the answer ['GGC', '', '', '', ''] which I don't understand at all. My question is 1) Why is this the case? and 2) How can I fix this? And does anyone have a simpler way of doing this?&#xD;&#xA;" />
  <row Id="5334" PostHistoryTypeId="1" PostId="2390" RevisionGUID="58f4b7b9-53e6-4f7f-a545-96846dcd9fa0" CreationDate="2017-08-26T21:14:09.057" UserId="1385" Text="Greedy Motif Search Using Probability Matrices" />
  <row Id="5335" PostHistoryTypeId="3" PostId="2390" RevisionGUID="58f4b7b9-53e6-4f7f-a545-96846dcd9fa0" CreationDate="2017-08-26T21:14:09.057" UserId="1385" Text="&lt;python&gt;" />
  <row Id="5336" PostHistoryTypeId="2" PostId="2391" RevisionGUID="eb2792eb-b717-46eb-9c77-dc0239891b94" CreationDate="2017-08-27T02:19:55.020" UserId="73" Text="Assuming you're talking about [Implement GreedyMotifSearch](http://rosalind.info/problems/ba2d/) from Rosalind, while I haven't actually finished this particular one, I've got an idea why `TTC,ATC,TTC,ATC,TTC` doesn't work. Here's my explanation from the &quot;Questions&quot; section of that problem:&#xD;&#xA;&#xD;&#xA;&gt; The profile is created from the first kmers in each string, and the most likely string from those is CAG. The answers will generally be similar to this.&#xD;&#xA;&#xD;&#xA;I've been trying to do Rosalind by only using the information provided in the questions; this is one of the many problems where there wasn't enough information in the Question for me to arrive at a correct solution when I tried it. It's possible that the debug datasets were added to this question since I last looked at it, as they seem to describe this situation:&#xD;&#xA;&#xD;&#xA;TEST DATASET 1:&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;Input:&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;    3 4&#xD;&#xA;    GCCCAA&#xD;&#xA;    GGCCTG&#xD;&#xA;    AACCTA&#xD;&#xA;    TTCCTT&#xD;&#xA;&#xD;&#xA;Output:&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;    GCC&#xD;&#xA;    GCC&#xD;&#xA;    AAC&#xD;&#xA;    TTC&#xD;&#xA;&#xD;&#xA;&gt; This dataset checks that your code always picks the first-occurring Profile-most Probable k-mer in a given sequence of Dna. In the first sequence (“GCCCAA”), “GCC” and “CCA” are both Profile-most Probable k-mers. However, you must return “GCC” since it occurs earlier than “CCA”. Thus, if the first sequence of your output is “CCA”, this test case fails your code." />
</posthistory>