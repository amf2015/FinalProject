<?xml version="1.0" encoding="utf-8"?>
<posts>
  <row Id="1" PostTypeId="1" AcceptedAnswerId="3" CreationDate="2017-05-16T18:01:06.383" Score="32" ViewCount="491" Body="&lt;p&gt;I'd like to learn which format it's commonly used for storing full sequence human genome (4 letters without quality score) and why.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume storing it in plain format would be very inefficient. I would think more about binary format like 2 bits per nucleotide.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, which format would be the most common in terms of space efficiency?&lt;/p&gt;&#xA;" OwnerUserId="43" LastEditorUserId="96" LastEditDate="2017-06-08T08:35:23.083" LastActivityDate="2017-06-08T08:35:23.083" Title="What's the most efficient format to store DNA sequence?" Tags="&lt;human-genome&gt;&lt;storage&gt;&lt;file-formats&gt;" AnswerCount="8" CommentCount="9" FavoriteCount="2" />
  <row Id="2" PostTypeId="2" ParentId="1" CreationDate="2017-05-16T18:08:41.067" Score="4" Body="&lt;p&gt;In terms of raw storage capacity 2 bits per nucleotide, and then further compressed with standard compression techniques would be the most efficient. However, you'd still have other storage considerations. Like what to do about non-standard bases: like if you want to indicate a gap or ambiguity. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd also query if it is really necessary to store them as binary since it reduces the readability of the data. It is quite convenient having a whole bunch of unix and programming tools that can operate on the string level in text files.&lt;/p&gt;&#xA;" OwnerUserId="62" LastEditorUserId="62" LastEditDate="2017-05-16T18:19:38.200" LastActivityDate="2017-05-16T18:19:38.200" CommentCount="1" />
  <row Id="3" PostTypeId="2" ParentId="1" CreationDate="2017-05-16T18:09:58.197" Score="30" Body="&lt;p&gt;Genomes are commonly stored as either fasta files (.fa) or twoBit (.2bit) files. Fasta files store the entire sequence as text and are thus not particularly compressed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;twoBit files store each nucleotide in two bits and contain additional metadata that indicates where there's regions containing &lt;code&gt;N&lt;/code&gt; (unknown) bases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more information, see the documentation on the twoBit format at the &lt;a href=&quot;http://genome.ucsc.edu/FAQ/FAQformat.html#format7&quot; rel=&quot;noreferrer&quot;&gt;UCSC genome browser&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can convert between twoBit and fasta format using the &lt;a href=&quot;https://genome.ucsc.edu/goldenpath/help/twoBit.html&quot; rel=&quot;noreferrer&quot;&gt;faToTwoBit and twoBitToFa utilities&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the human genome, you can download it in either fasta or twoBit format here: &lt;a href=&quot;http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/&quot; rel=&quot;noreferrer&quot;&gt;http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="9" LastEditorUserId="9" LastEditDate="2017-05-16T18:21:25.940" LastActivityDate="2017-05-16T18:21:25.940" CommentCount="0" />
  <row Id="4" PostTypeId="2" ParentId="1" CreationDate="2017-05-16T18:11:54.510" Score="21" Body="&lt;p&gt;The standard formats for storing sequence data are &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTA_format&quot; rel=&quot;noreferrer&quot;&gt;fasta&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format&quot; rel=&quot;noreferrer&quot;&gt;fastq&lt;/a&gt;.  Fasta is used if you only need the raw sequence data, fastq is used if you want to store the sequence data along with the quality information from base calling.  Each of these can be compressed using gzip or another standard compression algorithm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Typically we want to keep the quality information along with the raw sequence data, but the quality information accounts for half the storage space required.  Some people have developed &lt;a href=&quot;https://web.stanford.edu/~mainakch/papers/QualComp_Final_v2.pdf&quot; rel=&quot;noreferrer&quot;&gt;algorithms&lt;/a&gt; for lossy compression of the quality data that allow us to reduce the storage requirements.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are interested in storing variant calling data, the standard format for that is &lt;a href=&quot;http://www.internationalgenome.org/wiki/Analysis/vcf4.0/&quot; rel=&quot;noreferrer&quot;&gt;VCF&lt;/a&gt;.  VCF is useful if you want to store quality information of the variant calls, genomic positions, and any annotations you might have about the position.  VCFs can be compressed and indexed using &lt;a href=&quot;http://www.htslib.org/doc/tabix.html&quot; rel=&quot;noreferrer&quot;&gt;bgzip and tabix&lt;/a&gt;.  Many tools require variant data to be compressed and indexed using these tools.&lt;/p&gt;&#xA;" OwnerUserId="59" LastActivityDate="2017-05-16T18:11:54.510" CommentCount="4" />
  <row Id="5" PostTypeId="1" AcceptedAnswerId="13" CreationDate="2017-05-16T18:14:20.903" Score="12" ViewCount="170" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Human_Genome_Project&quot; rel=&quot;noreferrer&quot;&gt;The Human Genome Project&lt;/a&gt; was the project of 'determining the sequence of nucleotide base pairs that make up human DNA, and of identifying and mapping all of the genes of the human genome'. It was completed in 2003.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are the datasets provided by HGP still accurate? Or given the technology in the past (such as using old techniques), or any other reason (newer research studies), the datasets could be not accurate?&lt;/p&gt;&#xA;" OwnerUserId="43" LastEditorUserId="43" LastEditDate="2017-05-16T18:19:35.360" LastActivityDate="2017-05-16T19:40:38.183" Title="How accurate are human DNA datasets sequenced by Human Genome Project?" Tags="&lt;hgp&gt;&lt;dna&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="6" PostTypeId="5" CreationDate="2017-05-16T18:16:13.057" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-16T18:16:13.057" LastActivityDate="2017-05-16T18:16:13.057" CommentCount="0" />
  <row Id="7" PostTypeId="4" CreationDate="2017-05-16T18:16:13.057" Score="0" Body="The Human Genome Project was a scientific research project with the goal of determining the sequence of human DNA." OwnerUserId="43" LastEditorUserId="43" LastEditDate="2017-05-16T18:41:13.577" LastActivityDate="2017-05-16T18:41:13.577" CommentCount="0" />
  <row Id="9" PostTypeId="1" CreationDate="2017-05-16T18:28:49.047" Score="11" ViewCount="80" Body="&lt;p&gt;I'm interested working with the medication information provided by the &lt;a href=&quot;http://biobank.ctsu.ox.ac.uk/crystal/field.cgi?id=20003&quot; rel=&quot;noreferrer&quot;&gt;UK Biobank&lt;/a&gt;.  In order to get these into a usable form I would like to map them to &lt;a href=&quot;https://en.wikipedia.org/wiki/Anatomical_Therapeutic_Chemical_Classification_System&quot; rel=&quot;noreferrer&quot;&gt;ATC codes&lt;/a&gt;.  Since many of the drugs listed in the data showcase include dosage information, doing an exact string match between drug names is not very effective.  I've considered using something like &lt;a href=&quot;https://pypi.python.org/pypi/fuzzywuzzy&quot; rel=&quot;noreferrer&quot;&gt;fuzzywuzzy&lt;/a&gt; to do string matching between the medications in the data showcase and the ATC drug names but validating the matches could still be a laborious process.  Does anyone know of a tool that can match drug names to ATC codes or some other drug ontology?  If not, maybe there's a better way to do it that I haven't thought of.&lt;/p&gt;&#xA;" OwnerUserId="59" LastActivityDate="2017-05-16T19:36:42.037" Title="Mapping drug names to ATC codes" Tags="&lt;atc&gt;&lt;ukbiobank&gt;&lt;drugs&gt;&lt;ontology&gt;&lt;nlp&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="3" />
  <row Id="11" PostTypeId="2" ParentId="1" CreationDate="2017-05-16T18:31:11.587" Score="11" Body="&lt;p&gt;There are several things to consider when asking for &quot;the most efficient&quot; way to store data, it all depends on your use case. Do you just need ACGT, or are there also IUPAC codings for combinations? Do you need additional data (like quality values)? What kind of application are you using the data for (does it need to load all at once or in in chunks? Once or multiple times? Sequential or random access? etc.pp)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g., most efficient for:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Lowest footprint on disk, without a lot of hassle: use either FASTA or 2bit, but run through standard compressor (gzip, bzip2, others). The literature you want to consult here is that of standard text compression I think. Also of interest &lt;a href=&quot;http://mattmahoney.net/dc/text.html&quot; rel=&quot;noreferrer&quot;&gt;Large Text Compression Benchmark&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Keeping the file on disk, but ultra-fast loading small subsets into memory, being able to work in memory with character sized entities: a simple dump of the DNA as characters to disk, maybe combined with an index file to know which chromosome starts where. Then use &lt;a href=&quot;http://pubs.opengroup.org/onlinepubs/009695399/functions/mmap.html&quot; rel=&quot;noreferrer&quot;&gt;mmap&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Storing quality values: See papers like &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0059190&quot; rel=&quot;noreferrer&quot;&gt;Compression of FASTQ and SAM Format Sequencing Data&lt;/a&gt; or &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3637481/&quot; rel=&quot;noreferrer&quot;&gt;Sequence squeeze: an open contest for sequence compression&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Any combination of the above use cases + a lot more&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="44" LastActivityDate="2017-05-16T18:31:11.587" CommentCount="2" />
  <row Id="12" PostTypeId="1" CreationDate="2017-05-16T18:31:28.570" Score="10" ViewCount="79" Body="&lt;p&gt;I'm looking to dock a large ligand (~90kDa) to a receptor slightly larger receptor  (~125kDa) using Hex. If anyone is familiar with docking large structures, are there any recommended parameters for finding the best docking solution?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Parameters in particular:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Number of Solutions  &lt;/li&gt;&#xA;&lt;li&gt;N order of correlation for initial and final&#xA;searches &lt;/li&gt;&#xA;&lt;li&gt;Receptor Range &lt;/li&gt;&#xA;&lt;li&gt;Ligand Range&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="65" LastEditorUserId="48" LastEditDate="2017-05-18T00:01:22.427" LastActivityDate="2017-05-18T00:01:22.427" Title="What are the optimal parameters for docking a large ligand using Hex?" Tags="&lt;proteins&gt;&lt;docking&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="13" PostTypeId="2" ParentId="5" CreationDate="2017-05-16T18:37:07.157" Score="15" Body="&lt;p&gt;The HGP developed the first &quot;reference&quot; human genome - a genome that other genomes could be compared to, and was actually a composite of multiple human genome sequences. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The standard human reference genome is actually continually updated with major and minor revisions, a bit like software. The &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/grc/human&quot; rel=&quot;noreferrer&quot;&gt;latest major version&lt;/a&gt; is called GRCh38, was released in 2013, and has since had a number of minor updates. &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Are the datasets provided by HGP still accurate?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Yes, in a sense, but we certainly have better information now. One way to measure the quality of the assembly is that the initial release from the HGP &lt;a href=&quot;https://www.nature.com/nmeth/journal/v7/n5/full/nmeth0510-331.html&quot; rel=&quot;noreferrer&quot;&gt;had hundreds of thousands&lt;/a&gt; of gaps - sequences that could not be resolved (this often occurs because of repetitive sequences). The newest reference genome has less than 500 gaps.&lt;/p&gt;&#xA;" OwnerUserId="8" LastActivityDate="2017-05-16T18:37:07.157" CommentCount="0" />
  <row Id="14" PostTypeId="1" AcceptedAnswerId="385" CreationDate="2017-05-16T18:37:27.773" Score="3" ViewCount="279" Body="&lt;p&gt;I'd like to learn the differences between 3 common formats such as &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTA_format&quot; rel=&quot;noreferrer&quot;&gt;FASTA&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format&quot; rel=&quot;noreferrer&quot;&gt;FASTQ&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/SAM_(file_format)&quot; rel=&quot;noreferrer&quot;&gt;SAM&lt;/a&gt;. How they are different? Are there any benefits of using one over another?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Based on Wikipedia pages, I can't tell the differences between them.&lt;/p&gt;&#xA;" OwnerUserId="43" LastEditorUserId="96" LastEditDate="2017-08-18T04:33:47.617" LastActivityDate="2017-08-18T04:33:47.617" Title="What is the difference between FASTA, FASTQ, and SAM file formats?" Tags="&lt;file-formats&gt;&lt;fasta&gt;&lt;fastq&gt;&lt;sam&gt;" AnswerCount="4" CommentCount="7" FavoriteCount="1" />
  <row Id="15" PostTypeId="1" CreationDate="2017-05-16T18:41:23.147" Score="7" ViewCount="214" Body="&lt;p&gt;Many of my colleagues recommend I use BWA MEM instead of regular old BWA. The problem is I don't understand why and reading the BWA man page doesn't seem to help the matter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the difference between BWA and BWA MEM? And, in which instances would you employ one over the other?&lt;/p&gt;&#xA;" OwnerUserId="88" LastEditorUserId="298" LastEditDate="2017-06-07T12:39:34.250" LastActivityDate="2017-06-07T15:34:51.743" Title="Difference between BWA-backtrack and BWA-MEM" Tags="&lt;alignment&gt;&lt;bwa&gt;&lt;read-mapping&gt;&lt;reads&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="1" />
  <row Id="16" PostTypeId="2" ParentId="1" CreationDate="2017-05-16T18:46:51.883" Score="5" Body="&lt;p&gt;It is not yet standardized, but &lt;a href=&quot;https://www.sevenbridges.com/graph/better-reference/&quot; rel=&quot;noreferrer&quot;&gt;graph format&lt;/a&gt; has the potential for being the most space-efficient method for storing genomes.  The idea is this: rather than store a genome as a linear string of sequenced nucleotides, genomes are stored as overlapping graphs, where sequence variants branch off from the reference genome, and then rejoin when the alignment continues.  Basically, you start with a reference genome, and for every subsequent genome added to the graph, only the differences are stored.  This could allow for an enormous gain in space efficiency. &lt;/p&gt;&#xA;" OwnerUserId="47" LastActivityDate="2017-05-16T18:46:51.883" CommentCount="0" />
  <row Id="17" PostTypeId="2" ParentId="14" CreationDate="2017-05-16T18:53:38.760" Score="6" Body="&lt;p&gt;Incidentally, the first part of your question is something you could have looked up yourself as the first hits on Google of &quot;NAME format&quot; point you to primers on Wikipedia, no less. In future, please do that before asking a question.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/FASTA_format&quot; rel=&quot;noreferrer&quot;&gt;FASTA&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format&quot; rel=&quot;noreferrer&quot;&gt;FASTQ&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SAM_(file_format)&quot; rel=&quot;noreferrer&quot;&gt;SAM&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;FASTA (officially) just stores the name of a sequence and the sequence, inofficially people also add comment fields after the name of the sequence. FASTQ was invented to store both sequence and associated quality values (e.g. from sequencing instruments). SAM was invented to store alignments of (small) sequences (e.g. generated from sequencing) with associated quality values and some further data onto a larger sequences, called reference sequences, the latter being anything from a tiny virus sequence to ultra-large plant sequences.&lt;/p&gt;&#xA;" OwnerUserId="44" LastActivityDate="2017-05-16T18:53:38.760" CommentCount="0" />
  <row Id="18" PostTypeId="2" ParentId="14" CreationDate="2017-05-16T18:57:28.007" Score="10" Body="&lt;p&gt;In a nutshell, &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;FASTA&lt;/code&gt; file format is a DNA sequence format for specifying or representing DNA sequences and was first described by Pearson &lt;code&gt;(Pearson,W.R. and Lipman,D.J. (1988) Improved tools for biological sequence comparison. Proc. Natl Acad. Sci. USA, 85, 2444–2448)&lt;/code&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;FASTQ&lt;/code&gt; is another DNA sequence file format that extends the FASTA format with the ability to store the sequence quality. The quality scores are often represented in  ASCII characters which correspond to a phred score)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Both FASTA and FASTQ are common sequence representation formats and have emerged as key data interchange formats for molecular biology and bioinformatics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;SAM&lt;/code&gt; is format for representing sequence alignment information from a read aligner. It represents sequence information in respect to a given reference sequence. The information is stored in a series of tab delimited ascii columns. The full SAM format specification is available at &lt;a href=&quot;http://samtools.sourceforge.net/SAM1.pdf&quot; rel=&quot;noreferrer&quot;&gt;http://samtools.sourceforge.net/SAM1.pdf&lt;/a&gt; &lt;/p&gt;&#xA;" OwnerUserId="27" LastActivityDate="2017-05-16T18:57:28.007" CommentCount="3" />
  <row Id="19" PostTypeId="1" AcceptedAnswerId="293" CreationDate="2017-05-16T19:15:57.957" Score="17" ViewCount="261" Body="&lt;p&gt;A common bioinformatics task is to decompose a DNA sequence into its constituent k-mers and compute a hash value for each k-mer. &lt;a href=&quot;https://en.wikipedia.org/wiki/Rolling_hash&quot; rel=&quot;noreferrer&quot;&gt;Rolling hash functions&lt;/a&gt; are an appealing solution for this task, since they can be computed very quickly. A rolling hash does not compute each the hash value from scratch with each k-mer: rather it updates a running hash value using an update strategy and a sliding window over the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also very useful for many applications to have a k-mer hash to the same value as its reverse complement. Unless the data were generated using a strand-specific sample prep, it's impossible to distinguish a k-mer from its reverse complement, and they should be treated as the same sequence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any rolling hashes that will map reverse complements to the same value? If not, how would we develop such an algorithm?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: Ideally the hash function would be able to support k &gt; 32, which would be lossy unless using something larger than a 64-bit integer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ANOTHER UPDATE&lt;/strong&gt;: I don't think it's necessary to &lt;em&gt;store&lt;/em&gt; both the running k-mer and its reverse complement in a single value. If storing two k-mer strings and/or two hash values makes this easier, I'm totally cool with that.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-05-16T20:50:34.373" LastActivityDate="2017-07-26T16:10:55.117" Title="Are there any rolling hash functions that can hash a DNA sequence and its reverse complement to the same value?" Tags="&lt;hashing&gt;&lt;k-mer&gt;&lt;algorithms&gt;" AnswerCount="4" CommentCount="5" FavoriteCount="6" />
  <row Id="20" PostTypeId="2" ParentId="1" CreationDate="2017-05-16T19:17:38.697" Score="15" Body="&lt;p&gt;The standard and the most common sequence format is FASTA for sure. You can compress it with a compressor. For the ~3GB human genome, gzip reduces the size to ~900MB, depending on the option in use.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another often used format is UCSC's 2-bit format. This format keeps each A/C/G/T with 2 bits. As I remember, it keeps non-A/C/G/T bases and lowercases in two separate lists. These lists basically tell you that bases between offset x and y are all &quot;N&quot;/lowercase. The 2-bit format loses IUB codes that GRCh37 has. UCSC's hg19 differs from GRCh37 at a few bases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BWA also produces its own 2-bit format with indexing. You can generate it separately with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bwa fa2pac -f hg19.fa&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Unlike UCSC, BWA keeps all IUB codes but loses letter cases. BWA does not provide utilities to convert its 2-bit representation to FASTA, either.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 2-bit format typically reduces the file size down to 1/4 of its original size, unless there are too many scattered ambiguous bases. For human genome, you get a file ~784MB in size. You can compress it further with gzip, but that actually doesn't work well. A gzip'd 2-bit file is only ~5-10% smaller.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to achieve an even smaller file size, you can compress the BWT of 2-bit file. This gives you a ~633MB file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bwa pac2bwtgen hg19.fa.pac tmp.bwt &amp;amp;&amp;amp; gzip tmp.bwt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A bit-aware compression algorithm may achieve an even higher compression ratio. However, such BWT-based compression prevents you from extracting subsequences. It is probably of little use in practice.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-05-16T19:17:38.697" CommentCount="3" />
  <row Id="21" PostTypeId="1" AcceptedAnswerId="64" CreationDate="2017-05-16T19:24:16.303" Score="25" ViewCount="232" Body="&lt;p&gt;What are the actual differences between different annotation databases? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My lab, for reasons still unknown to me, prefers Ensembl annotations (we're working with transcript/exon expression estimation), while some software ship with RefSeq annotations. Are there significant differences between them today, or are they, for all intents and purposes, interchangeable (e.g., are exon coordinates between RefSeq and Ensembl annotations interchangeable)?&lt;/p&gt;&#xA;" OwnerUserId="82" LastActivityDate="2017-05-17T13:34:35.107" Title="Feature annotation: RefSeq vs Ensembl vs Gencode, what's the difference?" Tags="&lt;annotation&gt;&lt;ensembl&gt;&lt;refseq&gt;&lt;gencode&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="5" />
  <row Id="22" PostTypeId="2" ParentId="9" CreationDate="2017-05-16T19:33:06.700" Score="5" Body="&lt;p&gt;The &lt;a href=&quot;http://cart.embl.de/&quot; rel=&quot;noreferrer&quot;&gt;CART&lt;/a&gt; tool let's you upload a set of names and map them (optionally in a fuzzy way) to STITCH 4 identifiers, and then use those to map to ATC codes (using the chemicals sources &lt;a href=&quot;http://stitch4.embl.de/cgi/show_download_page.pl&quot; rel=&quot;noreferrer&quot;&gt;download file&lt;/a&gt;). It's a bit indirect, and I'm not sure what CART will do with the dosage info you mention.&lt;/p&gt;&#xA;" OwnerUserId="112" LastActivityDate="2017-05-16T19:33:06.700" CommentCount="0" />
  <row Id="23" PostTypeId="2" ParentId="9" CreationDate="2017-05-16T19:36:42.037" Score="5" Body="&lt;p&gt;&lt;a href=&quot;https://www.drugbank.ca&quot; rel=&quot;noreferrer&quot;&gt;DrugBank&lt;/a&gt; seems to have a &lt;a href=&quot;https://www.drugbank.ca/atc&quot; rel=&quot;noreferrer&quot;&gt;tool to map ATC codes to drug names&lt;/a&gt; and DrugBank IDs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A quick look in the XSD schema on the &lt;a href=&quot;https://www.drugbank.ca/releases/latest&quot; rel=&quot;noreferrer&quot;&gt;release page&lt;/a&gt; suggests the complete database includes ATC codes for drugs, you could then do a fuzzy match of the BioBank names against all of DrugBank's synonyms, or match on some other data (e.g. canonicalised SMILES) if available.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The downside is that there may not be complete overlap between UK BioBank and DrugBank. Additionally, DrugBank is under licence for commercial use.&lt;/p&gt;&#xA;" OwnerUserId="109" LastActivityDate="2017-05-16T19:36:42.037" CommentCount="0" />
  <row Id="24" PostTypeId="2" ParentId="5" CreationDate="2017-05-16T19:40:38.183" Score="5" Body="&lt;p&gt;While the quality of the reference human assembly keeps improving, there are still misassemblies in it. A common problem is recent segmental duplications are occasionally collapsed into one sequence in the reference. Another issue is that the centromeric sequences in the reference are computationally generated, which are probably different from real sequences. Issues like these often complicate data analyses.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You should also beware that each human has a different genome. A large region having one copy in the reference genome may have two copies in a specific sample. While this is not really the problem with the reference, such copy-number changes will have the same effect as reference errors and mess up your pipelines.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is still room for improvement to the human reference genome. In some regions, the CHM1 and CHM13 PacBio assemblies are better than the current reference genome at the larger scale. Illumina population data can produce better consensus at the single base level. GRC is continuously releasing new patches to the latest assembly.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-05-16T19:40:38.183" CommentCount="0" />
  <row Id="25" PostTypeId="2" ParentId="14" CreationDate="2017-05-16T19:50:20.683" Score="3" Body="&lt;p&gt;FASTA and FATSQ formats are both file formats that contain sequencing reads while SAM files are these reads aligned to a reference sequence. In other words, FASTA and FASTQ are the &quot;raw data&quot; of sequencing while SAM is the product of aligning the sequencing reads to a refseq. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A FASTA file contains a read name followed by the sequence. An example of one of these reads for RNASeq might be: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;Flow cell number: lane number: chip coordinates etc.&#xA;ATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTA&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The FASTQ version of this read will have two more lines, one + as a space holder and then a line of quality scores for the base calls. The qualities are given as characters with '!' being the lowest and '~' being the highest, in increasing ASCII value. It would look something like this&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@Flow cell number: lane number: chip coordinates etc.&#xA;ATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTAATTGGCTA&#xA;+&#xA;!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;CCCCCCC65&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A SAM file has many fields for each alignment, the header begins with the @ character. The alignment contains 11 mandatory fields and various optional ones. You can find the spec file here: &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMv1.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://samtools.github.io/hts-specs/SAMv1.pdf&lt;/a&gt; .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Often you'll see BAM files which are just compressed binary versions of SAM files. You can view these alignment files using various tools, such as SAMtools, IGV or USCS Genome browser. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As to the benefits, FASTA/FASTQ vs. SAM/BAM is comparing apples and oranges. I do a lot of RNASeq work so generally we take the FASTQ files and align them the a refseq using an aligner such as STAR which outputs SAM/BAM files. There's a lot you can do with just these alignment files, looking at expression, but usually I'll use a tool such as RSEM to &quot;count&quot; the reads from various genes to create an expression matrix, samples as columns and genes as rows. Whether you get FASTQ or FASTA files just depends on your sequencing platform. I've never heard of anybody really using the quality scores. &lt;/p&gt;&#xA;" OwnerUserId="94" LastEditorUserId="29" LastEditDate="2017-06-02T10:45:37.963" LastActivityDate="2017-06-02T10:45:37.963" CommentCount="1" />
  <row Id="26" PostTypeId="2" ParentId="19" CreationDate="2017-05-16T19:51:29.843" Score="6" Body="&lt;p&gt;Let &lt;code&gt;f&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt; be two integers. They always keep the k-mer on the forward and reverse strand, respectively. At a new base &lt;code&gt;c&lt;/code&gt; in a proper 2-bit encoding, we update the two integers as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;f = (f&amp;lt;&amp;lt;2|c) &amp;amp; ((1ULL&amp;lt;&amp;lt;2*k) - 1)&#xA;r = r&amp;gt;&amp;gt;2 | (3ULL-c)&amp;lt;&amp;lt;2*(k-1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With this updating rule, &lt;code&gt;f&lt;/code&gt; keeps the forward strand k-mer ending at &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt; is &lt;code&gt;f&lt;/code&gt;'s reverse complement. The hash of the k-mer can be &lt;code&gt;min(f,r)&lt;/code&gt;. If integer operations take constant time, computing the hashes of all k-mers of a sequence of length L takes O(L) time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The above only works if k-mer fits a word, which means &lt;code&gt;k&amp;lt;=32&lt;/code&gt; on x86. For long k-mers, you can overload bit operators in C++. In case of &lt;code&gt;32&amp;lt;k&amp;lt;=64&lt;/code&gt;, a simpler solution is to split into lower and higher bits:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;c1 = c&amp;amp;1, c2 = c&amp;gt;&amp;gt;1&#xA;f1 = (f1&amp;lt;&amp;lt;1|c1) &amp;amp; ((1ULL&amp;lt;&amp;lt;k) - 1)&#xA;f2 = (f2&amp;lt;&amp;lt;1|c2) &amp;amp; ((1ULL&amp;lt;&amp;lt;k) - 1)&#xA;r1 = r1&amp;gt;&amp;gt;1 | (1ULL-c1)&amp;lt;&amp;lt;(k-1)&#xA;r2 = r2&amp;gt;&amp;gt;1 | (1ULL-c2)&amp;lt;&amp;lt;(k-1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is twice as slow as the &lt;code&gt;k&amp;lt;=32&lt;/code&gt; version. It is possible to implement these lines with SSE2 intrinsics, but that is overkilling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When you want to hash a long k-mer into fewer bits, there are a few options:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;xor:      f^r&#xA;min:      min(f,r)&#xA;min+hash: hash(min(f,r))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;hash()&lt;/code&gt; is a generic randomization hash function, which could be Murmur3, Wang's integer hash function, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following is a microbenchmark. Here, we squeeze all k-mers into a 32-bit hash table. &lt;code&gt;collision_rate = 1 - #distinct_hash/#distinct_kmer&lt;/code&gt;. This is not the best metric, but it somehow measures the randomness, which is better than nothing.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;====================================================&#xA; Algorithm    data  max-k  k   %collision  CPU time&#xA;----------------------------------------------------&#xA; xor          chr11  32    31     8.5%       0.9s&#xA; xor+Wang     chr11  32    31     9.6%       1.3s&#xA; min+Wang     chr11  32    31     1.4%       1.3s&#xA; min+Wang     chr11  64    31     1.4%       1.9s&#xA; min+murmur3  chr11  inf   31     1.4%       5.7s&#xA; min+Wang     chr11  64    51     1.5%       2.1s&#xA; min+murmur3  chr11  inf   51     1.5%       6.8s&#xA; min+Wang     chr11  64    63     1.5%       2.1s&#xA; min+murmur3  chr11  inf   63     1.5%       7.5s&#xA; radix sort   chr11  -     -      -        5.6-7.3s&#xA;----------------------------------------------------&#xA; min+Wang     hg38   64    63    26.2%        37s&#xA; min+murmur3  hg38   inf   63    26.2%       192s&#xA; radix sort   hg38   -     -      -         ~138s&#xA;====================================================&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Observations:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Don't use XOR. In fact, XOR hashes all palindromes to &lt;code&gt;0xffff...&lt;/code&gt;. This is already worrying enough.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Unless you want to work with very long k-mers, don't use generic string hash functions like FNV and Murmur. Murmur is even slower than radix sorting all hashes.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;For &lt;code&gt;k&amp;lt;=64&lt;/code&gt;, min+Wang is the best here. It is fast and simple to compute and has randomness nearly as good as murmur.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="37" LastEditorUserId="37" LastEditDate="2017-05-18T16:54:03.623" LastActivityDate="2017-05-18T16:54:03.623" CommentCount="4" />
  <row Id="27" PostTypeId="1" AcceptedAnswerId="32" CreationDate="2017-05-16T20:00:43.050" Score="11" ViewCount="170" Body="&lt;p&gt;I have a set of BAM files that are aligned using the NCBI GRCh37 human genome reference (with the chromosome names as NC_000001.10) but I want to analyze it using a BED file that has the UCSC hg19 chromosome names (e.g. chr1). I want to use bedtools to pull out all the on-target and off-target reads.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Are NCBI and UCSC directly comparable? Or do I need to re-align the BAM/lift-over the BED to the UCSC reference?&lt;/li&gt;&#xA;&lt;li&gt;Should I convert the BED file or the BAM file? Everyone here uses the UCSC chromosome names/positions so I'll need to convert the eventual files to UCSC anyway.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="110" LastEditorUserId="37" LastEditDate="2017-06-25T02:10:00.757" LastActivityDate="2017-08-08T11:02:02.630" Title="Convert a BAM file from one reference to another?" Tags="&lt;bam&gt;&lt;file-formats&gt;&lt;bed&gt;&lt;format-conversion&gt;&lt;reference-genome&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" />
  <row Id="28" PostTypeId="2" ParentId="19" CreationDate="2017-05-16T20:17:30.960" Score="7" Body="&lt;p&gt;If your goal is to minimise storage by just having one hash per kmer and its reverse complement, there is a simple solution for non-rolling hashes. For any sequence S, you compute and store the hash of the smaller of S and its reverse complement. A simple lexicographical comparison for &quot;smaller&quot; is enough. In programming terms:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;hash=computeHash(min(S,rev(S));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If your goal is to minimise computing time via rolling hashes AND store only one kmer value, this will be hard. Hashes are normally designed to minimise collisions and you are asking it not only to collide, but also to collide in very, very specific circumstances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Question is: is the computation of a hash a bottleneck for an application? Maybe, maybe not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://softwareengineering.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed/145633#145633&quot;&gt;This&lt;/a&gt; post on StackExchange seems to imply that, in 2012, MurmurHash was able to compute de-novo a hash for a UUID (similar to a standard kmer in size and complexity) in ~250ns, i.e., ~4 million hashes per second. That's already quite fast.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My guess is that most applications will use a hash to look up things in memory or disk. And here, even with look-ups in memory, real life data will let you run into CPU cache miss speed penalties very quickly and I expect the impact of this to be higher than the speed of the hash computation itself.&lt;/p&gt;&#xA;" OwnerUserId="44" LastActivityDate="2017-05-16T20:17:30.960" CommentCount="1" />
  <row Id="29" PostTypeId="1" CreationDate="2017-05-16T20:22:01.540" Score="5" ViewCount="114" Body="&lt;p&gt;What's the most widely accepted tool for doing pseudo-temporal ordering from scRNAseq data? Also is there away to separate differential expression that occurs based on &quot;cell identity&quot; or maybe more accurately cell type fate from that which arises from cells being in different stages differentiation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be more concrete, lets say there's a population of cells, some of which were born at time 1, time 2, and time 3. The progression along the temporal trajectory can be described via a set of genes that are fluctuating as the cell matures. So you might have the same cell type which was born at time 1 be transcriptionally distinct from a younger one born at time 3.  On the other hand, within this population there are subpopulations which will have different cell fates and are transcriptionally distinct. Is there away to reliably separate the temporal axis from the &quot;cell fate axes&quot;. If not is this something people are working on or is flawed logic to think this kind of thing is possible?&lt;/p&gt;&#xA;" OwnerUserId="94" LastActivityDate="2017-06-05T17:42:37.170" Title="Pseudo-temporal ordering in heterogeneous populations" Tags="&lt;scrnaseq&gt;" AnswerCount="1" CommentCount="10" FavoriteCount="1" />
  <row Id="30" PostTypeId="5" CreationDate="2017-05-16T20:52:13.140" Score="0" Body="&lt;p&gt;The &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE_TYPE=BlastDocs&amp;amp;DOC_TYPE=BlastHelp&quot; rel=&quot;nofollow noreferrer&quot;&gt;FASTA format&lt;/a&gt; has a single-line definition (defline), followed by one or more lines of DNA, RNA or amino acid sequences. The definition line is specified by the greater than (&lt;code&gt;&amp;gt;&lt;/code&gt;) character.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="8" LastEditDate="2017-05-18T00:02:17.593" LastActivityDate="2017-05-18T00:02:17.593" CommentCount="0" />
  <row Id="31" PostTypeId="4" CreationDate="2017-05-16T20:52:13.140" Score="0" Body="should be used for questions specific to sequence file format `.fasta`.&#xD;&#xA;&#xD;&#xA;should **not** be used if you just used fasta file, or if the question is more general about sequence formats." OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-05T23:08:16.720" LastActivityDate="2017-06-05T23:08:16.720" CommentCount="0" />
  <row Id="32" PostTypeId="2" ParentId="27" CreationDate="2017-05-16T21:04:42.503" Score="19" Body="&lt;p&gt;You're the second person I have ever seen using NCBI &quot;chromosome names&quot; (they're more like supercontig IDs). Normally I would point you to &lt;a href=&quot;https://github.com/dpryan79/ChromosomeMappings&quot; rel=&quot;noreferrer&quot;&gt;a resource providing mappings between chromosome names&lt;/a&gt;, but since no one has added NCBI names (yet, maybe I'll add them now) you're currently out of luck there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyway, the quickest way to do what you want is to &lt;code&gt;samtools view -H foo.bam &amp;gt; header&lt;/code&gt; to get the BAM header and then change each NCBI &quot;chromosome name&quot; to its corresponding UCSC chromosome name. DO NOT REORDER THE LINES! You can then use &lt;code&gt;samtools reheader&lt;/code&gt; and be done.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why, you might ask, would this work? The answer is that chromosome/contig names in BAM files aren't stored in each alignment. Rather, the names are stored in a list in the header and each alignment just contains the integer index into that list (read group IDs are similar, for what it's worth). This also leads to the warning above against reordering entries, since that's a VERY convenient way to start swapping alignments between chromosomes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an aside, you'd be well served switching to Gencode or Ensembl chromosome names, they're rather more coherent than the &lt;code&gt;something_random&lt;/code&gt; mess that's present in hg19 from UCSC.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Because I'm nice, &lt;a href=&quot;https://github.com/dpryan79/ChromosomeMappings/blob/master/GRCh37_NCBI2UCSC.txt&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt; is the conversion between NCBI and UCSC. Note that if you have any alignments to patches that there is simply no UCSC equivalent. One of the many reasons not to use UCSC (avoid their annotations too).&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-05-16T21:46:53.327" LastActivityDate="2017-05-16T21:46:53.327" CommentCount="6" />
  <row Id="35" PostTypeId="2" ParentId="15" CreationDate="2017-05-16T22:49:39.000" Score="8" Body="&lt;p&gt;To quote the Introduction to &lt;code&gt;BWA&lt;/code&gt; on &lt;a href=&quot;http://bio-bwa.sourceforge.net/&quot; rel=&quot;noreferrer&quot;&gt;sourceforge&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the rest two for longer sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In short, for anything where you have read lengths over 70bp BWA-MEM is faster, and more accurate.&lt;/p&gt;&#xA;" OwnerUserId="149" LastActivityDate="2017-05-16T22:49:39.000" CommentCount="2" />
  <row Id="36" PostTypeId="1" AcceptedAnswerId="42" CreationDate="2017-05-17T02:18:01.720" Score="11" ViewCount="64" Body="&lt;p&gt;I have run Oxford Nanopore Technologies' MinION sequencing on the same DNA sample using three flowcells, each aligned against the same reference genome (E.coli K12 MG1655) using both BWA MEM and GraphMap and stored as BAM files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I quantitatively and efficiently analyse the quality of alignment (percentage identity, insertion rate, deletion rate) of each of these files?&lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="163" LastEditDate="2017-05-17T02:23:22.180" LastActivityDate="2017-05-18T00:01:18.567" Title="Compare alignment quality of multiple sequencing runs aligned against the same reference genome" Tags="&lt;bam&gt;&lt;alignment&gt;&lt;nanopore&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="38" PostTypeId="2" ParentId="21" CreationDate="2017-05-17T02:29:23.840" Score="6" Body="&lt;h2&gt;Ensembl vs Gencode&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.gencodegenes.org/faq.html&quot; rel=&quot;noreferrer&quot;&gt;https://www.gencodegenes.org/faq.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The GENCODE annotation is made by merging the Havana manual gene annotation and the Ensembl automated gene annotation. [...] In practical terms, the GENCODE annotation is identical to the Ensembl annotation.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Further, for the GTF file differences:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The only exception is that the genes which are common to the human chromosome X and Y PAR regions can be found twice in the GENCODE GTF, while they are shown only for chromosome X in the Ensembl file.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h2&gt;Gencode(Ensembl) vs RefSeq&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Gencode is in almost all cases &lt;a href=&quot;https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-16-S8-S2&quot; rel=&quot;noreferrer&quot;&gt;more comprehensive&lt;/a&gt;. For example, this is NCBI RefSeq vs Ensembl (v24, release 83) for BRCA gene:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/MO5Gb.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/MO5Gb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;RefSeq and Gencode are not interchangeable in most cases, though RefSeq annotations will often be a subset of the Gencode ones.&lt;/p&gt;&#xA;" OwnerUserId="161" LastActivityDate="2017-05-17T02:29:23.840" CommentCount="1" />
  <row Id="42" PostTypeId="2" ParentId="36" CreationDate="2017-05-17T03:10:02.837" Score="6" Body="&lt;p&gt;Qualimap will do this for you. &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Go to &lt;a href=&quot;http://qualimap.bioinfo.cipf.es&quot; rel=&quot;nofollow noreferrer&quot;&gt;qualimap.bioinfo.cipf.es&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Run qualimap (default params are fine) on each BAM file&lt;/li&gt;&#xA;&lt;li&gt;Open up the HTML output, and you can read off the %identity (they measure the opposite, i.e. mismatch rate, but 100% - mismatch rate is %identity of course), indel rate, etc.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;One thing to watch out for (you don't mention it in your question, but just in case) is that you cannot directly compare Q scores - these are a bit of a mess and calculated very differently in each piece of software. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unsolicited suggestion: you might also try &lt;a href=&quot;https://github.com/philres/ngmlr&quot; rel=&quot;nofollow noreferrer&quot;&gt;NGM-LR&lt;/a&gt; for mapping MinION data. We've found it beats the others for our data (though we map to a distant reference).&lt;/p&gt;&#xA;" OwnerUserId="156" LastEditorUserId="163" LastEditDate="2017-05-18T00:01:18.567" LastActivityDate="2017-05-18T00:01:18.567" CommentCount="2" />
  <row Id="43" PostTypeId="2" ParentId="1" CreationDate="2017-05-17T03:20:00.923" Score="7" Body="&lt;p&gt;I think the question is a bit ambiguous so please excuse this answer that's a bit redundant from the rest of the ones provided.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As others have mentioned, if you want to store a full genome, &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTA_format&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;FASTA&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://genome.ucsc.edu/FAQ/FAQformat.html#format7&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;2bit&lt;/code&gt;&lt;/a&gt; formats are appropriate.  For some context, &lt;a href=&quot;http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;hg19&lt;/code&gt;&lt;/a&gt; is about 900Mb compressed for the &lt;code&gt;FASTA&lt;/code&gt; file and about 780Mb compressed for the &lt;code&gt;2bit&lt;/code&gt; file .  &lt;code&gt;hg19&lt;/code&gt; is a reference and is haploid so doesn't represent a &quot;full&quot; human genome that would normally have two alleles for the autosome (non-sex chromosomes).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A common format for representing variant information is Variant Call Format (&lt;a href=&quot;https://vcftools.github.io/specs.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;VCF&lt;/code&gt;&lt;/a&gt;). The &lt;code&gt;VCF&lt;/code&gt; format represent differences from a reference (&lt;code&gt;hg19&lt;/code&gt;, say) that can be used to recover the original full sequence by using the reference and the differences encoded in the &lt;code&gt;VCF&lt;/code&gt; file.  I've seen &lt;code&gt;VCF&lt;/code&gt; files in the range of 100Mb, but a reference file is still needed to recover the full genome sequence which is the range of 800Mb+, as mentioned above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're considering just one &quot;whole genome&quot; in isolation, then the answer is pretty clear: &lt;code&gt;2bit&lt;/code&gt; format is probably approaching the entropy limit of the human genome and you probably won't be able to do much better.&#xA;The reason why your question is a bit ambiguous is that as soon as you start encoding more than one genome, a population of genomes, say, then you can start exploiting the redundancy of the genome as shared by the population.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, say you want to store two &quot;whole genomes&quot;.  You could download the &lt;code&gt;hg19&lt;/code&gt; reference and download two &lt;code&gt;VCF&lt;/code&gt; files which would give around 1Gb worth of data (around 800Mb for the &lt;code&gt;2bit&lt;/code&gt; file and around 200Mb for both of the &lt;code&gt;VCF&lt;/code&gt; files).  Now you've been able to represent a &quot;whole genome&quot; in 500Mb instead of the 800Mb.  You can see a similar argument for downloading 3 &lt;code&gt;VCF&lt;/code&gt; files and more.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The minimum amount of information needed to represent a population of genomes is, as far as I know, unknown, but I would guess in the 2.5Mb-5Mb range.  For example, see &lt;a href=&quot;https://academic.oup.com/bioinformatics/article/25/2/274/218156/Human-genomes-as-email-attachments&quot; rel=&quot;nofollow noreferrer&quot;&gt;&quot;Human genomes as email attachments&quot; by Christley, Lu, Li and Xie&lt;/a&gt; which claims a 4Mb encoding of a genome.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Things get tricky because you have to ask what you're claiming as a &quot;whole genome&quot;.  &lt;code&gt;VCF&lt;/code&gt; files are notoriously bad because older versions of the specification only store high quality differences from reference, throwing away high quality called sections.  If you want to store low quality information, the encoding is now going to depend on the sequencing technology in weird ways.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Insertions, deletions, mobile insertion elements, copy number variants, other structural variants, etc. all complicate this matter further.  &lt;a href=&quot;http://biorxiv.org/content/early/2017/01/18/101378&quot; rel=&quot;nofollow noreferrer&quot;&gt;Genome Graphs&lt;/a&gt; are trying to tackle at least some of these problems but the focus is on variant calling rather than efficient individual whole genome representation, though perhaps can be adapted in the future.&lt;/p&gt;&#xA;" OwnerUserId="113" LastEditorUserId="113" LastEditDate="2017-05-19T10:36:42.200" LastActivityDate="2017-05-19T10:36:42.200" CommentCount="2" />
  <row Id="44" PostTypeId="2" ParentId="12" CreationDate="2017-05-17T03:45:52.300" Score="1" Body="&lt;p&gt;While I have no experience with this &lt;em&gt;specific&lt;/em&gt; question you have, according to Feinstein &amp;amp; Brylinski (&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4468813/&quot; rel=&quot;nofollow noreferrer&quot;&gt;2015&lt;/a&gt;), a &quot;fully automated procedure&quot;, i.e. a Perl script, can be used to optimize the box size itself, and it can be found &lt;a href=&quot;http://brylinski.cct.lsu.edu/content/docking-box-size&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="172" LastEditorUserId="172" LastEditDate="2017-05-17T04:01:09.563" LastActivityDate="2017-05-17T04:01:09.563" CommentCount="0" />
  <row Id="45" PostTypeId="1" AcceptedAnswerId="68" CreationDate="2017-05-17T04:38:52.420" Score="19" ViewCount="354" Body="&lt;p&gt;I have a ~10GB FASTA file generated from an Oxford Nanopore Technologies' MinION run. How can I quickly and efficiently calculate the distribution of read lengths?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A naive approach would be to read the FASTA file in Biopython, check the length of each sequence, store the lengths in a numpy array and plot the results using matplotlib, but this seems like reinventing the wheel.&lt;/p&gt;&#xA;" OwnerUserId="163" LastActivityDate="2017-06-24T18:33:29.243" Title="Read length distribution from FASTA file" Tags="&lt;fasta&gt;&lt;nanopore&gt;" AnswerCount="7" CommentCount="4" FavoriteCount="2" />
  <row Id="46" PostTypeId="1" AcceptedAnswerId="47" CreationDate="2017-05-17T04:46:54.870" Score="-4" ViewCount="150" Body="&lt;p&gt;I have a computer engineering background, not biology.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I started working on a bioinformatics project recently, which involves &lt;em&gt;de-novo&lt;/em&gt; assembly. I came to know the terms &lt;code&gt;Transcriptome&lt;/code&gt; and &lt;code&gt;Genome&lt;/code&gt;, but I cannot identify the difference between these two.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know a transcriptome is the set of all messenger RNA molecules in a cell, but am not sure how this is different from a genome.&lt;/p&gt;&#xA;" OwnerUserId="173" LastEditorUserId="131" LastEditDate="2017-06-21T10:24:47.297" LastActivityDate="2017-06-21T10:24:47.297" Title="What is the difference between a transcriptome and a genome?" Tags="&lt;genome&gt;&lt;transcriptome&gt;" AnswerCount="2" CommentCount="4" FavoriteCount="1" ClosedDate="2017-07-11T10:57:28.610" />
  <row Id="47" PostTypeId="2" ParentId="46" CreationDate="2017-05-17T04:52:48.293" Score="14" Body="&lt;p&gt;In brief, the  “genome”  is the collection of all  DNA  present  in  the  nucleus  and  the  mitochondria of a  somatic  cell. The initial product of genome expression is the “transcriptome”, a collection of RNA molecules derived from those genes.&lt;/p&gt;&#xA;" OwnerUserId="23" LastEditorUserId="141" LastEditDate="2017-05-18T00:01:35.137" LastActivityDate="2017-05-18T00:01:35.137" CommentCount="2" />
  <row Id="49" PostTypeId="2" ParentId="45" CreationDate="2017-05-17T05:09:23.507" Score="5" Body="&lt;p&gt;There are several potential approaches. For example:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc295&quot; rel=&quot;noreferrer&quot;&gt;histogram of sequence lengths&lt;/a&gt; in the Biopython tutorial&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/maasha/biopieces/wiki/plot_distribution&quot; rel=&quot;noreferrer&quot;&gt;plot_distribution&lt;/a&gt; from the Ruby-based biopieces framework&lt;/li&gt;&#xA;&lt;li&gt;various solutions to get sequence length &lt;a href=&quot;https://gif.biotech.iastate.edu/calculate-sequence-lengths-fasta-file&quot; rel=&quot;noreferrer&quot;&gt;including bioawk&lt;/a&gt; and &lt;a href=&quot;https://www.biostars.org/p/118954/#119066&quot; rel=&quot;noreferrer&quot;&gt;EMBOSS infoseq&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;As to which of these are &quot;quick and efficient&quot; using a 10 GB file...it's hard to say in advance. You may have to try and benchmark a few of them.&lt;/p&gt;&#xA;" OwnerUserId="150" LastActivityDate="2017-05-17T05:09:23.507" CommentCount="0" />
  <row Id="50" PostTypeId="2" ParentId="46" CreationDate="2017-05-17T05:13:07.323" Score="3" Body="&lt;p&gt;They are two very different things. Your genome is a large section of about 3 billion DNA nucleotide bases. It has no concept of exon and introns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Transcriptome is a study of &lt;a href=&quot;https://en.wikipedia.org/wiki/Transcription_(biology)&quot; rel=&quot;nofollow noreferrer&quot;&gt;transcriptions&lt;/a&gt;. You have introns and exons. We can now talk about alternative splicing and gene expression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can think your genome is like a cooking recipe. While it's good to have a good recipe, you can't do much if you don't use it for cooking. &lt;/p&gt;&#xA;" OwnerUserId="174" LastActivityDate="2017-05-17T05:13:07.323" CommentCount="0" />
  <row Id="51" PostTypeId="1" CreationDate="2017-05-17T05:19:48.690" Score="10" ViewCount="89" Body="&lt;p&gt;I want to focus on transcriptome analysis. We know it's possible to analyze RNA-Seq experiment based on alignment or k-mers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Possible alignment workflow:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Align sequence reads with &lt;a href=&quot;https://ccb.jhu.edu/software/tophat/index.shtml&quot; rel=&quot;noreferrer&quot;&gt;TopHat2&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Quantify the gene expression with &lt;a href=&quot;http://cole-trapnell-lab.github.io/cufflinks/install/&quot; rel=&quot;noreferrer&quot;&gt;Cufflinks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Possible reference-free workflow:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Quantify sequence reads with &lt;a href=&quot;https://pachterlab.github.io/kallisto/&quot; rel=&quot;noreferrer&quot;&gt;Kallisto&lt;/a&gt; reference-free index&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Both strategy generate gene expression table.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What are pros and cons for each of the approach? Can you give guideline?&lt;/p&gt;&#xA;" OwnerUserId="174" LastEditorUserId="174" LastEditDate="2017-05-17T05:40:44.417" LastActivityDate="2017-05-18T00:01:14.437" Title="Alignment based vs reference-free (transcriptome analysis)?" Tags="&lt;transcriptome&gt;&lt;rna-seq&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="2" />
  <row Id="52" PostTypeId="1" AcceptedAnswerId="54" CreationDate="2017-05-17T05:24:25.273" Score="9" ViewCount="110" Body="&lt;p&gt;&lt;a href=&quot;https://www.thermofisher.com/order/catalog/product/4456740&quot; rel=&quot;noreferrer&quot;&gt;ERCC spike-in&lt;/a&gt; is a set of synthetic controls developed for RNA-Seq. I'm interested in using it to normalize my RNA-Seq samples. In particular, I'd like to use the spike-ins to remove technical bias and any variation that should not be part of my analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The site doesn't give any details on how I can do that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What are the possible normalization strategies? Can you briefly describe them?&lt;/p&gt;&#xA;" OwnerUserId="174" LastEditorUserId="203" LastEditDate="2017-07-28T11:18:36.177" LastActivityDate="2017-08-01T22:16:10.137" Title="Normalization methods with RNA-Seq ERCC spike in?" Tags="&lt;rna-seq&gt;&lt;transcriptome&gt;&lt;normalization&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="54" PostTypeId="2" ParentId="52" CreationDate="2017-05-17T05:43:40.693" Score="8" Body="&lt;p&gt;You may consider using &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/RUVSeq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;RUVSeq&lt;/a&gt;. Here is an excerpt from the &lt;a href=&quot;http://www.nature.com/nbt/journal/v32/n9/full/nbt.2931.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;2013 Nature Biotechnology publication&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;We evaluate the performance of the External RNA Control Consortium (ERCC) spike-in controls and investigate the possibility of using them directly for normalization. We show that the spike-ins are not reliable enough to be used in standard global-scaling or regression-based normalization procedures. We propose a normalization strategy, called remove unwanted variation (RUV), that adjusts for nuisance technical effects by performing factor analysis on suitable sets of control genes (e.g., ERCC spike-ins) or samples (e.g., replicate libraries).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;RUVSeq essentially fits a generalized linear model (GLM) to the expression data, where your expression matrix $Y$ is a $m$ by $n$ matrix, where $m$ is the number of samples and $n$ the number of genes. The model boils down to&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$Y = X*\beta + Z*\gamma + W*\alpha + \epsilon$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $X$ describes the conditions of interest (e.g., treatment vs. control), $Z$ describes observed covariates (e.g., gender) and $W$ describes unobserved covariates (e.g., batch, temperature, lab). $\beta$, $\gamma$ and $\alpha$ are parameter matrices which record the contribution of $X$, $Z$ and $W$, and $\epsilon$ is random noise. For subset of carefully selected genes (e.g., ERCC spike-ins, housekeeping genes, or technical replicates) we can assume that $X$ and $Z$ are zero, and find $W$ - the &quot;unwanted variation&quot; in your sample. &lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="163" LastEditDate="2017-07-28T02:29:08.227" LastActivityDate="2017-07-28T02:29:08.227" CommentCount="0" />
  <row Id="55" PostTypeId="2" ParentId="51" CreationDate="2017-05-17T06:29:37.357" Score="6" Body="&lt;p&gt;I wouldn't say &lt;a href=&quot;https://pachterlab.github.io/kallisto/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kallisto&lt;/a&gt; (or &lt;a href=&quot;http://salmon.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Salmon&lt;/a&gt;) are reference-free. They use a transcriptome as reference anda concept called &lt;em&gt;pseudo-alignment&lt;/em&gt; which greatly speed up the process of assigning your reads to a transcript.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, both approaches of (i) mapping against a reference genome (what you called &lt;em&gt;alignment workflow&lt;/em&gt; ) and (ii) mapping against a reference transcriptome will serve different purposes&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Transcriptome mapping using pseudoalignent is becoming the method of choice for gene/transcript quantification and differential expression analysis. The drawback is that you only focus on known transcripts&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Two typical workflows are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kallisto followed by &lt;a href=&quot;http://pachterlab.github.io/sleuth/&quot; rel=&quot;nofollow noreferrer&quot;&gt;sleuth&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Salmon, followed by tximport and DESeq2/EdgeR&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Genome mapping is useful for, per example discovery of new isoforms. You shouldn't use TopHat anymore as it has been discontinued by the author.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A typical workflow would be:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hisat2 (alignment)&lt;/li&gt;&#xA;&lt;li&gt;StringTie (transcript assembly and abundance estimation)&lt;/li&gt;&#xA;&lt;li&gt;Ballgown (differential expression)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="179" LastEditorUserId="179" LastEditDate="2017-05-17T08:11:35.430" LastActivityDate="2017-05-17T08:11:35.430" CommentCount="1" />
  <row Id="56" PostTypeId="2" ParentId="45" CreationDate="2017-05-17T06:38:49.653" Score="1" Body="&lt;p&gt;It is not exactly what you asked, but you can generate a histogram of read length distribution of your nanopore data directly from the HDF5 files using &lt;a href=&quot;https://poretools.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;poretools&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="179" LastActivityDate="2017-05-17T06:38:49.653" CommentCount="1" />
  <row Id="57" PostTypeId="1" AcceptedAnswerId="80" CreationDate="2017-05-17T07:03:57.863" Score="6" ViewCount="149" Body="&lt;p&gt;A popular framework to analyze differences between groups, either experiments or diseases, in transcriptomics is using linear models (&lt;a href=&quot;http://bioconductor.org/packages/limma&quot; rel=&quot;noreferrer&quot;&gt;limma&lt;/a&gt; is a popular choice). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance we have a disease D with three stages as defined by clinicians, A, B and C. 10 samples each stage and the healthy H to compare with is RNA-sequenced. A typical linear model would be to observe the three stages&lt;code&gt;~A+B+C&lt;/code&gt; independently. The data of each stage is not from the same person. (but for the question assume it isn't)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My understanding is that such a model would not take into account that stage C appears only on 30% of patients in stage B. And that a healthy patient upon external factors can jump to stage B. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we want to find the role of a gene in the disease we should include somehow this information in the model. Which makes me think about mixing linear models and hidden Markov chains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can such a disease be described in terms of linear models with such data and information?&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2017-05-17T13:04:24.530" LastActivityDate="2017-05-19T11:58:27.690" Title="Linear models of complex diseases" Tags="&lt;transcriptome&gt;&lt;disease-model&gt;" AnswerCount="1" CommentCount="16" FavoriteCount="1" />
  <row Id="58" PostTypeId="2" ParentId="45" CreationDate="2017-05-17T07:32:20.717" Score="6" Body="&lt;p&gt;Using Biopython and matplotlib would seem like the way to go, indeed.&#xA;It really just boils down to three lines of code to get that graph:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import Bio, pandas&#xA;lengths = map(len, Bio.SeqIO.parse('/path/to/the/seqs.fasta', 'fasta'))&#xA;pandas.Series(lengths).hist(color='gray', bins=1000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Of course you might want to make a longer script that's callable from the command line, with a couple options. You are welcome to use mine:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python2&#xA;&#xA;&quot;&quot;&quot;&#xA;A custom made script to plot the distribution of lengths&#xA;in a fasta file.&#xA;&#xA;Written by Lucas Sinclair.&#xA;Kopimi.&#xA;&#xA;You can use this script from the shell like this:&#xA;$ ./fastq_length_hist --input seqs.fasta --out seqs.pdf&#xA;&quot;&quot;&quot;&#xA;&#xA;###############################################################################&#xA;# Modules #&#xA;import argparse, sys, time, getpass, locale&#xA;from argparse import RawTextHelpFormatter&#xA;from Bio import SeqIO&#xA;import pandas&#xA;&#xA;# Matplotlib #&#xA;import matplotlib&#xA;matplotlib.use('Agg', warn=False)&#xA;from matplotlib import pyplot&#xA;&#xA;################################################################################&#xA;desc = &quot;fasta_length_hist v1.0&quot;&#xA;parser = argparse.ArgumentParser(description=desc, formatter_class=RawTextHelpFormatter)&#xA;&#xA;# All the required arguments #&#xA;parser.add_argument(&quot;--input&quot;, help=&quot;The fasta file to process&quot;, type=str)&#xA;parser.add_argument(&quot;--out&quot;, type=str)&#xA;&#xA;# All the optional arguments #&#xA;parser.add_argument(&quot;--x_log&quot;, default=True, type=bool)&#xA;parser.add_argument(&quot;--y_log&quot;, default=True, type=bool)&#xA;&#xA;# Parse it #&#xA;args        = parser.parse_args()&#xA;input_path  = args.input&#xA;output_path = args.out&#xA;x_log       = bool(args.x_log)&#xA;y_log       = bool(args.y_log)&#xA;&#xA;################################################################################&#xA;# Read #&#xA;lengths = map(len, SeqIO.parse(input_path, 'fasta'))&#xA;&#xA;# Report #&#xA;sys.stderr.write(&quot;Read all lengths (%i sequences)\n&quot; % len(lengths))&#xA;sys.stderr.write(&quot;Longest sequence: %i bp\n&quot; % max(lengths))&#xA;sys.stderr.write(&quot;Shortest sequence: %i bp\n&quot; % min(lengths))&#xA;sys.stderr.write(&quot;Making graph...\n&quot;)&#xA;&#xA;# Data #&#xA;values = pandas.Series(lengths)&#xA;&#xA;# Plot #&#xA;fig   = pyplot.figure()&#xA;axes  = values.hist(color='gray', bins=1000)&#xA;fig   = pyplot.gcf()&#xA;title = 'Distribution of sequence lengths'&#xA;axes.set_title(title)&#xA;axes.set_xlabel('Number of nucleotides in sequence')&#xA;axes.set_ylabel('Number of sequences with this length')&#xA;axes.xaxis.grid(False)&#xA;&#xA;# Log #&#xA;if x_log: axes.set_yscale('symlog')&#xA;if y_log: axes.set_xscale('symlog')&#xA;&#xA;# Adjust #&#xA;width=18.0; height=10.0; bottom=0.1; top=0.93; left=0.07; right=0.98&#xA;fig.set_figwidth(width)&#xA;fig.set_figheight(height)&#xA;fig.subplots_adjust(hspace=0.0, bottom=bottom, top=top, left=left, right=right)&#xA;&#xA;# Data and source #&#xA;fig.text(0.99, 0.98, time.asctime(), horizontalalignment='right')&#xA;fig.text(0.01, 0.98, 'user: ' + getpass.getuser(), horizontalalignment='left')&#xA;&#xA;# Nice digit grouping #&#xA;sep = ('x','y')&#xA;if 'x' in sep:&#xA;    locale.setlocale(locale.LC_ALL, '')&#xA;    seperate = lambda x,pos: locale.format(&quot;%d&quot;, x, grouping=True)&#xA;    axes.xaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(seperate))&#xA;if 'y' in sep:&#xA;    locale.setlocale(locale.LC_ALL, '')&#xA;    seperate = lambda x,pos: locale.format(&quot;%d&quot;, x, grouping=True)&#xA;    axes.yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(seperate))&#xA;&#xA;# Save it #&#xA;fig.savefig(output_path, format='pdf')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;EDIT - an example output:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/5kmaX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5kmaX.png&quot; alt=&quot;Sequence length distribution&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="40" LastEditorUserId="40" LastEditDate="2017-05-17T18:09:52.143" LastActivityDate="2017-05-17T18:09:52.143" CommentCount="0" />
  <row Id="59" PostTypeId="2" ParentId="51" CreationDate="2017-05-17T07:58:46.987" Score="9" Body="&lt;p&gt;First of all, I would emphasize that &quot;alignment-free&quot; quantification tools like Salmon and Kallisto are &lt;em&gt;not&lt;/em&gt; reference-free. The basic difference between them and more traditional aligners is that they do not report a specific position (either in a genome or transcriptome) to which a read maps. However, their overall purpose is still to quantify the expression levels (or differences) of a known set of transcripts; hence, they require a reference (which could be arbitrarily defined).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most important criterion for deciding which approach to use (and this is true of almost everything in genomics) is exactly what question you would like to answer. If you are primarily interested in quantifying and comparing expression of mature mRNA from known transcripts, then a transcriptome-based alignment may be fastest and best. However, you may miss potentially interesting features outside of those known transcripts, such as new isoforms, non-coding RNAs, or information about pre-mRNA levels, which can often be gleaned from intronic reads (see the &lt;a href=&quot;https://www.nature.com/nbt/journal/v33/n7/full/nbt.3269.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;EISA&lt;/a&gt; method).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4729156/&quot; rel=&quot;nofollow noreferrer&quot;&gt;This paper&lt;/a&gt; also has some good considerations about which tools may work best depending on the question you want to answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, another fast and flexible aligner (which can be used with or without a reference transcriptome) is &lt;a href=&quot;https://github.com/alexdobin/STAR&quot; rel=&quot;nofollow noreferrer&quot;&gt;STAR&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="182" LastEditorUserId="141" LastEditDate="2017-05-18T00:01:14.437" LastActivityDate="2017-05-18T00:01:14.437" CommentCount="0" />
  <row Id="60" PostTypeId="5" CreationDate="2017-05-17T07:58:51.323" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-17T07:58:51.323" LastActivityDate="2017-05-17T07:58:51.323" CommentCount="0" />
  <row Id="61" PostTypeId="4" CreationDate="2017-05-17T07:58:51.323" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-17T07:58:51.323" LastActivityDate="2017-05-17T07:58:51.323" CommentCount="0" />
  <row Id="62" PostTypeId="1" AcceptedAnswerId="63" CreationDate="2017-05-17T09:15:38.363" Score="2" ViewCount="92" Body="&lt;p&gt;I would like to test for positive selection in a large set of genes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to have a yes/no answer to the question if gene was evolving under positive selection. Therefore I chose model &lt;a href=&quot;https://doi.org/10.1093/molbev/msv035&quot; rel=&quot;nofollow noreferrer&quot;&gt;BUSTED&lt;/a&gt; for my analysis, which is implemented in &lt;a href=&quot;http://www.hyphy.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;HYPHY&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I launch HYPHY in the command-line, I get a series of question I  have to answer in the interactive form.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there way to perform a batch analysis involving multiple datasets, i.e. many sequence alignments?&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-05-17T11:03:54.253" LastActivityDate="2017-05-18T17:39:37.100" Title="How to run HYPHY on multiple files non-interactively?" Tags="&lt;codon-models&gt;&lt;positive-selection&gt;&lt;hyphy&gt;&lt;busted&gt;" AnswerCount="1" CommentCount="5" FavoriteCount="1" />
  <row Id="63" PostTypeId="2" ParentId="62" CreationDate="2017-05-17T09:15:38.363" Score="4" Body="&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I made the following bash script to generate an input file for HYPHY.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/bin/bash&#xA;cat &amp;lt;&amp;lt; EOF&#xA;inputRedirect = {};&#xA;inputRedirect[&quot;01&quot;]=&quot;Universal&quot;; // genetic code&#xA;inputRedirect[&quot;02&quot;]=&quot;$(readlink -f $1)&quot;; // codon data&#xA;inputRedirect[&quot;03&quot;]=&quot;$(readlink -f $2)&quot;; // tree&#xA;inputRedirect[&quot;04&quot;]=&quot;${3:-All}&quot;; // Test for selection on a branch&#xA;inputRedirect[&quot;05&quot;]=&quot;&quot;; // complete selection&#xA;&#xA;&#xA;ExecuteAFile (HYPHY_LIB_DIRECTORY+&quot;TemplateBatchFiles/BUSTED.bf&quot;, inputRedirect);&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you want to test all the branches jointly run:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./gen_busted.sh alignment.phy tree.nwk &amp;gt; script.bf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or for a particular branch:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./gen_busted.sh alignment.phy tree.nwk branchLabel &amp;gt; script.bf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And then run:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;HYPHYMP script.bf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now you can use the script to generate a &lt;code&gt;.bf&lt;/code&gt; file for every sequence alignment you have, and run HYPHY for every file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Longer version&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I will describe how to make a similar script for any model, such as MEME, RELAX or PARRIS.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, find the &lt;code&gt;.bf&lt;/code&gt; script which performs the analysis. On GNU/Linux it should be at &lt;code&gt;HYPHY_INSTALLATION_PATH/lib/hyphy/TemplateBatchFiles&lt;/code&gt; (it might be different between different OS). In case of BUSTED your script has a name &lt;code&gt;BUSTED.bf&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now run this model in the interactive mode in order to record all the inputs required by the model. Make sure to specify &lt;strong&gt;full paths&lt;/strong&gt; to all the files.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;HYPHYMP HYPHY_INSTALLATION_PATH/lib/hyphy/TemplateBatchFiles/BUSTED.bf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In case of BUSTED the inputs are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Genetic code.&lt;/li&gt;&#xA;&lt;li&gt;Codon data (sequence alignment in phylip format).&lt;/li&gt;&#xA;&lt;li&gt;Tree (in newick format).&lt;/li&gt;&#xA;&lt;li&gt;Which branches to test.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Now for every input alignment you need to generate a batch file which looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;inputRedirect = {};&#xA;inputRedirect[&quot;01&quot;]=&quot;Universal&quot;; // genetic code&#xA;inputRedirect[&quot;02&quot;]=&quot;/path/to/alignment.phy&quot;; // codon data&#xA;inputRedirect[&quot;03&quot;]=&quot;/path/to/tree.nwk&quot;; // tree&#xA;inputRedirect[&quot;04&quot;]=&quot;All&quot;; // Test for selection on all branches&#xA;inputRedirect[&quot;05&quot;]=&quot;BRANCH1&quot;; // Test for selection on branch1 &#xA;inputRedirect[&quot;06&quot;]=&quot;BRANCH2&quot;; // Test for selection on branch2 &#xA;inputRedirect[&quot;07&quot;]=&quot;&quot;; // complete selection&#xA;&#xA;&#xA;ExecuteAFile (HYPHY_LIB_DIRECTORY+&quot;TemplateBatchFiles/BUSTED.bf&quot;, inputRedirect);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now you can create a script which generates the file. The most important thing, don't forget to use full paths in the &lt;code&gt;.bf&lt;/code&gt; file.&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-05-18T17:39:37.100" LastActivityDate="2017-05-18T17:39:37.100" CommentCount="5" />
  <row Id="64" PostTypeId="2" ParentId="21" CreationDate="2017-05-17T09:34:08.350" Score="11" Body="&lt;p&gt;To add to &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/38/203&quot;&gt;rightskewed answer&lt;/a&gt;:&#xA;While it is true that:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Gencode is an additive set of annotation (the manual one done by Havana and an automated one done by Ensembl), &lt;/p&gt;&#xA;&#xA;&lt;p&gt;the annotation (GTF) files are quite similar for a few exceptions involving the X chromosome and Y par and additional remarks in the Gencode file (see more at &lt;a href=&quot;https://www.gencodegenes.org/faq.html&quot; rel=&quot;noreferrer&quot;&gt;FAQ - Gencode&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;What are the actual differences between different annotation databases?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;They are a few differences, but the main one  &lt;strong&gt;for me (and it could be stupid)&lt;/strong&gt; is &lt;/p&gt;&#xA;&#xA;&lt;p&gt;that &lt;strong&gt;Refseq is developed by the American NCBI&lt;/strong&gt; and &lt;/p&gt;&#xA;&#xA;&lt;p&gt;the &lt;strong&gt;ENSEMBL is mainly developed by the European EMBL-EBI.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Often, labs or people will just start using what is the best known to them (because of a course or workshop) or because they start working with one of the databases with one specific tool and keep with it later.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;My lab, for reasons still unknown to me, prefers Ensembl annotations (we're working with transcript/exon expression estimation), while some software ship with RefSeq annotations. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Your lab might be mostly European based people or they might also have read papers like the one from Frankish et al.  Comparison of GENCODE and RefSeq gene annotation and the impact of reference geneset on variant effect prediction. BMC Genomics 2015; 16(Suppl 8):S2 - DOI: 10.1186/1471-2164-16-S8-S2&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From the &lt;a href=&quot;https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-16-S8-S2&quot; rel=&quot;noreferrer&quot;&gt;Frankish et al. paper&lt;/a&gt; paper:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The GENCODE Comprehensive transcripts contain more exons, have greater genomic coverage and capture many more variants than RefSeq in both genome and exome datasets, while the GENCODE Basic set shows a higher degree of concordance with RefSeq and has fewer unique features.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;As for: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Are there significant differences between them today, or are they, for all intents and purposes, interchangeable (e.g., are exon coordinates between RefSeq and Ensembl annotations interchangeable)?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;No. I don't think they are great differences between them as that the global picture should stay the same (although you will see different results if you are interested in a small set of genes). However, &lt;strong&gt;they are not directly interchangeable&lt;/strong&gt;. Particularly as there are many versions of Ensembl and Refseq based on different genome annotations (and those won't be interchangeable between themselves either in most cases).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;However, you can easily translate&lt;/strong&gt; most[1] of your Refseq IDs to ENSEMBL IDs and vice-versa with tools as &lt;a href=&quot;http://www.ensembl.org/biomart/martview&quot; rel=&quot;noreferrer&quot;&gt;http://www.ensembl.org/biomart/martview&lt;/a&gt; for example (there are devoted libraries/API as well like &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/biomaRt.html&quot; rel=&quot;noreferrer&quot;&gt;Biocondutor: biomaRt&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[1] Most as sometimes, they might be annotated in one of the database but haven't (yet) an equivalent in the other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fine, even if people tends to keep to what they are used to (and that the annotations are constantly expanded and corrected) depending on the research subject one might be interested in using one database over another: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From &lt;a href=&quot;https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-015-1308-8&quot; rel=&quot;noreferrer&quot;&gt;Zhao S, Zhang B. A comprehensive evaluation of ensembl, RefSeq, and UCSC annotations in the context of RNA-seq read mapping and gene quantification. BMC Genomics. 2015;16: 97.&lt;/a&gt; paper:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;When choosing an annotation database, researchers should keep in mind that no database is perfect and some gene annotations might be inaccurate or entirely wrong. [..] Wu et al. [27] suggested that when conducting research that emphasizes reproducible and robust gene expression estimates, a less complex genome annotation, such as RefGene, might be preferred. When conducting more exploratory research, a more complex genome annotation, such as Ensembl, should be chosen. &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[..]&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[27] Wu P-Y, Phan JH, Wang MD. Assessing the impact of human genome annotation choice on RNA-seq expression estimates. BMC Bioinformatics. 2013;14(Suppl 11):S8. doi: 10.1186/1471-2105-14-S11-S8.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="203" LastEditorUserId="203" LastEditDate="2017-05-17T10:16:19.113" LastActivityDate="2017-05-17T10:16:19.113" CommentCount="0" />
  <row Id="65" PostTypeId="2" ParentId="19" CreationDate="2017-05-17T10:18:12.290" Score="4" Body="&lt;p&gt;At its easiest, you just store the &lt;em&gt;forward&lt;/em&gt; ($F$) and &lt;em&gt;reverse&lt;/em&gt; ($R$) hash value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You update the forward hash value by conventional means, e.g. bit-shifting the base value into its lower bits:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;F_{n + 1} = ((F_n \ll B \mid x) \mathop\&amp;amp; M,&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$B$ is the bit size of the base encoding, $M$ is the word mask for a word of length $W$ bits, and can be omitted if the width of the hash is the width of the machine word; and the reverse hash value by doing the mathematical inverse, e.g. bit-shifting the value of the  reverse complement of the base into the upper bits:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;R_{n + 1} = ((R_n \gg B) \mid (\operatorname{compl}x \ll (W - B))) \mathop\&amp;amp; M.&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And then you compute a &lt;em&gt;combined hash&lt;/em&gt; value by xoring the two hashes, $H  = F \oplus R$.&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-07-26T16:10:55.117" LastActivityDate="2017-07-26T16:10:55.117" CommentCount="9" />
  <row Id="66" PostTypeId="1" AcceptedAnswerId="69" CreationDate="2017-05-17T10:28:32.203" Score="10" ViewCount="318" Body="&lt;p&gt;I have the following data of fragment counts for each gene in 16 samples:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; str(expression)&#xA;'data.frame':   42412 obs. of  16 variables:&#xA; $ sample1 : int  4555 49 122 351 53 27 1 0 0 2513 ...&#xA; $ sample2 : int  2991 51 55 94 49 10 55 0 0 978 ...&#xA; $ sample3 : int  3762 28 136 321 94 12 15 0 0 2181 ...&#xA; $ sample4 : int  4845 43 193 361 81 48 9 0 0 2883 ...&#xA; $ sample5 : int  2920 24 104 151 50 20 32 0 0 1743 ...&#xA; $ sample6 : int  4157 11 135 324 58 26 4 0 0 2364 ...&#xA; $ sample7 : int  3000 19 155 242 57 12 18 2 0 1946 ...&#xA; $ sample8 : int  5644 30 227 504 91 37 11 0 0 2988 ...&#xA; $ sample9 : int  2808 65 247 93 272 38 1108 1 0 1430 ...&#xA; $ sample10: int  2458 37 163 64 150 29 729 2 1 1049 ...&#xA; $ sample11: int  2064 30 123 51 142 23 637 0 0 1169 ...&#xA; $ sample12: int  1945 63 209 40 171 41 688 3 2 749 ...&#xA; $ sample13: int  2015 57 432 82 104 47 948 4 0 1171 ...&#xA; $ sample14: int  2550 54 177 59 201 36 730 0 0 1474 ...&#xA; $ sample15: int  2425 90 279 73 358 34 1052 3 3 1027 ...&#xA; $ sample16: int  2343 56 365 67 161 43 877 3 1 1333 ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How do I compute RPKM values from these?&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-05-17T14:17:04.643" LastActivityDate="2017-07-26T16:13:26.200" Title="How to compute RPKM in R?" Tags="&lt;transcriptome&gt;&lt;rna-seq&gt;&lt;r&gt;&lt;rpkm&gt;" AnswerCount="4" CommentCount="1" FavoriteCount="2" />
  <row Id="67" PostTypeId="2" ParentId="66" CreationDate="2017-05-17T10:28:32.203" Score="6" Body="&lt;p&gt;RPKM is defined as:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;RPKM =   numberOfReads / ( geneLength/1000 * totalNumReads/1,000,000 )&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;As you can see, you need to have gene lengths for every gene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say &lt;code&gt;geneLength&lt;/code&gt; is a vector which have the same number of rows as your &lt;code&gt;data.frame&lt;/code&gt;, and every value of the vector corresponds to a gene (row) in &lt;code&gt;expression&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# compute number of reads in each sample&#xA;totalNumReads &amp;lt;- colSums(expression)&#xA;# compute RPKM&#xA;expression.rpkm &amp;lt;- data.frame(sapply(expression, function(column) 10^9 * column / geneLength / sum(column)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Regarding numerical stability&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is suggested in &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/69/191&quot;&gt;one of the answers&lt;/a&gt;, that all the computations should be performed in a log-transformed scale. In my opinion there is absolutely no reason for doing that. IEEE &lt;a href=&quot;https://en.wikipedia.org/wiki/Double-precision_floating-point_format&quot; rel=&quot;nofollow noreferrer&quot;&gt;binary64&lt;/a&gt; stores a number as binary number 1.b_{51}b{50}...b_0 times 2^{e-1023}. The &lt;em&gt;relative&lt;/em&gt; precision doesn't depend on the exponent value given that a number is in the range [~10^-308; 10^308].&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In case of RPKM we can only get out of the range if total number of reads is around 10^300, which is not realistic at all.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the bright site there is not much harm in doing computations in the log-scale either. Worst case you loose a bit of precision.&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-05-29T14:50:26.827" LastActivityDate="2017-05-29T14:50:26.827" CommentCount="4" />
  <row Id="68" PostTypeId="2" ParentId="45" CreationDate="2017-05-17T10:43:50.363" Score="15" Body="&lt;p&gt;If you want something quick and dirty you could rapidly index the FASTA with &lt;code&gt;samtools faidx&lt;/code&gt; and then put the lengths column through R (other languages are available) on the command line.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools faidx $fasta&#xA;cut -f2 $fasta.fai | Rscript -e 'data &amp;lt;- as.numeric (readLines (&quot;stdin&quot;)); summary(data); hist(data)'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This outputs a statistical summary, and creates a PDF in the current directory called Rplots.pdf, containing a histogram.&lt;/p&gt;&#xA;" OwnerUserId="215" LastActivityDate="2017-05-17T10:43:50.363" CommentCount="1" />
  <row Id="69" PostTypeId="2" ParentId="66" CreationDate="2017-05-17T10:46:41.653" Score="24" Body="&lt;p&gt;First off,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Don’t use RPKMs&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/&quot; rel=&quot;nofollow noreferrer&quot;&gt;They are truly deprecated&lt;/a&gt; because they’re confusing once it comes to paired-end reads. If anything, use &lt;em&gt;FPKM&lt;/em&gt;s, which are mathematically the same but use a more correct name (do we count paired reads separately? No, we count &lt;em&gt;fragments&lt;/em&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Even better, &lt;a href=&quot;http://rpubs.com/klmr/rnaseq-norm&quot; rel=&quot;nofollow noreferrer&quot;&gt;use TPM (= transcripts per million), or an appropriate cross-library normalisation method&lt;/a&gt;. TMP is defined as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\text{TPM}_\color{orchid}i =&#xA;    {\color{dodgerblue}{\frac{x_\color{orchid}i}{{l_\text{eff}}_\color{orchid}i}}}&#xA;    \cdot&#xA;    \frac{1}{\sum_\color{tomato}j \color{dodgerblue}{\frac{x_\color{tomato}j}{{l_\text{eff}}_\color{tomato}j}}}&#xA;    \cdot&#xA;    \color{darkcyan}{10^6}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$\color{orchid}i$: transcript index,&lt;/li&gt;&#xA;&lt;li&gt;$x_i$: transcript raw count,&lt;/li&gt;&#xA;&lt;li&gt;$\color{tomato}j$ iterates over all (known) transcripts,&lt;/li&gt;&#xA;&lt;li&gt;$\color{dodgerblue}{\frac{x_k}{{l_\text{eff}}_k}}$: rate of fragment coverage per nucleobase ($l_\text{eff}$ being the &lt;a href=&quot;https://bioinformatics.stackexchange.com/q/367/29&quot;&gt;effective length&lt;/a&gt;),&lt;/li&gt;&#xA;&lt;li&gt;$\color{darkcyan}{10^6}$: scaling factor (= “per millions”).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;That said, FPKM an be calculated in R as follows. Note that most of the calculation happens in log transformed number space, to avoid &lt;a href=&quot;https://en.wikipedia.org/wiki/Numerical_stability&quot; rel=&quot;nofollow noreferrer&quot;&gt;numerical instability&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;fpkm = function (counts, effective_lengths) {&#xA;    exp(log(counts) - log(effective_lengths) - log(sum(counts)) + log(1E9))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here, the &lt;em&gt;effective length&lt;/em&gt; is the transcript length minus the mean fragment length plus 1; that is, all the possible positions of an average fragment inside the transcript, which equals the number of all distinct fragments that can be sampled from a transcript.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This function handles &lt;em&gt;one&lt;/em&gt; library at a time. I (&lt;a href=&quot;http://varianceexplained.org/r/tidy-genomics-biobroom/&quot; rel=&quot;nofollow noreferrer&quot;&gt;and others&lt;/a&gt;) argue that this is the way functions should be written. If you want to apply the code to multiple libraries, nothing is easier using &lt;a href=&quot;http://dplyr.tidyverse.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;‹dplyr›&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;tidy_expression = tidy_expression %&amp;gt;%&#xA;    group_by(Sample) %&amp;gt;%&#xA;    mutate(FPKM = fpkm(Count, col_data$Lengths))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, the data in the question isn’t in tidy data format, so we first need to transform it accordingly using &lt;a href=&quot;http://tidyr.tidyverse.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;‹tidyr›&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;tidy_expression = expression %&amp;gt;%&#xA;    gather(Sample, Count)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;This equation fails if all your counts are zero; instead of zeros you will get a vector of NaNs. You might want to account for that.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;And I mentioned that TPMs are superior, so here’s their function as well:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;tpm = function (counts, effective_lengths) {&#xA;    rate = log(counts) - log(effective_lengths)&#xA;    exp(rate - log(sum(exp(rate))) + log(1E6))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-07-26T16:13:26.200" LastActivityDate="2017-07-26T16:13:26.200" CommentCount="2" />
  <row Id="70" PostTypeId="1" AcceptedAnswerId="77" CreationDate="2017-05-17T11:51:45.250" Score="8" ViewCount="77" Body="&lt;p&gt;I have two annotations of the same genome generated with different annotation pipelines. I want to identify overlapping gene models. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;An important feature of this genome is that there are many 'genes within genes', i.e. a genemodel in the intron of another genemodel&lt;/em&gt;. Therefore, I only want to count two genemodels as overlapping when their coding sequence exon annotations overlap.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using something like &lt;code&gt;bedtools intersect&lt;/code&gt; it is straightforward to calculate overlap between the gene-level annotations. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However: I am not sure how to select genes as overlapping when only their coding sequence exons (CDS features) overlap.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2017-05-17T14:56:36.030" Title="How to calculate overlapping genes between two genome annotation versions" Tags="&lt;annotation&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="2" />
  <row Id="71" PostTypeId="5" CreationDate="2017-05-17T11:57:13.740" Score="0" Body="&lt;h1&gt;R Programming Language&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.r-project.org&quot; rel=&quot;nofollow noreferrer&quot; title=&quot;R project homepage&quot;&gt;R&lt;/a&gt; is a free, open-source programming language and software environment for &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_statistics&quot; rel=&quot;nofollow noreferrer&quot;&gt;statistical computing&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Bioinformatics&quot; rel=&quot;nofollow noreferrer&quot;&gt;bioinformatics&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphics&quot; rel=&quot;nofollow noreferrer&quot;&gt;graphics&lt;/a&gt;. It is a multi-paradigm language and dynamically typed. R is an implementation of the &lt;a href=&quot;http://lib.stat.cmu.edu/S/Spoetry/Tutor/slanguage.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;S programming language&lt;/a&gt; combined with lexical scoping semantics inspired by &lt;a href=&quot;http://stackoverflow.com/tags/scheme/info&quot; title=&quot;Scheme on StackOverflow&quot;&gt;Scheme&lt;/a&gt;. R was created by &lt;a href=&quot;http://www.stat.auckland.ac.nz/~ihaka/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Ross Ihaka&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Robert_Gentleman_%28statistician%29&quot; rel=&quot;nofollow noreferrer&quot;&gt;Robert Gentleman&lt;/a&gt; and is now developed by the &lt;a href=&quot;http://www.r-project.org/contributors.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;R Development Core Team&lt;/a&gt;. The R environment is easily extended through a packaging system on &lt;a href=&quot;http://cran.r-project.org&quot; rel=&quot;nofollow noreferrer&quot; title=&quot;CRAN The Comprehensive R Archive Network&quot;&gt;CRAN&lt;/a&gt;, the Comprehensive R Archive Network. &lt;/p&gt;&#xA;" OwnerUserId="131" LastEditorUserId="131" LastEditDate="2017-05-18T00:04:34.707" LastActivityDate="2017-05-18T00:04:34.707" CommentCount="0" />
  <row Id="72" PostTypeId="4" CreationDate="2017-05-17T11:57:13.740" Score="0" Body="R is a free, open-source programming language and software environment for statistical computing, bioinformatics, and graphics. Please supplement your question with a minimal reproducible example. Use dput() for data and specify all non-base packages with library calls. For statistical questions please use http://stats.stackexchange.com." OwnerUserId="131" LastEditorUserId="131" LastEditDate="2017-05-18T00:02:05.573" LastActivityDate="2017-05-18T00:02:05.573" CommentCount="0" />
  <row Id="73" PostTypeId="1" AcceptedAnswerId="76" CreationDate="2017-05-17T13:02:17.453" Score="11" ViewCount="85" Body="&lt;p&gt;I am trying to understand the benefits of joint genotyping and would be grateful if someone could provide an argument (ideally mathematically) that would clearly demonstrate the benefit of joint vs. single-sample genotyping.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is what I've gathered from other resources (Biostars, GATK forums, etc.)&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Joint-genotyping helps control FDR because errors from individually genotyped samples are added up, and amplified when merging call-sets (by Heng Li on &lt;a href=&quot;https://www.biostars.org/p/10926/&quot; rel=&quot;noreferrer&quot;&gt;https://www.biostars.org/p/10926/&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;If someone understands this, can you please clarify what is the difference on the overall FDR rate between the two scenarios (again, with an example ideally)&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Greater sensitivity for low-frequency variants - By sharing information across all samples, joint calling makes it possible to “rescue” genotype calls at sites where a carrier has low coverage but other samples within the call set have a confident variant at that location. (from &lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/article.php?id=4150&quot; rel=&quot;noreferrer&quot;&gt;https://software.broadinstitute.org/gatk/documentation/article.php?id=4150&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I don't understand how the presence of a confidently called variant at the same locus in another individual can affect the genotyping of an individual with low coverage. Is there some valid argument that allows one to consider reads from another person as evidence of a particular variant in a third person? What are the assumptions for such an argument? What if that person is from a different population with entirely different allele frequencies for that variant?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having read several of the papers (or method descriptions) that describe the latest haplotype-aware SNP calling methods (HaplotypeCaller, freebayes, Platypus) the overall framework seems to be:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;ol&gt;&#xA;&lt;li&gt;Establish a prior on the allele frequency distribution at a site of interest using one (or combination) of: non-informative prior, population genetics model-based prior like Wright Fisher, prior based on established variation patterns like dbSNP, ExAC, or gnomAD.&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;Build a list of plausible haplotypes in a region around the locus of interest using local assembly.&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;ol start=&quot;3&quot;&gt;&#xA;&lt;li&gt;Select haplotype with highest likelihood based on prior and reads data and infer the locus genotype accordingly.&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;At which point(s) in the above procedure can information between samples be shared or pooled? Should one not trust the AFS from a large-scale resource like gnomAD much more than the distribution obtained from other samples that are nominally party of the same &quot;cohort&quot; but may have little to do with each other because of different ancestry, for example?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I really want to understand the justifications and benefits offered by multi-sample genotyping and would appreciate your insights.    &lt;/p&gt;&#xA;" OwnerUserId="133" LastActivityDate="2017-05-17T14:08:21.663" Title="Single-sample vs. joint genotyping" Tags="&lt;genotyping&gt;&lt;gatk&gt;&lt;freebayes&gt;&lt;platypus&gt;&lt;multi-sample&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="4" />
  <row Id="74" PostTypeId="2" ParentId="73" CreationDate="2017-05-17T13:13:18.863" Score="2" Body="&lt;p&gt;The benefit to additional samples is seen in your point 1. The likelihood of making a variant call is a function of (1) the depth of coverage supporting a given variant (ignoring mapping/base quality considerations) and (2) the likelihood of that variant existing given background knowledge. With low depth and no background knowledge, poorly covered variants will be assumed to be sequencing errors. Adding more samples can just serve then to increase the background knowledge on a position.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-17T13:13:18.863" CommentCount="1" />
  <row Id="75" PostTypeId="2" ParentId="21" CreationDate="2017-05-17T13:34:35.107" Score="4" Body="&lt;p&gt;To add practical advice to what others have said:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a practical sense, I think the biggest difference between RefSeq and Ensembl/GENCODE is in the sensitivity/specificity trade off. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ensembl aims more towards the inclusive end, including a far larger number of transcript variants, many of which are only weakly supported.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;RefSeq trades some of this sensitivity for specificity - you can be more confident that a RefSeq transcript exists, but less confident that the ReqSeq annotation includes all of the real transcripts for a gene. &lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-05-17T13:34:35.107" CommentCount="0" />
  <row Id="76" PostTypeId="2" ParentId="73" CreationDate="2017-05-17T14:08:21.663" Score="4" Body="&lt;p&gt;Say you are sequencing to 2X coverage. Suppose at a site, sample S has one reference base and one alternate base. It is hard to tell if this is a sequencing error or a heterozygote. Now suppose you have 1000 other samples, all at 2X read depth. One of them has two ALT bases; 10 of them have one REF and one ALT. It is usually improbable that all these samples have the same sequencing error. Then you can assert sample S has a het. Multi-sample calling helps to increase the sensitivity of not so rare SNPs. Note that what matters here is the assumption of error independency. Ancestry only has a tiny indirect effect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Multi-sample calling penalizes very rare SNPs, in particular singletons. When you care about variants only, this is for good. Naively combining single-sample calls yields a higher error rate. Multi-sample calling also helps variant filtering at a later stage. For example, for a sample sequenced to 30X coverage, you would not know if a site at 45X depth is caused by a potential CNV/mismapping or by statistical fluctuation. When you see 1000 30X samples at 45X depth, you can easily know you are looking at a CNV/systematic mismapping. Multiple samples enhance most statistical signals.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Older methods pool all BAMs when calling variants. This is necessary because a single low-coverage sample does not have enough data to recover hidden INDELs. However, this strategy is not that easy to massively parallelized; adding a new sample triggers re-calling, which is very expensive as well. As we are mostly doing high-coverage sequencing these days, the old problem with INDEL calling does not matter now. GATK has this new single-sample calling pipeline where you combine per-sample gVCFs at a later stage. Such sample combining strategy is perhaps the only sensible solution when you are dealing with 100k samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The so-called haplotype based variant calling is a separate question. This type of approach helps to call INDELs, but is not of much relevance to multi-sample calling. Also, of the three variant callers in your question, only GATK (and Scalpel which you have not mentioned) use assembly at large. Freebayes does not. Platypus does but only to a limited extent and does not work well in practice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess what you really want to talk about is imputation based calling. This approach further improves sensitivity with LD. With enough samples, you can measure the LD between two positions. Suppose at position 1000, you see one REF read and no ALT reads; at position 1500, you see one REF read and two ALT reads. You would not call any SNPs at position 1000 even given multiple samples. However, when you know the two positions are strongly linked and the dominant haplotypes are REF-REF and ALT-ALT, you know the sample under investigation is likely to have a missing ALT allele. LD transfers signals across sites and enhances the power to make correct genotyping calls. Nonetheless, as we are mostly doing high-coverage sequencing nowadays, imputation based methods only have a minor effect and are rarely applied.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-05-17T14:08:21.663" CommentCount="6" />
  <row Id="77" PostTypeId="2" ParentId="70" CreationDate="2017-05-17T14:56:36.030" Score="4" Body="&lt;p&gt;&lt;strong&gt;Short Answer:&lt;/strong&gt;&#xA;In my opinion, my approach would be to pull out the CDS exons and run bedtools on those.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;A Few More Details:&lt;/strong&gt;&#xA;When you pull out the exons, make sure that you assign them all IDs if the don't already have them assigned and record which IDs &quot;belong&quot; to which genes. Now when you get exons that overlap, you know that they are coding and you can tie them back to which genes they originate from.&lt;/p&gt;&#xA;" OwnerUserId="243" LastActivityDate="2017-05-17T14:56:36.030" CommentCount="0" />
  <row Id="78" PostTypeId="2" ParentId="66" CreationDate="2017-05-17T15:16:10.923" Score="-1" Body="&lt;p&gt;If you are looking for a more visual solution (in addition to the other answers), &lt;a href=&quot;https://docs.gdc.cancer.gov/Data/Bioinformatics_Pipelines/Expression_mRNA_Pipeline/&quot; rel=&quot;nofollow noreferrer&quot;&gt;NCI Genomic Data Commons (TCGA repository)&lt;/a&gt; offers a nice formula:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/RY53Q.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RY53Q.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Where:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;RCg: Number of reads mapped to the gene&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;RCpc: Number of reads mapped to all protein-coding genes&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;RCg75: The 75th percentile read count value for genes in the sample&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;L: Length of the gene in base pairs&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;As others have pointed out, FPKMs have some problems. GDC also calculates FPKM-UQ values that are upper quartile normalized. Those are recommended for cross-sample comparison and differential expression analysis.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-05-17T15:16:10.923" CommentCount="2" />
  <row Id="79" PostTypeId="2" ParentId="66" CreationDate="2017-05-17T15:33:34.013" Score="1" Body="&lt;p&gt;If you are planning to do a differential expression analysis, you will probably don't need the RPKM calculation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;RPK= No.of Mapped reads/ length of transcript in kb (transcript length/1000)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;RPKM = RPK/total no.of reads in million (total no of reads/ 1000000)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The whole formula together:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;RPKM = (10^9 * C)/(N * L)&#xA;Where,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;C = Number of reads mapped to a gene&lt;/p&gt;&#xA;&#xA;&lt;p&gt;N = Total mapped reads in the experiment&lt;/p&gt;&#xA;&#xA;&lt;p&gt;L = exon length in base-pairs for a gene&lt;/p&gt;&#xA;" OwnerUserId="206" LastActivityDate="2017-05-17T15:33:34.013" CommentCount="0" />
  <row Id="80" PostTypeId="2" ParentId="57" CreationDate="2017-05-17T15:55:59.503" Score="2" Body="&lt;p&gt;There are two potential sources of bias in this design.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;We cannot distinguish correlation from causation. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Imagine two cases. In the first, the disease progression is inducing immune response. Later stages will be associated with the higher gene expression levels. In the second scenario, the disease is caused by overexpression of a gene. Later stages will be also associated with the higher expression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is typical for observational studies. But I just want to mention that a special care should be taken during interpretation of the results.&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;If we are not following our individuals, we cannot distinguish correlation and avoidance.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Let's say the disease is lethal in certain cases; the survival is negatively correlated with the disease stage. Now imagine there is a gene, which causes severe symptoms when highly expressed. On the later stages you will only observe those patients in which the gene was not highly expressed. From that you would conclude that gene expression is decreasing with the disease progression. In reality this gene is very important and causal, you just do not have patients which are alive to see this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Abraham_Wald&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wald's studies of aircrafts&lt;/a&gt; (&lt;a href=&quot;https://en.wikipedia.org/wiki/Survivorship_bias&quot; rel=&quot;nofollow noreferrer&quot;&gt;Survival bias&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Researchers from the Center for Naval Analyses had conducted a study&#xA;  of the damage done to aircraft that had returned from missions, and&#xA;  had recommended that armor be added to the areas that showed the most&#xA;  damage.&#xA;  Wald proposed that the Navy instead reinforce the areas where the&#xA;  returning aircraft were unscathed, since those were the areas that, if&#xA;  hit, would cause the plane to be lost.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I think the second point is crucial, and can and will lead to false conclusions. I do not see immediately how to avoid this bias if you study different individuals.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore I suggest that the same individuals are followed for a long time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are different approaches you can use later. For example you can have two-stage procedure:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Identify genes which are differentially expressed between healthy (H) and sick (A, B or C).&lt;/li&gt;&#xA;&lt;li&gt;Build a linear model of disease stage stage ~ gene1 + gene2 + ..., using genes identified at step 1.&lt;/li&gt;&#xA;&lt;li&gt;Similarly build a linear model of survival as a function gene expression.&lt;/li&gt;&#xA;&lt;li&gt;It is possible you can use logistic regression to infer probability of transition from stage B to stage C.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;This is not the only possible approach of course. As you suggested, Markov model is also applicable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While following the same individuals it is possible to use continuous-time Markov model for such a study. In this case discrete or continuous measurements are recorded at certain time points, and the model parameters are inferred using maximum likelihood.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not an expert in this field, but I think &lt;a href=&quot;https://dx.doi.org/10.18637/jss.v038.i08&quot; rel=&quot;nofollow noreferrer&quot;&gt;this paper&lt;/a&gt; describing the &lt;a href=&quot;https://cran.r-project.org/web/packages/msm/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;msm&lt;/a&gt; package for R will be useful. It also supports hidden Markov models.&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-05-19T11:58:27.690" LastActivityDate="2017-05-19T11:58:27.690" CommentCount="3" />
  <row Id="81" PostTypeId="1" AcceptedAnswerId="82" CreationDate="2017-05-17T16:26:16.783" Score="12" ViewCount="258" Body="&lt;p&gt;I have a FASTA file with 100+ sequences like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;Sequence1&#xA;GTGCCTATTGCTACTAAAA ...&#xA;&amp;gt;Sequence2&#xA;GCAATGCAAGGAAGTGATGGCGGAAATAGCGTTA&#xA;......&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I also have a text file like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Sequence1 40&#xA;Sequence2 30&#xA;......&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would like to simulate next-generation paired-end reads for all the sequences in my FASTA file. For &lt;code&gt;Sequence1&lt;/code&gt;, I would like to simulate at 40x coverage. For &lt;code&gt;Sequence2&lt;/code&gt;, I would like to simulate at 30x coverage. In other words, I want to control my sequence coverage for each sequence in my simulation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What is the simplest way to do that? Any software I should use? Bioconductor?&lt;/p&gt;&#xA;" OwnerUserId="174" LastEditorUserId="298" LastEditDate="2017-07-05T08:32:11.957" LastActivityDate="2017-07-05T08:32:11.957" Title="How to simulate NGS reads, controlling sequence coverage?" Tags="&lt;fasta&gt;&lt;ngs&gt;&lt;simulated-data&gt;" AnswerCount="9" CommentCount="4" FavoriteCount="1" />
  <row Id="82" PostTypeId="2" ParentId="81" CreationDate="2017-05-17T16:36:36.997" Score="2" Body="&lt;p&gt;I am not aware of any software that can do this directly, but I would split the fasta file into one sequence per file, loop over them in BASH and invoke ART the sequence simulator (or another) on each sequence.&lt;/p&gt;&#xA;" OwnerUserId="31" LastEditorUserId="298" LastEditDate="2017-05-30T06:23:51.133" LastActivityDate="2017-05-30T06:23:51.133" CommentCount="5" />
  <row Id="83" PostTypeId="1" CreationDate="2017-05-17T16:52:03.917" Score="7" ViewCount="111" Body="&lt;p&gt;This question pertains to iCLIP, but it could just as easily be ChIP-seq or ATAC-seq or mutation frequencies. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have iCLIP read counts across the transcriptome and I wish to know if the signals are correlated - that is, where one of them is high, is the other likely to be high.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Often when dealing with such data (e.g. iCLIP data) we know that the data is generally sparse - that is at most positions both signals are zero and this is correct, and also zero-inflated - that is many bases that &quot;should&quot; have a signal are missing that data. So just calculating the Spearman's correlation is likely to give an artificially low value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What might be a way to asses the association? I should add that the aim is the assess association of binding patterns within genes, rather than (or as well as) between genes.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Things I have thought of:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Apply some sort of smoothing to the data (eg a rolling mean). Remove any bases with 0 in both samples. Compute the spearmans. &lt;/li&gt;&#xA;&lt;li&gt;Calculate the average pairwise distance between every read in sample one and every read in sample two. Compare this to data where the reads have been randomised within genes. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In the first case removing all bases with 0 in both samples seems wrong. But if 99.99% of all bases have zero in both samples, then this seems like its necessary for Spearman. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the second case, the result seems like it would be non-intuitive to interpret. And also calculating this would be massively computationally intensive. &lt;/p&gt;&#xA;" OwnerUserId="235" LastEditorUserId="235" LastEditDate="2017-05-18T08:41:44.793" LastActivityDate="2017-07-26T16:30:47.080" Title="How to correlate two zero inflated bedgraph-like signals?" Tags="&lt;iclip&gt;&lt;statistics&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="3" />
  <row Id="84" PostTypeId="2" ParentId="45" CreationDate="2017-05-17T17:33:19.747" Score="11" Body="&lt;p&gt;Statistics for nanopore reads are tricky because of the huge range of read lengths that can be present in a single run. I have found that the best way to display lengths is by using a log scale on both the x axis (length) and the y axis (sequenced bases, or counts, depending on preference).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have written my own scripts for doing this: one for generating the read lengths, and another for plotting the length distribution in various ways. The script that generates read lengths also spits out basic length summary statistics to standard error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ ~/scripts/fastx-length.pl &amp;gt; lengths_mtDNA_called.txt&#xA;Total sequences: 2110&#xA;Total length: 5.106649 Mb&#xA;Longest sequence: 107.414 kb&#xA;Shortest sequence: 219 b&#xA;Mean Length: 2.42 kb&#xA;Median Length: 1.504 kb&#xA;N50: 336 sequences; L50: 3.644 kb&#xA;N90: 1359 sequences; L90: 1.103 kb&#xA;&#xA;$ ~/scripts/length_plot.r lengths_mtDNA_called.txt&#xA;lengths_mtDNA_called.txt ... done&#xA;Number of sequences: 2110 &#xA;Length quantiles:&#xA;      0%      10%      20%      30%      40%      50%      60%      70% &#xA;   219.0    506.9    724.4    953.0   1196.2   1503.0   1859.2   2347.3 &#xA;     80%      90%     100% &#xA;  3128.2   4804.7 107414.0 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here are a couple of of the produced graphs:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/CnECX.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/CnECX.png&quot; alt=&quot;Digital electrophoresis plot&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/el6d3.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/el6d3.png&quot; alt=&quot;Read length distribution plot&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The scripts to generate these can be found here:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/gringer/bioinfscripts/blob/master/fastx-length.pl&quot; rel=&quot;noreferrer&quot;&gt;https://github.com/gringer/bioinfscripts/blob/master/fastx-length.pl&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/gringer/bioinfscripts/blob/master/length_plot.r&quot; rel=&quot;noreferrer&quot;&gt;https://github.com/gringer/bioinfscripts/blob/master/length_plot.r&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-05-17T17:33:19.747" CommentCount="0" />
  <row Id="85" PostTypeId="2" ParentId="81" CreationDate="2017-05-17T17:55:24.600" Score="2" Body="&lt;p&gt;If you're ok with some randomness you can generate reads from your sequence file using a Poisson random variable.  You'll need to do some math to figure out what value of lambda to use in order for the expected coverage at each base pair in your read to match what you set in your text file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example you have a sequence S of length 1,000, a read length of 50, and an insert size of 100.  For each base b in S generate a Poisson random variable p.  You will then generate p reads from base b to b+50.  Then, generate the paired read starting at b+50+100.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again, you would have to play with it to figure out what lambda to use but this would give you basically what you want, as long as you're ok with not having exactly the coverage you're targeting for each read.&lt;/p&gt;&#xA;" OwnerUserId="59" LastActivityDate="2017-05-17T17:55:24.600" CommentCount="3" />
  <row Id="86" PostTypeId="2" ParentId="83" CreationDate="2017-05-17T18:04:16.737" Score="3" Body="&lt;p&gt;Honestly I'd just use &lt;code&gt;multiBigwigSummary&lt;/code&gt; and then &lt;code&gt;plotCorrelation&lt;/code&gt; from deepTools for this, but I'm a bit biased. There, the idea would be to consider each gene as a unit (you could instead use bins, but I don't think that would as nicely do what you want), namely by giving the tools a BED or GTF file input. It would then calculate the average signal in each gene/transcript and you could do your spearman's correlation. Features with 0 in all samples could optionally be removed (&lt;code&gt;plotCorrelation --skipZeros&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While you certainly could go the whole 9 yards and use per-base comparisons, that seems a bit overkill and I suspect that it won't really yield appreciably more information (especially when one considers the additional time overhead).&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-17T18:04:16.737" CommentCount="2" />
  <row Id="87" PostTypeId="2" ParentId="83" CreationDate="2017-05-17T18:08:38.830" Score="3" Body="&lt;p&gt;Rather than working on the base level, you could probably work on say gene level counts. &lt;a href=&quot;https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kendall's tau&lt;/a&gt;, an ordinal association metric, can then be used as an appropriate correlation measure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If $X$ and $Y$ are your iCLIP replicates, $i$ represents gene index and $(x_i, y_i)$ represents the number of RBP binding sites in $X$ and $Y$ respectively for the $i^{th}$ gene, Kendall's tau is defined as :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\frac{\text{#(concordant pairs)} - \text{#(discordant pairs)}}{n(n-1)/2}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where any two pairs $(x_i, y_i)$ and $(x_j, y_j)$ are concordant if:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$x_i &amp;gt; x_j$ AND $y_i &amp;gt; y_j$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;OR&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$x_i &amp;lt; x_j$ AND $y_i &amp;lt; y_j$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Correspondingly they are discordant if:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$x_i &amp;lt; x_j$ AND $y_i &amp;gt; y_j$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;OR&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$x_i &amp;gt; x_j$ AND $y_i &amp;lt; y_j$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="161" LastEditorUserId="29" LastEditDate="2017-07-26T16:30:47.080" LastActivityDate="2017-07-26T16:30:47.080" CommentCount="5" />
  <row Id="88" PostTypeId="2" ParentId="27" CreationDate="2017-05-17T18:56:13.670" Score="2" Body="&lt;p&gt;The &quot;right&quot; solution would be realignment, but that's expensive and most of us would not go that route. My preferred solution would be to convert the bed file, as opposed to the bam. Here's why:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Reheadering the bam means that you may have reads aligned to contigs without a corresponding entry in UCSC (see &lt;a href=&quot;https://github.com/dpryan79/ChromosomeMappings/blob/master/GRCh37_NCBI2UCSC.txt&quot; rel=&quot;nofollow noreferrer&quot;&gt;Devon's list for the mappings&lt;/a&gt;). This is a problem because:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Some of those reads would likely have been mapped elsewhere if a reference without those contigs was used. &lt;/li&gt;&#xA;&lt;li&gt;I'm not even sure what happens to those reads after reheadering - I guess they would need to be marked as unmapped? Lots of potential for screwiness there.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;2) It seems cleaner to convert the bed file from UCSC-&gt;NCBI, where you are guaranteed that every entry has a &quot;home&quot;. Then, after you pull your info from the bam, you can always convert chromosome names back if you need to.&lt;/p&gt;&#xA;" OwnerUserId="74" LastActivityDate="2017-05-17T18:56:13.670" CommentCount="2" />
  <row Id="89" PostTypeId="1" CreationDate="2017-05-17T19:20:51.663" Score="4" ViewCount="41" Body="&lt;p&gt;I have SNP data from several cultivars of rice which I have used to produce alignments, but I don't think that the usual models and algorithms used for generating phylogenetic trees are appropriate, because these cultivars are not the result of speciation events and have been interbred in their histories. How can I best calculate and visualize their degree of relatedness?&lt;/p&gt;&#xA;" OwnerUserId="83" LastEditorUserId="131" LastEditDate="2017-05-18T00:00:52.860" LastActivityDate="2017-05-18T04:30:57.133" Title="How does one construct a cladogram of intraspecies relationships?" Tags="&lt;phylogeny&gt;&lt;cladistics&gt;&lt;snp&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="1" />
  <row Id="90" PostTypeId="2" ParentId="83" CreationDate="2017-05-17T20:18:09.070" Score="3" Body="&lt;p&gt;It depends whether you want to treat the peak intensities as binary (comparing presence/absence of peaks in the sets) or continuous (comparing the relative magnitudes of the peaks).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Binary&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For starting out, a simple binary comparison may be appropriate. You can use a &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0096303&quot; rel=&quot;nofollow noreferrer&quot;&gt;peak caller&lt;/a&gt; of your choosing to identify peaks in each sample according to your desired criteria. Then you can use a similarity metric such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot; rel=&quot;nofollow noreferrer&quot;&gt;Jaccard index&lt;/a&gt; to quantify the level of agreement among the peaks in the two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One potential obstacle is that defining the boundaries of your peaks won't be totally straightforward. For example, a peak in one sample might have 2 overlapping peaks in the other sample, one on each end. A rough solution for this is to divide the genome into bins (maybe around 100-1000 bp, depending on your desired resolution). You can treat a peak as present in a bin if more than half of the peak lies in the bin. That way, bins in one sample can be directly compared to the corresponding bins in the other sample. Obviously, this isn't the only way to do this; other appropriate methods exist too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Continuous&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to treat the peak intensities as continuous, you could apply a similar binning method, taking the &quot;score&quot; of a bin to be the average peak intensity at positions within that bin. You could then throw away all bins with no peaks or only low-intensity peaks throughout the genome. Then you could compute the Spearman's correlation for the remaining bins. I'm guessing it will be harder to find a strong correlation for continuous intensities, because of the amount of experimental variability that is inherently present.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If, after following these steps, the Spearman's correlation is still &quot;artificially low&quot; as you suggested, then this is likely a problem with the underlying data, not the overall analysis; maybe your two datasets actually don't agree that well.&lt;/p&gt;&#xA;" OwnerUserId="177" LastActivityDate="2017-05-17T20:18:09.070" CommentCount="4" />
  <row Id="91" PostTypeId="1" AcceptedAnswerId="133" CreationDate="2017-05-18T00:14:58.347" Score="10" ViewCount="266" Body="&lt;p&gt;I have a FASTA file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; Sequence_1&#xA;GCAATGCAAGGAAGTGATGGCGGAAATAGCGTTAGATGTATGTGTAGCGGTCCC...&#xA;&amp;gt; Sequence_2&#xA;GCAATGCAAGGAAGTGATGGCGGAAATAGCGTTAGATGTATGTGTAGCGGTCCC....&#xA;....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to generate a BED file for each sequence like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Sequence_1 0 1500&#xA;Sequence_2 0 1700&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The BED regions will simply be the size of the sequences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; I did that before with a one-line command. I don't remember what that is, it was on Biostars. I can't find the post now. What's the simplest way to do the conversion?&lt;/p&gt;&#xA;" OwnerUserId="174" LastEditorUserId="96" LastEditDate="2017-06-08T07:04:56.403" LastActivityDate="2017-06-08T07:04:56.403" Title="How to convert FASTA to BED" Tags="&lt;fasta&gt;&lt;bed&gt;&lt;file-formats&gt;&lt;format-conversion&gt;" AnswerCount="7" CommentCount="5" FavoriteCount="1" />
  <row Id="92" PostTypeId="1" AcceptedAnswerId="93" CreationDate="2017-05-18T00:28:24.290" Score="12" ViewCount="87" Body="&lt;p&gt;I am working with a set of (bulk) RNA-Seq data collected across multiple runs, run at different times of the year. I have normalized my data using library size / quantile / RUV normalization, and would like to check (quantitatively and/or qualitatively) whether or not normalization has succeeded in removing the batch effects. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is important to note that by &quot;normalization has succeeded&quot;, I simply mean that unwanted variation has been removed - further analysis is required to ensure that biological variation has not been removed. What are some plots / statistical tests / software packages which provide a first-step QC for normalization?&lt;/p&gt;&#xA;" OwnerUserId="163" LastActivityDate="2017-05-22T19:07:50.327" Title="Confirm success or failure RNA-Seq normalization" Tags="&lt;rna-seq&gt;&lt;normalization&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="2" />
  <row Id="93" PostTypeId="2" ParentId="92" CreationDate="2017-05-18T00:49:44.233" Score="12" Body="&lt;p&gt;You should use box plots and PCA plot. Let's take a look at the RUV paper:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4404308/&quot; rel=&quot;noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4404308/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Before normalization and after UQ normalization&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/aZBua.jpg&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/aZBua.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Libraries do not cluster as expected according to treatment. ... for UQ-normalized counts. UQ normalization does not lead to better clustering of the samples...&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Before normalization, the medians in the box-plot obviously look very different among replicates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After UQ normalization, the medians look closer but Trt.11 look like an outlier. Furthermore, the treatments aren't clustered on the PCA plot. Since they are replicates, you'd like them be close on the plot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;After RUV normalization&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/m4tYK.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/m4tYK.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;... RUVg shrinks the expression measures for Library 11 towards the median across libraries, suggesting robustness against outliers. ... Libraries cluster as expected by treatment. ...&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The RUV has made the distribution more robust and the samples closer on the PCA plot. However, it's still not perfect as one of the treatments is not close to the other two on the first PC.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The vignettes for Bioconductor &lt;code&gt;RUVSeq&lt;/code&gt; describes the two functions: &lt;code&gt;plotRLE&lt;/code&gt; and &lt;code&gt;plotPCA&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="174" LastEditorUserId="174" LastEditDate="2017-05-18T00:55:59.730" LastActivityDate="2017-05-18T00:55:59.730" CommentCount="1" />
  <row Id="94" PostTypeId="2" ParentId="91" CreationDate="2017-05-18T01:01:14.320" Score="5" Body="&lt;p&gt;You could adapt this &lt;a href=&quot;http://www.danielecook.com/generate-fasta-sequence-lengths/&quot; rel=&quot;noreferrer&quot;&gt;awk one-liner&lt;/a&gt;. Note that it assumes that sequence IDs are not longer than 100 characters and that there is no description following the sequence ID on the header line.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat myseqs.fasta | awk '$0 ~ &quot;&amp;gt;&quot; {print c; c=0;printf substr($0,2,100) &quot;\t0\t&quot;; } $0 !~ &quot;&amp;gt;&quot; {c+=length($0);} END { print c; }'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Otherwise, any Bio* library (Perl, Python, Ruby) provides FASTA format parsers which will extract sequence IDs and lengths.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd point out that whilst this resembles BED it is not, strictly-speaking, since BED maps to coordinates on a chromosome or some longer sequence object.&lt;/p&gt;&#xA;" OwnerUserId="150" LastEditorUserId="150" LastEditDate="2017-05-18T01:07:42.050" LastActivityDate="2017-05-18T01:07:42.050" CommentCount="0" />
  <row Id="95" PostTypeId="2" ParentId="91" CreationDate="2017-05-18T01:04:48.900" Score="0" Body="&lt;p&gt;If the FASTA sequences are all on a single line, then the following perl one-liner should work:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat myseqs.fasta | perl -ne 'if(/^&amp;gt;([^ ]+)/){print $1} else {print &quot; 0 &quot;,length,&quot;\n&quot;}'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Explanation:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If the line begins with a '&gt;', then print everything up to the first space (but don't put a line break at the end)&lt;/li&gt;&#xA;&lt;li&gt;Otherwise, print &quot; 0 &quot;, followed by the length of the line, followed by a line break&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-05-18T01:04:48.900" CommentCount="0" />
  <row Id="96" PostTypeId="2" ParentId="91" CreationDate="2017-05-18T01:08:32.233" Score="4" Body="&lt;p&gt;Inspired by &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/45/read-length-distribution-from-fasta-file/84#84&quot;&gt;this answer&lt;/a&gt; to a related question on read length distributions, you could do this with Biopython:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from Bio.SeqIO import parse&#xA;with open(&quot;regions.bed&quot;, &quot;w&quot;) as bed:&#xA;    for record in parse(&quot;regions.fasta&quot;, &quot;fasta&quot;):&#xA;        print(record.id, 0, len(record.seq), sep=&quot;\t&quot;, file=bed)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="163" LastEditorUserId="132" LastEditDate="2017-05-18T13:55:20.790" LastActivityDate="2017-05-18T13:55:20.790" CommentCount="10" />
  <row Id="97" PostTypeId="1" AcceptedAnswerId="172" CreationDate="2017-05-18T03:11:37.330" Score="9" ViewCount="156" Body="&lt;p&gt;I have a DNA sample which I know doesn't &lt;em&gt;quite&lt;/em&gt; match my reference genome - my culture comes from a subpopulation which has undergone significant mutation since the reference was created.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From visual inspection with IGV, a significant number of both SNPs and SVs appear to be present, but an assembly built entirely from my own sequencing data is not high enough quality for my purposes. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I modify this reference genome to match my sample with new sequencing data (preferably with Oxford Nanopore Technologies long reads, but I can also use these to scaffold short reads if necessary), taking advantage of my knowledge that the existing reference is &lt;em&gt;mostly&lt;/em&gt; very good, without having to access the reads which were originally used to construct the reference genome?&lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="163" LastEditDate="2017-05-18T06:31:08.643" LastActivityDate="2017-08-04T19:24:34.243" Title="Improve a reference genome with sequencing data" Tags="&lt;genome&gt;&lt;dna&gt;&lt;assembly&gt;" AnswerCount="5" CommentCount="5" />
  <row Id="98" PostTypeId="1" AcceptedAnswerId="99" CreationDate="2017-05-18T03:14:29.460" Score="3" ViewCount="90" Body="&lt;p&gt;After DNA sequencing, I generated a sam file through alignment of a fastq file. &#xA;Before using well known variant calling programs (eg. Annovar etc.), I want to pick some reads and know what kinds of mutations are there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any quick way of doing it?&lt;/p&gt;&#xA;" OwnerUserId="274" LastActivityDate="2017-05-21T17:47:47.147" Title="How to quickly determine mutations in a read of a sam file?" Tags="&lt;variant-calling&gt;" AnswerCount="3" CommentCount="1" />
  <row Id="99" PostTypeId="2" ParentId="98" CreationDate="2017-05-18T03:24:19.127" Score="5" Body="&lt;p&gt;For qualitative analysis, you're probably better off using something less granular like &lt;a href=&quot;http://software.broadinstitute.org/software/igv/&quot; rel=&quot;noreferrer&quot;&gt;IGV&lt;/a&gt; or &lt;a href=&quot;http://bioviz.org/igb/overview.html&quot; rel=&quot;noreferrer&quot;&gt;IGB&lt;/a&gt;. However, if you really want to look at a couple of reads:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're willing to ignore sequencing errors, you can inspect the CIGAR string or the MD tag, both of which give information on the alignment of a single read.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The CIGAR string gives details on insertions, deletions, clippings, matches and mismatches. From &lt;a href=&quot;http://genome.sph.umich.edu/wiki/SAM&quot; rel=&quot;noreferrer&quot;&gt;Genome Analysis Wiki&lt;/a&gt;,&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The sequence being aligned to a reference may have additional bases that are not in the reference or may be missing bases that are in the reference. The CIGAR string is a sequence of of base lengths and the associated operation. They are used to indicate things like which bases align (either a match/mismatch) with the reference, are deleted from the reference, and are insertions that are not in the reference. For example:&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;RefPos:     1  2  3  4  5  6  7     8  9 10 11 12 13 14 15 16 17 18 19&#xA;Reference:  C  C  A  T  A  C  T     G  A  A  C  T  G  A  C  T  A  A  C&#xA;Read:                   A  C  T  A  G  A  A     T  G  G  C  T&#xA;With the alignment above, you get:&#xA;POS: 5&#xA;CIGAR: 3M1I3M1D5M&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Most common usage of the CIGAR string uses M (match/mismatch), I (insertion), D (deletion), S (soft clipping), and H (hard clipping). Note that = (match) and X (mismatch) are available as alternatives to the less informative M, but they are less widely used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The MD tag gives specific details on mismatches and deletions. From the &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMtags.pdf&quot; rel=&quot;noreferrer&quot;&gt;SAMtools tags specification&lt;/a&gt;,&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The MD field aims to achieve SNP/indel calling without looking at the reference. For example, a string ‘10A5^AC6’ means from the leftmost reference base in the alignment, there are 10 matches followed by an A on the reference which is different from the aligned read base; the next 5 reference bases are matches followed by a 2bp deletion from the reference; the deleted sequence is AC; the last 6 bases are matches. The MD field ought to match the CIGAR string.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Note that neither of these will give you any idea of structural variants in short reads, and neither will be particularly readable (or helpful, due to the higher error rate) in long reads.&lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="163" LastEditDate="2017-05-18T03:33:12.387" LastActivityDate="2017-05-18T03:33:12.387" CommentCount="0" />
  <row Id="100" PostTypeId="2" ParentId="89" CreationDate="2017-05-18T04:30:57.133" Score="3" Body="&lt;p&gt;Even with inbreeding and other genetic phenomena that might mask actual evolution of these cultivars, any phylogenetic methodology would be capable of determining relationships accurately. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Try creating a Neighbour Joining tree with &lt;a href=&quot;http://www.megasoftware.net/download_form&quot; rel=&quot;nofollow noreferrer&quot;&gt;MEGA&lt;/a&gt;, which is one of the simplest methods available. This should give you enough to check the relationships of the cultivars.&lt;/p&gt;&#xA;" OwnerUserId="280" LastActivityDate="2017-05-18T04:30:57.133" CommentCount="0" />
  <row Id="101" PostTypeId="2" ParentId="97" CreationDate="2017-05-18T05:29:39.427" Score="2" Body="&lt;p&gt;You can use &lt;a href=&quot;https://github.com/jts/nanopolish&quot; rel=&quot;nofollow noreferrer&quot;&gt;nanopolish&lt;/a&gt; using the illumina reads. Also have a look at &lt;a href=&quot;https://github.com/broadinstitute/pilon/wiki&quot; rel=&quot;nofollow noreferrer&quot;&gt;pilon&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="282" LastEditorUserId="77" LastEditDate="2017-05-18T13:58:52.057" LastActivityDate="2017-05-18T13:58:52.057" CommentCount="0" />
  <row Id="102" PostTypeId="2" ParentId="97" CreationDate="2017-05-18T05:38:32.473" Score="3" Body="&lt;p&gt;If it is a short-read draft assembly and you have long-reads (ONT or Pacbio) run &lt;a href=&quot;https://github.com/warrenlr/LINKS&quot; rel=&quot;nofollow noreferrer&quot;&gt;links&lt;/a&gt; to scaffold the genome and then run &lt;a href=&quot;https://github.com/broadinstitute/pilon&quot; rel=&quot;nofollow noreferrer&quot;&gt;Pilon&lt;/a&gt; iteratively to try to polish and fill gaps using the short-reads. &lt;/p&gt;&#xA;" OwnerUserId="223" LastActivityDate="2017-05-18T05:38:32.473" CommentCount="0" />
  <row Id="103" PostTypeId="1" AcceptedAnswerId="104" CreationDate="2017-05-18T07:17:15.830" Score="1" ViewCount="117" Body="&lt;p&gt;&lt;em&gt;Bootstrap analysis&lt;/em&gt;, &lt;em&gt;bootstrapping&lt;/em&gt; etc are quite common jargons of bioinformatics and phylogenetics. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, it is not very much clear to us, what exactly being meant by &lt;em&gt;&quot;a boot's straps&quot;.&lt;/em&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does it means a &lt;em&gt;&quot;comparison&quot;&lt;/em&gt;? (Such as we hold the two straps of a boot (shoe) in &quot;two hands&quot;; may be an analogy to comparing 2 things (say left-'hand' side and right-'hand' side of an algebraic proof-task)) or it is something about &lt;em&gt;sequence alignment&lt;/em&gt; (since we 'tie' a knot in boot (shoe) so that the 2 ends of the strap remain attached to each-other). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of my classmates even assumed it may be a process during computer booting (but I could not agree with that). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've searched internet, but could not found anything very helpful. This &lt;a href=&quot;https://en.wikipedia.org/wiki/Bootstrapping#Etymology&quot; rel=&quot;nofollow noreferrer&quot;&gt;wikipedia&lt;/a&gt; page (&lt;a href=&quot;https://en.wikipedia.org/w/index.php?title=Bootstrapping&amp;amp;oldid=776992221#Etymology&quot; rel=&quot;nofollow noreferrer&quot;&gt;permalink&lt;/a&gt;) about bootstrapping, tells, &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;sup&gt; Tall boots may have a tab, loop or handle at the top known as a bootstrap, allowing one to use fingers or a boot hook tool to help pulling the boots on. The saying &quot;to pull oneself up by one's bootstraps&quot; was already in use during the 19th century as an example of an impossible task. The idiom dates at least to 1834, when it appeared in the Workingman's Advocate: &quot;It is conjectured that Mr. Murphee will now be enabled to hand himself over the Cumberland river or a barn yard fence by the straps of his boots.&quot; In 1860 it appeared in a comment on philosophy of mind: &lt;strong&gt;&lt;em&gt;&quot;The attempt of the mind to analyze itself an effort analogous to one who would lift himself by his own bootstraps&quot;&lt;/em&gt;&lt;/strong&gt;.  &lt;strong&gt;Bootstrap as a metaphor, meaning to better oneself by one's own unaided efforts&lt;/strong&gt;, was in use in 1922. This metaphor spawned additional metaphors for a series of self-sustaining processes that proceed without external help.&lt;/sup&gt;  &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;sup&gt;The term is sometimes attributed to a story in Rudolf Erich Raspe's The Surprising Adventures of Baron Munchausen, but in that story Baron Munchausen pulls himself (and his horse) out of a swamp by his hair (specifically, his pigtail), not by his bootstraps – and no explicit reference to bootstraps has been found elsewhere in the various versions of the Munchausen tales.&lt;/sup&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;(In the quotation, highlighted portion seemed to tried to tell a meaning), but it wasn't also very much helpful. Could not correlate with bioinformatics. (Btw the same wikipedia article mentions computer booting). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Wikipedia article about &lt;a href=&quot;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#History&quot; rel=&quot;nofollow noreferrer&quot;&gt;Bootstrapping (statistics)&lt;/a&gt; (&lt;a href=&quot;https://en.wikipedia.org/w/index.php?title=Bootstrapping_(statistics)&amp;amp;oldid=780324952#History&quot; rel=&quot;nofollow noreferrer&quot;&gt;permalink&lt;/a&gt;) tells: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;sup&gt;The bootstrap was published by Bradley Efron in &quot;Bootstrap methods: another look at the jackknife&quot; (1979), inspired by earlier work on the jackknife.Improved estimates of the variance were developed later. A Bayesian extension was developed in 1981. The bias-corrected and accelerated (BCa) bootstrap was developed by Efron in 1987, and the ABC procedure in 1992.&lt;/sup&gt; &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Still that is not very helpful. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, the textbook, &lt;em&gt;Introduction to Bioinformatics/ Arthur M. Lesk/ 3rd Edition&lt;/em&gt;/ Oxford / (Low Price edition) ; in its &lt;em&gt;chapter 5&lt;/em&gt; (Alignments and phylogenetic trees), section The problem of varying rates of evolution -&gt; &lt;em&gt;computational consideration&lt;/em&gt; (page 296); a clearer definition has been given: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;ol start=&quot;3&quot;&gt;&#xA;  &lt;li&gt;Formal statistical tests, involving rerunning the calculation on subsets of the original data, are known as &lt;strong&gt;jackknifing&lt;/strong&gt; and &lt;strong&gt;bootstrapping&lt;/strong&gt;.     &#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;strong&gt;jackknifing&lt;/strong&gt; is calculation with data sets sampled randomly from the original data. For phylogeny calculations from from multiple sequence alignments, select different subsets of the position in the alignment, and rerun the calculation. Finding that each subset gives the same phylogenetic tree lends it credibility. If each subset gives a different tree, none of them are trustworthy. &lt;/li&gt;&#xA;  &lt;li&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt; is  similar to jackknifing except that the position chosen at random may include multiple copies of the same position, to form data sets of the same size as original, to preserve statistical properties of data sampling. &lt;/li&gt;&#xA;  &lt;/ul&gt;&lt;/li&gt;&#xA;  &lt;/ol&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;More clear definition of &quot;Boot-strapping&quot;; but it does not explain, where the relation exists with the boot (shoe) and its straps. &lt;/p&gt;&#xA;" OwnerUserId="286" LastActivityDate="2017-05-18T19:59:53.980" Title="Bootstrap analysis - why it is called bootstrap?" Tags="&lt;terminology&gt;&lt;bootstrap-analysis&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="1" ClosedDate="2017-05-19T01:54:22.530" />
  <row Id="104" PostTypeId="2" ParentId="103" CreationDate="2017-05-18T07:30:52.260" Score="8" Body="&lt;p&gt;The method itself has nothing really to do with boots or straps, just as the jack-knife method also has nothing to do with knives. In the case of bootstrapping, the goal is to determine the accuracy of an estimate from random subsets of a population. Normally estimating something like the variance of the mean requires multiple independent samples, but bootstrapping allows you to perform estimates from a single population. This is essentially allowing the estimate to &quot;&lt;a href=&quot;https://en.wiktionary.org/wiki/pull_oneself_up_by_one%27s_bootstraps&quot; rel=&quot;nofollow noreferrer&quot;&gt;pull itself up by its bootstraps&lt;/a&gt;&quot;, which is the only reasonable source for the term. See also the more general wikipedia article on &lt;a href=&quot;https://en.wikipedia.org/wiki/Bootstrapping&quot; rel=&quot;nofollow noreferrer&quot;&gt;bootstrapping outside of statistics&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="43" LastEditDate="2017-05-18T19:59:53.980" LastActivityDate="2017-05-18T19:59:53.980" CommentCount="4" />
  <row Id="105" PostTypeId="2" ParentId="103" CreationDate="2017-05-18T07:43:12.813" Score="6" Body="&lt;p&gt;In general, branch support values indicate how much of the total data support that branch in a proposed tree. But what do you do if you used all of the available data to infer the tree in the first place? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;With Bootstrap supports, you &lt;em&gt;generate&lt;/em&gt; new alignments (almost) &lt;em&gt;ex nihilo&lt;/em&gt;. You calculate a trees support values, using its own data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is the connection to &quot;pulling yourself up by your bootstraps&quot;: you don't &quot;pull&quot; on additionaly collected data (a rope or something in the metaphor), but rather on the very thing that your observation is based on (your boots).&lt;/p&gt;&#xA;" OwnerUserId="204" LastActivityDate="2017-05-18T07:43:12.813" CommentCount="0" />
  <row Id="106" PostTypeId="2" ParentId="91" CreationDate="2017-05-18T08:04:25.233" Score="4" Body="&lt;p&gt;Here is an approach with &lt;a href=&quot;http://biopython.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;BioPython&lt;/code&gt;&lt;/a&gt;. The &lt;code&gt;with&lt;/code&gt; statement ensures both the input and output file handles are closed and a &lt;a href=&quot;https://stackoverflow.com/questions/20535342/lazy-evaluation-python&quot;&gt;&lt;em&gt;lazy&lt;/em&gt;&lt;/a&gt; approach is taken so that only a single fasta record is held in memory at a time, rather than reading the whole file into memory, which is a bad idea for large input files. The solution makes no assumptions about the sequence ID lengths or the number of lines that the sequences are spread across:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from Bio import SeqIO&#xA;&#xA;with open('sequences.fasta') as in_f, open('sequences.bed','w') as out_f:&#xA;    for record in SeqIO.parse(in_f, 'fasta'):&#xA;        out_f.write('{}\t0\t{}\n'.format(record.id, len(record)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="104" LastEditorUserId="-1" LastEditDate="2017-05-23T12:41:55.037" LastActivityDate="2017-05-18T08:56:58.783" CommentCount="0" />
  <row Id="107" PostTypeId="1" CreationDate="2017-05-18T08:05:10.337" Score="8" ViewCount="104" Body="&lt;p&gt;What are the best tools to design gRNAs in a high-throughput way for a CRISPR screen, e.g. targeting all protein-coding genes in a genome? I would like to take into account possible off-target effects, as well as to allow for flexibility in the PAM sequence, to have the possibility of using non-canonical PAMs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm aware of &lt;a href=&quot;https://www.bioconductor.org/packages/release/bioc/html/CRISPRseek.html&quot; rel=&quot;noreferrer&quot;&gt;CRISPRseek&lt;/a&gt;, but I'm afraid it might be quite slow to run for a genome-scale search.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2017-05-27T16:05:14.763" Title="Large-scale gRNA design for a CRISPR screen" Tags="&lt;crispr&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="1" />
  <row Id="108" PostTypeId="2" ParentId="91" CreationDate="2017-05-18T08:40:08.150" Score="5" Body="&lt;p&gt;It's good practice to have your &lt;code&gt;FASTA&lt;/code&gt; indexed, so you can leverage the &lt;code&gt;.fai&lt;/code&gt; you are likely to already have. If not, you can just generate the index with &lt;code&gt;samtools&lt;/code&gt; and use some &lt;code&gt;awk&lt;/code&gt; to make your &lt;code&gt;BED&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools faidx $fasta&#xA;awk 'BEGIN {FS=&quot;\t&quot;}; {print $1 FS &quot;0&quot; FS $2}' $fasta.fai &amp;gt; $fasta.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This will maintain tab separation but you can drop the &lt;code&gt;BEGIN&lt;/code&gt; statement to use spaces. The &lt;a href=&quot;https://genome.ucsc.edu/FAQ/FAQformat.html#format1&quot; rel=&quot;nofollow noreferrer&quot;&gt;BED spec&lt;/a&gt; only requires &quot;whitespace&quot; for the simple &lt;code&gt;BED&lt;/code&gt; format.&lt;/p&gt;&#xA;" OwnerUserId="215" LastEditorUserId="215" LastEditDate="2017-05-18T09:06:45.380" LastActivityDate="2017-05-18T09:06:45.380" CommentCount="0" />
  <row Id="109" PostTypeId="2" ParentId="81" CreationDate="2017-05-18T08:57:06.240" Score="4" Body="&lt;p&gt;The &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/polyester.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;polyester&lt;/a&gt; bioconductor package can do this. It says it simulates RNA-seq reads, but I don't know if that's really any different from other NGS reads. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It can use a range of error and bias models, or learn them from a dataset.&lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-05-18T08:57:06.240" CommentCount="1" />
  <row Id="110" PostTypeId="1" AcceptedAnswerId="121" CreationDate="2017-05-18T09:11:09.517" Score="5" ViewCount="72" Body="&lt;p&gt;I've executed the &lt;code&gt;BLAT&lt;/code&gt; aligner as a means to find alignments of translated DNA reads against a small protein database. I selected &lt;code&gt;BLAT&lt;/code&gt; as it provided the easiest means to access information on the actual alignment blocks (with the smallest amount of parsing). That is, the PSL has a comma delimited field that enumerates each of the starting positions of a query to a target.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, &lt;code&gt;BLAT&lt;/code&gt; outputs a significantly greater number of alignments than alternative tools. This is likely due to its identity percentage threshold defaulting to 25% when emulating a &lt;code&gt;blastx&lt;/code&gt; style query.&#xA;I can cope with this (apologies to my disks for the I/O), but for filtering purposes, the output &lt;code&gt;PSL&lt;/code&gt; format appears to leave me nothing but raw match counts to work with.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that the &lt;a href=&quot;http://genome.ucsc.edu/FAQ/FAQblat.html#blat4&quot; rel=&quot;nofollow noreferrer&quot;&gt;BLAT FAQ&lt;/a&gt; describes a Perl script for adding &quot;web-based identity and score calculations&quot;. I can actually get identity percentages and scores from this script, but I cannot tell what the score actually represents? Is it comparable to bit-score?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's probably another question in here for someone to discuss the pros and cons of match percentages, e-values and bitscores, but just so I can pretend that I am doing something more &quot;complicated&quot; than setting a match threshold, how can I recover something like an e-value, from a &lt;code&gt;PSL&lt;/code&gt; output file?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;To clarify&lt;/strong&gt; I'm attempting to reconstruct the read fragments that form a hit to a target, excluding/skipping gaps. BLA&lt;strong&gt;S&lt;/strong&gt;T's &lt;code&gt;outfmt 6&lt;/code&gt;, and alternatives such as Diamond's &lt;code&gt;M8&lt;/code&gt; format only give the number of gap opens on the alignment. One can't just use the &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; positions of the alignment as indices to slice the read DNA sequence, as this will include the nucleotides that were gapped by the alignment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;PSL&lt;/code&gt; appeared to be the only format I could find that gave the actual start positions (and lengths) of each of the pieces that formed a hit (that is, each gap forces a new block).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The regular BLA&lt;strong&gt;S&lt;/strong&gt;T output is human readable and is a nightmare to parse. The &lt;code&gt;XML&lt;/code&gt; format mentioned has a structure for each HSP, but an HSP can still include gaps:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  [...]&#xA;  &amp;lt;Hit_hsps&amp;gt;&#xA;    &amp;lt;Hsp&amp;gt;&#xA;      &amp;lt;Hsp_num&amp;gt;1&amp;lt;/Hsp_num&amp;gt;&#xA;      &amp;lt;Hsp_bit-score&amp;gt;51.6&amp;lt;/Hsp_bit-score&amp;gt;&#xA;      &amp;lt;Hsp_score&amp;gt;122&amp;lt;/Hsp_score&amp;gt;&#xA;      &amp;lt;Hsp_evalue&amp;gt;9.8e-06&amp;lt;/Hsp_evalue&amp;gt;&#xA;      &amp;lt;Hsp_query-from&amp;gt;2&amp;lt;/Hsp_query-from&amp;gt;&#xA;      &amp;lt;Hsp_query-to&amp;gt;98&amp;lt;/Hsp_query-to&amp;gt;&#xA;      &amp;lt;Hsp_hit-from&amp;gt;35&amp;lt;/Hsp_hit-from&amp;gt;&#xA;      &amp;lt;Hsp_hit-to&amp;gt;65&amp;lt;/Hsp_hit-to&amp;gt;&#xA;      &amp;lt;Hsp_query-frame&amp;gt;2&amp;lt;/Hsp_query-frame&amp;gt;&#xA;      &amp;lt;Hsp_hit-frame&amp;gt;0&amp;lt;/Hsp_hit-frame&amp;gt;&#xA;      &amp;lt;Hsp_identity&amp;gt;24&amp;lt;/Hsp_identity&amp;gt;&#xA;      &amp;lt;Hsp_positive&amp;gt;27&amp;lt;/Hsp_positive&amp;gt;&#xA;      &amp;lt;Hsp_gaps&amp;gt;1&amp;lt;/Hsp_gaps&amp;gt;&#xA;      &amp;lt;Hsp_align-len&amp;gt;32&amp;lt;/Hsp_align-len&amp;gt;&#xA;         &amp;lt;Hsp_qseq&amp;gt;ITAIGAGLQGPAGCEVIDAGGLLVMPGGIDTH&amp;lt;/Hsp_qseq&amp;gt;&#xA;         &amp;lt;Hsp_hseq&amp;gt;IAAVGTGLE-PAGAEIIDAGGLLVMPGGIDVH&amp;lt;/Hsp_hseq&amp;gt;&#xA;      &amp;lt;Hsp_midline&amp;gt;I A+G GL+ PAG E+IDAGGLLVMPGGID H&amp;lt;/Hsp_midline&amp;gt;&#xA;    &amp;lt;/Hsp&amp;gt;&#xA;  &amp;lt;/Hit_hsps&amp;gt;&#xA;&amp;lt;/Hit&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm trying to be lazy, I don't want to have to parse the &lt;code&gt;Hsp_qseq&lt;/code&gt; and &lt;code&gt;Hsp_hseq&lt;/code&gt; to determine exactly where my gaps are. I just want co-ordinates of where my read hits a protein. The &lt;code&gt;PSL&lt;/code&gt; format (below) tells me this (&lt;code&gt;tStarts&lt;/code&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;match   mis-    rep.    N's     Q gap   Q gap   T gap   T gap   strand  Q               Q       Q       Q       T               T       T       T       block   blockSizes      qStarts  tStarts&#xA;        match   match           count   bases   count   bases           name            size    start   end     name            size    start   end     count&#xA;---------------------------------------------------------------------------------------------------------------------------------------------------------------&#xA;[...]&#xA;21      4       0       0       1       8       1       24      ++      &amp;lt;PROTEIN&amp;gt;   662     321     354     &amp;lt;READ&amp;gt;      101     2       101     2       8,17,   321,337,        2,50,&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;PSL&lt;/code&gt; gives me what I want, except now I don't have information on its significance - which is what I was asking about.&lt;/p&gt;&#xA;" OwnerUserId="215" LastEditorUserId="131" LastEditDate="2017-05-18T14:38:40.857" LastActivityDate="2017-05-18T15:50:52.953" Title="Retrieve exact co-ordinates of where a read matches a sequence, from its alignment" Tags="&lt;alignment&gt;&lt;blat&gt;&lt;blast&gt;" AnswerCount="2" CommentCount="8" />
  <row Id="111" PostTypeId="2" ParentId="45" CreationDate="2017-05-18T09:23:17.877" Score="4" Body="&lt;p&gt;&lt;a href=&quot;https://github.com/lh3/bioawk&quot; rel=&quot;nofollow noreferrer&quot;&gt;bioawk&lt;/a&gt; could be reasonably efficient for this kind of task.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ bioawk -c fastx '{histo[length($seq)]++} END {for (l in histo) print l,histo[l]}' \&#xA;    | sort -n&#xA;0   33270&#xA;1   1542&#xA;2   1132&#xA;3   3397&#xA;4   8776&#xA;5   11884&#xA;6   12474&#xA;7   14341&#xA;8   13165&#xA;9   15467&#xA;10  21089&#xA;11  30469&#xA;12  45204&#xA;13  62311&#xA;14  88744&#xA;15  115767&#xA;16  140770&#xA;17  191810&#xA;18  313088&#xA;19  518111&#xA;20  1097867&#xA;21  4729715&#xA;22  6575557&#xA;23  2734062&#xA;24  1015476&#xA;25  493323&#xA;26  323827&#xA;27  164419&#xA;28  107120&#xA;29  72487&#xA;30  40120&#xA;31  24538&#xA;32  22295&#xA;33  13121&#xA;34  9382&#xA;35  4847&#xA;36  3858&#xA;37  3161&#xA;38  2852&#xA;39  2388&#xA;40  1639&#xA;41  961&#xA;42  686&#xA;43  377&#xA;44  199&#xA;45  114&#xA;46  78&#xA;47  59&#xA;48  50&#xA;49  52&#xA;50  48&#xA;51  42&#xA;52  39&#xA;53  28&#xA;54  49&#xA;55  59&#xA;56  55&#xA;57  51&#xA;58  55&#xA;59  43&#xA;60  52&#xA;61  56&#xA;62  48&#xA;63  67&#xA;64  95&#xA;65  488&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;-c fastx&lt;/code&gt; tells the program to parse the data as fastq or fasta. This gives access to the different parts of the records as &lt;code&gt;$name&lt;/code&gt;, &lt;code&gt;$seq&lt;/code&gt; (and &lt;code&gt;$qual&lt;/code&gt; in the case of fastq format) in the awk code (bioawk is based on awk, so you can use whatever language features you want from &lt;a href=&quot;http://www.grymoire.com/Unix/Awk.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;awk&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Between the single quotes come a series of &lt;code&gt;&amp;lt;condition&amp;gt; {&amp;lt;action&amp;gt;}&lt;/code&gt; blocks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first one has no &lt;code&gt;&amp;lt;condition&amp;gt;&lt;/code&gt; part, which mean it is executed for each record. Here, it updates the lengths counts in a table which I named &quot;histo&quot;. &lt;code&gt;length&lt;/code&gt; is a predefined function in awk.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the second block, the &lt;code&gt;END&lt;/code&gt; condition means we want it to be executed after all the input has been processed. The action part consists in looping over the recorded length values and print them together with the associated count.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output is piped to &lt;code&gt;sort -n&lt;/code&gt; in order to sort the results numerically.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On my workstation, the above code took 20 seconds to execute for a 1.2G fasta file.&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-05-18T09:23:17.877" CommentCount="1" />
  <row Id="112" PostTypeId="1" CreationDate="2017-05-18T09:27:51.110" Score="18" ViewCount="235" Body="&lt;p&gt;I am currently looking for a system which will allow me to version both the code and the data in my research.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think my way of analyzing data is not uncommon, and this will be useful for many people doing bioinformatics and aiming for the reproducibility.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the requrements:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Analysis is performed on multiple machines (local, cluster, server).&lt;/li&gt;&#xA;&lt;li&gt;All the code is transparently synchronized between the machines.&lt;/li&gt;&#xA;&lt;li&gt;Source code versioning.&lt;/li&gt;&#xA;&lt;li&gt;Generated data versioning.&lt;/li&gt;&#xA;&lt;li&gt;Support for large number of small generated files (&gt;10k). These also could be deleted.&lt;/li&gt;&#xA;&lt;li&gt;Support for large files (&gt;1Gb). At some point old generated files can  permanently deleted. It would be insane to have &lt;em&gt;transparent&lt;/em&gt; synchronization of those, but being able to synchronize them on demand would be nice.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So far I am using &lt;strong&gt;git&lt;/strong&gt; + rsync/scp. But there are several downsides to it.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Synchronization between multiple machines is a bit tedious, i.e. you have to git pull before you start working and git push after each update. I can live with that.&lt;/li&gt;&#xA;&lt;li&gt;You are not supposed to store large generated data files or large number of files inside your repository.&lt;/li&gt;&#xA;&lt;li&gt;Therefore I have to synchronize data files manually using rsync, which is error prone.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;There is something called &lt;a href=&quot;https://git-annex.branchable.com/&quot; rel=&quot;noreferrer&quot;&gt;git annex&lt;/a&gt;. It seems really close to what I need. But:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A bit more work than git, but that's ok.&lt;/li&gt;&#xA;&lt;li&gt;Unfortunately it seems it does not work well with the large number of files. Often I have more that 10k small files in my analysis. There are some tricks to &lt;a href=&quot;http://git-annex.branchable.com/tips/Repositories_with_large_number_of_files/&quot; rel=&quot;noreferrer&quot;&gt;improve indexing&lt;/a&gt;, but it doesn't solve the issue. What I need is one symlink representing the full contents of directory.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;One potential solution is to use Dropbox or something similar (like &lt;a href=&quot;https://syncthing.net/&quot; rel=&quot;noreferrer&quot;&gt;syncthing&lt;/a&gt;) in combination with git. But the downside is there will be no connection between the source code version and the data version.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any versioning system for the code and the data meeting the requirements you can recommend?&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-05-18T13:50:39.270" LastActivityDate="2017-05-21T07:27:58.337" Title="How to version the code and the data during the analysis?" Tags="&lt;versioning&gt;&lt;reproducibility&gt;&lt;data-management&gt;&lt;git&gt;" AnswerCount="6" CommentCount="8" FavoriteCount="1" />
  <row Id="113" PostTypeId="2" ParentId="112" CreationDate="2017-05-18T09:51:51.780" Score="8" Body="&lt;p&gt;Your question is somewhat open, but I think it could prove an interesting discussion. I don't believe in many cases it is worth storing the data you have created in &lt;code&gt;git&lt;/code&gt;. As you've noted, it isn't designed for large files (although we have &lt;code&gt;git-lfs&lt;/code&gt;) and it's definitely not designed for binary formats such as &lt;code&gt;BAM&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm of the opinion that &lt;code&gt;how&lt;/code&gt; a file was created and &lt;code&gt;what&lt;/code&gt; has been done to it since is key. Large files that took much effort to create should be mirrored somewhere (but not necessarily in a version control system). Other less-important (or less difficult to create) files that have been lost, clobbered or otherwise tainted can be regenerated as long as you know how they came to be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For what it's worth, I've been working on &lt;a href=&quot;https://github.com/SamStudio8/chitin/&quot; rel=&quot;noreferrer&quot;&gt;a piece of software called &lt;code&gt;chitin&lt;/code&gt;&lt;/a&gt; (self described as a shell for disorganised bioinformaticians). I also wrote &lt;a href=&quot;https://samnicholls.net/2016/11/16/disorganised-disaster/&quot; rel=&quot;noreferrer&quot;&gt;a long blog post&lt;/a&gt; on why I thought this was a necessary project for me, but the main reason was that despite my attempts to organise my filesystem and make good archives of my experiments, over time I forget what my shorthand directory names meant, or exactly what program generated which data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;chitin&lt;/code&gt;'s goal is to automatically capture changes made to the file system during the execution of a command. It knows what commands to run to re-create a particular file, what commands have used that file and can tell you when and why that file was changed (and by who ;) too).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's not finished (nothing ever is), but I feel that you might be going down the wrong road by wanting to store all your data and its versions when really, I think most people just want to know the commands that instigated changes. If the data history is important (and your code is well versioned), then you can simply check out any commit and execute your analysis to re-generate data.&lt;/p&gt;&#xA;" OwnerUserId="215" LastActivityDate="2017-05-18T09:51:51.780" CommentCount="1" />
  <row Id="114" PostTypeId="2" ParentId="110" CreationDate="2017-05-18T10:05:47.703" Score="1" Body="&lt;p&gt;I would suggest using blastx and parsing the output XML. As an example, have a look at this: &lt;a href=&quot;http://www.perlmonks.org/?node_id=1006367&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.perlmonks.org/?node_id=1006367&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The relevant part of the hit would be the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;Hit&amp;gt;&#xA;  &amp;lt;Hit_num&amp;gt;1&amp;lt;/Hit_num&amp;gt;&#xA;  &amp;lt;Hit_id&amp;gt;gnl|BL_ORD_ID|1515029&amp;lt;/Hit_id&amp;gt;&#xA;  &amp;lt;Hit_def&amp;gt;43989.cce_0262 (Cyanothece ATCC 51142)&amp;lt;/Hit_def&amp;gt;&#xA;  &amp;lt;Hit_accession&amp;gt;1515029&amp;lt;/Hit_accession&amp;gt;&#xA;  &amp;lt;Hit_len&amp;gt;65&amp;lt;/Hit_len&amp;gt;&#xA;  &amp;lt;Hit_hsps&amp;gt;&#xA;    &amp;lt;Hsp&amp;gt;&#xA;      &amp;lt;Hsp_num&amp;gt;1&amp;lt;/Hsp_num&amp;gt;&#xA;      &amp;lt;Hsp_bit-score&amp;gt;40.0466&amp;lt;/Hsp_bit-score&amp;gt;&#xA;      &amp;lt;Hsp_score&amp;gt;92&amp;lt;/Hsp_score&amp;gt;&#xA;      &amp;lt;Hsp_evalue&amp;gt;0.00664016&amp;lt;/Hsp_evalue&amp;gt;&#xA;      &amp;lt;Hsp_query-from&amp;gt;155&amp;lt;/Hsp_query-from&amp;gt;&#xA;      &amp;lt;Hsp_query-to&amp;gt;253&amp;lt;/Hsp_query-to&amp;gt;&#xA;      &amp;lt;Hsp_hit-from&amp;gt;12&amp;lt;/Hsp_hit-from&amp;gt;&#xA;      &amp;lt;Hsp_hit-to&amp;gt;44&amp;lt;/Hsp_hit-to&amp;gt;&#xA;      &amp;lt;Hsp_query-frame&amp;gt;-1&amp;lt;/Hsp_query-frame&amp;gt;&#xA;      &amp;lt;Hsp_hit-frame&amp;gt;0&amp;lt;/Hsp_hit-frame&amp;gt;&#xA;      &amp;lt;Hsp_identity&amp;gt;17&amp;lt;/Hsp_identity&amp;gt;&#xA;      &amp;lt;Hsp_positive&amp;gt;27&amp;lt;/Hsp_positive&amp;gt;&#xA;      &amp;lt;Hsp_gaps&amp;gt;0&amp;lt;/Hsp_gaps&amp;gt;&#xA;      &amp;lt;Hsp_align-len&amp;gt;33&amp;lt;/Hsp_align-len&amp;gt;&#xA;      &amp;lt;Hsp_qseq&amp;gt;LRGAICSMEHIEEALGKLKDWARKLIELLLGPR&amp;lt;/Hsp_qseq&amp;gt;&#xA;      &amp;lt;Hsp_hseq&amp;gt;ITGAVCLMDYLEKVLEKLRELAQKLIETLLGPQ&amp;lt;/Hsp_hseq&amp;gt;&#xA;      &amp;lt;Hsp_midline&amp;gt;+ GA+C M+++E+ L KL++ A+KLIE LLGP+&amp;lt;/Hsp_midline&amp;gt;&#xA;    &amp;lt;/Hsp&amp;gt;&#xA;  &amp;lt;/Hit_hsps&amp;gt;&#xA;&amp;lt;/Hit&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You should be able to get the position information from the following tags: &lt;code&gt;&amp;lt;Hsp_query-from&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;Hsp_query-to&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;Hsp_hit-from&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;Hsp_hit-to&amp;gt;&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To translate back to you original DNA sequence I can imagine you need the &lt;code&gt;&amp;lt;Hsp_query-frame&amp;gt;&lt;/code&gt; tag.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And then of course you get the evalue for free: &lt;code&gt;&amp;lt;Hsp_evalue&amp;gt;&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2017-05-18T10:05:47.703" CommentCount="5" />
  <row Id="115" PostTypeId="2" ParentId="97" CreationDate="2017-05-18T11:07:13.690" Score="4" Body="&lt;p&gt;One approach to this is to use whatever data you have to iteratively update the reference genome. You can keep chain files along the way so you can convert coordinates (e.g. in gff files) from the original reference to your new pseudoreference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A simple approach might be:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Align new data to existing reference&lt;/li&gt;&#xA;&lt;li&gt;Call variants (e.g. samtools mpileup, GATK, or whatever is best for you)&lt;/li&gt;&#xA;&lt;li&gt;Create new reference incorporating variants from 2&lt;/li&gt;&#xA;&lt;li&gt;Rinse and repeat (i.e. go to 1)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;You can track some simple stats as you do this - e.g. the number of new variants should decrease, the number of reads mapped should increase, and the mismatch rate should decrease, with every iteration of the above loop. Once the pseudoreference stabilises, you know you can't do much more. &lt;/p&gt;&#xA;" OwnerUserId="156" LastActivityDate="2017-05-18T11:07:13.690" CommentCount="0" />
  <row Id="116" PostTypeId="2" ParentId="91" CreationDate="2017-05-18T11:33:23.620" Score="11" Body="&lt;p&gt;You can do this easily with &lt;a href=&quot;https://github.com/lh3/bioawk&quot; rel=&quot;noreferrer&quot;&gt;bioawk&lt;/a&gt;, which is a version of awk with added features facilitating bioinformatics:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bioawk -c fastx '{print $name&quot;\t0\t&quot;length($seq)}' test.fa&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;-c fastx&lt;/code&gt; tells the program that the data should be parsed as fasta or fastq format. This makes the &lt;code&gt;$name&lt;/code&gt; and &lt;code&gt;$seq&lt;/code&gt; variables available in the awk commands.&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-05-18T11:33:23.620" CommentCount="0" />
  <row Id="117" PostTypeId="2" ParentId="112" CreationDate="2017-05-18T12:46:12.660" Score="3" Body="&lt;p&gt;The Open Science Framework uses versioning for all files and is free to use: &lt;a href=&quot;https://osf.io&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://osf.io&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can integrate data or code from various sources such as github, dropbox, google drive, figshare or amazon cloud&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can also store files on their server using OSF data storage, but I do not know exactly what the file size limit is.&lt;/p&gt;&#xA;" OwnerUserId="179" LastActivityDate="2017-05-18T12:46:12.660" CommentCount="0" />
  <row Id="118" PostTypeId="1" AcceptedAnswerId="129" CreationDate="2017-05-18T12:52:23.073" Score="5" ViewCount="90" Body="&lt;p&gt;I am interested in identifying mappings between different types of Affymetrix arrays. I am aware that mappings between gene and probeset can be extracted using Ensembl's Biomart database. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Ensembe gene id ENSG00000181019 maps to &#xA;1. AFFY HG-U133_Plus_2's 210519_s_at&#xA;2. AFFY HuGene-1_0-st-v1's 8002303&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any way to extract mappings of probeset ids between two arrays(eg: HG-U133_Plus_2, HuGene-1_0-st-v1)?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;eg: 210519_s_at and 8002303&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="23" LastEditorUserId="73" LastEditDate="2017-06-07T13:11:07.443" LastActivityDate="2017-06-07T13:11:07.443" Title="probeset to probeset mappings between Affymetrix arrays" Tags="&lt;ensembl&gt;&lt;biomart&gt;&lt;microarray&gt;" AnswerCount="3" CommentCount="7" FavoriteCount="1" />
  <row Id="119" PostTypeId="2" ParentId="112" CreationDate="2017-05-18T13:44:31.117" Score="2" Body="&lt;p&gt;Using Git for version-controlling code is a good practice, but it does not lend itself well to versioning large data files.  Manually syncing data across multiple nodes is asking for trouble, you want this syncing to either be handled automatically in a managed environment, or just keep the files on a single network-attached storage device.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One tool you might want to look into is &lt;a href=&quot;https://arvados.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Arvados&lt;/a&gt;, which is designed for syncing bioinformatics data and workflows across multiple machines.  From the project website:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Arvados is a platform for storing, organizing, processing, and sharing genomic and other big data. The platform is designed to make it easier for data scientists to develop analyses, developers to create genomic web applications and IT administers to manage large-scale compute and storage genomic resources. The platform is designed to run in the cloud or on your own hardware.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="47" LastActivityDate="2017-05-18T13:44:31.117" CommentCount="3" />
  <row Id="120" PostTypeId="1" AcceptedAnswerId="144" CreationDate="2017-05-18T14:28:47.653" Score="13" ViewCount="141" Body="&lt;p&gt;I have a reference genome and now I would like to call structural variants from Illumina pair-end whole genome resequencing data (insert size 700bp). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many tools for SV calls (I made an incomplete list of tools bellow). There is also a tool for merging SV calls from multiple methods / samples - &lt;a href=&quot;https://github.com/fritzsedlazeck/SURVIVOR&quot; rel=&quot;nofollow noreferrer&quot;&gt;SURVIVOR&lt;/a&gt;. Is there a combination of methods for SV detection with optimal balance between sensitivity and specificity?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a &lt;a href=&quot;http://www.ijpmbs.com/uploadfile/2016/1017/20161017025004545.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;benchmarking paper&lt;/a&gt;, evaluating sensitivity and specificity of SV calls of individual methods using simulated pair-end reads. However, there is no elaboration on the combination of methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;List of tools for calling structural variants:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/arq5x/lumpy-sv&quot; rel=&quot;nofollow noreferrer&quot;&gt;Lumpy&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://breakdancer.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;BreakDancer&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/Illumina/manta&quot; rel=&quot;nofollow noreferrer&quot;&gt;Manta&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/dellytools/delly&quot; rel=&quot;nofollow noreferrer&quot;&gt;Delly&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/PapenfussLab/gridss&quot; rel=&quot;nofollow noreferrer&quot;&gt;GRIDSS&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://compbio.med.harvard.edu/Meerkat/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Meerkat&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/genome/pindel&quot; rel=&quot;nofollow noreferrer&quot;&gt;Pindel&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://sourceforge.net/projects/softsv/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Softsv&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://compbio.cs.toronto.edu/prism/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Prism&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-04T14:08:11.640" LastActivityDate="2017-06-04T14:08:11.640" Title="How can I call structural variants (SVs) from pair-end short read resequencing data?" Tags="&lt;ngs&gt;&lt;structural-variation&gt;&lt;variant-calling&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="121" PostTypeId="2" ParentId="110" CreationDate="2017-05-18T15:44:11.847" Score="2" Body="&lt;p&gt;It would appear that &lt;code&gt;BLAT&lt;/code&gt; and &lt;a href=&quot;http://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;exonerate&lt;/code&gt;&lt;/a&gt; (as suggested by &lt;a href=&quot;https://bioinformatics.stackexchange.com/users/298/terdon&quot;&gt;@terdon&lt;/a&gt;) are the only pieces of software that provide enough information for &quot;HSPFragments&quot; to be parsed from their outputs, at least according to &lt;a href=&quot;http://biopython.org/DIST/docs/api/Bio.SearchIO._model-module.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;BioPython's documentation&lt;/a&gt;, anyway:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Most search programs only have HSPs with one HSPFragment in them,&#xA;  making these two objects inseparable. However, there are programs&#xA;  (e.g. BLAT and Exonerate) which may have more than one HSPFragment&#xA;  objects in any given HSP. If you are not using these programs, you can&#xA;  safely consider HSP and HSPFragment as a single union.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It would seem the options are sticking with &lt;code&gt;BLAT&lt;/code&gt; and generating scores afterwards with the &lt;a href=&quot;http://genome.ucsc.edu/FAQ/FAQblat.html#blat4&quot; rel=&quot;nofollow noreferrer&quot;&gt;script described in their FAQ&lt;/a&gt;, or using &lt;code&gt;exonerate&lt;/code&gt; which provides a raw score as part of its default output. What these scores actually mean I will have to investigate, but for now I think the case is closed!&lt;/p&gt;&#xA;" OwnerUserId="215" LastEditorUserId="215" LastEditDate="2017-05-18T15:50:52.953" LastActivityDate="2017-05-18T15:50:52.953" CommentCount="0" />
  <row Id="122" PostTypeId="2" ParentId="98" CreationDate="2017-05-18T16:23:09.657" Score="3" Body="&lt;p&gt;&lt;code&gt;samtools mpileup&lt;/code&gt; can do this quickly:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools mpileup -f reference.fasta -uv input.sam &amp;gt; variants.vcf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This will produce a &lt;a href=&quot;http://vcftools.sourceforge.net/VCF-poster.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;VCF-formatted&lt;/a&gt; file containing information about what variants have been seen in the SAM file, aggregated for all the mapped reads.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-05-18T16:23:09.657" CommentCount="0" />
  <row Id="123" PostTypeId="2" ParentId="112" CreationDate="2017-05-18T16:57:57.520" Score="11" Body="&lt;p&gt;There is a couple of points to consider here, which I outline below. The goal here should be to find a workflow that is minimally intrusive on top of already using &lt;code&gt;git&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As of yet, there is no ideal workflow that covers all use cases, but what I outline below is the closest I could come to it.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Reproducibility is not just keeping all your data&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;You have got your raw data that you start your project with.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All other data in your project directory should never just &quot;be there&quot;, but have some record of where it comes from. Data processing scripts are great for this, because they &lt;em&gt;already document how&lt;/em&gt; you went from your raw to your analytical data, and then the files needed for your analyses.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And those scripts can be versioned, with an appropriate single entry point of processing (e.g. a &lt;code&gt;Makefile&lt;/code&gt; that describes how to run your scripts).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This way, the state of all your project files is defined by the raw data, and the version of your processing scripts (and versions of external software, but that's a whole different kind of problem).&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;What data/code should and should not be versioned&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Just as you would not version generated code files, you should not want to version 10k intermediary data files that you produced when performing your analyses. The data that &lt;em&gt;should be&lt;/em&gt; versioned is your &lt;em&gt;raw data&lt;/em&gt; (at the start of your pipeline), not automatically generated files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You might want to take snapshots of your project directory, but not keep every version of every file ever produced. This already cuts down your problem by a fair margin.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Approach 1: Actual versioning of data&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;For your &lt;strong&gt;raw or analytical data&lt;/strong&gt;, &lt;a href=&quot;https://git-lfs.github.com/&quot; rel=&quot;noreferrer&quot;&gt;Git LFS&lt;/a&gt; (and alternatively &lt;a href=&quot;https://git-annex.branchable.com/&quot; rel=&quot;noreferrer&quot;&gt;Git Annex&lt;/a&gt;, that you already mention) is designed to solve exactly this problem: add tracking information of files in your Git tree, but do not store the content of those files in the repository (because otherwise it would add the size of a non-diffable file with every change you make).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For your &lt;strong&gt;intermediate files&lt;/strong&gt;, you do the same as you would do with intermediate code files: add them to your &lt;code&gt;.gitignore&lt;/code&gt; and do not version them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This begs a couple of considerations:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Git LFS is a paid service from Github (the free tier is limited to 1 GB of storage/bandwidth per month, which is very little), and it is more expensive than other comparable cloud storage solutions. You could consider paying for the storage at Github or running your own LFS server (there is a reference implementation, but I assume this would still be a substantial effort)&lt;/li&gt;&#xA;&lt;li&gt;Git Annex is free, but it replaces files by links and hence changes time stamps, which is a problem for e.g. GNU Make based workflows (major drawback for me). Also, fetching of files needs to be done manually or via a commit hook&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Approach 2: Versioning code only, syncing data&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If your analytical data stays the same for most of your analyses, so the actual need to version it (as opposed to back up and document data provenance, which is essential) may be limited.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The key to get this this working is to put all &lt;strong&gt;data files&lt;/strong&gt; in your &lt;code&gt;.gitignore&lt;/code&gt; and ignore all your &lt;strong&gt;code files&lt;/strong&gt; in &lt;code&gt;rsync&lt;/code&gt;, with a script in your project root (extensions and directories are an example only):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/bin/bash&#xA;cd $(dirname $0)&#xA;rsync -auvr \&#xA;    --exclude &quot;*.r&quot; \&#xA;    --include &quot;*.RData&quot; \&#xA;    --exclude &quot;dir with huge files that you don't need locally&quot; \&#xA;    yourhost:/your/project/path/* .&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The advantage here is that you don't need to remember the &lt;code&gt;rsync&lt;/code&gt; command you are running. The script itself goes into version control.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is especially useful if you do your heavy processing on a computing cluster but want to make plots from your result files on your local machine. I argue that you &lt;em&gt;generally don't need&lt;/em&gt; bidirectional sync.&lt;/p&gt;&#xA;" OwnerUserId="69" LastActivityDate="2017-05-18T16:57:57.520" CommentCount="3" />
  <row Id="124" PostTypeId="2" ParentId="81" CreationDate="2017-05-18T17:32:11.053" Score="4" Body="&lt;p&gt;This python script takes a fasta file and tsv file with counts and prints the sequences in the fasta files that many times as it is specified in the tsv file (assuming the format in the question). So if &lt;code&gt;bar.tsv&lt;/code&gt; and &lt;code&gt;foo.fasta&lt;/code&gt; will be your files:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from Bio import SeqIO&#xA;&#xA;repeat = {}&#xA;for line in open(&quot;bar.tsv&quot;):&#xA;    seq_id, coverage = line.split()&#xA;    repeat[seq_id] = int(coverage)&#xA;&#xA;for seq_record in SeqIO.parse(foo.fasta, &quot;fasta&quot;):&#xA;    for i in range(repeat.get(seq_record.name, 0)):&#xA;        print(&quot;&amp;gt;&quot;,seq_record.name,&quot;_&quot;,i,sep='')&#xA;        print(seq_record.seq)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-05-30T15:24:35.630" LastActivityDate="2017-05-30T15:24:35.630" CommentCount="3" />
  <row Id="125" PostTypeId="1" AcceptedAnswerId="127" CreationDate="2017-05-18T18:52:27.690" Score="7" ViewCount="59" Body="&lt;p&gt;Let's say I want to construct a phylogenetic tree based on orthologous nucleotide sequences; I do not want to use protein sequences to have a better resolution. These species have different GC-content.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we use a straightforward approach like maximum likelihood with JC69 or any other classical nucleotide model, conserved protein coding sequences of distant species with similar GC-content will artificially cluster together. This will happen because GC-content will mainly affect wobbling codon positions, and they will look similar on the nucleotide level.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are possible ways to overcome this? I considered the following options so far:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Using protein sequence. This is possible of course, but we lose a lot of information on the short distance. Not applicable to non-coding sequences. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://doi.org/10.1186/1471-2105-15-S2-S8&quot; rel=&quot;noreferrer&quot;&gt;Recoding&lt;/a&gt;. In this approach C and T can be combined into a single pyrimidine state Y (G and A could be also combined in some implementations). This sounds interesting, but, first, we also lose some information here. Mathematical properties of the resulting process are not clear. As a result, this approach is not widely used.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Excluding third codon position from the analysis. Losing some short-distance information again. Also, not all synonymous substitution are specific to the third codon positions, so we still expect to have some bias. Not applicable to non-coding sequence.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;It should be possible in theory to have a model which allows shifts in GC-content. This will be a non time-reversible Markov process. As far as I understand there are some computational difficulties estimating likelihood for such models.&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-05-18T19:37:39.770" LastActivityDate="2017-05-19T09:24:00.620" Title="What is the best way to account for GC-content shift while constructing nucleotide-based phylogenetic tree?" Tags="&lt;phylogeny&gt;&lt;nucleotide-models&gt;&lt;gc-content&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="127" PostTypeId="2" ParentId="125" CreationDate="2017-05-18T22:38:52.100" Score="3" Body="&lt;p&gt;There are models that take into account compositional heterogeneity both under the &lt;a href=&quot;https://doi.org/10.1080/10635150600975218&quot; rel=&quot;nofollow noreferrer&quot;&gt;maximum likelihood&lt;/a&gt; and &lt;a href=&quot;https://doi.org/10.1093/molbev/msl091&quot; rel=&quot;nofollow noreferrer&quot;&gt;Bayesian&lt;/a&gt; frameworks. Although the substitution process is not time-reversible, the computations are &lt;a href=&quot;https://doi.org/10.1080/10635150490445779&quot; rel=&quot;nofollow noreferrer&quot;&gt;simplified by assuming&lt;/a&gt; that the instantaneous rate matrix can be decomposed into an &quot;equilibrium frequency vector&quot; (non-homogeneous) and a symmetric, constant exchange rate matrix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess all your suggestions are also valid, and I remember recoding being used successfully to reduce the GC-content bias (examples in the references above and &lt;a href=&quot;https://doi.org/10.1093/molbev/msh137&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;).  &lt;/p&gt;&#xA;" OwnerUserId="45" LastEditorUserId="191" LastEditDate="2017-05-19T09:24:00.620" LastActivityDate="2017-05-19T09:24:00.620" CommentCount="0" />
  <row Id="129" PostTypeId="2" ParentId="118" CreationDate="2017-05-19T01:18:14.417" Score="7" Body="&lt;p&gt;If your question is: can probeset IDs from different platforms be mapped to one another in a similar way as mapping probesets to genes, then the answer is: Yes. BioMart allows you to map almost anything that has an ID to anything else that has an ID.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can use BioMart either via the web interface or programatically. A brief guide to using the web interface, for mapping HG U133 Plus 2 to HuGene 1.0:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Go to &lt;a href=&quot;http://www.ensembl.org/biomart/martview&quot; rel=&quot;noreferrer&quot;&gt;the start page&lt;/a&gt; &lt;/li&gt;&#xA;&lt;li&gt;Select H. sapiens Ensembl genes for your database; Human genes (GRCh38.p10) for your dataset&lt;/li&gt;&#xA;&lt;li&gt;Click Filters in the left-hand column&lt;/li&gt;&#xA;&lt;li&gt;Expand &quot;REGION&quot;, scroll down to GENE and select &quot;Input microarray probes/probesets ID list [Max 500 advised]&quot; &lt;/li&gt;&#xA;&lt;li&gt;Select AFFY HG U133 PLUS 2 probe ID(s) and either copy/paste or upload a list, one per line&lt;/li&gt;&#xA;&lt;li&gt;Click Attributes in the left-hand column&lt;/li&gt;&#xA;&lt;li&gt;Scroll through, uncheck what you don't want to see and choose what you do, for example Gene Stable ID, AFFY HuGene 1 0 st v1 probe and AFFY HG U133 Plus 2 probe&lt;/li&gt;&#xA;&lt;li&gt;Finally click &quot;Results&quot; in the menu at the top of the left-hand column&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;And you should see &lt;a href=&quot;http://www.ensembl.org/biomart/martview/5e02aa3df275d697dcd2c806c6e5c1a0&quot; rel=&quot;noreferrer&quot;&gt;a result like this&lt;/a&gt; (you'll need to click the &quot;Results&quot; button).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You should not expect that there be a one-to-one mapping, for two reasons:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Genes have multiple transcripts and probesets are mapped to each transcript&lt;/li&gt;&#xA;&lt;li&gt;Some transcripts have more than one probeset: in this case, the HuGene IDs 8002301 and 8002303 map to transcripts for this gene&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Finally: here's a programmatic example using &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/biomaRt.html&quot; rel=&quot;noreferrer&quot;&gt;R/BioMart&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(biomaRt)&#xA;mart.hs &amp;lt;- useMart(&quot;ensembl&quot;, &quot;hsapiens_gene_ensembl&quot;)&#xA;results &amp;lt;- getBM(attributes = c(&quot;ensembl_gene_id&quot;, &quot;affy_hugene_1_0_st_v1&quot;, &quot;affy_hg_u133_plus_2&quot;),&#xA;                 filters = &quot;affy_hg_u133_plus_2&quot;,&#xA;                 values = c(&quot;210519_s_at&quot;),&#xA;                 mart = mart.hs)&#xA;&#xA;results&#xA;  ensembl_gene_id affy_hugene_1_0_st_v1 affy_hg_u133_plus_2&#xA;1 ENSG00000181019               8002301         210519_s_at&#xA;2 ENSG00000181019               8002303         210519_s_at&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="150" LastEditorUserId="150" LastEditDate="2017-05-19T01:39:33.997" LastActivityDate="2017-05-19T01:39:33.997" CommentCount="0" />
  <row Id="130" PostTypeId="2" ParentId="118" CreationDate="2017-05-19T01:53:56.150" Score="5" Body="&lt;p&gt;Instead of biomaRt, it is also possible to use the mapping databases built into Bioconductor itself, and map from probe to gene, and then from gene to probe in the second. Some R code to convert between hgu133 and hgu95 using the same probe ID provided in another:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(hgu133plus2.db)&#xA;library(hgu95av2.db)&#xA;&#xA;query_probe &amp;lt;- &quot;210519_s_at&quot;&#xA;&#xA;hgu133_ensembl &amp;lt;- select(hgu133plus2.db, keys = query_probe, columns = &quot;ENSEMBL&quot;)&#xA;&#xA;ensembl_hgu95 &amp;lt;- select(hgu95av2.db, keys = hgu133_ensembl$ENSEMBL, keytype = &quot;ENSEMBL&quot;, columns = &quot;PROBEID&quot;)&#xA;&#xA;dplyr::inner_join(hgu133_ensembl, ensembl_hgu95, by = &quot;ENSEMBL&quot;, suffix = c(&quot;.133&quot;, &quot;.95&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="51" LastActivityDate="2017-05-19T01:53:56.150" CommentCount="0" />
  <row Id="131" PostTypeId="2" ParentId="118" CreationDate="2017-05-19T01:57:32.150" Score="1" Body="&lt;p&gt;Other answers explain why there might not be one to one mapping between the probes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;http://bioinformatics.louisville.edu/abid/&quot; rel=&quot;nofollow noreferrer&quot;&gt;AbsID database&lt;/a&gt; does conversion based on mapping the probe sequences to a genome build, and then determines mappings based on overlapping genome alignment coordinates. This is really useful if you want to be sure that two probes are &lt;em&gt;actually likely&lt;/em&gt; measuring the same transcript.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is dependent on both probes aligning to the genome, however.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Disclaimer: I worked in the group that provides the AbsID mapping tool.&lt;/p&gt;&#xA;" OwnerUserId="51" LastEditorUserId="51" LastEditDate="2017-05-19T02:16:24.147" LastActivityDate="2017-05-19T02:16:24.147" CommentCount="0" />
  <row Id="133" PostTypeId="2" ParentId="91" CreationDate="2017-05-19T05:14:21.100" Score="3" Body="&lt;p&gt;We have many excellent answers! This will be an excellent reference for future users.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found what exactly what I was asking in my question:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;a href=&quot;https://www.biostars.org/p/191052/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.biostars.org/p/191052/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ pip install pyfaidx  &#xA;$ faidx --transform bed test.fasta &amp;gt; test.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the one-line command I was asking. The other answers also work, but I want to accept my own answer.&lt;/p&gt;&#xA;" OwnerUserId="174" LastActivityDate="2017-05-19T05:14:21.100" CommentCount="1" />
  <row Id="134" PostTypeId="1" CreationDate="2017-05-19T05:49:46.937" Score="4" ViewCount="57" Body="&lt;p&gt;I have a BAM file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@SQ SN:chr1 LN:248956422&#xA;@SQ SN:chrx LN:248956423&#xA;ST-E00110:348:HGVKKALXX:1:1201:5822:48670   323 chr1    9999    0   67H66M16H   chrx    1000    0   GATAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC  JJJJJJJJJJJJJJJJAJJJJJJJJJJJJFJJJJJJFJFJJJJJJFJJJJJJJJJJJA77FJFJJJ  NM:i:0  MD:Z:66 AS:i:66 XS:i:65 SA:Z:chr5,18606834,-,73S76M,34,0;   RG:Z:g1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There is a read aligned to &lt;code&gt;chr1&lt;/code&gt;, and it's mate aligned to &lt;code&gt;chrx&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a BED file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;chr1    0   100000  TestOnly&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would like to filter out everything that falls outside my BED region, that includes cross-alignments. In my example, although my read aligned to &lt;code&gt;chr1&lt;/code&gt; but it's mate is not. I don't want this read.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I do:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;samtools view -L test.bed test.bam&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;the command gives me the read because it doesn't check cross-alignments.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My solution:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;samtools view -L test.bed test.bam | grep -v chrx&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;but this is very slow and clumsy. In my production pipeline I would have to do something like:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;samtools view -L test.bed test.bam | grep -v chrx | grep -v ... | grep -v ... | grep -v ... | grep -v ...&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Is there a better solution?&lt;/p&gt;&#xA;" OwnerUserId="174" LastEditorUserId="174" LastEditDate="2017-05-19T09:14:23.940" LastActivityDate="2017-05-21T23:21:14.657" Title="How to filter out cross alignments from a BED file?" Tags="&lt;bam&gt;&lt;bed&gt;&lt;genomics&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="135" PostTypeId="1" AcceptedAnswerId="137" CreationDate="2017-05-19T05:50:24.497" Score="7" ViewCount="70" Body="&lt;p&gt;I am working with over a million (long) reads, and aligning them to a large genome. I am considering running my alignment jobs in parallel, distributing horizontally across hundreds of nodes rather than trying to run a single job with dozens of cores.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to merge the sorted BAM files together for further downstream analysis. What is the most efficient way to do so while maintaining a valid file header and taking advantage of the fact that the input bam files are already sorted?&lt;/p&gt;&#xA;" OwnerUserId="163" LastActivityDate="2017-05-19T07:18:49.330" Title="Merge hundreds of small BAM files into a single BAM file" Tags="&lt;alignment&gt;&lt;bam&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="137" PostTypeId="2" ParentId="135" CreationDate="2017-05-19T07:06:35.520" Score="10" Body="&lt;p&gt;&lt;code&gt;samtools merge merged.bam *.bam&lt;/code&gt; is efficient enough since the input files are sorted. You can get a bit faster with sambamba and/or biobambam, but they're not typically already installed and IO quickly becomes a bottleneck anyway.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-19T07:06:35.520" CommentCount="3" />
  <row Id="138" PostTypeId="5" CreationDate="2017-05-19T07:08:03.053" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-19T07:08:03.053" LastActivityDate="2017-05-19T07:08:03.053" CommentCount="0" />
  <row Id="139" PostTypeId="4" CreationDate="2017-05-19T07:08:03.053" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-19T07:08:03.053" LastActivityDate="2017-05-19T07:08:03.053" CommentCount="0" />
  <row Id="140" PostTypeId="2" ParentId="135" CreationDate="2017-05-19T07:18:49.330" Score="7" Body="&lt;p&gt;Merging sorted files is a linear operation, so any well-implemented tools that do it will do it with approximately the same efficiency.  So &lt;code&gt;samtools merge&lt;/code&gt; (use the most up-to-date version, as there have been improvements in merge header handling in the 1.3.x and 1.4.x versions), &lt;code&gt;picard MergeSamFiles&lt;/code&gt;, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These tools need to hold all the input BAM files open simultaneously, so depending on how many hundred of input files you have you may run into the system limit on open file descriptors.  Use &lt;code&gt;ulimit&lt;/code&gt; to maximise this first; if there are still too many, you may need to merge the first 500 files, then merge the next 500 into that, etc.  Samtools does not do this internally; I'm not sure whether any of the other merge implementations do.&lt;/p&gt;&#xA;" OwnerUserId="134" LastActivityDate="2017-05-19T07:18:49.330" CommentCount="2" />
  <row Id="141" PostTypeId="2" ParentId="125" CreationDate="2017-05-19T09:18:45.637" Score="1" Body="&lt;p&gt;The following 2004 paper describes a way to model compositional changes across the tree, in a Bayesian framework: &lt;a href=&quot;https://doi.org/10.1080/10635150490445779&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://doi.org/10.1080/10635150490445779&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A python package implementing this (&quot;p4&quot;), and improvements added along the years, is available here: &lt;a href=&quot;https://github.com/pgfoster/p4-phylogenetics&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/pgfoster/p4-phylogenetics&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To get started, you may find useful examples here: &lt;a href=&quot;http://p4.nhm.ac.uk/scripts.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://p4.nhm.ac.uk/scripts.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This has been used in a few large-scale phylogenetic analyses.&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-05-19T09:18:45.637" CommentCount="0" />
  <row Id="142" PostTypeId="2" ParentId="112" CreationDate="2017-05-19T09:33:58.507" Score="2" Body="&lt;p&gt;The way we deal with this is:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;All work is done in a single filesystem mounted on the cluster&lt;/li&gt;&#xA;&lt;li&gt;This file system is mounted on local machines via sshfs/samba (depending on the location of the current &quot;local&quot; machine on the network).&lt;/li&gt;&#xA;&lt;li&gt;Code is versioned with git hub&lt;/li&gt;&#xA;&lt;li&gt;All computation is carried out via light-weight automated pipelines. We use ruffus in combination with an in-house utility layer. The system doesn't really matter as long is it no more work to add another step to the pipeline than it would be to execute it manually. &lt;/li&gt;&#xA;&lt;li&gt;All questionably design decisions are encoded in configuration files. These configuration files, along with a very detailed log output by the pipeline (what was run, what was the git commit of the code run, what was the time stamp of the files it was run on, etc) and the initial input files are version controlled.&lt;/li&gt;&#xA;&lt;li&gt;The benefit of this is that code + configuration + time = output. It is not expected that the whole pipeline will be rerun everytime anything is changed, but the pipeline will tell you if something is out of date (it can use timestamps or file hashes), and it can all be run in one go before publication.&lt;/li&gt;&#xA;&lt;li&gt;Any other analysis is carried out in juptyer notebooks. These are version controlled.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;To summarise, we don't synchronise because we only ever work from one disk location even if we use multiple CPU locations. We version control:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Code&lt;/li&gt;&#xA;&lt;li&gt;Inputs, configuration, logs&lt;/li&gt;&#xA;&lt;li&gt;Juptyer notebooks&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Log records the git commits used to produce the current outputs. &lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-05-19T09:33:58.507" CommentCount="2" />
  <row Id="143" PostTypeId="1" CreationDate="2017-05-19T10:09:11.327" Score="1" ViewCount="75" Body="&lt;p&gt;I currently have ~180 whole germlines and around 10M SNPs/indels. I would like to build a predictive model using Machine Learning (ML) techniques to predict cancer risk according to these germline variants. The thing is, most of these 10M variants are not relevant to cancer, so first I have to annotate them and remove non-relevant SNPs/indels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which tools/measurements do you think are important to use in order to filter out irrelevant variants and keep only, let's say, around 10,000 SNPs/indels?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess the most obvious is CADD score, so I could keep those SNPs/indels with PHRED &gt;= 10.&#xA;Which one would you suggest taking into account that downstream analysis would be ML algorithms that take as features variants that &lt;strong&gt;really&lt;/strong&gt; contribute to cancer?&lt;/p&gt;&#xA;" OwnerUserId="328" LastEditorUserId="298" LastEditDate="2017-05-20T21:54:37.907" LastActivityDate="2017-05-20T21:54:37.907" Title="How can I use annotations to remove variants not relevant to cancer risk?" Tags="&lt;ngs&gt;&lt;annotation&gt;&lt;machine-learning&gt;&lt;cancer&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="144" PostTypeId="2" ParentId="120" CreationDate="2017-05-19T10:18:40.500" Score="5" Body="&lt;p&gt;I think the best method or combination of methods will depend on aspects of the data that might vary from one dataset to another. E.g. the type, size, and frequency of structural variants, the number SNVs, the quality of the reference, contaminants or other issues (e.g. read quality, sequencing errors) etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For that reason, I'd take two approaches:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Try a lot of methods, and look at their overlap&lt;/li&gt;&#xA;&lt;li&gt;Validate a subset of calls from different methods by wet lab experiments - in the end this is the only real way of knowing the accuracy for a particular case.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="156" LastEditorUserId="57" LastEditDate="2017-05-22T03:36:59.117" LastActivityDate="2017-05-22T03:36:59.117" CommentCount="3" />
  <row Id="145" PostTypeId="2" ParentId="143" CreationDate="2017-05-19T10:24:51.223" Score="4" Body="&lt;p&gt;While your question is specific to cancerous germline mutations, I'd suggest you look at the &lt;a href=&quot;http://cancer.sanger.ac.uk/cosmic&quot; rel=&quot;nofollow noreferrer&quot;&gt;COSMIC&lt;/a&gt; database of somatic mutations to include in your analysis. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are other factors to include in this kind of analysis you're suggesting, such as predictive deleterious effects (PolyPhen for example can perform such predictions).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have 10M variants/indels, for the same form of cancer, then look for common variants, or maybe the frequency of variants identified across exons of a gene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What you've asked is highly broad and we'd need more specific details about your cohort to suggest more specific approaches. &lt;/p&gt;&#xA;" OwnerUserId="222" LastActivityDate="2017-05-19T10:24:51.223" CommentCount="0" />
  <row Id="146" PostTypeId="1" CreationDate="2017-05-19T10:32:45.047" Score="11" ViewCount="110" Body="&lt;p&gt;In Computer Science a &lt;a href=&quot;https://en.wikipedia.org/wiki/De_Bruijn_graph&quot; rel=&quot;noreferrer&quot;&gt;De Bruijn graph&lt;/a&gt; has (1) &lt;code&gt;m^n&lt;/code&gt; vertices representing all possible  sequences of length &lt;code&gt;n&lt;/code&gt; over &lt;code&gt;m&lt;/code&gt; symbols, and (2) directed edges connecting nodes that differ by a shift of &lt;code&gt;n-1&lt;/code&gt; elements (the successor having the new element at the right).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However in Bioinformatics while condition (2) is preserved, what is called a De Bruijn graph &lt;a href=&quot;http://www.homolog.us/Tutorials/index.php?p=2.1&amp;amp;s=1&quot; rel=&quot;noreferrer&quot;&gt;doesn't seem to respect condition&lt;/a&gt; (1). In some cases the graph doesn't look anything like a de Bruijn graph at all (e.g. &lt;a href=&quot;http://genome.cshlp.org/content/18/5/821.full&quot; rel=&quot;noreferrer&quot;&gt;http://genome.cshlp.org/content/18/5/821.full&lt;/a&gt;).     &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question is, if I want to make it explicit that I am using the Bioinformatics interpretation of a de Bruijn graph, is there a term for it? Something like &quot;simplified de Bruijn graph&quot;, &quot;projection of a de Bruijn graph&quot;, or &quot;graph of neighbouring k-mers&quot;? Are there any papers making this distinction, or did I get it all wrong?&lt;/p&gt;&#xA;" OwnerUserId="45" LastEditorUserId="57" LastEditDate="2017-05-19T14:56:34.173" LastActivityDate="2017-05-22T20:33:56.417" Title="How to make a distinction between the &quot;classical&quot; de Bruijn graph and the one described in NGS papers?" Tags="&lt;ngs&gt;&lt;assembly&gt;&lt;k-mer&gt;&lt;de-bruijn&gt;" AnswerCount="3" CommentCount="4" FavoriteCount="1" />
  <row Id="147" PostTypeId="2" ParentId="146" CreationDate="2017-05-19T11:07:00.467" Score="3" Body="&lt;p&gt;In addition to the regular De Bruijn graph as depicted on the wikipedia, some implementations in bioinformatics feature additional processing. I guess the main reason figure 1 in the paper you linked (concerning the Velvet genome assembler) is slightly different is that a node represents &lt;em&gt;a series of overlapping k-mers&lt;/em&gt;. In order to visualize this as a more classic De Bruin graph you would have to connect the k-mers depicted &lt;em&gt;above&lt;/em&gt; the nodes. The caption next to figure one describes the processing quite clearly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per your last question: I don't think there is a 'Bioinformatic interpretation of a De Bruijn graph'. There are different implementations, which all have there specifics. Thus it would be best to refer to the actual implementation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an example: &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/27587666&quot; rel=&quot;nofollow noreferrer&quot;&gt;this&lt;/a&gt; is a nice paper on how to construct a pan-genome De Bruijn graph of multiple genomes simultaneously.  &lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2017-05-19T11:07:00.467" CommentCount="2" />
  <row Id="149" PostTypeId="1" AcceptedAnswerId="173" CreationDate="2017-05-19T12:32:54.203" Score="6" ViewCount="83" Body="&lt;p&gt;Several gene set enrichment methods are available, the most famous/popular is the &lt;a href=&quot;http://software.broadinstitute.org/gsea/index.jsp&quot; rel=&quot;nofollow noreferrer&quot;&gt;Broad Institute tool&lt;/a&gt;. Many other tools are available (See for example the &lt;a href=&quot;https://bioconductor.org/packages/release/BiocViews.html#___GeneSetEnrichment&quot; rel=&quot;nofollow noreferrer&quot;&gt;biocView of GSE&lt;/a&gt; which list 82 different packages). There are several parameters in consideration :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the statistic used to order the genes, &lt;/li&gt;&#xA;&lt;li&gt;if it competitive or self-contained,&lt;/li&gt;&#xA;&lt;li&gt;if it is supervised or not,&lt;/li&gt;&#xA;&lt;li&gt;and how is the enrichment score calculated.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I am using the &lt;a href=&quot;http://bioconductor.org/packages/fgsea&quot; rel=&quot;nofollow noreferrer&quot;&gt;fgsea - Fast Gene Set Enrichment Analysis&lt;/a&gt; package to calculate the enrichment scores and someone told me that the numbers are different from the ones on the Broad Institute despite all the other parameters being equivalent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are these two methods (fgsea and Broad Institute GSEA) equivalent to calculate the enrichment score?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I looked to the algorithms of both papers, and they seem fairly similar, but I don't know if in real datasets they are equivalent or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;Is there any article reviewing and comparing how does the enrichment score method affect to the result?&lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="131" LastEditDate="2017-05-22T09:32:02.147" LastActivityDate="2017-05-22T09:32:02.147" Title="Are fgsea and Broad Institute GSEA equivalent?" Tags="&lt;r&gt;&lt;gse&gt;&lt;bioconductor&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="150" PostTypeId="1" CreationDate="2017-05-19T13:57:30.553" Score="4" ViewCount="119" Body="&lt;p&gt;I have an &lt;a href=&quot;http://datadryad.org/bitstream/handle/10255/dryad.122664/obitools_script.txt?sequence=1&quot; rel=&quot;nofollow noreferrer&quot;&gt;obitools script&lt;/a&gt; (de Barba et al. 2016) that I would like to run faster. How would you run it in parallel to cut down on time?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;illuminapairedend -r rawdata_scandinavia_R2.fastq rawdata_scandinavia_R1.fastq | tee rawdata_scandinavia.fastq | obiannotate -S goodAli:'&quot;Alignement&quot; if score&amp;gt;40.00 else &quot;Bad&quot;' | obisplit -t goodAli -p rawdata_scandinavia.&#xA;touch rawdata_scandinavia.Bad.fastq&#xA;touch rawdata_scandinavia.Alignement.fastq&#xA;touch rawdata_scandinavia.fastq&#xA;ngsfilter -t rawdata_scandinavia.ngsfilter -u rawdata_scandinavia.unidentified.fastq rawdata_scandinavia.Alignement.fastq &amp;gt; rawdata_scandinavia.filtered.fastq&#xA;obisplit -p MICROSAT.PCR_ -t experiment rawdata_scandinavia.filtered.fastq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Can someone describe what tools and steps one needs to take to run this job in parallel, given that the functions are not designed to be run in parallel out of the box?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;De Barba, M., Miquel, C., Lobréaux, S., Quenette, P. Y., Swenson, J. E., &amp;amp; Taberlet, P. (2016). High-throughput microsatellite genotyping in ecology: improved accuracy, efficiency, standardization and success with low-quantity and degraded DNA. Molecular Ecology Resources, 1–16. &lt;a href=&quot;https://doi.org/10.1111/1755-0998.12594&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://doi.org/10.1111/1755-0998.12594&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="200" LastEditorUserId="200" LastEditDate="2017-06-09T08:05:55.257" LastActivityDate="2017-06-09T08:05:55.257" Title="Parallel processing of scripts that use obitools" Tags="&lt;ngs&gt;&lt;obitools&gt;&lt;linux&gt;&lt;parallel&gt;" AnswerCount="2" CommentCount="7" />
  <row Id="151" PostTypeId="2" ParentId="150" CreationDate="2017-05-19T14:03:08.770" Score="3" Body="&lt;p&gt;Your only option here is to first split the fastq files into small chunks, process each and then merge them back together. Alternatively, find something else that runs faster.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-19T14:03:08.770" CommentCount="0" />
  <row Id="152" PostTypeId="2" ParentId="146" CreationDate="2017-05-19T14:14:34.317" Score="2" Body="&lt;p&gt;Let's first assume DNA only has one strand. An assembly de Bruijn graph is a subgraph of a complete de Bruijn graph. It contains a vertex u if u is a k-mer in reads; it contains an edge u-&gt;v, if u and v are adjacent k-mers on a read. Alternatively, we note that an edge u-&gt;v is represented by a (k+1)-mer. An assembly de Bruijn graph can be considered a subgraph edge induced from all (k+1)-mers in reads – in fact, some assemblers take the list of (k+1)-mer as a succinct representation of de Bruijn graphs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DNA has two strands. We just need to induce an assembly de Bruijn graph from all (k+1)-mers and their reverse complement. It is still a subgraph of a complete de Bruijn graph.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because an assembly de Bruijn graph is just a subgraph. It is not necessary to give it a new name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: I deleted my old answer as that was not what you are asking for based on your comments. I was confused by your mentioning velvet. Velvet uses an equivalent but uncommon representation of de Bruijn graphs, which complicates your question.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-05-19T14:14:34.317" CommentCount="0" />
  <row Id="153" PostTypeId="1" AcceptedAnswerId="192" CreationDate="2017-05-19T14:40:13.787" Score="8" ViewCount="116" Body="&lt;p&gt;I have a set of high-troughput experiments with 2 genotypes (&quot;WT&quot; and &quot;prg1&quot;) and 3 treatments (&quot;RT&quot;, &quot;HS30&quot; and &quot;HS30RT120&quot;), and there are 2 replicates for each of the genotype x treatment combinations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The read counts for the genes are summarized in a file that I load as follows in R:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; counts_data &amp;lt;- read.table(&quot;path/to/my/file&quot;, header=TRUE, row.names=&quot;gene&quot;)&#xA;&amp;gt; colnames(counts_data)&#xA; [1] &quot;WT_RT_1&quot;          &quot;WT_HS30_1&quot;        &quot;WT_HS30RT120_1&quot;   &quot;prg1_RT_1&quot;       &#xA; [5] &quot;prg1_HS30_1&quot;      &quot;prg1_HS30RT120_1&quot; &quot;WT_RT_2&quot;          &quot;WT_HS30_2&quot;       &#xA; [9] &quot;WT_HS30RT120_2&quot;   &quot;prg1_RT_2&quot;        &quot;prg1_HS30_2&quot;      &quot;prg1_HS30RT120_2&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I describe the experiments as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; col_data &amp;lt;- DataFrame(&#xA;    geno = c(rep(&quot;WT&quot;, times=3), rep(&quot;prg1&quot;, times=3), rep(&quot;WT&quot;, times=3), rep(&quot;prg1&quot;, times=3)),&#xA;    treat = rep(c(&quot;RT&quot;, &quot;HS30&quot;, &quot;HS30RT120&quot;), times=4),&#xA;    rep = c(rep(&quot;1&quot;, times=6), rep(&quot;2&quot;, times=6)),&#xA;    row.names = colnames(counts_data))&#xA;&amp;gt; col_data&#xA;DataFrame with 12 rows and 3 columns&#xA;                        geno       treat         rep&#xA;                 &amp;lt;character&amp;gt; &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;&#xA;WT_RT_1                   WT          RT           1&#xA;WT_HS30_1                 WT        HS30           1&#xA;WT_HS30RT120_1            WT   HS30RT120           1&#xA;prg1_RT_1               prg1          RT           1&#xA;prg1_HS30_1             prg1        HS30           1&#xA;...                      ...         ...         ...&#xA;WT_HS30_2                 WT        HS30           2&#xA;WT_HS30RT120_2            WT   HS30RT120           2&#xA;prg1_RT_2               prg1          RT           2&#xA;prg1_HS30_2             prg1        HS30           2&#xA;prg1_HS30RT120_2        prg1   HS30RT120           2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to build a DESeq2 object that I could use to either:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;find differentially expressed genes when the treatment varies for a given fixed genotype&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;or:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;find differentially expressed genes when the genotype varies for a given fixed treatment&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In the &lt;a href=&quot;https://support.bioconductor.org/p/74101/#74163&quot; rel=&quot;nofollow noreferrer&quot;&gt;bioconductor help forum&lt;/a&gt; I think I've found a somewhat similar situation, and I read the following:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Try a design of ~ genotype + genotype:condition&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Then you will have a condition effect for each level of genotype, including the reference level.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;You can constrast pairs of them using the list style of the 'contrast' argument.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;However, this doesn't explain how to apply this &quot;list style&quot; to the &quot;contrast&quot; argument. And the above situation seems to be asymmetrical. By that I mean that genotype and condition do not seem to have an interchangeable role.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I tried the following more symmetric formula:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dds &amp;lt;- DESeqDataSetFromMatrix(&#xA;                                countData = counts_data,&#xA;                                colData = col_data,&#xA;                                design = ~ geno + treat + geno:treat)&#xA;&amp;gt; dds &amp;lt;- DESeq(dds)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now, can I for instance &lt;strong&gt;get the differential expression results when comparing treatment &quot;HS30&quot; against &quot;RT&quot; as a reference, in genotype &quot;prg1&quot;&lt;/strong&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And how?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I understand correctly, the above-mentioned &quot;list style&quot; uses names given by the &lt;code&gt;resultsNames&lt;/code&gt; function. In my case, I have the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; resultsNames(dds)&#xA;[1] &quot;Intercept&quot;               &quot;geno_WT_vs_prg1&quot;        &#xA;[3] &quot;treat_HS30RT120_vs_HS30&quot; &quot;treat_RT_vs_HS30&quot;       &#xA;[5] &quot;genoWT.treatHS30RT120&quot;   &quot;genoWT.treatRT&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I guess I would need a contrast between &quot;genoprg1.treatRT&quot; and a &quot;genoprg1.treatHS30&quot;, but these are not in the above results names.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm lost.&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-06-02T16:04:30.200" LastActivityDate="2017-06-02T16:04:30.200" Title="Understanding DESeq2 design, contrast and results" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;deseq2&gt;&lt;differential-expression&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="154" PostTypeId="2" ParentId="153" CreationDate="2017-05-19T14:51:01.017" Score="9" Body="&lt;p&gt;The simplest manner is to not use a wald test, but rather an LRT with a reduced model lacking the factor of interest:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dds = DESeq(dds, test=&quot;LRT&quot; reduced=~geno+geno:Treatment)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The above would give you results for Treatment regardless of level while still accounting for a possible interaction (i.e., a &quot;main effect of treatment, regardless of the type of treatment&quot;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an aside, this is probably a case where the edgeR-preferred way of creating groups of genotype-treatment combinations and then using a model of &lt;code&gt;~0+group&lt;/code&gt; might make your life a bit easier. You'll get the same results (more or less) regardless, but it'll probably be easier for you to think in those terms rather than remembering that the base level will be &lt;code&gt;treatment HS30&lt;/code&gt; and &lt;code&gt;geno prg1&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-19T14:51:01.017" CommentCount="2" />
  <row Id="155" PostTypeId="2" ParentId="134" CreationDate="2017-05-19T17:44:29.193" Score="6" Body="&lt;p&gt;According to the &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMv1.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;SAM specification&lt;/a&gt;, the 3rd field of a SAM line (&lt;code&gt;RNAME&lt;/code&gt;) is:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;RNAME: Reference sequence NAME of the alignment. If @SQ header lines&#xA;  are present, RNAME (if not ‘*’) must be present in one of the SQ-SN&#xA;  tag. An unmapped segment without coordinate has a ‘*’ at this field.&#xA;  However, an unmapped segment may also have an ordinary coordinate such&#xA;  that it can be placed at a desired position after sorting. If RNAME is&#xA;  ‘*’, no assumptions can be made about POS and CIGAR.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And the 7th field is (emphasis mine, missing &quot;to&quot; theirs):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;RNEXT: Reference sequence name of the primary alignment of the NEXT&#xA;  read in the template. For the last read, the next read is the first&#xA;  read in the template. If @SQ header lines are present, RNEXT (if not&#xA;  ‘*’ or ‘=’) must be present in one of the SQ-SN tag. This field is set&#xA;  as ‘*’ when the information is unavailable, &lt;strong&gt;and set as ‘=’ if RNEXT is&#xA;  identical RNAME&lt;/strong&gt;. If not ‘=’ and the next read in the template has one&#xA;  primary mapping (see also bit 0x100 in FLAG), this field is identical&#xA;  to RNAME at the primary line of the next read. If RNEXT is ‘*’, no&#xA;  assumptions can be made on PNEXT and bit 0x20&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So, you want to remove those lines whose 7th field isn't &lt;code&gt;=&lt;/code&gt; and, just in case, those lines whose 7th field isn't &lt;code&gt;=&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; isn't the same as the 3rd field. You can therefore use something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view -L test.bed test.bam | awk '$7==&quot;=&quot; || $3==$7&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And, to save as a bam file again:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view -L test.bed test.bam | awk '$7==&quot;=&quot; &amp;amp;&amp;amp; $3==$7 | &#xA;    samtolls view -b &amp;gt; fixed.bam&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;On a separate note, there's very rarely a need to chain multiple grep commands like that. You can just use &lt;code&gt;\|&lt;/code&gt; (or &lt;code&gt;|&lt;/code&gt; with the &lt;code&gt;-E&lt;/code&gt; or &lt;code&gt;-P&lt;/code&gt; options) to separate them. Something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view -L test.bed test.bam | grep -v 'chrx\|chr2\|chr10\|chrN'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view -L test.bed test.bam | grep -Ev 'chrx|chr2|chr10|chrN'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-05-21T23:21:14.657" LastActivityDate="2017-05-21T23:21:14.657" CommentCount="1" />
  <row Id="156" PostTypeId="1" AcceptedAnswerId="158" CreationDate="2017-05-19T18:34:20.767" Score="15" ViewCount="80" Body="&lt;p&gt;Why do some assemblers like &lt;a href=&quot;https://github.com/aquaskyline/SOAPdenovo2&quot; rel=&quot;noreferrer&quot;&gt;SOAPdenovo2&lt;/a&gt; or &lt;a href=&quot;https://github.com/dzerbino/velvet&quot; rel=&quot;noreferrer&quot;&gt;Velvet&lt;/a&gt; require an odd-length kmer for the construction of De Brujin graph, while some other assemblers like &lt;a href=&quot;https://github.com/bcgsc/abyss&quot; rel=&quot;noreferrer&quot;&gt;ABySS&lt;/a&gt; are fine with even-length kmers?&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-05-22T09:29:36.113" LastActivityDate="2017-05-22T09:29:36.113" Title="Why do some assemblers require an odd-length kmer for the construction of De Brujin graphs?" Tags="&lt;ngs&gt;&lt;k-mer&gt;&lt;assembly&gt;&lt;de-bruijn&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="157" PostTypeId="1" AcceptedAnswerId="160" CreationDate="2017-05-19T18:50:05.307" Score="7" ViewCount="53" Body="&lt;p&gt;If there are soft clipped base pairs specified in the CIGAR string for a read in a SAM/BAM file, will these be used for variant calling in a &lt;a href=&quot;http://www.htslib.org/doc/samtools.html&quot; rel=&quot;noreferrer&quot;&gt;samtools&lt;/a&gt; + &lt;a href=&quot;http://www.htslib.org/doc/bcftools.html&quot; rel=&quot;noreferrer&quot;&gt;bcftools&lt;/a&gt; workflow? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_gatk_tools_walkers_haplotypecaller_HaplotypeCaller.php&quot; rel=&quot;noreferrer&quot;&gt;GATK HaplotypeCaller&lt;/a&gt;, for example, has an explicit option &lt;code&gt;--dontUseSoftClippedBases&lt;/code&gt; for whether to use soft clipped bases. The samtools documentation does not mention clipped bases. &lt;/p&gt;&#xA;" OwnerUserId="272" LastActivityDate="2017-05-19T20:15:11.303" Title="Are soft-clipped bases used for variant calling in samtools + bcftools?" Tags="&lt;bam&gt;&lt;sam&gt;&lt;samtools&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="158" PostTypeId="2" ParentId="156" CreationDate="2017-05-19T18:52:34.737" Score="15" Body="&lt;p&gt;From the &lt;a href=&quot;http://www.ebi.ac.uk/~zerbino/velvet/Manual.pdf&quot; rel=&quot;noreferrer&quot;&gt;manual of Velvet&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;it must be an odd number, to avoid palindromes. If you put in an even&#xA;  number, Velvet will just decrement it and proceed.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;the &lt;a href=&quot;https://en.wikipedia.org/wiki/Palindromic_sequence&quot; rel=&quot;noreferrer&quot;&gt;palindromes&lt;/a&gt; in biology are defined as reverse complementary sequences. The problem of palindromes is explained in this &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2874646/&quot; rel=&quot;noreferrer&quot;&gt;review&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Palindromes induce paths that fold back on themselves. At least one&#xA;  assembler avoids these elegantly; Velvet requires K, the length of a&#xA;  K-mer, to be odd. An odd-size K-mer cannot match its reverse&#xA;  complement.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It is possible to construct graph with palindromes, but then the interpretation will be harder. Allowing only graphs of odd kmers is just an elegant way how to avoid writing a code for interpretation of a more complicated graph.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-05-19T19:35:34.697" LastActivityDate="2017-05-19T19:35:34.697" CommentCount="1" />
  <row Id="159" PostTypeId="1" AcceptedAnswerId="163" CreationDate="2017-05-19T18:59:48.617" Score="3" ViewCount="36" Body="&lt;p&gt;Does anyone have experience generating pdb structures with Phyre and ITasser online tools. The results generated from each given the same amino acid sequence input are very different and I am wondering whether or not this is a usual experience. I know ITasser was the number 1 rated in the CASP trails, but should the results really be this disparate?&lt;/p&gt;&#xA;" OwnerUserId="65" LastEditorUserId="57" LastEditDate="2017-05-24T12:13:12.043" LastActivityDate="2017-05-24T18:17:03.157" Title="Phyre2 vs ITasser, completely different models generated" Tags="&lt;proteins&gt;&lt;homology-modelling&gt;&lt;protein-structure&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="160" PostTypeId="2" ParentId="157" CreationDate="2017-05-19T19:19:13.093" Score="8" Body="&lt;p&gt;No, samtools (and therefore bcftools) does not use soft-clipped bases. You can quickly confirm this by using either &lt;code&gt;samtools depth&lt;/code&gt; or &lt;code&gt;samtools mpileup&lt;/code&gt; to look at a region with a soft-clipped alignment. You'll note that the soft-clipped region isn't used in the depth/pileup (both tools use the same underlying code, so it doesn't matter which you use). If you're curious, samtools ignores soft-clipped bases because it's based on making a per-base stack of alignments covering each position. In the BAM format, alignments are sorted and assigned to bins according to their start/end positions, which won't include soft-clipping. Consequently, when samtools is making the pileup it won't even see the alignments that would overlap a given base if soft-clipped bases were included.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This then sort of begs the question of what GATK's HaplotypeCaller is doing differently. There, regions in the genome are essentially assembled in a small de Bruijn graph, which allows for soft-clipped bases around indels to then be resolved, given that the graph would start/end a little-way on past each side of indels. This is also why you don't need to do indel realignment with the HaplotypeCaller (this was needed in the old UnifiedGenotyper).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: For more details regarding the HaplotypeCaller, see &lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/article.php?id=4146&quot; rel=&quot;noreferrer&quot;&gt;this nice page&lt;/a&gt; on GATK's website, which goes into much more detail than I did here.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-05-19T20:15:11.303" LastActivityDate="2017-05-19T20:15:11.303" CommentCount="0" />
  <row Id="161" PostTypeId="1" AcceptedAnswerId="167" CreationDate="2017-05-20T00:21:08.950" Score="4" ViewCount="67" Body="&lt;p&gt;The peak calling tool MACS2 can call peaks in either narrow peak mode (for focused signals like transcription factor ChIPseq) or broad peak mode (for more defuse signals, like certain histone modifications). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The algorithm for narrow peak calling is well described in the MACS publication. But I don't find much documentation for how peak calling is different in broad peak mode. The manual only contains the following:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;--broad&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;When this flag is on, MACS will try to composite broad regions in&#xA;  BED12 ( a gene-model-like format ) by putting nearby highly enriched&#xA;  regions into a broad region with loose cutoff. The broad region is&#xA;  controlled by another cutoff through --broad-cutoff. The maximum&#xA;  length of broad region length is 4 times of d from MACS&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;But this doesn't really describe exactly how this is performed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So what is the algorithm MACS uses for calling broad peaks?&lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-05-24T10:01:42.090" Title="How are MACS2's narrow peak and broad peak algorithms different?" Tags="&lt;algorithms&gt;&lt;chip-seq&gt;&lt;macs2&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="163" PostTypeId="2" ParentId="159" CreationDate="2017-05-20T04:11:29.220" Score="3" Body="&lt;p&gt;I'm less familiar with Phyre, but I-TASSER is a really sophisticated system that takes the results of a search using multiple threaders and plugs them into an ab initio simulation which tries to minimize the energy of the models by sampling many possible 3D conformations, which I don't think Phyre does.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/I-TASSER#/media/File:I-TASSER-pipeline.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://en.wikipedia.org/wiki/I-TASSER#/media/File:I-TASSER-pipeline.jpg&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Compare with a similar workflow schematic for Phyre:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.nature.com/nprot/journal/v10/n6/images/nprot.2015.053-F1.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.nature.com/nprot/journal/v10/n6/images/nprot.2015.053-F1.jpg&lt;/a&gt;&#xA;&lt;a href=&quot;http://www.nature.com/nprot/journal/v10/n6/fig_tab/nprot.2015.053_F2.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.nature.com/nprot/journal/v10/n6/fig_tab/nprot.2015.053_F2.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Structure prediction still has a long way to go, and you'll always get better results if there are close homologues available in the PDB, but given the consistent high performance of I-TASSER in CASP I would treat those results as more significant. That said, it can't hurt to consider multiple answers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: included blmoore's link to second half of Phyre protocol schematic&lt;/p&gt;&#xA;" OwnerUserId="138" LastEditorUserId="138" LastEditDate="2017-05-24T18:17:03.157" LastActivityDate="2017-05-24T18:17:03.157" CommentCount="1" />
  <row Id="164" PostTypeId="2" ParentId="120" CreationDate="2017-05-20T08:33:58.813" Score="2" Body="&lt;p&gt;In case if you are really dedicated to obtain perfect results, you can use strategy described &lt;a href=&quot;http://www.nature.com/nature/journal/v526/n7571/full/nature15394.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;there&lt;/a&gt;, in 1000GP 3rd Phase SV detection paper - use these tools, validate your calls with IRS test, merge calls into one callset. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you do not wanna spend thousands human-hours as was spent during this paper preparation, from my experience, it is better to use 1 paired-end insert distance method and one read-depth based method. Each of them cover &quot;different&quot; regions in the genome. (even thou they have huge overlap, paired-end detection requires both SV breakpoints to be located within the regions with good mappability which is not always the case, but resolution of read-depth methods is lower in general, paired-ends works well for deletions/tandem duplications/inversions, but have troubles with non-tandem duplications).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hope it helps.&lt;/p&gt;&#xA;" OwnerUserId="349" LastActivityDate="2017-05-20T08:33:58.813" CommentCount="2" />
  <row Id="165" PostTypeId="1" CreationDate="2017-05-20T11:31:59.123" Score="5" ViewCount="73" Body="&lt;p&gt;Whether a module is complete can easily be checked by evaluating the &lt;code&gt;Definition&lt;/code&gt; entry associated with the module; e.g. in module &lt;a href=&quot;http://www.genome.jp/kegg-bin/show_module?M00010&quot; rel=&quot;noreferrer&quot;&gt;M00010&lt;/a&gt;, it is given by&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Definition  K01647 (K01681,K01682) (K00031,K00030)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which can be translated to&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;K01647 AND (K01681 OR K01682) AND (K00031 OR K00030)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If this expression evaluates to &lt;code&gt;TRUE&lt;/code&gt;, the module is complete.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am wondering whether analogue information exists for a single reaction. So e.g. for &lt;a href=&quot;http://www.genome.jp/dbget-bin/www_bget?rn:R00352&quot; rel=&quot;noreferrer&quot;&gt;R00352&lt;/a&gt;, one finds the following information about Orthology:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9jRBq.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9jRBq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But how do I now know in which logical relation the KOs are?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So it could be      &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;K01648 AND K15230 AND K15231 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;K01648 OR K15230 OR K15231&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;K01648 OR (K15230 AND K15231)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the above example, the correct expression would be:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;K01648 OR (K15230 AND K15231)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;One either needs &lt;code&gt;K01648&lt;/code&gt; or the other two subunits together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can this information be retrieved from KEGG for each reaction and if so, how?&lt;/p&gt;&#xA;" OwnerUserId="351" LastActivityDate="2017-05-20T11:31:59.123" Title="How to retrieve logical expression (KO based) for reactions from KEGG?" Tags="&lt;kegg&gt;&lt;orthology&gt;&lt;database&gt;&lt;data-retrieval&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="166" PostTypeId="2" ParentId="149" CreationDate="2017-05-20T20:24:45.047" Score="5" Body="&lt;p&gt;According to the &lt;a href=&quot;http://biorxiv.org/content/early/2016/06/20/060012&quot; rel=&quot;nofollow noreferrer&quot;&gt;FGSEA preprint&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;We ran reference GSEA with default parameters. The permutation number&#xA;  was set to 1000, which means that for each input gene set 1000&#xA;  independent samples were generated. The run took 100 seconds and&#xA;  resulted in 79 gene sets with GSEA-adjusted FDR q-value of less than&#xA;  10−2. All significant gene sets were in a positive mode. First, to get&#xA;  a similar nominal p-values accuracy we ran FGSEA algorithm on 1000&#xA;  permutations. This took 2 seconds, but resulted in no significant hits&#xA;  due after multiple testing correction (with FRD ≤ 1%).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Thus, FGSEA and GSEA are not identical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And again in the conclusion:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Consequently, gene sets can be ranked more precisely in the results&#xA;  and, which is even more important, standard multiple testing&#xA;  correction methods can be applied instead of approximate ones as in&#xA;  [GSEA].&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The author argues that FGSEA is more accurate, so it can't be equivalent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are interested specifically in the enrichment score, that was &lt;a href=&quot;http://disq.us/p/1f683mc&quot; rel=&quot;nofollow noreferrer&quot;&gt;addressed by the author in the preprint comments&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Values of enrichment scores and normalized enrichment scores are the&#xA;  same for both broad version and fgsea.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So that part seems to be the same.&lt;/p&gt;&#xA;" OwnerUserId="35" LastEditorUserId="35" LastEditDate="2017-05-21T19:52:04.773" LastActivityDate="2017-05-21T19:52:04.773" CommentCount="2" />
  <row Id="167" PostTypeId="2" ParentId="161" CreationDate="2017-05-20T21:38:54.743" Score="8" Body="&lt;p&gt;The key function is &lt;a href=&quot;https://github.com/taoliu/MACS/blob/master/MACS2/IO/CallPeakUnit.pyx#L1443&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;call_broadpeaks&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The description attached to the function says:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;This function try to find enriched regions within which, scores are&#xA;  continuously higher than a given cutoff for level 1, and link them&#xA;  using the gap above level 2 cutoff with a maximum length of&#xA;  lvl2_max_gap.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;scoring_function_s: symbols of functions to calculate&#xA;  score. 'p' for pscore, 'q' for qscore, 'f' for fold change, 's' for&#xA;  subtraction. for example: ['p', 'q']&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;lvl1_cutoff_s:  list of cutoffs&#xA;  at highly enriched regions, corresponding to scoring functions.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;lvl2_cutoff_s:  list of cutoffs at less enriched regions,&#xA;  corresponding to scoring functions.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;min_length :minimum peak length,&#xA;  default 200.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;lvl1_max_gap   :  maximum gap to merge nearby enriched&#xA;  peaks, default 50.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;lvl2_max_gap   :  maximum length of linkage&#xA;  regions, default 400.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Return both general PeakIO object for highly&#xA;  enriched regions and gapped broad regions in BroadPeakIO.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;To give some basic explanation, the algorithm (briefly) appears to be as follows:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Two separate levels of peaks are called, level 1 (a higher pval, ie more significant) and level 2 (a lower pval). Level 1 is controlled by &lt;code&gt;-p&lt;/code&gt; and level 2 is controlled by &lt;code&gt;--broad-cutoff&lt;/code&gt;. When each peakset is called, they are immediately linked by the max gap parameter for each set.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Then, assuming that all level 1 peaks should be inside level 2 peaks (this is an explicit assumption by MACS2), the algorithm groups level 1 peaks inside level 2 peaks to output a broad peak.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This has a few implications:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The broad peak calls really come from the level 2 peaks alone (+ linking). The level 1 peak calls allow you to distinguish sub peaks (so that you can have gapped peaks).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Aside from the linking, the broad peak calls would be the same as narrow peak calls, if you called both with the same pval threshold (for example, if you set &lt;code&gt;--broad-cutoff 0.1&lt;/code&gt; in broad peak mode, and the &lt;code&gt;-p 0.1&lt;/code&gt; for narrow peak mode)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="13" LastEditorUserId="29" LastEditDate="2017-05-24T10:01:42.090" LastActivityDate="2017-05-24T10:01:42.090" CommentCount="2" />
  <row Id="168" PostTypeId="1" AcceptedAnswerId="175" CreationDate="2017-05-20T21:49:51.210" Score="8" ViewCount="100" Body="&lt;p&gt;I'm looking for tools to check the quality of a VCF I have of a human genome. I would like to check the VCF against publicly known variants across other human genomes, e.g. how many SNPs are already in public databases, whether insertions/deletions are at known positions, insertion/deletion length distribution, other SNVs/SVs, etc.? I suspect that there are resources from previous projects to check for known SNPs and InDels by human subpopulations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What resources exist for this, and how do I do it? &lt;/p&gt;&#xA;" OwnerUserId="146" LastActivityDate="2017-05-21T14:48:39.303" Title="Given a VCF of a human genome, how do I assess the quality against known SNVs?" Tags="&lt;vcf&gt;&lt;snv&gt;&lt;public-databases&gt;&lt;variants&gt;&lt;structural-variation&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" />
  <row Id="169" PostTypeId="2" ParentId="168" CreationDate="2017-05-21T01:12:08.377" Score="4" Body="&lt;p&gt;The greatest &lt;strong&gt;protein&lt;/strong&gt; coding variant catalogue is definitely &lt;a href=&quot;http://www.nature.com/nature/journal/v536/n7616/full/nature19057.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;ExAC&lt;/a&gt; (&gt;65k individuals). They also published a &lt;a href=&quot;https://macarthurlab.org/2016/03/17/reproduce-all-the-figures-a-users-guide-to-exac-part-2/&quot; rel=&quot;nofollow noreferrer&quot;&gt;blogpost&lt;/a&gt; where they describe how to reproduce figures in the paper (it is a good start how to get familiar with the dataset).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the &lt;strong&gt;whole-genome&lt;/strong&gt; variants I would look at the data created by &lt;a href=&quot;http://www.internationalgenome.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;1000 genomes&lt;/a&gt; project (the latest release has more than 3k individuals). The integrated variant call sets can be downloaded though the &lt;a href=&quot;http://www.internationalgenome.org/data-portal/data-collection/phase-3&quot; rel=&quot;nofollow noreferrer&quot;&gt;portal&lt;/a&gt; and catalogue of SVs can be found &lt;a href=&quot;http://www.internationalgenome.org/phase-3-structural-variant-dataset&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In &lt;a href=&quot;http://www.nature.com/nature/journal/v526/n7571/full/nature15394.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;this&lt;/a&gt; paper (also 1000 genomes project) they speak about non-precise placement of SVs by SV callers. I would keep this in mind for the comparison of your genome to the known variants.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-05-21T11:13:48.910" LastActivityDate="2017-05-21T11:13:48.910" CommentCount="2" />
  <row Id="170" PostTypeId="2" ParentId="168" CreationDate="2017-05-21T05:18:15.573" Score="4" Body="&lt;p&gt;Your best bet is to use programs that provide you an complete annotation of variants present in your VCF. Two examples are &lt;a href=&quot;http://snpeff.sourceforge.net&quot; rel=&quot;nofollow noreferrer&quot;&gt;snpEff&lt;/a&gt; and &lt;a href=&quot;http://annovar.openbioinformatics.org/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Annovar&lt;/a&gt;. These programs work on known variants deem different sources and provide you with information on each item in your file, that you can filter after to try to understand the effects of each variant.&lt;/p&gt;&#xA;" OwnerUserId="280" LastActivityDate="2017-05-21T05:18:15.573" CommentCount="1" />
  <row Id="171" PostTypeId="2" ParentId="112" CreationDate="2017-05-21T07:27:58.337" Score="4" Body="&lt;p&gt;First of all, kudos to you for taking versioning seriously. The fact that you're mindful of this issue is a good sign that you want to do responsible research!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For many bioinformatics projects, data files are so large that versioning the data directly with a tool like git is impractical. But your question is really getting at a couple of different issues.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;How do I do my research reproducibly, and show full provenance for each data point and result I produce?&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;How do I manage and synchronize my research work across multiple machines?&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The short answer&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Archive the primary data.&lt;/li&gt;&#xA;&lt;li&gt;Place your workflow under version control.&lt;/li&gt;&#xA;&lt;li&gt;Version checksums of large data files.&lt;/li&gt;&#xA;&lt;li&gt;Use GitHub to synchronize your workflow between machines.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The long answer&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Archive the primary data&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;As far as reproducibility is concerned, what matters most is the primary data: the raw, unprocessed data you collect from the instrument. If you are analyzing data that has been published by others, then write a script that will automate the task of downloading the data from its primary official source, and place that script under version control.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you or a lab mate or a colleague produced the data and it is not yet published, then you should already have plans for submitting it to an archive. Indeed, most journals and funding agencies now require this prior to publication. I'd even go so far as to say the data should be submitted as soon as it's collected. Scientists worry a lot about having their data stolen and their ideas scooped, but statistically speaking, getting scooped is much less likely than nobody ever touching your data or reading your paper. But if you or an advisor insists, most data archives allow you to keep data private for an extended period of time until a supporting manuscript is published.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Putting (for example) Fastq files in a git repository is a bad idea for a lot of reasons. No hosting service will support files that big, git will be very slow with files that big, but most importantly git/GitHub is not archival! Use a proper data archive!&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Place your workflow under version control&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Treat your raw data as read-only. Only process the raw data using scripts, and keep these scripts under version control. Vince Buffalo describes this well in his book &lt;a href=&quot;http://shop.oreilly.com/product/0636920030157.do&quot; rel=&quot;nofollow noreferrer&quot;&gt;Bioinformatics Data Skills&lt;/a&gt;. Check it out!&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Version checksums of large data files&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;If there are any data files that you want to track but are too big to place under version control, compute checksums and place &lt;em&gt;these&lt;/em&gt; under version control. Checksums are very small alphanumeric strings that are, for all practical purposes, unique for each data file. So instead of putting that 5GB trimmed Fastq file or the 7GB BAM file under version control, compute their checksums and put the &lt;em&gt;checksums&lt;/em&gt; under version control. The checksums won't tell you the contents of your files, but they can tell you when the file contents change.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This should give full disclosure and complete provenance for every data point in your analysis. The workflow has a scripts/command for downloading the primary data, scripts/commands for processing the data, and checksums that serve as a signature to validate intermediate and final output files. With this, anyone should be able to reproduce you analysis!&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Use GitHub to synchronize your workflow between machines&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;If your workflow is already under version control with git, it's trivial to push this to a hosting service like GitHub, GitLab, or BitBucket. Then it's just a matter of using &lt;code&gt;git push&lt;/code&gt; and &lt;code&gt;git pull&lt;/code&gt; to keep your code up-to-date on your various machines.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-05-21T07:27:58.337" CommentCount="0" />
  <row Id="172" PostTypeId="2" ParentId="97" CreationDate="2017-05-21T07:43:43.940" Score="2" Body="&lt;p&gt;Depending on the coverage of your data and the complexity of the genome, you could either reassemble the genome &lt;em&gt;de novo&lt;/em&gt; or run a reference guided (or reference assisted) assembly. It sounds like you're leaning more towards the latter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are a couple of reference-guided assembly tools available: &lt;a href=&quot;https://doi.org/10.1093/bioinformatics/btu291&quot; rel=&quot;nofollow noreferrer&quot;&gt;AlignGraph&lt;/a&gt; and &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/24931998&quot; rel=&quot;nofollow noreferrer&quot;&gt;Ragout&lt;/a&gt;. These may or may not be appropriate depending on the organism of interest and your data types. For example, these tools are very unlikely to work well on Oxford Nanopore reads which have not been error corrected using &lt;a href=&quot;https://github.com/jts/nanopolish&quot; rel=&quot;nofollow noreferrer&quot;&gt;Nanopolish&lt;/a&gt; or &lt;a href=&quot;http://canu.readthedocs.io/en/latest/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Canu -correct&lt;/a&gt;. &lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="163" LastEditDate="2017-05-21T21:36:50.493" LastActivityDate="2017-05-21T21:36:50.493" CommentCount="1" />
  <row Id="173" PostTypeId="2" ParentId="149" CreationDate="2017-05-21T10:41:33.910" Score="2" Body="&lt;p&gt;I found that the author responded that in the &lt;a href=&quot;http://disq.us/p/1f7wolq&quot; rel=&quot;nofollow noreferrer&quot;&gt;discussion&lt;/a&gt; in the preprint, &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;I wonder is the enrichment score calculated the same way as in broad gsea?&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Values of enrichment scores and normalized enrichment scores are the same for both broad version and fgsea.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-05-21T10:41:33.910" CommentCount="0" />
  <row Id="174" PostTypeId="1" CreationDate="2017-05-21T11:49:58.693" Score="11" ViewCount="73" Body="&lt;p&gt;All the long-read sequencing platforms are based on single-molecule sequencing which causes higher per-base error rates. For this reason a polishing step was added to genome assembly pipelines - mapping raw reads back to assembly and correcting details of the assembly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have decent PacBio RSII dataset of single individual genome of heavily heterozygous non-model species. Assembly went well, but when I tried to polish the assembly using &lt;a href=&quot;https://github.com/PacificBiosciences/GenomicConsensus&quot; rel=&quot;nofollow noreferrer&quot;&gt;quiver&lt;/a&gt; it could not converge over a couple of iterations and I bet it is because of too great divergence of haplotypes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any other way to polish a genome with such properties?&#xA;For instance, is there a way to separate long reads by haplotype, so I could polish using one haplotype only?&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="73" LastEditDate="2017-05-22T09:28:42.417" LastActivityDate="2017-05-23T14:18:02.627" Title="How to deal with heterozygosity during polishing of genome assembly based on long reads?" Tags="&lt;ngs&gt;&lt;assembly&gt;&lt;long-reads&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="175" PostTypeId="2" ParentId="168" CreationDate="2017-05-21T14:48:39.303" Score="6" Body="&lt;p&gt;To achieve (at least some of) your goals, I would recommend the &lt;a href=&quot;http://grch37.ensembl.org/Homo_sapiens/Tools/VEP&quot; rel=&quot;noreferrer&quot;&gt;Variant Effect Predictor (VEP)&lt;/a&gt;. It is a flexible tool that provides several types of annotations on an input .vcf file.  I agree that ExAC is the &lt;em&gt;de facto&lt;/em&gt; gold standard catalog for human genetic variation in coding regions.  To see the frequency distribution of variants by global subpopulation make sure &quot;ExAC allele frequencies&quot; is checked in addition to the 1000 genomes. &lt;a href=&quot;https://i.stack.imgur.com/zDUTT.jpg&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/zDUTT.jpg&quot; alt=&quot;VEP ExAC&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Output in the web-browser:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/0OCCY.jpg&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/0OCCY.jpg&quot; alt=&quot;VEP_ExAC_res&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you download the annotated .vcf, frequencies will be in the &lt;code&gt;INFO&lt;/code&gt; field:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;##INFO=&amp;lt;ID=CSQ,Number=.,Type=String,Description=&quot;Consequence annotations from Ensembl VEP. Format: Allele|Consequence|IMPACT|SYMBOL|Gene|Feature_type|Feature|BIOTYPE|EXON|INTRON|HGVSc|HGVSp|cDNA_position|CDS_position|Protein_position|Amino_acids|Codons|Existing_variation|DISTANCE|STRAND|FLAGS|SYMBOL_SOURCE|HGNC_ID|TSL|SIFT|PolyPhen|AF|AFR_AF|AMR_AF|EAS_AF|EUR_AF|SAS_AF|AA_AF|EA_AF|ExAC_AF|ExAC_Adj_AF|ExAC_AFR_AF|ExAC_AMR_AF|ExAC_EAS_AF|ExAC_FIN_AF|ExAC_NFE_AF|ExAC_OTH_AF|ExAC_SAS_AF|CLIN_SIG|SOMATIC|PHENO|MOTIF_NAME|MOTIF_POS|HIGH_INF_POS|MOTIF_SCORE_CHANGE&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The previously mentioned Annovar can also &lt;a href=&quot;http://doc-openbio.readthedocs.io/projects/annovar/en/latest/user-guide/filter/#exac-annotations&quot; rel=&quot;noreferrer&quot;&gt;annotate with ExAC allele frequencies&lt;/a&gt;.  Finally, should mention the newest whole-genome resource, &lt;a href=&quot;http://gnomad.broadinstitute.org/&quot; rel=&quot;noreferrer&quot;&gt;gnomAD&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="84" LastActivityDate="2017-05-21T14:48:39.303" CommentCount="2" />
  <row Id="176" PostTypeId="2" ParentId="98" CreationDate="2017-05-21T17:47:47.147" Score="6" Body="&lt;p&gt;I wrote a program, &lt;a href=&quot;http://asciigenome.readthedocs.io/en/latest/description.html&quot; rel=&quot;noreferrer&quot;&gt;ASCIIGenome&lt;/a&gt;, that I find handy in cases where you want to have a quick look at genomic data. It's a genome browser for the command line. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;To view only reads containing mismatches you can use the internal function &lt;code&gt;awk&lt;/code&gt;. To filter for reads where the NM tag (number of mismatches) is &gt;0:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ASCIIGenome -fa genome.fa aln.bam&#xA;...&#xA;&#xA;[h] for help: awk 'getSamTag(&quot;NM&quot;) &amp;gt; 0'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The view on terminal screen may look something like this:&lt;a href=&quot;https://i.stack.imgur.com/qXKkp.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/qXKkp.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similarly, to get only reads containing indels you can use &lt;code&gt;awk '$6 ~ &quot;D|I&quot;'&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hope this helps and feel free to report bugs &amp;amp; issues.&lt;/p&gt;&#xA;" OwnerUserId="339" LastActivityDate="2017-05-21T17:47:47.147" CommentCount="1" />
  <row Id="177" PostTypeId="2" ParentId="174" CreationDate="2017-05-22T03:36:11.637" Score="2" Body="&lt;p&gt;A few possibilities:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Falcon&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Try falcon and falcon-unzip. These are designed exactly for your problem and your data: &lt;a href=&quot;https://github.com/PacificBiosciences/FALCON&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/PacificBiosciences/FALCON&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Not Falcon&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you think you have assembled haplotypes (which seems reasonable to expect given enough coverage), you should be able to see the two haplotypes by just doing all pairwise alignments of your contigs. Haplotypes should show up as pairs of contigs that are MUCH more similar (even with a lot of between-haplotype divergence) than other pairs. Once you have all such pairs, you can simply select one of each pair to polish.&lt;/p&gt;&#xA;" OwnerUserId="156" LastActivityDate="2017-05-22T03:36:11.637" CommentCount="1" />
  <row Id="178" PostTypeId="5" CreationDate="2017-05-22T06:32:44.180" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T06:32:44.180" LastActivityDate="2017-05-22T06:32:44.180" CommentCount="0" />
  <row Id="179" PostTypeId="4" CreationDate="2017-05-22T06:32:44.180" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T06:32:44.180" LastActivityDate="2017-05-22T06:32:44.180" CommentCount="0" />
  <row Id="180" PostTypeId="5" CreationDate="2017-05-22T06:33:31.493" Score="0" Body="&lt;p&gt;Bioconductor is &quot;is a free, open source and open development software project for the analysis and comprehension of genomic data generated by wet lab experiments in molecular biology.&quot; (&lt;a href=&quot;https://en.wikipedia.org/wiki/Bioconductor&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikipedia&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Learn more on the project's &lt;a href=&quot;https://www.bioconductor.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;home page&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="559" LastEditorUserId="559" LastEditDate="2017-06-03T18:40:03.477" LastActivityDate="2017-06-03T18:40:03.477" CommentCount="0" />
  <row Id="181" PostTypeId="4" CreationDate="2017-05-22T06:33:31.493" Score="0" Body="Bioconductor provides tools for the analysis and comprehension of high-throughput genomic data in the R language." OwnerUserId="131" LastEditorUserId="131" LastEditDate="2017-05-25T14:11:27.163" LastActivityDate="2017-05-25T14:11:27.163" CommentCount="0" />
  <row Id="182" PostTypeId="5" CreationDate="2017-05-22T06:38:11.750" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T06:38:11.750" LastActivityDate="2017-05-22T06:38:11.750" CommentCount="0" />
  <row Id="183" PostTypeId="4" CreationDate="2017-05-22T06:38:11.750" Score="0" Body="ngs (next generation sequencing) is a term refering to all high-throughput nucleotide sequencing methods." OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-04T19:17:01.873" LastActivityDate="2017-06-04T19:17:01.873" CommentCount="0" />
  <row Id="184" PostTypeId="5" CreationDate="2017-05-22T06:42:42.650" Score="0" Body="&lt;p&gt;BED is a file format that has been adapted for a variety of other more generic genome annotation use cases.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://genome.ucsc.edu/FAQ/FAQformat.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;UCSC description&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.ensembl.org/info/website/upload/bed.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Ensembl description&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="131" LastEditorUserId="37" LastEditDate="2017-06-25T02:12:47.427" LastActivityDate="2017-06-25T02:12:47.427" CommentCount="0" />
  <row Id="185" PostTypeId="4" CreationDate="2017-05-22T06:42:42.650" Score="0" Body="Browser Extensible Data, a collection of related plain text formats for describing genome features for visualization with genome browsers." OwnerUserId="131" LastEditorUserId="96" LastEditDate="2017-06-15T13:53:14.837" LastActivityDate="2017-06-15T13:53:14.837" CommentCount="0" />
  <row Id="186" PostTypeId="5" CreationDate="2017-05-22T06:43:49.197" Score="0" Body="&lt;p&gt;VCF, acronym for Variant Calling Format, is a text file type used to store information regarding genetic variants. Use this tag for questions related to the use of this file type.&lt;/p&gt;&#xA;" OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-02T18:25:20.317" LastActivityDate="2017-08-02T18:25:20.317" CommentCount="0" />
  <row Id="187" PostTypeId="4" CreationDate="2017-05-22T06:43:49.197" Score="0" Body="VCF, acronym for Variant Calling Format, is a text file type used to store information regarding genetic variants." OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-02T18:25:24.307" LastActivityDate="2017-08-02T18:25:24.307" CommentCount="0" />
  <row Id="188" PostTypeId="5" CreationDate="2017-05-22T06:46:31.737" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T06:46:31.737" LastActivityDate="2017-05-22T06:46:31.737" CommentCount="0" />
  <row Id="189" PostTypeId="4" CreationDate="2017-05-22T06:46:31.737" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T06:46:31.737" LastActivityDate="2017-05-22T06:46:31.737" CommentCount="0" />
  <row Id="190" PostTypeId="2" ParentId="174" CreationDate="2017-05-22T08:12:38.290" Score="2" Body="&lt;p&gt;You could also have a go at &lt;a href=&quot;https://github.com/marbl/canu/releases&quot; rel=&quot;nofollow noreferrer&quot;&gt;Canu&lt;/a&gt;. It's designed for long-read assembly (both PacBio and Nanopore), although not specifically for complex population sequencing. It tries to strip a genome down into its unique components, and generates paths from those components that are well-supported from the reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With regards to polishing, it seems to be the case that polishing doesn't converge, and there will be a lot of variants that just oscillate between two possibilities. For me and at least one other person at London Calling this year, there was basically no gain in accuracy for polishing past the third iteration. I used my own error correction algorithm, but they used the more &quot;standard&quot; polishing with Pilon. For what it's worth, the nanopore WGS consortium used Racon for polishing their Canu assemblies.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-05-23T14:18:02.627" LastActivityDate="2017-05-23T14:18:02.627" CommentCount="2" />
  <row Id="191" PostTypeId="1" CreationDate="2017-05-22T10:47:44.397" Score="8" ViewCount="92" Body="&lt;p&gt;I have a huge amount of ~20x human WGS samples, aligned, and all SNVs that were called with GATK under standard germline parameters set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I need to do is to model SNVs Allele Frequency (AF) for different underlying Copy Numbers. I'd better provide a toy example. For particular genomic region X:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;If X is presented by &lt;strong&gt;2&lt;/strong&gt; copies for the particular samples, we expect AF to be super-close to 1 or to 0.5.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;If X is presented by &lt;strong&gt;4&lt;/strong&gt; copies, I expect any particular AF to be close to 0.25, 0.5, 0.75 or 1.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, I can use &lt;em&gt;Binomial Distribution&lt;/em&gt; for these purposes. However, as we know, the distribution is not exactly Binomial due to alignment/sequencing biases and the median AF for all heterozygous SNVs is more close to 0.48 but not to 0.5 as we would expect. Another thing: for high copy numbers we expect higher coverages. And GATK use several filters so I suppose that we will not see SNVs with AF like 0.125 (in case if the segment has ploidy 8) - despite the super high coverage there GATK may reject this &quot;weird&quot; AF.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have read several papers that model SNVs AFs (and I agree that Beta Binomial Distribution may be quite accurate), however, I was not convinced enough that I should use the particular modelling. From your experience (in case if you do SNVs calling), which probabilistic distribution should I use? How should I estimate parameters for each of them (should I expect for CN4 AF=0.5 more frequent than AF=0.75 or vice versa, how to estimate this from data)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPD:&lt;/strong&gt; For simplicity we can say that we have a lot of previously identified regions with ploidy different from CN2, and I can take these coordinates from &lt;a href=&quot;http://www.nature.com/ng/journal/v47/n3/full/ng.3200.html&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;. So I can use more or less &quot;supervised&quot; learning for parameters' estimation.&lt;/p&gt;&#xA;" OwnerUserId="349" LastEditorUserId="73" LastEditDate="2017-06-01T19:58:14.903" LastActivityDate="2017-06-04T01:17:18.017" Title="Expected allele frequency distribution of SNVs in real NGS data" Tags="&lt;snv&gt;&lt;modelling&gt;&lt;statistics&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="192" PostTypeId="2" ParentId="153" CreationDate="2017-05-22T12:05:18.343" Score="4" Body="&lt;p&gt;It seems that the &quot;combining factors&quot; trick described in part 3.3 of DESeq2 current &quot;vignette&quot; (as of may 2017) under the title &quot;Interaction&quot; is a way to access to the desired contrasts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems possible do do it directly when building the &lt;code&gt;colData&lt;/code&gt; and when calling &lt;code&gt;DESeqDataSetFromMatrix&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's add a combined &quot;geno&quot; and &quot;treat&quot; factors to the future &lt;code&gt;colData&lt;/code&gt; parameter:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; col_data$geno_treat &amp;lt;- as.factor(paste(col_data$geno, col_data$treat, sep=&quot;_&quot;))&#xA;&amp;gt; col_data&#xA;DataFrame with 12 rows and 4 columns&#xA;                        geno       treat         rep     geno_treat&#xA;                 &amp;lt;character&amp;gt; &amp;lt;character&amp;gt; &amp;lt;character&amp;gt;       &amp;lt;factor&amp;gt;&#xA;WT_RT_1                   WT          RT           1          WT_RT&#xA;WT_HS30_1                 WT        HS30           1        WT_HS30&#xA;WT_HS30RT120_1            WT   HS30RT120           1   WT_HS30RT120&#xA;prg1_RT_1               prg1          RT           1        prg1_RT&#xA;prg1_HS30_1             prg1        HS30           1      prg1_HS30&#xA;...                      ...         ...         ...            ...&#xA;WT_HS30_2                 WT        HS30           2        WT_HS30&#xA;WT_HS30RT120_2            WT   HS30RT120           2   WT_HS30RT120&#xA;prg1_RT_2               prg1          RT           2        prg1_RT&#xA;prg1_HS30_2             prg1        HS30           2      prg1_HS30&#xA;prg1_HS30RT120_2        prg1   HS30RT120           2 prg1_HS30RT120&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We can now use a design where differential expression will be explained by these combined factors:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dds &amp;lt;- DESeqDataSetFromMatrix(&#xA;    countData = counts_data,&#xA;    colData = col_data,&#xA;    design = ~ geno_treat)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We run the analysis:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dds &amp;lt;- DESeq(dds)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then we can query results for a particular contrast between such factor combinations. For instance, to have the results for the effect of treatment &quot;HS30&quot; against the reference state &quot;RT&quot; in genotype &quot;prg1&quot;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;res &amp;lt;- results(dds, contrast=c(&quot;geno_treat&quot;, &quot;prg1_HS30&quot;, &quot;prg1_RT&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-05-22T12:05:18.343" CommentCount="0" />
  <row Id="193" PostTypeId="1" AcceptedAnswerId="201" CreationDate="2017-05-22T14:51:27.550" Score="7" ViewCount="54" Body="&lt;p&gt;The results obtained by running the &lt;code&gt;results&lt;/code&gt; command from DESeq2 contain a &quot;baseMean&quot; column, which I assume is the mean across samples of the normalized counts for a given gene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I access the normalized counts proper?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried the following (continuing with the example used &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/192/292&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dds &amp;lt;- DESeqDataSetFromMatrix(countData = counts_data, colData = col_data, design = ~ geno_treat)&#xA;&amp;gt; dds &amp;lt;- DESeq(dds)&#xA;estimating size factors&#xA;estimating dispersions&#xA;gene-wise dispersion estimates&#xA;mean-dispersion relationship&#xA;final dispersion estimates&#xA;fitting model and testing&#xA;&amp;gt; res &amp;lt;- results(dds, contrast=c(&quot;geno_treat&quot;, &quot;prg1_HS30&quot;, &quot;prg1_RT&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is what I have for the first gene:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; res[&quot;WBGene00000001&quot;,]$baseMean&#xA;[1] 181.7862&#xA;&amp;gt; mean(assays(dds)$mu[&quot;WBGene00000001&quot;,])&#xA;[1] 231.4634&#xA;&amp;gt; mean(assays(dds)$counts[&quot;WBGene00000001&quot;,])&#xA;[1] 232.0833&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;assays(dds)$counts&lt;/code&gt; corresponds to the raw counts. &lt;code&gt;assays(dds)$mu&lt;/code&gt; seems to be a transformation of these counts approximately preserving their mean, but this mean is very different from the &quot;baseMean&quot; value, so these are likely not the normalized values.&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-05-24T10:17:43.520" Title="How can I extract normalized read count values from DESeq2 results?" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;deseq2&gt;&lt;normalization&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="194" PostTypeId="1" CreationDate="2017-05-22T15:02:28.473" Score="2" ViewCount="57" Body="&lt;p&gt;In a weighted gene co-expression network analysis (using &lt;a href=&quot;https://cran.r-project.org/package=WGCNA&quot; rel=&quot;nofollow noreferrer&quot;&gt;WGCNA&lt;/a&gt;), the soft-threshold power is recommended as a noise filtering. It consists on raising the correlation to a certain number. To decide this power the scale-free topology is estimated for some powers. The function to estimate this prints:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   Power SFT.R.sq  slope truncated.R.sq mean.k. median.k. max.k.&#xA;1      1   0.9300  3.110          0.996  2960.0    3060.0   3970&#xA;2      2   0.7510  1.010          0.964  1750.0    1780.0   2900&#xA;3      3   0.1730  0.258          0.806  1170.0    1150.0   2280&#xA;4      4   0.0942 -0.183          0.713   833.0     782.0   1870&#xA;5      5   0.3800 -0.463          0.777   623.0     559.0   1580&#xA;6      6   0.5350 -0.656          0.834   481.0     412.0   1360&#xA;7      7   0.6270 -0.797          0.872   381.0     312.0   1190&#xA;8      8   0.6870 -0.910          0.900   307.0     241.0   1050&#xA;9      9   0.7270 -1.000          0.918   252.0     189.0    936&#xA;10    10   0.7490 -1.080          0.928   210.0     150.0    841&#xA;11    12   0.7850 -1.190          0.948   150.0      98.0    693&#xA;12    14   0.8090 -1.280          0.958   111.0      65.9    582&#xA;13    16   0.8290 -1.360          0.968    84.0      45.6    497&#xA;14    18   0.8410 -1.410          0.973    65.2      32.1    429&#xA;15    20   0.8490 -1.450          0.977    51.6      23.0    375&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The recommendations of the &lt;a href=&quot;https://labs.genetics.ucla.edu/horvath/CoexpressionNetwork/Rpackages/WGCNA/faq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;FAQ&lt;/a&gt; indicate that a SFT.R.sq value should be above&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;0.8 for reasonable powers (less than 15 for unsigned or signed hybrid networks, and less than 30 for signed networks) &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and the mean connectivity below the hundreds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Others have used the power just as noise filtering without caring much about the scale-free topology fit. I would pick the first power even if the mean connectivity is in the order of thousands because the scale-free topology fit is pretty high, however the slope is puzzling me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How should the soft-threshold power be selected?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;Based on an example &lt;a href=&quot;https://area51.stackexchange.com/proposals/109245/bioinformatics/109254#109254&quot;&gt;question&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-06-08T15:27:39.173" Title="How to select a power for a scale-free topology network" Tags="&lt;statistics&gt;&lt;networks&gt;&lt;threshold&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="195" PostTypeId="1" AcceptedAnswerId="220" CreationDate="2017-05-22T16:14:17.213" Score="9" ViewCount="116" Body="&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; We're increasingly needing some way of storing lots of variant data associated with lots of subjects: think clinical trials and hospital patients, looking for disease-causing or relevant genes. A thousand subjects is where we'd start, there's talk of millions on the horizon. With various genomic medicine initiatives, this is likely a wider need.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; While there's plenty of platforms out there, it's a rapidly evolving field. It's difficult to get a feel for how (and if) they perform and how they line up against each other:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What's scalable and can handle a lot of data? What sort of limits?&lt;/li&gt;&#xA;&lt;li&gt;What's robust and not a teetering pile of hacked-together components?&lt;/li&gt;&#xA;&lt;li&gt;What has a large community behind it and is actually used widely?&lt;/li&gt;&#xA;&lt;li&gt;What makes for easy access and search from another service? (Commandline, REST or software APIs)&lt;/li&gt;&#xA;&lt;li&gt;What sort of variants they handle?&lt;/li&gt;&#xA;&lt;li&gt;What sort of parameters can be used in searching? &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Solutions I've seen so far:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;BigQ: used with i2b2, but its wider use is unclear&lt;/li&gt;&#xA;&lt;li&gt;OpenCGA: looks the most developed, but I've heard complaints about the size of data it spits out&lt;/li&gt;&#xA;&lt;li&gt;Using BigQuery over a Google Genomics db: doesn't seem to be a general solution&lt;/li&gt;&#xA;&lt;li&gt;Gemini: recommended but is it really scalable and accessible from other services?&lt;/li&gt;&#xA;&lt;li&gt;SciDb: a commercial general db&lt;/li&gt;&#xA;&lt;li&gt;Quince&lt;/li&gt;&#xA;&lt;li&gt;LOVD&lt;/li&gt;&#xA;&lt;li&gt;Adam&lt;/li&gt;&#xA;&lt;li&gt;Whatever platform DIVAS &amp;amp; RVD run on: which may not be freely available&lt;/li&gt;&#xA;&lt;li&gt;Several graphical / graph genome solutions: We (and most other people) are probably not dealing with graph genome data at the moment, but is this a possible solution?&lt;/li&gt;&#xA;&lt;li&gt;Roll your own: Frequently recommended but I'm sceptical this is a plausible solution for a large dataset.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Anyone with experience give a review or high-level guide to this platform space?&lt;/p&gt;&#xA;" OwnerUserId="377" LastEditorUserId="377" LastEditDate="2017-05-22T17:06:53.827" LastActivityDate="2017-05-22T22:13:52.937" Title="The state, limitations and comparisons of large variant stores" Tags="&lt;genomics&gt;&lt;variants&gt;&lt;human-genome&gt;&lt;data-management&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="2" />
  <row Id="196" PostTypeId="1" AcceptedAnswerId="217" CreationDate="2017-05-22T16:32:40.663" Score="10" ViewCount="89" Body="&lt;p&gt;I am the resident Bioinfo Geek in a hospital academic lab that routinely employs NGS as well as CyTOF and other large volume data producing technologies. I am sick of our current &quot;protocol&quot; for metadata collection and association with the final products (miriad excel sheets and a couple poorly designed RedCap DBs).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to implement a central structured, controlled datastore that will take care of this. I know that the interface to the technicians how will be inputing the data is crucial to its adoption, but this is not the focus of THIS particular question: &lt;strong&gt;Does there exist a schema or schema guidelines for this type of database?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would rather use a model that has been developed by people who know how to do this well. I know of BioSQL but it seems more geared towards full protein/nucleotide records like those found in uniprot or genbank. That is not what we have here. What I want is something similar to the system touched on in this preprint: &lt;a href=&quot;http://biorxiv.org/content/early/2017/05/10/136358&quot; rel=&quot;noreferrer&quot; title=&quot;Managing The Analysis Of High-Throughput Sequencing Data&quot;&gt;http://biorxiv.org/content/early/2017/05/10/136358&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively, can anyone provide links to where I might find relevant guidelines or supply personal advice?&lt;/p&gt;&#xA;" OwnerUserId="243" LastEditorUserId="243" LastEditDate="2017-05-22T17:01:44.230" LastActivityDate="2017-06-02T12:27:05.573" Title="Designing a lab NGS file database schema" Tags="&lt;ngs&gt;&lt;database&gt;&lt;schema&gt;&lt;sample-database&gt;" AnswerCount="3" CommentCount="2" FavoriteCount="2" />
  <row Id="197" PostTypeId="2" ParentId="196" CreationDate="2017-05-22T17:01:00.507" Score="9" Body="&lt;p&gt;The &lt;a href=&quot;http://ga4gh.org/&quot; rel=&quot;noreferrer&quot;&gt;Global Alliance for Genomics and Health&lt;/a&gt; has been working on the issue of representing sequencing data and metadata for storage and sharing for quite some time, though with mixed results.  They do offer a model and API for storing NGS data in their &lt;a href=&quot;https://github.com/ga4gh/ga4gh-schemas&quot; rel=&quot;noreferrer&quot;&gt;GitHub repository&lt;/a&gt;, but it can be a bit of a pain to get a high-level view.  I am not sure if any better representation of this exists elsewhere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can say from personal experience (having built over a dozen genomic databases), there is no ideal data model and storage best practices.  Genomic data comes in many shapes and sizes, and your needs are going to vary from every other organization, so what works for one bioinformatics group won't necessarily work for you.  The best thing to do is design and implement a model that will cover all of the data types in your workflow and downstream analyses you might do with the data and metadata.&lt;/p&gt;&#xA;" OwnerUserId="47" LastEditorUserId="47" LastEditDate="2017-05-22T18:08:04.757" LastActivityDate="2017-05-22T18:08:04.757" CommentCount="0" />
  <row Id="199" PostTypeId="2" ParentId="196" CreationDate="2017-05-22T18:04:31.807" Score="4" Body="&lt;p&gt;I agree that there is no ideal data model that is going to be stable for very long in a quick-moving field like genome informatics. Perhaps a schema-less (NoSQL or some other document-based system, such as &lt;a href=&quot;https://www.mongodb.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;MongoDB&lt;/a&gt;) database approach would work better? This gives you ultimate flexibility to attach whatever information is relevant to database entries you're adding to your database now, without the need to rebuild the database later if you want to attach more/different information to subsequent database entries.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-05-22T18:04:31.807" CommentCount="0" />
  <row Id="200" PostTypeId="2" ParentId="1" CreationDate="2017-05-22T18:11:10.693" Score="4" Body="&lt;p&gt;In all seriousness, the most efficient way to store DNA sequence data is...you guessed it...in DNA. &lt;a href=&quot;http://dx.doi.org/10.1126/science.1226355&quot; rel=&quot;nofollow noreferrer&quot;&gt;(Church, Gao, and Kasuri, 2012)&lt;/a&gt; and others have used DNA synthesis and sequencing as an information writing/reading mechanism.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Practical? Not yet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Storage efficiency? Unparalleled!&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-05-22T18:11:10.693" CommentCount="0" />
  <row Id="201" PostTypeId="2" ParentId="193" CreationDate="2017-05-22T18:17:38.167" Score="5" Body="&lt;p&gt;The normalized counts themselves can be accessed with &lt;code&gt;counts(dds, normalized=T)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now as to what the baseMean actually means, that will depend upon whether an &quot;expanded model matrix&quot; is in use or not. Given your previous question, we can see that &lt;code&gt;geno_treat&lt;/code&gt; has a bunch of levels, which means that expanded models are not in use. In such cases, the baseMean should be the mean of the base factor in &lt;code&gt;geno_treat&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-22T18:17:38.167" CommentCount="4" />
  <row Id="202" PostTypeId="1" AcceptedAnswerId="273" CreationDate="2017-05-22T18:19:53.483" Score="11" ViewCount="114" Body="&lt;p&gt;Are there any free open source software tools available for simulating Oxford Nanopore reads?&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="73" LastEditDate="2017-06-08T11:52:35.820" LastActivityDate="2017-06-08T11:52:35.820" Title="Tools for simulating Oxford Nanopore reads" Tags="&lt;nanopore&gt;&lt;genome-sequencing&gt;&lt;software-recommendation&gt;" AnswerCount="5" CommentCount="2" FavoriteCount="1" />
  <row Id="203" PostTypeId="5" CreationDate="2017-05-22T18:22:02.093" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T18:22:02.093" LastActivityDate="2017-05-22T18:22:02.093" CommentCount="0" />
  <row Id="204" PostTypeId="4" CreationDate="2017-05-22T18:22:02.093" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T18:22:02.093" LastActivityDate="2017-05-22T18:22:02.093" CommentCount="0" />
  <row Id="205" PostTypeId="5" CreationDate="2017-05-22T18:24:39.177" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T18:24:39.177" LastActivityDate="2017-05-22T18:24:39.177" CommentCount="0" />
  <row Id="206" PostTypeId="4" CreationDate="2017-05-22T18:24:39.177" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T18:24:39.177" LastActivityDate="2017-05-22T18:24:39.177" CommentCount="0" />
  <row Id="207" PostTypeId="5" CreationDate="2017-05-22T18:29:07.460" Score="0" Body="&lt;p&gt;Many bioinformatics analyses involve decomposing long nucleotide (or, more rarely, peptide) sequences can into their constituent &lt;em&gt;k&lt;/em&gt;-mers, overlapping subsequences of length &lt;em&gt;k&lt;/em&gt;. For sequence &lt;code&gt;D = GATTACA&lt;/code&gt; and &lt;code&gt;k = 4&lt;/code&gt;, the &lt;em&gt;k&lt;/em&gt;-mers in &lt;code&gt;D&lt;/code&gt; are &lt;code&gt;{GATT, ATTA, TTAC, TACA}&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-05-25T14:18:50.583" LastActivityDate="2017-05-25T14:18:50.583" CommentCount="0" />
  <row Id="208" PostTypeId="4" CreationDate="2017-05-22T18:29:07.460" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T18:29:07.460" LastActivityDate="2017-05-22T18:29:07.460" CommentCount="0" />
  <row Id="209" PostTypeId="5" CreationDate="2017-05-22T18:34:08.353" Score="0" Body="&lt;p&gt;Hash functions map data of arbitrary size to a value (usually an integer value) of fixed size. In bioinformatics, hash functions are often used to convert DNA sequences (especially &lt;em&gt;k&lt;/em&gt;-mers) into values that can be stored and analyzed efficiently.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-05-25T14:11:44.250" LastActivityDate="2017-05-25T14:11:44.250" CommentCount="0" />
  <row Id="210" PostTypeId="4" CreationDate="2017-05-22T18:34:08.353" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T18:34:08.353" LastActivityDate="2017-05-22T18:34:08.353" CommentCount="0" />
  <row Id="211" PostTypeId="5" CreationDate="2017-05-22T18:36:31.463" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T18:36:31.463" LastActivityDate="2017-05-22T18:36:31.463" CommentCount="0" />
  <row Id="212" PostTypeId="4" CreationDate="2017-05-22T18:36:31.463" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T18:36:31.463" LastActivityDate="2017-05-22T18:36:31.463" CommentCount="0" />
  <row Id="213" PostTypeId="5" CreationDate="2017-05-22T18:53:31.967" Score="0" Body="&lt;p&gt;Reproducibility refers to the ability to repeat a scientific result that was reported previously. Several terms are frequently used synonymously with reproducibility, such as &lt;em&gt;replicability&lt;/em&gt; or &lt;em&gt;repeatability&lt;/em&gt;. These various terms come with subtle differences in connotation, although there is no universally accepted interpretation of each term.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the context of computation in biology, these terms could refer to any of the following, which constitute a continuum of reproducibility.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Given a scientist's exact code and data, the ability to produce the same exact result as the original scientist.&lt;/li&gt;&#xA;&lt;li&gt;Given a scientist's code and a description of the data, the ability to produce a similar and consistent result with similar data.&lt;/li&gt;&#xA;&lt;li&gt;Given a scientist's data and a description of the code, the ability to produce a similar and consistent result with a similar independent implementation of the code.&lt;/li&gt;&#xA;&lt;li&gt;Given a description of the code and the data, the ability to produce a similar result or reach a similar conclusion based on independent data collection and analysis.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;There is some debate as to what type/level of reproducibility can and should be expected of scientists. The strongest scientific claims come from completely reproducing a result from independent data collection and software implementation, but resources and career incentives make it difficult to reproduce studies/results in this way. On the other hand, reproducing a precise result can sometimes be difficult even with the exact same code and data as the original author, due to differences in computing platforms and the lack of computational training for many biologists.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-05-25T14:12:34.707" LastActivityDate="2017-05-25T14:12:34.707" CommentCount="0" />
  <row Id="214" PostTypeId="4" CreationDate="2017-05-22T18:53:31.967" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-22T18:53:31.967" LastActivityDate="2017-05-22T18:53:31.967" CommentCount="0" />
  <row Id="215" PostTypeId="2" ParentId="92" CreationDate="2017-05-22T19:07:50.327" Score="6" Body="&lt;p&gt;Visual inspection with histograms, boxplots, or some other distribution visualization is the way to go. Prior to normalization, your abundances may look something like this.&#xA;&lt;a href=&quot;https://i.stack.imgur.com/VFVcC.jpg&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/VFVcC.jpg&quot; alt=&quot;Pre-norm&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Post-normalization, they should look something like this.&#xA;&lt;a href=&quot;https://i.stack.imgur.com/XrgU2.jpg&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/XrgU2.jpg&quot; alt=&quot;Post-norm&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&quot;https://biowize.wordpress.com/2013/12/12/normalization-for-differential-expression-analysis/&quot; rel=&quot;noreferrer&quot;&gt;this blog post&lt;/a&gt; for example code.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-05-22T19:07:50.327" CommentCount="0" />
  <row Id="216" PostTypeId="2" ParentId="81" CreationDate="2017-05-22T19:13:49.987" Score="4" Body="&lt;p&gt;The &lt;a href=&quot;https://github.com/lh3/wgsim&quot; rel=&quot;nofollow noreferrer&quot;&gt;wgsim package&lt;/a&gt; by Heng Li (of BWA and samtools fame) is my go-to tool for simulating Illumina reads. It doesn't provide any convenient way to simulate differential coverage across different sequences, but it shouldn't be to hard to run wgsim multiple times, generating the desired level of coverage for each sequence of interest.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would implement a Python script to slurp up your test file, and call wgsim (using the &lt;code&gt;subprocess&lt;/code&gt; module) for each sequence. This will probably require you to have each sequence in a separate file. :-(&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-05-22T19:13:49.987" CommentCount="1" />
  <row Id="217" PostTypeId="2" ParentId="196" CreationDate="2017-05-22T19:31:41.353" Score="4" Body="&lt;p&gt;For &lt;em&gt;metadata&lt;/em&gt;, I would use a SQL schema something like the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CREATE TABLE Project (&#xA;    ac TEXT, -- project/Study accession&#xA;    PRIMARY KEY (ac)&#xA;);&#xA;CREATE TABLE Sample ( -- biological sample/biopsy&#xA;    ac TEXT,&#xA;    PRIMARY KEY (ac)&#xA;);&#xA;CREATE TABLE AnalysisSample (&#xA;    prj_ac TEXT, -- project acccession (Project.ac)&#xA;    symbol TEXT, -- a short name unique in the project&#xA;    sample_ac TEXT, -- sample accession (Sample.ac)&#xA;    PRIMARY KEY (prj_ac, symbol)&#xA;);&#xA;CREATE TABLE Collection ( -- a BAM file&#xA;    ac TEXT, -- collection/alignment file accession&#xA;    prj_ac TEXT, -- project accession (Project.ac)&#xA;    PRIMARY KEY (ac)&#xA;);&#xA;CREATE TABLE ReadGroup (&#xA;    cl_ac TEXT, -- collection accession (Collection.ac)&#xA;    rg_id TEXT, -- @RG-ID&#xA;    sample_sym TEXT, -- @RG-SM; matching AnalysisSample.symbol&#xA;    PRIMARY KEY (cl_ac, rg_id)&#xA;);&#xA;CREATE TABLE VariantSet ( -- a VCF file&#xA;    ac TEXT, -- VCF file accession&#xA;    prj_ac TEXT, -- project accession (Project.ac)&#xA;    PRIMARY KEY (ac)&#xA;);&#xA;CREATE TABLE VariantSample (&#xA;    vs_ac TEXT, -- VCF file accession (VariantSet.ac)&#xA;    sample_sym TEXT, -- sample symbol in the VCF file; matching AnalysisSample.symbol&#xA;    PRIMARY KEY (vs_ac, sample_sym)&#xA;);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In the schema, you have &lt;code&gt;Project&lt;/code&gt; and biological &lt;code&gt;Sample&lt;/code&gt; tables, which are independent of each other at the high level. An &lt;code&gt;AnalysisSample&lt;/code&gt; describes a sample used in BAM or VCF and connects &lt;code&gt;Project&lt;/code&gt; and biological &lt;code&gt;Sample&lt;/code&gt;. Importantly, each &lt;code&gt;AnalysisSample&lt;/code&gt; has a symbol unique in a project (see the primary index). This is the symbol on a BAM read group line or on a VCF sample line. A &lt;code&gt;Collection&lt;/code&gt; is in effect a BAM/CRAM file. In theory, a BAM file may contain more than one samples (though rare in practice), which is addressed by a separate &lt;code&gt;ReadGroup&lt;/code&gt; table. Finally, a &lt;code&gt;VariantSet&lt;/code&gt; is a VCF file. &lt;code&gt;VariantSample&lt;/code&gt; tells you which samples are included in each VCF file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is the skeleton of a full schema. You can add extra fields to appropriate tables (e.g. file path and hg19/hg38/etc to &lt;code&gt;Collection&lt;/code&gt;, read length to &lt;code&gt;ReadGroup&lt;/code&gt; and family ID to &lt;code&gt;Sample&lt;/code&gt;). You also need indices for efficient table joining and perhaps more tables for complex structures (e.g. pedigree).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the projects I have participated, this schema should work most of time. It is inspired by GA4GH's JSON schema, but my version is in SQL, is simpler and also has a slightly different structure which I think is better.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-05-22T19:31:41.353" CommentCount="0" />
  <row Id="218" PostTypeId="2" ParentId="146" CreationDate="2017-05-22T20:33:56.417" Score="6" Body="&lt;p&gt;Several papers have made this distinction, and a few indeed use different terms to distinguish between them. For example, &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0022000016300502&quot; rel=&quot;noreferrer&quot;&gt;Kazaux et al. (2016)&lt;/a&gt; acknowledge that:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;These constraints favour the use of a version of the de Bruijn Graph (dBG) dedicated to genome assembly – a version which differs from the combinatorial structure invented by N.G. de Bruijn.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-21&quot; rel=&quot;noreferrer&quot;&gt;Kingsford et al. (2010)&lt;/a&gt; also recognise the distinction:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Note that this definition of a de Bruijn graph differs from the traditional definition described in the mathematical literature in the 1940s that requires the graph to contain all length-k strings that can be formed from an alphabet (rather than just those strings present in the genome). &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The oldest reference I found for a specific term to refer to the assembly-related structure is &lt;a href=&quot;http://online.liebertpub.com/doi/abs/10.1089/cmb.1995.2.333&quot; rel=&quot;noreferrer&quot;&gt;Skiena and Sundaram (1995)&lt;/a&gt;, where they call it a &lt;strong&gt;subgraph of the de Bruijn digraph&lt;/strong&gt;. Later, in 2002, &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0012365X01001339&quot; rel=&quot;noreferrer&quot;&gt;Błażewicz et al.&lt;/a&gt; will refer to it as a &lt;strong&gt;de Bruijn induced subgraph&lt;/strong&gt;.  The term &lt;strong&gt;de Bruijn subgraph&lt;/strong&gt; is also formally defined in &lt;a href=&quot;https://pub.uni-bielefeld.de/publication/2302499&quot; rel=&quot;noreferrer&quot;&gt;Quitzau’s thesis (2009)&lt;/a&gt;. There, and also in the article (&lt;a href=&quot;http://link.springer.com/chapter/10.1007/978-3-540-87361-7_29&quot; rel=&quot;noreferrer&quot;&gt;Quitzau and Stoye, 2008)&lt;/a&gt; the authors describe the &lt;strong&gt;sequence graph&lt;/strong&gt; as a modification of the sparse de Bruijn subgraph (commonly used in assembly problems), where non-branching paths are replaced by a single vertex. The term &lt;strong&gt;sparse de Bruijn graph&lt;/strong&gt; is also used by &lt;a href=&quot;https://arxiv.org/abs/1306.4353&quot; rel=&quot;noreferrer&quot;&gt;Chauve et al. (2013)&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another term that I found was &lt;strong&gt;word graph&lt;/strong&gt;, described by both &lt;a href=&quot;https://academic.oup.com/bioinformatics/article/21/8/1371/249821/A-graph-based-algorithm-for-generating-EST&quot; rel=&quot;noreferrer&quot;&gt;Malde et al. (2005)&lt;/a&gt; and by &lt;a href=&quot;http://link.springer.com/chapter/10.1007/978-3-540-72031-7_29&quot; rel=&quot;noreferrer&quot;&gt;Heath and Pati (2007)&lt;/a&gt; as a &lt;em&gt;subgraph&lt;/em&gt; or as a &lt;em&gt;generalization&lt;/em&gt; of a de Bruijn graph. &lt;a href=&quot;http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-313&quot; rel=&quot;noreferrer&quot;&gt;Rødland (2013)&lt;/a&gt; summarises some of the terms used for this data structure:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The data structure is best understood in terms of the de Bruijn subgraph representation of S[k]. (...) Some authors may refer to this as a word graph, or even just a de Bruijn graph.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Although we can recognise that the distinction is not very relevant, the question is asking specifically for the situation where one wants to make such a distinction. &lt;/p&gt;&#xA;" OwnerUserId="45" LastActivityDate="2017-05-22T20:33:56.417" CommentCount="2" />
  <row Id="219" PostTypeId="2" ParentId="202" CreationDate="2017-05-22T20:43:59.303" Score="7" Body="&lt;p&gt;By chance, just today I've heard of a nanopore read simulator, &lt;a href=&quot;http://biorxiv.org/content/early/2016/03/18/044545.1&quot; rel=&quot;noreferrer&quot;&gt;NanoSim&lt;/a&gt;. It is  &lt;a href=&quot;https://github.com/bcgsc/NanoSim&quot; rel=&quot;noreferrer&quot;&gt;released under a GPL license&lt;/a&gt;. I have never used it, though...&lt;/p&gt;&#xA;" OwnerUserId="45" LastActivityDate="2017-05-22T20:43:59.303" CommentCount="0" />
  <row Id="220" PostTypeId="2" ParentId="195" CreationDate="2017-05-22T22:13:52.937" Score="8" Body="&lt;p&gt;An epic question. Unfortunately, the short answer is: no, there are no widely used solutions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For several thousand samples, BCF2, the binary representation of VCF, should work well. I don't see the need of new tools at this scale. For a larger sample size, ExAC people are using spark-based hail. It keeps all per-sample annotations (like GL, GQ and DP) in addition to genotypes. Hail is at least something heavily used in practice, although mostly by a few groups so far.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A simpler problem is to store genotypes only. This is sufficient to the majority of end users. There are better approaches to store and query genotypes. GQT, developed by the Gemini team, enables fast query of samples. It allows you to quickly pull samples under certain genotype configurations. As I remember, GQT is orders of magnitude faster than google genomics API to do PCA. Another tool is BGT. It produces a much smaller file and provides fast and convenient queries over sites. Its paper talks about ~32k whole-genome samples. I am in the camp who believe specialized binary formats like GQT and BGT are faster than solutions built on top of generic databases. I would encourage you to have a look if you only want to query genotypes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Intel's GenomicDB approaches the problem in a different angle. It does not actually keep a &quot;squared&quot; multi-sample VCF internally. It instead keeps per-sample genotypes/annotations and generates merged VCF on the fly (this is my understanding, which could be wrong). I don't have first-hand experience with GenomicDB, but I think something in this line should be the ultimate solution in the era of 1M samples. I know GATK4 is using it at some step.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As to others in your list, Gemini might not scale that well, I guess. It is partly the reason why they work on GQT. Last time I checked, BigQuery did not query individual genotypes. It only queries over site statistics. Google genomics APIs access individual genotypes, but I doubt it can be performant. Adam is worth trying. I have not tried, though.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-05-22T22:13:52.937" CommentCount="2" />
  <row Id="221" PostTypeId="2" ParentId="202" CreationDate="2017-05-22T23:05:06.307" Score="4" Body="&lt;p&gt;In addition to the already mentioned &lt;a href=&quot;https://github.com/bcgsc/NanoSim&quot; rel=&quot;nofollow noreferrer&quot;&gt;NanoSim&lt;/a&gt;, there is also &lt;a href=&quot;https://github.com/ethanagbaker/SiLiCO&quot; rel=&quot;nofollow noreferrer&quot;&gt;SiLiCO&lt;/a&gt; and &lt;a href=&quot;https://sourceforge.net/p/readsim/wiki/Home/&quot; rel=&quot;nofollow noreferrer&quot;&gt;ReadSim&lt;/a&gt; (although it hasn't been updated in over 2 years, so I am not sure how relevant it is at this point considering how fast the technology is progressing).&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-05-22T23:05:06.307" CommentCount="0" />
  <row Id="222" PostTypeId="2" ParentId="202" CreationDate="2017-05-23T00:10:55.690" Score="4" Body="&lt;p&gt;There's a brand new tool that appeared today &lt;a href=&quot;http://biorxiv.org/content/early/2017/05/22/133652&quot; rel=&quot;nofollow noreferrer&quot;&gt;SNaReSim&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Nanopores represent the first commercial technology in decades to&#xA;  present a significantly different technique for DNA sequencing, and&#xA;  one of the first technologies to propose direct RNA sequencing.&#xA;  Despite significant differences with previous sequencing technologies,&#xA;  read simulators to date make similar assumptions with respect to error&#xA;  profiles and their analysis. This is a great disservice to both&#xA;  nanopore sequencing and to algorithm developers who seek to optimize&#xA;  their tools to the platform. Previous works have discussed the&#xA;  occurrence of some k-mer bias, but this discussion has been focused on&#xA;  homopolymers, leaving unanswered the question of whether k-mer bias&#xA;  exists over general k-mers, how it occurs, and what can be done to&#xA;  reduce the effects. In this work, we demonstrate that current read&#xA;  simulators fail to accurately represent k-mer error distributions, We&#xA;  explore the sources of k-mer bias in nanopore basecalls, and we&#xA;  present a model for predicting k-mers that are difficult to identify.&#xA;  We also propose a new SNaReSim, a new state-of-the-art simulator, and&#xA;  demonstrate that it provides higher accuracy with respect to 6-mer&#xA;  accuracy biases.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="64" LastActivityDate="2017-05-23T00:10:55.690" CommentCount="2" />
  <row Id="223" PostTypeId="1" AcceptedAnswerId="224" CreationDate="2017-05-23T18:43:01.447" Score="-4" ViewCount="76" Body="&lt;p&gt;As a newcomer, what are some influential research papers published in the past few years that I should read to catch up in the field?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: @Devon Ryan answered it perfectly. I tried a google search but it turned up a bunch of opinionated articles that varied. The biostar discussions are perfect and was the link I was searching for. I hope someone else new to the field finds this useful!&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="33" LastEditDate="2017-05-25T14:21:11.510" LastActivityDate="2017-05-25T14:21:11.510" Title="Influential papers in bioinformatics" Tags="&lt;reference&gt;" AnswerCount="1" CommentCount="6" FavoriteCount="1" ClosedDate="2017-05-23T22:06:43.123" />
  <row Id="224" PostTypeId="2" ParentId="223" CreationDate="2017-05-23T19:15:08.387" Score="4" Body="&lt;p&gt;Over on biostars there's a thread like this every year or so. I'll link to &lt;a href=&quot;https://www.biostars.org/p/229807/&quot; rel=&quot;nofollow noreferrer&quot;&gt;the 2016 edition&lt;/a&gt; and the (much shorter) &lt;a href=&quot;https://www.biostars.org/p/171860/&quot; rel=&quot;nofollow noreferrer&quot;&gt;2015 edition&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My personal picks from those would be:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.nature.com/nature/journal/v536/n7616/full/nature19057.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;ExAC&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://biorxiv.org/content/early/2015/06/27/021592&quot; rel=&quot;nofollow noreferrer&quot;&gt;salmon&lt;/a&gt;, which is now published&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1505.02710&quot; rel=&quot;nofollow noreferrer&quot;&gt;kallisto&lt;/a&gt;, which is also now published&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-23T19:15:08.387" CommentCount="0" />
  <row Id="225" PostTypeId="1" AcceptedAnswerId="227" CreationDate="2017-05-24T03:26:50.163" Score="14" ViewCount="115" Body="&lt;p&gt;I am using a reference genome for mm10 mouse downloaded from &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/genome?term=mus%20musculus&quot; rel=&quot;noreferrer&quot;&gt;NCBI&lt;/a&gt;, and would like to understand in greater detail the difference between lowercase and uppercase letters, which make up roughly equal parts of the genome. I understand that N is used for 'hard masking' (areas in the genome that could not be assembled) and lowercase letters for 'soft masking' in repeat regions.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What does this soft masking actually mean? &lt;/li&gt;&#xA;&lt;li&gt;How confident can I be about the sequence in these regions?&lt;/li&gt;&#xA;&lt;li&gt;What does a lowercase n represent? &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="163" LastEditorUserId="163" LastEditDate="2017-05-24T11:56:09.067" LastActivityDate="2017-05-24T14:24:15.477" Title="Uppercase vs lowercase letters in reference genome" Tags="&lt;fasta&gt;&lt;genome&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="226" PostTypeId="2" ParentId="202" CreationDate="2017-05-24T03:36:03.760" Score="2" Body="&lt;p&gt;The best nanopore read simulators would be associated with the best base-callers. For a base-caller to effectively model the DNA strand, it needs to take into account the expected underlying electrical model together with the associated signal noise (both in the time dimension as well as the amplitude dimension). In theory, it should be possible to reverse the algorithm and generate an electrical signal given an underlying sequence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, I'm not aware of any tools that attempt to simulate nanopore reads at the electrical level. Any &quot;nanopore read simulator&quot; that concentrates only on base sequence would need to encompass all the possible base-calling software models that exist, which is an impossible task (particularly given how quickly ONT updates their own base callers).&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-05-24T03:36:03.760" CommentCount="0" />
  <row Id="227" PostTypeId="2" ParentId="225" CreationDate="2017-05-24T06:01:32.763" Score="11" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;What does this soft masking actually mean?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;A lot of the sequence in genomes are repetitive. Human genome, for example, has (at least) two-third repetitive elements.[1]. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;These repetitive elements are soft-masked by converting the upper case letters to lower case. An important use-case of these soft-masked bases will be in homology searches: An &lt;code&gt;atatatatatat&lt;/code&gt; will tend to appear both in human and mouse genomes but is likely non-homologous.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;How confident can I be about the sequence in these regions?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;As you can be about in non soft-masked based positions. Soft-masking is done after determining portions in the genome that are likely repetitive. There is no uncertainty whether a particular base is 'A' or 'G', just that it is part of a repeat and hence should be represented as an 'a'.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;What does a lowercase n represent?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;UCSC uses &lt;a href=&quot;https://tandem.bu.edu/trf/trfdesc.html&quot; rel=&quot;noreferrer&quot;&gt;Tandom Repeat Finder&lt;/a&gt; and &lt;a href=&quot;http://www.repeatmasker.org/&quot; rel=&quot;noreferrer&quot;&gt;RepeatMasker&lt;/a&gt; for soft-masking potential repeats. NCBI most likely uses &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3045581/&quot; rel=&quot;noreferrer&quot;&gt;TANTAN&lt;/a&gt;. 'N's represents no sequence information is available for that base. It being replaced by 'n' is likely an artifact of the repeat-masking software where it soft-masks an 'N' by an 'n' to indicate that portion of the genome is likely a repeat too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[1] &lt;a href=&quot;http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1002384&quot; rel=&quot;noreferrer&quot;&gt;http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1002384&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="161" LastActivityDate="2017-05-24T06:01:32.763" CommentCount="1" />
  <row Id="228" PostTypeId="1" CreationDate="2017-05-24T06:41:10.320" Score="3" ViewCount="41" Body="&lt;p&gt;I have sequenced numerous multiplexed pools of BS amplicon-seq libraries derived from human samples on a MiSeq over the past few weeks. I have been utilising trim-galore and Bismark for alignment and am finding the mapping efficiency to be really low for two pools (55% &amp;amp; 30% respectively) both of which had a cytosine per base sequence of around 10-20% throughout the entire read in their fastqc files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The high C content visible in the fastqc files makes me think the poor mapping efficiency is due to poor bisulfite conversion, as this would be expected to be close to zero is bisulfite conversion had actually took place.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am new to the field so any help would be greatly appreciated.&lt;/p&gt;&#xA;" OwnerUserId="88" LastActivityDate="2017-05-24T06:46:36.547" Title="The effects of incomplete bisulfite conversion upon mapping efficiency" Tags="&lt;alignment&gt;&lt;fastq&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="229" PostTypeId="2" ParentId="228" CreationDate="2017-05-24T06:46:36.547" Score="5" Body="&lt;p&gt;Bisulfite conversion efficiency has no effect on the mapping rate in bismark and similar tools. The reason is that the reads are fully bisulfite converted &lt;em&gt;in silico&lt;/em&gt; before alignment to minimize mapping bias.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would suggest that you play around with the settings handed to bowtie2, such as using local alignment and modifying the &lt;code&gt;--score-min&lt;/code&gt; option to allow more mismatches. Alternatively, you might try a different aligner like &lt;a href=&quot;https://github.com/brentp/bwa-meth&quot; rel=&quot;noreferrer&quot;&gt;bwameth&lt;/a&gt;, which will always do local alignment.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-24T06:46:36.547" CommentCount="6" />
  <row Id="230" PostTypeId="1" AcceptedAnswerId="240" CreationDate="2017-05-24T07:35:31.097" Score="8" ViewCount="88" Body="&lt;p&gt;As enrichment analysis a usual step is to infer the pathways enriched in a list of genes. However I can't find a discussion about which database is better. Two of the most popular (in my particular environment) are Reactome and KEGG (Maybe because there are tools using them in Bioconductor).&#xA;KEGG requires a subscription for ftp access, and for my research I would need to download huge amounts of KGML files I am now leaning towards Reactome&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which is the one with more genes associated to pathways ? &#xA;Which is more completely annotated ?&#xA;Is there any paper comparing them ? &lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2017-05-24T10:16:12.237" LastActivityDate="2017-06-01T07:05:29.963" Title="What are the advantages and disadvantages between using KEGG or Reactome?" Tags="&lt;database&gt;" AnswerCount="3" CommentCount="7" FavoriteCount="1" />
  <row Id="231" PostTypeId="2" ParentId="225" CreationDate="2017-05-24T08:16:19.220" Score="5" Body="&lt;p&gt;The use of lower/upper case letters and &lt;code&gt;N&lt;/code&gt;/&lt;code&gt;n&lt;/code&gt; letters in genomes sequences is not completely standardised and you should always check the specification of the resource you are using.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lower case letters are most commonly used to represent “soft-masked sequences”, a convention popularised by &lt;a href=&quot;http://www.repeatmasker.org/&quot; rel=&quot;noreferrer&quot;&gt;RepeatMasker&lt;/a&gt;, where interspersed repeats (which covers transposons, retrotransposons and processed pseudogenes) and low complexity sequences are marked with lower case letters. Note that larger repeats, such as sizable tandem repeats, segmental duplications, and whole gene duplications are not generally masked.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, there are other uses for lower/upper case letters, for example, &lt;a href=&quot;http://www.ensembl.org/Help/View?id=155&quot; rel=&quot;noreferrer&quot;&gt;Ensembl have used&lt;/a&gt; upper/lower case letters to represent exonic and intronic sequences respectively.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;N&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt; nucleotides may represent “hard masked sequences”, where interspersed repeats and low complexity sequences are replaced by &lt;code&gt;N&lt;/code&gt;s. But &lt;code&gt;N&lt;/code&gt;/&lt;code&gt;n&lt;/code&gt;s may alternatively represent ambiguous nucleotides, indeed this is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Nucleic_acid_notation#IUPAC_notation&quot; rel=&quot;noreferrer&quot;&gt;IUPAC&lt;/a&gt; specification.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also note occasionally (although fortunately rarely) &lt;code&gt;X&lt;/code&gt;/&lt;code&gt;x&lt;/code&gt; is used to represent ambiguous nucleotides or “hard-masked sequences” too.&lt;/p&gt;&#xA;" OwnerUserId="104" LastActivityDate="2017-05-24T08:16:19.220" CommentCount="0" />
  <row Id="232" PostTypeId="1" CreationDate="2017-05-24T09:32:26.523" Score="7" ViewCount="81" Body="&lt;p&gt;I’d normally use &lt;code&gt;collapseReplicates&lt;/code&gt; (or do the collapsing upstream) to handle technical replicates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in my current RNA-seq experimental design, samples were sequenced twice using different library preparation protocols, leading to marked differences in the resulting count estimates (in fact, the choice of different protocol explains most of the variance in the bulk data according to PCA). I would therefore like to model these differences by adding them as a covariate to the DESeq2 model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I &lt;em&gt;thought&lt;/em&gt; I could just add another factor to the design formula, equivalent to &lt;a href=&quot;https://www.bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#multi-factor-designs&quot; rel=&quot;noreferrer&quot;&gt;the “pasilla” example in the DESeq2 vignette&lt;/a&gt;, which adds the factor &lt;code&gt;type&lt;/code&gt; for the library type (single-end vs paired-end). However, the &lt;a href=&quot;http://bioconductor.org/packages/release/data/experiment/vignettes/pasilla/inst/doc/create_objects.html&quot; rel=&quot;noreferrer&quot;&gt;pasilla vignette&lt;/a&gt; states that these different libraries are actually independent &lt;em&gt;biological&lt;/em&gt; replicates, not technical replicates as I had previously assumed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I’m now concerned that having technical replicates in the design might artificially inflate the power. Can the design be salvaged?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It’s worth noting that we &lt;em&gt;also&lt;/em&gt; have biological replicates, and a full rank design; i.e. every combination of treatment and library prep, with biological replicates.&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-05-24T10:02:03.190" LastActivityDate="2017-05-25T10:04:02.870" Title="Can I model technical replicates in DESeq2?" Tags="&lt;rna-seq&gt;&lt;deseq2&gt;&lt;technical-replicates&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="233" PostTypeId="2" ParentId="193" CreationDate="2017-05-24T10:17:43.520" Score="2" Body="&lt;p&gt;It depends what you mean by “normalised”. As Devon said, the &lt;code&gt;normalized = TRUE&lt;/code&gt; argument to the &lt;code&gt;count&lt;/code&gt; function gives you normalised counts. However, these are “only” library-size normalised (i.e. divided by the &lt;code&gt;sizeFactors(dds)&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, &lt;a href=&quot;https://www.bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#data-transformations-and-visualization&quot; rel=&quot;nofollow noreferrer&quot;&gt;as the vignette explains&lt;/a&gt;, downstream processing generally requires more advanced normalisation, to account for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Heteroscedasticity&quot; rel=&quot;nofollow noreferrer&quot;&gt;heteroscedasticity&lt;/a&gt; of the counts. This is often done by simply &lt;code&gt;log&lt;/code&gt;ging the counts but this has obvious drawbacks (most trivially, what do we do with 0 counts? A workaround is to add a pseudocount but that’s problematic too).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DESeq2 offers two different methods to perform a more rigorous analysis:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;rlog&lt;/code&gt; — a &lt;em&gt;regularised log&lt;/em&gt;, and&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;vst&lt;/code&gt; — a &lt;em&gt;variance stabilising transformation&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;You’d generally use either of these for downstream analysis, not &lt;code&gt;count(dds, normalized = TRUE)&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-05-24T10:17:43.520" CommentCount="0" />
  <row Id="234" PostTypeId="2" ParentId="232" CreationDate="2017-05-24T10:20:25.597" Score="5" Body="&lt;p&gt;Practically speaking, there's no way to include the technical replicates in that design (in DESeq2 at least). Your concern regarding inflating the power is exactly correct and the only way to combat that would be to add a pairing factor like one might do with case-control or tumor-normal studies. That is, something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  group libraryPrep sample&#xA;1    WT           A      1&#xA;2    WT           B      1&#xA;3   MUT           A      2&#xA;4   MUT           B      2&#xA;5    WT           A      3&#xA;6    WT           B      3&#xA;7   MUT           A      4&#xA;8   MUT           B      4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here &lt;code&gt;sample&lt;/code&gt; pairs the technical replicates together. However this ends up either being rank deficient or in the end not more informative.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-24T10:20:25.597" CommentCount="0" />
  <row Id="235" PostTypeId="2" ParentId="232" CreationDate="2017-05-24T11:09:19.483" Score="3" Body="&lt;p&gt;If they're truly technical replicates, then &lt;strike&gt;there's no way to model them using DESeq2&lt;/strike&gt;*, as you've alluded to with the &lt;code&gt;collapseReplicates&lt;/code&gt; function. DESeq2/ Mike Love's general recommendation with &lt;code&gt;collapseReplicates&lt;/code&gt; is to just add the reads together for technical replicates. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to model them instead of collapsing them down, you can &lt;code&gt;voom&lt;/code&gt; transform your data and follow an example similar to section 11.3 in the &lt;a href=&quot;https://www.bioconductor.org/packages/devel/bioc/vignettes/limma/inst/doc/usersguide.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;Limma users guide&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;(Code example from 11.3):&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Design&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;| FileName  | Cy3   | Cy5  |&#xA;| --------- |:-----:| ----:|&#xA;| File1     | wt1   | mu1  |&#xA;| File2     | wt1   | mu1  |&#xA;| File3     | wt2   | mu2  |&#xA;| File4     | wt2   | mu2  |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; biolrep &amp;lt;- c(1, 1, 2, 2)&#xA;&amp;gt; corfit &amp;lt;- duplicateCorrelation(MA, ndups = 1, block = biolrep)&#xA;&amp;gt; fit &amp;lt;- lmFit(MA, block = biolrep, cor = corfit$consensus)&#xA;&amp;gt; fit &amp;lt;- eBayes(fit)&#xA;&amp;gt; topTable(fit, adjust = &quot;BH&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;*Edit: As @Devon Ryan mentions, you can design a paired model in &lt;code&gt;DESeq2&lt;/code&gt;, but beware of making this full rank. &lt;/p&gt;&#xA;" OwnerUserId="222" LastEditorUserId="222" LastEditDate="2017-05-25T10:04:02.870" LastActivityDate="2017-05-25T10:04:02.870" CommentCount="2" />
  <row Id="236" PostTypeId="2" ParentId="225" CreationDate="2017-05-24T12:29:31.210" Score="2" Body="&lt;ol&gt;&#xA;&lt;li&gt;Lower case nucleotides commonly denotes a &lt;strong&gt;soft&lt;/strong&gt; masked sequences. How exactly the genome was masked you can find in FAQ of &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/genome/doc/ftpfaq/#masking&quot; rel=&quot;nofollow noreferrer&quot;&gt;NCBI&lt;/a&gt;:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;strong&gt;Are repetitive sequences in eukaryotic genomes masked?&lt;/strong&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Repetitive sequences in eukaryotic genome assembly sequence files, as&#xA;  identified by &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/16287941&quot; rel=&quot;nofollow noreferrer&quot;&gt;WindowMasker&lt;/a&gt;, have been masked to lower-case.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The location and identity of repeats found by &lt;a href=&quot;http://www.repeatmasker.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RepeatMasker&lt;/a&gt; are also&#xA;  provided in a separate file. These spans could be used to mask the&#xA;  genomic sequences if desired. Be aware, however, that many less&#xA;  studied organisms do not have good repeat libraries available for&#xA;  RepeatMasker to use.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;&lt;p&gt;IMHO, low complexity regions are always more likely to be missassembled than high complexity sequences. However, this will be problem for non-model organisms. I would guess that the reliability of the softmasked regions of mouse genome will very high.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;No idea, looks like an artefact.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;An example of usage of the soft mask&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mapping of sequence to reference usually starts with perfect matches of seeds (substrings) of the mapped reads and the reference sequence. Soft masked (low complexity) regions are not used for matches of seeds, but they are used only for the extension of the alignment if there was a seed in a neighbouring region. This application of softmasking applied to problem of long read assembly is described on this &lt;a href=&quot;https://dazzlerblog.wordpress.com/2016/04/01/detecting-and-soft-masking-repeats/&quot; rel=&quot;nofollow noreferrer&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-05-24T14:24:15.477" LastActivityDate="2017-05-24T14:24:15.477" CommentCount="0" />
  <row Id="237" PostTypeId="1" AcceptedAnswerId="239" CreationDate="2017-05-24T15:37:17.157" Score="4" ViewCount="45" Body="&lt;p&gt;I produced a bam file by aligning reads to a small set of synthetic sequences using bwa-mem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am heavily filtering reads that are not paired and of a certain orientation.&#xA;Applying the filtering, I get a few thousands of reads:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view -h $myfilebam | \&#xA;samtools view -h -F4 - | \&#xA;samtools view -h -F8 - | \&#xA;samtools view -h -F256 - | \&#xA;samtools view -h -F512 - | \&#xA;samtools view -h -F1024 - | \&#xA;samtools view -h -F2048 - | \&#xA;samtools view -h -f16 - | \&#xA;samtools view -h -f32 -  | wc -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Gives me &lt;code&gt;89502&lt;/code&gt; reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I then pipe this into &lt;code&gt;samtools mpileup&lt;/code&gt;, I get no results:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view -h $myfilebam | \&#xA;samtools view -h -F4 - | \&#xA;samtools view -h -F8 - | \&#xA;samtools view -h -F256 - | \&#xA;samtools view -h -F512 - | \&#xA;samtools view -h -F1024 - | \&#xA;samtools view -h -F2048 - | \&#xA;samtools view -h -f16 - | \&#xA;samtools view -h -f32 -  | \&#xA;samtools mpileup --excl-flags 0 -Q0 -B -d 999999 - | wc -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Returns 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried different combinations of filtering, and when I do both &lt;code&gt;-f 16&lt;/code&gt; and &lt;code&gt;-f 32&lt;/code&gt; returns empty, but if I do either of those, then it works:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view -h $myfilebam | \&#xA;samtools view -h -F4 - | \&#xA;samtools view -h -F8 - | \&#xA;samtools view -h -F256 - | \&#xA;samtools view -h -F512 - | \&#xA;samtools view -h -F1024 - | \&#xA;samtools view -h -F2048 - | \&#xA;samtools view -h -f16 - | \&#xA;samtools mpileup --excl-flags 0 -Q0 -B -d 999999 - | wc -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Returns &lt;code&gt;1056&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas why? My thinking was that it would work with &lt;code&gt;--excl-flags 0&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: substituting &lt;code&gt;mpileup&lt;/code&gt; for &lt;code&gt;depth&lt;/code&gt; does work, and prints out each position and the depth as expected.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT2: adding &lt;code&gt;-q 0&lt;/code&gt; to &lt;code&gt;mpileup&lt;/code&gt; gives the same empty result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance&lt;/p&gt;&#xA;" OwnerUserId="180" LastEditorUserId="180" LastEditDate="2017-05-24T20:53:49.517" LastActivityDate="2017-05-24T22:35:24.987" Title="samtools mpileup empty when filtering out flags" Tags="&lt;bam&gt;&lt;samtools&gt;&lt;mpileup&gt;" AnswerCount="1" CommentCount="8" FavoriteCount="1" />
  <row Id="238" PostTypeId="1" AcceptedAnswerId="241" CreationDate="2017-05-24T15:41:28.400" Score="7" ViewCount="64" Body="&lt;p&gt;I have a few sets of marker genes that I can classify RNA-seq samples using semi-supervised clustering.  I would like to automate the process, however, I am struggling to find the ideal algorithm that could generate some kind of score for marker gene set from a given sample.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I presume that this is a standard analyses in many groups but I am not sure which method(s) are yielding good results in practice.&lt;/p&gt;&#xA;" OwnerUserId="64" LastActivityDate="2017-05-24T23:32:41.013" Title="Classifying samples based on marker gene expression" Tags="&lt;rna-seq&gt;&lt;classification&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="239" PostTypeId="2" ParentId="237" CreationDate="2017-05-24T22:35:24.987" Score="2" Body="&lt;p&gt;By using &lt;code&gt;-h&lt;/code&gt; in the &lt;code&gt;samtools view&lt;/code&gt; command, you're including all the header lines in your word count. If you happen to have about 89500 reference sequences, then the lengths of those would all appear in the header and inflate the &lt;code&gt;-h&lt;/code&gt; word count, but not the mpileup count. Try piping it through an additional &lt;code&gt;samtools view&lt;/code&gt; (i.e. without &lt;code&gt;-h&lt;/code&gt;) and see if the counts change:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;samtools view -h -f32 -  | \&#xA;samtools view | wc -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Also, &lt;code&gt;samtools mpileup&lt;/code&gt; by default only considers high-quality bases and concordant reads. Try adding a &lt;code&gt;-A&lt;/code&gt; to your mpileup line (which stops anomalous read pairs from being discarded):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;samtools mpileup -A -Q0 -B -d 999999 - | wc -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-05-24T22:35:24.987" CommentCount="1" />
  <row Id="240" PostTypeId="2" ParentId="230" CreationDate="2017-05-24T22:59:54.537" Score="3" Body="&lt;p&gt;One big downside of KEGG is the licensing issue. One big advantage of Reacome are various crosslinks to other databases and data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ad 1,&lt;/strong&gt; This depends on which pathway, they are both primary databases. Sometimes other databases that for instance combine data of primary databases have better annotation of pathways (there is an example in the review paper bellow)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ad 3,&lt;/strong&gt; There is very extensive relatively new (2015) review on this topic focused on human pathways: &lt;a href=&quot;https://academic.oup.com/database/article/2433126/Comparison-of-human-cell-signaling-pathway&quot; rel=&quot;nofollow noreferrer&quot;&gt;Comparison of human cell signaling pathway databases—evolution, drawbacks and challenges&lt;/a&gt;. However I could not find there which one is more complete ...&lt;/p&gt;&#xA;" OwnerUserId="57" LastActivityDate="2017-05-24T22:59:54.537" CommentCount="0" />
  <row Id="241" PostTypeId="2" ParentId="238" CreationDate="2017-05-24T23:32:41.013" Score="5" Body="&lt;p&gt;I would consider using gene expression signatures to classify samples (especially cancer subtypes but the same principles apply to other problems of this type) one of the classic problems of bioinformatics. Quite a lot of work has been done on methods to derive gene sets that provide good classification performance. This is slightly different from your problem since you already have a gene signature but it may still prove useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These methods will typically fit a model that selects a (small) number of genes from genome-wide expression data that distinguish between the cell types/conditions in question, i.e. they derive a gene signature. The resulting model then allows for the classification of new samples. I've had success using &lt;a href=&quot;http://bioinformatics.csiro.au/generave/&quot; rel=&quot;noreferrer&quot;&gt;GeneRave&lt;/a&gt; for this purpose (but note that this was designed for microarray data, I haven't used it with RNA-seq data and don't know how well it holds up there). A more recent paper relating to this issue can be found &lt;a href=&quot;https://www.nature.com/articles/srep32976.&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how does that help you? One option would be to fit one of these classifiers to gene expression data for the genes you already know to obtain a model that can then be applied to new samples automatically.&lt;/p&gt;&#xA;" OwnerUserId="154" LastActivityDate="2017-05-24T23:32:41.013" CommentCount="3" />
  <row Id="242" PostTypeId="1" AcceptedAnswerId="276" CreationDate="2017-05-25T11:46:36.237" Score="12" ViewCount="123" Body="&lt;p&gt;I'm currently trying to assembly a genome from a rodent parasite, &lt;em&gt;Nippostrongylus brasiliensis&lt;/em&gt;. This genome does have an existing reference genome, but it is highly fragmented. Here are some continuity statistics for the scaffolds of the current Nippo reference genome (assembled from Illumina reads):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Total sequences: 29375&#xA;Total length: 294.400206 Mb&#xA;Longest sequence: 394.171 kb&#xA;Shortest sequence: 500 b&#xA;Mean Length: 10.022 kb&#xA;Median Length: 2.682 kb&#xA;N50: 2024 sequences; L50: 33.527 kb&#xA;N90: 11638 sequences; L90: 4.263 kb&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This genome is most likely difficult to assemble because of the highly repetitive nature of the genomic sequences. These repetitive sequences come in (at least) three classes:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Tandem repeats with a &lt;em&gt;repeat unit&lt;/em&gt; length greater than the read length of Illumina sequencers (e.g. 171bp)&lt;/li&gt;&#xA;&lt;li&gt;Tandem repeats with a &lt;em&gt;cumulative&lt;/em&gt; length greater than the fragment length of Illumina sequencers, or the template length for linked reads (e.g. 20kb)&lt;/li&gt;&#xA;&lt;li&gt;Complex (i.e. non-repetitive) sequence that appears at multiple places throughout the genome&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Canu seems to deal quite well with the first two types of repeats, despite the abundance of repetitive structure in the genome. Here's the unitigging summary produced by Canu on one of the assemblies I've attempted. Notice that about 30% of the reads either span or contain a long repeat:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;category            reads     %          read length        feature size or coverage  analysis&#xA;----------------  -------  -------  ----------------------  ------------------------  --------------------&#xA;middle-missing        694    0.07     7470.92 +- 5552.00        953.06 +- 1339.13    (bad trimming)&#xA;middle-hump           549    0.05     3770.05 +- 3346.10         74.23 +- 209.86     (bad trimming)&#xA;no-5-prime           3422    0.33     6711.32 +- 5411.26         70.92 +- 272.99     (bad trimming)&#xA;no-3-prime           3161    0.30     6701.35 +- 5739.86         87.41 +- 329.42     (bad trimming)&#xA;&#xA;low-coverage        27158    2.59     3222.51 +- 1936.79          4.99 +- 1.79       (easy to assemble, potential for lower quality consensus)&#xA;unique             636875   60.76     6240.20 +- 3908.44         25.22 +- 8.49       (easy to assemble, perfect, yay)&#xA;repeat-cont         48398    4.62     4099.55 +- 3002.72        335.54 +- 451.43     (potential for consensus errors, no impact on assembly)&#xA;repeat-dove           135    0.01    16996.33 +- 6860.08        397.37 +- 319.52     (hard to assemble, likely won't assemble correctly or even at all)&#xA;&#xA;span-repeat        137927   13.16     9329.94 +- 6906.27       2630.06 +- 3539.53    (read spans a large repeat, usually easy to assemble)&#xA;uniq-repeat-cont   155725   14.86     6529.83 +- 3463.16                             (should be uniquely placed, low potential for consensus errors, no impact on assembly)&#xA;uniq-repeat-dove    28248    2.70    12499.99 +- 8446.95                             (will end contigs, potential to misassemble)&#xA;uniq-anchor          5721    0.55     8379.86 +- 4575.71       3166.22 +- 3858.35    (repeat read, with unique section, probable bad read)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, the third type of repeat is giving me a bit of grief. Using the above assembly, here are the continuity parameters from the assembled contigs:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Total sequences: 3505&#xA;Total length: 322.867456 Mb&#xA;Longest sequence: 1.762243 Mb&#xA;Shortest sequence: 2.606 kb&#xA;Mean Length: 92.116 kb&#xA;Median Length: 42.667 kb&#xA;N50: 417 sequences; L50: 194.126 kb&#xA;N90: 1996 sequences; L90: 35.634 kb&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It's not a &lt;em&gt;bad&lt;/em&gt; assembly, particularly given the complexity of the genome, but I feel like it could be improved by tackling the complex genomic repeats in some fashion. About 60Mb of the contigs in this assembly are linked with each other in a huge web (based on the GFA output from Canu):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Tw8SQ.jpg&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Tw8SQ.jpg&quot; alt=&quot;60Mb linked structure from Canu GFA&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The repetitive regions are typically over 500bp in length, average about 3kb, and I've seen at least one case which seems to be a 20kb sequence duplicated in multiple regions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Canu defaults seem to give the best assembly results for the few parameters that I've tried, with one exception: trimming. I've tried playing around a little bit with the trimming parameters, and curiously a trimming coverage of 5X (with overlap of 500bp) seems to give a more contiguous assembly than with a trimming coverage of 2X (with the same overlap).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anyone is interested in having a look at these data themselves, called FASTQ files from Nippo sequencing runs can be found &lt;a href=&quot;https://www.ncbi.nlm.nih.gov//sra/?term=SRP092357&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;. I'm still in the process of uploading the raw nanopore signal files, but they will be available in the next couple of weeks associated with ENA project PRJEB20824. There's also a Zenodo archive &lt;a href=&quot;https://doi.org/10.5281/zenodo.569899&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt; that contains the GFA and assembly contigs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have any other suggestions on how I could resolve these complex repeats?&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-05-25T11:51:41.337" LastActivityDate="2017-05-29T23:41:52.653" Title="How can I improve a long-read assembly with a repetitive genome?" Tags="&lt;assembly&gt;&lt;nanopore&gt;&lt;long-reads&gt;&lt;canu&gt;&lt;repeat&gt;" AnswerCount="1" CommentCount="7" FavoriteCount="2" />
  <row Id="243" PostTypeId="5" CreationDate="2017-05-25T17:51:23.720" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-25T17:51:23.720" LastActivityDate="2017-05-25T17:51:23.720" CommentCount="0" />
  <row Id="244" PostTypeId="4" CreationDate="2017-05-25T17:51:23.720" Score="0" Body="When dealing with associating sequencing reads to a matching position in a set of reference sequences (typically a genome)." OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-05-31T13:48:58.563" LastActivityDate="2017-05-31T13:48:58.563" CommentCount="0" />
  <row Id="245" PostTypeId="5" CreationDate="2017-05-25T17:59:21.297" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-25T17:59:21.297" LastActivityDate="2017-05-25T17:59:21.297" CommentCount="0" />
  <row Id="246" PostTypeId="4" CreationDate="2017-05-25T17:59:21.297" Score="0" Body="The tricky art of scaling quantitative data across libraries, typically to account for differences in sequencing depth. This can also be about scaling for read source length, like transcript or gene length, in order to enable comparisons across genes." OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-05-31T13:46:57.663" LastActivityDate="2017-05-31T13:46:57.663" CommentCount="0" />
  <row Id="247" PostTypeId="5" CreationDate="2017-05-25T18:05:03.600" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-25T18:05:03.600" LastActivityDate="2017-05-25T18:05:03.600" CommentCount="0" />
  <row Id="248" PostTypeId="4" CreationDate="2017-05-25T18:05:03.600" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-05-25T18:05:03.600" LastActivityDate="2017-05-25T18:05:03.600" CommentCount="0" />
  <row Id="249" PostTypeId="1" AcceptedAnswerId="250" CreationDate="2017-05-25T18:21:49.743" Score="4" ViewCount="44" Body="&lt;p&gt;&lt;a href=&quot;https://genome.ucsc.edu/ENCODE/dataMatrix/encodeDataMatrixHuman.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;The ENCODE Experiment Matrix at UCSC&lt;/a&gt; lists the different available cell types under the categories &quot;Tier 1&quot;, &quot;Tier 2&quot; and &quot;Tier 3&quot;. What is the difference between these classifications?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What, for example, makes GM12878 a Tier 1 cell type and A549 a Tier 2 cell type?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Ejhqx.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Ejhqx.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="9" LastEditorUserId="61" LastEditDate="2017-05-26T09:05:34.530" LastActivityDate="2017-05-26T20:27:47.253" Title="What is the difference between ENCODE Tier 1, 2 and 3 cell types?" Tags="&lt;encode&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="250" PostTypeId="2" ParentId="249" CreationDate="2017-05-25T18:44:04.180" Score="5" Body="&lt;p&gt;The tiers denote the original priority of sequencing and processing the samples. This is understandable given the number of marks and experiment types that ENCODE tried to sequence. Further details are available on &lt;a href=&quot;http://genome.ucsc.edu/ENCODE/cellTypes.html&quot; rel=&quot;noreferrer&quot;&gt;ENCODE's website&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: Just to expand a bit, tier one cells were supposed to get processed first. Then tier 2 came second. Of course tier 2 had a LOT more cells, so it got split into 2 and 2.5, since there were already apparently tier 3 cells.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-25T18:44:04.180" CommentCount="0" />
  <row Id="251" PostTypeId="1" AcceptedAnswerId="263" CreationDate="2017-05-25T22:47:46.183" Score="2" ViewCount="33" Body="&lt;p&gt;Does &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3218867/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GISTIC 2.0&lt;/a&gt; estimate the background model:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;G = -log(Probability | Background)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;by permuting within the sample or across all samples in the set?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paper describes the probabilistic scoring method based on permutations, but I could not understand if this permutation is performed within the sample only. The &lt;a href=&quot;ftp://ftp.broadinstitute.org/pub/GISTIC2.0/GISTICDocumentation_standalone.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;documentation page&lt;/a&gt; seems to suggest across samples, but this would mean that different sized sets might lead to different outcomes on the sample sample.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, does it matter if the set is constituted by, for example, 10 samples of the same tissue of origin or ~1000 from multiple tissues?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks,&lt;/p&gt;&#xA;" OwnerUserId="193" LastEditorUserId="215" LastEditDate="2017-05-29T06:36:18.757" LastActivityDate="2017-05-29T21:34:18.543" Title="Does GISTIC (v 2.0) estimate amplified/deleted probabilities on a single sample basis?" Tags="&lt;gistic&gt;&lt;cnv&gt;&lt;snp6&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="252" PostTypeId="2" ParentId="230" CreationDate="2017-05-26T09:41:03.727" Score="3" Body="&lt;p&gt;For you the main point would be whether an enrichment analysis is going to give you an &lt;em&gt;informative&lt;/em&gt; answer. That's what will make a particular database better. And there's all sorts of subjective decisions made in their construction as to what what a pathway is, what to include, where to draw boundaries, etc. so different databases will give different answers and it will not be clear which is more correct.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So browse over the references / pointers people have given, select a service and use it and no others. Don't jump around databases until you get an answer you like, that's just fishing. &lt;/p&gt;&#xA;" OwnerUserId="377" LastActivityDate="2017-05-26T09:41:03.727" CommentCount="1" />
  <row Id="253" PostTypeId="1" AcceptedAnswerId="257" CreationDate="2017-05-26T13:23:00.853" Score="3" ViewCount="42" Body="&lt;p&gt;I am running samtools mpileup (v1.4) on a bam file with very choppy coverage (ChIP-seq style data). I want to get a first-pass list of positions with SNVs and their frequency as reported by the read counts, but no matter what I do, I keep getting all SNVs filtered out as not passing QC.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What's the magic parameter set for an initial list of SNVs and frequencies?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: this is a question I posted on &quot;the other&quot; website, but didn't get a reply there.&lt;/p&gt;&#xA;" OwnerUserId="180" LastActivityDate="2017-05-29T14:47:26.867" Title="variant calling on ChIP-seq style data: samtools mpileup with minimal filters" Tags="&lt;variant-calling&gt;&lt;samtools&gt;&lt;chip-seq&gt;&lt;mpileup&gt;" AnswerCount="2" CommentCount="6" />
  <row Id="254" PostTypeId="1" CreationDate="2017-05-26T14:06:59.617" Score="4" ViewCount="70" Body="&lt;p&gt;I have perform an enrichment analysis to a cluster of genes. The output is a list of pathways and their p-value (the pathways are selected because p-value &amp;lt; 0.05). The list is still quite long, so I want to reduce it. For that purpose I have a calculated the Dice coefficient of the pathways in a matrix $p$x$p$ where $p$ is the number of pathways in the list. I want both the ones that are more different (they overlap less, their Dice coefficient is lower) and the pathways more representative of the most similar pathways (So if a there is a group of 5 pathways that overlap over 0.8 take just one).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I select the most representatives pathways? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a similar &lt;a href=&quot;http://revigo.irb.hr/&quot; rel=&quot;nofollow noreferrer&quot;&gt;tool&lt;/a&gt; for GO but it relays on discarding not significant GO, while here all the initial pathways are already significant. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I do a clustering of the genes using the Dice coefficient matrix I don't know where (or how) to cut.&#xA; &lt;a href=&quot;https://i.stack.imgur.com/mMdEH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/mMdEH.png&quot; alt=&quot;circular dendrogara&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried using the height to select the pathways. But I am unsure of the interpretation of height. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some other tools I have seen use a multidimensional scaling plot, but I am not sure if performing it and cutting at certain point of the first dimension would help.&#xA;&lt;a href=&quot;https://i.stack.imgur.com/CeetJ.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/CeetJ.png&quot; alt=&quot;MDS plot&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="73" LastEditDate="2017-05-28T10:50:21.933" LastActivityDate="2017-05-28T10:50:21.933" Title="How to select the most representative pathways from a gene enrichment analysis?" Tags="&lt;report&gt;&lt;pathway&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="1" />
  <row Id="255" PostTypeId="2" ParentId="249" CreationDate="2017-05-26T20:27:47.253" Score="1" Body="&lt;p&gt;From &lt;a href=&quot;https://www.genome.gov/26524238/encode-project-common-cell-types/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Encode's website&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;... To aid in the integration and&#xA;  comparison of data produced using different technologies and&#xA;  platforms, the ENCODE Consortium has designated cell types that will&#xA;  be used by all investigators. These common cell types include both&#xA;  cell lines and primary cell types, and plans are being made to explore&#xA;  the use of primary tissues and embryonic stem (ES) cells.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Cell types were selected largely for practical reasons, including&#xA;  their wide availability, the ability to grow them easily, and their&#xA;  capacity to produce sufficient numbers of cells for use in all&#xA;  technologies being used by ENCODE investigators. Secondary&#xA;  considerations were the diversity in tissue source of the cells, germ&#xA;  layer lineage representation, the availability of existing data&#xA;  generated using the cell type, and coordination with other ongoing&#xA;  projects. Effort was also made to select at least some cell types that&#xA;  have a relatively normal karyotype...&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="99" LastActivityDate="2017-05-26T20:27:47.253" CommentCount="0" />
  <row Id="257" PostTypeId="2" ParentId="253" CreationDate="2017-05-27T01:16:15.507" Score="5" Body="&lt;p&gt;I used this in the past for ChIP-seq data and it generated SNVs:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools mpileup \&#xA;--uncompressed --max-depth 10000 --min-MQ 20 --ignore-RG --skip-indels \&#xA;--fasta-ref ref.fa file.bam \&#xA;| bcftools call --consensus-caller \&#xA;&amp;gt; out.vcf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This was samtools 1.3 in case that makes a difference.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-05-27T01:16:15.507" CommentCount="1" />
  <row Id="259" PostTypeId="2" ParentId="254" CreationDate="2017-05-27T10:43:34.673" Score="1" Body="&lt;p&gt;If you're happy with a more confident ranking of the most representative gene sets, rather than necessarily cutting down the list, you might try EGSEA. It uses an ensemble approach to give a ranking of the most relevant gene sets, and also produces an interactive HTML output with statistics, heatmaps, pathway maps, summary plots and GO graphs which allows you to examine the output at varying levels of granularity. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can read the paper on &lt;a href=&quot;http://biorxiv.org/content/early/2016/07/08/042580&quot; rel=&quot;nofollow noreferrer&quot;&gt;bioRxiv&lt;/a&gt; or download the package from &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/EGSEA.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Bioconductor&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="163" LastEditDate="2017-05-27T23:58:34.373" LastActivityDate="2017-05-27T23:58:34.373" CommentCount="2" />
  <row Id="261" PostTypeId="2" ParentId="107" CreationDate="2017-05-27T16:05:14.763" Score="2" Body="&lt;p&gt;Perhaps &lt;a href=&quot;http://genomecrispr.dkfz.de/#!/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GenomeCRISPR&lt;/a&gt;, I've personally never used it, but it has a well-documented API and appears you could automate it to go through the whole genome.&lt;/p&gt;&#xA;" OwnerUserId="33" LastActivityDate="2017-05-27T16:05:14.763" CommentCount="0" />
  <row Id="262" PostTypeId="2" ParentId="254" CreationDate="2017-05-28T10:23:33.793" Score="1" Body="&lt;p&gt;This sounds like something that might be amenable to a clustered heatmap plot, or a correlation matrix plot, or something similar. Have you looked at a correlation matrix of the dice coefficient matrix (or maybe just a heatmap plot of that matrix without the correlation matrix)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;corrplot&lt;/code&gt; package looks like it might be useful, in particular the &lt;code&gt;hclust&lt;/code&gt; / drawing rectangles presentation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can't vouch for this package though; it's just something I found by a search for &quot;R plot correlation matrix&quot;.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-05-28T10:23:33.793" CommentCount="1" />
  <row Id="263" PostTypeId="2" ParentId="251" CreationDate="2017-05-28T11:40:16.593" Score="2" Body="&lt;p&gt;There doesn't seem to be much differences between GISTIC 1.0 and 2.0 as it says:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;As with GISTIC 1.0, we obtain P-values for each marker by comparing the score at each locus to a background score distribution generated by random permutation of the marker locations in each sample&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;But on the &lt;a href=&quot;http://www.pnas.org.sare.upf.edu/content/suppl/2007/11/20/0710052104.DC1#ST&quot; rel=&quot;nofollow noreferrer&quot;&gt;supplementary material&lt;/a&gt; of the GISTIC 1.0 there is a more detailed explanation of the method. See the &quot;Stage 2&quot; section:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Second, we compare these G-scores to the distribution of scores expected if only random aberrations were observed. This distribution can be determined by rescoring the genome after permuting marker locations within each sample; we instead derive a semiexact estimate.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Furthermore in another section (&quot;Stage 2: Aggregation of Data from Different Tumors to Differentiate Between Driver and Passenger Aberrations&quot;) it says:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;To determine which of the aberrations identified in Stage 1 are likely to represent driver events, we aggregate the data from all tumors used in the analysis to generate summary scores for amplifications, deletions, and LOH. The statistical significance of each score is determined by comparison to the distribution of scores obtained by all permutations of the data (using a semiexact approximation), with correction for multiple hypothesis testing. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;However, the relevant section (of the supplementary materials) seems to be &quot;Null Hypothesis Generation: An Analytic Derivation of the Null Distribution&quot;, where it describes the semiexact approximation used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a &lt;a href=&quot;http://www.pnas.org.sare.upf.edu/content/suppl/2007/11/20/0710052104.DC1/10052Fig9.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;supplementary file&lt;/a&gt; it describes it as &quot;Generate all permutations of SNP labels within each sample to simulate datasets with random aberrations&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems that the background probabilities are calculated within sample.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can't say if it matters how the sets are constituted but I would say that the broader the sets are, the better estimation of the structural variations it will perform but the same estimation of the background will be done. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ultimately you can check the code of the program, or test with 10 samples and replace one of them to see if the results are changing accordingly. &lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2017-05-29T21:34:18.543" LastActivityDate="2017-05-29T21:34:18.543" CommentCount="1" />
  <row Id="264" PostTypeId="1" AcceptedAnswerId="270" CreationDate="2017-05-28T21:40:13.940" Score="8" ViewCount="75" Body="&lt;p&gt;I got a bunch of vcf files (v4.1) with structural variations of bunch of non-model organisms (i.e. there are no known variants). I found there are quite a some tools to manipulate vcf files like &lt;a href=&quot;https://vcftools.github.io/&quot; rel=&quot;nofollow noreferrer&quot;&gt;VCFtools&lt;/a&gt;, R package &lt;a href=&quot;https://cran.r-project.org/web/packages/vcfR/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;vcfR&lt;/a&gt; or python library &lt;a href=&quot;https://pypi.python.org/pypi/PyVCF&quot; rel=&quot;nofollow noreferrer&quot;&gt;PyVCF&lt;/a&gt;. However none of them seems to provide a quick summary, something like (preferably categorised by size as well):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;type    count&#xA;DEL     x&#xA;INS     y&#xA;INV     z&#xA;....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any tool or a function I overlooked that produces summaries of this style?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that vcf file is just a plain text file and if I will dissect &lt;code&gt;REF&lt;/code&gt; and &lt;code&gt;ALT&lt;/code&gt; columns I should be able to write a script that will do the job, but I hoped that I could avoid to write my own parser.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;--- edit ---&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far it seems that only tool that aims to do summaries (@gringer answer) is not working on vcf v4.1. Other tools would provide just partial solution by filtering certain variant type. Therefore I accept my own parser perl/R solutions, till there will be a &lt;strong&gt;working&lt;/strong&gt; tool for &lt;strong&gt;stats&lt;/strong&gt; of vcf with &lt;strong&gt;structural variants&lt;/strong&gt;.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-03T11:03:39.833" LastActivityDate="2017-06-03T11:03:39.833" Title="Is there an easy way to create a summary of a VCF file (v4.1) with structural variations?" Tags="&lt;vcf&gt;&lt;structural-variation&gt;" AnswerCount="3" CommentCount="5" FavoriteCount="1" />
  <row Id="266" PostTypeId="1" CreationDate="2017-05-29T00:01:03.100" Score="4" ViewCount="47" Body="&lt;p&gt;I'm just getting started with the Mauve aligner and I'm finding the documentation a bit lacking. I'm using the &lt;code&gt;progressiveMauve&lt;/code&gt; tool from the command line and would like to output an identity matrix file. The set of output files it generates by default for &lt;code&gt;progressiveMauve --output=outfile&lt;/code&gt; are &lt;code&gt;outfile&lt;/code&gt;, &lt;code&gt;outfile.backbone&lt;/code&gt; and &lt;code&gt;output.bbcols&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;http://darlinglab.org/mauve/user-guide/progressivemauve.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;documentation&lt;/a&gt; for the tool lists an &lt;code&gt;--input-id-matrix&lt;/code&gt; option but this doesn't seem to have any effect. The &lt;a href=&quot;http://darlinglab.org/mauve/user-guide/files.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;documentation page for output files&lt;/a&gt; does describe an identity matrix file but doesn't say anything about how to generate it.&lt;/p&gt;&#xA;" OwnerUserId="126" LastActivityDate="2017-07-31T12:57:27.853" Title="How can I output an identity matrix in progressiveMauve?" Tags="&lt;alignment&gt;&lt;mauve&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="268" PostTypeId="2" ParentId="264" CreationDate="2017-05-29T05:16:36.277" Score="5" Body="&lt;p&gt;According to the &lt;code&gt;bcftools&lt;/code&gt; man page, it is able to produce statistics using the command &lt;code&gt;bcftools stats&lt;/code&gt;. Running this myself, the statistics look like what you're asking for:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# This file was produced by bcftools stats (1.2-187-g1a55e45+htslib-1.2.1-256-ga356746) and can be plotted using plot-vcfstats.&#xA;# The command line was: bcftools stats  OVLNormalised_STARout_KCCG_called.vcf.gz&#xA;#&#xA;# Definition of sets:&#xA;# ID    [2]id   [3]tab-separated file names&#xA;ID  0   OVLNormalised_STARout_KCCG_called.vcf.gz&#xA;# SN, Summary numbers:&#xA;# SN    [2]id   [3]key  [4]value&#xA;SN  0   number of samples:  108&#xA;SN  0   number of records:  333&#xA;SN  0   number of no-ALTs:  0&#xA;SN  0   number of SNPs: 313&#xA;SN  0   number of MNPs: 0&#xA;SN  0   number of indels:   20&#xA;SN  0   number of others:   0&#xA;SN  0   number of multiallelic sites:   0&#xA;SN  0   number of multiallelic SNP sites:   0&#xA;# TSTV, transitions/transversions:&#xA;# TSTV  [2]id   [3]ts   [4]tv   [5]ts/tv    [6]ts (1st ALT) [7]tv (1st ALT) [8]ts/tv (1st ALT)&#xA;TSTV    0   302 11  27.45   302 11  27.45&#xA;# SiS, Singleton stats:&#xA;...&#xA;# IDD, InDel distribution:&#xA;# IDD   [2]id   [3]length (deletions negative)  [4]count&#xA;IDD 0   -9  1&#xA;IDD 0   -2  4&#xA;IDD 0   -1  6&#xA;IDD 0   1   4&#xA;IDD 0   2   1&#xA;IDD 0   3   3&#xA;IDD 0   4   1&#xA;# ST, Substitution types:&#xA;# ST    [2]id   [3]type [4]count&#xA;ST  0   A&amp;gt;C 2&#xA;ST  0   A&amp;gt;G 78&#xA;ST  0   A&amp;gt;T 2&#xA;ST  0   C&amp;gt;A 5&#xA;ST  0   C&amp;gt;G 0&#xA;ST  0   C&amp;gt;T 66&#xA;ST  0   G&amp;gt;A 67&#xA;ST  0   G&amp;gt;C 0&#xA;ST  0   G&amp;gt;T 1&#xA;ST  0   T&amp;gt;A 1&#xA;ST  0   T&amp;gt;C 91&#xA;ST  0   T&amp;gt;G 0&#xA;# DP, Depth distribution&#xA;# DP    [2]id   [3]bin  [4]number of genotypes  [5]fraction of genotypes (%)    [6]number of sites  [7]fraction of sites (%)&#xA;DP  0   &amp;gt;500    0   0.000000    333 100.000000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-05-29T05:16:36.277" CommentCount="5" />
  <row Id="269" PostTypeId="2" ParentId="264" CreationDate="2017-05-29T05:23:32.083" Score="1" Body="&lt;p&gt;You might want to try &lt;a href=&quot;https://github.com/pjotrp/bioruby-vcf&quot; rel=&quot;nofollow noreferrer&quot;&gt;Bio-VCF&lt;/a&gt;. From the authors description &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Bio-vcf is a new generation VCF parser, filter and converter. Bio-vcf&#xA;  is not only very fast for genome-wide (WGS) data, it also comes with a&#xA;  really nice filtering, evaluation and rewrite language and it can&#xA;  output any type of textual data, including VCF header and contents in&#xA;  RDF and JSON.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In addition it is a very fast parser. The rewrite DSL might help you customise your filtering and needs. &lt;/p&gt;&#xA;" OwnerUserId="27" LastActivityDate="2017-05-29T05:23:32.083" CommentCount="1" />
  <row Id="270" PostTypeId="2" ParentId="264" CreationDate="2017-05-29T06:22:59.417" Score="2" Body="&lt;p&gt;The &quot;my own parser&quot; solutions. The information I was searching for in part of column &lt;code&gt;INFO&lt;/code&gt;, namely in variables &lt;code&gt;SVLEN&lt;/code&gt; and &lt;code&gt;SVTYPE&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;very fast SV types + counts (by @user172818 in commnent) :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;zcat var.vcf.gz | perl -ne 'print &quot;$1\n&quot; if /[;\t]SVTYPE=([^;\t]+)/' | sort | uniq -c&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;quite slow SV types + counts + sizes :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SV_colnames &amp;lt;- c('CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO', 'FORMAT', 'SAMPLE1')&#xA;&#xA;ssplit &amp;lt;- function(s, split = '='){&#xA;    unlist(strsplit(s, split = split))&#xA;}&#xA;&#xA;# note, capital letters just respect original naming conventions of the VCF file&#xA;getSVTYPE &amp;lt;- function(info){&#xA;    ssplit(grep(&quot;SVTYPE&quot;, info, value = T))[2]&#xA;}&#xA;&#xA;getSVLEN &amp;lt;- function(info){&#xA;    SVLEN &amp;lt;- ssplit(grep(&quot;SVLEN&quot;, info, value = T))&#xA;    ifelse(length(SVLEN) == 0, NA, as.numeric(SVLEN[2]))&#xA;}&#xA;&#xA;load_sv &amp;lt;- function(file){&#xA;    vcf_sv_table &amp;lt;- read.table(file, stringsAsFactors = F)&#xA;    colnames(vcf_sv_table) &amp;lt;- SV_colnames&#xA;    # possible filtering&#xA;    # vcf_sv_table &amp;lt;- vcf_sv_table[vcf_sv_table$FILTER == 'PASS',]&#xA;    vcf_sv_table_info &amp;lt;- strsplit(vcf_sv_table$INFO, ';')&#xA;    vcf_sv_table$SVTYPE &amp;lt;- unlist(lapply(vcf_sv_table_info, getSVTYPE))&#xA;    vcf_sv_table$SVLEN &amp;lt;- unlist(lapply(vcf_sv_table_info, getSVLEN))&#xA;    return(vcf_sv_table)&#xA;}&#xA;&#xA;sv_df &amp;lt;- load_sv('my_sv_calls.vcf')&#xA;table(sv_df$SVTYPE)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-03T09:46:11.493" LastActivityDate="2017-06-03T09:46:11.493" CommentCount="2" />
  <row Id="271" PostTypeId="1" AcceptedAnswerId="466" CreationDate="2017-05-29T06:25:30.990" Score="8" ViewCount="101" Body="&lt;p&gt;The IGSR has a &lt;a href=&quot;http://www.internationalgenome.org/wiki/Analysis/Variant%20Call%20Format/VCF%20%28Variant%20Call%20Format%29%20version%204.0/encoding-structural-variants&quot; rel=&quot;nofollow noreferrer&quot;&gt;sample&lt;/a&gt; for encoding structural variants in the VCF 4.0 format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An example from the site (the first record):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#CHROM  POS   ID  REF ALT   QUAL  FILTER  INFO  FORMAT  NA00001&#xA;1 2827693   . CCGTGGATGCGGGGACCCGCATCCCCTCTCCCTTCACAGCTGAGTGACCCACATCCCCTCTCCCCTCGCA  C . PASS  SVTYPE=DEL;END=2827680;BKPTID=Pindel_LCS_D1099159;HOMLEN=1;HOMSEQ=C;SVLEN=-66 GT:GQ 1/1:13.9&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How to read it? From what I can see:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This is a deletion (&lt;code&gt;SVTYPE=DEL&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;The end position of the variant comes before the starting position (reverse strand?)&lt;/li&gt;&#xA;&lt;li&gt;The reference starts from &lt;code&gt;2827693&lt;/code&gt; to &lt;code&gt;2827680&lt;/code&gt; (13 bases on the reverse strand)&lt;/li&gt;&#xA;&lt;li&gt;The difference between reference and alternative is &lt;code&gt;66&lt;/code&gt; bases (&lt;code&gt;SVLEN=-66&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;This doesn't sound right to me. For instance, I don't see where exactly the deletion starts. The &lt;code&gt;SVLEN&lt;/code&gt; field says &lt;code&gt;66&lt;/code&gt; bases deleted, but where? &lt;code&gt;2827693&lt;/code&gt; to &lt;code&gt;2827680&lt;/code&gt; only has 13 bases between.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; How to read the deletion correctly from this structural VCF record? Where is the missing 66-13=53 bases?&lt;/p&gt;&#xA;" OwnerUserId="174" LastEditorUserId="125" LastEditDate="2017-05-29T21:07:13.593" LastActivityDate="2017-06-05T21:05:51.263" Title="How to read structural variant VCF?" Tags="&lt;vcf&gt;&lt;variants&gt;&lt;structural&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="272" PostTypeId="2" ParentId="253" CreationDate="2017-05-29T14:47:26.867" Score="3" Body="&lt;p&gt;Another approach is &lt;a href=&quot;https://github.com/lh3/htsbox&quot; rel=&quot;nofollow noreferrer&quot;&gt;htsbox&lt;/a&gt;. You can get a candidate list with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;htsbox pileup -Cvcf ref.fa -q20 -Q20 -s5 file.bam &amp;gt; out.vcf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here, &lt;code&gt;-q&lt;/code&gt; sets min mapping quality, &lt;code&gt;-Q&lt;/code&gt; sets min base quality, &lt;code&gt;-v&lt;/code&gt; outputs variants only &lt;code&gt;-c&lt;/code&gt; outputs VCF, &lt;code&gt;-C&lt;/code&gt; gives you base counts on both strands and finally &lt;code&gt;-s5&lt;/code&gt; requires at least 5 high-quality bases to call out an allele. It is useful when your data are failing the assumptions made by typical variant callers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why not samtools+bcftools or varscan? Transparency and speed. This command line simply counts based on the parameters you use. It does not apply additional operations. And because of this, it is over an order of magnitude faster than samtools mpileup or varscan. It is worth noting that samtools uses BAQ by default, which reduces FPs occasionally. However, BAQ is not quite necessary for longer Illumina reads and it hurts sensitivity at the same time.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-05-29T14:47:26.867" CommentCount="1" />
  <row Id="273" PostTypeId="2" ParentId="202" CreationDate="2017-05-29T19:09:21.650" Score="1" Body="&lt;p&gt;&lt;strong&gt;Simulators designed specifically for Oxford Nanopore:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/bcgsc/NanoSim&quot; rel=&quot;nofollow noreferrer&quot;&gt;NanoSim&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/karel-brinda/nanosim-h&quot; rel=&quot;nofollow noreferrer&quot;&gt;NanoSim-H&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/ethanagbaker/SiLiCO&quot; rel=&quot;nofollow noreferrer&quot;&gt;SiLiCO&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://sourceforge.net/p/readsim/wiki/Home/&quot; rel=&quot;nofollow noreferrer&quot;&gt;ReadSim&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;General long read simulators:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/gt1/loresim&quot; rel=&quot;nofollow noreferrer&quot;&gt;Loresim&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/gt1/loresim2&quot; rel=&quot;nofollow noreferrer&quot;&gt;Loresim 2&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://sourceforge.net/projects/fastqsim/&quot; rel=&quot;nofollow noreferrer&quot;&gt;FASTQsim&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/bioinform/longislnd&quot; rel=&quot;nofollow noreferrer&quot;&gt;LongISLND&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;For an exhaustive list of existing read simulators, see page 15 of this &lt;a href=&quot;http://brinda.cz/publications/brinda_phd.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;thesis&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-05-29T19:14:45.463" LastActivityDate="2017-05-29T19:14:45.463" CommentCount="0" />
  <row Id="274" PostTypeId="2" ParentId="81" CreationDate="2017-05-29T19:46:25.187" Score="-1" Body="&lt;p&gt;You can split your FASTA file sequence-by-sequence using&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;split -a 6 -p '^&amp;gt;' your_file.fa seq_&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and then use any existing read simulator supporting coverage (ART, DWGsim, etc.). If you want to have all the reads mixed (not ordered by the original sequence), you can use RNFtools.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit 1:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As @terdon pointed out, the previous command works on OS X only. An analogical one liner for Linux (but with a slightly different naming scheme using numbers rather than letters) can be&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;csplit -f seq_ -n 6 your_file.fa '/^&amp;gt;/' {*}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To make this command work also on OS X, one needs to install coreutils (e.g., using brew) and then use &lt;code&gt;gcsplit&lt;/code&gt; instead of &lt;code&gt;csplit&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit 2:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once FASTA is split by sequences, the simulation becomes straightforward and many different approaches can be used. My favorite one is using GNU Parallel. Imagine that you have your coverages in a text file called &lt;code&gt;covs.txt&lt;/code&gt; on separate lines and in the same order as the sequences in &lt;code&gt;your_file.fa&lt;/code&gt;, e.g.,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;40&#xA;30&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then you can simulate reads from the original sequences using DWGsim by&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ls -1 seq_* | paste covs.txt - \&#xA;    | parallel -v --colsep '\t' dwgsim -1 100 -2 100 -C {1} {2} sim_{2}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and merge the obtained FASTQ files using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat sim_seq_*.bwa.read1.fastq &amp;gt; reads.1.fq&#xA;cat sim_seq_*.bwa.read2.fastq &amp;gt; reads.2.fq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;One possible danger of this approach is that we have assumed that the number of seq_* files is the same as the number of lines in &lt;code&gt;covs.txt&lt;/code&gt;, which might not be true (by mistake). We should check this prior to the simulation step, e.g., by:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[[ &quot;$(ls -1 seq_* | wc -l)&quot; == &quot;$(cat covs.txt | wc -l)&quot; ]] \&#xA;    || echo &quot;Incorrent number of lines in covs.txt&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Another caveat is that the simulated reads are not in a random order (they are grouped by source sequences).&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-05-30T18:09:35.290" LastActivityDate="2017-05-30T18:09:35.290" CommentCount="3" />
  <row Id="275" PostTypeId="2" ParentId="271" CreationDate="2017-05-29T22:34:37.427" Score="3" Body="&lt;p&gt;So, first off, as others have pointed out, I'm pretty sure that example is just wrong. At least, the numbers don't match as you've pointed out. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, it is impossible to be sure without showing us the header of the VCF file as well. The INFO field (the 5th field of a VCF file) is very, very variable and depends entirely on the header lines. Each program (or human) implementing a VCF is free to choose to have whatever they feel like in the INFO field. However, each &lt;code&gt;IDENTIFIER=&lt;/code&gt; needs to have an associated INFO line at the beginning of the file. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, the &lt;code&gt;SVTYPE&lt;/code&gt;, &lt;code&gt;SVLEN&lt;/code&gt;, &lt;code&gt;HOMLEN&lt;/code&gt; etc will have commented (start with a &lt;code&gt;#&lt;/code&gt;) lines at the beginning of the file explaining what these values are. So check those, even though they're relatively standard, you never know, the obvious reading you used might be wrong despite its seeming so reasonable. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a newer example of a VCF line for an SV taken from the current &lt;a href=&quot;https://github.com/samtools/hts-specs/blob/master/VCFv4.3.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;VCF specification&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;##fileformat=VCFv4.1&#xA;##fileDate=20100501&#xA;##reference=1000GenomesPilot-NCBI36&#xA;##assembly=ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/sv/breakpoint_assemblies.fasta&#xA;##INFO=&amp;lt;ID=BKPTID,Number=.,Type=String,Description=&quot;ID of the assembled alternate allele in the assembly file&quot;&amp;gt;&#xA;##INFO=&amp;lt;ID=CIEND,Number=2,Type=Integer,Description=&quot;Confidence interval around END for imprecise variants&quot;&amp;gt;&#xA;##INFO=&amp;lt;ID=CIPOS,Number=2,Type=Integer,Description=&quot;Confidence interval around POS for imprecise variants&quot;&amp;gt;&#xA;##INFO=&amp;lt;ID=END,Number=1,Type=Integer,Description=&quot;End position of the variant described in this record&quot;&amp;gt;&#xA;##INFO=&amp;lt;ID=HOMLEN,Number=.,Type=Integer,Description=&quot;Length of base pair identical micro-homology at event breakpoints&quot;&amp;gt;&#xA;##INFO=&amp;lt;ID=HOMSEQ,Number=.,Type=String,Description=&quot;Sequence of base pair identical micro-homology at event breakpoints&quot;&amp;gt;&#xA;##INFO=&amp;lt;ID=SVLEN,Number=.,Type=Integer,Description=&quot;Difference in length between REF and ALT alleles&quot;&amp;gt;&#xA;##INFO=&amp;lt;ID=SVTYPE,Number=1,Type=String,Description=&quot;Type of structural variant&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=DEL,Description=&quot;Deletion&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=DEL:ME:ALU,Description=&quot;Deletion of ALU element&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=DEL:ME:L1,Description=&quot;Deletion of L1 element&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=DUP,Description=&quot;Duplication&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=DUP:TANDEM,Description=&quot;Tandem Duplication&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=INS,Description=&quot;Insertion of novel sequence&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=INS:ME:ALU,Description=&quot;Insertion of ALU element&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=INS:ME:L1,Description=&quot;Insertion of L1 element&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=INV,Description=&quot;Inversion&quot;&amp;gt;&#xA;##ALT=&amp;lt;ID=CNV,Description=&quot;Copy number variable region&quot;&amp;gt;&#xA;##FORMAT=&amp;lt;ID=GT,Number=1,Type=String,Description=&quot;Genotype&quot;&amp;gt;&#xA;##FORMAT=&amp;lt;ID=GQ,Number=1,Type=Float,Description=&quot;Genotype quality&quot;&amp;gt;&#xA;##FORMAT=&amp;lt;ID=CN,Number=1,Type=Integer,Description=&quot;Copy number genotype for imprecise events&quot;&amp;gt;&#xA;##FORMAT=&amp;lt;ID=CNQ,Number=1,Type=Float,Description=&quot;Copy number genotype quality for imprecise events&quot;&amp;gt;&#xA;#CHROM POS     ID        REF             ALT QUAL FILTER INFO                                               FORMAT NA00001&#xA;1      2827694 rs2376870 CGTGGATGCGGGGAC C   .    PASS   SVTYPE=DEL;END=2827708;HOMLEN=1;HOMSEQ=G;SVLEN=-14 GT:GQ  1/1:13.9&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note how the numbers do match and also note how each of the subfields in the INFO field is explained with an &lt;code&gt;##INFO&lt;/code&gt; line. &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-05-29T22:34:37.427" CommentCount="0" />
  <row Id="276" PostTypeId="2" ParentId="242" CreationDate="2017-05-29T23:41:52.653" Score="5" Body="&lt;p&gt;You can't resolve 20kb near identical repeats/segdups with 10kb reads. All you can do is to bet your luck on a few excessively long reads spanning some units by chance. For divergent copies, it is worth looking at &lt;a href=&quot;http://biorxiv.org/content/early/2016/05/14/053256&quot; rel=&quot;noreferrer&quot;&gt;this paper&lt;/a&gt;. It uses Illumina reads to identify k-mers in unique regions and ignores non-unique k-mers at the overlapping stage. The paper said that this strategy is better than using standard overlappers, which I buy, but probably it can't resolve a 20kb segdup with a handful of mismatches, either.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Such mismatch-based approaches always have limitations and may not work for recent segdups/repeats. The ultimate solution is to get long reads, longer than your repeat/segdup units. The ~100kb reads in the &lt;a href=&quot;http://biorxiv.org/content/early/2017/04/20/128835&quot; rel=&quot;noreferrer&quot;&gt;recent preprint&lt;/a&gt; will be a game changer for you. If your ~20kb repeats are not tandem, 10X's ~100kb linked reads may help, too.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-05-29T23:41:52.653" CommentCount="2" />
  <row Id="277" PostTypeId="2" ParentId="81" CreationDate="2017-05-30T07:06:02.543" Score="5" Body="&lt;p&gt;I am working on a Illumina sequencing simulator for metagenomics: &lt;a href=&quot;https://github.com/HadrienG/InSilicoSeq&quot; rel=&quot;noreferrer&quot;&gt;InSilicoSeq&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is still in alpha release and very experimental, but given a multi-fasta and an abundance file, it will generate reads from your input genomes with different coverages.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From the documentation:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;iss generate --genomes genomes.fasta --abundance abundance_file.txt \&#xA;    --model_file HiSeq2500 --output HiSeq_reads&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# multi-fasta file&#xA;&amp;gt;genome_A&#xA;ATGC...&#xA;&amp;gt;genome_B&#xA;CCGT...&#xA;...&#xA;&#xA;# abundance file (total abundance must be 1!)&#xA;genome_A    0.2&#xA;genome_B    0.4&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I didn't design it to work with coverage but rather abundance of the genome in a metagenome, so you might have to do a tiny bit of math ;)&lt;/p&gt;&#xA;" OwnerUserId="179" LastActivityDate="2017-05-30T07:06:02.543" CommentCount="0" />
  <row Id="278" PostTypeId="1" AcceptedAnswerId="340" CreationDate="2017-05-30T07:30:54.223" Score="4" ViewCount="53" Body="&lt;p&gt;Using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Laser_capture_microdissection&quot; rel=&quot;nofollow noreferrer&quot;&gt;laser-capture microdissection&lt;/a&gt; of cells a group of cells stained with the marker of interest was sequenced. In another cohort of patients (this is all  human liver tissue) the whole tissue was sequenced (RNA-seq in both cases)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can I estimate the contribution of the cells marked in the whole liver (&quot;weight of these cells&quot; in the liver in words of my PI)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My gut feeling is that it can't be done this way, it would require both single cell sequencing and whole tissue sequencing to estimate the contribution of each cell line. But perhaps there is a tool that given the cell lines or the main expression of other cells lines it can be compared to using &lt;a href=&quot;http://www.bioconductor.org/packages/GSVA/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GSVA&lt;/a&gt; or some similar tool. &lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="73" LastEditDate="2017-06-01T12:28:58.280" LastActivityDate="2017-06-01T12:48:19.277" Title="How can the cell line contribution be estimated from RNASeq data?" Tags="&lt;rna-seq&gt;&lt;estimation&gt;&lt;cell-line&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="280" PostTypeId="1" AcceptedAnswerId="281" CreationDate="2017-05-30T09:05:00.953" Score="6" ViewCount="39" Body="&lt;p&gt;I have been using &lt;a href=&quot;https://github.com/alexdobin/STAR&quot; rel=&quot;nofollow noreferrer&quot;&gt;STAR&lt;/a&gt; for our RNA-Seq samples. The &lt;code&gt;final.out&lt;/code&gt; log file reports percentage of uniquely mapped reads along with percentage of reads that map to &lt;code&gt;multiple loci&lt;/code&gt; (less than or equal to 10) and percentage of reads mapping to &lt;code&gt;too many loci&lt;/code&gt; (greater than 10). However, I want to break down the &lt;code&gt;multiple loci&lt;/code&gt; part to individual counts: Reads mapping to 2 locations, 3 locations, 4 locations .. 10 locations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;NH&lt;/code&gt; tag seems to be used by &lt;code&gt;STAR&lt;/code&gt;. However a naive read counting approach results in it reporting more number of reads than total reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, my &lt;code&gt;final.out&lt;/code&gt; looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   Mapping speed, Million of reads per hour |       1403.36&#xA;&#xA;                      Number of input reads |       53015978&#xA;                  Average input read length |       26&#xA;                                UNIQUE READS:&#xA;               Uniquely mapped reads number |       368916&#xA;                    Uniquely mapped reads % |       0.70%&#xA;                      Average mapped length |       26.45&#xA;                   Number of splices: Total |       1057&#xA;        Number of splices: Annotated (sjdb) |       0&#xA;                   Number of splices: GT/AG |       802&#xA;                   Number of splices: GC/AG |       1&#xA;                   Number of splices: AT/AC |       0&#xA;           Number of splices: Non-canonical |       254&#xA;                  Mismatch rate per base, % |       0.31%&#xA;                     Deletion rate per base |       0.00%&#xA;                    Deletion average length |       1.45&#xA;                    Insertion rate per base |       0.00%&#xA;                   Insertion average length |       1.00&#xA;                         MULTI-MAPPING READS:&#xA;    Number of reads mapped to multiple loci |       45766732&#xA;         % of reads mapped to multiple loci |       86.33%&#xA;    Number of reads mapped to too many loci |       3757890&#xA;         % of reads mapped to too many loci |       7.09%&#xA;                              UNMAPPED READS:&#xA;   % of reads unmapped: too many mismatches |       0.00%&#xA;             % of reads unmapped: too short |       5.89%&#xA;                 % of reads unmapped: other |       0.00%&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Counting histogram of number of positions a read maps to using &lt;code&gt;pysam&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def get_reads_hist(bam): &#xA;    bam = pysam.AlignmentFile(bam, 'rb')&#xA;    counts = Counter()&#xA;    for query in bam.fetch():&#xA;        nh_count = Counter(dict(query.get_tags())['NH'])&#xA;        counts += nh_count            &#xA;    return counts&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;results in &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Counter({1: 330606,&#xA;     2: 86772164,&#xA;     3: 329,&#xA;     4: 38083,&#xA;     5: 31,&#xA;     6: 1094,&#xA;     7: 129,&#xA;     8: 50,&#xA;     10: 50})&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The count &lt;code&gt;1&lt;/code&gt; reads are fine even though they do not match the counts in &lt;code&gt;final.out&lt;/code&gt; file since I am counting a certain category of reads (say those mapping to &lt;code&gt;tRNA&lt;/code&gt; only), but the reads mapping to 2 locations are highly overestimated. Why is that?&lt;/p&gt;&#xA;" OwnerUserId="161" LastEditorUserId="77" LastEditDate="2017-05-30T09:33:43.417" LastActivityDate="2017-05-30T09:33:43.417" Title="Quantifying reads mapping to multiple loci" Tags="&lt;rna-seq&gt;&lt;bam&gt;&lt;samtools&gt;&lt;star&gt;&lt;pysam&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="281" PostTypeId="2" ParentId="280" CreationDate="2017-05-30T09:08:11.583" Score="7" Body="&lt;p&gt;You almost had the correct python code already, you just need to filter out secondary alignments:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def get_reads_hist(bam): &#xA;    bam = pysam.AlignmentFile(bam, 'rb')&#xA;    counts = Counter()&#xA;    for query in bam.fetch():&#xA;        if query.is_secondary:&#xA;            continue&#xA;        nh_count = Counter(dict(query.get_tags())['NH'])&#xA;        counts += nh_count            &#xA;    return counts&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-30T09:08:11.583" CommentCount="0" />
  <row Id="282" PostTypeId="1" AcceptedAnswerId="305" CreationDate="2017-05-30T12:16:20.780" Score="7" ViewCount="82" Body="&lt;p&gt;How can I manipulate protein-interaction network graph from the String database using &lt;code&gt;STRINGdb&lt;/code&gt; bioconductor package and R?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have downloaded the whole graph for &lt;em&gt;Homo sapiens&lt;/em&gt; from STRING, which has about 20.000 proteins.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How do I read the file using that package? &lt;/li&gt;&#xA;&lt;li&gt;How do I filter things I don't need? Supposing that I want to keep tumor data, as an example.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="416" LastEditorUserId="191" LastEditDate="2017-05-31T17:40:17.260" LastActivityDate="2017-06-08T08:20:19.730" Title="How to manipulate protein interaction network from String database in R?" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;database&gt;&lt;networks&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="2" />
  <row Id="283" PostTypeId="1" AcceptedAnswerId="284" CreationDate="2017-05-30T12:28:59.600" Score="2" ViewCount="108" Body="&lt;p&gt;I am searching for a multi-omics dataset which may include genomics, transcriptomics and proteomics (eg. &lt;a href=&quot;http://snyderome.stanford.edu/&quot; rel=&quot;nofollow noreferrer&quot;&gt;snyderome&lt;/a&gt;).&#xA;I need such a dataset for an introductory data-exploration purpose. So it can be from any host organism and any experimental methods/conditions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any database/repository where I can find it?&lt;/p&gt;&#xA;" OwnerUserId="274" LastEditorUserId="57" LastEditDate="2017-05-30T19:07:49.063" LastActivityDate="2017-06-03T23:06:42.720" Title="Is there any publicly available multi-omics dataset?" Tags="&lt;multi-omics&gt;&lt;data-request&gt;" AnswerCount="3" CommentCount="5" />
  <row Id="284" PostTypeId="2" ParentId="283" CreationDate="2017-05-30T12:53:39.583" Score="10" Body="&lt;p&gt;There is a database called &lt;a href=&quot;http://www.omicsdi.org/search?q=omics_type:%22Multi-Omics%22&quot; rel=&quot;nofollow noreferrer&quot;&gt;OmicsDI&lt;/a&gt;, where one can search for multi-omics datasets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a &lt;a href=&quot;https://doi.org/10.1101/049205&quot; rel=&quot;nofollow noreferrer&quot;&gt;link&lt;/a&gt; of the associated publication (Perez-Riverol, Yasset, et al. &quot;Omics Discovery Index-Discovering and Linking Public Omics Datasets.&quot; bioRxiv (2016): 049205.) for more details. &lt;/p&gt;&#xA;" OwnerUserId="274" LastEditorUserId="191" LastEditDate="2017-06-02T08:05:19.020" LastActivityDate="2017-06-02T08:05:19.020" CommentCount="0" />
  <row Id="285" PostTypeId="1" AcceptedAnswerId="286" CreationDate="2017-05-30T14:35:07.713" Score="6" ViewCount="42" Body="&lt;p&gt;I'm interested in obtaining coding sequences of my favourite gene in all individuals from the 1000Genomes (and similar projects). I use GATK to get the right subset of variants, vcf-consensus to map these variants onto the reference genome and finally samtools to extract the individual exons. This works fine if the variants are SNPs but if there are any indels, this changes the coordinates of exons and I end up getting the wrong region. Is there any generic way of remapping genomic coordinates to account for the changes created by indels?&lt;/p&gt;&#xA;" OwnerUserId="148" LastActivityDate="2017-05-31T01:03:59.797" Title="Remapping genomic coordinates to account for indels" Tags="&lt;variants&gt;&lt;structural-variation&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="286" PostTypeId="2" ParentId="285" CreationDate="2017-05-30T15:23:33.850" Score="5" Body="&lt;p&gt;I think that you need a LiftOver Chain file to transform your coordinates. You can obtain such a file using &lt;code&gt;bcftools consensus&lt;/code&gt; with the &lt;code&gt;-c&lt;/code&gt; parameter:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;-c, --chain &amp;lt;file&amp;gt;         write a chain file for liftover&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then you can use it to transform coordinates in various genomic formats using &lt;a href=&quot;http://crossmap.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;CrossMap&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-05-31T01:03:59.797" LastActivityDate="2017-05-31T01:03:59.797" CommentCount="4" />
  <row Id="287" PostTypeId="2" ParentId="83" CreationDate="2017-05-30T19:18:22.740" Score="2" Body="&lt;p&gt;It is one of my favorite stories. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Drop a glance to &lt;a href=&quot;http://stereogene.bioinf.fbb.msu.ru/&quot; rel=&quot;nofollow noreferrer&quot;&gt;StereoGene software&lt;/a&gt;, it for genomic track correlation, it described in a &lt;a href=&quot;http://biorxiv.org/content/early/2017/05/25/059584&quot; rel=&quot;nofollow noreferrer&quot;&gt;preprint&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You also can run MACS or another peak caller and estimate the correlation of two interval sets using the &lt;a href=&quot;http://genometricorr.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GenomtriCorr&lt;/a&gt; package.&lt;/p&gt;&#xA;" OwnerUserId="318" LastEditorUserId="57" LastEditDate="2017-06-15T18:16:11.983" LastActivityDate="2017-06-15T18:16:11.983" CommentCount="0" />
  <row Id="288" PostTypeId="1" CreationDate="2017-05-30T21:18:07.883" Score="6" ViewCount="37" Body="&lt;p&gt;I have about 200 short nucleotide motifs (6-12 bp in length) from the human genome, and I'm trying to see how conserved they are across vertebrates. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was thinking that I'd need to make a bed file for each motif that lists all of its occurrences in the human genome. From there, I could map the beds to a bigwig files of &lt;a href=&quot;http://compgen.cshl.edu/phast/phastCons-HOWTO.html&quot; rel=&quot;noreferrer&quot;&gt;PhastCons&lt;/a&gt; scores (essentially doing the reverse of what the PhastCons software was designed to do). Does that sound like the best approach? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm getting stuck at the step of going from motifs to bed files. I've tried using BLAST to find all occurrences of motifs, but their short length is causing issues.&lt;br&gt;&#xA;I've tried messing with the e-value threshold, word size, and filter parameters, but I still don't get any hits. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a work-around for this issue, or should I just rethink my entire approach?&lt;/p&gt;&#xA;" OwnerUserId="250" LastEditorUserId="44" LastEditDate="2017-05-30T23:42:19.883" LastActivityDate="2017-05-31T01:51:14.620" Title="Have DNA motifs 6-12bp long, trying to get conservation scores" Tags="&lt;k-mer&gt;&lt;motifs&gt;&lt;conservation&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="289" PostTypeId="1" CreationDate="2017-05-30T21:36:46.213" Score="7" ViewCount="103" Body="&lt;p&gt;Is there any resource (paper, blogpost, Github gist, etc.) describing the BWA-MEM algorithm for assigning mapping qualities? I vaguely remember that I have somewhere seen a formula for SE reads, which looked like&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$C * (s_1 - s_2) / s_1,$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $s_1$ and $s_2$ denoted the alignment scores of two best alignments and &lt;code&gt;C&lt;/code&gt; was some constant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe that a reimplementation of this algorithm in some scripting language could be very useful for the bioinfo community. For instance, I sometimes test various mapping methods and some of them tend to find good alignments, but fail in assigning appropriate qualities. Therefore, I would like to re-assign all the mapping qualities in a SAM file with the BWA-MEM algorithm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Btw. This algorithm must already have been implemented outside BWA, see the BWA-MEM paper: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;GEM does not compute mapping quality. Its&#xA;  mapping quality is estimated with a BWA-like algorithm with suboptimal&#xA;  alignments available.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, the &lt;a href=&quot;https://github.com/lh3/mem-paper/&quot; rel=&quot;nofollow noreferrer&quot;&gt;BWA-MEM paper repo&lt;/a&gt; contains only the resulting &lt;code&gt;.eval&lt;/code&gt; files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; The question is &lt;em&gt;not&lt;/em&gt; about the algorithm for computing alignment scores. Mapping qualities and alignment scores are two different things:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Alignment score quantifies the similarity between two sequences (e.g., a read and a reference sequence)&lt;/li&gt;&#xA;&lt;li&gt;Mapping quality (MAQ) quantifies the probability that a read is aligned to a wrong position.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Even alignments with high scores can have a very low mapping quality.&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="298" LastEditDate="2017-07-26T16:07:05.550" LastActivityDate="2017-07-26T16:07:05.550" Title="How does the BWA-MEM algorithm assign its mapping qualities?" Tags="&lt;alignment&gt;&lt;bwa&gt;&lt;read-mapping&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="290" PostTypeId="2" ParentId="288" CreationDate="2017-05-30T21:53:30.923" Score="6" Body="&lt;p&gt;&lt;strong&gt;In case you have only ACGT in your motifs&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The short motifs make it sound as if you are in the business of looking for a kmer counter. You can choose to use existing software or build your own.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Using existing software might be your easiest path. An older post from 2014 will probably give you a first idea what's out there: &lt;a href=&quot;http://homolog.us/blogs/blog/2014/04/07/kmer-counting-a-2014-recap/&quot; rel=&quot;noreferrer&quot;&gt;http://homolog.us/blogs/blog/2014/04/07/kmer-counting-a-2014-recap/&lt;/a&gt; . Note that a couple of algorithms mentioned there got successors, so it is worthwhile digging a bit around. The small kmer size will make most of them usable for your needs.&lt;/li&gt;&#xA;&lt;li&gt;As the maximum size of your kmers is comparatively small (12 nt need 24 bits, i.e., max 16.7 million entries in your kmer table), you should be able to easily roll your own kmer counting in about any language you like and on about any of nowadays computer. The pseudocode section on the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-mer&quot; rel=&quot;noreferrer&quot;&gt;Wikipedia&lt;/a&gt; entry for kmers will give you first pointers for that. Might be a bit more work, but maybe more flexible depending on your needs.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In case you have IUPAC bases (N, W, etc.) in your motifs&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know any any pre-existing software doing what you need. I could imagine that the short motifs make using &lt;a href=&quot;https://en.wikipedia.org/wiki/Regular_expression&quot; rel=&quot;noreferrer&quot;&gt;regular expressions&lt;/a&gt; doable for this kind of search, but I may be wrong. Testing this should be easy in a simple script as all major programming languages have modules or libraries for REs. Even if it should take a couple of hours to run on your data set, that would be good enough for a one-off calculation.&lt;/p&gt;&#xA;" OwnerUserId="44" LastActivityDate="2017-05-30T21:53:30.923" CommentCount="0" />
  <row Id="291" PostTypeId="2" ParentId="289" CreationDate="2017-05-30T22:15:53.533" Score="2" Body="&lt;p&gt;Yes, there bwa-mem was published as a &lt;a href=&quot;https://arxiv.org/pdf/1303.3997.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;preprint&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;BWA-MEM’s seed extension differs from the standard seed extension in two aspects. Firstly, suppose at a certain extension step we come to reference&#xA;  position x with the best extension score achieved at query position y.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;...&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Secondly, while extending a seed, BWA-MEM tries to keep track of the&#xA;  best extension score reaching the end of the query sequence&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And there is a description of the scoring algorithm directly in the source code of &lt;a href=&quot;https://github.com/lh3/bwa/blob/master/bwamem.c&quot; rel=&quot;nofollow noreferrer&quot;&gt;bwa-mem&lt;/a&gt; (lines 22 - 44), but maybe the only solution is really to go though the source code.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-02T18:39:54.083" LastActivityDate="2017-06-02T18:39:54.083" CommentCount="5" />
  <row Id="292" PostTypeId="2" ParentId="288" CreationDate="2017-05-30T22:46:52.913" Score="2" Body="&lt;p&gt;To scan motifs in a genome (or database) I would use &lt;a href=&quot;http://meme-suite.org/tools/fimo&quot; rel=&quot;nofollow noreferrer&quot;&gt;FIMO&lt;/a&gt; which will give you the exact locations of these motifs in your genome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have the locations, you can use a &lt;code&gt;phastCons&lt;/code&gt; bigiwig from UCSC to calculate the basewise conservation scores. However, please remember that &lt;code&gt;phastCons&lt;/code&gt; scores are smoothed across windows and it might not be the best metric if you are trying to compare the conservation levels at your motif matching sites as compared to the sequences flanking them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wrote a &lt;a href=&quot;https://github.com/saketkc/moca&quot; rel=&quot;nofollow noreferrer&quot;&gt;package&lt;/a&gt; a while back to do this, including doing de-novo motif discovery. However, it might be an overkill for your use case. &lt;/p&gt;&#xA;" OwnerUserId="161" LastEditorUserId="161" LastEditDate="2017-05-31T01:51:14.620" LastActivityDate="2017-05-31T01:51:14.620" CommentCount="2" />
  <row Id="293" PostTypeId="2" ParentId="19" CreationDate="2017-05-31T02:30:14.807" Score="2" Body="&lt;p&gt;A rolling hash function for DNA sequences called &lt;a href=&quot;http://www.bcgsc.ca/platform/bioinfo/software/nthash&quot; rel=&quot;nofollow noreferrer&quot;&gt;ntHash&lt;/a&gt; has recently been &lt;a href=&quot;https://doi.org/10.1093/bioinformatics/btw397&quot; rel=&quot;nofollow noreferrer&quot;&gt;published in Bioinformatics&lt;/a&gt; and the authors dealt with reverse complements:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Using this table, we can easily compute the hash value for the reverse-complement (as well as the canonical form) of a sequence efficiently, without actually reverse- complementing the input sequence, as follows:&#xA;  ...&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;EDIT (by @user172818): I will add more details about how ntHash works. The notations used in its paper are somewhat uncommon. The source code is more informative.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's first define rotation functions for 64-bit integers:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rol(x,k) := x &amp;lt;&amp;lt; k | x &amp;gt;&amp;gt; (64-k)&#xA;ror(x,k) := x &amp;gt;&amp;gt; k | x &amp;lt;&amp;lt; (64-k)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We then define a hash function &lt;code&gt;h()&lt;/code&gt; for each base. In the implementation, the authors are using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;h(A) = 0x3c8bfbb395c60474&#xA;h(C) = 0x3193c18562a02b4c&#xA;h(G) = 0x20323ed082572324&#xA;h(T) = 0x295549f54be24456&#xA;h(N) = 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The rolling hash function of a forward k-mer &lt;code&gt;s[i,i+k-1]&lt;/code&gt; is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;f(s[i,i+k-1]) := rol(h(s[i]),k-1) ^ rol(h(s[i+1]),k-2) ^ ... ^ h(s[i+k-1])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;^&lt;/code&gt; is the XOR operator. The hash function of its reverse complement is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;r(s[i,i+k-1]) := f(~s[i,i+k-1])&#xA;               = rol(h(~s[i+k-1]),k-1) ^ rol(h(~s[i+k-2]),k-2) ^ ... ^ h(~s[i])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;~&lt;/code&gt; gives the reverse complement of a DNA sequence. Knowing &lt;code&gt;f(s[i,i+k-1])&lt;/code&gt; and &lt;code&gt;r(s[i,i+k-1])&lt;/code&gt;, we can compute their values for the next k-mer:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;f(s[i+1,i+k]) = rol(f(s[i,i+k-1]),1) ^ rol(h(s[i]),k)  ^ h(s[i+k])&#xA;r(s[i+1,i+k]) = ror(r(s[i,i+k-1]),1) ^ ror(h(~s[i]),1) ^ rol(h(~s[i+k]),k-1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This works because &lt;code&gt;rol&lt;/code&gt;, &lt;code&gt;ror&lt;/code&gt; and &lt;code&gt;^&lt;/code&gt; can all be switched in order. Finally, for a k-mer &lt;code&gt;s&lt;/code&gt;, the hash function considering both strands is the smaller between &lt;code&gt;f(s)&lt;/code&gt; and &lt;code&gt;r(s)&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;h(s) = min(f(s),r(s))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is a linear algorithm regardless of the k-mer length. It only uses simple arithmetic operations, so should be fairly fast. I have briefly tested its randomness. It seems comparable to murmur. ntHash is probably the best algorithm so far if you want to hash an arbitrarily long k-mer into 64 bits.&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-05-31T19:53:13.073" LastActivityDate="2017-05-31T19:53:13.073" CommentCount="6" />
  <row Id="294" PostTypeId="1" AcceptedAnswerId="304" CreationDate="2017-05-31T06:22:09.180" Score="10" ViewCount="121" Body="&lt;p&gt;I vaguely remember, that the original plan of Oxford Nanopore was to provide cheap sequencers (MinION), but charge for base-calling. For that reason the base-calling was performed in the cloud, and the plan was to make it commercial once the technology is established.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, this limits the potential uses of MinION in the field, since huge areas do not have decent internet connection. Also, not all the data can be legally transferred to a third-party company in the clinical studies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example for the &lt;a href=&quot;https://dx.doi.org/10.1038/nature16996&quot; rel=&quot;nofollow noreferrer&quot;&gt;Ebola paper&lt;/a&gt;, they had to create a special version of their software:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;An offline-capable version of MinKNOW, with internet ‘ping’ disabled&#xA;  and online updates disabled was made available to us by Oxford&#xA;  Nanopore Technologies specifically for the project&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;There are couple of third-party base-callers available today. I am aware of &lt;a href=&quot;https://nanoporetech.com/publications/nanocall-open-source-basecaller-oxford-nanopore-sequencing-data&quot; rel=&quot;nofollow noreferrer&quot;&gt;Nanocall&lt;/a&gt; and &lt;a href=&quot;https://nanoporetech.com/publications/deepnano-deep-recurrent-neural-networks-base-calling-minion-nanopore-reads&quot; rel=&quot;nofollow noreferrer&quot;&gt;DeepNano&lt;/a&gt;, but since they are not official, it can be hard for them to keep-up with the latest versions of sequencers and cells.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Is it possible as of today to sequence offline without a special arrangement (like the Ebola one).&lt;/li&gt;&#xA;&lt;li&gt;If not, what's the policy of Oxford Nanopore toward third-party base-callers? Are they going to help them, or to sue them eventually?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="191" LastEditorUserId="73" LastEditDate="2017-06-08T20:21:12.793" LastActivityDate="2017-06-08T20:21:12.793" Title="Is it possible to perform MinION sequencing offline?" Tags="&lt;sequencing&gt;&lt;minion&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="295" PostTypeId="2" ParentId="294" CreationDate="2017-05-31T07:09:09.510" Score="5" Body="&lt;p&gt;In short, yes offline basecalling is supported.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Oxford nanopore has made available an official offline basecaller, called albacore. If you have an account to the nanpore community website you can download it from &lt;a href=&quot;https://community.nanoporetech.com/downloads&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Concerning running it in the field, last time I checked live basecalling was still requiring a lot of computing power, so you might want to run the MinION in the field, but basecall once back a your lab / computing facility.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not aware of any plans to sue the third-party basecallers, I think Oxford Nanopore is pretty positive about them :-) &lt;/p&gt;&#xA;" OwnerUserId="179" LastActivityDate="2017-05-31T07:09:09.510" CommentCount="2" />
  <row Id="296" PostTypeId="1" AcceptedAnswerId="298" CreationDate="2017-05-31T09:48:18.563" Score="6" ViewCount="66" Body="&lt;p&gt;This is a frequently-asked question within the nanopore community. Oxford Nanopore currently claims that they are able to generate run yields of 10-15 gigabases (e.g. see &lt;a href=&quot;https://twitter.com/Clive_G_Brown/status/865211116208697344&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/Clive_G_Brown/status/860106072081739778&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;), and yet it's more common to see users only managing in the 1-5 gigabase range.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So why the big difference in yield?&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-08T20:23:34.517" LastActivityDate="2017-06-10T05:01:59.620" Title="How can I improve the yield of MinION sequencing runs?" Tags="&lt;fastq&gt;&lt;minion&gt;&lt;yield&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="297" PostTypeId="1" AcceptedAnswerId="384" CreationDate="2017-05-31T09:49:29.980" Score="7" ViewCount="57" Body="&lt;p&gt;I have often downloaded datasets from the SRA where the authors failed to mention which adapters were trimmed during the processing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Local alignments tend to overcome this obstacle, but it feels a bit barbaric.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;fastQC works occasionally to pick them up, but sometimes fails to find the actual adapter sequences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Usually, I ended up looking up the kits they used and trying to grep for all the possible barcodes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a more robust/efficient way to do this?&lt;/p&gt;&#xA;" OwnerUserId="383" LastEditorUserId="298" LastEditDate="2017-05-31T13:17:49.717" LastActivityDate="2017-06-03T09:09:53.957" Title="How can I systematically detect unknown barcode/adapter sequences within a set of samples?" Tags="&lt;barcode&gt;&lt;adapter&gt;&lt;trim&gt;&lt;trimming&gt;&lt;clipping&gt;" AnswerCount="4" CommentCount="2" />
  <row Id="298" PostTypeId="2" ParentId="296" CreationDate="2017-05-31T10:02:35.193" Score="6" Body="&lt;p&gt;Our lab so far hasn't achieved any run yields over 1 gigabase, but we've had trouble extracting good DNA, and all of our runs were carried out before the individual pore blocking software fixes happened (in March 2017). I am hopeful that our next run will be a good one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I attended a talk by Josh Quick at PoreCampAU 2017, in which he discussed some common barriers to getting both good sequencing yield and long read length. It mostly boils down to being more careful with the sample preparation. Bear in mind that the MinION will still sequence a dirty sample, it will just be at a reduced yield. Here are my notes from that talk:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The hardest thing about MinION sequencing is getting the sample in the first place&lt;/li&gt;&#xA;&lt;li&gt;There are lots of different sample types and extraction methods&lt;/li&gt;&#xA;&lt;li&gt;You can't get longer reads than what you put in; shit in leads to shit out&lt;/li&gt;&#xA;&lt;li&gt;DNA is very stable when not moving, but very sensitive to lateral damage&lt;/li&gt;&#xA;&lt;li&gt;The phenol chloroform method of DNA extraction is very good, and can be used with a phase-locked gel to make extraction easier&lt;/li&gt;&#xA;&lt;li&gt;A simple salt + alcohol extraction might be the best method for extraction (because it involves the least amount of work on the DNA)&lt;/li&gt;&#xA;&lt;li&gt;EDTA (e.g. as found in TE buffer, and many extraction kits) is not compatible with the rapid kit&lt;/li&gt;&#xA;&lt;li&gt;The most consistently good Nanopore runs produced by Josh's lab were 1D ligations runs on R9.4; the best overall run was a phenol-chloroform extraction + rapid kit&lt;/li&gt;&#xA;&lt;li&gt;John Tyson can tune himself out of a low-yield hole (via software)&lt;/li&gt;&#xA;&lt;li&gt;Getting small numbers of short reads is very important&lt;/li&gt;&#xA;&lt;li&gt;Suggested (and mostly untested) purification techniques: spin column (60-100kb); ethanol extraction (100-150kb), dialysis (150-250kb); low melting-point agarose plug (~1Mb, DNA extraction in situ)&lt;/li&gt;&#xA;&lt;li&gt;The nanopore protocol input is in nanograms, but should really be stated as molarity; the kit expects about 0.2 pmol input&lt;/li&gt;&#xA;&lt;li&gt;Picture molecules tethered to the surface of the membrane. You can then see that the flow cell density is independent of the sequence length&lt;/li&gt;&#xA;&lt;li&gt;Tapestation, Qubit and Nanodrop are all a good idea; a DNA sample that can pass all three tests will work well: no short-length shoulders by Tapestation, sufficient DNA by Qubit, high purity by Nanodrop&lt;/li&gt;&#xA;&lt;li&gt;RNA can interfere with sequencing; digesting RNA is recommended&lt;/li&gt;&#xA;&lt;li&gt;Freezing DNA is a really bad idea. The ice crystals are very good at chopping DNA up into small pieces&lt;/li&gt;&#xA;&lt;li&gt;DNA that is kept in the fridge is remarkably stable; can be kept for over two years (and probably indefinitely)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-05-31T10:02:35.193" CommentCount="0" />
  <row Id="299" PostTypeId="1" CreationDate="2017-05-31T10:05:01.120" Score="3" ViewCount="46" Body="&lt;p&gt;Given WGS data or RNA-seq data, which tools can I use to detect gene fusions?&lt;/p&gt;&#xA;" OwnerUserId="283" LastActivityDate="2017-05-31T16:58:30.973" Title="Which tools can detect chimeric RNA (fusion genes) from WGS or RNA-Seq data?" Tags="&lt;rna-seq&gt;&lt;wgs&gt;&lt;chimeric-rna&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="300" PostTypeId="2" ParentId="299" CreationDate="2017-05-31T10:05:01.120" Score="8" Body="&lt;p&gt;Most of these use RNA-seq data, some use WGS data, and some use both. They are listed alphabetically. I will add to the list when I discover more.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Barnacle: &lt;a href=&quot;http://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-14-550&quot; rel=&quot;noreferrer&quot;&gt;http://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-14-550&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Bellerophontes: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/28/16/2114.long&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/28/16/2114.long&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;BreakDancer: &lt;a href=&quot;http://www.nature.com/nmeth/journal/v6/n9/abs/nmeth.1363.html&quot; rel=&quot;noreferrer&quot;&gt;http://www.nature.com/nmeth/journal/v6/n9/abs/nmeth.1363.html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;BreakFusion: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3389765/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3389765/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;BreakPointer: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3561864/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3561864/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;ChimeraScan: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3187648/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3187648/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Comrad: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/27/11/1481.long&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/27/11/1481.long&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;CRAC: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4053775/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4053775/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;deFuse: &lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001138&quot; rel=&quot;noreferrer&quot;&gt;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001138&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Dissect: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/28/12/i179.abstract&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/28/12/i179.abstract&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;EBARDenovo: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/early/2013/03/01/bioinformatics.btt092&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/early/2013/03/01/bioinformatics.btt092&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;EricScript: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/28/24/3232&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/28/24/3232&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;FusionAnalyser: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3439881/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3439881/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;FusionCatcher: &lt;a href=&quot;http://biorxiv.org/content/early/2014/11/19/011650.full-text.pdf+html&quot; rel=&quot;noreferrer&quot;&gt;http://biorxiv.org/content/early/2014/11/19/011650.full-text.pdf+html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;FusionFinder: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3384600/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3384600/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;FusionHunter: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/27/12/1708.long&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/27/12/1708.long&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;FusionMap: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/27/14/1922&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/27/14/1922&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;FusionQ: &lt;a href=&quot;http://www.biomedcentral.com/1471-2105/14/193&quot; rel=&quot;noreferrer&quot;&gt;http://www.biomedcentral.com/1471-2105/14/193&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;FusionSeq: &lt;a href=&quot;http://www.genomebiology.com/2010/11/10/R104&quot; rel=&quot;noreferrer&quot;&gt;http://www.genomebiology.com/2010/11/10/R104&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;IDP-fusion: &lt;a href=&quot;http://nar.oxfordjournals.org/content/early/2015/06/03/nar.gkv562.full&quot; rel=&quot;noreferrer&quot;&gt;http://nar.oxfordjournals.org/content/early/2015/06/03/nar.gkv562.full&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;iFUSE: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/29/13/1700.long&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/29/13/1700.long&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;InFusion: &lt;a href=&quot;https://bitbucket.org/kokonech/infusion/wiki/Home&quot; rel=&quot;noreferrer&quot;&gt;https://bitbucket.org/kokonech/infusion/wiki/Home&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;INTEGRATE: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/26556708&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pubmed/26556708&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;JAFFA: &lt;a href=&quot;http://www.genomemedicine.com/content/7/1/43&quot; rel=&quot;noreferrer&quot;&gt;http://www.genomemedicine.com/content/7/1/43&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;LifeScope: &lt;a href=&quot;http://www.thermofisher.com/no/en/home/life-science/sequencing/next-generation-sequencing/solid-next-generation-sequencing/solid-next-generation-sequencing-data-analysis-solutions/lifescope-data-analysis-solid-next-generation-sequencing/lifescope-genomic-analysis-software-solid-next-generation-sequencing.html&quot; rel=&quot;noreferrer&quot;&gt;http://www.thermofisher.com/no/en/home/life-science/sequencing/next-generation-sequencing/solid-next-generation-sequencing/solid-next-generation-sequencing-data-analysis-solutions/lifescope-data-analysis-solid-next-generation-sequencing/lifescope-genomic-analysis-software-solid-next-generation-sequencing.html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;MapSplice: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/20802226&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pubmed/20802226&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;MOJO &lt;a href=&quot;https://github.com/cband/MOJO&quot; rel=&quot;noreferrer&quot;&gt;https://github.com/cband/MOJO&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;nFuse: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3483554/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3483554/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Pegasus: &lt;a href=&quot;http://www.biomedcentral.com/1752-0509/8/97&quot; rel=&quot;noreferrer&quot;&gt;http://www.biomedcentral.com/1752-0509/8/97&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;PRADA: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/24695405&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pubmed/24695405&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;ShortFuse: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3072550/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3072550/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;SnowShoes-FTD: &lt;a href=&quot;http://nar.oxfordjournals.org/content/39/15/e100&quot; rel=&quot;noreferrer&quot;&gt;http://nar.oxfordjournals.org/content/39/15/e100&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;SOAPFuse: &lt;a href=&quot;http://www.genomebiology.com/2013/14/2/R12&quot; rel=&quot;noreferrer&quot;&gt;http://www.genomebiology.com/2013/14/2/R12&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;SOAPFusion: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/24123671&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pubmed/24123671&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;STAR: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/29/1/15&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/29/1/15&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;STAR-Fusion: &lt;a href=&quot;https://github.com/STAR-Fusion/STAR-Fusion/wiki&quot; rel=&quot;noreferrer&quot;&gt;https://github.com/STAR-Fusion/STAR-Fusion/wiki&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;TopHat-Fusion: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3245612/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3245612/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;TRUP: &lt;a href=&quot;http://www.genomebiology.com/2015/16/1/7&quot; rel=&quot;noreferrer&quot;&gt;http://www.genomebiology.com/2015/16/1/7&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;ViralFusionSeq: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3582262/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3582262/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Other useful programs:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Chimeraviz (visualization tools for gene fusions): &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/28525538&quot; rel=&quot;noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pubmed/28525538&lt;/a&gt; (disclaimer: I created this)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Chimera: &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4253834/&quot; rel=&quot;noreferrer&quot;&gt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4253834/&lt;/a&gt;&lt;br&gt;&#xA;OncoFuse: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/29/20/2539.long&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/29/20/2539.long&lt;/a&gt;&lt;br&gt;&#xA;FuMa: &lt;a href=&quot;http://bioinformatics.oxfordjournals.org/content/early/2015/12/09/bioinformatics.btv721.abstract&quot; rel=&quot;noreferrer&quot;&gt;http://bioinformatics.oxfordjournals.org/content/early/2015/12/09/bioinformatics.btv721.abstract&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="283" LastActivityDate="2017-05-31T10:05:01.120" CommentCount="0" />
  <row Id="301" PostTypeId="2" ParentId="297" CreationDate="2017-05-31T10:28:26.410" Score="1" Body="&lt;p&gt;If you happen to know a sequence that should be highly abundant in the library, you can grep its beginning or end (with pattern match highlighting) and see if the same sequence systematically comes just before or just after respectively. This kind of visual inspection can help you finding the adaptor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, in a previous lab, we were working on &lt;em&gt;D. melanogaster&lt;/em&gt; small RNA sequencing data and my colleague knew from previous experience with this kind of data that the following small RNA was likely to be abundant: &lt;a href=&quot;http://flybase.org/reports/FBgn0065042.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://flybase.org/reports/FBgn0065042.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We just had to grep it in the fastq file to see many lines with this sequence, next to another sequence that happened to be always the same: the unknown adapter.&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-06-03T09:09:53.957" LastActivityDate="2017-06-03T09:09:53.957" CommentCount="6" />
  <row Id="302" PostTypeId="2" ParentId="297" CreationDate="2017-05-31T10:45:23.277" Score="2" Body="&lt;p&gt;I'm not aware of any existing methods to do this, but here are a couple of ideas about how it might be done:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Canu has a method of adapter trimming which involves looking for the absence of overlap for reads. If there are no other reads which share sequence across a particular region, then the read is broken up at the point of low coverage, and small pieces are discarded. It would be possible to use a method like this to hunt for possible adapter/barcode sequences by preserving the short reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another option is to do a kmer search at the start of reads, and see if any of the high-abundance kmers can be assembled together and/or matched to existing known adapters or barcodes.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-05-31T10:45:23.277" CommentCount="0" />
  <row Id="303" PostTypeId="1" CreationDate="2017-05-31T11:01:10.113" Score="5" ViewCount="51" Body="&lt;p&gt;Microbial genomes can contain extensive duplications. Often we'd like to transfer annotations from an annotated species to one that is newly sequenced. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Existing tools (e.g. RATT, LiftOver, Kraken) either make specific assumptions about how closely related the species are or fail to transfer when multiple matches are found in the new genome, especially if the sequences are highly similar. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically, I have a synthetic biology application where genes can duplicate extensively. They are identical in sequence but duplicated many times and be relocated (i.e., not just adjacent to each other). None of the above mentioned tools are able to transfer coordinates of annotations to genomes with multiple copies of features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any pre-existing tools or software that transfer annotations in this scenario? Ideas for ways to do this robustly?&lt;/p&gt;&#xA;" OwnerUserId="442" LastEditorUserId="77" LastEditDate="2017-05-31T14:31:42.897" LastActivityDate="2017-05-31T17:43:32.240" Title="How to transfer gff annotations in genome with extensive duplications?" Tags="&lt;annotation&gt;&lt;sequencing&gt;" AnswerCount="2" CommentCount="10" />
  <row Id="304" PostTypeId="2" ParentId="294" CreationDate="2017-05-31T11:09:16.597" Score="8" Body="&lt;p&gt;Short answer: yes, but you need to get permission (and modified software) from ONT before doing that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;... but that doesn't tell the whole story. This question has the potential to be very confusing, and that's through no fault of the questioner. The issue is that for the MinION, sequencing (or more specifically, generating the raw data in the form of an electrical signal trace) is distinct and separable from base calling. Many other sequencers also have distinct raw data and base-calling phases, but they're not democratised to the degree they are on the MinION.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &quot;sequencing&quot; part of MinION sequencing is carried out by ONT software, namely MinKNOW. As explained to me during PoreCampAU 2017, when the MinION is initially plugged into a computer it is missing the firmware necessary to carry out the sequencing. The most recent version of this firmware is usually downloaded at the start of a sequencing run by sending a request to ONT servers. In the usual case, you can't do sequencing without being able to access those servers, and you can't do sequencing without ONT knowing about it. However, ONT acknowledge that there are people out there who won't have Internet access when sequencing (e.g. sequencing Ebola in Africa, or metagenomic sequencing in the middle of the ocean), and an email to &lt;code&gt;&amp;lt;support@nanoporetech.com&amp;gt;&lt;/code&gt; with reasons is likely to result in a quick software fix to the local sequencing problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once the raw signals are acquired, the &quot;base-calling&quot; part of MinION sequencing can be done anywhere. The ONT-maintained basecaller is Albacore, and this will get the first model updates whenever the sequencing technology is changed (which happens a lot). Albacore is a local basecaller which can be obtained from ONT by browsing through their community pages (available to anyone who has a MinION); ONT switched to only allowing people to do basecalling locally in about April 2017, after establishing that using AWS servers was just too expensive. Albacore is open source and free-as-in-beer, but has a restrictive licensing agreement which limits the distribution (and modification) of the program. However, Albacore is not the only available basecaller. ONT provide a FOSS basecaller called &lt;a href=&quot;https://github.com/nanoporetech/nanonet&quot; rel=&quot;nofollow noreferrer&quot;&gt;nanonet&lt;/a&gt;. It's a little bit behind Albacore on technology, but ONT have said that all useful Albacore changes will eventually propagate through to nanonet. There is another non-ONT basecaller that I'm aware of which uses a neural network for basecalling: &lt;a href=&quot;https://bitbucket.org/vboza/deepnano&quot; rel=&quot;nofollow noreferrer&quot;&gt;deepnano&lt;/a&gt;. Other basecallers exist, each varying distances away technology-wise, and I expect that more will appear in the future as the technology stabilises and more change-resistant computer scientists get in on the act.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: ONT has just pulled back the curtain on their basecalling software; all the repositories that I've looked at so far (except for the Cliveome) have been released under the Mozilla Public License (free and open source, with some conditions and limitations). Included in &lt;a href=&quot;https://github.com/nanoporetech/&quot; rel=&quot;nofollow noreferrer&quot;&gt;that software repository&lt;/a&gt; is Scrappie, which is their testing / bleeding-edge version of Albacore.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-07T22:47:06.100" LastActivityDate="2017-06-07T22:47:06.100" CommentCount="0" />
  <row Id="305" PostTypeId="2" ParentId="282" CreationDate="2017-05-31T11:12:06.970" Score="2" Body="&lt;p&gt;I think the easiest way is to download the graph using &lt;code&gt;STRINGdb&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(STRINGdb)&#xA;string_db &amp;lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xA;                          score_threshold=400, input_directory=&quot;&quot; )&#xA;full.graph &amp;lt;- string_db$get_graph()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now you can use &lt;a href=&quot;http://igraph.org/r/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;igraph&lt;/code&gt;&lt;/a&gt;, to manipulate the graph. Let's assume you want to take 200 proteins with the highest degree, i.e. number of edges they have.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(igraph)&#xA;&#xA;# see how many proteins do you have    &#xA;vcount(full.graph)&#xA;&#xA;# find top 200 proteins with the highest degree&#xA;top.degree.verticies &amp;lt;- names(tail(sort(degree(full.graph)), 200))&#xA;&#xA;# extract the relevant subgraph&#xA;top.subgraph &amp;lt;- induced_subgraph(full.graph, top.degree.verticies)&#xA;&#xA;# count the number of proteins in it&#xA;vcount(top.subgraph)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How to get disease specific genes?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's no GO annotation for cancer or Alzheimer's disease. It is out of scope of the GO consortium.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What you can do, you can either take KEGG Pathways annotation, or manually select list of relevant GO-terms. Or acquire the list from one of the papers. For example annotation term &lt;code&gt;05200&lt;/code&gt; corresponds to the cancer KEGG pathway. You can easily retrieve proteins associated with the annotation:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cancer.pathway.proteins &amp;lt;-&#xA;    string_db$get_term_proteins('05200')$STRING_id&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And then perform subgraphing as described above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively you can try to get an enrichment score for an every gene given it's neighbors (the way enrichment is shown on the string-db website). Then you can keep only those having top enrichment scores. Probably &lt;code&gt;get_ppi_enrichment_full&lt;/code&gt; or &lt;code&gt;get_ppi_enrichment&lt;/code&gt; functions will help you to do that.&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-06-08T08:20:19.730" LastActivityDate="2017-06-08T08:20:19.730" CommentCount="6" />
  <row Id="306" PostTypeId="1" CreationDate="2017-05-31T12:17:46.297" Score="2" ViewCount="48" Body="&lt;p&gt;I recently used the minION (Nanopore, 9.4 flow cell, RAD001 kit) to generate a metagenome out of environmental samples.&#xA;Passed reads weren't brilliant (196, average 1,594bp lenght), but working with &lt;a href=&quot;https://github.com/infphilo/centrifuge&quot; rel=&quot;nofollow noreferrer&quot;&gt;centrifuge&lt;/a&gt; the classification outputs turned out to have quite low hitLength to queryLength scores (average 2%, max 14%). This plus score values don't give me a lot of confidence towards the results I got.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anyone else used &lt;em&gt;centrifuge&lt;/em&gt; and experienced the same?&lt;/p&gt;&#xA;" OwnerUserId="445" LastEditorUserId="77" LastEditDate="2017-05-31T12:22:34.797" LastActivityDate="2017-05-31T15:45:09.357" Title="Rapid metagenomics classifiers on long read data" Tags="&lt;nanopore&gt;&lt;long-reads&gt;&lt;minion&gt;&lt;metagenome&gt;&lt;centrifuge&gt;" AnswerCount="1" CommentCount="2" ClosedDate="2017-06-01T15:05:02.230" />
  <row Id="307" PostTypeId="1" AcceptedAnswerId="310" CreationDate="2017-05-31T13:40:38.773" Score="6" ViewCount="53" Body="&lt;p&gt;I have the following FASTA file, &lt;code&gt;original.fasta&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;foo&#xA;GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I need to change the record id from &lt;code&gt;foo&lt;/code&gt; to &lt;code&gt;bar&lt;/code&gt;, so I wrote the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from Bio import SeqIO&#xA;&#xA;original_file = r&quot;path\to\original.fasta&quot;&#xA;corrected_file = r&quot;path\to\corrected.fasta&quot;&#xA;&#xA;with open(original_file) as original, open(corrected_file, 'w') as corrected:&#xA;    records = SeqIO.parse(original_file, 'fasta')&#xA;    for record in records:&#xA;        print record.id             # prints 'foo'&#xA;        if record.id == 'foo':&#xA;            record.id = 'bar'&#xA;        print record.id             # prints 'bar' as expected&#xA;        SeqIO.write(record, corrected, 'fasta')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We printed the record id before and after the change, and get the expected result. We can even doublecheck by reading in the corrected file again with BioPython and printing out the record id:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;with open(corrected_file) as corrected:&#xA;    for record in SeqIO.parse(corrected, 'fasta'):&#xA;        print record.id                  # prints 'bar', as expected&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, if we open the corrected file in a text editor, we see that the record id is not &lt;code&gt;bar&lt;/code&gt; but  &lt;code&gt;bar foo&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;bar foo&#xA;GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We can confirm that this is what is written to the file if we read the file using plain Python:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;with open(corrected_file) as corrected:&#xA;    print corrected.readlines()[0][1:] # prints 'bar foo'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is this a bug in BioPython? And if not, what did I do wrong and how do I change the record id in a FASTA file using BioPython?&lt;/p&gt;&#xA;" OwnerUserId="450" LastActivityDate="2017-05-31T19:39:46.323" Title="Changing the record id in a FASTA file using BioPython" Tags="&lt;fasta&gt;&lt;biopython&gt;" AnswerCount="1" CommentCount="8" />
  <row Id="308" PostTypeId="1" AcceptedAnswerId="389" CreationDate="2017-05-31T13:47:17.630" Score="3" ViewCount="103" Body="&lt;p&gt;&lt;a href=&quot;http://www.sanger.ac.uk/science/tools/smalt-0&quot; rel=&quot;nofollow noreferrer&quot;&gt;SMALT&lt;/a&gt; seems to be one of the most used read mappers for bacterial data, see, e.g., &lt;a href=&quot;https://scholar.google.com/scholar?q=%22smalt%22+bacteria&amp;amp;as_ylo=2016&quot; rel=&quot;nofollow noreferrer&quot;&gt;this query&lt;/a&gt;. I do not say that it is not a great mapper, but I cannot easily see what are its main strengths compared to mappers such as BWA-MEM, Bowtie2, NovoAlign or GEM. Moreover, it is not even published. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could you name some of its distinguishing features (e.g., user support by Sanger Pathogens)? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I have heard only arguments like &quot;We use SMALT because everyone does it.&quot;, but this is not convincing enough for me.&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="298" LastEditDate="2017-06-07T12:39:23.120" LastActivityDate="2017-06-07T12:39:23.120" Title="Why is SMALT better for microbial genomics than other mappers?" Tags="&lt;read-mapping&gt;&lt;smalt&gt;" AnswerCount="1" CommentCount="11" />
  <row Id="309" PostTypeId="2" ParentId="303" CreationDate="2017-05-31T14:04:22.683" Score="0" Body="&lt;p&gt;I think you will have to first identify the regions homologous to the ones defined in your GFF and then transfer the annotations. Of course, the assumption there is that the homolog will also have the same annotation which is often not true. However, I don't see how you can do it in any other way since you cannot use genomic coordinates (and you would still be making the same assumption even if you could, anyway) when the genomes are so different. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a very simplistic approach (which might be enough if, as you say, your sequences are almost identical), you can do something like:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Collect the sequences of interest from your already annotated species. &lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Use a tool like &lt;a href=&quot;https://www.ebi.ac.uk/Tools/psa/genewise/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;genewise&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;https://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;exonerate&lt;/code&gt;&lt;/a&gt; to map these into the target genome. Both tools can return gff-formatted output and both can find multiple hits in the target genome. For what you want, I would suggest using a very high threshold of sequence similarity and query coverage (where the target sequence found covers all or most of the query sequence used). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since these are microbial genomes and therefore splicing isn't a problem, you could do the same thing with even a simple BLASTn or tBLASTn if you start from protein sequences. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;At this point, you should have a list of homologs (some of which will be &lt;a href=&quot;https://biology.stackexchange.com/a/4964/1306&quot;&gt;orthologs and others paralogs&lt;/a&gt;) and you can transfer the annotations of the query sequence over to the target. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Again, I stress that this is making a whopping huge assumption: that homologous sequences have the same function and can automatically be annotated as whatever you had in the query genome. This is going to be true for many cases but it will also be false for others. Especially if you are looking at paralogs (genes whose duplication occurred after the speciation event and are therefore likely to have diverged in function). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, as I said before, this problem would be exactly the same even if you did manage to transfer annotations just by identifying the syntenic regions of the genomes&lt;sup&gt;1&lt;/sup&gt;, so there's not much difference there. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;sub&gt;As I said in the comments, I don't see how this could be possible. By definition, if you have extensive duplications, the genomic coordinates will be completely different and it is impossible to map from one genome into the other. &lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-05-31T14:04:22.683" CommentCount="0" />
  <row Id="310" PostTypeId="2" ParentId="307" CreationDate="2017-05-31T14:36:06.500" Score="7" Body="&lt;p&gt;If I use &lt;code&gt;SeqIO.parse(filehandle, 'fasta')&lt;/code&gt; to parse a FASTA file, then it will return a &lt;code&gt;SeqRecord&lt;/code&gt; object where the &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;name&lt;/code&gt; are the first word (everything before the first whitespace) of the line beginning with &lt;code&gt;&amp;gt;&lt;/code&gt; and the &lt;code&gt;description&lt;/code&gt; is the complete line (all not including the initial &lt;code&gt;&amp;gt;&lt;/code&gt;). (This behaviour can overruled by providing &lt;a href=&quot;https://github.com/biopython/biopython/blob/dbdf7c155af3e7f203ea966acedc12740271ed73/Bio/SeqIO/FastaIO.py#L102&quot; rel=&quot;nofollow noreferrer&quot;&gt;a custom &lt;code&gt;title2ids&lt;/code&gt; function&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, if I for example have a &lt;code&gt;original.fasta&lt;/code&gt; file like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;Lorem|ipsum&amp;gt;dolor sit amet&#xA;GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I will get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from Bio import SeqIO&#xA;&amp;gt;&amp;gt;&amp;gt; path = r'C:\path\to\original.fasta'&#xA;&amp;gt;&amp;gt;&amp;gt; records = SeqIO.parse(open(path), 'fasta')&#xA;&amp;gt;&amp;gt;&amp;gt; record = next(records)&#xA;&amp;gt;&amp;gt;&amp;gt; record.id&#xA;'Lorem|ipsum&amp;gt;dolor'&#xA;&amp;gt;&amp;gt;&amp;gt; record.name&#xA;'Lorem|ipsum&amp;gt;dolor'&#xA;&amp;gt;&amp;gt;&amp;gt; record.description&#xA;'Lorem|ipsum&amp;gt;dolor sit amet'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, what happens in my case when you do &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    if record.id == 'foo':&#xA;        record.id = 'bar'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;is that the &lt;code&gt;record.id&lt;/code&gt; is successfully changed from &lt;code&gt;foo&lt;/code&gt; to &lt;code&gt;bar&lt;/code&gt;, but that the &lt;code&gt;record.description&lt;/code&gt; is not changed and stays &lt;code&gt;foo&lt;/code&gt;. That's why, when the FASTA file is printed out, I see the described behaviour of &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;bar foo&#xA;GCTCACACATAGTTGATGCAGATGTTGAATTCACTATGAGGTGGGAGGATGTAGGGCCA&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, the solution to my problem is to both change the &lt;code&gt;id&lt;/code&gt; AND the &lt;code&gt;description&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from Bio import SeqIO&#xA;&#xA;original_file = r&quot;path\to\original.fasta&quot;&#xA;corrected_file = r&quot;path\to\corrected.fasta&quot;&#xA;&#xA;with open(original_file) as original, open(corrected_file, 'w') as corrected:&#xA;    records = SeqIO.parse(original_file, 'fasta')&#xA;    for record in records:&#xA;        if record.id == 'foo':&#xA;            record.id = 'bar'&#xA;            record.description = 'bar'&#xA;        SeqIO.write(record, corrected, 'fasta')&#xA;&#xA;with open(corrected_file) as corrected:&#xA;    records = SeqIO.parse(corrected, 'fasta')&#xA;    for record in records:&#xA;        print record.id             # prints bar&#xA;        print record.name           # prints bar&#xA;        print record.description    # prints bar&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="450" LastEditorUserId="280" LastEditDate="2017-05-31T19:39:46.323" LastActivityDate="2017-05-31T19:39:46.323" CommentCount="0" />
  <row Id="311" PostTypeId="2" ParentId="303" CreationDate="2017-05-31T15:34:47.833" Score="3" Body="&lt;p&gt;There is one very simplistic way I use which &lt;em&gt;might&lt;/em&gt; work for what you are doing, it is similar to what terdon proposed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Take a de-novo microbial genome annotation tool (I have my own, but you could use/modify &lt;a href=&quot;http://www.vicbioinformatics.com/software.prokka.shtml&quot; rel=&quot;nofollow noreferrer&quot;&gt;prokka&lt;/a&gt;). Tools like these often first predict gene boundaries (with other tools like &lt;a href=&quot;http://prodigal.ornl.gov/&quot; rel=&quot;nofollow noreferrer&quot;&gt;prodigal&lt;/a&gt; or &lt;a href=&quot;https://ccb.jhu.edu/software/glimmer/&quot; rel=&quot;nofollow noreferrer&quot;&gt;glimmer&lt;/a&gt;) and then try to assign a function to found genes. This function assignment is often done with BLAST and other tools ... and that is where you can go in and modify to do what you need.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use a &quot;knowledge&quot; protein database of genes I want to have very strictly annotated as a first line of annotation (e.g. in your case: the annotated genomes). For that I loop through very strict identity/similarity parameters which get gradually relaxed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g.:&#xA;Loop 0: only transfer annotations at 100% DNA identity, same length.&#xA;Loop 1: only transfer annotations at 100% similarity, same length.&#xA;Loop 2: only transfer annotations at 99% similarity, length +/- 1%.&#xA;...&#xA;Loop n: only transfer annotations at 100-(n-1)% similarity, length +/- (n-1)%.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In each loop, obviously only annotate what has not been annotated in previous loops.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After that, use &quot;normal&quot; annotation pipeline of the tool to annotate the rest.&lt;/p&gt;&#xA;" OwnerUserId="44" LastEditorUserId="44" LastEditDate="2017-05-31T17:43:32.240" LastActivityDate="2017-05-31T17:43:32.240" CommentCount="5" />
  <row Id="312" PostTypeId="2" ParentId="306" CreationDate="2017-05-31T15:45:09.357" Score="2" Body="&lt;p&gt;You should also keep in mind that single read accuracy of ONT still is a bit lacking, a 2D or 1D^2 accuracy of 95% still means that on average there's one error every 20 bp, and due to the nature of the data some stretches may be more junky than others. Maybe centrifuge doesn't like that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Getting a feel of what the data can tell you (and what not) is pretty important.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You may want to try other classification tools to get an idea how well centrifuge works, Kraken comes to mind. But also: what about BLASTing some of these reads at NCBI and simply look at the results to get a first impression?&lt;/p&gt;&#xA;" OwnerUserId="44" LastActivityDate="2017-05-31T15:45:09.357" CommentCount="0" />
  <row Id="313" PostTypeId="2" ParentId="299" CreationDate="2017-05-31T16:58:30.973" Score="0" Body="&lt;p&gt;1- Chimerascan + Star&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2- Tophat Fusion&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Two pipelines I have used extensively and recommend. The others listed by @L42 I can't speak to but Star Fusion sounds promising. Star is fast. &lt;/p&gt;&#xA;" OwnerUserId="460" LastActivityDate="2017-05-31T16:58:30.973" CommentCount="0" />
  <row Id="314" PostTypeId="1" CreationDate="2017-05-31T18:06:26.410" Score="4" ViewCount="36" Body="&lt;p&gt;I have:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A list of differentially phosphorylated sites in a knockout condition. Some genes contain as many as 70 possible phosphorylation sites; others contain only one.&lt;/li&gt;&#xA;&lt;li&gt;A list of genes belonging to a specific gene set annotation.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I test the differentially phosphorylated proteins for enrichment of this annotation?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A few ideas I’ve considered:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Ignore the number of phosphorylation events detected within a gene and simply count the gene as differentially phosphorylated if it contains at least a single site that is differentially phosphorylated. Compare the enrichment score for this selected set to the enrichment score for the set genes containing at least one site that is not differentially phosphorylated. The problem here is that genes with a large number of phosphorylation sites have almost no influence in the enrichment score, since it’s almost certain that they’ll have at least one site that is differentially phosphorylated and at least one that is not.&lt;/li&gt;&#xA;&lt;li&gt;Mark each candidate phosphorylation site as either “in the set” or “not in the set” based on the protein in which it’s found. Then perform the enrichment analysis using the set of annotated phosphorylation sites instead of using the traditional enrichment analysis performed at the gene level. The potential problem with this approach is that it may place too much influence on genes with many potential phosphorylation sites.&lt;/li&gt;&#xA;&lt;li&gt;Aggregate all candidate phosphorylation sites within a gene and use some numerical threshold to determine whether the gene is differentially phosphorylated or not. (There are various ways that this could be done.) Then perform enrichment analysis using the resulting set of differentially phosphorylated genes. A possible problem here is that some of the phosphorylation sites may be more functionally important than others, so it’s not clear how to weight the relative importance of individual phosphorylation sites within a gene.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I realize the goal here isn’t well-defined mathematically; I’m mainly curious what approach makes the most sense given the biological context. Of the above approaches, I’m currently leaning towards approach 2 because it’s straightforward to implement and it at least &lt;em&gt;attempts&lt;/em&gt; to account for the variable number of phosphorylation events within a gene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;NOTE: I also have normalized phosphoprotein and protein abundances for all of these sites, obtained from mass spec. So if the solution requires an alternative method of computing differential phosphorylation, that’s fine.&lt;/p&gt;&#xA;" OwnerUserId="177" LastActivityDate="2017-05-31T19:05:58.757" Title="Gene set enrichment analysis on differential phosphorylation sites" Tags="&lt;gse&gt;&lt;differential-expression&gt;&lt;phosphoproteomics&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="315" PostTypeId="2" ParentId="314" CreationDate="2017-05-31T18:35:14.477" Score="1" Body="&lt;p&gt;Your dataset looks perfect for the &lt;a href=&quot;https://dx.doi.org/10.1186/1753-6561-3-S7-S96&quot; rel=&quot;nofollow noreferrer&quot;&gt;SUMSTAT&lt;/a&gt; enrichment test.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;You need to come up with a statistic representing your gene. The simplest ideas here are the number of sites or, probably better, a proportion of phosphorylated sites.&lt;/li&gt;&#xA;&lt;li&gt;Now you can compute a statistic for every gene set, for example just sum (SUMSTAT). You can also have average, sum of squares or something else.&lt;/li&gt;&#xA;&lt;li&gt;Geta a null distribution of your gene set statistics by permutations. You can either randomize statistics for genes, or just randomly assign phosphorylated sites across the genome keeping the total number of sites.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Now you can compute p-value just by comparing your value with the null-distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You have to be cautious of two things:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;You are testing multiple gene sets, i.e. you perform a statistical test for every gene set. Therefore correct your significance for multiple testing. I suggest using FDR for this.&lt;/li&gt;&#xA;&lt;li&gt;Beware of potential biases when doing permutations. The obvious one is gene length, i.e. longer genes have higher chance of getting at least one site. But there can be other ones like GC-content, chromosomes, etc. You can overcome this by using more realistic permutations, or by controlling for the potential correlations. You can get a few ideas on fighting biases from &lt;a href=&quot;https://doi.org/10.1093/molbev/msx083&quot; rel=&quot;nofollow noreferrer&quot;&gt;this paper&lt;/a&gt; (sorry for the self-advertisement).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-05-31T19:05:58.757" LastActivityDate="2017-05-31T19:05:58.757" CommentCount="0" />
  <row Id="316" PostTypeId="1" AcceptedAnswerId="320" CreationDate="2017-05-31T20:27:51.153" Score="0" ViewCount="25" Body="&lt;p&gt;We have arrayCGH (aCGH) results for one sample. There is a 0.5 Mb terminal duplication on chromosome 19 (62995490-63407936, according to NCBI36/hg18). The duplication is rare: a literature review suggests there are only 3-4 samples with clinical information.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the steps to validate the results? How do we ascertain that this duplication is the cause of the clinical symptoms?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have some ideas:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;aCGH the parents. Not sure how this would help.   &lt;/li&gt;&#xA;&lt;li&gt;whole genome exome sequencing. Worried this might make it more difficult to pinpoint genetic cause.&lt;/li&gt;&#xA;&lt;li&gt;whole genome sequencing?&lt;/li&gt;&#xA;&lt;li&gt;other ideas?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I am new to aCGH and high-throughput sequencing, any advice is welcome.&lt;/p&gt;&#xA;" OwnerUserId="131" LastEditorUserId="73" LastEditDate="2017-06-02T03:49:26.877" LastActivityDate="2017-06-02T03:49:26.877" Title="How do I validate a single sample ArrayCGH result?" Tags="&lt;sequencing&gt;&lt;array-cgh&gt;&lt;exome&gt;" AnswerCount="1" CommentCount="7" />
  <row Id="317" PostTypeId="1" CreationDate="2017-05-31T20:39:57.870" Score="4" ViewCount="50" Body="&lt;p&gt;There are many posts on the web regarding QC steps pre and post-imputation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does applying below (new?) 10% &lt;a href=&quot;https://en.wikipedia.org/wiki/Minor_allele_frequency&quot; rel=&quot;nofollow noreferrer&quot;&gt;MAF&lt;/a&gt; difference rule make sense, pitfalls?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the process:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Get MAF for imputed set, using &lt;a href=&quot;https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;SNPTEST&lt;/a&gt; with flag &lt;code&gt;-summary_stats_only&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Convert imputed set to hard-calls using &lt;a href=&quot;http://www.well.ox.ac.uk/~cfreeman/software/gwas/gtool.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;gtools&lt;/a&gt; with flag &lt;code&gt;--threshold 0.9&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;If the MAF from step 1 and step 2 differs more than 10% than exclude the variant.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;More info:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;GWAS is 50K vs 50K case control samples.&lt;/li&gt;&#xA;&lt;li&gt;This step is applied after &lt;code&gt;info &amp;gt; 0.4&lt;/code&gt; filter.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="131" LastEditorUserId="131" LastEditDate="2017-06-05T06:30:45.717" LastActivityDate="2017-06-05T06:30:45.717" Title="Filtering imputed GWAS SNPs based on a MAF difference of 10%" Tags="&lt;gwas&gt;&lt;imputation&gt;&lt;gtools&gt;&lt;maf&gt;&lt;qc&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="318" PostTypeId="1" AcceptedAnswerId="319" CreationDate="2017-05-31T20:47:18.167" Score="6" ViewCount="83" Body="&lt;p&gt;Is working with and relying on old genome builds still valid?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example &lt;a href=&quot;https://genome-euro.ucsc.edu/cgi-bin/hgGateway?db=hg18&amp;amp;redirect=manual&amp;amp;source=genome.ucsc.edu&quot; rel=&quot;noreferrer&quot;&gt;NCBI36/hg18&lt;/a&gt;. Would results from papers based on old builds require &lt;a href=&quot;https://genome.ucsc.edu/cgi-bin/hgLiftOver&quot; rel=&quot;noreferrer&quot;&gt;LiftOver&lt;/a&gt; and re-analysis to be useful?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;^&lt;/strong&gt; This post might be too broad, off topic, but would like to know your views.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A bit of context, this is related to other post, where we have aCGH results based on old build: &lt;a href=&quot;https://bioinformatics.stackexchange.com/q/316/131&quot;&gt;ArrayCGH single sample result validation&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="131" LastEditorUserId="96" LastEditDate="2017-06-08T05:55:17.747" LastActivityDate="2017-06-08T06:38:40.213" Title="Working with old genome builds" Tags="&lt;human-genome&gt;&lt;array-cgh&gt;&lt;liftover&gt;&lt;hg18&gt;" AnswerCount="4" CommentCount="1" />
  <row Id="319" PostTypeId="2" ParentId="318" CreationDate="2017-05-31T21:03:50.680" Score="8" Body="&lt;p&gt;In my opinion, it is not very reliable. LiftOver is very limited in terms of transformations it can support. The &lt;a href=&quot;https://genome.ucsc.edu/goldenpath/help/chain.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;LiftOver Chain format&lt;/a&gt; can capture only matching regions in the same order. It means that it can account for indels, but even simple structural variations become problematic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, when a newer assembly is available, it is usually a recommended practice to remap all the reads rather than transform the existing alignments.&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-06-07T21:09:58.510" LastActivityDate="2017-06-07T21:09:58.510" CommentCount="0" />
  <row Id="320" PostTypeId="2" ParentId="316" CreationDate="2017-05-31T21:28:07.403" Score="2" Body="&lt;p&gt;Why not just good old qPCR? That's (A) quick, (B) cheap and (C) easy to analyze. If you care about the exact location of the break point (I'm guessing from the context that you don't), then targeted sequencing with a custom capture kit would work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding validating the biological relevance of this, there are multiple (parallel) routes one can take. Firstly, screen unaffected family members for this same alteration. If you find this in unaffected individuals then it's obviously not the causative alteration. Ideally, one would also do either a cell-line experiment or a mouse (or other model organism) experiment to see if you can reconstitute at least some component of the clinical phenotype. This may not always be possible, of course.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-05-31T21:28:07.403" CommentCount="0" />
  <row Id="321" PostTypeId="1" CreationDate="2017-05-31T22:32:59.663" Score="6" ViewCount="91" Body="&lt;p&gt;The most variant calling pipeline GATK include a &lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/article?id=44&quot; rel=&quot;nofollow noreferrer&quot;&gt;Base Quality Score Recalibration (BQSR)&lt;/a&gt; which requires a list of known variants. Recently, some work has been done for reference-free recalibration of scores as well: &lt;a href=&quot;http://biorxiv.org/content/early/2017/04/27/130732&quot; rel=&quot;nofollow noreferrer&quot;&gt;Lancer&lt;/a&gt; and &lt;a href=&quot;http://www.genetics.org/content/early/2016/11/07/genetics.116.189985&quot; rel=&quot;nofollow noreferrer&quot;&gt;atlas&lt;/a&gt;, which is motivated by making the most for aDNA and low coverage datasets. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The importance for aDNA is explained in &lt;a href=&quot;https://www.youtube.com/watch?v=oJ9pbQsyaUg&amp;amp;list=PLoCxWrRWjqB1JUCntl4X09ezmOtKx1Gke&amp;amp;index=4&quot; rel=&quot;nofollow noreferrer&quot;&gt;this lecture&lt;/a&gt;, but it is not clear to me if / how is important BQSR is for fresh DNA samples with decent (&gt;15x) coverage. Especially when I work with non-model organisms and I can not simply use the standard tools.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Q: How big an impact does recalibration of scores have on variant calling? Is there a rule of thumb for which it is / it is not worth the effort?&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-01T14:17:04.697" LastActivityDate="2017-06-06T11:07:55.663" Title="Is there a point in recalibration of scores for variant calling?" Tags="&lt;ngs&gt;&lt;variant-calling&gt;" AnswerCount="3" CommentCount="2" FavoriteCount="1" />
  <row Id="322" PostTypeId="1" AcceptedAnswerId="323" CreationDate="2017-05-31T22:56:26.803" Score="5" ViewCount="63" Body="&lt;p&gt;How do I download a reference genome that I can use with bowtie2? Specifically HG19. On UCSC there are a lot of file options.&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="37" LastEditDate="2017-06-25T02:11:38.870" LastActivityDate="2017-06-25T02:11:38.870" Title="Downloading a reference Genome for Bowtie2" Tags="&lt;human-genome&gt;&lt;reference-genome&gt;&lt;genome-sequencing&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="323" PostTypeId="2" ParentId="322" CreationDate="2017-05-31T23:21:53.943" Score="5" Body="&lt;p&gt;tl;dr: Just use the either the downloads on the &lt;a href=&quot;http://bowtie-bio.sourceforge.net/bowtie2/index.shtml&quot; rel=&quot;noreferrer&quot;&gt;Bowtie2 homepage&lt;/a&gt; or the &lt;a href=&quot;https://support.illumina.com/sequencing/sequencing_software/igenome.html&quot; rel=&quot;noreferrer&quot;&gt;Illumina iGenomes&lt;/a&gt;. Or just uncompress and concatenate the &lt;a href=&quot;http://hgdownload.soe.ucsc.edu/goldenPath/hg19/chromosomes/&quot; rel=&quot;noreferrer&quot;&gt;FASTA files found on UCSC goldenpath&lt;/a&gt; and then build the index.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A bit longer answer:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are two components to &quot;genome for a read mapper&quot; such as Bowtie or BWA.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, you need to choose the actual sequence (genome release such as GRCh37/hg19 or GRCh38/hg38). There are patch releases such as GRCh37.p3 where some bases might be exchanged and depending on the release, some &quot;unmapped&quot; loci contigs might be added, but generally GRCh37.p1 is roughly the same as GRCh37.p2, for example. Usually, people have agreed on some specific patch version for each read and use this for read mapping.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Generally, there is the UCSC flavour hg19/hg38 etc. and the NCBI/GRC flavour GRCh37, GRCh38 etc. (similar with mouse). UCSC has no versioning besides the genome release and (to the best of my knowledge) does not update the genome sequence after releasing a hg19 FASTA file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Second, you have to build the index files for each genome. Depending on the read mapper you use, you might or might not need the original FASTA files for the alignment. For Bowtie and Bowtie 2, you don't need the original FASTA files after building the index as Bowtie 1/2 can reconstruct the sequence &quot;on the fly&quot; from the index files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;HTH&lt;/p&gt;&#xA;" OwnerUserId="247" LastActivityDate="2017-05-31T23:21:53.943" CommentCount="1" />
  <row Id="324" PostTypeId="2" ParentId="321" CreationDate="2017-05-31T23:27:28.080" Score="2" Body="&lt;p&gt;That's a good question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd say that you don't need to bother with variant recalibration for&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;low number of samples (e.g., just two trios); I could not get GTAK recalibration of variant scores to work anyway&lt;/li&gt;&#xA;&lt;li&gt;high-coverage samples (e.g., X Ten genomes with 30x coverage) where the DNA samples themselves are of high, comparable quality and have been sequenced with consistent technology.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Generally, it is my impression that a lot of the thoughts and advanced statistical models built into GATK come from the earlier phases of the 1000 Genomes project. This means (1) low-coverage, (2) different coverage genomes (3) sequenced with varying technology versions by (4) different samples and (5) population sequencing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are in a clinical setting where you do 30x sequencing on X Ten platforms only anyway, then variant recalibration will probably not help you that much.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand, if you are integrating many data sets from different data centers and machine versions etc., variant recalibration might be worth a shot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A good check would be looking at genotype quality distributions and other variant/quality related metrics before and after recalibration.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyone: please correct me if I'm wrong!&lt;/p&gt;&#xA;" OwnerUserId="247" LastActivityDate="2017-05-31T23:27:28.080" CommentCount="3" />
  <row Id="325" PostTypeId="2" ParentId="318" CreationDate="2017-05-31T23:34:31.387" Score="4" Body="&lt;p&gt;I think that right now, the only human builds that are worth considering are hg19/GRCh37 as many data bases such as gnomAD still exclusively use this release. On the other hand, hg38/GRCh8 has many important fixes and the useful (but yet underused) feature of alternative loci.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anything from older releases should be remapped to a more recent one.&lt;/p&gt;&#xA;" OwnerUserId="247" LastActivityDate="2017-05-31T23:34:31.387" CommentCount="0" />
  <row Id="326" PostTypeId="1" CreationDate="2017-05-31T23:36:32.283" Score="2" ViewCount="52" Body="&lt;p&gt;What is the &quot;best&quot; assembly for the popular model organisms:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;human (GRCh37 and GRCh38 are obvious, I'd pick whatever bwakit uses)&lt;/li&gt;&#xA;&lt;li&gt;mouse (GRCm37/GRCm38, OK)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;but what about non-human/mouse ones?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;fruit fly&lt;/li&gt;&#xA;&lt;li&gt;zebrafish&lt;/li&gt;&#xA;&lt;li&gt;E. coli&lt;/li&gt;&#xA;&lt;li&gt;any other idea?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="247" LastActivityDate="2017-06-01T16:50:34.150" Title="Which reference to use for read mapping for popular model organisms" Tags="&lt;read-mapping&gt;&lt;reference-genome&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="327" PostTypeId="1" CreationDate="2017-05-31T23:38:44.793" Score="2" ViewCount="215" Body="&lt;p&gt;What are good means for performing quality control (QC) or NGS reads?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm aware of:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;FastQC&lt;/li&gt;&#xA;&lt;li&gt;NGS Screen&lt;/li&gt;&#xA;&lt;li&gt;Kraken (e.g., for screening against contaminants)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What are other popular means for such QC?&lt;/p&gt;&#xA;" OwnerUserId="247" LastEditorUserId="57" LastEditDate="2017-06-04T20:02:08.037" LastActivityDate="2017-06-20T16:44:39.103" Title="QC measures for NGS sequencing" Tags="&lt;ngs&gt;&lt;quality-control&gt;" AnswerCount="5" CommentCount="3" FavoriteCount="1" />
  <row Id="328" PostTypeId="1" CreationDate="2017-05-31T23:40:03.393" Score="4" ViewCount="89" Body="&lt;p&gt;One big problem that I'm regularly facing is that URLs for downloading Bioinformatics data (e.g., RefSeq releases or NCBI genome releases) disappear.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have any good solution for this?&lt;/p&gt;&#xA;" OwnerUserId="247" LastActivityDate="2017-06-03T13:36:28.887" Title="Stable download URLs" Tags="&lt;data-download&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="329" PostTypeId="1" CreationDate="2017-05-31T23:46:41.107" Score="3" ViewCount="40" Body="&lt;p&gt;PacBio is selling ~10x PacBio SEQUEL long reads as an upgrade to Illumina data for SV discovery.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a clinical setting, the main requirements are proper sensitivity and specificity but also the processing of cohorts, at least families. This requires a genotyping step, such that it can be identified whether a given variant is shared by two or more individuals or whether it is not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the tools of the trade for this task?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As having 50-60x PacBio reads is not an option from a economic point of view, one has to make do with 10x coverage.&lt;/p&gt;&#xA;" OwnerUserId="247" LastEditorUserId="57" LastEditDate="2017-06-01T15:11:31.337" LastActivityDate="2017-06-01T15:11:31.337" Title="Structural variant calling for low-coverage PacBio data" Tags="&lt;structural-variation&gt;&lt;long-reads&gt;&lt;pacbio&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="330" PostTypeId="2" ParentId="329" CreationDate="2017-05-31T23:48:37.513" Score="0" Body="&lt;p&gt;I'm aware of the following (very few and suboptimal) options:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/fritzsedlazeck/Sniffles/releases&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sniffles&lt;/a&gt; -- sadly not very reliable in my experience, also no genotyping step or multi-sample support&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-180&quot; rel=&quot;nofollow noreferrer&quot;&gt;PB Honey&lt;/a&gt; -- no genotyping step or multi-sample support&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="247" LastActivityDate="2017-05-31T23:48:37.513" CommentCount="0" />
  <row Id="332" PostTypeId="2" ParentId="326" CreationDate="2017-06-01T02:02:50.910" Score="2" Body="&lt;p&gt;If the programs you are using allow for it, take the most recent available genome, as it will be most likely to have the fewest errors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many of the well-known model organisms have an official release site (e.g. &lt;a href=&quot;http://flybase.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;flybase&lt;/a&gt; for drosophila, &lt;a href=&quot;http://www.wormbase.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;wormbase&lt;/a&gt; for nematodes) from which genomes are fairly promptly propagated through to larger public databases, of which the most well-known are probably &lt;a href=&quot;http://www.ensembl.org/info/about/species.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Ensembl&lt;/a&gt; and &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/genome/browse/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RefSeq Genome&lt;/a&gt;. There's usually enough cross-talk between the large data repositories that it doesn't matter all that much where the source is, as long as it is at least a few months since the last genome release.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-01T02:02:50.910" CommentCount="0" />
  <row Id="333" PostTypeId="1" AcceptedAnswerId="339" CreationDate="2017-06-01T03:27:11.300" Score="2" ViewCount="44" Body="&lt;p&gt;I would like to show that certain types of RBP motifs are enriched in RNA editing islands (i.e. clusters of RNA editing). However, I am unsure about how to think about sequence motifs with respect to their occurrence in other genomic features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand how to find the probability of a motif at a location. i.e.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P(sequence is at position i) = P(A)^[A] * P(C)^[C] * P(G)^[G] * P(T)^[T]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where (.) is the base pair and [.] is the number of bps. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Instead what I would like to find is: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;P(sequence S is contained in a feature type T) = ???&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where feature type T is an gene, intron, editing island, etc. I think I should incorporate length since I will mainly be comparing genes or introns vs. editing islands. Also, I am not sure what to do about the editing islands being located within genes. How can I keep from counting the same motif twice?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas would be greatly appreciated. Thank you for your time. &lt;/p&gt;&#xA;" OwnerUserId="269" LastActivityDate="2017-06-01T08:22:35.933" Title="How to calculate statistical significance of sequence motifs" Tags="&lt;statistics&gt;&lt;motifs&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="334" PostTypeId="2" ParentId="326" CreationDate="2017-06-01T04:40:50.010" Score="4" Body="&lt;p&gt;I will answer one of the points – E.coli.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Bacteria, and in particular E.coli, are highly variable and there is usually no single best assembly. Large scale WGS studies should come with multiple assemblies for individual monophyletic clusters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Long answer:&lt;/strong&gt; Whereas a single reference sequence can make sense for human or mice (to a certain extent), bacteria are not the case. The variability between them even within a single species can be enormous and various lineages can contain very different genes. This is the reason why biologists usually distinguish a &lt;em&gt;core genome&lt;/em&gt; consisting of the genes shared among a vast majority (typically 95%) of individuals within a given phylogenetic clade, (i.e., of those genes which are essential for this clade – imagine a CPU and RAM in a computer), and an &lt;em&gt;accessory genome&lt;/em&gt; consisting of the other, unnecessary, genes – imagine a joystick in our analogy). The size of the accessory genome can vary a lot for different bacteria. For instance, Escherichia coli is said to have an &lt;em&gt;open&lt;/em&gt; accessory genome (i.e., much bigger than the core genome; imagine a Raspberry-PI and all the extension kits) whereas Chlamydia trachomatis has a &lt;em&gt;closed&lt;/em&gt; accessory genome (i.e., small compared to the core genome; imagine an ATM).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For species having a small accessory genome, a single reference might be sufficient. However, if a core genome is large, we can use either pan-genomes, or multiple references. The word pan-genome tends to be used for different things: the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/27769991&quot; rel=&quot;nofollow noreferrer&quot;&gt;computational biologists&lt;/a&gt; usually use it as a representation of all genomic content in a certain phylogenetic clade (which can be mathematically modeled using finite automata, hence as graphs) whereas some biologists define it as a concatenation of all (core and accessory) genes from the clade (without considering SNPs, etc.). Unfortunately, methods for graph references (especially for read mapping) are still not sufficiently developed, therefore, researchers still have to resort to reference sequence-based methods and use either concatenated genes or multiple reference sequences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When particular bacterial species are studied using massive whole-genome sequencing of many isolates, researchers usually cluster the isolates to monophyletic sequence clusters and infer a special reference sequence for each of these clusters (see, e.g., this &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3725542/&quot; rel=&quot;nofollow noreferrer&quot;&gt;example&lt;/a&gt; for the pneumococcus). All of the inferred references should be then published together with the paper or in a separate data paper (see, e.g., the &lt;a href=&quot;http://datadryad.org/resource/doi:10.5061/dryad.t55gq/8&quot; rel=&quot;nofollow noreferrer&quot;&gt;sequences from the previous example&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; I am not a biologist. If anything in this post is too simplified or incorrect, please, leave me a comment and I will try to fix it. I tried to make this text accessible for my younger self.&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-06-01T15:41:32.643" LastActivityDate="2017-06-01T15:41:32.643" CommentCount="1" />
  <row Id="335" PostTypeId="2" ParentId="230" CreationDate="2017-06-01T07:05:29.963" Score="0" Body="&lt;p&gt;One big advantage of Reactome, in my opinion, is its visualization using the web interface. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many pathways (in Reactome and KEGG) consist of genes / proteins that are up- and down-regulated through the respective pathway. If you do a simple overrepresentation analysis this is not taken into consideration. Therefore, you might end up seeing a pathway as &quot;overexpressed&quot; although only the down-regulated genes were observed more frequently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In Reactome, you can zoom in on the different pathways very conveniently and then pick up these inconsistencies. I haven't really found a public database and tool that can take these different regulations into consideration. Therefore, you'll probably always will need some manual investigation of your data. In my opinion, this is easier with Reactome than with KEGG.&lt;/p&gt;&#xA;" OwnerUserId="476" LastActivityDate="2017-06-01T07:05:29.963" CommentCount="0" />
  <row Id="336" PostTypeId="1" AcceptedAnswerId="356" CreationDate="2017-06-01T07:27:25.760" Score="4" ViewCount="49" Body="&lt;p&gt;What is the current standard for imputing missing genotypes between two genotyping panels?  I have two populations genotyped using two different panels (A &amp;amp; B), and I would like to impute all the genotypes in population B for those positions on used in panel A.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've read the examples for impute2, and I think the closest thing to what I am looking for is &lt;a href=&quot;https://mathgen.stats.ox.ac.uk/impute/impute_v2.html#ex5&quot; rel=&quot;nofollow noreferrer&quot;&gt;this example&lt;/a&gt;, &quot;Imputation with one unphased reference panel&quot;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simply put, I want to provide a list of SNPs, some variant file for population B, and haplotype information from 1,000 Genomes and get imputed genotypes for each SNP in the list.  Is impute2 the state of the art for this?&lt;/p&gt;&#xA;" OwnerUserId="59" LastEditorUserId="73" LastEditDate="2017-06-01T19:55:30.320" LastActivityDate="2017-06-01T21:36:32.873" Title="Imputing missing genotypes from separate genotyping panels" Tags="&lt;imputation&gt;&lt;impute2&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="0" />
  <row Id="338" PostTypeId="1" CreationDate="2017-06-01T07:33:33.863" Score="1" ViewCount="89" Body="&lt;p&gt;I find that in many contexts, the terms computational biology, bioinformatics and biostatistics are often treated as functionally equivalent, and yet for students selecting PhD programs and the like the difference could be quite significant. Is there a standard or rigorous definition of these terms and the difference between them?&lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="93" LastEditDate="2017-06-03T17:33:30.207" LastActivityDate="2017-06-03T17:33:30.207" Title="Difference between computational biology, bioinformatics and biostatistics" Tags="&lt;statistics&gt;&lt;terminology&gt;&lt;computation&gt;" AnswerCount="0" CommentCount="6" ClosedDate="2017-06-01T08:33:53.843" />
  <row Id="339" PostTypeId="2" ParentId="333" CreationDate="2017-06-01T08:22:35.933" Score="3" Body="&lt;p&gt;As far as I remember the exact probability computation is an open problem. The reason is that potential motifs can overlap, which makes probability computations for an arbitrary string non-trivial, and depends on the motif.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example if he have a binary string of four digits, the probability of &quot;01&quot; will be 11/16, while the probability of &quot;11&quot; will be 8/16=1/2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The simplest approximation is to assume that probability of the motif on every position of the sequence is equal and independent. In &lt;a href=&quot;https://stepik.org/lesson/Some-Hidden-Messages-are-More-Surprising-than-Others-3/step/5?course=Bioinformatics-Algorithms&amp;amp;unit=7&quot; rel=&quot;nofollow noreferrer&quot;&gt;such case&lt;/a&gt; (this comes from &lt;a href=&quot;https://stepik.org/course/Bioinformatics-Algorithms-2&quot; rel=&quot;nofollow noreferrer&quot;&gt;this course&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WZH86.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WZH86.png&quot; alt=&quot;Pr(N,A,Pattern,t)&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Probability of text of length N having at least t occurrences of a k-mer Pattern, A is the number of letters in the alphabet, n = N – t * k)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This approximation is very rough, there are better &lt;a href=&quot;https://doi.org/10.1016/0097-3165(81)90005-4&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="191" LastActivityDate="2017-06-01T08:22:35.933" CommentCount="0" />
  <row Id="340" PostTypeId="2" ParentId="278" CreationDate="2017-06-01T09:01:04.053" Score="4" Body="&lt;p&gt;There are couple of computational methods which try to do this (I never used them, so no experience):&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://doi.org/10.1093/bioinformatics/btt351&quot; rel=&quot;nofollow noreferrer&quot;&gt;CellMix&lt;/a&gt;, based on sets of marker gene lists&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://dx.doi.org/10.1186/1471-2105-12-258&quot; rel=&quot;nofollow noreferrer&quot;&gt;Subset Prediction from Enrichment Correlation&lt;/a&gt;, which is based on correlations with subset-specific genes across a set of samples.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://dx.doi.org/10.1186/1471-2164-13-460&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cell type enrichment&lt;/a&gt;, which uses our highly expressed, cell specific gene database&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://dx.doi.org/10.1038/nmeth.1439&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cell type-specific significance analysis&lt;/a&gt; using differential gene expression for each cell type&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;You might have to get some reference expression levels from public databases or papers for some of the methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One thing to keep in mind: you cannot really compute the cells proportion, only RNA proportion. If you have a good reason to assume that RNA quantity &#xA;per cell is very similar, this is a good proxy for the cells proportion in a tissue.&lt;/p&gt;&#xA;" OwnerUserId="191" LastActivityDate="2017-06-01T09:01:04.053" CommentCount="1" />
  <row Id="341" PostTypeId="2" ParentId="322" CreationDate="2017-06-01T09:38:53.087" Score="6" Body="&lt;p&gt;It’s a matter of preference I guess but I recommend the &lt;a href=&quot;http://grch37.ensembl.org/index.html&quot; rel=&quot;noreferrer&quot;&gt;&lt;strong&gt;Ensembl&lt;/strong&gt; builds&lt;/a&gt;. Decide whether you want the toplevel or primary assembly, and whether you want soft-masked, repeat-masked or unmasked files. The naming schema is very straightforward; the combinations are described in the &lt;a href=&quot;ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/dna/README&quot; rel=&quot;noreferrer&quot;&gt;&lt;code&gt;README&lt;/code&gt; file&lt;/a&gt;, and all files &lt;a href=&quot;ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/dna/&quot; rel=&quot;noreferrer&quot;&gt;reside in one directory&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if you want the unmasked primary assembly, the file to download would be &lt;a href=&quot;ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa.gz&quot; rel=&quot;noreferrer&quot;&gt;&lt;code&gt;Homo_sapiens.GRCh37.75.dna.primary_assembly.fa.gz&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for &lt;strong&gt;GoldenPath/UCSC&lt;/strong&gt;, there’s no need to download and concatenate separate chromosomes (contrary to what the other answer said); you can download the whole (toplevel) reference &lt;a href=&quot;http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/&quot; rel=&quot;noreferrer&quot;&gt;from the &lt;code&gt;bigZips&lt;/code&gt; directory&lt;/a&gt;; from the &lt;code&gt;README&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;This directory contains the Feb. 2009 assembly of the human genome (hg19,&#xA;  GRCh37 Genome Reference Consortium Human Reference 37 (GCA_000001405.1)),&#xA;  as well as repeat annotations and GenBank sequences.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;There are essentially three options here:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;chromFa.tar.gz&lt;/code&gt;, which contains the whole genome in one chromosome per file;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;chromFaMasked.tar.gz&lt;/code&gt;, the same with repeats masked by &lt;code&gt;N&lt;/code&gt;;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;hg19.2bit&lt;/code&gt;, which is the whole genome in one file, but needs to be extracted using the utility program &lt;code&gt;twoBitToFa&lt;/code&gt;, which needs to be downloaded &lt;a href=&quot;http://hgdownload.cse.ucsc.edu/admin/exe&quot; rel=&quot;noreferrer&quot;&gt;separately&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In any case, I always download the reference and build my own index for mapping, since this allows me more control; not everybody might need this much control, but then building the index once is fairly fast anyway.&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-06-01T09:49:08.560" LastActivityDate="2017-06-01T09:49:08.560" CommentCount="1" />
  <row Id="342" PostTypeId="2" ParentId="327" CreationDate="2017-06-01T09:56:56.927" Score="12" Body="&lt;p&gt;&lt;a href=&quot;http://multiqc.info/&quot; rel=&quot;noreferrer&quot;&gt;MultiQC&lt;/a&gt; can merge all your different reports into a single one.&#xA;Which could be useful once you manage to know which QC tools to use.&lt;/p&gt;&#xA;" OwnerUserId="485" LastActivityDate="2017-06-01T09:56:56.927" CommentCount="1" />
  <row Id="343" PostTypeId="1" CreationDate="2017-06-01T11:11:09.893" Score="7" ViewCount="82" Body="&lt;p&gt;I’m using the &lt;a href=&quot;http://www.girinst.org/server/RepBase/index.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;RepBase libraries&lt;/a&gt; in conjunction with RepeatMasker to get genome-wide repeat element annotations, in particular for transposable elements.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This works well enough, and seems to be the de facto standard in the field.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, there are two issues with the use of RepBase, which is why I (and others) have been (so far without success) for alternatives:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.girinst.org/accountservices/register.php?commercial=0&quot; rel=&quot;nofollow noreferrer&quot;&gt;RepBase isn’t open data&lt;/a&gt;. Their academic license agreement includes a clause that &lt;em&gt;explicitly forbids dissemination of data derived from RepBase&lt;/em&gt;. It’s unclear to what extent this is binding/enforceable, but it effectively prevents publishing at least some of the data I’m using and generating. This is unacceptable for &lt;a href=&quot;https://en.wikipedia.org/wiki/Open_science&quot; rel=&quot;nofollow noreferrer&quot;&gt;open science&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Subordinate to this, the subscription model of RepBase also makes it impossible to integrate RepBase into fully automated pipelines, because user interaction is required to subscribe to RepBase, and to provide the login credentials.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;RepBase is heavily manually curated. This is both good and bad. Good, because manual curation of sequence data is often the most reliable form of curation. On the flip side, manual curation is inherently biased; and worse, it’s hard to quantify this bias — &lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-474&quot; rel=&quot;nofollow noreferrer&quot;&gt;this is acknowledged by the RepBase maintainers&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-06-02T11:08:54.493" LastActivityDate="2017-06-27T08:06:22.357" Title="Are there any RepBase alternatives for genome-wide repeat element annotations?" Tags="&lt;annotation&gt;&lt;database&gt;&lt;repeat-elements&gt;&lt;transposable-elements&gt;" AnswerCount="3" CommentCount="10" FavoriteCount="1" />
  <row Id="344" PostTypeId="1" AcceptedAnswerId="345" CreationDate="2017-06-01T11:15:55.563" Score="6" ViewCount="57" Body="&lt;p&gt;What are the key differences between VCF versions 4.1 and 4.2?&#xA; It looks like v4.3 contains a changelog (specs available &lt;a href=&quot;http://samtools.github.io/hts-specs/&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;) but earlier specifications do not. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.biostars.org/p/211152/&quot; rel=&quot;noreferrer&quot;&gt;This&lt;/a&gt; biostar post points out one difference: the introduction of &lt;code&gt;Number=R&lt;/code&gt; for fields with one value per allele including REF — can anyone enumerate the other changes between these two versions?&lt;/p&gt;&#xA;" OwnerUserId="61" LastEditorUserId="96" LastEditDate="2017-06-08T08:47:34.387" LastActivityDate="2017-06-08T08:47:34.387" Title="What's the difference between VCF spec versions 4.1 and 4.2?" Tags="&lt;vcf&gt;&lt;htslib&gt;&lt;file-formats&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="345" PostTypeId="2" ParentId="344" CreationDate="2017-06-01T11:40:33.550" Score="8" Body="&lt;p&gt;This is easy to check, you can download both specs in .tex format and do &lt;code&gt;diff&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Changes to the v4.2 compared to v4.1:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Information field format: adding source and version as recommended fields.&lt;/li&gt;&#xA;&lt;li&gt;INFO field can have one value for each possible allele (code &lt;code&gt;R&lt;/code&gt;).&lt;/li&gt;&#xA;&lt;li&gt;For all of the ##INFO, ##FORMAT, ##FILTER, and ##ALT metainformation, extra fields can be included after the default fields.&lt;/li&gt;&#xA;&lt;li&gt;Alternate base (ALT) can include &lt;code&gt;*&lt;/code&gt;: missing due to a upstream deletion.&lt;/li&gt;&#xA;&lt;li&gt;Quality scores, a sentence removed: &lt;em&gt;High QUAL scores indicate high confidence calls. Although traditionally people use integer phred scores, this field is permitted to be a floating point to enable higher resolution for low confidence calls if desired.&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Examples changed a bit.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="191" LastActivityDate="2017-06-01T11:40:33.550" CommentCount="0" />
  <row Id="346" PostTypeId="2" ParentId="266" CreationDate="2017-06-01T11:52:11.200" Score="0" Body="&lt;p&gt;mauve documentation have a similar command but the usage is &lt;code&gt;--id-matrix=&amp;lt;file&amp;gt;&lt;/code&gt;. So very similar to the progressivemauve command (just doesn't include --input). Try it and see if it works. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think it's a bug for sure as several other people on biostar have complained. &lt;/p&gt;&#xA;" OwnerUserId="490" LastActivityDate="2017-06-01T11:52:11.200" CommentCount="0" />
  <row Id="347" PostTypeId="2" ParentId="343" CreationDate="2017-06-01T12:09:58.040" Score="2" Body="&lt;p&gt;You could use &lt;a href=&quot;http://bix.ucsd.edu/repeatscout/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RepeatScout&lt;/a&gt;, which has defined repeat libraries for a limited number of species (including human, mouse, and rat). If your taxon is not represented, you can also do de novo repeat prediction with RepeatScout to build your own library to feed to RepeatMasker. The &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.517.8206&amp;amp;rep=rep1&amp;amp;type=pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;RepeatScout publication&lt;/a&gt; includes some comparisons with RepBase. Another related tool is &lt;a href=&quot;http://www.repeatmasker.org/RepeatModeler/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RepeatModeler&lt;/a&gt;, which wraps RepeatScout with &lt;a href=&quot;http://eddylab.org/software/recon/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RECON&lt;/a&gt; and some other programs, and shares authors with the RepeatMasker team.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the plus side RepeatScout/RepeatModeler are &lt;a href=&quot;https://github.com/mmcco/RepeatScout&quot; rel=&quot;nofollow noreferrer&quot;&gt;open source&lt;/a&gt; and do not use manual curation, meeting your criteria. On the negative, I'm not sure exactly how RepeatModeler and the component tools are maintained. The RepeatScout web and github pages have not been updated for several years, although the RepeatModeler page shows its latest release was in 2017. Anyway, I know that some combination of RepeatScout/RepeatModeler have been used to annotate repeats for some &lt;em&gt;fairly recent&lt;/em&gt; newly sequenced genomes, e.g. for &lt;a href=&quot;https://www.nature.com/nature/journal/v513/n7518/full/nature13726.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;cichlids&lt;/a&gt;, &lt;a href=&quot;https://www.nature.com/nature/journal/v496/n7445/full/nature12027.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;coelacanth&lt;/a&gt;, and &lt;a href=&quot;https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-14-95&quot; rel=&quot;nofollow noreferrer&quot;&gt;Darwin's finch&lt;/a&gt;, so I think it's fair to say this kind of approach is accepted in the field, at least for vertebrate genome projects.&lt;/p&gt;&#xA;" OwnerUserId="104" LastActivityDate="2017-06-01T12:09:58.040" CommentCount="0" />
  <row Id="348" PostTypeId="2" ParentId="343" CreationDate="2017-06-01T12:15:47.873" Score="5" Body="&lt;p&gt;Dfam has recently launched a sister resource, &lt;a href=&quot;http://www.dfam-consensus.org/&quot; rel=&quot;noreferrer&quot;&gt;&lt;strong&gt;Dfam_consensus&lt;/strong&gt;&lt;/a&gt;, whose &lt;a href=&quot;https://xfam.wordpress.com/2017/05/18/introducing-dfam_consensus-dfams-consensus-sequence-twin/&quot; rel=&quot;noreferrer&quot;&gt;stated aim&lt;/a&gt; is to replace RepBase. From the annoucement:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Dfam_consensus provides an open framework for the community to store both seed alignments (multiple alignments of instances for a given family) and the corresponding consensus sequence model.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Both RepeatMasker and RepeatModeler have been updated to support Dfam_consensus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven’t tried it yet but it looks promising.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-06-01T12:15:47.873" CommentCount="0" />
  <row Id="349" PostTypeId="2" ParentId="278" CreationDate="2017-06-01T12:48:19.277" Score="3" Body="&lt;p&gt;Cell deconvolution is mentioned in &lt;a href=&quot;https://www.biostars.org/p/160961/&quot; rel=&quot;nofollow noreferrer&quot;&gt;this Biostars post&lt;/a&gt;, which mentions CIBERSORT for immune cell mixes, and the Bioconductor package &lt;a href=&quot;http://www.bioconductor.org/packages/release/bioc/html/DeconRNASeq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DeconRNASeq&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I'm aware, it is only possible at best to get proportional representation for transcript expression from standard high-throughput sequencing results, because the sequencers and sample preparation workflow are designed in such a way that the same number of reads are output regardless of the input amount.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-01T12:48:19.277" CommentCount="1" />
  <row Id="350" PostTypeId="1" AcceptedAnswerId="353" CreationDate="2017-06-01T13:04:41.200" Score="7" ViewCount="70" Body="&lt;p&gt;Models of structures deposited in the Protein Data Bank vary in the quality, depending both on the data quality and expertise and patience of the person who built the model. Is there a well-accepted subset of the PDB entries that has only &quot;high quality&quot; structures? Ideally these structures would be representative for classes of proteins in the whole PDB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;based on a &lt;a href=&quot;https://biology.stackexchange.com/questions/56946/subset-of-protein-crystal-structures-from-pdb/&quot;&gt;real question&lt;/a&gt; from biology.SE&lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="266" LastActivityDate="2017-06-20T08:27:16.083" Title="How to select high quality structures from the Protein Data Bank?" Tags="&lt;protein-structure&gt;&lt;pdb&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="351" PostTypeId="1" AcceptedAnswerId="420" CreationDate="2017-06-01T13:08:19.863" Score="6" ViewCount="78" Body="&lt;p&gt;Can anyone recommend a good tool for estimating the tumor content given a matched tumor and normal file for DNA NGS whole genome sequencing data or whole exome data?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to estimate this without a normal sample as well?&lt;/p&gt;&#xA;" OwnerUserId="374" LastEditorUserId="374" LastEditDate="2017-06-02T08:06:06.787" LastActivityDate="2017-06-04T13:37:19.093" Title="Tumor purity/contamination/admixture estimation" Tags="&lt;cancer&gt;&lt;wgs&gt;&lt;wes&gt;&lt;contamination&gt;&lt;admixture&gt;" AnswerCount="3" CommentCount="4" />
  <row Id="352" PostTypeId="2" ParentId="351" CreationDate="2017-06-01T13:38:07.977" Score="2" Body="&lt;p&gt;It's usually CNV callers that make use of Tumour/Normal WGS pairs to estimate purity. It can also be done with WES (exome) Tumour/Normal pairs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are several tools out there, I have some experience with the one written by Illumina (public on Github):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/Illumina/canvas&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/Illumina/canvas&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It requires realigning things with bowtie2, so I don't think it can take existing data aligned with other aligners. Not sure why.&lt;/p&gt;&#xA;" OwnerUserId="180" LastActivityDate="2017-06-01T13:38:07.977" CommentCount="3" />
  <row Id="353" PostTypeId="2" ParentId="350" CreationDate="2017-06-01T13:55:32.567" Score="7" Body="&lt;p&gt;There is a very nice database, &lt;a href=&quot;http://dunbrack.fccc.edu/Guoli/pisces_download.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;pdbcull&lt;/a&gt; (also known as the PISCES server in the literature). It filters the PDB for high resolution and reduced sequence identity. It also seems to be updated regularly. Depending on the cut-offs, you get between 3000 and 35000 structures.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are specifically interested in rotamers, you may want to look at &lt;a href=&quot;http://kinemage.biochem.duke.edu/databases/top8000.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;top8000&lt;/a&gt; instead, where they have checked for high resolution, and good MolProbity scores. They also provide a rotamer database.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PDB also provides &lt;a href=&quot;http://www.rcsb.org/pdb/statistics/clusterStatistics.do&quot; rel=&quot;nofollow noreferrer&quot;&gt;their own clustering.&lt;/a&gt; They first cluster the sequences, and then extract a representative structure for each one, based on the quality factor (&lt;code&gt;1/resolution - R_value&lt;/code&gt;). This has the advantage of being comprehensive, but you will have bad structures when no good ones were ever obtained.&lt;/p&gt;&#xA;" OwnerUserId="501" LastEditorUserId="501" LastEditDate="2017-06-20T08:27:16.083" LastActivityDate="2017-06-20T08:27:16.083" CommentCount="0" />
  <row Id="355" PostTypeId="2" ParentId="329" CreationDate="2017-06-01T15:11:07.053" Score="1" Body="&lt;p&gt;There is an evaluation of PB Honey and Sniffles algorithms for low coverage PacBio datasets in &lt;a href=&quot;http://www.biorxiv.org/content/early/2016/12/17/092544.full.pdf+html&quot; rel=&quot;nofollow noreferrer&quot;&gt;this preprint&lt;/a&gt; and another evaluation is shown on &lt;a href=&quot;http://www.pacb.com/wp-content/uploads/2016-ashg-poster-andrew-carroll-dnanexus.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;this poster&lt;/a&gt;. Both reports agree that optimal is (surprisingly) combination of &lt;a href=&quot;https://sourceforge.net/projects/pb-jelly/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PB Honey&lt;/a&gt; and &lt;a href=&quot;https://github.com/fritzsedlazeck/Sniffles&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sniffles&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Author of Sniffles have benchmarked Sniffles against PB Honey, where he shown that Sniffles performs significantly better. Take a look on &lt;a href=&quot;http://schatzlab.cshl.edu/presentations/2016/2016.10.28.BIODATA.PacBioSV.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;this presentation (slide 15)&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another option is &lt;a href=&quot;https://github.com/EichlerLab/pacbio_variant_caller&quot; rel=&quot;nofollow noreferrer&quot;&gt;SMRT-SV&lt;/a&gt;, but I am not aware of any benchmarking.&lt;/p&gt;&#xA;" OwnerUserId="57" LastActivityDate="2017-06-01T15:11:07.053" CommentCount="4" />
  <row Id="356" PostTypeId="2" ParentId="336" CreationDate="2017-06-01T15:22:21.113" Score="3" Body="&lt;p&gt;Given that you mention wanting to use 1000 Genomes as a reference panel for imputing genotypes into your two SNP chip panels, I am going to assume that you are working with human data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In that case there are several options you can go with:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If your two panels are of European descent, then you are probably best off using the &lt;a href=&quot;http://www.haplotype-reference-consortium.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;HRC reference panel&lt;/a&gt; together with a fast genotype imputation tool such as &lt;a href=&quot;http://www.cell.com/ajhg/fulltext/S0002-9297(15)00491-7&quot; rel=&quot;nofollow noreferrer&quot;&gt;Beagle 4.1&lt;/a&gt; to impute genotypes in each of your two SNP chip panels separately.&lt;/li&gt;&#xA;&lt;li&gt;If your panels are not of European descent, then you will likely want to use the 1000 Genomes phase 3 reference panel with Beagle 4.1, Impute2, or Minimac3.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In either case, there are two phasing services available that will do much of the heavy lifting for you &lt;a href=&quot;https://imputationserver.sph.umich.edu/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;1&lt;/a&gt;,&lt;a href=&quot;https://imputation.sanger.ac.uk/&quot; rel=&quot;nofollow noreferrer&quot;&gt;2&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second &lt;a href=&quot;https://www.wtccc.org.uk/ccc2/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wellcome Trust Case-Control Consortium paper&lt;/a&gt; performed a cross-imputation analysis as you describe.  I don't see many studies using multiple SNP chip panels.  You will need to take care in your analysis that you are not hit by batch effects from using two different SNP chip panels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, none of these methods will work if the region you are imputing into has too few variants.  I'm not sure what the minimum number of variants is, but if you are using a whole genome genotyping panel of at least 500k SNPs, then you should be ok if you impute a whole chromosome at a time.&lt;/p&gt;&#xA;" OwnerUserId="492" LastEditorUserId="492" LastEditDate="2017-06-01T21:36:32.873" LastActivityDate="2017-06-01T21:36:32.873" CommentCount="3" />
  <row Id="357" PostTypeId="1" AcceptedAnswerId="359" CreationDate="2017-06-01T15:29:48.250" Score="5" ViewCount="309" Body="&lt;p&gt;As someone who's beginning to delve into bioinformatics, I'm noticing that like biology there are industry standards here, similar to Illumina in genomics and bowtie for alignment, many people use bash as shell. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is using a shell besides bash going to cause issues for me?&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="77" LastEditDate="2017-06-06T09:09:06.957" LastActivityDate="2017-06-13T15:08:56.573" Title="Using shells other than bash" Tags="&lt;linux&gt;&lt;shell&gt;" AnswerCount="5" CommentCount="8" FavoriteCount="1" />
  <row Id="358" PostTypeId="2" ParentId="357" CreationDate="2017-06-01T15:44:33.430" Score="4" Body="&lt;p&gt;I would not say bash as a &quot;standard&quot;, but it is indeed likely to be the most widely used unix shell and available by default on most modern unix/linux distros. There are a few other more convenient shells like &lt;a href=&quot;http://www.zsh.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;zsh&lt;/a&gt; that are broadly compatible with &lt;code&gt;/bin/sh&lt;/code&gt;, but they are not as widely available. There is also C-shell and in particular its open-source implementation &lt;a href=&quot;https://en.wikipedia.org/wiki/Tcsh&quot; rel=&quot;nofollow noreferrer&quot;&gt;tcsh&lt;/a&gt;. C-shell is quite different from bash. Over ten years ago, I saw it was used from time to time, but nowadays, I rarely see its use, except by programmers from older generations.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-01T15:44:33.430" CommentCount="0" />
  <row Id="359" PostTypeId="2" ParentId="357" CreationDate="2017-06-01T15:53:21.003" Score="14" Body="&lt;p&gt;Bioinformatics tools written in shell and other shell scripts generally specify the shell they want to use (via &lt;code&gt;#!/bin/sh&lt;/code&gt; or e.g. &lt;code&gt;#!/bin/bash&lt;/code&gt; if it matters), so won't be affected by your choice of user shell.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are writing significant shell scripts yourself, there are reasons to do it in a Bourne-style shell.  See &lt;em&gt;Csh Programming Considered Harmful&lt;/em&gt; and other essays/polemics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A Bourne-style shell is pretty much the industry standard, and if you choose a substantially different shell you'll have to translate some of the documentation of your bioinformatics tools.  It's not uncommon to have things like&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Set some variables pointing at reference data and add the script to your PATH to run it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;export FOO_REF=/path/to/stuff&#xA;export PATH=/path/to/foo-x.y:$PATH&#xA;foo blah blah&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;These will typically be shown in Bourne-shell syntax.  By using a different shell you have to translate the &lt;code&gt;export&lt;/code&gt; commands to your local syntax, and especially PATH munging is somewhat shell-dependent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're experienced in Unix, this will be only a minor niggle.  If you're a beginner, IMHO this will add a non-negligible amount of friction on top of all the other things you're learning.&lt;/p&gt;&#xA;" OwnerUserId="134" LastActivityDate="2017-06-01T15:53:21.003" CommentCount="0" />
  <row Id="360" PostTypeId="2" ParentId="357" CreationDate="2017-06-01T15:59:22.890" Score="7" Body="&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SH adheres to an official industry standard, but it is not suitable for scientific computing. BASH is considered an informal standard (e.g., by Google). BASH 3 is preferable in most of situations in the world of bioinformatics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Long answer&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As already described in other answers, SH (&lt;code&gt;/bin/sh&lt;/code&gt;, plain Bourne shell, the original UNIX shell) should fully adhere to POSIX which is a real industry standard. However, SH is too limited for scientific computing since many key features were incorporated later in SH successors, especially in BASH (&lt;code&gt;/bin/bash&lt;/code&gt;, Bourne Again Shell): &lt;code&gt;set -o pipefail&lt;/code&gt;, &lt;code&gt;[[ ... ]]&lt;/code&gt;, or process substitutions &lt;code&gt;&amp;lt; ()&lt;/code&gt; to name at least few.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In practice, it is much more difficult to write &quot;safe&quot; scripts in pure SH and only shell experts are usually capable to prevent unexpected behavior. For example, it may be hard to ensure that no command in a pipeline failed in the middle of computation. For BASH, various easy-to-follow defensive-programming recommendations have been developed and they should prevent all these problems. From this reason, many computer scientists, software engineers and companies use BASH as a kind of a standard. For instance, Google internal policy allows &lt;a href=&quot;https://google.github.io/styleguide/shell.xml?showone=Which_Shell_to_Use#Which_Shell_to_Use&quot; rel=&quot;nofollow noreferrer&quot;&gt;only BASH&lt;/a&gt; for writing any shell scripts. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Even though we cannot expect that BASH is present on completely every Unix machine (e.g., on mobile devices as @terdon pointed out), a vast majority of *nix machines used for scientific computation should have it. We also should be aware of the fact that BASH can be slower than SH and that it has recently suffered from  major &lt;a href=&quot;https://www.symantec.com/connect/blogs/shellshock-all-you-need-know-about-bash-bug-vulnerability&quot; rel=&quot;nofollow noreferrer&quot;&gt;security issues&lt;/a&gt;. Moreover, various BASH versions exist and scripts working on modern Linux machines with &lt;a href=&quot;http://wiki.bash-hackers.org/bash4&quot; rel=&quot;nofollow noreferrer&quot;&gt;BASH 4&lt;/a&gt; might not work on OS X, which is still based on BASH 3. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;To sum up, BASH 3 is probably be the most reasonable choice for scientific computing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I addressed the comments from @terdon and @John Marshall. In particular, I added an explanation why BASH is more suitable for scientific computing than SH (in my opinion).&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-06-05T19:16:50.987" LastActivityDate="2017-06-05T19:16:50.987" CommentCount="10" />
  <row Id="361" PostTypeId="1" AcceptedAnswerId="369" CreationDate="2017-06-01T16:47:02.727" Score="13" ViewCount="497" Body="&lt;p&gt;I used to work with publicly available genomic references, where basic statistics are usually available and if they are not, you have to compute them only once so there is no reason to worry about performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Recently I started sequencing project of couple of different species with mid-sized genomes (~Gbp) and during testing of different assembly pipelines I had compute number of unknown nucleotides many times in both raw reads (in fastq) and assembly scaffolds (in fasta), therefore I thought that I would like to optimize the computation.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For me it is reasonable to expect 4-line formatted fastq files, but general solution is still prefered&lt;/li&gt;&#xA;&lt;li&gt;It would be nice if solution would work on gzipped files as well&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Q : What is the fastest way (performance-wise) to compute the number of unknown nucleotides (Ns) in fasta and fastq files?&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-02T08:28:52.683" LastActivityDate="2017-06-23T13:39:36.970" Title="What is the fastest way to calculate the number of unknown nucleotides in FASTA / FASTQ files?" Tags="&lt;fasta&gt;&lt;fastq&gt;&lt;benchmarking&gt;" AnswerCount="9" CommentCount="1" FavoriteCount="2" />
  <row Id="362" PostTypeId="2" ParentId="326" CreationDate="2017-06-01T16:50:34.150" Score="0" Body="&lt;p&gt;The &quot;best&quot; assembly depends on the species. For more common species, Ensembl/UCSC/NCBI has their own version that corresponds to the most popular assembly. Usually the actual genetic sequence is identical across all of them, but the chromosome names and gene annotations will vary. If those three sources agree for a species, there is a &quot;standard&quot; reference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For fruit fly specifically, FlyBase Consortium/Berkeley Drosophila Genome Project is the gold standard. Other species have their own organizations.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-06-01T16:50:34.150" CommentCount="0" />
  <row Id="363" PostTypeId="2" ParentId="361" CreationDate="2017-06-01T17:00:25.523" Score="6" Body="&lt;p&gt;&lt;strong&gt;FASTQ&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As it was pointed out, fastq can be complicated. But in a simple case when you have four lines per record, one possible solution in bash is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sed -n '2~4p' seqs.fastq | grep -io N | wc -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;sed -n '2~4p'&lt;/code&gt; will print every fourth line&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;grep -o N&lt;/code&gt; will output a line with &lt;code&gt;N&lt;/code&gt; for every matching symbol&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;wc -l&lt;/code&gt; will count the lines&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I suspect this python approach will work faster:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat seqs.fastq | python3 -c &quot;import sys; print(sum(line.upper().count('N') for line in sys.stdin))&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;FASTA&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Coreutils:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;grep -v &quot;^&amp;gt;&quot; seqs.fasta | grep -io N | wc -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Python:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat seqs.fasta | python3 -c &quot;import sys; print(sum(line.upper().count('N') for line in sys.stdin if not line.startswith('&amp;gt;')))&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="191" LastEditorUserId="298" LastEditDate="2017-06-02T13:41:57.900" LastActivityDate="2017-06-02T13:41:57.900" CommentCount="4" />
  <row Id="364" PostTypeId="2" ParentId="361" CreationDate="2017-06-01T17:10:02.993" Score="4" Body="&lt;p&gt;Honestly, the easiest way (especially for FASTQ) is probably to use a dedicated parsing library, such as R/Bioconductor:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;suppressMessages(library(ShortRead))&#xA;&#xA;seq = readFasta(commandArgs(TRUE)[1]) # or readFastq&#xA;cat(colSums(alphabetFrequency(sread(seq))[, 'N', drop = FALSE]), '\n')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This may not be the fastest, but it’s &lt;em&gt;pretty fast&lt;/em&gt;, since the relevant methods are highly optimised. The biggest overhead is actually probably from loading &lt;code&gt;ShortRead&lt;/code&gt;, which is an unjustifiable slog.&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-06-02T14:53:35.553" LastActivityDate="2017-06-02T14:53:35.553" CommentCount="0" />
  <row Id="365" PostTypeId="2" ParentId="361" CreationDate="2017-06-01T17:15:22.907" Score="4" Body="&lt;p&gt;If it is raw speed you're after, then writing an own little C/C++ program is probably what you need to do.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fortunately, the worst part (a fast and reliable parser) has already been tackled: the &lt;a href=&quot;https://github.com/lh3/readfq&quot; rel=&quot;nofollow noreferrer&quot;&gt;readfq&lt;/a&gt; from Heng Li is probably the fastest FASTA/FASTQ parser around.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And it's easy to use, the example on GitHub can easily be expanded to do what you need. Just add in a parameter parser (for the filename you want to analyse), program a simple 'N'-counter loop and have the results printed to stdout. Done.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Else, if it is simplicity you are after, see Konrad's answer for an R based solution. Or a very simple script using Biopython.&lt;/p&gt;&#xA;" OwnerUserId="44" LastActivityDate="2017-06-01T17:15:22.907" CommentCount="3" />
  <row Id="366" PostTypeId="2" ParentId="361" CreationDate="2017-06-01T17:22:20.160" Score="7" Body="&lt;p&gt;I think that this should be pretty fast:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;FASTA:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;grep -v &quot;^&amp;gt;&quot; seqs.fa | tr -cd N | wc -c&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;FASTQ:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sed -n '1d;N;N;N;P;d' seqs.fq | tr -cd N | wc -c&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;See this &lt;a href=&quot;https://stackoverflow.com/a/16679640/4641846&quot;&gt;answer on SO&lt;/a&gt; about how to count characters in BASH using different approaches.&lt;/p&gt;&#xA;" OwnerUserId="425" LastActivityDate="2017-06-01T17:22:20.160" CommentCount="4" />
  <row Id="367" PostTypeId="1" AcceptedAnswerId="795" CreationDate="2017-06-01T19:49:22.177" Score="12" ViewCount="132" Body="&lt;p&gt;According to &lt;a href=&quot;https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/&quot; rel=&quot;nofollow noreferrer&quot;&gt;this famous blog post&lt;/a&gt;, the effective transcript length is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\tilde{l}_i = l_i - \mu$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $l_i$ is the length of transcript and $\mu$ is the average fragment length. However, typically fragment length is about 300bp. What if when the transcript $l_i$ is smaller than 300? How do you compute the effective length in this case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A related question: when computing the FPKM of a gene, how to choose a transcript? Do we choose a &quot;canonical&quot; transcript (how?) or combine the signals from all transcripts to a gene-level FPKM?&lt;/p&gt;&#xA;" OwnerUserId="37" LastEditorUserId="298" LastEditDate="2017-07-26T16:06:30.160" LastActivityDate="2017-07-26T16:15:39.647" Title="How exactly is &quot;effective length&quot; used in FPKM calculated?" Tags="&lt;rna-seq&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="368" PostTypeId="2" ParentId="367" CreationDate="2017-06-01T20:03:41.287" Score="10" Body="&lt;p&gt;The effective length is $\tilde{l}_i = l_i - \mu + 1$ (note the R code at the bottom of Harold's blog post), which in the case of $\mu &amp;lt; l_i$ should be 1. Ideally, you'd use the mean fragment length mapped to the particular feature, rather than a global $\mu$, but that's a lot more work for likely 0 benefit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding choosing a particular transcript, ideally one would use a method like salmon or kallisto (or RSEM if you have time to kill). Otherwise, your options are (A) choose the major isoform (if it's known in your tissue and condition) or (B) use a &quot;union gene model&quot; (sum the non-redundant exon lengths) or (C) take the median transcript length. None of those three options make much of a difference if you're comparing between samples, though they're all inferior to a salmon/kallisto/etc. metric.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why are salmon et al. better methods? They don't use arbitrary metrics that will be the same across samples to determine the feature length. Instead, they use expectation maximization (or similarish, since at least salmon doesn't actually use EM) to quantify individual isoform usage. The effective gene length in a sample is then the average of the transcript lengths after weighting for their relative expression (yes, one should remove $\mu$ in there). This can then vary between samples, which is quite useful if you have isoform switching between samples/groups in such a way that methods A-C above would miss (think of cases where the switch is to a smaller transcript with higher coverage over it...resulting in the coverage/length in methods A-C to be tamped down).&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="29" LastEditDate="2017-07-26T16:04:12.083" LastActivityDate="2017-07-26T16:04:12.083" CommentCount="2" />
  <row Id="369" PostTypeId="2" ParentId="361" CreationDate="2017-06-01T21:46:22.750" Score="16" Body="&lt;p&gt;For FASTQ:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;seqtk fqchk in.fq | head -2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It gives you percentage of &quot;N&quot; bases, not the exact count, though.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For FASTA:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-sh prettyprint-override&quot;&gt;&lt;code&gt;seqtk comp in.fa | awk '{x+=$9}END{print x}'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This command line also works with FASTQ, but it will be slower as awk is slow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: ok, based on @BaCH's reminder, here we go (you need &lt;a href=&quot;https://github.com/lh3/readfq/blob/master/kseq.h&quot; rel=&quot;nofollow noreferrer&quot;&gt;kseq.h&lt;/a&gt; to compile):&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-c prettyprint-override&quot;&gt;&lt;code&gt;// to compile: gcc -O2 -o count-N this-prog.c -lz&#xA;#include &amp;lt;zlib.h&amp;gt;&#xA;#include &amp;lt;stdio.h&amp;gt;&#xA;#include &amp;lt;stdint.h&amp;gt;&#xA;#include &quot;kseq.h&quot;&#xA;KSEQ_INIT(gzFile, gzread)&#xA;&#xA;unsigned char dna5tbl[256] = {&#xA;    0, 1, 2, 3,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 5, 4, 4,&#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 0, 4, 1,  4, 4, 4, 2,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  3, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 0, 4, 1,  4, 4, 4, 2,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  3, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4, &#xA;    4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4,  4, 4, 4, 4&#xA;};&#xA;&#xA;int main(int argc, char *argv[]) {&#xA;    long i, n_n = 0, n_acgt = 0, n_gap = 0;&#xA;    gzFile fp;&#xA;    kseq_t *seq;&#xA;    if (argc == 1) {&#xA;        fprintf(stderr, &quot;Usage: count-N &amp;lt;in.fa&amp;gt;\n&quot;);&#xA;        return 1;&#xA;    }&#xA;    if ((fp = gzopen(argv[1], &quot;r&quot;)) == 0) {&#xA;        fprintf(stderr, &quot;ERROR: fail to open the input file\n&quot;);&#xA;        return 1;&#xA;    }&#xA;    seq = kseq_init(fp);&#xA;    while (kseq_read(seq) &amp;gt;= 0) {&#xA;        for (i = 0; i &amp;lt; seq-&amp;gt;seq.l; ++i) {&#xA;            int c = dna5tbl[(unsigned char)seq-&amp;gt;seq.s[i]];&#xA;            if (c &amp;lt; 4) ++n_acgt;&#xA;            else if (c == 4) ++n_n;&#xA;            else ++n_gap;&#xA;        }&#xA;    }&#xA;    kseq_destroy(seq);&#xA;    gzclose(fp);&#xA;    printf(&quot;%ld\t%ld\t%ld\n&quot;, n_acgt, n_n, n_gap);&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It works for both FASTA/Q and gzip'ed FASTA/Q. The following uses SeqAn:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cpp prettyprint-override&quot;&gt;&lt;code&gt;#include &amp;lt;seqan/seq_io.h&amp;gt;&#xA;&#xA;using namespace seqan;&#xA;&#xA;int main(int argc, char *argv[]) {&#xA;    if (argc == 1) {&#xA;        std::cerr &amp;lt;&amp;lt; &quot;Usage: count-N &amp;lt;in.fastq&amp;gt;&quot; &amp;lt;&amp;lt; std::endl;&#xA;        return 1;&#xA;    }&#xA;    std::ios::sync_with_stdio(false);&#xA;    CharString id;&#xA;    Dna5String seq;&#xA;    SeqFileIn seqFileIn(argv[1]);&#xA;    long i, n_n = 0, n_acgt = 0;&#xA;    while (!atEnd(seqFileIn)) {&#xA;        readRecord(id, seq, seqFileIn);&#xA;        for (i = beginPosition(seq); i &amp;lt; endPosition(seq); ++i)&#xA;            if (seq[i] &amp;lt; 4) ++n_acgt;&#xA;            else ++n_n;&#xA;    }&#xA;    std::cout &amp;lt;&amp;lt; n_acgt &amp;lt;&amp;lt; '\t' &amp;lt;&amp;lt; n_n &amp;lt;&amp;lt; std::endl;&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On a FASTQ with 4-million 150bp reads:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The C version: ~0.74 sec&lt;/li&gt;&#xA;&lt;li&gt;The C++ version: ~2.15 sec&lt;/li&gt;&#xA;&lt;li&gt;An older C version without a lookup table (see &lt;a href=&quot;https://bioinformatics.stackexchange.com/posts/369/revisions&quot;&gt;the previous edit&lt;/a&gt;): ~2.65 sec&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="37" LastEditorUserId="37" LastEditDate="2017-06-09T01:02:55.120" LastActivityDate="2017-06-09T01:02:55.120" CommentCount="7" />
  <row Id="370" PostTypeId="2" ParentId="321" CreationDate="2017-06-01T22:07:27.130" Score="3" Body="&lt;p&gt;I personally don't think BQSR has a huge impact on variant calling, but you don't really need to guess. If you run GATK BQSR, it outputs a table and charts of exactly how much quality scores are adjusted. The adjustment will vary depending on the position in the read and genomic context (previous and following base). In my experience, the difference is a few points at most, but it's certainly noticeable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;GATK recommends BQSR for both genome and exome data, which is normally much higher than 15x.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-06-01T22:07:27.130" CommentCount="0" />
  <row Id="371" PostTypeId="2" ParentId="361" CreationDate="2017-06-01T22:20:21.293" Score="15" Body="&lt;p&gt;5 hours and no benchmarks posted? I am sorely disappointed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'll restrict the comparison to just be fasta files, since fastq will end up being the same. So far, the contenders are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;R with the &lt;code&gt;ShortRead&lt;/code&gt; package (even if not the fastest, certainly a super convenient method).&lt;/li&gt;&#xA;&lt;li&gt;A pipeline of &lt;code&gt;grep -v &quot;^&amp;gt;&quot; | tr -cd A | wc -c&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;A pipeline of &lt;code&gt;grep -v &quot;^&amp;gt;&quot; | grep -io A | wc -l&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;A pipeline of &lt;code&gt;seqtk comp | awk&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Something custom in C/C++&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The R code can't actually count N records for some reason when I try it, so I counted &lt;code&gt;A&lt;/code&gt; for everything.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'll exclude all python solutions, because there's no conceivable universe in which python is fast. For the C/C++ solution I just used what @user172818 posted (everything I tried writing took the same amount of time).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Repeating 100 times and taking the average (yes, one should take the median):&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;1.7 seconds&lt;/li&gt;&#xA;&lt;li&gt;0.65 seconds&lt;/li&gt;&#xA;&lt;li&gt;15 seconds&lt;/li&gt;&#xA;&lt;li&gt;1.2 seconds&lt;/li&gt;&#xA;&lt;li&gt;0.48 seconds&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Unsurprisingly, anything in straight C or C++ is going to win. &lt;code&gt;grep&lt;/code&gt; with &lt;code&gt;tr&lt;/code&gt; is surprisingly good, which surprises me since even though &lt;code&gt;grep&lt;/code&gt; is very very optimized I still expected it to have more overhead. Piping to &lt;code&gt;grep -io&lt;/code&gt; is a performance killer. The R solution is surprisingly good given the typical R overhead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update1:&lt;/strong&gt; As suggested in the comments&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;6&quot;&gt;&#xA;&lt;li&gt;&lt;code&gt;sum(x.count('A') for x in open('seqs.fa','r') if x[0] != '&amp;gt;')&lt;/code&gt; in python&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;This yields a time of&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;6&quot;&gt;&#xA;&lt;li&gt;1.6 seconds (I'm at work now, so I've adjusted the time as best I can to account for the different computers)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update2:&lt;/strong&gt; More benchmarks from the comments&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;7&quot;&gt;&#xA;&lt;li&gt;&lt;code&gt;awk -F A '!/^&amp;gt;&quot;/ {cnt+=NF-1}END{print cnt}' foo.fa&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;perl -ne 'if(!/^&amp;gt;/){$count += tr/A//} END{print &quot;$count\n&quot;}' foo.fa&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;These yield times of:&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;7&quot;&gt;&#xA;&lt;li&gt;5.2 seconds&lt;/li&gt;&#xA;&lt;li&gt;1.18 seconds&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The awk version is the best hack I could come up with, there are likely better ways.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update 3&lt;/strong&gt;: Right, so regarding fastq files, I'm running this on a 3.3GB gzipped file with 10 repetitions (this takes a bit of time to run), so I'm not going to initially limit tests to commands that I can trivially modify to handle compressed files (after all, uncompressed fastq files are an abomination).&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;seqtk fqchk&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;sed -n '2~4p' &amp;lt;(zcat foo.fastq.gz) | grep -io A | wc -l&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;sed -n '1d;N;N;N;P;d' &amp;lt;(zcat Undetermined_S0_R1_001.fastq.gz) | tr -cd A | wc -c&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;The C example from user1728181 (the comparison to C++ is already provided there, so no need to include it).&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' foo.fastq.gz&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The average times are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;1 minute 44 seconds&lt;/li&gt;&#xA;&lt;li&gt;2 minutes 6 seconds&lt;/li&gt;&#xA;&lt;li&gt;1 minute 52 seconds&lt;/li&gt;&#xA;&lt;li&gt;1 minute 15 seconds&lt;/li&gt;&#xA;&lt;li&gt;3 minutes 8 seconds&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;N.B., these will often not handle fastq files with entries spanning more than 4 lines. However, such files are the dodo birds of bioinformatics, so that's not practically important. Also, I couldn't get &lt;code&gt;ShortRead&lt;/code&gt; to work with the compressed fastq file. I suspect there's a way to fix that.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-02T11:41:38.503" LastActivityDate="2017-06-02T11:41:38.503" CommentCount="24" />
  <row Id="372" PostTypeId="1" AcceptedAnswerId="373" CreationDate="2017-06-01T22:57:13.273" Score="3" ViewCount="45" Body="&lt;p&gt;Without going into too much background, I just joined up with a lab as a bioinformatics intern while I'm completing my masters degree in the field. The lab has data from an RNA-seq they outsourced, but the only problem is that the only data they have is preprocessed from the company that did the sequencing: filtering the reads, aligning them, and putting the aligned reads through RSEM. I currently have output from RSEM for each of the four samples consisting of: gene id, transcript id(s), length, expected count, and FPKM. I am attempting to get the FASTQ files from the sequencing, but for now, this is what I have, and I'm trying to get something out of it if possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found &lt;a href=&quot;https://biowize.wordpress.com/2014/03/04/understanding-rsem-raw-read-counts-vs-expected-counts/&quot; rel=&quot;nofollow noreferrer&quot;&gt;this article&lt;/a&gt; that talks about how expected read counts can be better than raw read counts when analyzing differential expression using EBSeq; it's just one guy's opinion, and it's from 2014, so it may be wrong or outdated, but I thought I'd give it a try since I have the expected counts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I have just a couple of questions about running EBSeq that I can't find the answers to:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1: In the output RSEM files I have, not all genes are represented in each, about 80% of them are, but for the ones that aren't, should I remove them before analysis with EBSeq? It runs when I do, but I'm not sure if it is correct.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2: How do I know which normalization factor to use when running EBSeq? This is more of a conceptual question rather than a technical question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="506" LastEditorUserId="73" LastEditDate="2017-06-02T07:50:21.887" LastActivityDate="2017-06-02T07:50:21.887" Title="Missing genes and normalisation of RSEM output using EBSeq" Tags="&lt;rna-seq&gt;&lt;differential-expression&gt;&lt;ebseq&gt;&lt;rsem&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="373" PostTypeId="2" ParentId="372" CreationDate="2017-06-02T00:14:20.617" Score="5" Body="&lt;p&gt;Yes, that blog post &lt;em&gt;does&lt;/em&gt; represent just one guy's opinion (hi!) and it does date &lt;em&gt;all the way back to 2014&lt;/em&gt;, which is, like, decades in genomics years. :-) By the way, there is quite a bit of literature discussing the improvements that expected read counts derived from an Expectation Maximization algorithm provide over raw read counts. I'd suggest reading the RSEM papers for a start&lt;sup&gt;[&lt;a href=&quot;http://dx.doi.org/10.1093/bioinformatics/btp692&quot; rel=&quot;nofollow noreferrer&quot;&gt;1&lt;/a&gt;][&lt;a href=&quot;http://dx.doi.org/10.1186/1471-2105-12-323&quot; rel=&quot;nofollow noreferrer&quot;&gt;2&lt;/a&gt;]&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But your main question is about the mechanics of running RSEM and EBSeq. First, RSEM was written explicitly to be compatible with EBSeq, so I'd be very surprised if it does not work correctly out-of-the-box. Second, EBSeq's &lt;code&gt;MedianNorm&lt;/code&gt; function worked very well in my experience for normalizing the library counts. Along those lines, the blog you mentioned above has &lt;a href=&quot;https://biowize.wordpress.com/2013/12/12/normalization-for-differential-expression-analysis/&quot; rel=&quot;nofollow noreferrer&quot;&gt;another post&lt;/a&gt; that you may find useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But all joking aside, these tools &lt;em&gt;are indeed&lt;/em&gt; dated. Alignment-free RNA-Seq tools provide orders-of-magnitude improvements in runtime over the older alignment-based alternatives, with comparable accuracy. &lt;a href=&quot;https://github.com/kingsfordgroup/sailfish&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sailfish&lt;/a&gt; was the first in a growing list of tools that now includes &lt;a href=&quot;https://github.com/COMBINE-lab/salmon&quot; rel=&quot;nofollow noreferrer&quot;&gt;Salmon&lt;/a&gt; and &lt;a href=&quot;https://github.com/pachterlab/kallisto&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kallisto&lt;/a&gt;. When starting a new analysis from scratch (i.e. if you ever get the original FASTQ files), there's really no good reason not to estimate expression using these much faster tools, followed by a differential expression analysis with DESeq2, edgeR, or sleuth.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;strong&gt;Li B, Ruotti V, Stewart RM, Thomson JA, Dewey CN&lt;/strong&gt; (2010) RNA-Seq gene expression estimation with read mapping uncertainty. &lt;em&gt;Bioinformatics&lt;/em&gt;, 26(4):493–500, &lt;a href=&quot;http://dx.doi.org/10.1093/bioinformatics/btp692&quot; rel=&quot;nofollow noreferrer&quot;&gt;doi:10.1093/bioinformatics/btp692&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;strong&gt;Li B, Dewey C&lt;/strong&gt; (2011) RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome. &lt;em&gt;BMC Bioinformatics&lt;/em&gt;, 12:323, &lt;a href=&quot;http://dx.doi.org/10.1186/1471-2105-12-323&quot; rel=&quot;nofollow noreferrer&quot;&gt;doi:10.1186/1471-2105-12-323&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-06-02T02:54:54.193" LastActivityDate="2017-06-02T02:54:54.193" CommentCount="5" />
  <row Id="374" PostTypeId="2" ParentId="372" CreationDate="2017-06-02T01:40:28.860" Score="3" Body="&lt;ol&gt;&#xA;&lt;li&gt;Include all genes/transcripts in your analysis.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;A transcript that is not detected could be undetected through sampling error (i.e. the sequencer / library prep just happened to miss that transcript), or it could be because the transcript isn't generated in a particular sample. It's not uncommon for genes to be switched off in response to different biological factors, so zero-count genes shouldn't be ignored. I can't speak from experience with EBSeq, but as long as the analysis package treats a zero count as &quot;unobserved&quot; rather than &quot;absent&quot; (and makes relevant corrections), it's a good idea to keep them in.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-02T01:40:28.860" CommentCount="0" />
  <row Id="375" PostTypeId="1" CreationDate="2017-06-02T02:48:04.570" Score="8" ViewCount="32" Body="&lt;p&gt;After discovering a few difficulties with genome assembly, I've taken an interest in finding and categorising repetitive DNA sequences, such as this one from &lt;em&gt;Nippostrongylus brasiliensis&lt;/em&gt; [each base is colour-coded as A: green; C: blue; G: yellow; T: red]:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/QSk09.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/QSk09.png&quot; alt=&quot;Repeat sequence represented in a rectangular fashion&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/RjVLd.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RjVLd.png&quot; alt=&quot;Repeat sequence represented in a circular fashion&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[FASTA file associated with this sequence can be found &lt;a href=&quot;http://www.gringene.org/data/longRepeat_ch192_r13894.fa&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These sequences with large repeat unit sizes are only detectable (and assembleable) using long reads (e.g. PacBio, nanopore) because any subsequence smaller than the unit length will not be able to distinguish between sequencing error and hitting a different location within the repeat structure. I have been tracking these sequences down in a bulk fashion by two methods:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Running an all-vs-all mapping, and looking for sequences that map to themselves lots of times&lt;/li&gt;&#xA;&lt;li&gt;Carrying out a compression of the sequence (e.g. bzip2), and finding sequences that have a compression rate that is substantially higher than normal&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;After I've found a suspicious sequence, I then want to be able to categorise the repeat (e.g. major repeat length, number of tandem repeats, repetitive sequence). This is where I'm getting stuck.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For doing a &quot;look, shiny&quot; demonstration, I currently have a very manual process of getting these sequences into a format that I can visualise. My process is as follows:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use LAST to produce a dot plot of self-mapping for the mapping&lt;/li&gt;&#xA;&lt;li&gt;Visually identify the repetitive region, and extract out the region from the sequence&lt;/li&gt;&#xA;&lt;li&gt;Use a combination of &lt;code&gt;fold -w &amp;lt;width&amp;gt;&lt;/code&gt; and &lt;code&gt;less -S&lt;/code&gt; to visually inspect the sequence with various potential repeat unit widths to find the most likely repeat unit size&lt;/li&gt;&#xA;&lt;li&gt;Display the sequence in a rectangular and circular fashion using &lt;a href=&quot;https://github.com/gringer/bioinfscripts/blob/master/seqmat.r&quot; rel=&quot;noreferrer&quot;&gt;my own script&lt;/a&gt;, wrapping at the repeat unit length&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;But that process is by no means feasible when I've got thousands of potential repetitive sequences to fish through.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any better way to do this? Given an arbitrary DNA sequence of length &gt;10kb, how can I (in an automated fashion) find both the location of the repeat region, and also the unit length (bearing in mind that there might be multiple repeat structures, with unit lengths from 30bp to 10kb)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An example sequence can be found &lt;a href=&quot;http://www.gringene.org/data/fd2.fa&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;, which has a ~21kb repeat region with ~171bp repeat units about 1/3 of the way into the sequence.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-02T14:20:35.313" Title="Finding the location and unit length of repetitive sequences within a long read" Tags="&lt;nanopore&gt;&lt;long-reads&gt;&lt;repeat-elements&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="376" PostTypeId="2" ParentId="357" CreationDate="2017-06-02T03:42:33.323" Score="4" Body="&lt;p&gt;The generic command &lt;code&gt;sh&lt;/code&gt; is quite literally an industry standard, a POSIX standard, to be precise (IEEE 1003.2 and 1003.2a, available for purchase for hundreds of dollars at various websites). In theory, any script that starts with &lt;code&gt;#!/bin/sh&lt;/code&gt; should conform to this standard. In practise, most Linux systems have a shell that is close to this standard, but has a few quirks and extensions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Problems crop up when these quirks and extensions become standard practise in shell scripts. The Debian operating system changed to &lt;code&gt;dash&lt;/code&gt; as their &lt;code&gt;sh&lt;/code&gt; shell to encourage people to stop using &quot;bashisms&quot; in shell scripts that didn't specify a particular shell, i.e. those that began with &lt;code&gt;#!/bin/sh&lt;/code&gt;. The &lt;code&gt;dash&lt;/code&gt; shell tries to be as standards-compliant as possible:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;dash is the standard command interpreter for the system.  The current version of dash is in the process of being changed to&#xA;       conform with the POSIX 1003.2 and 1003.2a specifications for the shell.  This version has many features which make it&#xA;       appear similar in some respects to the Korn shell, but it is not a Korn shell clone (see ksh(1)).  Only features designated&#xA;       by POSIX, plus a few Berkeley extensions, are being incorporated into this shell.  This man page is not intended to be a&#xA;       tutorial or a complete specification of the shell.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I'm not familiar with the differences, and generally try to stick to the &lt;code&gt;sh&lt;/code&gt; man pages to instruct me with regards to correct standards-compliant shell scripts.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-06T19:45:54.183" LastActivityDate="2017-06-06T19:45:54.183" CommentCount="3" />
  <row Id="377" PostTypeId="2" ParentId="357" CreationDate="2017-06-02T06:54:02.307" Score="5" Body="&lt;p&gt;&lt;a href=&quot;http://pubs.opengroup.org/onlinepubs/9699919799.2016edition/&quot; rel=&quot;nofollow noreferrer&quot;&gt;The Open Group Base Specifications Issue 7&#xA;IEEE Std 1003.1™-2008, 2016 Edition&lt;/a&gt;, or &quot;The POSIX Standard&quot; for short, is the standard that defines the interfaces and utilities provided by a Unix system.  Among these is the command line shell language and tools (see &quot;Shell &amp;amp; Utilities&quot; in the main index on the page linked above).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I know, there is no shell that implement &lt;em&gt;exactly&lt;/em&gt; what's specified by the standard, but both &lt;code&gt;bash&lt;/code&gt; and &lt;code&gt;ksh93&lt;/code&gt; does a pretty good job of adhering to the standard along with their own, sometimes conflicting, extensions. The &lt;code&gt;ksh93&lt;/code&gt; shell in particular has had a big impact on the past development of the POSIX shell specification, but future POSIX specifications &lt;em&gt;may&lt;/em&gt; borrow more from &lt;code&gt;bash&lt;/code&gt; due to its wide use on Linux.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;bash&lt;/code&gt; shell is pretty much ubiquitous on Linux systems, and may be installed on all other Unices too. &lt;code&gt;ksh93&lt;/code&gt; is also available for most Unices but is usually not installed by default on Linux. &lt;code&gt;ksh93&lt;/code&gt; is available by default on at least macOS (as &lt;code&gt;ksh&lt;/code&gt;) and Solaris.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are concerned about portability when writing a shell script (which is IMHO a good thing to be concerned about), you should make sure that you use only the POSIX utilities and their POSIX command line flags, as well as only use POSIX shell syntax. You should then ensure that you script is executed by &lt;code&gt;/bin/sh&lt;/code&gt; which is supposed to be a shell that understands the POSIX specification.  &lt;code&gt;/bin/sh&lt;/code&gt; is often implemented by &lt;code&gt;bash&lt;/code&gt; running in &quot;POSIX mode&quot;, but it may also be &lt;code&gt;dash&lt;/code&gt;, &lt;code&gt;ash&lt;/code&gt; or &lt;code&gt;pdksh&lt;/code&gt; (or something else) depending on what Unix you are using.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a Linux user, the most difficult bit in writing a portable script is often not the shell per se, but the multitude of non-standard command line flags provided by the GNU implementation of the many shell utilities.  The GNU coreutils (basic shell utilities) may, like &lt;code&gt;bash&lt;/code&gt;, be installed on all Unices though.&lt;/p&gt;&#xA;" OwnerUserId="434" LastEditorUserId="434" LastEditDate="2017-06-13T15:08:56.573" LastActivityDate="2017-06-13T15:08:56.573" CommentCount="0" />
  <row Id="378" PostTypeId="1" CreationDate="2017-06-02T07:13:13.107" Score="9" ViewCount="96" Body="&lt;p&gt;Hidden Markov models (HMMs) are used extensively in bioinformatics, and have been adapted for gene prediction, protein family classification, and a variety of other problems. Indeed, &lt;a href=&quot;http://www.cambridge.org/gb/academic/subjects/life-sciences/genomics-bioinformatics-and-systems-biology/biological-sequence-analysis-probabilistic-models-proteins-and-nucleic-acids&quot; rel=&quot;noreferrer&quot;&gt;the treatise by Durbin, Eddy and colleagues&lt;/a&gt; is one of the defining volumes in this field.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Although the details of each of these different applications of HMMs differ, the core mathematical model remains unchanged, and there are efficient algorithms for computing the probability of the observed sequence given the model, or (perhaps more useful) the most likely hidden sequence given the sequence of observed states.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Accordingly, it seems plausible that there could be a generic software library for solving HMMs. As far as I can tell that's not the case, and most bioinformaticians end up writing HMMs from scratch. Perhaps there's a good reason for this? (Aside from the obvious fact that it's already difficult, nigh impossible, to get funding to build and provide long-term support for open source science software. Academic pressures incentivize building a new tool that you can publish a paper on much more than building on and extending existing tools.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do any generic HMM solver libraries exist? If so, would this be tempting enough for bioinformaticians to use rather than writing their own from scratch?&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-06-04T13:37:40.803" Title="Generic HMM solvers in bioinformatics?" Tags="&lt;hidden-markov-models&gt;&lt;sequence-analysis&gt;" AnswerCount="3" CommentCount="1" />
  <row Id="379" PostTypeId="2" ParentId="297" CreationDate="2017-06-02T07:52:08.220" Score="4" Body="&lt;p&gt;You mention that FastQC &lt;em&gt;&quot;fails to find the actual adapter sequences&quot;&lt;/em&gt; - I guess you mean in the Adapter Sequence Contamination plot. However, the kmer and Sequence Content Plots are often useful even when the former fails. I've used these in the past - you can sometimes just read off the adapter sequence from the start of the Sequence Content Plot (or at least see how many bases to trim).&lt;/p&gt;&#xA;" OwnerUserId="224" LastActivityDate="2017-06-02T07:52:08.220" CommentCount="0" />
  <row Id="380" PostTypeId="2" ParentId="361" CreationDate="2017-06-02T09:28:46.290" Score="5" Body="&lt;h2&gt;Using bioawk&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;With &lt;a href=&quot;https://github.com/lh3/bioawk&quot; rel=&quot;nofollow noreferrer&quot;&gt;bioawk&lt;/a&gt; (counting &quot;A&quot; in the &lt;em&gt;C. elegans&lt;/em&gt; genome, because there seem to be no &quot;N&quot; in this file), on my computer:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time bioawk -c fastx '{n+=gsub(/A/, &quot;&quot;, $seq)} END {print n}' genome.fa &#xA;32371810&#xA;&#xA;real    0m1.645s&#xA;user    0m1.548s&#xA;sys     0m0.088s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;bioawk is an extension of awk with convenient parsing options. For instance &lt;code&gt;-c fastx&lt;/code&gt; makes the sequence available as &lt;code&gt;$seq&lt;/code&gt; in fasta and fastq format (&lt;strong&gt;including gzipped versions&lt;/strong&gt;), so the above command &lt;strong&gt;should also handle fastq format robustly&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;gsub&lt;/code&gt; command is a normal awk function. It returns the number of substituted characters (got it from &lt;a href=&quot;https://unix.stackexchange.com/a/169575/55127&quot;&gt;https://unix.stackexchange.com/a/169575/55127&lt;/a&gt;). I welcome comments about how to count inside awk in a less hackish way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On this fasta example it is almost 3 times slower than the &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;tr&lt;/code&gt;, &lt;code&gt;wc&lt;/code&gt; pipeline:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time grep -v &quot;^&amp;gt;&quot; genome.fa | tr -cd &quot;A&quot; | wc -c&#xA;32371810&#xA;&#xA;real    0m0.609s&#xA;user    0m0.744s&#xA;sys     0m0.184s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(I repeated the timings, and the above values seem representative)&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Using python&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;readfq&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I tried the python readfq retrieved from the repository mentioned in this answer: &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/365/292&quot;&gt;https://bioinformatics.stackexchange.com/a/365/292&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('A') for _, seq, _ in readfq(stdin)))&quot; &amp;lt; genome.fa&#xA;32371810&#xA;&#xA;real    0m0.976s&#xA;user    0m0.876s&#xA;sys     0m0.100s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3&gt;&quot;pure python&quot;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Comparing with something adapted from this answer: &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/363/292&quot;&gt;https://bioinformatics.stackexchange.com/a/363/292&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time python -c &quot;from sys import stdin; print sum((line.count('A') for line in stdin if not line.startswith('&amp;gt;')))&quot; &amp;lt; genome.fa&#xA;32371810&#xA;&#xA;real    0m1.143s&#xA;user    0m1.100s&#xA;sys     0m0.040s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(It is slower with python 3.6 than with python 2.7 on my computer)&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;pyGATB&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I just learned (08/06/2017) that &lt;a href=&quot;http://gatb.inria.fr/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GATB&lt;/a&gt; includes a fasta/fastq parser and has recently released a python API. I tried to use it yesterday to test another answer to the present question and &lt;a href=&quot;https://github.com/GATB/pyGATB/issues/2&quot; rel=&quot;nofollow noreferrer&quot;&gt;found a bug&lt;/a&gt;. This bug is now fixed, so here is a &lt;a href=&quot;https://pypi.python.org/pypi/pyGATB/0.1.2&quot; rel=&quot;nofollow noreferrer&quot;&gt;pyGATB&lt;/a&gt;-based answer:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.count(b\&quot;A\&quot;) for seq in Bank(\&quot;genome.fa\&quot;))))&quot;&#xA;32371810&#xA;&#xA;real    0m0.663s&#xA;user    0m0.568s&#xA;sys     0m0.092s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(You can also do &lt;code&gt;sequence.decode(&quot;utf-8&quot;).count(&quot;A&quot;)&lt;/code&gt; but this seems &lt;a href=&quot;https://github.com/GATB/pyGATB/issues/2#issuecomment-307131176&quot; rel=&quot;nofollow noreferrer&quot;&gt;a little slower&lt;/a&gt;.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Although I used python3.6 here (pyGATB seems python3-only), this is faster than the other two python approaches (for which the reported timings are obtained with python 2.7). This is even almost as fast as the &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;tr&lt;/code&gt;, &lt;code&gt;wc&lt;/code&gt; pipeline.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Biopython&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;And, to have even more comparisons, here is a solution using &lt;code&gt;SeqIO.parse&lt;/code&gt; from Biopython (with python2.7):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time python -c &quot;from Bio import SeqIO; print(sum((rec.seq.count(\&quot;A\&quot;) for rec in SeqIO.parse(\&quot;genome.fa\&quot;, \&quot;fasta\&quot;))))&quot;&#xA;32371810&#xA;&#xA;real    0m1.632s&#xA;user    0m1.532s&#xA;sys     0m0.096s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is a bit slower than the &quot;pure python&quot; solution, but perhaps more robust.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There seems to be a slight improvement with @peterjc's suggestion to use the lower level &lt;code&gt;SimpleFastaParser&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time python -c &quot;from Bio.SeqIO.FastaIO import SimpleFastaParser; print(sum(seq.count('A') for title, seq in SimpleFastaParser(open('genome.fa'))))&quot;&#xA;32371810&#xA;&#xA;real    0m1.618s&#xA;user    0m1.500s&#xA;sys     0m0.116s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(I did a series of timings and tried to take one that seemed representative, but there's a lot of overlap with the higher-level parser's timings.)&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;scikit-bio&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I read about &lt;a href=&quot;http://scikit-bio.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;scikit-bio&lt;/a&gt; today (23/06/2017), which has a sequence reader.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time python3 -c  &quot;import skbio; print(sum(seq.count('A') for seq in skbio.io.read('genome.fa', format='fasta', verify=False)))&quot;&#xA;32371810&#xA;&#xA;real    0m3.643s&#xA;user    0m3.440s&#xA;sys     0m1.228s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Benchmarks on a fastq.gz file&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I tested some of the above solutions (or adaptations thereof), counting &quot;N&quot; in the same file that was used here: &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/400/292&quot;&gt;https://bioinformatics.stackexchange.com/a/400/292&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;bioawk&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time bioawk -c fastx '{n+=gsub(/N/, &quot;&quot;, $seq)} END {print n}' SRR077487_2.filt.fastq.gz&#xA;306072&#xA;&#xA;real    1m9.686s&#xA;user    1m9.376s&#xA;sys     0m0.304s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3&gt;pigz + readfq python module&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;readfq doesn't complain and is very fast when I pass directly the compressed fastq, but returns something wrong, so don't forget to manually take care of the decompression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here I tried with &lt;code&gt;pigz&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time pigz -dc SRR077487_2.filt.fastq.gz | python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('N') for _, seq, _ in readfq(stdin)))&quot;&#xA;306072&#xA;&#xA;real    0m52.347s&#xA;user    1m40.716s&#xA;sys     0m8.604s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The computer has 16 cores, but I suspect the limiting factor for &lt;code&gt;pigz&lt;/code&gt; is reading from the disk: the processors are very far from running full speed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And with &lt;code&gt;gzip&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time gzip -dc SRR077487_2.filt.fastq.gz | python -c &quot;from sys import stdin; from readfq import readfq; print sum((seq.count('N') for _, seq, _ in readfq(stdin)))&quot;&#xA;306072&#xA;&#xA;real    0m49.448s&#xA;user    1m31.984s&#xA;sys     0m2.312s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here gzip and python both used a full processor. Resource usage balance was slightly better. I work on a desktop computer. I suppose a computing server would take advantage of &lt;code&gt;pigz&lt;/code&gt; better.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;pyGATB&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time python3 -c &quot;from gatb import Bank; print(sum((seq.sequence.count(b\&quot;N\&quot;) for seq in Bank(\&quot;SRR077487_2.filt.fastq.gz\&quot;))))&quot;&#xA;306072&#xA;&#xA;real    0m46.784s&#xA;user    0m46.404s&#xA;sys     0m0.296s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3&gt;biopython&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time python -c &quot;from gzip import open as gzopen; from Bio.SeqIO.QualityIO import FastqGeneralIterator; print(sum(seq.count('N') for (_, seq, _) in FastqGeneralIterator(gzopen('SRR077487_2.filt.fastq.gz'))))&quot;&#xA;306072&#xA;&#xA;real    3m18.103s&#xA;user    3m17.676s&#xA;sys     0m0.428s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Conclusion&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;pyGATB and bioawk both handle transparently compression (gzipped or not) and format differences (fasta or fastq). pyGATB is quite a new tool, but seems more efficient compared to the other python modules I tested.&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-06-23T13:39:36.970" LastActivityDate="2017-06-23T13:39:36.970" CommentCount="4" />
  <row Id="382" PostTypeId="2" ParentId="150" CreationDate="2017-06-02T09:35:00.113" Score="3" Body="&lt;p&gt;Here is a bash script and GNU &lt;code&gt;parallel&lt;/code&gt; along with some other tools to split-apply-combine the data. It may or may not work out of the box, but it's not far off and it should give the general idea of how one way of how to approach this.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/bin/bash&#xA;&#xA;# ====== USER PROVIDED PARAMETERS ========&#xA;N=20 # number of cores to use&#xA;ngsfilter=&quot;rawdata_scandinavia.ngsfilter&quot;&#xA;unidentified=&quot;unidentified_samples.fastq&quot;&#xA;&#xA;R1=&quot;rawdata_scandinavia_R1.fastq&quot;&#xA;R2=&quot;rawdata_scandinavia_R2.fastq&quot;&#xA;# ==== END USER PROVIDED PARAMETERS =====&#xA;&#xA;# fastqutils available @ http://ngsutils.org/modules/fastqutils&#xA;fastqutils split $R1 to_illumina_R1_ $N &amp;amp;&#xA;fastqutils split $R2 to_illumina_R2_ $N &amp;amp;&#xA;wait&#xA;&#xA;R1=$( ls | grep -P &quot;to_illumina_R1_&quot; )&#xA;R2=$( ls | grep -P &quot;to_illumina_R2_&quot; )&#xA;&#xA;# Do alignment of two reads.&#xA;for i in $( seq $N )&#xA;do&#xA;    partR1=$( ls | grep -P &quot;^to_illumina_R1_\\.$i\\.fastq$&quot; )&#xA;    partR2=$( ls | grep -P &quot;^to_illumina_R2_\\.$i\\.fastq$&quot; )&#xA;    illuminapairedend -r $partR1 $partR2 | tee to_ngsfilter_$i.fastq | obiannotate -S goodAli:'&quot;Align&quot; if score&amp;gt;40.00 else &quot;Bad&quot;' | obisplit -t goodAli -p to_ngsfilter_$i. &amp;amp;&#xA;done&#xA;&#xA;# Remove intermediate aligned files.&#xA;echo $R1 | xargs rm -r&#xA;echo $R2 | xargs rm -r&#xA;ls | grep -P &quot;\\.seq$|\\.err$&quot; | xargs rm -r&#xA;&#xA;input=$( ls | grep -P &quot;to_ngsfilter_[0-9]+\\.Align\\.fastq&quot; )&#xA;&#xA;# For some reasons, files need to be &quot;touched&quot; in order to work. Perhaps&#xA;# it's not closing them...&#xA;touch $unidentified&#xA;&#xA;touchedbyangel=$( ls | grep -P &quot;to_ngsfilter_&quot;  )&#xA;for i in $touchedbyangel&#xA;do&#xA;    touch $i&#xA;done&#xA;&#xA;parallel -j$N --result filtered_{} ngsfilter -t $ngsfilter -u $unidentified {} ::: $input&#xA;&#xA;# Remove intermediate results.&#xA;ls | grep -P &quot;\\.seq$|\\.err$&quot; | xargs rm -r&#xA;&#xA;ls | grep -P &quot;filtered_.*\\.fastq$&quot; | xargs cat &amp;gt; filtered_data.fastq&#xA;&#xA;# remove intermediate results to conserve space&#xA;rm to_ngsfilter*&#xA;&#xA;# Split reads by locus.&#xA;filtered=$( ls | grep -P &quot;^filtered_&quot;  )&#xA;parallel -j$N --result to_split_{} obisplit -p MS.PCR_ -t experiment {} ::: $filtered&#xA;&#xA;ls | grep -P &quot;\\.err$|\\.seq$&quot; | xargs rm -r&#xA;ls | grep -P &quot;^to_split_&quot; | xargs rm -r&#xA;&#xA;# Wasn't able to parallelize this, so it's just pushing tasks into the background.&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_03.fastq &amp;gt; MS.PCR_UA_MxRout1_03.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_06.fastq &amp;gt; MS.PCR_UA_MxRout1_06.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_14.fastq &amp;gt; MS.PCR_UA_MxRout1_14.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_16.fastq &amp;gt; MS.PCR_UA_MxRout1_16.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_17.fastq &amp;gt; MS.PCR_UA_MxRout1_17.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_25.fastq &amp;gt; MS.PCR_UA_MxRout1_25.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_51.fastq &amp;gt; MS.PCR_UA_MxRout1_51.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_57.fastq &amp;gt; MS.PCR_UA_MxRout1_57.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_63.fastq &amp;gt; MS.PCR_UA_MxRout1_63.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_64.fastq &amp;gt; MS.PCR_UA_MxRout1_64.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_65.fastq &amp;gt; MS.PCR_UA_MxRout1_65.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_67.fastq &amp;gt; MS.PCR_UA_MxRout1_67.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_68.fastq &amp;gt; MS.PCR_UA_MxRout1_68.uniq.fasta &amp;amp;&#xA;obiuniq -m sample MS.PCR_UA_MxRout1_ZF.fastq &amp;gt; MS.PCR_UA_MxRout1_ZF.uniq.fasta &amp;amp;&#xA;wait&#xA;&#xA;uniqfiles=$( ls | grep -P &quot;.{2}+\\.uniq\\.fasta$&quot; )&#xA;&#xA;for i in $uniqfiles&#xA;do&#xA;    # this section removes the file suffix&#xA;    # https://stackoverflow.com/questions/125281/how-do-i-remove-the-file-suffix-and-path-portion-from-a-path-string-in-bash&#xA;    tab=${i%.fasta}&#xA;    tab=${tab##*/}&#xA;    obigrep -p 'count&amp;gt;1' $i | obiannotate -k merged_sample -k count | obiannotate --length | obisort -r -k seq_length | obitab --no-definition --output-seq &amp;gt; $tab.tab&#xA;done&#xA;&#xA;ls | grep -P &quot;\\.uniq\\.fasta&quot; | xargs rm -r&#xA;&#xA;echo &quot;Done. I think.&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="200" LastEditorUserId="292" LastEditDate="2017-06-02T13:18:13.977" LastActivityDate="2017-06-02T13:18:13.977" CommentCount="0" />
  <row Id="383" PostTypeId="2" ParentId="378" CreationDate="2017-06-02T10:59:48.487" Score="3" Body="&lt;p&gt;There are certainly software libraries for working with HMMs. For a general-purpose implementation in C++, take a look at the &lt;a href=&quot;http://docs.seqan.de/seqan/2.1.1/group_HmmAlgorithms.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;SeqAn HMM algorithms&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For your purposes, i.e. “computing … the most likely hidden sequence given the sequence of observed states”, you’d invoke &lt;code&gt;viterbiAlgorithm&lt;/code&gt; with your observed sequence and the HMM graph.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More fundamentally I think that most existing, mature implementations are probably found in the domain of signal processing, which has been using them longer than biology, and where most of the underlying theory was developed.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-06-02T10:59:48.487" CommentCount="0" />
  <row Id="384" PostTypeId="2" ParentId="297" CreationDate="2017-06-02T11:41:15.883" Score="1" Body="&lt;p&gt;The &lt;code&gt;minion&lt;/code&gt; utility from the kraken/reaper toolkit may be helpful for this: &lt;a href=&quot;http://wwwdev.ebi.ac.uk/enright-dev/kraken/reaper/src/reaper-latest/doc/minion.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://wwwdev.ebi.ac.uk/enright-dev/kraken/reaper/src/reaper-latest/doc/minion.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="306" LastActivityDate="2017-06-02T11:41:15.883" CommentCount="1" />
  <row Id="385" PostTypeId="2" ParentId="14" CreationDate="2017-06-02T12:16:24.783" Score="9" Body="&lt;p&gt;Let’s start with what they have in common: All three formats store&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;sequence data, and&lt;/li&gt;&#xA;&lt;li&gt;sequence metadata.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Furthermore, all three formats are text-based.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, beyond that all three formats are different and serve different purposes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let’s start with the simplest format:&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;FASTA&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;FASTA stores a variable number of sequence records, and for each record it stores the sequence itself, and a sequence ID. Each record starts with a header line whose first character is &lt;code&gt;&amp;gt;&lt;/code&gt;, followed by the sequence ID. The next lines of a record contain the actual sequence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTA_format&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikipedia artice&lt;/a&gt; gives several examples for peptide sequences, but since FASTQ and SAM are used exclusively (?) for nucleotide sequences, here’s a nucleotide example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;Mus_musculus_tRNA-Ala-AGC-1-1 (chr13.trna34-AlaAGC)&#xA;GGGGGTGTAGCTCAGTGGTAGAGCGCGTGCTTAGCATGCACGAGGcCCTGGGTTCGATCC&#xA;CCAGCACCTCCA&#xA;&amp;gt;Mus_musculus_tRNA-Ala-AGC-10-1 (chr13.trna457-AlaAGC)&#xA;GGGGGATTAGCTCAAATGGTAGAGCGCTCGCTTAGCATGCAAGAGGtAGTGGGATCGATG&#xA;CCCACATCCTCCA&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The ID can be in any arbitrary format, although &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTA_format#Sequence_identifiers&quot; rel=&quot;nofollow noreferrer&quot;&gt;several conventions exist&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the context of nucleotide sequences, FASTA is mostly used to store reference data; that is, data extracted from a curated database; the above is adapted from &lt;a href=&quot;http://gtrnadb.ucsc.edu/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GtRNAdb&lt;/a&gt; (a database of tRNA sequences).&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;FASTQ&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;FASTQ was conceived to solve a specific problem of FASTA files: when sequencing, the confidence in a given &lt;a href=&quot;https://biology.stackexchange.com/a/1873/166&quot;&gt;base call&lt;/a&gt; (that is, the identity of a nucleotide) varies. This is expressed in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Phred_quality_score&quot; rel=&quot;nofollow noreferrer&quot;&gt;Phred quality score&lt;/a&gt;. FASTA had no standardised way of encoding this. By contrast, a FASTQ record &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format#Encoding&quot; rel=&quot;nofollow noreferrer&quot;&gt;contains a sequence of quality scores&lt;/a&gt; for each nucleotide.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A FASTQ record has the following format:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;A line starting with &lt;code&gt;@&lt;/code&gt;, containing the sequence ID.&lt;/li&gt;&#xA;&lt;li&gt;One or more lines that contain the sequence.&lt;/li&gt;&#xA;&lt;li&gt;A new line starting with the character &lt;code&gt;+&lt;/code&gt;, and being either empty or repeating the sequence ID.&lt;/li&gt;&#xA;&lt;li&gt;One or more lines that contain the quality scores.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Here’s an example of a FASTQ file with two records:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@071112_SLXA-EAS1_s_7:5:1:817:345&#xA;GGGTGATGGCCGCTGCCGATGGCGTC&#xA;AAATCCCACC&#xA;+&#xA;IIIIIIIIIIIIIIIIIIIIIIIIII&#xA;IIII9IG9IC&#xA;@071112_SLXA-EAS1_s_7:5:1:801:338&#xA;GTTCAGGGATACGACGTTTGTATTTTAAGAATCTGA&#xA;+&#xA;IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII6IBI&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;FASTQ files are mostly used to store short-read data from high-throughput sequencing experiments. As a consequence, the sequence and quality scores are usually put into a single line each, and indeed many tools assume that each record in a FASTQ file is exactly four lines long, even though this isn’t guaranteed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for FASTA, the format of the sequence ID isn’t standardised, but different producers of FASTQ use &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format#Illumina_sequence_identifiers&quot; rel=&quot;nofollow noreferrer&quot;&gt;fixed notations that follow strict conventions&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;SAM&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;SAM files are so complex that a &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMv1.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;complete description&lt;/a&gt; &lt;sup&gt;[PDF]&lt;/sup&gt; takes 15 pages. So here’s the short version.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The original purpose of SAM files is to store mapping information for sequences from high-throughput sequencing. As a consequence, a SAM record needs to store more than just the sequence and its quality, it also needs to store information about where and how a sequence maps into the reference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unlike the previous formats, SAM is tab-based, and each record, consisting of either 11 or 12 fields, fills exactly one line. Here’s an example (tabs replaced by fixed-width spacing):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;r001  99  chr1  7 30  17M         =  37  39  TTAGATAAAGGATACTG   IIIIIIIIIIIIIIIII&#xA;r002  0   chrX  9 30  3S6M1P1I4M  *  0   0   AAAAGATAAGGATA      IIIIIIIIII6IBI    NM:i:1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For a description of the individual fields, refer to the documentation. The relevant bit is this: SAM can express exactly the same information as FASTQ, plus, as mentioned, the mapping information. However, SAM is also used to store read data &lt;em&gt;without&lt;/em&gt; mapping information.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition to sequence records, SAM files can also contain a &lt;em&gt;header&lt;/em&gt;, which stores information about the reference that the sequences were mapped to, and the tool used to create the SAM file. Header information precede the sequence records, and consist of lines starting with &lt;code&gt;@&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SAM itself is almost never used as a storage format; instead, files are stored in BAM format, which is a compact binary representation of SAM. It stores the same information, just more efficiently, and in conjunction with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Database_index&quot; rel=&quot;nofollow noreferrer&quot;&gt;search index&lt;/a&gt;, allows fast retrieval of individual records from the middle of the file (= fast &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_access&quot; rel=&quot;nofollow noreferrer&quot;&gt;random access&lt;/a&gt;). BAM files are also much more compact than compressed FASTQ or FASTA files.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;The above implies a &lt;em&gt;hierarchy&lt;/em&gt; in what the formats can store: FASTA ⊂ FASTQ ⊂ SAM.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a typical high-throughput analysis workflow, you will encounter all three file types:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;FASTA to store the reference genome/transcriptome that the sequence fragments will be mapped to.&lt;/li&gt;&#xA;&lt;li&gt;FASTQ to store the sequence fragments before mapping.&lt;/li&gt;&#xA;&lt;li&gt;SAM/BAM to store the sequence fragments after mapping.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-08-17T14:05:01.363" LastActivityDate="2017-08-17T14:05:01.363" CommentCount="3" />
  <row Id="386" PostTypeId="2" ParentId="378" CreationDate="2017-06-02T13:16:00.703" Score="5" Body="&lt;p&gt;I would also recommend to take a look at &lt;a href=&quot;https://pomegranate.readthedocs.io/en/latest/&quot; rel=&quot;noreferrer&quot;&gt;pomegranate&lt;/a&gt;, a nice Python package for probabilistic graphical models. It includes solvers for HMMs and much more. Under the hood it uses Cythonised code, so it's also quite fast.&lt;/p&gt;&#xA;" OwnerUserId="81" LastActivityDate="2017-06-02T13:16:00.703" CommentCount="0" />
  <row Id="388" PostTypeId="1" AcceptedAnswerId="393" CreationDate="2017-06-02T14:08:49.563" Score="9" ViewCount="115" Body="&lt;p&gt;I want some templates of different file formats that I can use to test my scripts and identify possible bugs in my code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, consider FASTA, a simple but often abused format, I would want templates to capture regular and irregular formats, like I have seen all of these:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Single line sequence&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;1&#xA;ATG&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;2) Multi-line sequence&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;1&#xA;AT&#xA;G&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;3) Upper and lower case letters in sequence&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;1&#xA;Atg&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;4) Ns and Xs in sequence&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;1&#xA;ANnxX&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;5) Unusual headers (sometimes even non-ASCI characters in headers)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;ATG | &amp;gt;Name @species | (MULTI_SPECIES)[taxa]&#xA;ATG&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;6) Whitespace between records&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;1&#xA;ATG&#xA;&#xA;&amp;gt;2&#xA;ATG&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;7) Duplicated headers&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;1&#xA;ATG&#xA;&amp;gt;1&#xA;ATC&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There should be separate templates for nucleotide and protein FASTA, and separate ones for aligned FASTA.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would ideally include other aspects too, like different compression formats (such as &lt;code&gt;.gz&lt;/code&gt;, &lt;code&gt;.bzip2&lt;/code&gt;) and different file extensions (such as &lt;code&gt;.fa&lt;/code&gt;, &lt;code&gt;.fasta&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have never seen resources that provide templates covering these, but I think it would be useful. Of course I could build my own templates but it would take time to capture all the likely variations of the formats, particularly for more complex file formats.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note, I am not just interested in FASTA format, it was an example.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also note, I know about tools (such as &lt;code&gt;BioPython&lt;/code&gt;) that should handle many formats well, but they may also have bugs. Anyway, in practice sometimes I end up parsing files myself directly because I don't want the overhead or dependency of an external package.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;EDIT: Please don't answer this question to say you don't know of any such resources, me neither, hence the question. &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/393/104&quot;&gt;bli's helpful answer&lt;/a&gt; shows that there is at least one test suite that could be used as a starting point. I know that it is normally easy to look up the specification of any particular file format.&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="104" LastEditorUserId="96" LastEditDate="2017-06-08T06:42:47.810" LastActivityDate="2017-06-08T06:42:47.810" Title="Are there any databases of templates for common bioinformatic file formats?" Tags="&lt;fasta&gt;&lt;database&gt;&lt;file-formats&gt;" AnswerCount="5" CommentCount="5" FavoriteCount="0" />
  <row Id="389" PostTypeId="2" ParentId="308" CreationDate="2017-06-02T14:20:08.550" Score="2" Body="&lt;p&gt;I have used smalt a couple of times. Smalt is more flexible than bwa/bowtie and can be tuned to be more sensitive to divergent hits, which is useful to certain applications. I heard Sanger did evaluate a few mappers for some non-typical applications (not sure what). They found smalt to perform better. Also smalt was developed at Sanger. I guess they could modify smalt based on their needs, though I don't know the details.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-02T14:20:08.550" CommentCount="1" />
  <row Id="390" PostTypeId="2" ParentId="375" CreationDate="2017-06-02T14:20:35.313" Score="2" Body="&lt;p&gt;It could be an idea to fragment the long reads into small sequences, like simulating Illumina reads of 150 bp, and then map these small sequences against the original long reads and extract regions with a high coverage? &lt;/p&gt;&#xA;" OwnerUserId="446" LastActivityDate="2017-06-02T14:20:35.313" CommentCount="0" />
  <row Id="391" PostTypeId="2" ParentId="388" CreationDate="2017-06-02T14:24:46.343" Score="0" Body="&lt;p&gt;These may not be exactly what your are looking for, but they do contain a wide range of formats with examples. If you want more, you might try searching other major databases hosting other types of into e.g. Uniprot, PDB, NCBI. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://genome.ucsc.edu/FAQ/FAQformat.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://genome.ucsc.edu/FAQ/FAQformat.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.ensembl.org/info/website/upload/bed.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.ensembl.org/info/website/upload/bed.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="269" LastActivityDate="2017-06-02T14:24:46.343" CommentCount="1" />
  <row Id="392" PostTypeId="2" ParentId="388" CreationDate="2017-06-02T14:24:55.750" Score="1" Body="&lt;p&gt;No. At least none that I've heard of and I doubt there ever will be. There is no central repository for formats and each tool, community, field etc have their own. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The best you can do is look up the official standard for each format and hope they include examples. Having a truly comprehensive collection of all possible variations is basically impossible for &lt;del&gt;horrible&lt;/del&gt; complex formats like VCF. Just consider the simple fact that it allows user-defined &lt;code&gt;INFO&lt;/code&gt; fields with arbitrary contents. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the best you can do is make sure your scripts conform with the standard and hope your input does as well. &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-06-02T14:24:55.750" CommentCount="4" />
  <row Id="393" PostTypeId="2" ParentId="388" CreationDate="2017-06-02T16:18:48.537" Score="4" Body="&lt;p&gt;You mention Biopython, which contains tests: &lt;a href=&quot;https://github.com/biopython/biopython/tree/master/Tests&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/biopython/biopython/tree/master/Tests&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of the tests consist in reading files present in the folders listed in the above link. These files could be a starting point for a database of test files. Whenever one comes across a test case not covered with these files, one could construct a new test file and contribute it to Biopython, along with a test, or at least file an issue: &lt;a href=&quot;https://github.com/biopython/biopython/issues&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/biopython/biopython/issues&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That would be a way to contribute to Biopython while constituting a database of test files.&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-06-02T16:18:48.537" CommentCount="2" />
  <row Id="394" PostTypeId="2" ParentId="388" CreationDate="2017-06-02T17:08:55.260" Score="1" Body="&lt;p&gt;As far as I know, there is no single repository that collects all of the common data formats used in bioinformatics.  Typically, you have to go to the source to find the specifications for each format.  There are a few places that collect descriptions of file formats, though:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://software.broadinstitute.org/software/igv/FileFormats&quot; rel=&quot;nofollow noreferrer&quot;&gt;IGV File Formats&lt;/a&gt;, coveres all of the formats usable in the Broad Institute's Integrative Genomics Viewer software (which is a lot).&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://wiki.nci.nih.gov/display/TCGA/File+Format+Specifications&quot; rel=&quot;nofollow noreferrer&quot;&gt;NCI File Formats&lt;/a&gt;, mostly formats used by TGCA (including MAF and VCF).&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://cgwb.nci.nih.gov/FAQ/FAQformat.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;UCSC Genomics&lt;/a&gt;, covers BED, MAF, and a few others.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://software.broadinstitute.org/cancer/software/genepattern/file-formats-guide&quot; rel=&quot;nofollow noreferrer&quot;&gt;GenePattern&lt;/a&gt;, covers many of the file formats related to microarray data.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://software.broadinstitute.org/cancer/software/gsea/wiki/index.php/Data_formats&quot; rel=&quot;nofollow noreferrer&quot;&gt;GSEA&lt;/a&gt;, Broad Gene Set Enrichment Analysis documentation.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="47" LastActivityDate="2017-06-02T17:08:55.260" CommentCount="0" />
  <row Id="395" PostTypeId="1" AcceptedAnswerId="399" CreationDate="2017-06-02T19:22:34.750" Score="3" ViewCount="48" Body="&lt;p&gt;In the analyses of single-cell RNA-seq data there are different unsupervised approaches to identify putative subpopulations (e.g. as available with Suerat or SCDE packages). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a good way of computationally validating the cluster solutions? Different methods may results in slightly different clustering results. How to know which one is the best i.e. representative of biological sub-populations?&lt;/p&gt;&#xA;" OwnerUserId="467" LastEditorUserId="57" LastEditDate="2017-06-02T19:37:06.107" LastActivityDate="2017-06-14T01:38:01.883" Title="validating identified sub-populations of cells in scRNA-seq" Tags="&lt;scrnaseq&gt;&lt;clustering&gt;&lt;unsupervised&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="396" PostTypeId="1" AcceptedAnswerId="398" CreationDate="2017-06-02T19:36:07.733" Score="6" ViewCount="56" Body="&lt;p&gt;I have indexed a gzipped reference with bwa: &lt;code&gt;bwa index reference.fa.gz&lt;/code&gt;, which produces a series of other files &lt;code&gt;reference.fa.gz.{amb,ann,bwt,pac,sa}&lt;/code&gt;. These are working fine with bwa alignment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have discovered that samtools does not take a gzipped reference, so I am planning to use an unzipped version of the reference for my workflow instead of dealing with two separate representations of the reference. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are the files &lt;code&gt;reference.fa.gz.{amb,ann,bwt,pac,sa}&lt;/code&gt; dependent upon the reference being gzipped? Do I need to reindex the unzipped reference.fa with bwa, or can I just rename the current files to remove the &lt;code&gt;.gz&lt;/code&gt; portion of the filename? &lt;/p&gt;&#xA;" OwnerUserId="272" LastEditorUserId="37" LastEditDate="2017-06-25T02:11:15.843" LastActivityDate="2017-06-25T02:11:15.843" Title="What are all the reference files produced by bwa index, and are these dependent upon whether the reference is zipped?" Tags="&lt;bwa&gt;&lt;samtools&gt;&lt;reference-genome&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="397" PostTypeId="2" ParentId="388" CreationDate="2017-06-02T19:44:00.670" Score="4" Body="&lt;p&gt;Not that I am aware. It is best to go with format specifications when coding.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also it may be good to look at the example files that come together with various tools performing file conversions and handling. E.g. &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Trimmomatic comes with few .fa files (&lt;a href=&quot;http://www.usadellab.org/cms/?page=trimmomatic&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.usadellab.org/cms/?page=trimmomatic&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Samtools with a toy.sam (&lt;a href=&quot;https://github.com/lh3/samtools-legacy/blob/master/examples/toy.sam&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/lh3/samtools-legacy/blob/master/examples/toy.sam&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="375" LastEditorUserId="77" LastEditDate="2017-06-02T19:58:28.837" LastActivityDate="2017-06-02T19:58:28.837" CommentCount="0" />
  <row Id="398" PostTypeId="2" ParentId="396" CreationDate="2017-06-02T19:46:16.340" Score="6" Body="&lt;p&gt;You'll get the exact same index (the &lt;code&gt;amb&lt;/code&gt;, &lt;code&gt;ann&lt;/code&gt;, &lt;code&gt;bwt&lt;/code&gt;, &lt;code&gt;pac&lt;/code&gt; and &lt;code&gt;sa&lt;/code&gt; files) whether the reference is gzipped or not. BWA also makes its own packed reference sequence (the .pac file) so you don't even need the genome around after you index.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-02T19:46:16.340" CommentCount="0" />
  <row Id="399" PostTypeId="2" ParentId="395" CreationDate="2017-06-02T19:50:21.880" Score="5" Body="&lt;p&gt;A SC3, single-cell consensus clustering, approach could be helpful here. It aims at achieving &quot;high accuracy and robustness by combining multiple clustering solutions through a consensus approach&quot; &lt;a href=&quot;https://www.nature.com/nmeth/journal/v14/n5/full/nmeth.4236.html&quot; rel=&quot;noreferrer&quot;&gt;https://www.nature.com/nmeth/journal/v14/n5/full/nmeth.4236.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="375" LastActivityDate="2017-06-02T19:50:21.880" CommentCount="0" />
  <row Id="400" PostTypeId="2" ParentId="361" CreationDate="2017-06-02T20:28:26.517" Score="5" Body="&lt;h3&gt;Decompression of gzipped FASTQ is the main issue&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If we take real world gzipped FASTQ (which as the OP suggested would be beneficial) rather than trivial FASTA as the starting point then the real issue is actually decompressing the file not counting the Ns and in this case the C program &lt;code&gt;count-N&lt;/code&gt; is no longer the fastest solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally it would be good to use a specific file for benchmarking which actually has Ns, because you'll get some quite interesting execution time differences with some methods counting the more frequently occurring As rather than Ns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One such file is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/SRR077487_2.filt.fastq.gz&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/SRR077487_2.filt.fastq.gz&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also worth checking the various solutions return the correct answer there should be 306072 Ns in the above file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next note that decompression of this file redirected to &lt;code&gt;/dev/null&lt;/code&gt; is slower with &lt;code&gt;zcat&lt;/code&gt; and &lt;code&gt;gzip&lt;/code&gt; (which are both gzip 1.6 on my system) than say a parallel implementation of gzip like &lt;a href=&quot;http://zlib.net/pigz/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mark Adler's&lt;/a&gt; &lt;code&gt;pigz&lt;/code&gt;, which appears to use 4 threads for decompression.  All timings represent an average of 10 runs reporting real (wall clock time).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time pigz -dc SRR077487_2.filt.fastq.gz &amp;gt; /dev/null&#xA;&#xA;real    0m29.0132s&#xA;&#xA;time gzip -dc SRR077487_2.filt.fastq.gz &amp;gt; /dev/null&#xA;&#xA;real    0m40.6996s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There is an ~11.7 second difference between the two.  Next if I then try to benchmark a one-liner which performs on FASTQ and gives the correct answer (Note I've yet to encounter FASTQ which is not 4 line, and seriously who generates these files!)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time pigz -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | tr -cd N | wc -c&#xA;306072&#xA;&#xA;real    0m34.793s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see the counting adds ~5.8 second to the total run time versus &lt;code&gt;pigz&lt;/code&gt; based decompression.  Additionally this time delta is higher when using &lt;code&gt;gzip&lt;/code&gt; ~6.7 seconds above gzip decompression alone.   &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time gzip -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | tr -cd N | wc -c                                                                &#xA;306072&#xA;&#xA;real    0m44.399s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;pigz awk tr wc&lt;/code&gt; based solution is however ~4.5 seconds faster than the &lt;code&gt;count-N&lt;/code&gt; based C code solution:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time count-N SRR077487_2.filt.fastq.gz &#xA;2385855128      306072  0&#xA;&#xA;real    0m39.266s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This difference appears to be robust to re-running as many times as I like.  I expect if you could use pthread in the C based solution or alter it to take the standard out from pigz it would also show an increase in performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Benchmarking another alternative &lt;code&gt;pigz grep&lt;/code&gt; variant appears to take more or less the same time as the &lt;code&gt;tr&lt;/code&gt; based variant:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time pigz -dc SRR077487_2.filt.fastq.gz | awk 'NR%4==2{print $1}' | grep -o N | wc -l&#xA;306072&#xA;&#xA;real    0m34.869s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note that the &lt;code&gt;seqtk&lt;/code&gt; based solution discussed above is noticeably slower,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time seqtk comp  SRR077487_2.filt.fastq.gz | awk '{x+=$9}END{print x}'&#xA;306072&#xA;&#xA;real    1m42.062s &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However it's worth noting &lt;code&gt;seqtk comp&lt;/code&gt; is doing a bit more than the other solutions.&lt;/p&gt;&#xA;" OwnerUserId="532" LastEditorUserId="532" LastEditDate="2017-06-02T20:57:56.203" LastActivityDate="2017-06-02T20:57:56.203" CommentCount="0" />
  <row Id="401" PostTypeId="1" CreationDate="2017-06-02T23:38:13.547" Score="5" ViewCount="62" Body="&lt;p&gt;I have a BAM created by Picard. I want to filter alignments by flags with &lt;code&gt;samtools view&lt;/code&gt;. However, I noticed that even if I apply no filters, the output BAM is different from my input BAM. Are BAMs produced by different tools also different in size? How can I check if two BAMs are the same?&lt;/p&gt;&#xA;" OwnerUserId="549" LastEditorUserId="73" LastEditDate="2017-06-03T02:05:48.400" LastActivityDate="2017-06-03T12:14:11.773" Title="Why do BAM files created by different tools have different file sizes?" Tags="&lt;bam&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="402" PostTypeId="1" AcceptedAnswerId="406" CreationDate="2017-06-02T23:44:44.637" Score="5" ViewCount="36" Body="&lt;p&gt;I know how to downsample a BAM file to lower coverage. I know I can randomly select lines in SAM, but this procedure can't guarantee two reads in a pair are always sampled the same time. Is there a way to downsample BAM while keeping pairing information intact?&lt;/p&gt;&#xA;" OwnerUserId="549" LastActivityDate="2017-06-03T01:08:48.127" Title="How can I downsample a BAM file while keeping both reads in pairs?" Tags="&lt;bam&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="403" PostTypeId="1" CreationDate="2017-06-02T23:50:11.167" Score="4" ViewCount="52" Body="&lt;p&gt;After some google searches, I found multiple tools with overlapping functionality for viewing, merging, pileuping, etc. I have not got time to try these tools, so will just see if anyone already know the answer: what is the difference between them? Performance? Features? Or something else? Which one is generally preferred? Samtools?&lt;/p&gt;&#xA;" OwnerUserId="549" LastActivityDate="2017-06-03T01:50:55.283" Title="What is the difference between samtools, bamtools, picard, sambamba and biobambam?" Tags="&lt;bam&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="0" />
  <row Id="404" PostTypeId="1" AcceptedAnswerId="421" CreationDate="2017-06-02T23:58:10.703" Score="5" ViewCount="55" Body="&lt;p&gt;As I read the SAM spec, the &quot;X&quot; CIGAR operator represents a mismatch. This seems useful as we can know where are the mismatches without looking at the reference genome. However, many popular aligners such as BWA do not output &quot;X&quot;. Why do they omit &quot;X&quot;?&lt;/p&gt;&#xA;" OwnerUserId="549" LastActivityDate="2017-06-08T09:10:32.403" Title="Why most aligners do not output the &quot;X&quot; CIGAR operation?" Tags="&lt;bwa&gt;&lt;sam&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="405" PostTypeId="2" ParentId="401" CreationDate="2017-06-03T00:20:02.360" Score="3" Body="&lt;p&gt;It's unlikely that two different mapping tools will give exactly the same alignment, scores, and match strings for the same sequence mapped to the same reference. For some sequence/reference alignments, it's impossible to determine which is the &quot;best&quot; alignment, and slight differences in code can have large effects on the chosen alignment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, even if the actual mapping location and match string are exactly the same (for example when using a tool like Picard to filter BAM/SAM files), different tools will incorporate different metadata with each mapping. This is allowed in the &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMv1.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;SAM file format specification&lt;/a&gt; by the addition of optional fields beyond the 11th column. There are a few standard optional tags that can be used in these fields, and additional custom non-standard tags can be used as well. It is very likely that Picard is adding additional metadata to the alignments in the BAM/SAM file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's an additional complication in that the underlying &lt;em&gt;SAM&lt;/em&gt; alignment (and metadata) could be identical, but the &lt;em&gt;BAM&lt;/em&gt; file can still have different file sizes. One reason for this is that BAM file compression methods can be changed. For example, alignment tools might choose a quick compression method, while filtering tools might choose a method that results in better compression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Checking for alignment similarity is more difficult than just comparing files at a binary level, and your particular application (or context, or story) will change what the best method of comparison is. It would be helpful to know why you want to compare BAM files in order to provide a better answer to your question.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-03T00:20:02.360" CommentCount="0" />
  <row Id="406" PostTypeId="2" ParentId="402" CreationDate="2017-06-03T01:08:48.127" Score="6" Body="&lt;p&gt;&lt;a href=&quot;http://www.htslib.org/doc/samtools.html&quot; rel=&quot;noreferrer&quot;&gt;samtools&lt;/a&gt; has a subsampling option:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;-s FLOAT: &#xA;  Integer part is used to seed the random number generator [0]. Part after the decimal point sets the fraction of templates/pairs to subsample [no subsampling]&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view -bs 42.1 in.bam &amp;gt; subsampled.bam&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;will subsample 10 percent mapped reads with 42 as the seed for the random number generator.&lt;/p&gt;&#xA;" OwnerUserId="161" LastActivityDate="2017-06-03T01:08:48.127" CommentCount="2" />
  <row Id="407" PostTypeId="1" AcceptedAnswerId="416" CreationDate="2017-06-03T01:22:37.273" Score="3" ViewCount="60" Body="&lt;p&gt;I am calling SNPs from WGS samples produced at my lab. I am currently using bwa-mem for mapping Illumina reads as it is recommended by GATK best practice. However, bwa is a bit slow. I heard from my colleague that &lt;a href=&quot;https://github.com/amplab/snap&quot; rel=&quot;nofollow noreferrer&quot;&gt;SNAP&lt;/a&gt; is much faster than bwa. I tried it on a small set of reads and it is indeed faster. However, I am not sure how it works with downstream SNP callers, so here are my questions: have you used SNAP for short-read mapping? What is your experience? Does SNAP work well with SNP callers like GATK and freebayes? Thanks!&lt;/p&gt;&#xA;" OwnerUserId="549" LastActivityDate="2017-06-04T00:35:38.640" Title="Do you use SNAP for short-read mapping?" Tags="&lt;bwa&gt;&lt;snap&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="408" PostTypeId="2" ParentId="403" CreationDate="2017-06-03T01:50:55.283" Score="7" Body="&lt;p&gt;The obvious answer is that different people wrote them. It's fairly common in bioinformatics for people with a computer science background to get frustrated with existing tools and create their own alternative tool (rather than improving an existing tool). Over time, tools with similar initial aims will have popular functionality implemented in them (and eventually have bugs fixed), such that it matters less which particular tool is used for common methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's my impression of the tools:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/samtools/&quot; rel=&quot;noreferrer&quot;&gt;samtools&lt;/a&gt; -- originally written by Heng Li (who also wrote BWA). The people who now work on samtools also maintain the alignment file format specification for &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMv1.pdf&quot; rel=&quot;noreferrer&quot;&gt;SAM, BAM, and CRAM&lt;/a&gt;, so any new file format features are likely to be implemented in samtools first.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pezmaster31/bamtools&quot; rel=&quot;noreferrer&quot;&gt;bamtools&lt;/a&gt; -- this looks like it was written by Derek Barnett, Erik Garrison, Gabor Marth, Michael Stromberg to mirror the samtools toolkit, but using C++ instead of C&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://broadinstitute.github.io/picard/&quot; rel=&quot;noreferrer&quot;&gt;picard&lt;/a&gt; -- Java tools written by the Broad Institute for manipulating BAM/SAM files. Being written in Java makes it easier to port to other operating systems, so it may work better on Windows systems. I'm more familiar with picard being used at a filtering level (e.g. removing PCR duplicates), and for statistical analysis, but it links in with the Java HTS library from samtools, so probably shares a lot of the functionality.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/lomereiter/sambamba&quot; rel=&quot;noreferrer&quot;&gt;sambamba&lt;/a&gt; -- a GPL2-licensed toolkit written in the D programming language (presumably by Artem Tarasov and Pjotr Prins). I haven't used it (and don't know people who have used it), but the github page suggests &quot;For almost 5 years the main advantage over samtools was parallelized BAM reading. Finally in March 2017 samtools 1.4 was released, reaching parity on this.&quot;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/gt1/biobambam2&quot; rel=&quot;noreferrer&quot;&gt;biobambam&lt;/a&gt; -- written by German Tischler in C++. I also have no experience with this toolkit. This seems to have some multithreading capability, but is otherwise similar to other toolkits.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-03T01:50:55.283" CommentCount="0" />
  <row Id="409" PostTypeId="2" ParentId="327" CreationDate="2017-06-03T09:06:32.320" Score="6" Body="&lt;p&gt;We routinely run both &lt;a href=&quot;http://www.bioinformatics.babraham.ac.uk/projects/fastqc/&quot; rel=&quot;noreferrer&quot;&gt;FastQC&lt;/a&gt; and &lt;a href=&quot;http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/&quot; rel=&quot;noreferrer&quot;&gt;FastQ Screen&lt;/a&gt; on all of our raw sequencing reads. FastQ Screen is a tool for detecting cross-species contamination. &lt;a href=&quot;https://github.com/crukci-bioinformatics/MGA&quot; rel=&quot;noreferrer&quot;&gt;MGA&lt;/a&gt; is another similar tool.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are then lots of QC tools specific to different types of data, most of which run after alignment. For example &lt;a href=&quot;http://rseqc.sourceforge.net/&quot; rel=&quot;noreferrer&quot;&gt;RSeQC&lt;/a&gt; (RNA data), &lt;a href=&quot;http://qualimap.bioinfo.cipf.es/&quot; rel=&quot;noreferrer&quot;&gt;Qualimap&lt;/a&gt; and many many others. Without specifying what kind of data you have this is a bit difficult to make recommendations for though.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Phil&lt;/p&gt;&#xA;" OwnerUserId="224" LastActivityDate="2017-06-03T09:06:32.320" CommentCount="0" />
  <row Id="410" PostTypeId="2" ParentId="283" CreationDate="2017-06-03T11:31:03.630" Score="2" Body="&lt;p&gt;Next to OmicsDI the EBI has a special repository for multi-omics datasets: &lt;a href=&quot;https://www.ebi.ac.uk/biosamples&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ebi.ac.uk/biosamples&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It links the different datasets between repositories, ie. PRIDE for MS/MS based data and ArrayExpress for RNASeq data.&lt;/p&gt;&#xA;" OwnerUserId="476" LastActivityDate="2017-06-03T11:31:03.630" CommentCount="0" />
  <row Id="411" PostTypeId="1" CreationDate="2017-06-03T11:43:30.357" Score="8" ViewCount="103" Body="&lt;p&gt;I'm trying to download three WGS datasets from the SRA that are each between 60 and 100GB in size.  So far I've tried:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fetching the .sra files directly from NCBI's ftp site&lt;/li&gt;&#xA;&lt;li&gt;Fetching the .sra files directly using the aspera command line (&lt;code&gt;ascp&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Using the SRA toolkit's &lt;code&gt;fastqdump&lt;/code&gt; and &lt;code&gt;samdump&lt;/code&gt; tools&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;It's excruciatingly slow.  I've had three &lt;code&gt;fastqdump&lt;/code&gt; processes running in parallel now for approximately 18 hours.  They're running on a large AWS instance in the US east (Virginia) region, which I figure is about as close to NCBI as I can get.  In 18 hours they've downloaded a total of 33GB of data.  By my calculation that's ~500kb/s.  They do appear to still be running - the fastq files continue to grow and their timestamps continue to update.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At this rate it's going to take me days or weeks just to download the datasets.  Surely the SRA must be capable of moving data at higher rates that this?  I've also looked, and unfortunately the datasets I'm interested have not been mirrored out to ENA or the Japanese archive, so it looks like I'm stuck working with the SRA.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a better way to fetch this data that wouldn't take multiple days?&lt;/p&gt;&#xA;" OwnerUserId="556" LastActivityDate="2017-06-06T07:56:48.763" Title="What's the best way to download data from the SRA? Is it really this slow?" Tags="&lt;archive&gt;" AnswerCount="3" CommentCount="3" />
  <row Id="412" PostTypeId="2" ParentId="411" CreationDate="2017-06-03T12:10:48.387" Score="9" Body="&lt;p&gt;Proximity to NCBI may not necessarily give you the fastest transfer speed. AWS may be deliberately throttling the Internet connection to limit the likelihood that people will use it for undesirable things. There's a chance that a home network might be faster, but you're likely to get the fastest connection to NCBI by using an academic system that is linked to NCBI via a research network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another possibility is using Aspera for downloads. This is unlikely to help if bandwidth is being throttled, but it might help if there's a bit of congestion through the regular methods:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/public/&quot; rel=&quot;noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/public/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;NCBI also has an online &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK51062/?report=reader&quot; rel=&quot;noreferrer&quot;&gt;book&lt;/a&gt; about best practises for downloading data from their servers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On a related note, just in case someone sees this and EBI/ENA is an option, there's a great guide for how to do file transfer using Aspera on the EBI web site:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ebi.ac.uk/ena/browse/read-download#downloading_files_aspera&quot; rel=&quot;noreferrer&quot;&gt;https://www.ebi.ac.uk/ena/browse/read-download#downloading_files_aspera&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Your command should look similar to this on Unix:&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ascp -QT -l 300m -i &amp;lt;aspera connect installation directory&amp;gt;/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:&amp;lt;file or files to download&amp;gt; &amp;lt;download location&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In my case, I've just started downloading some files from a MinION sequencing run. The estimated completion time via standard FTP was 12 hours for about 32GB of data; &lt;code&gt;ascp&lt;/code&gt; has reduced that estimated download time to about an hour. Here's the command I used for downloading:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ascp -QT -l 300m -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:/vol1/ERA932/ERA932268/oxfordnanopore_native/20160804_Mock.tar.gz .&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-05T02:20:43.573" LastActivityDate="2017-06-05T02:20:43.573" CommentCount="0" />
  <row Id="413" PostTypeId="2" ParentId="401" CreationDate="2017-06-03T12:14:11.773" Score="5" Body="&lt;p&gt;It's worth bearing in mind that when outputting compressed BAM, as most tools do by default, they may well be using different levels of compression and/or different libraries, or versions of said, libraries for doing (de)compression which will result in different file sizes.  Additionally coordinate sorted BAM will compress more than unsorted BAM. The current version of Picard uses &lt;a href=&quot;https://github.com/samtools/htsjdk&quot; rel=&quot;noreferrer&quot;&gt;HTSJDK&lt;/a&gt; which in turn uses java.util.zip.Deflater/Inflater, current versions of samtools should be using the &lt;a href=&quot;https://github.com/samtools/htslib&quot; rel=&quot;noreferrer&quot;&gt;HTSlib&lt;/a&gt; library which in turn depends on the &lt;a href=&quot;http://zlib.net/&quot; rel=&quot;noreferrer&quot;&gt;standard zlib library&lt;/a&gt;. You can see the effect of different implementations of zlib have on file size and execution time in benchmarking done by the &lt;a href=&quot;http://www.htslib.org/benchmarks/zlib.html&quot; rel=&quot;noreferrer&quot;&gt;samtools team&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in your case the best way to see if there is any difference between the BAM files is to rule out the effect of different levels of compression or libraries used for compression and save both BAM files as uncompressed.  Both samtools and Picard have options for disabling or changing the levels of compression, since the BAM compression standard BGZF is implemented on top of the gzip format it has inherited the ability, just like with gzip, to change the compression level from 0 to 9.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;samtools view -bu&lt;/code&gt; will allow you to produce uncompressed BAM output (which is also handy for piping into other programs as it saves time wasted compressing decompressing what is essentially a stream).  Also note that &lt;code&gt;samtools sort&lt;/code&gt; has a &lt;code&gt;-l INT&lt;/code&gt; setting where INT can be set between 0 (compression off, as with &lt;code&gt;-u&lt;/code&gt;) 1 (for fastest compression, but increased file size) or -9 (for maximal compression, with increased run time).  Some of the effects of increased runtime for higher compression settings might be ameliorated using the &lt;code&gt;-@&lt;/code&gt; argument which allows you to set the number of extra threads used for BAM compression, by default samtools won't use any.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Picard tools has a general setting &lt;code&gt;COMPRESSION_LEVEL&lt;/code&gt; which is applicable to most of its tools setting this to 0, &lt;code&gt;COMPRESSION_LEVEL=0&lt;/code&gt; should disable compression. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So re-running whatever Picard tool you used in the first instance with &lt;code&gt;COMPRESSION_LEVEL=0&lt;/code&gt; will then enable you to check that the file is not further modified by &lt;code&gt;samtools view -bu&lt;/code&gt;. The assumption here being that if both files have exactly the same content they should be the same size uncompressed, of course if they have trivial differences with regard to white space formatting things may still differ.&lt;/p&gt;&#xA;" OwnerUserId="532" LastActivityDate="2017-06-03T12:14:11.773" CommentCount="0" />
  <row Id="414" PostTypeId="1" AcceptedAnswerId="415" CreationDate="2017-06-03T12:42:57.640" Score="4" ViewCount="58" Body="&lt;p&gt;I need to merge sequencing data from different sequencing runs but for the same ChiP-seq library (HiSeq 2000).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any potential advantages or disadvantages when merging files at .fastq or .BAM stage (alignment with Bowite/1.1.2)?&lt;/p&gt;&#xA;" OwnerUserId="375" LastActivityDate="2017-06-06T17:40:13.550" Title="Merging sequencing data for ChIP-seq experiments" Tags="&lt;chip-seq&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="415" PostTypeId="2" ParentId="414" CreationDate="2017-06-03T12:48:05.047" Score="4" Body="&lt;p&gt;I don’t think it matters. Both are easy to merge (BAM via &lt;code&gt;samtools merge&lt;/code&gt;, and (gzipped) FASTQ via &lt;code&gt;cat&lt;/code&gt;), and neither method has specific disadvantages, unless your FASTQ files are sorted for some reason (but they generally shouldn’t be).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One advantage of keeping the FASTQ files separate is that it makes it slightly easier to parallelise the mapping step: just run the mapper in parallel on the separate FASTQ files. Although &lt;code&gt;bowtie&lt;/code&gt; has an option (&lt;code&gt;-p&lt;/code&gt;) for this, throughput from that is slightly worse than running the mapping on split files.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-06-03T12:48:05.047" CommentCount="0" />
  <row Id="416" PostTypeId="2" ParentId="407" CreationDate="2017-06-03T13:07:50.953" Score="5" Body="&lt;p&gt;GATK best practices are explicably meant to consume BWA MEM generated BAM.  Whilst SNAP may be faster, the Broad will not have tested it for compatibility with GATK as such you can't guaranty using it won't have unexpected consequences.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As such you'd be better off using BWA MEM because I assume accurately called variation is always better than fast and incorrectly called variation.  The main issue you'll have is ensuring shorter split hits and mapping quality are reported in the same way as &lt;code&gt;bwa MEM -M&lt;/code&gt; which GATK/Picard is expecting.  Ultimately however you'd be better off posting this question on the &lt;a href=&quot;http://gatkforums.broadinstitute.org/gatk&quot; rel=&quot;nofollow noreferrer&quot;&gt;GATK forum&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also worth noting that the soon to be released GATK 4 will utilise bwaspark which can distribute it's alignment processes across Apache Spark for increase performance.  Consequently I can't see SNAP being adopted anytime soon.&lt;/p&gt;&#xA;" OwnerUserId="532" LastEditorUserId="532" LastEditDate="2017-06-04T00:35:38.640" LastActivityDate="2017-06-04T00:35:38.640" CommentCount="0" />
  <row Id="417" PostTypeId="2" ParentId="328" CreationDate="2017-06-03T13:36:28.887" Score="3" Body="&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Persistent_uniform_resource_locator&quot; rel=&quot;nofollow noreferrer&quot;&gt;Persistent uniform resource locator or PURL&lt;/a&gt; is one such solution, these are designed to be a bit more robust than permalinks in so much as they are supposed to survive the change of domain name.  The bio ontology community already use them &lt;a href=&quot;http://purl.bioontology.org/docs/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://purl.bioontology.org/docs/index.html&lt;/a&gt; &lt;/p&gt;&#xA;" OwnerUserId="532" LastActivityDate="2017-06-03T13:36:28.887" CommentCount="0" />
  <row Id="418" PostTypeId="1" AcceptedAnswerId="433" CreationDate="2017-06-03T13:58:26.980" Score="8" ViewCount="114" Body="&lt;p&gt;The industry standard for aligning short reads seems to be bwa-mem. However, in my tests I have seen that using bwa backtrack (bwa-aln + bwa-sampe + bwa-samse) performs better. It is slightly slower, but gives significantly better results in terms of both sensitivity and specificity. I have tested it using the genome in a bottle data and public samples (NA12878 and NA12877 among others) and found that backtrack consistently outperformed bwa-mem. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So why is bwa-mem the standard? Am I wrong and other tests have shown the opposite? I don't really see how since I tested using the most common datasets and validation data. Is it that the slight increase in efficiency outweighs the decrease in performance? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only other explanation I can see is that bwa backtrack is designed specifically for Illumina reads and all my tests have been on Illumina data. Is it just that bwa-mem is &quot;sequencer agnostic&quot;? So that we can use the same algorithm irrespective of what sequencing platform is used? In that case, it makes sense to use backtrack for if we only deal with Illumina data and mem if we can have different sequencers. But, if so, seeing as Illumina is so widespread, why isn't backtrack used more often on Illumina data? I feel I must be missing something.&lt;/p&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-06-08T09:26:37.033" LastActivityDate="2017-06-08T09:26:37.033" Title="Why is bwa-mem the standard algorithm when using bwa?" Tags="&lt;ngs&gt;&lt;bwa&gt;&lt;read-alignment&gt;" AnswerCount="1" CommentCount="7" />
  <row Id="420" PostTypeId="2" ParentId="351" CreationDate="2017-06-03T16:18:46.493" Score="3" Body="&lt;p&gt;I have previously estimated tumour purity with the &lt;a href=&quot;http://dna-discovery.stanford.edu/software/expands/&quot; rel=&quot;nofollow noreferrer&quot;&gt;EXPANDS&lt;/a&gt; an inferred tumour heterogeneity program which is designed to calculate the number of clonal subpopulations in matched tumour/normal samples.  The purity is essentially the size of the largest subpopulation identified in that sample - this is discussed in the programs &lt;a href=&quot;http://dna-discovery.stanford.edu/software/expands/expands_faq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;FAQ&lt;/a&gt;.  In addition to a matched tumour / normal exome or genome sequencing you'll also need match somatic copy number as a seg file as input.  Other programs also exist for this sort of inferred heterogeneity analysis - some of which may also give you a measure of purity are: &lt;a href=&quot;https://www.nature.com/nbt/journal/v30/n5/full/nbt.2203.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Absolute&lt;/a&gt;, &lt;a href=&quot;https://genomebiology.biomedcentral.com/articles/10.1186/gb-2013-14-7-r80&quot; rel=&quot;nofollow noreferrer&quot;&gt;ThetA&lt;/a&gt;, &lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003665&quot; rel=&quot;nofollow noreferrer&quot;&gt;SciClone&lt;/a&gt;, &lt;a href=&quot;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-014-0473-4&quot; rel=&quot;nofollow noreferrer&quot;&gt;CHAT&lt;/a&gt;, &lt;a href=&quot;https://www.nature.com/nmeth/journal/v11/n4/full/nmeth.2883.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;PyClone&lt;/a&gt; and &lt;a href=&quot;http://www.pnas.org/content/113/37/E5528.full&quot; rel=&quot;nofollow noreferrer&quot;&gt;Canopy&lt;/a&gt;. A more complete list looks to be &lt;a href=&quot;https://omictools.com/tumor-purity-and-heterogeneity-category&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only other thing I'd suggest is have an estimate via some orthogonal measure of the purity to try at least judge how the different methods are performing with you data, &lt;em&gt;e&lt;/em&gt;.&lt;em&gt;g&lt;/em&gt;. you might have clonality indications from &lt;a href=&quot;https://en.wikipedia.org/wiki/Cytogenetics&quot; rel=&quot;nofollow noreferrer&quot;&gt;cytogenetics/FISH&lt;/a&gt; or histology work, or perhaps &lt;a href=&quot;https://en.wikipedia.org/wiki/Flow_cytometry#Fluorescence-activated_cell_sorting_.28FACS.29&quot; rel=&quot;nofollow noreferrer&quot;&gt;FACS&lt;/a&gt;.  Picking samples known to be pure / impure via one or more of these might help you get a handle on how well various methods are performing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As regards estimation without a normal sample the only program which looks like it might help is &lt;a href=&quot;https://github.com/DeveauP/QuantumClone/&quot; rel=&quot;nofollow noreferrer&quot;&gt;QuantumClone&lt;/a&gt;, but that requires more than one tumour sample form said patient either spatially or temporally distinct to perform the analysis. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also If you have low coverage DNA sequencing &lt;a href=&quot;http://www.precancer.leeds.ac.uk/software-and-datasets/cnanorm/&quot; rel=&quot;nofollow noreferrer&quot;&gt;CNAnorm&lt;/a&gt; looks like it could be useful with just a single sample. &lt;/p&gt;&#xA;" OwnerUserId="532" LastActivityDate="2017-06-03T16:18:46.493" CommentCount="0" />
  <row Id="421" PostTypeId="2" ParentId="404" CreationDate="2017-06-03T18:39:37.780" Score="7" Body="&lt;p&gt;The SAM format originally had only &lt;strong&gt;M&lt;/strong&gt;, &lt;strong&gt;I&lt;/strong&gt;, &lt;strong&gt;D&lt;/strong&gt;, &lt;strong&gt;N&lt;/strong&gt;, &lt;strong&gt;S&lt;/strong&gt;, &lt;strong&gt;H&lt;/strong&gt;, and &lt;strong&gt;P&lt;/strong&gt; CIGAR operators.  See the &lt;a href=&quot;https://github.com/samtools/hts-specs/tree/a9b16d4c54d8b2bf900b210b8319474163d7ce2a&quot; rel=&quot;nofollow noreferrer&quot;&gt;original SAM specification&lt;/a&gt; (if you can view Apple Pages documents) and Table 1 in &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/19505943&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;The Sequence Alignment/Map format and SAMtools&lt;/em&gt;&lt;/a&gt; (Li &lt;em&gt;et al&lt;/em&gt;, 2009).  This was in line with previous tools using CIGAR strings, notably &lt;a href=&quot;http://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;exonerate&lt;/em&gt;&lt;/a&gt; which introduced them with just the &lt;strong&gt;M&lt;/strong&gt;, &lt;strong&gt;I&lt;/strong&gt;, and &lt;strong&gt;D&lt;/strong&gt; operators.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BWA-backtrack was written contemporaneously with the SAM format in 2008 and 2009 (and &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/19451168&quot; rel=&quot;nofollow noreferrer&quot;&gt;published in May 2009&lt;/a&gt;).  Its &lt;em&gt;ChangeLog&lt;/em&gt; shows that it was outputting mismatch information in an &lt;strong&gt;MD&lt;/strong&gt; tag from January 2009 and that &lt;strong&gt;MD&lt;/strong&gt; was defined in the SAM specification of the time (and that the tag value's syntax was somewhat in flux in February 2009).  The &lt;strong&gt;MD&lt;/strong&gt; tag is also described in that early v1.0 Pages-formatted SAM specification.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;strong&gt;=&lt;/strong&gt; and &lt;strong&gt;X&lt;/strong&gt; CIGAR operators were introduced later in &lt;a href=&quot;https://github.com/samtools/hts-specs/commit/07dc1c67a717a6c5cc9d65eeb8b3c99612744cde&quot; rel=&quot;nofollow noreferrer&quot;&gt;SAM v1.3&lt;/a&gt; as a result of &lt;a href=&quot;https://sourceforge.net/p/samtools/mailman/samtools-devel/thread/ee957a250907301016y22c96c5x2907a33c2e719da2%40mail.gmail.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;this lengthy samtools-devel mailing list thread&lt;/a&gt;.  The characters used for the operators and the initial implementations were essentially in place by November 2009.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since then, the &lt;strong&gt;=&lt;/strong&gt;/&lt;strong&gt;X&lt;/strong&gt; operators have not really taken over from &lt;strong&gt;M&lt;/strong&gt;, as you've seen.  There's probably a number of factors contributing to this:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Their later introduction, long after the basic &lt;strong&gt;M&lt;/strong&gt;/&lt;strong&gt;I&lt;/strong&gt;/&lt;strong&gt;D&lt;/strong&gt; operators and &lt;strong&gt;MD&lt;/strong&gt; tag were well-established;&lt;/li&gt;&#xA;&lt;li&gt;Conceivably their being specific to SAM and unavailable in other CIGAR flavours;&lt;/li&gt;&#xA;&lt;li&gt;The &lt;strong&gt;MD&lt;/strong&gt; tag still provides more information — &lt;strong&gt;X&lt;/strong&gt; doesn't tell you what the mismatched reference bases were.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="134" LastEditorUserId="134" LastEditDate="2017-06-08T09:10:32.403" LastActivityDate="2017-06-08T09:10:32.403" CommentCount="0" />
  <row Id="422" PostTypeId="1" CreationDate="2017-06-03T20:57:33.417" Score="3" ViewCount="50" Body="&lt;p&gt;I've come across a bit of confusion about the initialism NGS, so think it would be a good idea to clarify this term (and similar terms like 2GS, SBS, and HTS) for this site with a bit of discussion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the most common initialisms used to describe different sequencing technologies?&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-05T13:19:33.923" LastActivityDate="2017-06-05T17:43:34.723" Title="What is the difference between NGS, 2GS, SBS and HTS?" Tags="&lt;ngs&gt;&lt;terminology&gt;&lt;sbs&gt;&lt;hts&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="423" PostTypeId="2" ParentId="422" CreationDate="2017-06-03T20:57:33.417" Score="3" Body="&lt;p&gt;Here are my attempts at definitions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Sanger_sequencing&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sanger&lt;/a&gt;: A method of sequencing that depends on chain-terminatiing dideoxynucleotides. This sequencing uses the differential flow of DNA sequences of different lengths through a gel to determine the original DNA sequence, producing a single sequence per reaction container.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;NGS: Next-generation sequencing, also referred to as 2GS (second-generation sequencing). This term is used to describe the first wave of sequencing technologies that followed Sanger sequencing technology. The use of NGS has become more confusing with the advent of long-read sequencing, because it's a common assumption that &quot;next-generation&quot; refers to the most recent technology (which is incorrect in this case).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;HTS: High-throughput sequencing. This term describes any type of sequencing technology that produces large amounts of data, usually in the form of millions of different sequences produced from the same sequencing run.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SBS: Sequencing by synthesis. This term describes a method of sequencing that depends on the synthesis of [DNA] bases in order for sequencing to be carried out. This definition can extend into long-read sequencing (e.g. PacBio sequencers depend on synthesis during sequencing), but is more typically associated with only the second-generation sequencing technology.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-03T20:57:33.417" CommentCount="6" CommunityOwnedDate="2017-06-04T07:42:56.527" />
  <row Id="424" PostTypeId="1" CreationDate="2017-06-03T23:01:33.863" Score="4" ViewCount="28" Body="&lt;p&gt;Blast reports E-values, but short-read mappers report mapping qualities. Are they the same thing? Can they be converted to each other? If not, why blast doesn't report mapping quality while short-read mappers do not report E-values?&lt;/p&gt;&#xA;" OwnerUserId="549" LastEditorUserId="73" LastEditDate="2017-06-04T10:57:51.070" LastActivityDate="2017-06-04T10:57:51.070" Title="What is the difference between SAM mapping quality and Blast E-value?" Tags="&lt;terminology&gt;&lt;blast&gt;&lt;read-mapping&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="425" PostTypeId="2" ParentId="283" CreationDate="2017-06-03T23:06:42.720" Score="3" Body="&lt;p&gt;There are several &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/browse/?view=series&amp;amp;display=20&amp;amp;zsort=samples&quot; rel=&quot;nofollow noreferrer&quot;&gt;datasets available on GEO&lt;/a&gt;, though you do have to search for them. For example, here are three data sets that have both Illumina methylation and gene expression microarray profiling:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE87650&quot; rel=&quot;nofollow noreferrer&quot;&gt;GSE87650&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE72874&quot; rel=&quot;nofollow noreferrer&quot;&gt;GSE72874&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE56047&quot; rel=&quot;nofollow noreferrer&quot;&gt;GSE56047&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="563" LastActivityDate="2017-06-03T23:06:42.720" CommentCount="0" />
  <row Id="426" PostTypeId="1" AcceptedAnswerId="430" CreationDate="2017-06-03T23:10:32.853" Score="4" ViewCount="169" Body="&lt;p&gt;I learned that GATK 4 is using &lt;a href=&quot;https://spark.apache.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Spark&lt;/a&gt; for parallelization. I googled around, though I am still not quite sure how spark really works and how to use it in practice. Besides GATK 4, are any other bioinformatics tool using spark? Generally, is spark widely used? Is it a necessary skill to learn? Thanks in advance.&lt;/p&gt;&#xA;" OwnerUserId="549" LastEditorUserId="292" LastEditDate="2017-06-05T10:13:33.140" LastActivityDate="2017-06-06T21:40:05.000" Title="Is spark widely used in bioinformatics?" Tags="&lt;gatk&gt;&lt;spark&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="427" PostTypeId="1" CreationDate="2017-06-03T23:19:39.530" Score="2" ViewCount="21" Body="&lt;p&gt;I was given a list of target regions in BED and many exome alignments in BAM. I was asked to extract on-target alignments from these BAMs to save disk space. I know I can use bedtools to extract sub-BAMs. I am thinking to write a script to apply bedtools to all BAMs at my hand, but I speculate there may be some more convenient command lines to achieve my goal. How would you do? Thanks.&lt;/p&gt;&#xA;" OwnerUserId="549" LastActivityDate="2017-06-04T11:30:33.870" Title="How to extract exome on-target reads in batch?" Tags="&lt;bam&gt;&lt;linux&gt;&lt;bedtools&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="428" PostTypeId="2" ParentId="191" CreationDate="2017-06-04T01:17:18.017" Score="4" Body="&lt;p&gt;I don't have enough experience to answer which probabilistic distribution should be used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, this questions also also asks how to estimate parameters of the distributions. If a binomial distribution is chosen, then Heng Li's paper titled &quot;A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data&quot; &lt;a href=&quot;https://doi.org/10.1093/bioinformatics/btr509&quot; rel=&quot;nofollow noreferrer&quot;&gt;1&lt;/a&gt; is probably the definitive one. Section 2.3.1 of that paper describes an EM algorithm for estimating allele frequencies from multiple samples under the assumption of Hardy-Weinberg equilibrium for arbitrary but constant ploidy.&lt;/p&gt;&#xA;" OwnerUserId="492" LastActivityDate="2017-06-04T01:17:18.017" CommentCount="1" />
  <row Id="429" PostTypeId="2" ParentId="427" CreationDate="2017-06-04T01:31:45.110" Score="2" Body="&lt;p&gt;It sounds like &lt;code&gt;bedtools intersect&lt;/code&gt; will work for you:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bedtools intersect -wa -a &amp;lt;alignment.bam&amp;gt; -b &amp;lt;region.bed&amp;gt; &amp;gt; &amp;lt;intersect_alignment.bam&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;When a BAM file is used for the A file, the alignment is retained if overlaps exist, and exlcuded if an overlap cannot be found.  If multiple overlaps exist, they are not reported, as we are only testing for one or more overlaps.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It's easy enough to wrap this up into a &lt;code&gt;for&lt;/code&gt; loop for batch processing (line breaks can be removed if desired):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; for alnBam in alignmentFiles*.bam;&#xA;   do echo ${alnBam};&#xA;   bedtools intersect -wa -a ${alnBam} -b &amp;lt;region.bed&amp;gt; &amp;gt; intersect_${alnBam};&#xA; done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are a lot of other options which should cover most related uses. For more details, just run &quot;bedtools intersect&quot; (with no arguments).&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-04T11:30:33.870" LastActivityDate="2017-06-04T11:30:33.870" CommentCount="1" />
  <row Id="430" PostTypeId="2" ParentId="426" CreationDate="2017-06-04T01:33:49.913" Score="8" Body="&lt;p&gt;The only bioinformatics tool other than GATK4 that I am aware of that uses spark is &lt;a href=&quot;https://github.com/hail-is/hail&quot; rel=&quot;noreferrer&quot;&gt;Hail&lt;/a&gt; (a Spark based replacement for Plink). Hail is also supported by researchers at the Broad.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most places I have worked at have not switched over to Spark.  As such, I don't think it is widely used generally. Therefore, I don't think knowing Spark qualifies as a necessary skill to learn for bioinformatics at this time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I think Spark is a superior paradigm for handling the large amount of data that bioinformaticians routinely deal with, and I think we will soon see the field move towards using it more.  Any bioinformatician would do well to acquaint themselves with Spark and to play around with Hail.&lt;/p&gt;&#xA;" OwnerUserId="492" LastActivityDate="2017-06-04T01:33:49.913" CommentCount="3" />
  <row Id="431" PostTypeId="5" CreationDate="2017-06-04T07:23:24.257" Score="0" Body="&lt;h2&gt;&lt;code&gt;minion&lt;/code&gt; -- The Oxford Nanopore MinION&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;A small, portable sequencer that uses electrical current flowing through small molecules (e.g. DNA nucleotides) to determine the underlying sequence.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;What questions should have this tag?&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Anything that is associated with the generation or analysis of data from the MinION, e.g. long-read FASTQ files, raw signal data, nanopore sample prep QC. If questions are not specific to the &lt;em&gt;MinION&lt;/em&gt; (e.g. also applicable to PromethION), then the &lt;code&gt;nanopore&lt;/code&gt; tag should be used.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Brief Introduction&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The MinION's flow cell is comprised of 2048 wells containing a membrane perforated by nanopores. Ligated with a molecular motor, a single stranded DNA molecule passes through the pore, altering the recorded current. After the electronic sequencing is carried out, a software basecalling algorithm transforms the current trace into a modelled DNA sequence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The advantages of the MinION are rapid library preparation, portability (&lt;a href=&quot;http://dx.doi.org/10.1016/j.jbiotec.2016.12.006&quot; rel=&quot;nofollow noreferrer&quot;&gt;Walter &lt;em&gt;et al.&lt;/em&gt;, 2016&lt;/a&gt;; &lt;a href=&quot;http://biorxiv.org/content/early/2016/09/27/077651&quot; rel=&quot;nofollow noreferrer&quot;&gt;Castro-Wallace &lt;em&gt;et al.&lt;/em&gt;, 2016&lt;/a&gt;), long molecule sequencing (&lt;a href=&quot;http://dx.doi.org/10.1101/019281&quot; rel=&quot;nofollow noreferrer&quot;&gt;Urban &lt;em&gt;et al.&lt;/em&gt;, 2015&lt;/a&gt;), and sequencing of non-model modifications of the DNA strand (&lt;a href=&quot;http://dx.doi.org/10.1038/nmeth.4184&quot; rel=&quot;nofollow noreferrer&quot;&gt;Simpson &lt;em&gt;et al.&lt;/em&gt;, 2017&lt;/a&gt;). With the recent improvement in the chemistry of the MinION, Oxford Nanopore has overcome the majority of issues associated with low yield and high error rates that have limited the range of its application. The MinION sequencing device has now been successfully applied to sequence genomes of a wide range of sizes, from bacterial and viral genomes (&lt;a href=&quot;http://dx.doi.org/10.1038/srep28625&quot; rel=&quot;nofollow noreferrer&quot;&gt;Deschamps &lt;em&gt;et al.&lt;/em&gt;, 2016&lt;/a&gt;; &lt;a href=&quot;http://dx.doi.org/10.1101/098913&quot; rel=&quot;nofollow noreferrer&quot;&gt;Quick &lt;em&gt;et al.&lt;/em&gt;, 2017&lt;/a&gt;), amplicon sequencing like bacterial 16S rRNA sequencing (&lt;a href=&quot;http://dx.doi.org/10.1186/s13742-016-0111-z&quot; rel=&quot;nofollow noreferrer&quot;&gt;Benitez-paez &lt;em&gt;et al.&lt;/em&gt;, 2016&lt;/a&gt;), and more recently a human genome (&lt;a href=&quot;http://dx.doi.org/10.1101/128835&quot; rel=&quot;nofollow noreferrer&quot;&gt;Jain &lt;em&gt;et al.&lt;/em&gt;, 2017&lt;/a&gt;). The MinION has also been used for cDNA sequencing (&lt;a href=&quot;http://dx.doi.org/10.7717/peerj.1441&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hargreaves &lt;em&gt;et al.&lt;/em&gt;, 2015&lt;/a&gt;), for detecting DNA methylation patterns without chemical treatment (&lt;a href=&quot;http://dx.doi.org/10.1038/nmeth.4184&quot; rel=&quot;nofollow noreferrer&quot;&gt;Simpson &lt;em&gt;et al.&lt;/em&gt;, 2017&lt;/a&gt;; &lt;a href=&quot;http://dx.doi.org/10.1038/nmeth.4189&quot; rel=&quot;nofollow noreferrer&quot;&gt;Rand &lt;em&gt;et al.&lt;/em&gt;, 2017&lt;/a&gt;), and for direct RNA sequencing with detection of modified 16S rRNA nucleotides (&lt;a href=&quot;http://dx.doi.org/10.1101/132274&quot; rel=&quot;nofollow noreferrer&quot;&gt;Smith &lt;em&gt;et al.&lt;/em&gt;, 2017&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[CC-by-4.0 text source by White &lt;em&gt;et al.&lt;/em&gt;, 2017 &lt;a href=&quot;https://f1000research.com/articles/6-631/v1&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;]&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Important links for learning more&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://nanoporetech.com/how-it-works&quot; rel=&quot;nofollow noreferrer&quot;&gt;Summary of nanopore DNA sequencing&lt;/a&gt; (official ONT website)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://f1000research.com/gateways/nanoporeanalysis&quot; rel=&quot;nofollow noreferrer&quot;&gt;F1000 Research Nanopore Analysis gateway&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-15T13:53:23.610" LastActivityDate="2017-06-15T13:53:23.610" CommentCount="0" />
  <row Id="432" PostTypeId="4" CreationDate="2017-06-04T07:23:24.257" Score="0" Body="Questions associated with the generation or analysis of data from sequencer MinION. For questions about long reads (e.g. including PacBio), use `long-reads` instead. For questions that are more generally about nanopore devices (e.g. including PromethION), use `nanopore` instead." OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-15T13:54:11.900" LastActivityDate="2017-06-15T13:54:11.900" CommentCount="0" />
  <row Id="433" PostTypeId="2" ParentId="418" CreationDate="2017-06-04T07:38:44.210" Score="5" Body="&lt;p&gt;&lt;code&gt;bwa mem&lt;/code&gt; is newer, faster, and [should be] more accurate, particularly for longer reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From the &lt;code&gt;bwa&lt;/code&gt; man page (presumably in Heng Li's own words):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms:  BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm  is designed for Illumina sequence reads up to 100bp, while the rest two for longer sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar  features such as long-read support and split alignment, but &lt;strong&gt;&lt;em&gt;BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and  more  accurate.&lt;/em&gt;&lt;/strong&gt; BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-04T07:38:44.210" CommentCount="7" />
  <row Id="434" PostTypeId="1" CreationDate="2017-06-04T09:28:02.797" Score="1" ViewCount="36" Body="&lt;p&gt;Our research institute processes a lot of flow cytometry data, but the produced data is under-utilised due to the effort required to process it. A typical run will produce 5 million events (ideally one event per cell), with up to 14 dimensions of [ideally] log-normal fluorescence values for particular groups of cells (&quot;populations&quot;). Due to various systematic errors, negative values, scatter, and a non-zero &quot;zero&quot; value can happen, but I'm going to ignore those for the purpose of this question and assume that the data are well-distributed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Researchers will typically probe these data using manual filters (set up specifically for each experiment) to find populations of interest. I suppose a picture might help. This one shows three identifiable cell populations, in order of size one at about (X/Ly6C:2,Y/CD86:2), one at about (4.5,3), and one small population at (2,4).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/ErbhE.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ErbhE.png&quot; alt=&quot;CD86 vs Ly6C flow plot&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's another plot that has two cell populations that are close to each other, such that the population &quot;humps&quot; overlap substantially.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/jJ72F.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/jJ72F.png&quot; alt=&quot;B220 vs CD11b flow plot&quot;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Manual filters are typically used because it can be very difficult to distinguish between a noisy data point from a large cell population and a less noisy data point from a small cell population, particularly when considering populations that make up about 0.01% of the total cells.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an additional complication, counting these populations can be difficult when populations overlap (as in the second image). A filter/slice through the plot that separates populations could count many cells as being members of the wrong population.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can these populations be detected in an automated way? If it is assumed that the populations are spread in a gaussian fashion at some point in each dimension, is there some method that can be used to approximate the number of cells in each population, even when populations are close by?&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-04T12:09:34.170" Title="Finding peaks and estimating cell population sizes in multi-dimensional flow cytometry data" Tags="&lt;flow-cytometry&gt;&lt;gaussian&gt;&lt;fluorescence&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="435" PostTypeId="2" ParentId="424" CreationDate="2017-06-04T10:24:42.397" Score="7" Body="&lt;p&gt;The E-value and the mapping qualities are two very different things.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The E-value is &quot;a parameter that describes the number of hits one can 'expect' to see by chance when searching a database of a particular size&quot;. More details can be found here: &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE_TYPE=BlastDocs&amp;amp;DOC_TYPE=FAQ#expect&quot; rel=&quot;noreferrer&quot;&gt;https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE_TYPE=BlastDocs&amp;amp;DOC_TYPE=FAQ#expect&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The mapping quality is an attempt to estimate the probability that a given base from a given read is mapped correctly to a particular place in the reference genome. Different aligners calculate mapping qualities very differently, so there is no simple way to compare them across aligners. But in general the mapping quality will consider things like: (i) the quality of the base call; (ii) the repeat structure of the reference; (iii) the alignment algorithm; (iv) whether the read has a mapped pair; (v) anything else the author of the mapping software thought might help. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;E-values and mapping qualities are, therefore, measuring two fundamentally different properties with very different uses.&lt;/p&gt;&#xA;" OwnerUserId="156" LastActivityDate="2017-06-04T10:24:42.397" CommentCount="0" />
  <row Id="436" PostTypeId="2" ParentId="434" CreationDate="2017-06-04T11:13:06.240" Score="2" Body="&lt;p&gt;There are quite a few algorithms developed for the automatic classification of multidimensional flow cytometry data, you can see a (not so recent?) review here: &lt;a href=&quot;http://www.nature.com/nmeth/journal/v10/n3/full/nmeth.2365.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.nature.com/nmeth/journal/v10/n3/full/nmeth.2365.html&lt;/a&gt;. In your case, you are interested in the &lt;strong&gt;unsupervised&lt;/strong&gt; methods, since you do not have data from single populations (which would provide you &quot;labels&quot; associated to each sample), as I understood. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words, if you have your data set as a matrix of 5 million samples (cells) with 14 dimensions (FC values) each, then you are looking basically for algorithms that will do a &lt;em&gt;clustering&lt;/em&gt; of the samples into groups. By the way your description of each sample belonging to a multivariate Normal distribution leads to a &lt;strong&gt;Gaussian Mixture Model&lt;/strong&gt;, which can be estimated from your data using e.g. the scikit-learn library for python: &lt;a href=&quot;http://scikit-learn.org/stable/modules/mixture.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://scikit-learn.org/stable/modules/mixture.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="45" LastActivityDate="2017-06-04T11:13:06.240" CommentCount="0" />
  <row Id="437" PostTypeId="2" ParentId="434" CreationDate="2017-06-04T12:09:34.170" Score="2" Body="&lt;p&gt;Automated gating methods are gaining popularity over manual and time consuming analysis in say &lt;a href=&quot;https://www.flowjo.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;FlowJo&lt;/a&gt;, you should take a look at the &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/openCyto.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;openCyto&lt;/a&gt; bioconductor package this is a framework which builds on top of existing Bioconductor infrastructure for flow cytometry &lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003806&quot; rel=&quot;nofollow noreferrer&quot;&gt;(PLOS compbio paper)&lt;/a&gt; with the aim of making various analysis routes more accessible to the researcher by assisting getting data into and out various gating algorithms. You can find a nice intro &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/vignettes/openCyto/inst/doc/openCytoVignette.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; which starts with importing a manual gating scheme from FlowJo and a how-to on 1D and 2D gating can also be found &lt;a href=&quot;https://www.bioconductor.org/packages/devel/bioc/vignettes/openCyto/inst/doc/HowToAutoGating.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;An excellent YouTube intro to analysing flow data in R by Ryan Duggan &lt;a href=&quot;https://www.youtube.com/watch?v=_B7mo6dB3BU&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cytometry on Air&lt;/a&gt; is worth watching especially for those which have no R experience and are curious as to what can be achieved.  Additionally it's worth having a look at the growing list of Bioconductor packages and datasets that exists for flow data: &lt;a href=&quot;https://www.bioconductor.org/help/search/index.html?q=cytometry/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.bioconductor.org/help/search/index.html?q=cytometry/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="532" LastActivityDate="2017-06-04T12:09:34.170" CommentCount="0" />
  <row Id="438" PostTypeId="1" AcceptedAnswerId="447" CreationDate="2017-06-04T13:21:05.390" Score="3" ViewCount="36" Body="&lt;p&gt;Working on various cancers I have an interest in detecting structural variation (SV) in human, we've successfully used various tools like &lt;a href=&quot;http://gmt.genome.wustl.edu/packages/pindel/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Pindel&lt;/a&gt;, &lt;a href=&quot;http://svdetect.sourceforge.net/Site/Home.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;SVDetect&lt;/a&gt;, &lt;a href=&quot;https://github.com/Illumina/manta&quot; rel=&quot;nofollow noreferrer&quot;&gt;Manta&lt;/a&gt;, and &lt;a href=&quot;https://github.com/arq5x/lumpy-sv&quot; rel=&quot;nofollow noreferrer&quot;&gt;LUMPY&lt;/a&gt;, to name a few for detecting SVs in illumina short-read sequencing.  I curious if anyone has successfully used ONTs &lt;a href=&quot;https://nanoporetech.com/products/minion&quot; rel=&quot;nofollow noreferrer&quot;&gt;MinION&lt;/a&gt; sequencer for detecting SV, as there are many cases where longer reads would be beneficial for SV detection especially where events are long and/or occur in repetitive regions.  Has anyone tried or had success with tools previously designed for PacBio data such as &lt;a href=&quot;https://github.com/fritzsedlazeck/Sniffles&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sniffles&lt;/a&gt;? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Supplementary question, obviously the MinION has not quite got the throughput of say the PacBio SEQUEL so can't generate low-pass coverage for the whole human genome as easily, has anyone got experience trying to generate reads targeting specific areas of the genome in areas that are known for translations, long inversions, duplications, &lt;em&gt;etc&lt;/em&gt;?&lt;/p&gt;&#xA;" OwnerUserId="532" LastEditorUserId="73" LastEditDate="2017-06-08T05:32:58.860" LastActivityDate="2017-06-08T05:32:58.860" Title="Detecting structural variants with MinION data" Tags="&lt;nanopore&gt;&lt;structural-variation&gt;&lt;long-reads&gt;&lt;cancer&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="0" />
  <row Id="439" PostTypeId="2" ParentId="378" CreationDate="2017-06-04T13:31:13.200" Score="3" Body="&lt;p&gt;If I remember correctly Ewan Birney's Dynamite (a &lt;a href=&quot;https://en.wikipedia.org/wiki/Compiler-compiler&quot; rel=&quot;nofollow noreferrer&quot;&gt;compiler-compiler&lt;/a&gt;) as presented at &lt;a href=&quot;https://pdfs.semanticscholar.org/2345/f68bb0b2f079f9f201034da462d69a7817b1.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;ISMB 1997&lt;/a&gt; had this functionality, there is also some code here on GitHub &lt;a href=&quot;https://github.com/birney/wise3&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/birney/wise3&lt;/a&gt; which at least mentions Dynamite.  Suspect Ewan is too busy these days to work on this, although he has tweeted about blowing dust of his old Dynamite, sorry Dynamite code: &lt;a href=&quot;https://twitter.com/ewanbirney/status/788121636973142016&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://twitter.com/ewanbirney/status/788121636973142016&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="532" LastEditorUserId="532" LastEditDate="2017-06-04T13:37:40.803" LastActivityDate="2017-06-04T13:37:40.803" CommentCount="0" />
  <row Id="440" PostTypeId="2" ParentId="351" CreationDate="2017-06-04T13:37:19.093" Score="1" Body="&lt;p&gt;&lt;a href=&quot;https://github.com/mathieu-lemire/celluloid_0.11&quot; rel=&quot;nofollow noreferrer&quot;&gt;Celluloid&lt;/a&gt; emits copy number profiles and tumour purity / ploidy information.  There's a nice tutorial https://github.com/mathieu-lemire/celluloid_0.11_tutorial. One thing to keep in mind is that celluloid will find multiple solutions and taking advantage of its plotting tools is essential to determining which solution is correct. Typically the first solution is correct, but occasionally it may not be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another tool similar is &lt;a href=&quot;http://genome.cshlp.org/content/early/2014/07/24/gr.180281.114&quot; rel=&quot;nofollow noreferrer&quot;&gt;TITAN&lt;/a&gt;, which may also require hand annotated the ideal solution.&lt;/p&gt;&#xA;" OwnerUserId="64" LastActivityDate="2017-06-04T13:37:19.093" CommentCount="0" />
  <row Id="441" PostTypeId="1" CreationDate="2017-06-04T13:50:23.627" Score="7" ViewCount="119" Body="&lt;p&gt;Some of the work in our lab requires a comparison of a strain across several experimental conditions. We are looking to identify most similar experimental conditions based on the gene transcription response similarity from the cell.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While we could easily invent and create home-grown methods to do it, their implementation and testing are a laborious project in themselves and are outside the scope of our current work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any methods for RNA expression profile similarity computation that has been already published? If yes, what is your experience using them?&lt;/p&gt;&#xA;" OwnerUserId="569" LastEditorUserId="163" LastEditDate="2017-06-07T06:19:11.957" LastActivityDate="2017-06-07T06:19:11.957" Title="What methods exist to calculate RNA expression profile similarity" Tags="&lt;rna-seq&gt;" AnswerCount="4" CommentCount="0" />
  <row Id="442" PostTypeId="1" CreationDate="2017-06-04T14:13:44.537" Score="4" ViewCount="42" Body="&lt;p&gt;I use SOAPdenovo2 to assemble a large genome (4.8G) using ~20X paired-end reads. The  total length of contig sizes is 6.3G while total length of scaffolds is 2.7G. Note that this is a haploid genome, so there is no issue of heterozygosity for scaffolding. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am wondering what happened during the scaffolding procedure? How comes that sum of scaffolds is so much smaller than sum of contigs?&lt;/p&gt;&#xA;" OwnerUserId="570" LastEditorUserId="73" LastEditDate="2017-06-05T02:22:40.943" LastActivityDate="2017-06-05T10:36:44.423" Title="What causes the difference in total length of assembled contigs and scaffolds in SOAPdenovo2?" Tags="&lt;assembly&gt;&lt;scaffold&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="443" PostTypeId="2" ParentId="327" CreationDate="2017-06-04T17:58:23.480" Score="4" Body="&lt;p&gt;The quality control of ngs reads is heavily dependent on type of the project. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;For genome assembly&lt;/strong&gt; projects based on short reads, beside already covered checking quality of sequencing, you would like to look at the kmer spectra to find out, if your reads are going to make sense when they will be translated to De Brujin graph. You will get also a clues about genome coverage, genome size, repetitive content and small clue about heterozygocity. A lot of useful info about interpretation you can find in the README of GenomeScope.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A list tools I used:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.genome.umd.edu/jellyfish.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;jellyfish&lt;/a&gt; - to count k-mer frequencies&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/schatzlab/genomescope&quot; rel=&quot;nofollow noreferrer&quot;&gt;GenomeScope&lt;/a&gt; - a package for analysis and visualisation of k-mer frequencies (they recommend to use jellyfish for counting k-mer frequencies)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://kmergenie.bx.psu.edu/&quot; rel=&quot;nofollow noreferrer&quot;&gt;kmergenie&lt;/a&gt; - for prediction of optimal kmer for assembly&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Using these tools can save you a lot of frustration if you accidentally sequence a contaminated sample or if your colleagues / a sequencing company have sent you a wrong file!&lt;/p&gt;&#xA;" OwnerUserId="57" LastActivityDate="2017-06-04T17:58:23.480" CommentCount="0" />
  <row Id="444" PostTypeId="2" ParentId="426" CreationDate="2017-06-04T20:22:22.863" Score="4" Body="&lt;p&gt;&lt;a href=&quot;https://github.com/bigdatagenomics/adam&quot; rel=&quot;nofollow noreferrer&quot;&gt;ADAM&lt;/a&gt; and &lt;a href=&quot;https://github.com/bigdatagenomics/avocado&quot; rel=&quot;nofollow noreferrer&quot;&gt;avocado&lt;/a&gt; are Spark-based alignment and variant calling tools under active development by a collaboration (&lt;a href=&quot;http://bdgenomics.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://bdgenomics.org&lt;/a&gt;) that also includes the Broad, but I do not believe they have wide adoption.&lt;/p&gt;&#xA;" OwnerUserId="462" LastEditorUserId="462" LastEditDate="2017-06-04T21:17:09.970" LastActivityDate="2017-06-04T21:17:09.970" CommentCount="0" />
  <row Id="445" PostTypeId="2" ParentId="441" CreationDate="2017-06-04T22:35:10.487" Score="4" Body="&lt;p&gt;There's a new paper that's just appeared on the subject of &quot;proportionality&quot;, including a method by which RNA expression might be compared:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004075&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004075&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a new concept for me, and the article is not easy enough for me to read that I can write up a quick summary; the authors don't seem to devote a section in the paper to defining &quot;proportionality&quot;. However, here's an interesting chunk from the article:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;We graphed the network of relationships between these mRNAs (S5 Fig.), an approach similar to gene co-expression network [12] or weighted gene co-expression analysis [13] but founded on proportionality and therefore valid for relative data. The network revealed one cluster of 96, and many other smaller clusters of mRNAs behaving proportionally across conditions.&lt;br&gt;&#xA;  ...&lt;br&gt;&#xA;  We are also keen to raise awareness that correlation (and other statistical methods that assume measurements come from real coordinate space) should not be applied to relative abundances. This is highly relevant to gene coexpression networks [12]. Correlation is at the heart of methods like Weighted Gene Co-expression Network Analysis [13] and heatmap visualization [14]. These methods are potentially misleading if applied to relative data.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-04T22:35:10.487" CommentCount="4" />
  <row Id="446" PostTypeId="2" ParentId="441" CreationDate="2017-06-04T22:42:23.850" Score="3" Body="&lt;p&gt;If you have a lot of samples with very different environmental conditions, then a Weighted Gene Correlation Network Analysis (WGCNA) might be appropriate:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631488/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631488/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This type of analysis looks for genes that trace similar (or opposing) expression patterns throughout different conditions (e.g. high-medium-medium-low-absent-high would be highly negatively correlated with low-medium-medium-high-very high-low).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That particular paper introduces the concept of &quot;modules&quot;, which are groups of genes that share similar expression patterns. Functions are available for plotting how the expression of canonical module members changes throughout the different conditions, and for identifying which module (or modules) a particular gene is likely to be a member of.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WGCNA works best when there's a lot of different expression changes in the different conditions, which sounds like it would fit well with your project. However, it concentrates more on the genes, rather than the conditions (which seems like it would be less useful for you).&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-04T22:42:23.850" CommentCount="0" />
  <row Id="447" PostTypeId="2" ParentId="438" CreationDate="2017-06-04T23:02:47.750" Score="4" Body="&lt;p&gt;There was a Structural Variant breakout session at the London Calling conference this year. Unfortunately I didn't attend that session, but MinION community members have access to Constance Donnell's summary of that:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://community.nanoporetech.com/posts/breakout-structural-varia&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://community.nanoporetech.com/posts/breakout-structural-varia&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are my attempts at grabbing non-creative chunks from those notes:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Professor Wigard Kloosterman has been developing a bioinformatics pipeline called NanoSV for mapping genomic structural variants in patients with congenital abnormalities&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Tomas Sesani at the University of Utah studied the precise copy number of duplicated genes in individual virus genes over time, and tracked the SNP within copy number variable regions of the genome [no public tool mentioned]&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Dr. Sudha Rao from Genotypic Technology in India used the MinION sequencer with Sniffles for structural variation calling&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;ONT have said that the sessions/talks from London Calling 2017 will all be made publicly available at some point in the future.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-04T23:02:47.750" CommentCount="1" />
  <row Id="448" PostTypeId="2" ParentId="317" CreationDate="2017-06-04T23:11:39.370" Score="1" Body="&lt;p&gt;I cannot think of any principled rationale for choosing this filtering strategy. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I am going to take a guess that this filtering strategy is supposed to filter out SNPs for which imputation did not work well?  In that case the appropriate statistic to filter on is the INFO score as described &lt;a href=&quot;https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html#data_summaries&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="492" LastActivityDate="2017-06-04T23:11:39.370" CommentCount="2" />
  <row Id="449" PostTypeId="1" CreationDate="2017-06-04T23:25:12.173" Score="5" ViewCount="35" Body="&lt;p&gt;The &lt;code&gt;samtools mpileup&lt;/code&gt; command has quite a neat feature that it is able to correct mapping errors associated with misalignment of INDELs. By default, the &lt;code&gt;mpileup&lt;/code&gt; command will not work for reads that have more than 250X coverage of the reference genome. While this limit can be increased, very high coverage causes the mpileup program to grind to a halt, so it'd be nice to know if there's some easy way to make that faster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To add a bit more context, I've been doing this with mitochondrial genome reads that were extracted from both Illumina whole-genome sequencing (coverage ~1000X), and from targeted amplicon sequencing done on the IonTorrent (coverage up to ~4000X).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see that @rightskewed has mentioned the downsampling ability of samtools with &lt;code&gt;samtools view -s &amp;lt;float&amp;gt;&lt;/code&gt; (see &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/406/73&quot;&gt;here&lt;/a&gt;), which seems like it might work as a solution for this if used prior to the mpileup operation.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-05T10:26:20.287" Title="How can I speed up INDEL calling/correction on BAM files?" Tags="&lt;bam&gt;&lt;samtools&gt;&lt;variant-calling&gt;&lt;indel&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="450" PostTypeId="2" ParentId="449" CreationDate="2017-06-04T23:34:21.633" Score="3" Body="&lt;p&gt;I wasn't aware of the samtools subsampling when I had this problem a couple of years ago, so ended up writing my own digital normalisation method to deal with &lt;em&gt;mapped&lt;/em&gt; reads. This method reduces the genome coverage, but preserves reads where coverage is low.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because I was working with IonTorrent reads (which have variable length), I came up with the idea of selecting the longest read that mapped to each location in the genome (assuming such a read existed). This meant that the highly variable coverage for different samples (sometimes as low as 200X, sometimes as high as 4000X) was flattened out to a much more consistent coverage of about 100-200X. Here's the core of the Perl code that I wrote:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  if(($F[2] ne $seqName) || ($F[3] != $pos) || (length($bestSeq) &amp;lt;= length($F[9]))){&#xA;    if(length($bestSeq) == length($F[9])){&#xA;      ## reservoir sampling with a reservoir size of 1&#xA;      ## See https://en.wikipedia.org/wiki/Reservoir_sampling&#xA;      ## * with probability 1/i, keep the new item instead of the current item&#xA;      $seenCount++;&#xA;      if(!rand($seenCount)){&#xA;        ## i.e. if rand($seenCount) == 0, then continue with replacement&#xA;        next;&#xA;      }&#xA;    } else {&#xA;      $seenCount = 1;&#xA;    }&#xA;    if(($F[2] ne $seqName) || ($F[3] != $pos)){&#xA;      if($output eq &quot;fastq&quot;){&#xA;        printSeq($bestID, $bestSeq, $bestQual);&#xA;      } elsif($output eq &quot;sam&quot;){&#xA;        print($bestLine);&#xA;      }&#xA;    }&#xA;    $seqName = $F[2];&#xA;    $pos = $F[3];&#xA;    $bestLine = $line;&#xA;    $bestID = $F[0];&#xA;    $bestFlags = $F[1];&#xA;    $bestSeq = $F[9];&#xA;    $bestQual = $F[10];&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The full code (which works as a filter on uncompressed SAM files) can be found &lt;a href=&quot;https://github.com/gringer/bioinfscripts/blob/master/sam2LongestBase.pl&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-05T10:26:20.287" LastActivityDate="2017-06-05T10:26:20.287" CommentCount="4" />
  <row Id="451" PostTypeId="2" ParentId="442" CreationDate="2017-06-05T02:34:11.050" Score="1" Body="&lt;p&gt;Something else that could be happening is that contigs that are being collapsed into &quot;heterozygous&quot; groups. This would be a particular problem when a genome has a substantial amount of repeated sequence. Digging deep into the supplementary information of the &lt;a href=&quot;https://academic.oup.com/gigascience/article-lookup/doi/10.1186/2047-217X-1-18&quot; rel=&quot;nofollow noreferrer&quot;&gt;SOAPdenovo2 paper&lt;/a&gt;, I've found the following information:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In SOAPdenovo2, heterozygous contig pairs are recognized by utilizing the information of contig depth and the locality of contig. The recognized heterozygous contig pairs should obey the following rules: 1) the similarity between contigs should be high enough, for example, ≥ 95%; 2) the depth of both contigs should be near half of the average depth or all contigs, complying Poisson distribution; 3) the two contigs should be located adjacently in a scaffold and have no relationship to each other inferred by paired-end reads information. The normal contigs neighboring the heterozygous regions, if they exist, could be connected to both of the heterozygous contig pairs (H1 and H2). Only the contig with relatively higher depth in a heterozygous contig pair were kept for scaffolding. The method reduces the influence of genome heterozygosity on final scaffold length. All heterozygous contig pairs were outputted to a file to facilitate further analysis. &lt;strong&gt;However, the trade-off of this method is that it might incorrectly remove paralogous contigs&lt;/strong&gt;. This problem could be relieved by a gap-filling procedure while the removed copy of paralogous contigs would be represented by gaps during scaffolding.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-05T10:36:44.423" LastActivityDate="2017-06-05T10:36:44.423" CommentCount="5" />
  <row Id="452" PostTypeId="1" CreationDate="2017-06-05T04:50:53.453" Score="6" ViewCount="69" Body="&lt;p&gt;The Albertsen lab has recently put out a &lt;a href=&quot;http://albertsenlab.org/can-you-beat-our-nanopore-read-error-correction-we-hope-so/&quot; rel=&quot;nofollow noreferrer&quot;&gt;competition/challenge&lt;/a&gt; for read error correction&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I only found out about this today, and I think the conference where high-ranking participants were going to be mentioned has just finished, but the data is all public and there's no reason why this can't be continued in the future as a benchmarking test for nanopore base calling and/or read error correction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Data: Nanopore reads (as called FAST5 files) can be found &lt;a href=&quot;http://www.ebi.ac.uk/ena/data/view/PRJEB20906&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. The initial called FASTQ files can be found &lt;a href=&quot;https://www.dropbox.com/sh/cw8n7df1z61lkcj/AACc4ElVSefVfD5csnv0Klc_a?dl=0&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. Reference sequences (from which the reads were generated) can be found &lt;a href=&quot;https://www.dropbox.com/s/u6w993jrca05w8u/mockrRNAall.fasta?dl=0&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every [passed] read should be a 2D nanopore read with the following sequence structure, where the decamer, &lt;strong&gt;NNNNNNNNNN&lt;/strong&gt;, is the unique molecular identifier and is attached to common primer sites at the read extremities:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;strong&gt;&lt;em&gt;AAAGATGAAGAT&lt;/em&gt;&lt;/strong&gt;–&lt;strong&gt;NNNNNNNNNN&lt;/strong&gt; CGTACTAGACTTGCCTGTCGCTCTATCTTCTTTTTTTTTTTTTTTTTTTT&lt;br&gt;&#xA;  &amp;lt;—- fragment of SSU cDNA molecule—-&gt;&lt;br&gt;&#xA;  GGGCAATATCAGCACCAACAGAAATAGATCGC&lt;strong&gt;NNNNNNNNNN&lt;/strong&gt;–&lt;strong&gt;&lt;em&gt;ATGGATGAGTCT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So... what's the best consensus accuracy you can get from these reads? How did you get that accuracy?&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-08T05:34:30.047" LastActivityDate="2017-06-08T05:34:30.047" Title="Improving consensus assembly from UMI-tagged nanopore reads" Tags="&lt;nanopore&gt;&lt;benchmarking&gt;&lt;consensus&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="453" PostTypeId="1" CreationDate="2017-06-05T12:49:24.763" Score="9" ViewCount="206" Body="&lt;p&gt;This has come up repeatedly recently: I have a very large text file (in the order of several GiB) and I need to perform line-based subsetting for around 10,000 lines. There exist solutions for specific scenarios (e.g. &lt;code&gt;samtools view -s&lt;/code&gt; for randomly sampling BAM files) but sometimes my use-case doesn’t fit into these categories.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unfortunately a naïve &lt;code&gt;sed&lt;/code&gt;-based solution is &lt;em&gt;extremely&lt;/em&gt; slow:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time sed -n -f &amp;lt;(awk -vOFS='' '{print $0, &quot;p&quot;}' line_numbers.txt) input_file &amp;gt; selected_lines.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where &lt;code&gt;line_numbers.txt&lt;/code&gt; is a file containing one line number per line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Forget running this for 10,000 lines; it’s already grinding to a halt for a mere 1000.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I speed this up, ideally so that it scales only with the size of the input file, and has more or less constant runtime n the number of lines that I subset?&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="73" LastEditDate="2017-06-05T19:44:59.737" LastActivityDate="2017-06-10T01:29:46.010" Title="How do I efficiently subset a very large line-based file?" Tags="&lt;shell&gt;&lt;benchmarking&gt;&lt;subset&gt;&lt;text&gt;" AnswerCount="4" CommentCount="11" />
  <row Id="454" PostTypeId="2" ParentId="453" CreationDate="2017-06-05T12:49:24.763" Score="4" Body="&lt;p&gt;Turns out, simply keeping track of the next candidate line (after sorting the sample line numbers) fixes the performance issue, and most of the remaining slowness seems to be due to the overhead of actually reading the file so there’s not very much to improve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since I don’t know how how to do this in &lt;code&gt;sed&lt;/code&gt;, and it’s not trivial in &lt;code&gt;awk&lt;/code&gt; either, here’s a Perl script:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-perl prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/env perl&#xA;&#xA;use strict;&#xA;use warnings;&#xA;&#xA;my $file = $ARGV[0];&#xA;my $lines_file = $ARGV[1];&#xA;&#xA;open my $lines_fh, '&amp;lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xA;chomp (my @lines = &amp;lt;$lines_fh&amp;gt;);&#xA;close $lines_fh;&#xA;&#xA;@lines = sort {$a &amp;lt;=&amp;gt; $b} @lines;&#xA;&#xA;open my $fh, '&amp;lt;', $file or die &quot;Cannot read file $file&quot;;&#xA;my $line = 1;&#xA;my $next_line = 0;&#xA;while (&amp;lt;$fh&amp;gt;) {&#xA;    last if $next_line == scalar @lines;&#xA;    if ($line++ == $lines[$next_line]) {&#xA;        $next_line++;&#xA;        print;&#xA;    }&#xA;}&#xA;close $fh;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I’ve implemented a &lt;a href=&quot;https://gist.github.com/klmr/a3e88c0ce102be0d3b2ce52ae83825e5&quot; rel=&quot;nofollow noreferrer&quot;&gt;similar function in C++&lt;/a&gt; for an &lt;a href=&quot;https://github.com/HannahVMeyer/PhenotypeSimulator&quot; rel=&quot;nofollow noreferrer&quot;&gt;R package&lt;/a&gt;, that's only slightly longer than the Perl script. It is ~3 times faster than the Perl script on my test file.&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="298" LastEditDate="2017-06-09T01:58:30.053" LastActivityDate="2017-06-09T01:58:30.053" CommentCount="3" />
  <row Id="455" PostTypeId="2" ParentId="453" CreationDate="2017-06-05T14:17:42.013" Score="3" Body="&lt;p&gt;Some related questions appear in other sites, with potentially interesting solutions, which I report here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To sample approximately 1% of the non-empty lines:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;awk 'BEGIN {srand()} !/^$/ { if (rand() &amp;lt;= .01) print $0}' input_file&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(from &lt;a href=&quot;https://stackoverflow.com/a/692321/1878788&quot;&gt;https://stackoverflow.com/a/692321/1878788&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To select 1000 random lines:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;shuf -n 1000 input_file&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(from &lt;a href=&quot;https://stackoverflow.com/a/15065490/1878788&quot;&gt;https://stackoverflow.com/a/15065490/1878788&lt;/a&gt;, and &lt;a href=&quot;https://unix.stackexchange.com/a/108604/55127&quot;&gt;https://unix.stackexchange.com/a/108604/55127&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Edit: Python solutions using a list of lines&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Using a set of line indices and selecting lines by testing set membership:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python3&#xA;&#xA;import sys&#xA;&#xA;with open(sys.argv[2], &quot;r&quot;) as line_numbers_file:&#xA;    line_indices = set(int(line) - 1 for line in line_numbers_file)&#xA;&#xA;with open(sys.argv[1], &quot;r&quot;) as input_file:&#xA;    print(*(line.strip() for (idx, line) in enumerate(input_file)&#xA;            if idx in line_indices), sep=&quot;\n&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Using a numpy boolean array together with &lt;code&gt;itertools.compress&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python3&#xA;&#xA;import sys&#xA;from itertools import compress&#xA;from numpy import zeros&#xA;&#xA;with open(sys.argv[2], &quot;r&quot;) as line_numbers_file:&#xA;    line_indices = [int(line) - 1 for line in line_numbers_file]&#xA;&#xA;selector = zeros(max(line_indices) + 1, dtype=bool)&#xA;selector[line_indices] = 1&#xA;&#xA;with open(sys.argv[1], &quot;r&quot;) as input_file:&#xA;    print(*(line.strip() for line in compress(input_file, selector)), sep=&quot;\n&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I did some tests on a file containing 15774756 sam records and a list of 10000 pre-generated random line numbers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The perl script proposed by Konrad Rudolph (&lt;a href=&quot;https://bioinformatics.stackexchange.com/a/454/292&quot;&gt;https://bioinformatics.stackexchange.com/a/454/292&lt;/a&gt;) runs in about 5.3 seconds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The set membership testing python solution runs in about 4.45 seconds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The compress based solution runs in about 3.4 seconds.&#xA;I suspect this may vary a lot depending on the highest line number we want, since the number of iterations will depend on the length of the boolean array. Here the highest line number was 15773768, so pretty high compared with the total number of lines.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried with python 3.6. I suspect that python 2.7 could be slightly faster, but haven't tested.&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-06-06T07:10:27.073" LastActivityDate="2017-06-06T07:10:27.073" CommentCount="6" />
  <row Id="456" PostTypeId="1" CreationDate="2017-06-05T16:36:54.550" Score="4" ViewCount="70" Body="&lt;p&gt;We are designing a CRISPR/Cas9 experiment and thinking of the down-stream data analyses. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any R packages for analysing raw NGS read count data from pooled genetic screens using CRIPSR/Cas9 to disrupt gene expression in a population of cells? I guess we will need to start with basics, i.e. sequences processing, data exploration, visualisation etc. &lt;/p&gt;&#xA;" OwnerUserId="573" LastActivityDate="2017-06-07T12:08:54.520" Title="R packages for data analyses of pooled CRISPR screens" Tags="&lt;r&gt;&lt;crispr-cas9&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="457" PostTypeId="1" CreationDate="2017-06-05T16:51:28.363" Score="6" ViewCount="64" Body="&lt;p&gt;Is it possible to draw e.g. 95% confidence ellipses around samples from the same group on the results from the plotMDS function under edgeR? If so, how?&lt;/p&gt;&#xA;" OwnerUserId="467" LastActivityDate="2017-06-06T14:18:36.500" Title="confidence ellipses for MDS plot in edgeR?" Tags="&lt;r&gt;&lt;edger&gt;" AnswerCount="1" CommentCount="5" FavoriteCount="0" />
  <row Id="458" PostTypeId="1" CreationDate="2017-06-05T17:15:39.767" Score="7" ViewCount="32" Body="&lt;p&gt;We have heard in the group that it is important to keep track of and to filter artifact regions when analysing data from functional genomics experiments, especially ChIP-seq. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here, we have seen pipelines that remove the ENCODE tracks i) before cross-correlations QCs, ii) after cross-correlations QC but before peak calling and iii) after peak calling. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;We have noticed that removal of the tracks does not affect significantly cross-correlation and peak-independent QCs. However, we are not sure whether peak calling should be done on the filtered tracks or not?&lt;/p&gt;&#xA;" OwnerUserId="375" LastActivityDate="2017-06-05T17:41:43.290" Title="When to account for the blacklisted genomic regions in ChIP-seq data analyses?" Tags="&lt;chip-seq&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="459" PostTypeId="2" ParentId="458" CreationDate="2017-06-05T17:41:43.290" Score="5" Body="&lt;p&gt;Aside: Cross-correlation is largely meaningless, regardless of what some of the ENCODE folks might argue. When we process our DEEP samples we don't even look at that value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regardless, if you're using SPP/phantomPeakQual for cross-correlation then note that it already removes the highest peaks from your dataset before computing the cross-correlation (in fact, it can remove most of the actual peaks too, which makes one further wonder what it's actually telling you). I don't know that this is actually documented anywhere, it's something I noticed when going through the code while pondering whether to implement it in deepTools. But at least it's ignoring these regions already :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, it's most convenient to just remove peaks overlapping blacklisted regions. In an ideal world you'd filter out the blacklisted reads before peak calling, but (1) this is really inconvenient (more time and disk required) and (2) I've never seen an appreciable gain in peak calling performance. In theory at least you should be losing sensitivity right around blacklisted regions if you don't remove reads overlapping blacklisted regions, but you have to ask yourself whether you want to trust such peaks anyway. For other QC steps, at least with deepTools we provide a parameter with every tool to specify a BED file of blacklisted regions to skip.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an aside, there are many fewer blacklisted regions in more recent genome builds (GRCh38 and GRCm38 at least), so this is less of an issue in general with them.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-05T17:41:43.290" CommentCount="0" />
  <row Id="460" PostTypeId="2" ParentId="29" CreationDate="2017-06-05T17:42:37.170" Score="2" Body="&lt;p&gt;Regarding the first part of the question: scRNA-seq is a rapidly developing field so it may be hard to talk about &quot;widely accepted tool for doing pseudo-temporal ordering from scRNAseq data&quot;. Few of the tools aiming to do just that include Monocole, Waterfall or Sincell (see this paper for references &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4122333/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4122333/&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second part is a bit more complex. Many aspects have come together here, e.g. cell-cycles, sub-populaitons identification and pseudo-temporal ordering, to get a true reflection of the biological processes. There are efforts on all (and more) these fronts (see again e.g. the above paper, though not the latest) and there are most likely people working on their integration. I'm not aware of any studies published yet at this depth though&lt;/p&gt;&#xA;" OwnerUserId="375" LastActivityDate="2017-06-05T17:42:37.170" CommentCount="0" />
  <row Id="461" PostTypeId="2" ParentId="422" CreationDate="2017-06-05T17:43:34.723" Score="2" Body="&lt;p&gt;I am not sure how appropriate it is at this point to still refer to sequencing as next-generation sequencing. The leading NGS technology is Illumina/Solexa that has been around for over 10 years at this point. 454 was around even earlier. It's not really &quot;next&quot; at this point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Opinions aside, I would also add &quot;third-generation sequencing&quot; to that list, referring to long read technologies like PacBio and Oxford Nanopore. See &lt;a href=&quot;https://biology.stackexchange.com/questions/21080/what-is-the-difference-between-second-and-third-generation-sequencing&quot;&gt;this question on Biology SE&lt;/a&gt; for more details.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-06-05T17:43:34.723" CommentCount="1" />
  <row Id="462" PostTypeId="1" CreationDate="2017-06-05T18:06:35.393" Score="6" ViewCount="84" Body="&lt;p&gt;We have whole genome sequencing data for patients (not-cancer) (n=60) and for healthy controls (n=20). The sequencing centre has provided us with the best practice bioinformatics analyses including reads mapping (.BAM) and variant calling using GATK (.vcf) as well as annotation (annotated .vcf and .gVCF). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What should be our next steps? We are interested to see if there are any differences (global and/or specific) between the groups. &lt;/p&gt;&#xA;" OwnerUserId="589" LastActivityDate="2017-06-06T15:26:23.913" Title="How to compare groups using WGS data?" Tags="&lt;wgs&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="463" PostTypeId="5" CreationDate="2017-06-05T19:19:33.367" Score="0" Body="&lt;p&gt;BAM was the first widely-adopted binary standard for storing NGS alignments. Its specification is openly maintained &lt;a href=&quot;https://github.com/samtools/hts-specs&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. BAM is a compressed format, using bgzf to compress reads in blocks. This is convenient, as it allows sorted files to be indexed and then rapid queried.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Questions with this tag should broadly fall into one of the following categories:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Questions regarding the standard itself (e.g., &quot;What is the relationship between the BAM header and alignments?&quot;).&lt;/li&gt;&#xA;&lt;li&gt;Questions regarding using BAM files (e.g., &quot;How do I sort a BAM file?&quot; or &quot;How do I ensure that all reads in one/more fastq files are contained in a BAM file?).&lt;/li&gt;&#xA;&lt;li&gt;When seeing errors, such as corrupt or incomplete BAM files.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Particularly for category 3, it is then vital to provide information regarding exactly how the BAM file was made, ideally including the version of upstream tools as well as the exact command used.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-05T20:44:04.017" LastActivityDate="2017-06-05T20:44:04.017" CommentCount="0" />
  <row Id="464" PostTypeId="4" CreationDate="2017-06-05T19:19:33.367" Score="0" Body="The &quot;Binary Alignment Map&quot; (BAM) format is one of the common binary formats used to store sequence alignment information. Questions should include this tag if they directly pertain to the format itself, details of BAM file usage, or errors relating to likely malformed BAM files." OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-05T20:43:58.390" LastActivityDate="2017-06-05T20:43:58.390" CommentCount="0" />
  <row Id="465" PostTypeId="2" ParentId="453" CreationDate="2017-06-05T19:54:57.300" Score="3" Body="&lt;p&gt;Perl should be fairly fast with this when using a hash set to store the list of lines. A structure like this also works for subsetting based on a field value, where the comparison would be with the field rather than &quot;$.&quot;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/perl&#xA;&#xA;use strict;&#xA;use warnings;&#xA;&#xA;my $lines_file = $ARGV[0];&#xA;my %include_lines = ();&#xA;&#xA;open my $lines_fh, '&amp;lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xA;while(&amp;lt;$lines_fh&amp;gt;){&#xA;  chomp;&#xA;  $include_lines{$_} = 1;&#xA;}&#xA;close $lines_fh;&#xA;&#xA;while(&amp;lt;&amp;gt;){&#xA;  if($include_lines{$.}){ # &quot;$.&quot; -- line number of current file&#xA;    print;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note that according to &lt;a href=&quot;https://stackoverflow.com/questions/5920686/how-to-get-the-current-line-number-of-a-file-open-using-perl#comment6821096_5920709&quot;&gt;this SO answer&lt;/a&gt;, the &quot;$.&quot; operator is not strictly the current line number, and can be influenced by different file operations or other settings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: just saw your comment about speed in your answer, comparing hash sets to a sorted list. The &lt;code&gt;$lines[$next_line]&lt;/code&gt; bit feels a bit odd to me. Have you tried out using &lt;code&gt;shift&lt;/code&gt; or &lt;code&gt;pop&lt;/code&gt; on a sorted list to fetch the next line:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/perl&#xA;&#xA;use strict;&#xA;use warnings;&#xA;&#xA;my $lines_file = $ARGV[0];&#xA;&#xA;open my $lines_fh, '&amp;lt;', $lines_file or die &quot;Cannot read file $lines_file&quot;;&#xA;chomp (my @lines = &amp;lt;$lines_fh&amp;gt;);&#xA;close $lines_fh;&#xA;&#xA;@lines = sort {$a &amp;lt;=&amp;gt; $b} @lines;&#xA;my $next_line = shift(@lines);&#xA;&#xA;while (&amp;lt;&amp;gt;) {&#xA;    if ($. == $next_line) {&#xA;        $next_line = shift(@lines);&#xA;        print;&#xA;        last if (!@lines);&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-06T08:58:49.697" LastActivityDate="2017-06-06T08:58:49.697" CommentCount="4" />
  <row Id="466" PostTypeId="2" ParentId="271" CreationDate="2017-06-05T21:05:51.263" Score="5" Body="&lt;p&gt;I just received a reply from 1000Genomes regarding this. I'll post it in its entirety below:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Looking at the example you mention, I find it difficult to come up with an&#xA;  interpretation of the information whereby the stated end seems to be correct,&#xA;  so believe that this may indeed be an error.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Since the v4.0 was created, however, new versions of VCF have been introduced,&#xA;  improving and correcting the specification. The current version is v4.3&#xA;  (&lt;a href=&quot;http://samtools.github.io/hts-specs/&quot; rel=&quot;noreferrer&quot;&gt;http://samtools.github.io/hts-specs/&lt;/a&gt;). I believe the first record shown on&#xA;  page 11 provides an accurate example of this type of deletion.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;I will update the web page to include this information.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So we can take this as official confirmation that we were all correct in suspecting the example was just wrong.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-05T21:05:51.263" CommentCount="1" />
  <row Id="467" PostTypeId="2" ParentId="462" CreationDate="2017-06-05T21:16:01.070" Score="1" Body="&lt;p&gt;I'm not familiar with the program, but apparently &lt;a href=&quot;https://hail.is/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hail&lt;/a&gt; is setting itself up as a swiss-army chainsaw project for doing downstream analysis on variant-called datasets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An overview of Hail can be found here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2017/05/hail-scalable-genomics-analysis-with-spark/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://blog.cloudera.com/blog/2017/05/hail-scalable-genomics-analysis-with-spark/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A tutorial on association testing can be found here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://hail.is/hail/tutorial.html#Association-testing&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://hail.is/hail/tutorial.html#Association-testing&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-05T21:16:01.070" CommentCount="0" />
  <row Id="468" PostTypeId="2" ParentId="411" CreationDate="2017-06-06T02:47:36.653" Score="2" Body="&lt;p&gt;In general, the best way to download SRA data is: don't download from SRA. However, as ENA has not be sync'd yet, I would recommend to download from SRA ftp and then convert to fastq locally. You can find files in the SRA format &lt;a href=&quot;https://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. Downloading and then converting locally is much faster than direct retrieval from NCBI for some mysterious reasons.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-06T02:47:36.653" CommentCount="1" />
  <row Id="469" PostTypeId="2" ParentId="411" CreationDate="2017-06-06T07:56:48.763" Score="2" Body="&lt;p&gt;By far the fastest method in my experience has been to use the &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/SRAdb.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;SRAdb&lt;/a&gt; library in R. For most entries, you can download fastq files directly. Some older experiments don't have them, but I've still found it much faster to download SRA files via &lt;code&gt;getSRAfile()&lt;/code&gt; and then to convert them using &lt;code&gt;fastqdump&lt;/code&gt; than to use &lt;code&gt;fastqdump&lt;/code&gt; directly. &lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2017-06-06T07:56:48.763" CommentCount="1" />
  <row Id="470" PostTypeId="5" CreationDate="2017-06-06T08:05:14.003" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-06T08:05:14.003" LastActivityDate="2017-06-06T08:05:14.003" CommentCount="0" />
  <row Id="471" PostTypeId="4" CreationDate="2017-06-06T08:05:14.003" Score="0" Body="These questions are about sequence alignment. " OwnerUserId="450" LastEditorUserId="450" LastEditDate="2017-06-06T09:16:52.090" LastActivityDate="2017-06-06T09:16:52.090" CommentCount="0" />
  <row Id="472" PostTypeId="5" CreationDate="2017-06-06T08:08:22.230" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-06T08:08:22.230" LastActivityDate="2017-06-06T08:08:22.230" CommentCount="0" />
  <row Id="473" PostTypeId="4" CreationDate="2017-06-06T08:08:22.230" Score="0" Body="Questions specific to nanopore sequencing. For general question about long reads, use tag long-reads instead and for questions about specific sequencer use a specific sequencer tag (i.e. minion, gridion)" OwnerUserId="450" LastEditorUserId="57" LastEditDate="2017-06-08T11:42:01.353" LastActivityDate="2017-06-08T11:42:01.353" CommentCount="0" />
  <row Id="474" PostTypeId="5" CreationDate="2017-06-06T08:10:34.667" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-06T08:10:34.667" LastActivityDate="2017-06-06T08:10:34.667" CommentCount="0" />
  <row Id="475" PostTypeId="4" CreationDate="2017-06-06T08:10:34.667" Score="0" Body="Questions specific to interacting with and post-processing short DNA sequence read alignments in the SAM (Sequence Alignment/Map), BAM (Binary Alignment/Map) or CRAM formats, using the SAMtools package" OwnerUserId="450" LastEditorUserId="450" LastEditDate="2017-06-06T09:16:33.847" LastActivityDate="2017-06-06T09:16:33.847" CommentCount="0" />
  <row Id="476" PostTypeId="2" ParentId="414" CreationDate="2017-06-06T08:19:37.980" Score="2" Body="&lt;p&gt;For ChIP-seq it shouldn't really matter. But do be aware that by default, &lt;code&gt;samtools merge&lt;/code&gt; retains read group information (the &lt;code&gt;@RG&lt;/code&gt; field in the header) from each input file. This could pose a problem for some downstream analyses (e.g. for the GATK HaplotypeCaller) if you want the merged data to be considered as all part of the same sample. You can change this behaviour using the &lt;code&gt;-c&lt;/code&gt; option. &lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2017-06-06T08:19:37.980" CommentCount="2" />
  <row Id="477" PostTypeId="2" ParentId="456" CreationDate="2017-06-06T09:05:07.703" Score="3" Body="&lt;p&gt;Assuming you mean CRISPR screens targetting many loci (for example using the GeCKO library) there is an R package here: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://git.embl.de/msmith/geckoR&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://git.embl.de/msmith/geckoR&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It uses a linear mixed effect model to compare guide counts before and after a selection step, allowing for multiple guides/gene and multiple replicates. &#xA;It can also do the initial read alignment afaik and the author should be quite responsive in case you have questions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another opiton could be the &lt;em&gt;python&lt;/em&gt; program MAGeCK (&lt;a href=&quot;https://bitbucket.org/liulab/mageck-vispr&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://bitbucket.org/liulab/mageck-vispr&lt;/a&gt;)&lt;/p&gt;&#xA;" OwnerUserId="595" LastActivityDate="2017-06-06T09:05:07.703" CommentCount="0" />
  <row Id="478" PostTypeId="2" ParentId="441" CreationDate="2017-06-06T10:57:34.970" Score="3" Body="&lt;p&gt;The suggestion to use proportionality is probably the correct one if you are interested in similar &lt;em&gt;patterns&lt;/em&gt; between samples. But not if you are interested in &lt;em&gt;absolute&lt;/em&gt; differences. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: the following two samples are similar in pattern, but not similar on absolute levels:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        Sample 1    Sample 2    Sample 3&#xA;Gene A    10          100           80&#xA;Gene B     8           80          100  &#xA;Gene C    12          120          120&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Samples 1 and 2 have a perfect proportionality (phi is 0) and also a perfect correlation (as a side note any pair with a perfect proportionality will always have a perfect correlation). However, in terms of logfold changes, samples 2 and 3 are more like each other. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course in real life you would never see a comparison like the sample 1 - sample 2 one because normalisation would have removed the scale difference. This was exactly the point brought up by the proportionality paper. But normalisation methods don't normally guarantee that the sum of expression for each sample is identical, and such differences can still occur. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;An alternative that might suit more in the second case is either the euclidean distance between the samples, or the euclidean distance on the first two components of a principle component or multi-dimensional scaling. The latter is effectively using mean logFC between samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assuming that &lt;code&gt;x&lt;/code&gt; is a matrix containing normalised, log transformed expression values, you could use R and limma to calculate distance in multi-dimensional scaled space as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(limma)&#xA;mds &amp;lt;- plotMDS(x, plot=FALSE)&#xA;mds &amp;lt;- data.frame(mds$x, mds$y)&#xA;distances &amp;lt;- dist(mds)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-06-06T10:57:34.970" CommentCount="2" />
  <row Id="479" PostTypeId="2" ParentId="321" CreationDate="2017-06-06T11:07:55.663" Score="1" Body="&lt;p&gt;Ideally these BQSR methods were made keeping in mind how technical errors will actually screw up the base quality calls and when the machines were still more on development phase while being used for the 1000G project. As of now machines are more powerful and strong where it will be unlikely to use it but still we use with listed SNPs to find the covariates and build a model around the data using the information with machine learning trciks to improve the quality of those base calls. Ideally it should be more appropriate when old machines from Illumina or other standard companies are being used but with new machines which are much powerful and having high throughput they should tend to go down. I do not recall if such tests have been made but obviously I know new sequencing machine always make such tests to show that they have reduced such errors but still recommend such BQSR for variant calls. Now the problem is the list of SNPs, this to me is the real problem since the list we use is far from being Gold standard and if that is not properly taken care of everything we infer about quality is still shaky. This link is pretty informative &lt;a href=&quot;http://Ideally%20these%20BQSR%20methods%20were%20made%20keeping%20in%20mind%20how%20technical%20errors%20will%20actually%20screw%20up%20the%20base%20quality%20calls%20and%20when%20the%20machines%20were%20still%20more%20on%20development%20phase%20while%20being%20used%20for%20the%201000G%20project.%20As%20of%20now%20machines%20are%20more%20powerful%20and%20strong%20where%20it%20will%20be%20unlikely%20to%20use%20it%20but%20still%20we%20use%20with%20listed%20SNPs%20to%20find%20the%20covariates%20and%20build%20a%20model%20around%20that%20to%20improve%20the%20quality%20of%20those%20base%20calls.%20Ideally%20it%20should%20be%20more%20appropriate%20when%20old%20machines%20from%20Illumina%20or%20other%20standard%20companies%20are%20being%20used%20but%20with%20new%20machines%20which%20are%20much%20powerful%20and%20having%20high%20throughput%20they%20should%20tend%20to%20go%20down.%20I%20do%20not%20recall%20if%20such%20tests%20have%20been%20made%20but%20obviously%20I%20know%20new%20sequencing%20machine%20always%20make%20such%20tests%20to%20show%20that%20they%20have%20reduced%20such%20errors%20but%20still%20recommend%20such%20BQSR%20for%20variant%20calls.%20Now%20the%20problem%20is%20the%20list%20of%20SNPs,%20this%20to%20me%20is%20the%20real%20problem%20since%20the%20list%20we%20use%20is%20far%20from%20being%20Gold%20standard%20and%20if%20that%20is%20not%20properly%20taken%20care%20of%20everything%20we%20infer%20about%20quality%20is%20still%20shaky.%20This%20link%20is%20pretty%20informative%20http://gatkforums.broadinstitute.org/gatk/discussion/44/base-quality-score-recalibration-bqsr%20but%20its%20an%20old%20one.%20I%20would%20really%20see%20improvements%20with%20new%20sequencers.&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://gatkforums.broadinstitute.org/gatk/discussion/44/base-quality-score-recalibration-bqsr&lt;/a&gt; but its an old one. I would really see improvements with new sequencers.  However very less people care about such tests in academic research and also translational lab will really not invest time and money on such unless the facility has some bioinformaticians who always does such testing while buying a new sequencer for the institute. In terms of clinical genomics for finding variants I reckon most powerful and up-to-date sequencers should be used but not sure if they still use BQSR and if so what is the list they use to build model of covariation around the data.&lt;/p&gt;&#xA;" OwnerUserId="451" LastActivityDate="2017-06-06T11:07:55.663" CommentCount="7" />
  <row Id="480" PostTypeId="2" ParentId="457" CreationDate="2017-06-06T11:28:46.080" Score="1" Body="&lt;p&gt;I can see that there would be a three step process to doing this:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Merge counts from all samples in the group and then resample pseudo-replicates from this. If &lt;code&gt;x&lt;/code&gt; is a matrix with samples in the group being columns and genes being rows&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;s &amp;lt;- sample(row.names(x), n = mean(colSums(x)), probs=rowSums(x)/sum(rowSums(x))&#xA;stab &amp;lt;- table(s)&#xA;s &amp;lt;- as.vector(stab)&#xA;names(s) &amp;lt;- names(stab)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do this thousands of times.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;You would then want to project those onto your MDS plot space - i'm less clear about how you could do this without perturbing the space itself.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Calculate the parameter for an ellipise that would contain 95% of these points. Again, I'm not so sure about how to do this, but I think the &lt;code&gt;car&lt;/code&gt; pacakge might be a good place to start looking.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="235" LastEditorUserId="29" LastEditDate="2017-06-06T14:18:36.500" LastActivityDate="2017-06-06T14:18:36.500" CommentCount="0" />
  <row Id="481" PostTypeId="1" CreationDate="2017-06-06T12:44:58.087" Score="3" ViewCount="58" Body="&lt;p&gt;&lt;em&gt;This is a question from &lt;a href=&quot;https://www.reddit.com/user/beneficii9&quot; rel=&quot;nofollow noreferrer&quot;&gt;/u/beneficii9&lt;/a&gt; on reddit. The original post can be found &lt;a href=&quot;https://www.reddit.com/r/bioinformatics/comments/6f040g/how_do_you_do_admixture_testing_with_a_whole/&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Through the Personal Genome Project, I have had my whole genome sequenced by Veritas, and have it in the form of a single VCF file for the whole genome and one BAS file for each chromosome. The reference genome associated with the VCF file is hg19. It has been helpful in health data; for example, I discovered I'm homozygous for the non-functional variant CYP-2D6 gene (&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=3892097&quot; rel=&quot;nofollow noreferrer&quot;&gt;rs3892097&lt;/a&gt;), which can render several common medications useless, and helps explain why some medicines didn't really work for me. My doctor has found this information very helpful.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Unfortunately, I can't find any way of looking at admixture or ancestry. I've tried setting everything up using a combination of VCFTools, Plink1.9, and ADMIXTURE, but I can't get it to work. I think for ADMIXTURE you have to have a bunch of genomes sorted by geographical origin to compare your genome against, but I'm not sure how to do that, and what's online isn't very clear to me. So scratch that one off.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;I've tried converting the file to 23andme format (and at this &lt;a href=&quot;https://www.reddit.com/u/psychosomaticism&quot; rel=&quot;nofollow noreferrer&quot;&gt;/u/psychosomaticism&lt;/a&gt; has been very helpful). I did that (though it seems there were problems because of the way the VCF file was set up). But the websites that take the data want you to point them to your 23andme account, and that doesn't really work if you only have the file. 23andme doesn't provide for people who had their whole genomes sequenced. They want you to give them a saliva sample like everyone else.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;So, what can I do?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-08T11:50:57.103" LastActivityDate="2017-06-08T11:50:57.103" Title="How do I carry out an ancestry/admixture test on a single VCF file?" Tags="&lt;vcf&gt;&lt;ancestry&gt;&lt;personal-genomics&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="482" PostTypeId="5" CreationDate="2017-06-06T12:47:47.490" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-06T12:47:47.490" LastActivityDate="2017-06-06T12:47:47.490" CommentCount="0" />
  <row Id="483" PostTypeId="4" CreationDate="2017-06-06T12:47:47.490" Score="0" Body="Posts originally from reddit, where the question is well-structured and fits in with the StackExchange question/answer format. StackExchange questions should link to the original post at the top of the question." OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-06T12:56:19.700" LastActivityDate="2017-06-06T12:56:19.700" CommentCount="0" />
  <row Id="484" PostTypeId="2" ParentId="481" CreationDate="2017-06-06T13:03:11.313" Score="3" Body="&lt;p&gt;A suggestion from the UK Biobank QC methods white paper:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a set of SNPs common to your VCF and the 1000 genomes phase 3 call set.&lt;/li&gt;&#xA;&lt;li&gt;Perform PCA of the 1000 genomes samples using eigenstrat smartpca. You might have to convert to binary plink format.&lt;/li&gt;&#xA;&lt;li&gt;Project your genotypes onto this pre-computed PCA space and visualize using ggplot to see which cluster you fall in.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="601" LastActivityDate="2017-06-06T13:03:11.313" CommentCount="0" />
  <row Id="485" PostTypeId="2" ParentId="481" CreationDate="2017-06-06T13:16:05.703" Score="1" Body="&lt;p&gt;Ancestry testing is a tricky subject. I spent a good chunk of &lt;a href=&quot;http://researcharchive.vuw.ac.nz/handle/10063/1987&quot; rel=&quot;nofollow noreferrer&quot;&gt;my PhD project&lt;/a&gt; on questions similar to this, and haven't really found a good answer for how to detect unmodelled ancestry.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The general idea of how I tried to approach ancestry determination was to create a model set containing well-defined groups of individuals with a known, specific ancestral background. A query individual (or individuals) were then added to this group, and an ancestry-estimating program was run (specifically &lt;a href=&quot;http://web.stanford.edu/group/pritchardlab/structure.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Structure&lt;/a&gt;) to work out what proportion of the unknown individual was attributable to each of the known groups. I would expect that most genetic ancestry tests follow a similar approach to this, although perhaps with a little bit less care about the definition of the model population groups.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Problems arise when individuals are incorrectly assigned to a particular group, when groups are present in the model set that represent a larger proportion of individuals than most other groups, when groups are closely-related to other groups, and when an ancestral history is present in a test individual that doesn't match any of the model groups. And all that assumes that the marker set used for ancestry determination is perfect: no bias towards any particular group, and no systematic genotyping error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This doesn't mean that ancestry testing won't work, but it's a good idea to take the results with a large grain of salt. There's a &lt;a href=&quot;https://www.theguardian.com/world/2017/apr/13/a-dna-test-showed-im-100-maori-many-thought-there-were-none-of-us-left&quot; rel=&quot;nofollow noreferrer&quot;&gt;good example&lt;/a&gt; of a media personality in New Zealand who was told that she had a very high probability of being 100% Māori, despite having a good knowledge of her own family history that indicated one European ancestor a few generations back on both sides of her family.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-06T13:16:05.703" CommentCount="0" />
  <row Id="486" PostTypeId="1" CreationDate="2017-06-06T13:27:44.753" Score="1" ViewCount="97" Body="&lt;p&gt;&lt;em&gt;This is a question from &lt;a href=&quot;https://www.reddit.com/user/wipeyourmit&quot; rel=&quot;nofollow noreferrer&quot;&gt;/u/wipeyourmit&lt;/a&gt; on reddit. The original post can be found &lt;a href=&quot;https://www.reddit.com/r/bioinformatics/comments/6dfu3c/question_about_eggnog_mapper_and_annotation_in/&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If I have a metagenomic dataset that contains reads from both&#xA;  eukaryotes and prokaryotes and then I annotate by running DIAMOND or&#xA;  HMMER against a bacterial database how much of a risk do I run of&#xA;  eukaryotic reads being annotated in the process?&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;I was hoping to use the eggNOG mapper to search against the bacterial&#xA;  and archaeal databases and to exclude the eukaryotic portion of my&#xA;  dataset. Is the eukaryotic filtering something that I would need to do&#xA;  in a step prior to this?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="73" LastEditorUserId="55" LastEditDate="2017-06-08T11:43:24.667" LastActivityDate="2017-06-08T11:43:24.667" Title="Accidental mapping of eukaryotic reads in a metagenomic dataset" Tags="&lt;read-mapping&gt;&lt;metagenome&gt;" AnswerCount="3" CommentCount="4" />
  <row Id="487" PostTypeId="2" ParentId="486" CreationDate="2017-06-06T13:31:44.300" Score="0" Body="&lt;p&gt;&lt;em&gt;Answer from &lt;a href=&quot;https://www.reddit.com/user/Romanticon&quot; rel=&quot;nofollow noreferrer&quot;&gt;/u/Romanticon&lt;/a&gt; on reddit. The original answer can be found &lt;a href=&quot;https://www.reddit.com/r/bioinformatics/comments/6dfu3c/question_about_eggnog_mapper_and_annotation_in/di2qv4a/&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Depending on the length of your reads, the level of error in the&#xA;  reads, the quality of the database, and the specificity settings you&#xA;  give DIAMOND, you can change the level of &quot;bleed&quot; that you get - but&#xA;  you'll always have a couple reads, especially at metagenome dataset&#xA;  sizes, that will be annotated incorrectly.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;But you'll always have this happen, whenever you annotate any large&#xA;  dataset. The best way to handle it is to set a threshold after&#xA;  annotation that removes wrongly annotated reads (looking for 'clearly&#xA;  wrong' organisms in your annotated dataset can help you figure out&#xA;  where to set the threshold level - there's probably no Bos taurus&#xA;  reads in a mouse metagenome).&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;You can do things like enable the sensitive flag on the DIAMOND&#xA;  annotation search to help improve annotation accuracy.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="73" LastEditorUserId="298" LastEditDate="2017-06-07T10:45:03.643" LastActivityDate="2017-06-07T10:45:03.643" CommentCount="0" CommunityOwnedDate="2017-06-06T13:31:44.300" />
  <row Id="488" PostTypeId="1" AcceptedAnswerId="490" CreationDate="2017-06-06T13:38:19.540" Score="8" ViewCount="205" Body="&lt;p&gt;&lt;em&gt;This is a question from &lt;a href=&quot;https://www.reddit.com/user/apivan19&quot; rel=&quot;noreferrer&quot;&gt;/u/apivan19&lt;/a&gt; on reddit. The original post can be found &lt;a href=&quot;https://www.reddit.com/r/bioinformatics/comments/6ddvkv/converting_gene_names/&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have some proteomics data that was given to me with the UniProt gene identifiers in column 1. I've been trying to convert these to normal gene symbols using various programs, but it is proving to be difficult.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Uniprot website does it fairly decently, but it is not able to convert all of them and then adds some unknown genes into my list.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, I will give it 5439 genes in UniProt notation, and will say &quot;5420 of 5439 UniProt identifiers have been converted to 5450 gene symbols&quot;... which is ridiculous.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried using David to change the symbols, but it returns them to me in some ridiculous, random order and there's no way I can sort... actually there might be but it'll take a second.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are some of the easiest ways to do this? It's already very time consuming and am looking for simpler solutions&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="55" LastEditDate="2017-06-08T11:43:09.197" LastActivityDate="2017-07-11T09:18:11.997" Title="Converting gene names from one public database format to another" Tags="&lt;database&gt;&lt;gene&gt;&lt;conversion&gt;" AnswerCount="6" CommentCount="0" />
  <row Id="489" PostTypeId="2" ParentId="488" CreationDate="2017-06-06T13:46:44.057" Score="3" Body="&lt;p&gt;My favourite gene database conversion site is &lt;a href=&quot;https://biodbnet-abcc.ncifcrf.gov/db/db2db.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;db2db&lt;/a&gt;. You provide a list of IDs in one of a large number of different public formats, and can select one or more IDs as translation targets. It will then walk through various known paths to do the translation, picking what it determines to be the most reliable route to get the information you have asked for. Results appear in the browser as a table, but can also be exported as an Excel file, or as a tab-separated text file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that the mapping of genes from one database to another is not a one-to-one mapping. It is likely the case that there will be some genes in the source database that map to multiple genes in the target database (and &lt;em&gt;vice versa&lt;/em&gt;), and some genes that aren't present in the target database. These phenomena probably account for the &quot;ridiculous&quot; results that have been seen here.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-06T13:46:44.057" CommentCount="1" />
  <row Id="490" PostTypeId="2" ParentId="488" CreationDate="2017-06-06T14:16:38.920" Score="8" Body="&lt;p&gt;I tend to use &lt;a href=&quot;http://www.ensembl.org/biomart&quot; rel=&quot;noreferrer&quot;&gt;Ensembl Biomart&lt;/a&gt; for such queries since there are APIs for various programming languages, e.g. &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/biomaRt.html&quot; rel=&quot;noreferrer&quot;&gt;biomaRt&lt;/a&gt;, and, maybe more interestingly, via a &lt;a href=&quot;http://www.ensembl.org/info/data/biomart/biomart_restful.html&quot; rel=&quot;noreferrer&quot;&gt;REST API&lt;/a&gt; (although it’s a pretty terrible one).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To translate identifiers from different databases, proceed as follows:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Choose database “Ensembl genes”&lt;/li&gt;&#xA;&lt;li&gt;Choose dataset &lt;em&gt;your desired oganism&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Go on “Filters” › “Gene:” › “Input external reference ID list”&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Select the chosen source database&lt;/li&gt;&#xA;&lt;li&gt;Provide a list of IDs, delimited by newline&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;li&gt;Go to “Attributes” › “Gene:” › untick “Transcript stable ID”&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;If Ensembl IDs are desired, leave “Gene stable ID” ticked …&lt;/li&gt;&#xA;&lt;li&gt;Otherwise untick it; go to “External:”, tick your desired identifier format&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;li&gt;Click “Results” at the top left. This gives a preview that can be exported into various formats; alternatively the top centre buttons “XML” and “Perl” provide the query in XML (for SOAP/REST requests) and as a (horrendously formatted) executable Perl script.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-06-06T14:16:38.920" CommentCount="2" />
  <row Id="491" PostTypeId="1" AcceptedAnswerId="501" CreationDate="2017-06-06T14:29:59.870" Score="3" ViewCount="133" Body="&lt;p&gt;I'm fairly new to &lt;a href=&quot;https://snakemake.readthedocs.io/en/stable/&quot; rel=&quot;nofollow noreferrer&quot;&gt;snakemake&lt;/a&gt; and I'm trying to understand the difference between the &lt;code&gt;--cluster&lt;/code&gt; and &lt;code&gt;--drmaa&lt;/code&gt; flags, both of which are used to submit jobs to compute clusters/nodes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The docs give a few hints about the advantages of using &lt;code&gt;--drmaa&lt;/code&gt; &lt;a href=&quot;http://snakemake.readthedocs.io/en/stable/executable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If your cluster system supports DRMAA, Snakemake can make use of that&#xA;  to increase the control over jobs. E.g. jobs can be cancelled upon&#xA;  pressing Ctrl+C, which is not possible with the generic --cluster&#xA;  support.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And &lt;a href=&quot;http://snakemake.readthedocs.io/en/stable/tutorial/additional_features.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If available, DRMAA is preferable over the generic cluster modes&#xA;  because it provides better control and error handling.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So I have a conceptual understanding of the advantages of using &lt;code&gt;--drmaa&lt;/code&gt;. However, I don't consider the above a very complete explanation, and I don't know how these flags are implemented in snakemake under the hood, can someone elaborate?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note, that although this could be considered a more general programming question, snakemake is predominantly used in bioinformatics; I was persuaded that this question would be considered on-topic because of &lt;a href=&quot;https://bioinformatics.meta.stackexchange.com/questions/61/serving-as-a-support-forum-for-specific-tools&quot;&gt;this meta-post&lt;/a&gt; and &lt;a href=&quot;https://bioinformatics.meta.stackexchange.com/a/66/104&quot;&gt;this answer&lt;/a&gt;. &lt;/p&gt;&#xA;" OwnerUserId="104" LastEditorUserId="73" LastEditDate="2017-06-06T21:58:34.553" LastActivityDate="2017-06-06T21:58:34.553" Title="How are snakemake's --cluster and --drmaa options implemented?" Tags="&lt;snakemake&gt;&lt;documentation&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="492" PostTypeId="2" ParentId="488" CreationDate="2017-06-06T14:37:23.240" Score="3" Body="&lt;p&gt;I'm not a huge fan of the Ensembl BioMart system because I find it difficult to use. The &lt;a href=&quot;http://llama.mshri.on.ca/synergizer/doc/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Synergizer&lt;/a&gt; has a very straightforward interface and works pretty well for most lists. Note: it hasn't been updated in a while.&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2017-06-06T14:37:23.240" CommentCount="0" />
  <row Id="493" PostTypeId="2" ParentId="327" CreationDate="2017-06-06T15:04:52.107" Score="2" Body="&lt;p&gt;The kind of QC you do routinely depends on what your lab's focus is. We do a lot of low-quality, multiplexed DNA and RNA. If you routinely do fresh frozen whole genomes, your QC will be different.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Weighing in from the resequencing side of things (i.e. sequencing an organism that has a good reference), there are several things we are testing with quality control:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Did the library preparation succeed? Was the capture successful? Is our input material of high enough quality? &lt;/li&gt;&#xA;&lt;li&gt;Did the sequencing work? For Illumina, did enough clusters pass filter? What's our Q30? Did we sequence into adapters?&lt;/li&gt;&#xA;&lt;li&gt;Can we proceed with analysis? Are the indices on multiplexed samples correct?&#xA;Do we have the correct reference sequence? Do we have the right targets?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;You may use a combination of tools:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://support.illumina.com/sequencing/sequencing_software/bcl2fastq-conversion-software.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;bcl2fastq&lt;/a&gt; : if you multiplexed, check the undetermined indices file after converting to fastq to make sure you caught all of the indexed reads&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.bioinformatics.babraham.ac.uk/projects/fastqc/&quot; rel=&quot;nofollow noreferrer&quot;&gt;FastQC&lt;/a&gt;: General sequencing quality, sequencing into adapters, handy flagging for obvious problems&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/Illumina/interop&quot; rel=&quot;nofollow noreferrer&quot;&gt;Illumina interop&lt;/a&gt;/SAV: Cluster pass filter, Q30&lt;/li&gt;&#xA;&lt;li&gt;Alignment (with &lt;a href=&quot;http://bio-bwa.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;BWA&lt;/a&gt;, bowtie, etc), &lt;a href=&quot;http://bedtools.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;bedtools&lt;/a&gt;, &lt;a href=&quot;http://www.htslib.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;samtools&lt;/a&gt;, &lt;a href=&quot;https://broadinstitute.github.io/picard/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Picard&lt;/a&gt;: check for contamination, make sure we're using the right reference, overlap with target regions, depth of coverage, soft-clipping for low-quality sequence&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="110" LastActivityDate="2017-06-06T15:04:52.107" CommentCount="0" />
  <row Id="494" PostTypeId="2" ParentId="462" CreationDate="2017-06-06T15:26:23.913" Score="2" Body="&lt;p&gt;If you have gVCFs, the first thing you should try is joint variant calling. According to GATK, joint variant calling &quot;empowers variant discovery by providing the ability to leverage population-wide information from a cohort of multiple sample[sic], allowing us to detect variants with great sensitivity and genotype samples as accurately as possible.&quot; &lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/article.php?id=4150&quot; rel=&quot;nofollow noreferrer&quot;&gt;source&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have two sets of high-quality variants, what you do next depends on your research question. Are you looking for druggable mutations? Underlying causes? Biomarkers? Patient prognosis? You can look at the most frequently mutated positions in the patient cohort and compare them to your reference cohort, check for co-occurring mutations, cluster them, do principal component analysis, do some machine learning to stratify them, etc. &lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2017-06-06T15:26:23.913" CommentCount="0" />
  <row Id="495" PostTypeId="2" ParentId="491" CreationDate="2017-06-06T15:31:11.623" Score="4" Body="&lt;p&gt;&lt;strong&gt;DRMAA&lt;/strong&gt; (Distributed Resource Management Application API) appears to be an open API that describes a specification for submission and management of work submitted to some grid/cluster. &lt;a href=&quot;https://en.wikipedia.org/wiki/DRMAA#Implementations&quot; rel=&quot;nofollow noreferrer&quot;&gt;If your scheduler is DRMAA compliant&lt;/a&gt;, my guess would be that using &lt;code&gt;snakemake&lt;/code&gt;'s &lt;code&gt;--drmaa&lt;/code&gt; flag will afford you the additional controls exposed by that API.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As mentioned in your question with DRMAA support, a Ctrl+C on one's console would be passed on to the grid to kill or stop submitted jobs. A task that would otherwise require use of the &lt;code&gt;qdel&lt;/code&gt; command on &lt;code&gt;Sun Grid Engine&lt;/code&gt;, for example.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could see why the suggestion would be to enable &lt;code&gt;--drmaa&lt;/code&gt; where possible as it's likely to expose functionality (such as control of jobs, and availability of logs and errors) that makes the submission and management of jobs a little easier for you and &lt;code&gt;snakemake&lt;/code&gt;. Perhaps someone with &lt;code&gt;snakemake&lt;/code&gt; experience can explain a little more, but I hope this helps?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For what it's worth, I use Sun Grid Engine without DRMAA and it's quite a pain to get good information out of &lt;code&gt;qstat&lt;/code&gt;, and its &lt;code&gt;qacct&lt;/code&gt; log format is quite possibly the worst file format I have ever encountered. I suspect DRMAA provides a nice (or at least more reasonable) API for &lt;code&gt;snakemake&lt;/code&gt; (and others) to get at this information more readily.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Appended:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Under the hood, the flags are parsed by Python's &lt;code&gt;argparse&lt;/code&gt; module in the &lt;code&gt;__init__.py&lt;/code&gt;. &lt;a href=&quot;https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/__init__.py?fileviewer=file-view-default#__init__.py-747&quot; rel=&quot;nofollow noreferrer&quot;&gt;A mutually exclusive group of options&lt;/a&gt; forces a selection of &lt;code&gt;--cluster&lt;/code&gt; or &lt;code&gt;--drmaa&lt;/code&gt; (or &lt;code&gt;--cluster-sync&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;--cluster&lt;/code&gt; or &lt;code&gt;-c&lt;/code&gt; requires you to specify an argument; the command for which to submit a job on your cluster (the example names &lt;code&gt;qsub&lt;/code&gt;). The &lt;code&gt;--drmaa&lt;/code&gt; flag just seems to indicate to &lt;code&gt;snakemake&lt;/code&gt; that DRMAA is to be used, which doesn't change a lot in terms of how the command you run from your console is processed in the &lt;code&gt;__init__.py&lt;/code&gt; or &lt;a href=&quot;https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/workflow.py?at=master&amp;amp;fileviewer=file-view-default&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;workflow.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, when it is time to interface with your scheduler (as specified in &lt;a href=&quot;https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/scheduler.py?at=master&amp;amp;fileviewer=file-view-default&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;scheduler.py&lt;/code&gt;&lt;/a&gt;), an &lt;code&gt;elif&lt;/code&gt; statement checks whether you're in some form of cluster mode and the &lt;code&gt;else&lt;/code&gt; catches the case where you've raised the &lt;code&gt;--drmaa&lt;/code&gt; flag instead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's where the magic happens, as now your job is submitted with the &lt;code&gt;DRMAAExecutor&lt;/code&gt; as specified in &lt;a href=&quot;https://bitbucket.org/snakemake/snakemake/src/e7bdc7587c6e41113b972e3eba1b9404e741873c/snakemake/executors.py?at=master&amp;amp;fileviewer=file-view-default&quot; rel=&quot;nofollow noreferrer&quot;&gt;executors.py&lt;/a&gt;, instead of the default executor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Without investigating much more, I can see that the &lt;code&gt;DRMAAExecutor&lt;/code&gt; features some more class attributes and exposes additional functions when compared to the others.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hope this is of more use? Again, I've only just looked at the code myself but in lieu of a &lt;code&gt;snakemake&lt;/code&gt; user chiming in, I thought I'd have a nose.&lt;/p&gt;&#xA;" OwnerUserId="215" LastEditorUserId="215" LastEditDate="2017-06-06T18:54:34.823" LastActivityDate="2017-06-06T18:54:34.823" CommentCount="5" />
  <row Id="496" PostTypeId="2" ParentId="414" CreationDate="2017-06-06T17:40:13.550" Score="2" Body="&lt;p&gt;Agree with the others that it doesn't really matter. One thing to note though - if you're deduplicating your BAM files (you probably should for ChIP-seq data), make sure that you do this &lt;em&gt;after&lt;/em&gt; merging.. :)&lt;/p&gt;&#xA;" OwnerUserId="224" LastActivityDate="2017-06-06T17:40:13.550" CommentCount="1" />
  <row Id="497" PostTypeId="2" ParentId="488" CreationDate="2017-06-06T17:52:17.670" Score="7" Body="&lt;p&gt;If you're comfortable doing a little programming, check out &lt;a href=&quot;http://mygene.info&quot; rel=&quot;nofollow noreferrer&quot;&gt;mygene.info&lt;/a&gt; (web services for gene annotations of all sorts).  ID translation is specifically one of the use cases addressed in the &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/mygene.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;bioconductor client&lt;/a&gt; (see the &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/vignettes/mygene/inst/doc/mygene.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;vignette&lt;/a&gt;), and there is a &lt;a href=&quot;https://pypi.python.org/pypi/mygene&quot; rel=&quot;nofollow noreferrer&quot;&gt;python client&lt;/a&gt; as well available through pypi. The documentation for mygene can be found &lt;a href=&quot;http://mygene-py.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="612" LastEditorUserId="73" LastEditDate="2017-06-06T19:53:55.077" LastActivityDate="2017-06-06T19:53:55.077" CommentCount="0" />
  <row Id="498" PostTypeId="2" ParentId="488" CreationDate="2017-06-06T18:32:15.170" Score="4" Body="&lt;p&gt;You can do the same using &lt;a href=&quot;https://www.bioconductor.org/packages/release/bioc/html/AnnotationDbi.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;AnnotationDbi&lt;/a&gt; Package from Bioconductor. Download the organism specific annotation file like &lt;a href=&quot;https://bioconductor.org/packages/release/data/annotation/html/org.Mm.eg.db.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;org.Mm.eg.db&lt;/a&gt; for mouse and map current gene ids to the gene names/gene symbols.&lt;/p&gt;&#xA;" OwnerUserId="206" LastActivityDate="2017-06-06T18:32:15.170" CommentCount="0" />
  <row Id="499" PostTypeId="2" ParentId="441" CreationDate="2017-06-06T19:05:40.117" Score="1" Body="&lt;p&gt;The euclidean distance is probably the simplest, both conceptually and in terms of implementation. It's by no means an elegant solution, and it may not perform well in certain circumstances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Euclidean distance is easiest to conceptualize as the distance between two points in a two dimensional space.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Y&#xA;^&#xA;|&#xA;|&#xA;|&#xA;|            * p = (3, 3)&#xA;|&#xA;|&#xA;|&#xA;|        * q = (2, 1)&#xA;|&#xA;----------------------------&amp;gt; X&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In this example, the distance between the two points is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;d(p, q) = sqrt( (p_x-q_x)^2 + (p_y-q_y)^2 )&#xA;        = sqrt( (2-3)^2 + (1-3)^2 )&#xA;        = sqrt(5)&#xA;        ≈ 2.24&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For a gene expression profile with two genes, this is exactly how the euclidean distance would be calculated, using expression values from one gene as the X axis and expression values from the other gene as the Y axis. Realistically, though, gene expression profiles typically contain thousands or tens of thousands of genes, so instead we use the generalization of the distance calculation for N dimensions.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;d(p, q) = sqrt( (p_1-q_1)^2 + (p_2-q_2)^2 + ... + (p_N-q_N)^2 )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Packages for R and Python make these types of calculations trivial once you have the data loaded into the correct data structure. See Ian's answer for some example R code.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-06-06T19:05:40.117" CommentCount="0" />
  <row Id="500" PostTypeId="1" AcceptedAnswerId="503" CreationDate="2017-06-06T19:19:47.827" Score="7" ViewCount="102" Body="&lt;p&gt;I am writing a software tool to which I would like to add the ability to compute alignments using the efficient Burrows-Wheeler Transform (BWT) approach made popular by tools such as BWA and Bowtie. As far as I can tell, though, both of these tools and their derivatives are invoked strictly via a command-line interface. Are there any libraries that implement BWT-based read alignment with a C/C++ API?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Python bindings would also be great, but probably too much to expect.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="298" LastEditDate="2017-06-07T12:39:30.463" LastActivityDate="2017-06-07T12:39:30.463" Title="Library for computing BWT-based alignments" Tags="&lt;alignment&gt;&lt;bwa&gt;&lt;read-mapping&gt;&lt;api&gt;" AnswerCount="3" CommentCount="1" />
  <row Id="501" PostTypeId="2" ParentId="491" CreationDate="2017-06-06T19:38:15.237" Score="4" Body="&lt;p&gt;I'd always kind of wondered how this worked too, so I took this as an excuse to look into the snakemake code. At the end of the day this becomes a question of (1) how are jobs actually submitted and (2) how is it determined if jobs are done (and then whether they failed)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For DRMAA, python has a module (appropriately named &quot;drmaa&quot;) that wraps around the libdrmaa library that comes with most schedulers. This is a very popular route, for example the Galaxy project uses this for dealing with most clusters (e.g., I use it to connect our internal Galaxy instance to our slurm cluster). The huge benefit here is that DRMAA does magic to allow you to simply submit commands to your cluster without needing to know whether you should execute &lt;code&gt;qsub&lt;/code&gt; or &lt;code&gt;srun&lt;/code&gt; or something else. Further, it then provides methods to simply query whether a job is running or not and what its exit status was.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the &lt;code&gt;--cluster&lt;/code&gt; command requires a LOT more magic on the side of snakemake. At the end of the day it creates a shell script that it then submits using the command you provided. Importantly, it includes some secret files in that script that it can then watch for (ever noticed that &lt;code&gt;.snakemake&lt;/code&gt; directory where you execute a script? That appears to be among the things it's used for.). These are named &lt;code&gt;{jobid}.jobfinished&lt;/code&gt; and &lt;code&gt;{jobid}.jobfailed&lt;/code&gt; and one of them will get touched, depending on the exit status of your command/script. Once one of those is there, then snakemake can move on in its DAG (or not, if there's a failure). This is obviously a LOT more to keep track of and it's then not possible for snakemake to cancel running jobs, which is something one can do easily with DRMAA.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-06T19:38:15.237" CommentCount="2" />
  <row Id="502" PostTypeId="2" ParentId="500" CreationDate="2017-06-06T19:44:56.890" Score="3" Body="&lt;p&gt;&lt;a href=&quot;http://seqan.de/&quot; rel=&quot;nofollow noreferrer&quot;&gt;SeqAn&lt;/a&gt; supports BWT tables for use with their parametrisable alignment algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To use it, &lt;a href=&quot;http://seqan.readthedocs.io/en/seqan-v1.4.2/Tutorial/SimpleReadMapping.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;follow the general outline for building a SeqAn short-read aligner&lt;/a&gt;, and use the &lt;a href=&quot;http://docs.seqan.de/seqan/2.2.0/index.html?p=FMIndex&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;FMIndex&lt;/code&gt;&lt;/a&gt; specialisation instead of — as in the example — the &lt;code&gt;IndexQGram&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="73" LastEditDate="2017-06-06T19:50:47.123" LastActivityDate="2017-06-06T19:50:47.123" CommentCount="0" />
  <row Id="503" PostTypeId="2" ParentId="500" CreationDate="2017-06-06T20:17:34.890" Score="10" Body="&lt;p&gt;First, let us remark that there exist several hundred read mappers, most of which have been even published (see, e.g., pages 25-29 of &lt;a href=&quot;http://brinda.cz/publications/brinda_phd.pdf&quot; rel=&quot;noreferrer&quot;&gt;this thesis&lt;/a&gt;). Developing a new mapper probably makes sense only as a programming exercise. Whereas developing a quick proof-of-concept read mapper is usually easy, turning it into a real competitor of existing and well-tuned mappers can last for years.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is not clear from the provided description how long is your reference, how many alignments you need to compute, etc. In certain situations, it may be useful to write a wrapper over existing mappers (e.g., using &lt;a href=&quot;https://docs.python.org/3/library/subprocess.html#subprocess.Popen&quot; rel=&quot;noreferrer&quot;&gt;subprocess.Popen&lt;/a&gt; for running the mapper and &lt;a href=&quot;http://pysam.readthedocs.io/&quot; rel=&quot;noreferrer&quot;&gt;PySam&lt;/a&gt; for parsing the output); while in some other situations, standard dynamic programming may be sufficient (e.g., using &lt;a href=&quot;https://github.com/mengyao/complete-striped-smith-waterman-library&quot; rel=&quot;noreferrer&quot;&gt;SSW&lt;/a&gt; with its Python binding).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let assume that you want to develop a toy read mapper. Most of read mappers are based on a so called seed-and-extend paradigm. Simply speaking, first you detect candidates for alignments, usually as exact matches between a read and the reference (using either a hash table or some full-text index – e.g., BWT-index). Then you would need to compute alignments for these candidates, typically using some algorithm based on dynamic programming, and report the best obtained alignments (e.g., the ones with the highest alignment score).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There exist two big, powerful and well debugged libraries implementing BWT-indexes which can be easily used for building a read mapper:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://seqan.rtfd.io&quot; rel=&quot;noreferrer&quot;&gt;SeqAn&lt;/a&gt;. See the &lt;a href=&quot;http://seqan.readthedocs.io/en/master/Tutorial/DataStructures/Indices/FMIndex.html?highlight=bwt&quot; rel=&quot;noreferrer&quot;&gt;FMIndex tutorial&lt;/a&gt; and the &lt;a href=&quot;http://seqan.readthedocs.io/en/master/Tutorial/Algorithms/Alignment/PairwiseSequenceAlignment.html&quot; rel=&quot;noreferrer&quot;&gt;Pairwise Sequence Alignment tutorial&lt;/a&gt; for quick examples of how to detect exact matches and how to do pairwise alignments. Also, they provide a tutorial about a &lt;a href=&quot;http://seqan.readthedocs.io/en/seqan-v1.4.2/Tutorial/SimpleReadMapping.html&quot; rel=&quot;noreferrer&quot;&gt;quick development of a read mapper&lt;/a&gt;, but the resulting mapper seems to be hash-table based, not BWT-based. SeqAn is used, e.g., in &lt;a href=&quot;https://github.com/seqan/seqan/tree/master/apps/yara&quot; rel=&quot;noreferrer&quot;&gt;YARA&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/simongog/sdsl-lite&quot; rel=&quot;noreferrer&quot;&gt;SDSL-Lite&lt;/a&gt;. This is a general library for succinct data structures. See the &lt;a href=&quot;http://simongog.github.io/assets/data/sdsl-slides/tutorial#1&quot; rel=&quot;noreferrer&quot;&gt;tutorial slides&lt;/a&gt; for an idea of how it works. For instance, &lt;a href=&quot;https://github.com/iqbal-lab/gramtools&quot; rel=&quot;noreferrer&quot;&gt;GramTools&lt;/a&gt; are based on SDSL.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-06-06T20:46:40.420" LastActivityDate="2017-06-06T20:46:40.420" CommentCount="4" />
  <row Id="504" PostTypeId="2" ParentId="426" CreationDate="2017-06-06T20:53:30.430" Score="5" Body="&lt;p&gt;I think others have provided fine answers to your question already. I just thought it might be relevant to mention about a tool that allows using Spark to distribute computations that re-use existing commandline bioinformatics software ... so, avoiding the need to re-implement proven algorithms in a language that Spark supports, &lt;a href=&quot;https://github.com/mcapuccini/EasyMapReduce&quot; rel=&quot;nofollow noreferrer&quot;&gt;EasyMapReduce&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This perceived need to re-implement algorithms, has otherwise been one of the biggest road blocks for wider adoption of Spark in the bioinformatics community in my understanding. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;EasyMapReduce was presented SparkSummit East this year (video link &lt;a href=&quot;https://www.youtube.com/watch?v=4C4R9qptUQo&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;, as shown on the repository home page).&lt;/p&gt;&#xA;" OwnerUserId="630" LastEditorUserId="73" LastEditDate="2017-06-06T21:40:05.000" LastActivityDate="2017-06-06T21:40:05.000" CommentCount="0" />
  <row Id="505" PostTypeId="1" AcceptedAnswerId="512" CreationDate="2017-06-06T20:58:15.443" Score="3" ViewCount="59" Body="&lt;p&gt;I recently got some &lt;a href=&quot;https://www.illumina.com/products/by-type/microarray-kits/infinium-methylation-epic.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;EPIC DNA methylation&lt;/a&gt; data and I was wondering what are some good practices to follow? &#xA;I am interested in knowing about normalization and differential analysis. Thank you.&lt;/p&gt;&#xA;" OwnerUserId="626" LastEditorUserId="73" LastEditDate="2017-06-06T21:50:14.550" LastActivityDate="2017-06-06T23:10:14.527" Title="What are some good practices to follow during EPIC DNA methylation data analysis?" Tags="&lt;r&gt;&lt;normalization&gt;&lt;methylation&gt;&lt;microarray&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="506" PostTypeId="2" ParentId="486" CreationDate="2017-06-06T21:32:30.483" Score="3" Body="&lt;p&gt;If you know the eukaryotic contaminant present you could use &lt;code&gt;bbsplit.sh&lt;/code&gt; from &lt;a href=&quot;https://sourceforge.net/projects/bbmap/&quot; rel=&quot;nofollow noreferrer&quot;&gt;BBMap suite&lt;/a&gt; to split/bin reads first using a reference for that contaminant (into one or as many bins as reference sequences you provide). &lt;/p&gt;&#xA;" OwnerUserId="634" LastActivityDate="2017-06-06T21:32:30.483" CommentCount="4" />
  <row Id="508" PostTypeId="1" CreationDate="2017-06-06T22:23:31.397" Score="3" ViewCount="298" Body="&lt;p&gt;&lt;em&gt;This is based on a question from &lt;a href=&quot;https://www.biostars.org/u/39680/&quot; rel=&quot;nofollow noreferrer&quot;&gt;betsy.s.collins&lt;/a&gt; on BioStars. The original post can be found &lt;a href=&quot;https://www.biostars.org/p/256448/&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have any suggestions for other tags or filtering steps on BWA-generated BAM files that can be used so reads only map to one location? One example application would be to find seeds for the &lt;a href=&quot;https://github.com/Generade-nl/TULIP&quot; rel=&quot;nofollow noreferrer&quot;&gt;TULIP&lt;/a&gt; assembler/scaffolder, which works best for reads that map to unique genomic locations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The concept of &quot;uniquely mapped reads&quot; is a loaded term, and most sources suggest filtering by MAPQ should do the trick. However, this approach doesn't seem to work when using BWA as a read mapper.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="55" LastEditDate="2017-06-08T11:40:57.807" LastActivityDate="2017-06-08T11:40:57.807" Title="Obtaining uniquely mapped reads from BWA mem alignment" Tags="&lt;bam&gt;&lt;alignment&gt;&lt;bwa&gt;" AnswerCount="1" CommentCount="8" FavoriteCount="1" />
  <row Id="509" PostTypeId="5" CreationDate="2017-06-06T22:38:50.510" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-06T22:38:50.510" LastActivityDate="2017-06-06T22:38:50.510" CommentCount="0" />
  <row Id="510" PostTypeId="4" CreationDate="2017-06-06T22:38:50.510" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-06T22:38:50.510" LastActivityDate="2017-06-06T22:38:50.510" CommentCount="0" />
  <row Id="511" PostTypeId="2" ParentId="500" CreationDate="2017-06-06T23:07:51.320" Score="7" Body="&lt;p&gt;BWA-MEM can be used as a library. File &lt;a href=&quot;https://github.com/lh3/bwa/blob/master/example.c&quot; rel=&quot;noreferrer&quot;&gt;bwa/example.c&lt;/a&gt; shows the basic functionality for single-end mapping. It should give identical mapping to the bwa-mem command line. Header &lt;a href=&quot;https://github.com/lh3/bwa/blob/master/bwamem.h&quot; rel=&quot;noreferrer&quot;&gt;bwa/bwamem.h&lt;/a&gt; contains basic documentation. Paired-end mapping is doable, but not well exposed. Several teams, including GATK and MS genomics, have been using bwa-mem as a library.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-06T23:07:51.320" CommentCount="1" />
  <row Id="512" PostTypeId="2" ParentId="505" CreationDate="2017-06-06T23:10:14.527" Score="4" Body="&lt;p&gt;EPIC data can be processed in the same manner as the previous iteration of methylation array data from Illumina (450k). This means that starting with .idat files, normalization should be performed (for example, via the minfi package). A &lt;a href=&quot;http://biorxiv.org/content/early/2016/07/23/065490&quot; rel=&quot;nofollow noreferrer&quot;&gt;recent paper&lt;/a&gt; from the creators of minfi is particularly helpful because it makes clear that normalized EPIC data from their package can be immediately compared against, for example, &lt;a href=&quot;https://wiki.nci.nih.gov/display/TCGA/Data+level&quot; rel=&quot;nofollow noreferrer&quot;&gt;level 3 TCGA data&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After that, I suggest using the manifest to attach genomic coordinates to your probes and segregating them into functional regions. By testing differential methylation in only regulatory regions, for example, you can increase the statistical power by reducing the overall number of tests to the ones you expect to yield major differences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are existing packages out there for differential methylation analysis, but without knowing your replicate structure or aims, it is difficult to point you in the right direction.&lt;/p&gt;&#xA;" OwnerUserId="643" LastActivityDate="2017-06-06T23:10:14.527" CommentCount="0" />
  <row Id="513" PostTypeId="1" AcceptedAnswerId="515" CreationDate="2017-06-06T23:44:21.290" Score="5" ViewCount="50" Body="&lt;p&gt;I'm working on a university project of predicting Translation Initiation Sites in human DNA. I searched the net for papers and documentation to get guidelines and inspiration, but I feel uncertain that I was able to find the state of the art in solving this problem, so I thought of asking the community here: What's the state of the art method of predicting Translation Initiation Sites?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(I'm also posting the question to help this nascent Q&amp;amp;A site by participating, please close it if it completely misses the mark of being a good question.)&lt;/p&gt;&#xA;" OwnerUserId="548" LastActivityDate="2017-06-07T01:53:56.240" Title="State of the art in predicting Translation Initiation Sites" Tags="&lt;human-genome&gt;&lt;dna&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="515" PostTypeId="2" ParentId="513" CreationDate="2017-06-07T01:53:56.240" Score="3" Body="&lt;p&gt;&lt;a href=&quot;http://science.sciencemag.org/content/324/5924/218&quot; rel=&quot;nofollow noreferrer&quot;&gt;Ribosome Profiling&lt;/a&gt; experiments profile the positions of ribosome-protected mRNA. a cycloheximide treatment fixes the translating ribosomes. A naive approach to define Translation Initiation Sites (TIS) would be to look at positions where you see distinct 'peaks' in the distribution of these fragments since the ribosome density will be higher at the start of the CDS (at the TIS).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://tisdb.human.cornell.edu/&quot; rel=&quot;nofollow noreferrer&quot;&gt;TISDB&lt;/a&gt; is a database comprised of &lt;a href=&quot;http://www.pnas.org/content/109/37/E2424.full&quot; rel=&quot;nofollow noreferrer&quot;&gt;GTI-Seq&lt;/a&gt; global mapping of TIS codons at nearly single-nucleotide resolution. GTI-Seq uses two 'fixers' that fix ribosomes where one of them specifically fixes the initiating ribosomes. &lt;/p&gt;&#xA;" OwnerUserId="161" LastActivityDate="2017-06-07T01:53:56.240" CommentCount="1" />
  <row Id="516" PostTypeId="1" CreationDate="2017-06-07T03:39:25.560" Score="5" ViewCount="18" Body="&lt;p&gt;I'm trying to use HOMER to make a metagene profile over gene bodies using a bedgraph file I've generated. The problem is that every time I do, I get really weird scaling on the y-axis. I should be getting average values across the gene body on the order of 5-10, but instead I'm getting values on the order of 0.03-0.05 or less. The weird thing is, when I don't use metagene script or when I don't use histogram with -size given, I get perfectly normal values -- but that's not what I want, unfortunately, for the metagene profile.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What am I missing?&lt;/p&gt;&#xA;" OwnerUserId="651" LastEditorUserId="73" LastEditDate="2017-06-07T04:27:00.783" LastActivityDate="2017-06-07T06:15:01.453" Title="What's the scaling for HOMER metagenes?" Tags="&lt;ngs&gt;&lt;bed&gt;&lt;metagenome&gt;&lt;homer&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="517" PostTypeId="2" ParentId="486" CreationDate="2017-06-07T03:49:20.207" Score="3" Body="&lt;p&gt;You can use &lt;a href=&quot;https://ccb.jhu.edu/software/centrifuge/&quot; rel=&quot;nofollow noreferrer&quot;&gt;centrifuge&lt;/a&gt; with the NT library to profile the taxonomy, and then remove the reads from eukaryotes. &lt;/p&gt;&#xA;" OwnerUserId="652" LastActivityDate="2017-06-07T03:49:20.207" CommentCount="0" />
  <row Id="518" PostTypeId="1" CreationDate="2017-06-07T04:20:05.333" Score="6" ViewCount="57" Body="&lt;p&gt;I am aligning a dataset of 1,000,000 reads oh human mRNA sequenced on Oxford Nanopore Technologies' MinION, and would like to use the STAR aligner, using the &lt;a href=&quot;https://github.com/PacificBiosciences/cDNA_primer/wiki/Bioinfx-study:-Optimizing-STAR-aligner-for-Iso-Seq-data#param&quot; rel=&quot;noreferrer&quot;&gt;parameters recommended by Pacific Biosciences&lt;/a&gt; for long reads. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to &lt;a href=&quot;https://groups.google.com/forum/#!searchin/rna-star/sjdboverhang%7Csort:relevance/rna-star/h9oh10UlvhI/ZwAs1xFVCAAJ&quot; rel=&quot;noreferrer&quot;&gt;this Google Groups thread&lt;/a&gt;, in setting up the genome index for short reads, the parameter &lt;code&gt;sjdbOverhang&lt;/code&gt; should be set to 1 less than the read length. Obviously, with long (mean 1.7Kb, max &gt;50Kb) reads, this doesn't make sense.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Running &lt;code&gt;STARlong --runMode genomeGenerate&lt;/code&gt; without setting &lt;code&gt;sjdbOverhang&lt;/code&gt; sets the parameter to &lt;code&gt;0&lt;/code&gt;. Does anyone know what this means, how it might affect mapping, and what I should set it to for my long reads?&lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="73" LastEditDate="2017-06-07T04:50:06.280" LastActivityDate="2017-06-08T02:58:54.080" Title="Building STAR Genome Index for nanopore RNA sequencing" Tags="&lt;rna-seq&gt;&lt;alignment&gt;&lt;nanopore&gt;&lt;star&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="519" PostTypeId="2" ParentId="508" CreationDate="2017-06-07T04:21:28.520" Score="5" Body="&lt;p&gt;To exclude &lt;em&gt;all possible&lt;/em&gt; multi-mapped reads from a BWA-mapped BAM file, it looks like you need to use &lt;code&gt;grep&lt;/code&gt; on the uncompressed SAM fields:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view -h mapped.bam | grep -v -e 'XA:Z:' -e 'SA:Z:' | samtools view -b &amp;gt; unique_mapped.bam&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Explanation follows...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm going to assume a situation in which a bioinformatician is presented with a mapped BAM file produced by BWA, and has no way of getting the original reads. One high-effort solution would be to extract the mapped reads from the BAM file and re-map with a different mapper that uses the MAPQ score to indicate multiple mappings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;... but what if that were not possible?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My understanding of BWA's output is that if a read maps perfectly to multiple genomic locations, it will be given a high mapping quality (MAPQ) score for both locations. Many people expect that a read that maps to at least two locations can have (at best) a 50% probability of mapping to one of those locations (i.e. MAPQ = 3). Because BWA doesn't do this, it makes it difficult to filter out multiply-mapped reads from BWA results using the MAPQ filter that works for other aligners; this is likely to be why the current answer on Biostars [&lt;code&gt;samtools view -bq 1&lt;/code&gt;] probably won't work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example line from a BWA mem alignment that I've just made. These are Illumina reads mapped to a parasite genome that has a &lt;em&gt;lot&lt;/em&gt; of repetitive sequence:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ERR063640.7     16      tig00019544     79974   21      21M2I56M1I20M   *       0       0       TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC    &amp;lt;AABBEGABFJKKKIM7GHKKJK&amp;gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&amp;gt;D    NM:i:4  MD:Z:83A13      AS:i:77 XS:i:67 XA:Z:tig00019544,-78808,21M2I56M1I20M,6;tig00019544,-84624,79M1I20M,6;tig00019544,-79312,33M4I42M1I20M,8;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;BWA mem has found that this particular read, ERR063460.7, maps to at least three different locations: tig00019544, tig00019544, and tig00019544. Note that the MAPQ for this read is 21, so even though the read maps to multiple locations, MAPQ can't be used to determine that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, the alternative locations are shown by the presence of the &lt;code&gt;XA&lt;/code&gt; tag in the custom fields section of the SAM output. Perhaps just filtering on lines that contain the &lt;code&gt;XA&lt;/code&gt; tag will be able to exclude multiply-mapped reads. The &lt;code&gt;samtools view&lt;/code&gt; man page suggests that &lt;code&gt;-x&lt;/code&gt; will filter out a particular tag:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ samtools view -x XA output.bam | grep '^ERR063640\.7[[:space:]]'&#xA;ERR063640.7 16  tig00019544 79974   21  21M2I56M1I20M   *   0   0   TATCACATATCATCCGACTCAGCTCGACGAGTACAATGCTAATTTAACACTTAGAATGCCCGGCAATGAAATTCGTTTTCCGTCAATTCTTGAAAATTTC    &amp;lt;AABBEGABFJKKKIM7GHKKJK&amp;gt;JLKLDGMHLIMIHHCGIJKKLJKLNJGLLLKLILKLMFNDLKGHJEKMKKMIJHGLOJLLLKIJLKKJEJLIGG&amp;gt;D    NM:i:4  MD:Z:83A13  AS:i:77 XS:i:67&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;... so it filtered out the tag (i.e. the tag no longer exists in the SAM output), but not the &lt;em&gt;read&lt;/em&gt;. There are no useful bits in the FLAG field to indicate multiple genomic mappings (which I know can be filtered to exclude the read as well), so I have to resort to other measures.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this particular case, I can use &lt;code&gt;grep -v&lt;/code&gt; on the uncompressed SAM output to exclude alignment lines that have the &lt;code&gt;XA&lt;/code&gt; tag (and re-compress to BAM afterwards, just to be tidy):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ samtools view -h output.bam | grep -v 'XA:Z:' | samtools view -b &amp;gt; output_filtered.bam&#xA;$ samtools view output_filtered.bam | grep '^ERR063640\.7[[:space:]]'&#xA;&amp;lt;no output&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Hurray! reads filtered. As a little aside, this &lt;code&gt;grep&lt;/code&gt; search has a fairly substantial computational load: it's looking for some string with the text &lt;code&gt;XA:Z:&lt;/code&gt; somewhere in the line, and doesn't actually capture every situation. Some masochistic person might come along at a later date and decide that they're going to call all their reads &lt;code&gt;HAXXA:Z:AWESOME!:&amp;lt;readNumber&amp;gt;&lt;/code&gt;, in which case a tweak to this grep search would be needed to make sure that there's a space (or more specifically a tab character) prior to the &lt;code&gt;XA:Z:&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I do a check for &lt;em&gt;any&lt;/em&gt; duplicated read names, just to be sure:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xA;ERR063640.1194&#xA;ERR063640.1429&#xA;ERR063640.1761&#xA;ERR063640.2336&#xA;ERR063640.2825&#xA;ERR063640.3458&#xA;ERR063640.4421&#xA;ERR063640.4474&#xA;ERR063640.4888&#xA;ERR063640.49&#xA;ERR063640.4974&#xA;ERR063640.5070&#xA;ERR063640.5130&#xA;ERR063640.5300&#xA;ERR063640.5868&#xA;ERR063640.6116&#xA;ERR063640.6198&#xA;ERR063640.6468&#xA;ERR063640.6717&#xA;ERR063640.6797&#xA;ERR063640.7322&#xA;ERR063640.750&#xA;ERR063640.7570&#xA;ERR063640.7900&#xA;ERR063640.8115&#xA;ERR063640.8405&#xA;ERR063640.911&#xA;ERR063640.9206&#xA;ERR063640.9765&#xA;ERR063640.9986&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Oh... damn. I wonder what they are:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ samtools view output_filtered.bam | grep '^ERR063640.3458[[:space:]]'&#xA;ERR063640.3458  16  tig00002961 5402    60  58S38M  *   0   0   AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATATCCGTATCGCGAAGATCAGGACTTACTCCGCAGAAGAA    DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJDHLNKCK;KMKGMFKJILIIIMKI9JLKKHEJFII?CC    NM:i:0  MD:Z:38 AS:i:38 XS:i:0  SA:Z:tig00002377,202353,-,14M3I5M1I35M38S,19,5;&#xA;ERR063640.3458  2064    tig00002377 202353  19  14M3I5M1I35M38H *   0   0   AGGTACCATTCGATAGAGGGAGAAAGGCACTACTAAAGATTTTGCCACATTTGCTATA  DD6HFFJBKFH=KDILKLGLJEKLKGFJIH8IKHLLMJEK:L:HBGJIHJKFLLKIHJ  NM:i:5  MD:Z:5G48   AS:i:35 XS:i:27 SA:Z:tig00002961,5402,-,58S38M,60,0;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Aha! Supplemental alignments, which use the &lt;em&gt;official&lt;/em&gt; &lt;code&gt;SA&lt;/code&gt; tag [other canonical alignments in a chimeric alignment]. These appear to be situations where a single read has been split up, and maps to multiple locations. Note that in this case, &lt;strong&gt;&lt;em&gt;both of the alignments still have MAPQ scores of over 3&lt;/em&gt;&lt;/strong&gt;. It sounds like the questioner would also want to get rid of these situations as well. This time, there are standard flag fields as well to deal with these situations (0x800: secondary alignment). Except it's not enough to just filter the supplemental alignment, because &lt;em&gt;both&lt;/em&gt; read mappings should be removed, rather than just the one (or ones) that happened to be tagged as secondary.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Luckily, BWA appears to put the &lt;code&gt;SA&lt;/code&gt; tag into all reads containing supplementary alignments (if this is not the case, I'm sure someone will correct me on that). So, I add in the &lt;code&gt;SA&lt;/code&gt; search as an additional &lt;code&gt;grep&lt;/code&gt; filter:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ samtools view -h output.bam | grep -v -e 'XA:Z:' -e 'SA:Z:' | samtools view -b &amp;gt; output_filtered.bam&#xA;$ samtools view output_filtered.bam | awk '{print $1}' | sort | uniq -d&#xA;&amp;lt;no output&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Done. Easy peasy! &lt;code&gt;&amp;lt;/sarcasm&amp;gt;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;... that original &quot;high-effort&quot; solution of using a different aligner doesn't look so bad now.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-07T10:11:54.673" LastActivityDate="2017-06-07T10:11:54.673" CommentCount="9" />
  <row Id="520" PostTypeId="2" ParentId="518" CreationDate="2017-06-07T04:50:54.920" Score="6" Body="&lt;p&gt;The parameter is used to determine how much sequence STAR indexes on each side of a splice junction to improve its alignment accuracy.  For very long reads, this may not be ideal. I am not sure if STAR is capable of including multiple splice junctions since a long read is more than likely to span more than one.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It may be worthwhile to consider aligning your reads directly to transcripts and taking the reads that align poorly and using them to look for novel transcripts.&lt;/p&gt;&#xA;" OwnerUserId="64" LastEditorUserId="64" LastEditDate="2017-06-08T02:58:54.080" LastActivityDate="2017-06-08T02:58:54.080" CommentCount="0" />
  <row Id="521" PostTypeId="2" ParentId="518" CreationDate="2017-06-07T04:58:25.327" Score="2" Body="&lt;p&gt;I've found the &lt;a href=&quot;https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;STAR manual&lt;/a&gt; to be incredibly helpful with trying to navigate my way round all the STAR command line parameters. Here's the section on the &lt;code&gt;sjdbOverhang&lt;/code&gt; parameter:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;code&gt;--sjdbOverhang&lt;/code&gt; specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the &lt;em&gt;ReadLength-1&lt;/em&gt;, where &lt;em&gt;ReadLength&lt;/em&gt; is the length of the reads. For instance, for Illumina 2x100b paired-end reads, the ideal value is 100-1=99. In case of reads of varying length, the ideal value is &lt;em&gt;max(ReadLength)-1&lt;/em&gt;. &lt;strong&gt;In most cases, the default value of 100 will work as well as the ideal value.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;[emphasis preserved from the original text]&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would recommend trying out running STAR with this value unset, or set to 100, and seeing how it goes. If you want to be particularly adventurous, you could set it to 1000 and see if anything changes.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-07T04:58:25.327" CommentCount="0" />
  <row Id="522" PostTypeId="1" CreationDate="2017-06-07T05:25:27.387" Score="4" ViewCount="71" Body="&lt;p&gt;I'm using several datasets that are encoded using either Entrez or Ensembl IDs to specify genes, and need to decide on which to standardise on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any major reasons to use one over the other?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which tools and conversion tables are best to use for this?&lt;/p&gt;&#xA;" OwnerUserId="657" LastEditorUserId="73" LastEditDate="2017-06-07T09:37:00.570" LastActivityDate="2017-06-07T09:37:00.570" Title="Entrez or Ensembl gene IDs?" Tags="&lt;conversion&gt;&lt;gene&gt;&lt;public-databases&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="523" PostTypeId="2" ParentId="516" CreationDate="2017-06-07T06:15:01.453" Score="4" Body="&lt;p&gt;Figured it out! For anyone who finds this thread in the future and is wondering what is going on: HOMER, when using the -size given option, normalizes the y-axis to the number of basepairs in the intervals -- in this case, the length of the gene body. To get the &quot;raw&quot; values, as I wanted, you need to multiply by the length of the gene body to &quot;undo&quot; the normalization.&lt;/p&gt;&#xA;" OwnerUserId="651" LastActivityDate="2017-06-07T06:15:01.453" CommentCount="0" />
  <row Id="524" PostTypeId="1" AcceptedAnswerId="530" CreationDate="2017-06-07T06:21:23.533" Score="5" ViewCount="44" Body="&lt;p&gt;I have performed RNA-seq analysis using HISAT2 &amp;amp; StringTie workflow suggested in: &lt;a href=&quot;http://www.nature.com/nprot/journal/v11/n9/full/nprot.2016.095.html&quot; rel=&quot;noreferrer&quot;&gt;Transcript-level expression analysis of RNA-seq experiments with HISAT, StringTie and Ballgown&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of the transcripts/exons have decimal coverage values (eg., 1.1 or 2.59).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My question is:&lt;/strong&gt; How can these tools report decimal coverage? If only half of the read overlaps exon will this exon have 0.5 coverage?&lt;/p&gt;&#xA;" OwnerUserId="186" LastActivityDate="2017-06-07T08:31:35.480" Title="How can HISAT2/StringTie report decimal coverage values" Tags="&lt;rna-seq&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="525" PostTypeId="1" AcceptedAnswerId="542" CreationDate="2017-06-07T07:20:47.170" Score="11" ViewCount="174" Body="&lt;p&gt;I have a pipeline for generating a BigWig file from a BAM file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;BAM -&amp;gt; BedGraph -&amp;gt; BigWig&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which uses &lt;code&gt;bedtools genomecov&lt;/code&gt; for the &lt;code&gt;BAM -&amp;gt; BedGraph&lt;/code&gt; part and &lt;code&gt;bedGraphToBigWig&lt;/code&gt; for the &lt;code&gt;BedGraph -&amp;gt; BigWig&lt;/code&gt; part.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The use of &lt;code&gt;bedGraphToBigWig&lt;/code&gt; to create the &lt;code&gt;BigWig&lt;/code&gt; file requires a BedGraph file to reside on disk in uncompressed form as it performs seeks. This is problematic for large genomes and variable coverage BAM files when there are more step changes/lines in the &lt;code&gt;BedGraph&lt;/code&gt; file. My &lt;code&gt;BedGraph&lt;/code&gt; files are in the order of 50 Gbytes in size and all that IO for 10-20 BAM files seems unnecessary.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Are there any tools capable generate &lt;code&gt;BigWig&lt;/code&gt; without having to use an uncompressed &lt;code&gt;BedGraph&lt;/code&gt; file on disk? I'd like this conversion to hapen as quickly as possible.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried the following tools, but they still create/use a &lt;code&gt;BedGraph&lt;/code&gt; intermediary file:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;deepTools&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h1&gt;Some Benchmarks, Ignoring IO&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Here are some timings I get for creating a &lt;code&gt;BigWig&lt;/code&gt; file from a &lt;code&gt;BAM&lt;/code&gt; file using 3 different pipelines. All files reside on a &lt;code&gt;tmpfs&lt;/code&gt; i.e. in memory.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;BEDTools and Kent Utils&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;This is the approach taken by most.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time $(bedtools genomecov -bg -ibam test.bam -split -scale 1.0 &amp;gt; test.bedgraph \&#xA;  &amp;amp;&amp;amp; bedGraphToBigWig test.bedgraph test.fasta.chrom.sizes kent.bw \&#xA;  &amp;amp;&amp;amp; rm test.bedgraph)&#xA;&#xA;real    1m20.015s&#xA;user    0m56.608s&#xA;sys     0m27.271s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;SAMtools and Kent Utils&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Replacing &lt;code&gt;bedtools genomecov&lt;/code&gt; with &lt;code&gt;samtools depth&lt;/code&gt; and a custom &lt;code&gt;awk&lt;/code&gt; script (&lt;a href=&quot;https://gist.github.com/nathanhaigh/c8322002a42e40c479d3d766aab32547&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;depth2bedgraph.awk&lt;/code&gt;&lt;/a&gt;) to output &lt;code&gt;bedgraph&lt;/code&gt; format has a significant performance improvement:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time $(samtools depth -Q 1 --reference test.fasta test.bam \&#xA;  | mawk -f depth2bedgraph.awk \&#xA;  &amp;gt; test.bedgraph \&#xA;  &amp;amp;&amp;amp; bedGraphToBigWig test.bedgraph test.fasta.chrom.sizes kent.bw \&#xA;  &amp;amp;&amp;amp; rm test.bedgraph)&#xA;&#xA;real    0m28.765s&#xA;user    0m44.999s&#xA;sys     0m1.166s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Although it is has less features, we used &lt;code&gt;mawk&lt;/code&gt; here as it's faster than &lt;code&gt;gawk&lt;/code&gt; (we don't need those extra features here).&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Parallelising with xargs&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If you want to have a &lt;code&gt;BigWig&lt;/code&gt; file per chromosome, you can easily parallelise this across chromosome/reference sequences. We can use &lt;code&gt;xargs&lt;/code&gt; to run 5 parallel BAM-&gt;BedGraph-&gt;BigWig pipelines, each using the &lt;code&gt;tmpfs&lt;/code&gt; mounted &lt;code&gt;/dev/shm&lt;/code&gt; for the intermediary &lt;code&gt;BedGraph&lt;/code&gt; files.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cut -f1 test.fasta.chrom.sizes \&#xA;  | xargs -I{} -P 5  bash -c 'mkdir /dev/shm/${1} \&#xA;  &amp;amp;&amp;amp; samtools depth -Q 1 --reference test.fasta -r &quot;${1}&quot; test.bam \&#xA;  | mawk -f scripts/depth2bedgraph.awk \&#xA;  &amp;gt; &quot;/dev/shm/${1}/test.bam.bedgraph&quot; \&#xA;  &amp;amp;&amp;amp; mkdir &quot;./${1}&quot; \&#xA;  &amp;amp;&amp;amp; bedGraphToBigWig \&#xA;    &quot;/dev/shm/${1}/test.bam.bedgraph&quot; \&#xA;    test.fasta.chrom.sizes \&#xA;    &quot;./${1}/test.bam.bw&quot; \&#xA;  &amp;amp;&amp;amp; rm &quot;/dev/shm/${1}/test.bam.bedgraph&quot;' -- {}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;deepTools&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Let's see how &lt;code&gt;deepTools&lt;/code&gt; performs.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time bamCoverage --numberOfProcessors max \&#xA;  --minMappingQuality 1 \&#xA;  --bam test.bam --binSize 1 --skipNonCoveredRegions \&#xA;  --outFileName deeptools.bw&#xA;&#xA;real    0m40.077s&#xA;user    3m56.032s&#xA;sys     0m9.276s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="400" LastEditorUserId="96" LastEditDate="2017-06-08T08:47:24.203" LastActivityDate="2017-08-09T16:13:38.660" Title="BAM to BigWig without intermediary BedGraph" Tags="&lt;bam&gt;&lt;file-formats&gt;&lt;format-conversion&gt;" AnswerCount="3" CommentCount="10" />
  <row Id="526" PostTypeId="2" ParentId="522" CreationDate="2017-06-07T08:08:25.347" Score="1" Body="&lt;p&gt;I think there is no general answer to your question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which type of ID you will use depends on how practical it will be for your downstream processing. This depends on what other source of information you have, how much conversion work you will need to do in order to match your data to these information if you use one or the other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think the choice may emerge by trial and error, as you realize that a given choice is actually not convenient in your particular use case. Try to set up easy to use conversion procedures that you can easily insert in some steps of your workflow, so that you can easily switch from one choice to the other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Concerning the tools to do the conversion, see the following question and its answers: &lt;a href=&quot;https://bioinformatics.stackexchange.com/q/488/292&quot;&gt;Converting gene names from one public database format to another&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-06-07T08:08:25.347" CommentCount="0" />
  <row Id="527" PostTypeId="1" CreationDate="2017-06-07T08:24:32.383" Score="4" ViewCount="70" Body="&lt;p&gt;miRBase 21 was published June 26, 2014 and was still in its growth phase. Why is it not being updated anymore or the project declared officially dead? ENSEMBL also uses miRBase as a starting point (&lt;a href=&quot;http://www.ensembl.org/info/genome/genebuild/ncrna.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.ensembl.org/info/genome/genebuild/ncrna.html&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What do people use now as a good reference for miRNAs especially for less well annotated/sequenced species?&lt;/p&gt;&#xA;" OwnerUserId="391" LastEditorUserId="391" LastEditDate="2017-06-07T08:34:20.397" LastActivityDate="2017-06-08T10:51:40.737" Title="Where is an up to date miRNA database and what happened to miRBase?" Tags="&lt;annotation&gt;&lt;database&gt;" AnswerCount="4" CommentCount="3" />
  <row Id="528" PostTypeId="1" AcceptedAnswerId="533" CreationDate="2017-06-07T08:24:35.987" Score="9" ViewCount="108" Body="&lt;p&gt;I'm trying to find a programmatic way to automatically extract the following information from a &lt;a href=&quot;http://www.wwpdb.org/documentation/file-format&quot; rel=&quot;nofollow noreferrer&quot;&gt;PDB file&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;RNA sequence &lt;/li&gt;&#xA;&lt;li&gt;Secondary structure restraints in bracket format, e.g. &lt;code&gt;. (( . ( . ) . ))&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Does software exist that can take a PDB file as input and generate these two pieces of information? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;e.g. file. &lt;a href=&quot;http://www.rcsb.org/pdb/explore.do?structureId=3NDB&quot; rel=&quot;nofollow noreferrer&quot;&gt;3NDB_ba.pdb&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The nucleotide sequence is: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;gUCUCGUCCCGUGGGGCUCGGCGGUGGGGGAGCAUCUCCUGUAGGGGAGAUGUAACCCCCUUUACCUGCCGAACCCCGCCAGGCCCGGAAGGGAGCAACGGUAGGCAGGACGUCGGCGCUCACGGGGGUGCGGGAC&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And the secondary structure :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;.(((.(..(((((((((.(((((..(((((.(((((((((....)))))))))..)))))....((((((.....(((.....(((....))).....)))..)))))).))))))).))))))).....).))).&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="682" LastEditorUserId="640" LastEditDate="2017-06-08T18:31:15.580" LastActivityDate="2017-06-08T18:31:15.580" Title="How to extract RNA sequence and secondary structure restrains from a PDB file" Tags="&lt;public-databases&gt;&lt;pdb&gt;&lt;rna&gt;&lt;rna-structure&gt;" AnswerCount="3" CommentCount="3" FavoriteCount="1" />
  <row Id="529" PostTypeId="2" ParentId="456" CreationDate="2017-06-07T08:28:15.090" Score="0" Body="&lt;p&gt;From my experience, there is few about CRISPR pooled screenings in R. One of them is &lt;a href=&quot;https://github.com/jyyu/ScreenBEAM&quot; rel=&quot;nofollow noreferrer&quot;&gt;ScreenBEAM&lt;/a&gt;. It uses a linear mixture model to determine significance at gene level by considering all the guides together at the same step in the analysis. However, it does not work very well for me. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are other ways of analyze these screenings outside of R that are easy to use such the suggested MAGeCK or others such HiTSelect, BAGEL or &lt;a href=&quot;http://crispr-analyzer.dkfz.de/&quot; rel=&quot;nofollow noreferrer&quot;&gt;CRISPR-Analyzer&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;CRISPR-Analyzer is an online tool that could be useful and that recapitulates some of the mentioned methods in a single analysis. However, if you want to work from command line I would recommend you BAGEL for gene essentiality experiments and MAGeCK for other experimental setup (i.e, control vs treatment)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sorry for not putting all the links but I can only put two at the moment!&lt;/p&gt;&#xA;" OwnerUserId="678" LastEditorUserId="678" LastEditDate="2017-06-07T12:08:54.520" LastActivityDate="2017-06-07T12:08:54.520" CommentCount="0" />
  <row Id="530" PostTypeId="2" ParentId="524" CreationDate="2017-06-07T08:31:35.480" Score="5" Body="&lt;p&gt;I suspect this is average coverage across the transcript or exon. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Formula for average coverage:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?Cov%20%3D%20N%20%5Ccdot%20%5Cfrac%7BL%7D%7BT%7D&quot; alt=&quot;coverage-formula&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;N: number of reads mapping/aligning to your transcript/exon&lt;/li&gt;&#xA;&lt;li&gt;L: average read length&lt;/li&gt;&#xA;&lt;li&gt;T: Transcript/exon length&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="81" LastActivityDate="2017-06-07T08:31:35.480" CommentCount="0" />
  <row Id="531" PostTypeId="1" CreationDate="2017-06-07T08:39:33.900" Score="5" ViewCount="71" Body="&lt;p&gt;I am trying to use &lt;code&gt;samtools depth&lt;/code&gt; (v1.4) with the -a option and a bed file listing the human chromosomes chr1-chr22, chrX, chrY, and chrM to print out the coverage at every position:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat GRCh38.karyo.bed | awk '{print $3}' | datamash sum 1&#xA;3088286401&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would like to know how to run &lt;code&gt;samtools depth&lt;/code&gt; so that it produces 3,088,286,401 entries when run against a GRCh38 bam file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools depth -b $bedfile -a $inputfile&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried it for a few bam files that were aligned the same way, and I get differing number of entries:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;3087003274&#xA;3087005666&#xA;3087007158&#xA;3087009435&#xA;3087009439&#xA;3087009621&#xA;3087009818&#xA;3087010065&#xA;3087010408&#xA;3087010477&#xA;3087010481&#xA;3087012115&#xA;3087013147&#xA;3087013186&#xA;3087013500&#xA;3087149616&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a special flag in &lt;code&gt;samtools depth&lt;/code&gt; so that it reports all entries from the bed file?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If &lt;code&gt;samtools depth&lt;/code&gt; is not the best tool for this, what would be the equivalent with &lt;code&gt;sambamba depth base&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sambamba depth base --min-coverage=0 --regions $bedfile $inputfile&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any other options?&lt;/p&gt;&#xA;" OwnerUserId="180" LastEditorUserId="57" LastEditDate="2017-06-07T22:19:56.217" LastActivityDate="2017-06-07T22:34:32.847" Title="samtools depth print out all positions" Tags="&lt;bam&gt;&lt;samtools&gt;&lt;sambamba&gt;" AnswerCount="2" CommentCount="7" />
  <row Id="532" PostTypeId="2" ParentId="527" CreationDate="2017-06-07T10:04:21.943" Score="1" Body="&lt;p&gt;I would guess DIANA-Tarbase ( evidences, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/25416803&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pubmed/25416803&lt;/a&gt; ) and TargetScan (predictions, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/26267216&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pubmed/26267216&lt;/a&gt; ) are good starting points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Especially for the less annotated species, TargetScan might be helpful.&lt;/p&gt;&#xA;" OwnerUserId="690" LastActivityDate="2017-06-07T10:04:21.943" CommentCount="2" />
  <row Id="533" PostTypeId="2" ParentId="528" CreationDate="2017-06-07T10:06:32.310" Score="7" Body="&lt;p&gt;I suggest you take a look at &lt;code&gt;rna-pdb-tools&lt;/code&gt; we do way more than you need! :-) The tools can get you a sequence, secondary structure and much more using various algorithms, and all is well documented &lt;a href=&quot;http://rna-pdb-tools.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://rna-pdb-tools.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To get sequence &lt;a href=&quot;http://rna-pdb-tools.readthedocs.io/en/latest/main.html#get-sequence&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://rna-pdb-tools.readthedocs.io/en/latest/main.html#get-sequence&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ rna_pdb_tools.py --get_seq 5_solution_1.pdb&#xA;&amp;gt; 5_solution_1.pdb A:1-576&#xA;CAUCCGGUAUCCCAAGACAAUCUCGGGUUGGGUUGGGAAGUAUCAUGGCUAAUCACCAUGAUGCAAUCGGGUUGAACACUUAAUUGGGUUAAAACGGUGGGGGACGAUCCCGUAACAUCCGUCCUAACGGCGACAGACUGCACGGCCCUGCCUCAGGUGUGUCCAAUGAACAGUCGUUCCGAAAGGAAG&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or you can get sequence and secondary structure via x3dna (&lt;a href=&quot;http://x3dna.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://x3dna.org/&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[mm] py3dna$ git:(master) ✗ ./rna_x3dna.py test_data/1xjr.pdb&#xA;test_data/1xjr.pdb&#xA; &amp;gt;1xjr nts=47 [1xjr] -- secondary structure derived by DSSR&#xA; gGAGUUCACCGAGGCCACGCGGAGUACGAUCGAGGGUACAGUGAAUU&#xA; ..(((((((...((((.((((.....))..))..))).).)))))))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem is not trivial. I hope rna-pdb-tools has stuff that can solve this issue for you (&lt;a href=&quot;http://rna-pdb-tools.readthedocs.io/en/latest/want.html#module-rna_pdb_tools.utils.rna_x3dna.rna_x3dna&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://rna-pdb-tools.readthedocs.io/en/latest/want.html#module-rna_pdb_tools.utils.rna_x3dna.rna_x3dna&lt;/a&gt;)&lt;/p&gt;&#xA;" OwnerUserId="640" LastEditorUserId="640" LastEditDate="2017-06-07T18:36:43.480" LastActivityDate="2017-06-07T18:36:43.480" CommentCount="8" />
  <row Id="534" PostTypeId="5" CreationDate="2017-06-07T11:15:08.160" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-07T11:15:08.160" LastActivityDate="2017-06-07T11:15:08.160" CommentCount="0" />
  <row Id="535" PostTypeId="4" CreationDate="2017-06-07T11:15:08.160" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-07T11:15:08.160" LastActivityDate="2017-06-07T11:15:08.160" CommentCount="0" />
  <row Id="537" PostTypeId="2" ParentId="525" CreationDate="2017-06-07T12:07:52.207" Score="2" Body="&lt;p&gt;The &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/QuasR.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;QuasR&lt;/a&gt; package in R has a useful function called &lt;code&gt;qExportWig&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;To use it you first need to create a sample sheet and a &lt;code&gt;qProject&lt;/code&gt; object - this can be done starting with bam files or fastq files (in which case it also  performs alignments). You can then create wig or bigWig files without writing a bedGraph file to disk.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An example would look something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(QuasR)&#xA;&#xA;# Create qProject object using qAlign&#xA;# Sample file should contain tab-separated file names (BAM or fastq) and sample names&#xA;myproj &amp;lt;- qAlign(&quot;samples.txt&quot;, genome=mygenome, paired=&quot;no&quot;)&#xA;&#xA;# Export all samples in project as bigWig, scaling to mean number of alignments by default&#xA;qExportWig(myproj, binsize=50, createBigWig=T)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It has a lot of flexible options, including scaling or log-transforming counts, shifting reads and changing the bin size.&lt;/p&gt;&#xA;" OwnerUserId="182" LastEditorUserId="182" LastEditDate="2017-06-08T06:56:00.910" LastActivityDate="2017-06-08T06:56:00.910" CommentCount="1" />
  <row Id="538" PostTypeId="5" CreationDate="2017-06-07T13:13:19.417" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-07T13:13:19.417" LastActivityDate="2017-06-07T13:13:19.417" CommentCount="0" />
  <row Id="539" PostTypeId="4" CreationDate="2017-06-07T13:13:19.417" Score="0" Body="questions involving microarray analysis (e.g. Illumina BeadChip, Affymetrix GeneChip)" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-08T11:39:46.183" LastActivityDate="2017-06-08T11:39:46.183" CommentCount="0" />
  <row Id="540" PostTypeId="1" CreationDate="2017-06-07T13:23:52.380" Score="8" ViewCount="234" Body="&lt;p&gt;When you look at all the genome files available from Ensembl. You are presented with a bunch of options. Which one is the best to use/download?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You have a combination of choices.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First part options:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;dna_sm&lt;/strong&gt; - Repeats soft-masked (converts repeat nucleotides to lowercase)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;dna_rm&lt;/strong&gt; - Repeats masked (converts repeats to to N's)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;dna&lt;/strong&gt; - No masking&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Second part options:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;.toplevel&lt;/strong&gt; - Includes haplotype information (not sure how aligners deal with this)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;.primary_assembly&lt;/strong&gt; - Single reference base per position&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Right now I usually use a non-masked primary assembly for analysis, so in the case of humans:&#xA;&lt;em&gt;Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does this make sense for standard RNA-Seq, ChIP-Seq, ATAC-Seq, CLIP-Seq, scRNA-Seq, etc... ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In what cases would I prefer other genomes? Which tools/aligners take into account softmasked repeat regions?&lt;/p&gt;&#xA;" OwnerUserId="383" LastActivityDate="2017-06-07T15:04:08.313" Title="What Ensembl genome version should I use for alignments? (e.g. toplevel.fa vs. primary_assembly.fa)" Tags="&lt;fasta&gt;&lt;genome&gt;&lt;ensembl&gt;&lt;biomart&gt;" AnswerCount="4" CommentCount="2" FavoriteCount="1" />
  <row Id="541" PostTypeId="2" ParentId="540" CreationDate="2017-06-07T13:29:18.277" Score="7" Body="&lt;p&gt;There's rarely a good reason to use a hard-masked genome (sometimes for blast, but that's it). For that reason, we use soft-masked genomes, which only have the benefit of showing roughly where repeats are (we never make use of this for our *-seq experiments, but it's there in case we ever want to).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For primary vs. toplevel, very few aligners can properly handle additional haplotypes. If you happen to be using BWA, then the toplevel assembly would benefit you. For STAR/hisat2/bowtie2/BBmap/etc. the haplotypes will just cause you problems due to increasing multimapper rates incorrectly. Note that none of these actually use soft-masking.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-07T13:29:18.277" CommentCount="2" />
  <row Id="542" PostTypeId="2" ParentId="525" CreationDate="2017-06-07T13:32:48.943" Score="8" Body="&lt;p&gt;This can be done in R very easily from an indexed .bam file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given single-end file for &lt;strong&gt;sample1&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(GenomicAlignments)&#xA;library(rtracklayer)&#xA;&#xA;## read in BAM file (use readGAlignmentPairs for paired-end files)&#xA;gr &amp;lt;- readGAlignments('sample1.bam')&#xA;&#xA;## convert to coverages&#xA;gr.cov &amp;lt;- coverage(gr)&#xA;&#xA;## export as bigWig&#xA;export.bw(gr.cov,'sample1.bigwig')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Be aware that this method doesn't include normalization steps (such as normalizing to total coverage). Most of these additional steps can be added if necessary. &lt;/p&gt;&#xA;" OwnerUserId="383" LastActivityDate="2017-06-07T13:32:48.943" CommentCount="2" />
  <row Id="543" PostTypeId="2" ParentId="540" CreationDate="2017-06-07T14:08:52.377" Score="2" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;Which tools/aligners take into account softmasked repeat regions?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;If you're doing whole genome - whole genome alignment (rather than read alignment) then using the softmasked genome is definitely best. Tools suitable for such large scale alignments task tend to skip marked repeats completely in their initial steps to prevent the build up of bogus short alignments that can have a massive performance impact in terms of time and memory usage. For example, &lt;a href=&quot;http://www.bx.psu.edu/miller_lab/dist/README.lastz-1.02.00/README.lastz-1.02.00a.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;LASTZ&lt;/a&gt; skips lower case letters during the seeding stage. &lt;/p&gt;&#xA;" OwnerUserId="104" LastActivityDate="2017-06-07T14:08:52.377" CommentCount="0" />
  <row Id="544" PostTypeId="2" ParentId="540" CreationDate="2017-06-07T14:44:42.053" Score="1" Body="&lt;p&gt;TOPLEVEL&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These files contains all sequence regions flagged as toplevel in an Ensembl&#xA;schema. This includes chromsomes, regions not assembled into chromosomes and&#xA;N padded haplotype/patch regions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g: I used the soft masked assemblies for genome annotation pipelines like MAKER, also toplevel unmasked ones for RNA-seq, ChipSeq analysis&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PRIMARY ASSEMBLY&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Primary assembly contains all toplevel sequence regions excluding haplotypes&#xA;and patches. This file is best used for performing sequence similarity searches&#xA;where patch and haplotype sequences would confuse analysis.&lt;/p&gt;&#xA;" OwnerUserId="705" LastActivityDate="2017-06-07T14:44:42.053" CommentCount="0" />
  <row Id="545" PostTypeId="2" ParentId="540" CreationDate="2017-06-07T15:04:08.313" Score="4" Body="&lt;p&gt;Generally, you should use the soft-masked or unmasked primary assembly. Cross-species whole-genome aligners, especially older ones, do need to know soft-masked regions; otherwise they can be impractically slow for mammalian genomes. Modern read aligners are designed to work with repeats efficiently and therefore they don't need to see the soft mask.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For GRCh38, though, I would recommend to use the official build &lt;a href=&quot;ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/&quot; rel=&quot;nofollow noreferrer&quot;&gt;at GRC FTP&lt;/a&gt;. Most people will probably choose &quot;no_alt_analysis_set&quot;. Using the Ensembl version is discouraged due to its chromosome naming. We more often use &quot;chr1&quot; instead of &quot;1&quot; for GRCh38. At one point, Ensembl actually agreed to use &quot;chr1&quot; as well, but didn't make that happen due to technical issues, I guess.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As to alternate haplotypes, most aligners can't work with them; no variant callers can take the advantage of these sequences, either. When you align to a reference genome containing haplotypes with an aligner not supporting these extra sequences, you will get poor mapping results.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-07T15:04:08.313" CommentCount="0" />
  <row Id="546" PostTypeId="2" ParentId="15" CreationDate="2017-06-07T15:34:51.743" Score="7" Body="&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BWA-backtrack is based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Backtracking&quot; rel=&quot;noreferrer&quot;&gt;backtracking&lt;/a&gt;.&#xA;This approach is appropriate only when the dissimilarity between the reads and the reference is low,&#xA;or when you want to find all best hits or enumerate all possible alignments up to a specified&#xA;number of errors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In all other situations, BWA-MEM is preferable as it can,&#xA;thanks to its sophisticated strategy based on maximum exact matches,&#xA;deal with errors better and also automatically switch between local and&#xA;global alignment modes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Long description:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to provide some algorithmic insight since I believe that it&#xA;can be very useful in this case.&#xA;Both BWA-backtrack and BWA-MEM use the same indexing strategy&#xA;(heavily relying on BWT-index), but the actual algorithms are quite different.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/FM-index&quot; rel=&quot;noreferrer&quot;&gt;BWT-index&lt;/a&gt; (and also other full-text indexes such as&#xA;&lt;a href=&quot;https://en.wikipedia.org/wiki/Suffix_array&quot; rel=&quot;noreferrer&quot;&gt;suffix arrays&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Suffix_tree&quot; rel=&quot;noreferrer&quot;&gt;suffix trees&lt;/a&gt;)&#xA;can easily find exact matches (imagine Ctrl+F-like search in a text editor),&#xA;but any differences between the read and the reference,&#xA;such as sequencing errors or genomic variants,&#xA;make the situation complicated. One then needs to&#xA;somehow transform inexact matching to exact matching,&#xA;and all three BWA mappers (note that there exists also BWA-SW, but it is deprecated)&#xA;use quite different strategies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;BWA-backtrack&lt;/strong&gt; looks for substrings of the reference, which would be similar&#xA;to the entire read (end-to-end) using an algorithm&#xA;called &lt;a href=&quot;https://en.wikipedia.org/wiki/Backtracking&quot; rel=&quot;noreferrer&quot;&gt;backtracking&lt;/a&gt;.&#xA;First, it searches occurences of the read without any &quot;corrections&quot;.&#xA;If nothing found, it consideres all possible single edits; then two edits, etc.&#xA;To make mapping efficient, one usually wants to stop with the first&#xA;found alignment as this would be the best one.&#xA;When required, it is also possible to find the other equally good alignments&#xA;or to enumerate all alignments up to some edit distance or withing some divergance rate&#xA;(see the -N option of BWA-backtrack).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It turns out that&#xA;the time required for finding an alignment&#xA;can be exponential in the number of errors, which is probably the main problem of&#xA;backtracking-based approaches.&#xA;To prevent huge overheads due to dissimilar reads,&#xA;one needs to limit the number of allowed errors to some reasonable number&#xA;(see maxDiff in the &lt;a href=&quot;http://bio-bwa.sourceforge.net/bwa.shtml&quot; rel=&quot;noreferrer&quot;&gt;BWA man page&lt;/a&gt;)&#xA;and consider the other reads unaligned.&#xA;In the case of BWA-backtrack,&#xA;the minimum required identity level is ~97% with the default options (see the -n option).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact, the algorithm is more complicated and uses various heuristics such as&#xA;seed-and-extend or Z-dropoff in order to make the computation fast enough (at the price of lower accuracy). If you are interested&#xA;in more details, all these tricks are well described in the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/19451168&quot; rel=&quot;noreferrer&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;BWA-MEM&lt;/strong&gt; uses quite a different strategy. It detects long exact matches&#xA;between the read and the reference, and then chains them into local or global alignments,&#xA;based on what is more appropriate in that specific case.&#xA;Such an automatic local-global switching can be very powerful and&#xA;BWA-MEM works well with various types of data&#xA;(short reads, long reads, low error rates, high error rates, etc.).&lt;/p&gt;&#xA;" OwnerUserId="425" LastActivityDate="2017-06-07T15:34:51.743" CommentCount="0" />
  <row Id="547" PostTypeId="2" ParentId="531" CreationDate="2017-06-07T16:20:19.863" Score="3" Body="&lt;p&gt;You might try using &lt;a href=&quot;http://bedtools.readthedocs.io/en/latest/content/tools/genomecov.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;bedtools genomecov&lt;/a&gt; instead. If you provide the -d option, it reports the coverage at every position in the BAM file.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bedtools genomecov -d -ibam $inputfile &amp;gt; &quot;${inputfile}.genomecov&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You can also provide a BED file if you just want to calculate in the target  region.&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2017-06-07T16:20:19.863" CommentCount="0" />
  <row Id="548" PostTypeId="1" AcceptedAnswerId="550" CreationDate="2017-06-07T16:54:42.373" Score="10" ViewCount="694" Body="&lt;p&gt;As a small introductory project, I want to compare gene sequences of  different strains of flu virus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there publicly available databases of flu virus gene sequences?&lt;/p&gt;&#xA;" OwnerUserId="711" LastEditorUserId="191" LastEditDate="2017-06-07T22:19:26.737" LastActivityDate="2017-08-22T21:01:56.533" Title="Publicly available gene sequence database for viruses?" Tags="&lt;database&gt;&lt;public-databases&gt;&lt;genome-sequencing&gt;" AnswerCount="3" CommentCount="1" FavoriteCount="2" />
  <row Id="549" PostTypeId="2" ParentId="548" CreationDate="2017-06-07T17:31:26.533" Score="2" Body="&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/genomes/FLU/Database/nph-select.cgi?go=database&quot; rel=&quot;nofollow noreferrer&quot;&gt;Influenza virus resource&lt;/a&gt; at NCBI or &lt;a href=&quot;https://www.fludb.org/brc/home.spg?decorator=influenza&quot; rel=&quot;nofollow noreferrer&quot;&gt;FluDB&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="634" LastActivityDate="2017-06-07T17:31:26.533" CommentCount="1" />
  <row Id="550" PostTypeId="2" ParentId="548" CreationDate="2017-06-07T17:45:42.067" Score="13" Body="&lt;p&gt;There area few different influenza virus database resources:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The &lt;a href=&quot;http://www.fludb.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;Influenza Research Database (IRD)&lt;/a&gt; (a.k.a FluDB - based upon URL)&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A NIAID Bioinformatics Resource Center or BRC which highly curates the data brought in and integrates it with numerous other relevant data types&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;The &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/genomes/FLU/Database/nph-select.cgi?go=database&quot; rel=&quot;nofollow noreferrer&quot;&gt;NCBI Influenza Virus Resource&lt;/a&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A sub-project of the NCBI with data curated over and above the GenBank data that is part of the NCBI&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;The &lt;a href=&quot;http://platform.gisaid.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GISAID EpiFlu Database&lt;/a&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A database of sequences from the Global Initiative on Sharing All Influenza Data. Has unique data from many countries but requires user agree to a data sharing policy.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;The &lt;a href=&quot;http://openflu.vital-it.ch/browse.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;OpenFluDB&lt;/a&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Former GISAID database that contains some sequence data that GenBank does not have.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;For those who also may be interested in other virus databases, there are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.viprbrc.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;Virus Pathogen Resource (VIPR)&lt;/a&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A companion portal to the IRD, which hosts curated and integrated data for most other NIAID A-C virus pathogens including (but not limited to) Ebola, Zika, Dengue, Enterovirus, and Hepatitis C&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.hiv.lanl.gov/&quot; rel=&quot;nofollow noreferrer&quot;&gt;LANL HIV database&lt;/a&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Los Alamos National Laboratory HIV database with HIV data and many useful tools for all virus bioinformatics&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://pave.niaid.nih.gov&quot; rel=&quot;nofollow noreferrer&quot;&gt;PaVE: Papilloma virus genome database&lt;/a&gt; (from quintik comment)&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;NIAID developed and maintained Papilloma virus bioinformatics portal&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Disclaimer: I used to work for the IRD / VIPR and currently work for NIAID.&lt;/p&gt;&#xA;" OwnerUserId="714" LastEditorUserId="-1" LastEditDate="2017-08-22T21:01:56.533" LastActivityDate="2017-08-22T21:01:56.533" CommentCount="3" />
  <row Id="551" PostTypeId="2" ParentId="528" CreationDate="2017-06-07T19:22:15.420" Score="3" Body="&lt;h2&gt;Context&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The &lt;strong&gt;PDB file format&lt;/strong&gt; is a fixed-column file format designed in 1970s for storing structural models of macromolecules. The format has been around for long time, has many uses, and although it has &lt;a href=&quot;https://www.wwpdb.org/documentation/file-format-content/format33/v3.3.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;official spec&lt;/a&gt; the files in circulation may not strictly conform to it. It always has a list of atoms with coordinates&#xA;(the first two lines are added to stress that it's a fixed-column format, they are not part of the file):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;         1         2         3         4         5         6         7         8&#xA;12345678901234567890123456789012345678901234567890123456789012345678901234567890&#xA;ATOM      1  N   VAL A   1       3.320  14.780   4.844  1.00 35.53           N&#xA;ATOM      2  CA  VAL A   1       3.577  16.239   4.984  1.00 35.39           C&#xA;ATOM      3  C   VAL A   1       4.896  16.398   5.727  1.00 30.43           C&#xA;ATOM      4  O   VAL A   1       5.143  15.702   6.732  1.00 30.51           O&#xA;ATOM      5  CB  VAL A   1       2.343  16.975   5.494  1.00 45.39           C&#xA;ATOM      6  CG1 VAL A   1       2.586  18.497   5.590  1.00 60.06           C&#xA;ATOM      7  CG2 VAL A   1       1.103  16.811   4.634  1.00 53.93           C&#xA;ATOM      8  N   LEU A   2       5.748  17.241   5.158  1.00 28.04           N&#xA;ATOM      9  CA  LEU A   2       7.116  17.471   5.661  1.00 24.31           C&#xA;ATOM     10  C   LEU A   2       7.166  18.490   6.792  1.00 24.00           C&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or, to show some RNA:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ATOM     42  N3    G B   2       9.252  12.871  -1.168  1.00 36.98           N&#xA;ATOM     43  C4    G B   2       8.424  12.964  -2.233  1.00 34.09           C&#xA;ATOM     44  P     A B   3      10.376   8.321  -4.834  1.00 40.53           P&#xA;ATOM     45  OP1   A B   3      11.773   8.279  -5.364  1.00 38.97           O&#xA;ATOM     46  OP2   A B   3       9.396   7.283  -5.218  1.00 39.32           O&#xA;ATOM     47  O5'   A B   3      10.429   8.473  -3.211  1.00 36.55           O&#xA;ATOM     48  C5'   A B   3      11.698   8.232  -2.554  1.00 35.13           C&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;strong&gt;Protein Data Bank&lt;/strong&gt; -- international institution that archives and annotates structural models of biological molecules that anyone can &lt;a href=&quot;https://www.wwpdb.org/deposition/deposition-resources&quot; rel=&quot;nofollow noreferrer&quot;&gt;deposit&lt;/a&gt; -- is now using &lt;strong&gt;mmCIF&lt;/strong&gt; as the primary format. The fixed-column PDB format had inherent limitations (max. 99,999 atoms, but also it was hard to include additional info). They still generate PDB files if possible (i.e. except the largest structures), but the only format accepted now for depositions is mmCIF.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;mmCIF has a rather not intuitive &lt;a href=&quot;https://www.iucr.org/resources/cif/spec/version1.1/cifsyntax&quot; rel=&quot;nofollow noreferrer&quot;&gt;CIF syntax&lt;/a&gt; (think JSON or XML, but designed before XML),&#xA;plus ontology defined by &lt;a href=&quot;http://mmcif.wwpdb.org/dictionaries/mmcif_pdbx_v50.dic/Groups/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;PDBx/mmCIF dictionary&lt;/a&gt; (think XML Schema).&#xA;The content of mmCIF is divided into table-like categories with relations between tables (it was designed at the peak of RDMBS popularity) and is a bit harder to work with, so the old PDB format is more popular.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Sequence from PDB&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The PDB format has a record called &lt;code&gt;SEQRES&lt;/code&gt; that explicitly lists the sequence, for example (5NEO):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SEQRES   1 A   18    G   G   U   G   G   G   G   A   C   G   A   C   C          &#xA;SEQRES   2 A   18    C   C   A CBV   C&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The mmCIF format has more details:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;_entity_poly.pdbx_seq_one_letter_code       'GGUGGGGACGACCCCA(CBV)C' &#xA;_entity_poly.pdbx_seq_one_letter_code_can   GGUGGGGACGACCCCACC &#xA;_entity_poly.pdbx_strand_id                 A &#xA;&#xA;loop_&#xA;_entity_poly_seq.entity_id &#xA;_entity_poly_seq.num &#xA;_entity_poly_seq.mon_id &#xA;_entity_poly_seq.hetero &#xA;1 1  G   n &#xA;1 2  G   n &#xA;1 3  U   n &#xA;1 4  G   n &#xA;1 5  G   n &#xA;1 6  G   n &#xA;1 7  G   n &#xA;1 8  A   n &#xA;1 9  C   n &#xA;1 10 G   n &#xA;1 11 A   n &#xA;1 12 C   n &#xA;1 13 C   n &#xA;1 14 C   n &#xA;1 15 C   n &#xA;1 16 A   n &#xA;1 17 CBV n &#xA;1 18 C   n &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The sequence is conveniently extracted for you.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally, &lt;code&gt;_entity_poly_seq&lt;/code&gt; in mmCIF includes information about microheterogeneity, and the SEQRES record doesn't. It is relevant when the model has two alternative residues in the same place, because part of the sample had a point mutation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PDB files that didn't come from PDB usually don't have the SEQRES record.&#xA;You may get the sequence from the list of atoms (residue names in columns 18-20), but some residues may be missing in the atom list if atomic positions could not be determined.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So it should be easy to extract the sequence either way.&#xA;For example, BioPython has module BioSeqIO with two PDB pseudo-formats:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;pdb-seqres - Reads a Protein Data Bank (PDB) file to determine the complete protein sequence as it appears in the header (no dependencies).&lt;/li&gt;&#xA;&lt;li&gt;pdb-atom - Uses Bio.PDB to determine the (partial) protein sequence as it appears in the structure based on the atom coordinate section of the file (requires NumPy for Bio.PDB).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;I just tried BioPython 1.66 and while &lt;code&gt;SeqIO.parse()&lt;/code&gt; can extract protein sequence, it fails with RNA, such as the 5NEO above. Ooops.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Secondary structure restraints&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We may be thinking about different restraints here. I'll write about restraints used in refinement.&#xA;The experimental data alone is normally not sufficient to refine a model, so one needs restraints that represent prior knowledge - lengths of atomic bonds, angles, planarity restraints, restraints based on local similarity to other structures that were determined from higher resolution data, etc, whatever can help to make a sensible model. Refinement programs (such as Refmac, BUSTER, Phenix.refine) try to fit the model to the data and satisfy geometrical restraints at the same time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Macromolecular crystallographic software is quite fragmented and one normally uses several different programs in the process. It happens that the programs to generate secondary structure restraints are separate from the actual refinement programs. (Secondary structure restraints are less essential than restraints for covalent bonds). CCP4 has a program called LibG that makes DNA/RNA restraints for Refmac. Phenix has a program called &lt;a href=&quot;https://www.phenix-online.org/documentation/reference/secondary_structure_restraints.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;phenix.secondary_structure_restraints&lt;/a&gt;,&#xA;or you can use a &lt;a href=&quot;http://rna.ucsc.edu/pdbrestraints/&quot; rel=&quot;nofollow noreferrer&quot;&gt;server from UCSC&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These restraints don't have a &quot;bracket format&quot;, but they explicitly specify expected distances and angles between atoms. For example (for phenix.refine):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  bond { &#xA;    action = *add &#xA;    atom_selection_1 = chain  A and resid    1  and name  O6 &#xA;    atom_selection_2 = chain  A and resid   18  and name  N4 &#xA;    distance_ideal = 2.91&#xA;    sigma = 0.1&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or (for Refmac):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;exte dist first chain A resi 16 ins  . atom N6 second chain A resi 3 ins . atom O4 value 2.94 sigma 0.15 type 1 &#xA;exte dist first chain A resi 16 ins  . atom N1 second chain A resi 3 ins . atom N3 value 2.84 sigma 0.1 type 1 &#xA;exte torsion first chain A resi 16 ins  . atom C2 second chain A resi 16 ins . atom N1 third chain A resi 3 ins . atom N3 fourth chain A resi 3 ins . atom C4 value 180 sigma 15 type 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="266" LastEditorUserId="266" LastEditDate="2017-06-07T23:53:58.087" LastActivityDate="2017-06-07T23:53:58.087" CommentCount="0" />
  <row Id="552" PostTypeId="2" ParentId="548" CreationDate="2017-06-07T19:58:48.613" Score="4" Body="&lt;p&gt;In addition to what others have suggested, I would also recommend &lt;a href=&quot;https://pave.niaid.nih.gov/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PaVE&lt;/a&gt; as a resource. This is a curated database maintained by the NIAID and current holds over 300 papilloma virus genomes.&lt;/p&gt;&#xA;" OwnerUserId="712" LastEditorUserId="77" LastEditDate="2017-06-08T06:47:19.023" LastActivityDate="2017-06-08T06:47:19.023" CommentCount="3" />
  <row Id="553" PostTypeId="1" AcceptedAnswerId="555" CreationDate="2017-06-07T22:30:55.380" Score="5" ViewCount="57" Body="&lt;p&gt;I'm a newcomer to the world of bioinformatics, and in need of help solving a problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to take a list of human proteins, and identify segments (13-17aa in length) with a high degree of similarity to microbial sequences. Ideally, I would like to start with list of FASTA sequences, and have an easy way to generate an output of the corresponding high similarity segments of each protein.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there existing tools or software that I should be aware of that will make my life easier?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;" OwnerUserId="727" LastEditorUserId="45" LastEditDate="2017-06-10T14:39:04.273" LastActivityDate="2017-06-10T14:39:04.273" Title="Detecting portions of human proteins with high degree of microbial similarity" Tags="&lt;fasta&gt;&lt;sequence-homology&gt;" AnswerCount="1" CommentCount="8" FavoriteCount="0" />
  <row Id="554" PostTypeId="2" ParentId="531" CreationDate="2017-06-07T22:34:32.847" Score="0" Body="&lt;p&gt;Just replace the &lt;code&gt;-a&lt;/code&gt; option with &lt;code&gt;-aa&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools depth -b $bedfile -aa $inputfile&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I see that you're using the GRCh38 human reference genome build, which includes alternate scaffolds that represent a wider variety of genomic variation in the human genome. This was quite a substantial change from &lt;a href=&quot;https://www.quora.com/What-are-the-differences-between-GRCh38-and-GRCh37&quot; rel=&quot;nofollow noreferrer&quot;&gt;previous reference genome builds&lt;/a&gt;, and many bioinformatics tools still haven't been updated to accept the GRCh38 reference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The discrepancy in entry size is probably because there are alternate contigs that are uncovered by the reads that have been mapped. This is expected, but can cause a bit of confusion.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-07T22:34:32.847" CommentCount="1" />
  <row Id="555" PostTypeId="2" ParentId="553" CreationDate="2017-06-07T22:51:54.867" Score="4" Body="&lt;p&gt;Sounds like precisely the job BLAST was developed for. Now, which flavor will depend on what you want to do and what data you have available. Some options:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;PSI-BLAST&lt;/strong&gt;: this is usually the best choice if you are trying to find protein homologs. It works by building a hidden markov model describing your query sequence and using that model to query a database of proteins. The advantage is that it is run in multiple iterations, giving you the chance to add or remove results (so you add the ones that are true positives and remove false ones), eventually building a pretty good model of your protein. This is far moer powerful than a simple homology-based approach since proteins work via protein domains and simple homology is not as important as specific conserved functional residues. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For this, go to the NCBI &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&amp;amp;PAGE_TYPE=BlastSearch&amp;amp;LINK_LOC=blasthome&quot; rel=&quot;nofollow noreferrer&quot;&gt;protein blast page&lt;/a&gt; and select PSI-BLAST:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/q4yAK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/q4yAK.png&quot; alt=&quot;psi-blast option at ncbi&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;BLASTp&lt;/strong&gt;: Simple protein-protein blast. It will identify homologous proteins based on sequence similarity. Whether or not that also implies functional homology is not that simple and will depend on each case you investigate. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As above, go to the NCBI &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&amp;amp;PAGE_TYPE=BlastSearch&amp;amp;LINK_LOC=blasthome&quot; rel=&quot;nofollow noreferrer&quot;&gt;protein blast page&lt;/a&gt;, but this time use the defaults.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;tBLASTn&lt;/strong&gt;: this is a tool that takes protein sequences as input and compares them to a database of DNA which is dynamically translated in all 6 possible reading frames. Very good for finding homologous sequences when you don't have well annotated protein information for the target species. It has the benefit of being more sensitive and able to find more distant homologies than basic nucleotide BLASTn and the go-to approach when your target species is distant and not well annotated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=tblastn&amp;amp;PAGE_TYPE=BlastSearch&amp;amp;BLAST_SPEC=&amp;amp;LINK_LOC=blasttab&amp;amp;LAST_PAGE=blastp&quot; rel=&quot;nofollow noreferrer&quot;&gt;NCBI's tBLASTn page&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;All of these can be run online through the &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi&quot; rel=&quot;nofollow noreferrer&quot;&gt;NCBI's BLAST page&lt;/a&gt;. If you want to investigate hundreds of proteins, I suggest you install blast locally. You can then either download the relevant target sequences from NCBI and rebuild the blast database locally (if so, I suggest you ask a new question about how to do that) or, use NCBI's remote blast client which lets you use a locally stored query file and will run blast on the NCBI's servers. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, these programs will return what are known as High Scoring Pairs (HSPs), the regions of your query sequence(s) that align well to the target. There are various options you can play with to improve the sensitivity or the specificity but a discussion of those would require far more details about what you're doing and would be best in a new question as well. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have your HSPs, you can relatively easily parse them to select regions with a given range of sequence similarity values and of a specific length. Once again, that would be better discussed in a separate question once you have your results and can show an example. &lt;/p&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-06-07T23:04:27.220" LastActivityDate="2017-06-07T23:04:27.220" CommentCount="2" />
  <row Id="556" PostTypeId="2" ParentId="528" CreationDate="2017-06-07T23:31:46.327" Score="1" Body="&lt;p&gt;If you know how to use python:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1 - Download the ModeRNA module from &lt;a href=&quot;https://github.com/lenarother/moderna&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; and install&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2 - from a python IDLE execute:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from moderna import *&#xA;m = load_model('file.pdb', 'A') #A is the chain&#xA;seq = m.get_sequence()&#xA;sec = m.get_secstruc()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;the variable &lt;code&gt;sec&lt;/code&gt; store the secondary structure in dot-bracket notation.&lt;/p&gt;&#xA;" OwnerUserId="729" LastActivityDate="2017-06-07T23:31:46.327" CommentCount="0" />
  <row Id="557" PostTypeId="2" ParentId="318" CreationDate="2017-06-08T00:09:09.243" Score="0" Body="&lt;p&gt;For mouse, I still see people using mm9/NCBI37 in high profile publications even though mm10/GRCm38 was released more than 5 years ago (2011). I personally don't think that's a great idea, but it's certainly valid according to the peer reviewers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It also depends on your application. If you are working with coding regions (likely well known for a long time) or extracting genome-wide stats (enrichment at TSS, for example), the differences should be negligible.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-06-08T00:09:09.243" CommentCount="0" />
  <row Id="558" PostTypeId="1" CreationDate="2017-06-08T01:03:40.633" Score="2" ViewCount="60" Body="&lt;p&gt;It's supposed that quantum computers could help to find the native state of all human DNA proteins.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know how to formulate the protein folding problem into a decision problem solvable either by &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_annealing&quot; rel=&quot;nofollow noreferrer&quot;&gt;quantum annealers&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_Turing_machine&quot; rel=&quot;nofollow noreferrer&quot;&gt;universal quantum computers&lt;/a&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would also be interesting to know, how to model a decision problem for classical computers, like &lt;a href=&quot;http://folding.stanford.edu/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Folding@Home&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="732" LastEditorUserId="96" LastEditDate="2017-06-08T03:39:52.487" LastActivityDate="2017-06-08T17:54:28.040" Title="Protein folding problem model for quantum computers?" Tags="&lt;proteins&gt;&lt;computation&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="559" PostTypeId="1" AcceptedAnswerId="560" CreationDate="2017-06-08T02:25:46.173" Score="9" ViewCount="107" Body="&lt;p&gt;I have a gene expression quantification file from &lt;a href=&quot;https://portal.gdc.cancer.gov/files/92e73892-811b-4edd-b3db-d452bc5d28e0&quot; rel=&quot;nofollow noreferrer&quot;&gt;TCGA&lt;/a&gt; that contains the following lines:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ENSG00000242268.2   591.041000514&#xA;ENSG00000270112.3   0.0&#xA;ENSG00000167578.15  62780.6543066&#xA;ENSG00000273842.1   0.0&#xA;ENSG00000078237.5   36230.832883&#xA;ENSG00000146083.10  189653.152706&#xA;ENSG00000225275.4   0.0&#xA;ENSG00000158486.12  420.761140072&#xA;ENSG00000198242.12  2914738.3675&#xA;ENSG00000259883.1   1632.83700531&#xA;ENSG00000231981.3   0.0&#xA;ENSG00000269475.2   0.0&#xA;ENSG00000201788.1   0.0&#xA;ENSG00000134108.11  925529.547944&#xA;ENSG00000263089.1   2646.63769677&#xA;ENSG00000172137.17  23162.6989867&#xA;ENSG00000167700.7   291192.25157&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;What is the &lt;code&gt;.number&lt;/code&gt; that are added to the Gene e.g. the &lt;code&gt;.2&lt;/code&gt; in &lt;code&gt;ENSG00000242268.2&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Why isn't the quantified value an integer; what does &lt;code&gt;591.041000514&lt;/code&gt; mean?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In case someone one wants more info &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/gene/?term=100507661&quot; rel=&quot;nofollow noreferrer&quot;&gt;about the gene LINC02082&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="734" LastEditorUserId="292" LastEditDate="2017-06-08T10:28:06.803" LastActivityDate="2017-06-08T10:28:06.803" Title="How to read and interpret a gene expression quantification file?" Tags="&lt;gene&gt;&lt;normalization&gt;&lt;identifiers&gt;&lt;quantification&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="560" PostTypeId="2" ParentId="559" CreationDate="2017-06-08T03:05:39.547" Score="8" Body="&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The first column contains Ensembl gene identifiers, and the suffix is a version number that can be used to track changes to the gene annotations over time. From the &lt;a href=&quot;http://www.ensembl.org/info/genome/stable_ids/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;strong&gt;Ensembl Stable IDs&lt;/strong&gt; documentation&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Ensembl annotation uses a system of stable IDs that have prefixes based on the species scientific name plus the feature type, followed by a series of digits and a version e.g. ENSG00000139618.1. The version may be omitted.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Following the &lt;a href=&quot;https://portal.gdc.cancer.gov/files/92e73892-811b-4edd-b3db-d452bc5d28e0&quot; rel=&quot;nofollow noreferrer&quot;&gt;first link&lt;/a&gt; you provided leads to a page with details for the file &lt;code&gt;2edcaaa7-63b4-40b4-abbe-5d7a84012e60.FPKM-UQ.txt.gz&lt;/code&gt;. The first thing that caught my eye about this filename was &lt;code&gt;FPKM&lt;/code&gt;, or &quot;fragments per kilobase of exon per million reads&quot;, which is a commonly used unit of RNA expression. Since these are not raw read counts, there's no expectation that these values should be integers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The best explanation I've seen of FPKM comes from &lt;a href=&quot;https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/&quot; rel=&quot;nofollow noreferrer&quot;&gt;a blog post&lt;/a&gt; written by Harold Pimentel of &lt;a href=&quot;http://pachterlab.github.io/kallisto/&quot; rel=&quot;nofollow noreferrer&quot;&gt;kallisto&lt;/a&gt; and &lt;a href=&quot;https://pachterlab.github.io/sleuth/&quot; rel=&quot;nofollow noreferrer&quot;&gt;sleuth&lt;/a&gt; fame. From the blog post:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The interpretation of FPKM is as follows: if you were to sequence this pool of RNA again, you expect to see &lt;code&gt;FPKM_i&lt;/code&gt; fragments for each thousand bases in the feature for every &lt;code&gt;N/10^6&lt;/code&gt; fragments you’ve sequenced. It’s basically just the rate of fragments per base multiplied by a big number (proportional to the number of fragments you sequenced) to make it more convenient.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;More generally though, even when FPKM is not the unit of expression abundance used, most quantification methods and associated units of expression will not produce integer estimates.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-06-08T06:45:32.780" LastActivityDate="2017-06-08T06:45:32.780" CommentCount="1" />
  <row Id="561" PostTypeId="1" CreationDate="2017-06-08T04:54:36.177" Score="4" ViewCount="143" Body="&lt;p&gt;I have some FASTQ sequence files and a FASTA file for some regions I'm interested in.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Build an index for the FASTA file&lt;/li&gt;&#xA;&lt;li&gt;Use the index to count number of k-mers occurred in my sequence files&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I know how to do this in many k-mer counting tools, such as Kallisto, Jellyfish, DSK, Salmon. However, I'm more interested in Python (khmer is a Python package, all the other k-mer counting tools are C++).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, I can't figure out how to do that in khmer from the documentation:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;a href=&quot;http://khmer.readthedocs.io/en/v2.1.1/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://khmer.readthedocs.io/en/v2.1.1/introduction.html&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It talks about RNA-Seq, metagenomics, de Bruijn graph etc... All I need are just counts. Khmer is a popular k-mer tool, so it should be able to do it, but how?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; How to build an index and count k-mers using khmer?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't need number of unique k-mers, I'm looking for a solution like Jellyfish where we build an index and count k-mers to the index.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;MY_SEQ_1  100 counts&#xA;MY_SEQ_2  134 counts&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="174" LastEditorUserId="174" LastEditDate="2017-06-08T05:12:12.183" LastActivityDate="2017-07-27T20:08:44.917" Title="How to use khmer to count k-mers?" Tags="&lt;genome&gt;&lt;k-mer&gt;&lt;metagenome&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="562" PostTypeId="2" ParentId="561" CreationDate="2017-06-08T05:03:42.780" Score="1" Body="&lt;p&gt;If you want to count the number of unique k-mers that occur in your data set, you should use the &lt;a href=&quot;http://khmer.readthedocs.io/en/v2.1.1/user/scripts.html#unique-kmers-py&quot; rel=&quot;nofollow noreferrer&quot;&gt;unique-kmers.py&lt;/a&gt; script, which implements a HyperLogLog-based cardinality estimator.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the number of unique k-mers is not what you're after, please clarify.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;khmer uses probabilistic data structures internally, which store k-mers as hashed values that cannot be un-hashed back into k-mer sequences uniquely. Querying a k-mer's abundance requires you to know which k-mer(s) you're looking for, which in this case would require a second pass over the reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The closest thing to what you're asking provided in khmer's command-line scripts is &lt;a href=&quot;http://khmer.readthedocs.io/en/v2.1.1/user/scripts.html#abundance-dist-py&quot; rel=&quot;nofollow noreferrer&quot;&gt;abundance-dist.py&lt;/a&gt; (or the alternative abundance-dist-single.py), which will produce a k-mer abundance histogram but not per-kmer abundance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;http://khmer.readthedocs.io/en/v2.1.1/user/scripts.html#load-into-counting-py&quot; rel=&quot;nofollow noreferrer&quot;&gt;load-into-counting.py&lt;/a&gt; scripts computes the k-mer abundances and stores them in a probabilistic data structure, which is written to disk and can subsequently be re-loaded into memory quickly. Many of the scripts in khmer expect the reads to have been pre-processed by &lt;code&gt;load-into-counting.py&lt;/code&gt;, while others will invoke the counting routines directly themselve. It's also fairly easy to do this with the Python API. In every case, the accuracy of the k-mer counts is dependant on &lt;a href=&quot;http://khmer.readthedocs.io/en/v2.1.1/user/choosing-table-sizes.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;memory considerations&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import khmer&#xA;&amp;gt;&amp;gt;&amp;gt; counts = khmer.Counttable(31, 1e7, 4)&#xA;&amp;gt;&amp;gt;&amp;gt; counts.consume_seqfile('reads.fq.gz')&#xA;(25000, 1150000)&#xA;&amp;gt;&amp;gt;&amp;gt; counts.get('TGACTTTCTTCGCTTCCTGACGGCTTATGCC')&#xA;117&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To check the false positive rate:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; fpr = khmer.calc_expected_collisions(counts)&#xA;&amp;gt;&amp;gt;&amp;gt; fpr&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you &lt;em&gt;must&lt;/em&gt; have the counts of each k-mer, it's not much more work...although reporting each k-mer's abundance only once would consume a lot of memory with a naive approach.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; outfile = open('outfile.txt', 'w')&#xA;&amp;gt;&amp;gt;&amp;gt; seenkmers = set()  # Consumes a lot of memory for large input!!!&#xA;&amp;gt;&amp;gt;&amp;gt; for read in khmer.ReadParser('reads.fq.gz):&#xA;...     for kmer in counts.get_kmers(read.sequence):&#xA;...         if kmer not in seenkmers:&#xA;...             print(kmer, counttable.get(kmer), file=outfile)&#xA;...             seenkmers.add(kmer)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-06-08T06:04:19.577" LastActivityDate="2017-06-08T06:04:19.577" CommentCount="1" />
  <row Id="563" PostTypeId="2" ParentId="527" CreationDate="2017-06-08T05:48:07.953" Score="1" Body="&lt;p&gt;I mostly use TargetScan predictions. But here are few curated databases:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://mirgate.bioinfo.cnio.es/miRGate/&quot; rel=&quot;nofollow noreferrer&quot;&gt;miRGate&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;miRGate is a curated database of human, mouse and rat miRNAs/mRNAs targets. It is designed to analyze miRNA and gene isoforms lists under a common and consistent space of annotations. Including all existing 3 UTR and the entirely known miRNAs. All Havana biotypes and ENCODE principal isoforms for the three organisms are also included. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/25858286&quot; rel=&quot;nofollow noreferrer&quot;&gt;Publication&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.mir2disease.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;miR2Disease&lt;/a&gt; : &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;a manually curated database, aims at providing a comprehensive resource of miRNA deregulation in various human diseases. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2686559/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Publication&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://pcsb.ahau.edu.cn:8080/PASmiR/f&quot; rel=&quot;nofollow noreferrer&quot;&gt;PASmiR&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;PASmiR, a literature-curated and web-accessible database, was thus developed to provide a solid platform for the collection, standardization, and searching of these miRNA-stress regulation data in plants. &lt;a href=&quot;https://bmcplantbiol.biomedcentral.com/articles/10.1186/1471-2229-13-33&quot; rel=&quot;nofollow noreferrer&quot;&gt;Publication&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="161" LastActivityDate="2017-06-08T05:48:07.953" CommentCount="0" />
  <row Id="564" PostTypeId="2" ParentId="318" CreationDate="2017-06-08T06:38:40.213" Score="1" Body="&lt;p&gt;You could use liftOver which isn't always great.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Whenever I encounter this (especially NGS data readily available on the SRA), I often just get the raw files (e.g. fastqs) and re-align/re-map.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your case (arrays) it may be a bit tough. Not impossible though, as I recently took some old yeast DNA/RNA microarray data and updated it to the newest genome. Just requires the right data (like DNA for normalization) and a good understanding of the entire process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A last resort/alternative is to align your &lt;strong&gt;new&lt;/strong&gt; data to the old genome to be able to make comparisons. This isn't ideal but works in cases where upgrading one source isn't possible or is a HUGE amount of time/effort. I've done this for a few fly experiments where all the available/previous data was done in dm3. All the old genomes can generally be found on &lt;a href=&quot;http://archive.ensembl.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://archive.ensembl.org&lt;/a&gt; .&lt;/p&gt;&#xA;" OwnerUserId="383" LastActivityDate="2017-06-08T06:38:40.213" CommentCount="0" />
  <row Id="565" PostTypeId="1" CreationDate="2017-06-08T07:06:29.710" Score="7" ViewCount="113" Body="&lt;p&gt;Bashing file formats is a favorite pastime in bioinformatics, and annotation file formats such as GFF and BED seem to get special attention. A lot of this frustration stems from community's shockingly inconsistent adherence to specifications and conventions, but there are also some (dare I say objectively) problematic design choices in each of these formats.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;GFF (and its more common derivatives GTF and GFF3) use 1-based closed interval notation, which optimizes for human comprehension but is far inferior to 0-based half-open interval notation (such as used by BED) for computations involving interval arithmetic.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Although BED and GTF were designed for very specific use cases (visualization and gene prediction, respectively), they have been used and abused in a much wider set of contexts. For example, the BED fields related to the &lt;strong&gt;thick part&lt;/strong&gt; are irrelevant if you're not plotting them in a genome browser.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;BED supports a single level of feature decomposition (a feature can be broken up into blocks). GTF supports two levels (exons grouped by transcript_id, transcripts grouped by gene_id). In contrast, GFF3 supports an arbitrary number of levels, and uses parent/child relationships defined by &lt;code&gt;ID&lt;/code&gt; and &lt;code&gt;Parent&lt;/code&gt; attributes to declare a directed acyclic graph of features.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Data that does not fit into mandatory pre-defined fields must be relegated to optional fields or free-form attribute key/value pairs. While this flexibility is powerful, a common complaint is that &quot;all the action&quot; happens in these optional/free-form fields.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;There is a dearth of validation tools, and those that do exist focus primarily on validating syntax and not semantics. To use an aging analogy, it's one thing to say an XML file is valid, but it's completely different to validate it against a schema. There are essentially no widely used tools that do the latter for annotation files.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;If we were tasked with creating a new annotation format, and if we were guaranteed the resources needed to develop it, and interest and wide adoption from the wider community (one can dream!), what design criteria should be considered in the development of this new format? &lt;strong&gt;What, if anything, makes an objectively good annotation data format?&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="73" LastEditDate="2017-06-08T14:46:51.810" LastActivityDate="2017-06-09T01:59:00.970" Title="Annotation format design" Tags="&lt;file-formats&gt;&lt;bed&gt;&lt;gff3&gt;&lt;sequence-annotation&gt;" AnswerCount="4" CommentCount="1" />
  <row Id="566" PostTypeId="2" ParentId="565" CreationDate="2017-06-08T07:18:13.830" Score="4" Body="&lt;p&gt;Presuming we consider &quot;human readable&quot;, &quot;easily parsable&quot;, and &quot;quickly queryable&quot; to be objectively good qualities (and if not, I worry for the future):&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Text-based: It's absurdly common to want to use &lt;code&gt;grep&lt;/code&gt; or &lt;code&gt;awk&lt;/code&gt; on annotations. Sure, one could make variants of these that are binary-format aware, but why reinvent the wheel. Of course text files don't innately allow region-based queries of their contents, so on to point 2...&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This should further explicitly be line-based. Fields would need to be tab-separated (ideally it'd use the ascii record separator character instead, but I fear that ship has long ago sailed).&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;A strictly defined line order: This is one of my personal pet-peeves about BED/GFF/GTF files. If you're going to make a text-based annotation format, everyone would end up ahead if said format made explicit that it should be sorted. This then allows things like tabix to be used so the &quot;query a region&quot; problem is then solved. But I would go further than that. The issue with things like GFF is that there are multiple inter-dependent lines and there's no strict rule about whether a parent line absolutely must come before a child line. This just makes implementing things a nightmare and more often than not a randomly ordered file will just break tools. Since GTF or GFF-style interdependent lines likely be how any annotation format works, this ordering between lines with the same start position should be made explicit in the format.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-08T08:50:10.537" LastActivityDate="2017-06-08T08:50:10.537" CommentCount="5" />
  <row Id="567" PostTypeId="1" AcceptedAnswerId="569" CreationDate="2017-06-08T07:21:27.027" Score="7" ViewCount="126" Body="&lt;p&gt;Assume I have found the top 0.01% most frequent genes from a &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/559/how-to-read-and-interpret-a-gene-expression-quantification-file&quot;&gt;gene expression file&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say, these are 10 genes and I want to study the &lt;a href=&quot;https://en.wikipedia.org/wiki/Protein%E2%80%93protein_interaction&quot; rel=&quot;nofollow noreferrer&quot;&gt;protein protein interactions&lt;/a&gt;, the protein network and pathway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought to use &lt;a href=&quot;https://string-db.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;string-db&lt;/a&gt; or &lt;a href=&quot;http://interactome.dfci.harvard.edu&quot; rel=&quot;nofollow noreferrer&quot;&gt;interactome&lt;/a&gt;, but I am not sure what would be a plausible way to approach this problem and to build the protein network etc. Are there other more suitable databases?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I build a mathematical graph or network for these data?&lt;/p&gt;&#xA;" OwnerUserId="734" LastEditorUserId="734" LastEditDate="2017-06-10T06:04:33.900" LastActivityDate="2017-06-16T10:50:45.063" Title="How can I build a protein network pathway from a gene expression quantification file?" Tags="&lt;proteins&gt;&lt;networks&gt;&lt;gene&gt;&lt;pathway&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="1" />
  <row Id="568" PostTypeId="1" AcceptedAnswerId="571" CreationDate="2017-06-08T07:57:21.707" Score="4" ViewCount="52" Body="&lt;p&gt;This question is based on &lt;a href=&quot;https://www.biostars.org/p/129057/&quot; rel=&quot;nofollow noreferrer&quot;&gt;a question&lt;/a&gt; on BioStars  posted &gt;2 years ago by user jack.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It describes a very frequent problem of generating GO annotations for non-model organisms. While it is based on some specific format and single application (Ontologizer), it would be useful to have a general description of the pathway to getting to a GAF file. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note, that the input format is lacking a bit of essential information, like how it was obtained. Therefore, it is har to assign evidence code. Therefore, lets assume that the assignments of GO terms were done automagically. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;I want to do the Gene enrichment using Ontologizer without a&#xA;  predefined association file(it's not model organism). &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;I have parsed a file with two columns for that organism like this : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;geneA  GO:0006950,GO:0005737&#xA;geneB  GO:0016020,GO:0005524,GO:0006468,GO:0005737,GO:0004674,GO:0006914,GO:0016021,GO:0015031&#xA;geneC  GO:0003779,GO:0006941,GO:0005524,GO:0003774,GO:0005516,GO:0005737,GO:0005863&#xA;geneD  GO:0005634,GO:0003677,GO:0030154,GO:0006350,GO:0006355,GO:0007275,GO:0030528&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;I have downloaded the .ob file from Gene ontology file which contain&#xA;  this information (from &lt;a href=&quot;http://www.geneontology.org/doc/GO.terms_and_ids&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;) : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;!&#xA;! GO IDs (primary only) and name text strings&#xA;! GO:0000000 [tab] text string [tab] F|P|C&#xA;! where F = molecular function, P = biological process, C = cellular component&#xA;!&#xA;GO:0000001  mitochondrion inheritance   P&#xA;GO:0000002  mitochondrial genome maintenance    P&#xA;GO:0000003  reproduction    P&#xA;GO:0000005  ribosomal chaperone activity    F&#xA;GO:0000006  high affinity zinc uptake transmembrane transporter activity    F&#xA;GO:0000007  low-affinity zinc ion transmembrane transporter activity    F&#xA;GO:0000008  thioredoxin F&#xA;GO:0000009  alpha-1,6-mannosyltransferase activity  F&#xA;GO:0000010  trans-hexaprenyltranstransferase activity   F&#xA;GO:0000011  vacuole inheritance P&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;What I need as output is .gaf file in the following format (in the&#xA;  format of the files &lt;a href=&quot;http://geneontology.org/page/download-annotations&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;!gaf-version: 2.0&#xA;&#xA;!Project_name: Leishmania major GeneDB&#xA;&#xA;!URL: http://www.genedb.org/leish&#xA;&#xA;!Contact Email: mb4@sanger.ac.uk&#xA;&#xA; GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0003723    PMID:22396527    ISO    GeneDB:Tb927.10.10130    F    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor       &#xA; GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0044429    PMID:20660476    ISS        C    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20100803 GeneDB_Lmajor             GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0016554    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene   taxon:347515    20120910    GeneDB_Lmajor       &#xA; GeneDB_Lmajor    LmjF.36.4770    LmjF.36.4770        GO:0048255    PMID:22396527    ISO    GeneDB:Tb927.10.10130    P    mitochondrial RNA binding complex 1 subunit, putative    LmjF36.4770    gene    taxon:347515    20120910    GeneDB_Lmajor  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;How to create your own GO association file (gaf)?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="311" LastEditorUserId="298" LastEditDate="2017-06-08T11:20:04.643" LastActivityDate="2017-06-08T20:43:01.263" Title="How can I create my own GO association file (gaf)?" Tags="&lt;annotation&gt;&lt;gene-ontology&gt;&lt;gaf&gt;" AnswerCount="1" CommentCount="10" />
  <row Id="569" PostTypeId="2" ParentId="567" CreationDate="2017-06-08T08:24:57.997" Score="6" Body="&lt;p&gt;There are multiple ways to do this, and multiple protein interaction databases besides the ones you mentioned, such as &lt;a href=&quot;https://thebiogrid.org/&quot; rel=&quot;noreferrer&quot;&gt;BioGRID&lt;/a&gt; or &lt;a href=&quot;http://www.ebi.ac.uk/intact/&quot; rel=&quot;noreferrer&quot;&gt;IntAct&lt;/a&gt;. Interaction databases are different in how interactions are defined, sometimes it can be experimental evidence of interaction, sometimes coexpression, orthology-based predictions, etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is no single solution to your problem. For String-DB you can use their R package &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/STRINGdb.html&quot; rel=&quot;noreferrer&quot;&gt;STRINGdb&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# string-db&#xA;library(STRINGdb)&#xA;&#xA;# retrieve full graph for H. sapiens&#xA;string_db &amp;lt;- STRINGdb$new(version=&quot;10&quot;, species=9606,&#xA;                          score_threshold=400, input_directory=&quot;&quot; )&#xA;&#xA;# define genes of interest&#xA;genes.of.interest &amp;lt;- ...&#xA;&#xA;# get their neighbors&#xA;neighbors &amp;lt;- string_db$get_neighbors(genes.of.interest)&#xA;&#xA;# get the subgraph of interest (your genes + their neighbors)&#xA;my.subgraph &amp;lt;- string_db$get_subnetwork(c(genes.of.interest, neighbors))&#xA;&#xA;# look how many genes and interactions you have&#xA;my.subgraph&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you prefer GUI, you might consider using &lt;a href=&quot;http://www.cytoscape.org/&quot; rel=&quot;noreferrer&quot;&gt;Cytoscape&lt;/a&gt;. They have &lt;a href=&quot;http://apps.cytoscape.org/apps/with_tag/interactiondatabase&quot; rel=&quot;noreferrer&quot;&gt;multiple addons for interaction databases&lt;/a&gt;, e.g. &lt;a href=&quot;http://apps.cytoscape.org/apps/stringapp&quot; rel=&quot;noreferrer&quot;&gt;for STRINGdb&lt;/a&gt;. There you can just provide your gene list and get a network in a few clicks.&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-06-08T08:49:31.847" LastActivityDate="2017-06-08T08:49:31.847" CommentCount="1" />
  <row Id="570" PostTypeId="1" AcceptedAnswerId="574" CreationDate="2017-06-08T09:00:53.407" Score="7" ViewCount="387" Body="&lt;p&gt;I have many alignments from Rfam Database, and I would like to edit them. &#xA;I saw that many tools are used for Protein sequence alignments, but there is something specific to edit RNA alignments ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;e.g. Stockholm Alignment of Pistol (just few entries). &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;FP929053.1/1669026-1668956   AGUGGUCACAGCCACUAUAAACA-GGGCUU-UAAGCUGUG-AGCGUUGACCGUC----------ACAA-----CGGCGGUCAGGUAGUC&#xA;AFOX01000025.1/1981-1912     ACUCGUCUGAGCGAGUAUAAACA-GGUCAU-UAAGCUCAG-AGCGUUCACCGGG----------AUCA------UUCGGUGAGGUUGGC&#xA;HE577054.1/3246821-3246752   ACUCGUCUGAGCGAGUAUAAACA-GGUCAU-UAAGCUCAG-AGCGUUCACCGGG----------AUCA------UGCGGUGAGGUUGGC&#xA;CP000154.1/3364237-3364168   GUUCGUCUGAGCGAACGCAAACA-GGCCAU-UAAGCUCAG-AGCGUUCACUGGA----------UUCG------UCCAGUGAGAUUGGC`&#xA;`#=GC SS_cons                 &amp;lt;&amp;lt;&amp;lt;&amp;lt;__AAAAA_&amp;gt;&amp;gt;&amp;gt;&amp;gt;-------..&amp;lt;&amp;lt;&amp;lt;&amp;lt;-.----aaaaa.----&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;..........____....._&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;--&amp;gt;&amp;gt;&amp;gt;&amp;gt;`&#xA;`#=GC RF                      acUCGUCuggGCGAguAUAAAuA..cgCaU.UAgGCccaG.AGCGUcccggcgg..........uUau.....uccgccgggGGUuGcg&#xA;//&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="682" LastEditorUserId="640" LastEditDate="2017-06-08T20:08:04.130" LastActivityDate="2017-06-08T20:08:04.130" Title="What to use to edit RNA alignments?" Tags="&lt;rna&gt;&lt;rna-alignment&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="0" />
  <row Id="571" PostTypeId="2" ParentId="568" CreationDate="2017-06-08T09:08:04.750" Score="6" Body="&lt;p&gt;Here's a Perl script that can do this:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-perl prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/env perl &#xA;use strict;&#xA;use warnings;&#xA;&#xA;## Change this to whatever taxon you are working with&#xA;my $taxon = 'taxon:1000';&#xA;chomp(my $date = `date +%Y%M%d`);&#xA;&#xA;my (%aspect, %gos);&#xA;## Read the GO.terms_and_ids file to get the aspect (sub ontology)&#xA;## of each GO term. &#xA;open(my $fh, $ARGV[0]) or die &quot;Need a GO.terms_and_ids file as 1st arg: $!\n&quot;;&#xA;while (&amp;lt;$fh&amp;gt;) {&#xA;    next if /^!/;&#xA;    chomp;&#xA;    my @fields = split(/\t/);&#xA;    ## $aspect{GO:0000001} = 'P'&#xA;    $aspect{$fields[0]} = $fields[2];&#xA;}&#xA;close($fh);&#xA;&#xA;## Read the list of gene annotations&#xA;open($fh, $ARGV[1]) or die &quot;Need a list of gene annotattions as 2nd arg: $!\n&quot;;&#xA;while (&amp;lt;$fh&amp;gt;) {&#xA;    chomp;&#xA;    my ($gene, @terms) = split(/[\s,]+/);&#xA;    ## $gos{geneA} = (go1, go2 ... goN)&#xA;    $gos{$gene} = [ @terms ];&#xA;}&#xA;close($fh);&#xA;&#xA;foreach my $gene (keys(%gos)) {&#xA;    foreach my $term (@{$gos{$gene}}) {&#xA;        ## Warn and skip if there is no aspect for this term&#xA;        if (!$aspect{$term}) {&#xA;            print STDERR &quot;Unknown GO term ($term) for gene $gene\n&quot;;&#xA;            next;&#xA;        }&#xA;        ## Build a pseudo GAF line &#xA;        my @out = ('DB', $gene, $gene, ' ', $term, 'PMID:foo', 'TAS', ' ', $aspect{$term},&#xA;                             $gene, ' ', 'protein', $taxon, $date, 'DB', ' ', ' ');&#xA;        print join(&quot;\t&quot;, @out). &quot;\n&quot;;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Make it executable and run it with the &lt;code&gt;GO.terms_and_ids&lt;/code&gt; file as the 1st argument and the list of gene annotations as the second. Using the current &lt;code&gt;GO.terms_and_ids&lt;/code&gt; and the example annotations in the question, I get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ foo.pl GO.terms_and_ids file.gos &#xA;DB  geneD   geneD       GO:0005634  PMID:foo    TAS     C   geneD       protein taxon:1000  20170308    DB       &#xA;DB  geneD   geneD       GO:0003677  PMID:foo    TAS     F   geneD       protein taxon:1000  20170308    DB       &#xA;DB  geneD   geneD       GO:0030154  PMID:foo    TAS     P   geneD       protein taxon:1000  20170308    DB       &#xA;Unknown GO term (GO:0006350) for gene geneD&#xA;DB  geneD   geneD       GO:0006355  PMID:foo    TAS     P   geneD       protein taxon:1000  20170308    DB       &#xA;DB  geneD   geneD       GO:0007275  PMID:foo    TAS     P   geneD       protein taxon:1000  20170308    DB       &#xA;DB  geneD   geneD       GO:0030528  PMID:foo    TAS     F   geneD       protein taxon:1000  20170308    DB       &#xA;DB  geneB   geneB       GO:0016020  PMID:foo    TAS     C   geneB       protein taxon:1000  20170308    DB       &#xA;DB  geneB   geneB       GO:0005524  PMID:foo    TAS     F   geneB       protein taxon:1000  20170308    DB       &#xA;DB  geneB   geneB       GO:0006468  PMID:foo    TAS     P   geneB       protein taxon:1000  20170308    DB       &#xA;DB  geneB   geneB       GO:0005737  PMID:foo    TAS     C   geneB       protein taxon:1000  20170308    DB       &#xA;DB  geneB   geneB       GO:0004674  PMID:foo    TAS     F   geneB       protein taxon:1000  20170308    DB       &#xA;DB  geneB   geneB       GO:0006914  PMID:foo    TAS     P   geneB       protein taxon:1000  20170308    DB       &#xA;DB  geneB   geneB       GO:0016021  PMID:foo    TAS     C   geneB       protein taxon:1000  20170308    DB       &#xA;DB  geneB   geneB       GO:0015031  PMID:foo    TAS     P   geneB       protein taxon:1000  20170308    DB       &#xA;DB  geneA   geneA       GO:0006950  PMID:foo    TAS     P   geneA       protein taxon:1000  20170308    DB       &#xA;DB  geneA   geneA       GO:0005737  PMID:foo    TAS     C   geneA       protein taxon:1000  20170308    DB       &#xA;DB  geneC   geneC       GO:0003779  PMID:foo    TAS     F   geneC       protein taxon:1000  20170308    DB       &#xA;DB  geneC   geneC       GO:0006941  PMID:foo    TAS     P   geneC       protein taxon:1000  20170308    DB       &#xA;DB  geneC   geneC       GO:0005524  PMID:foo    TAS     F   geneC       protein taxon:1000  20170308    DB       &#xA;DB  geneC   geneC       GO:0003774  PMID:foo    TAS     F   geneC       protein taxon:1000  20170308    DB       &#xA;DB  geneC   geneC       GO:0005516  PMID:foo    TAS     F   geneC       protein taxon:1000  20170308    DB       &#xA;DB  geneC   geneC       GO:0005737  PMID:foo    TAS     C   geneC       protein taxon:1000  20170308    DB       &#xA;DB  geneC   geneC       GO:0005863  PMID:foo    TAS     C   geneC       protein taxon:1000  20170308    DB       &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note that this is very much a pseudo-GAF file since most of the fields apart from the gene name, GO term and sub-ontology are fake. It should still work for what you need, however.&lt;/p&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-06-08T20:43:01.263" LastActivityDate="2017-06-08T20:43:01.263" CommentCount="3" />
  <row Id="572" PostTypeId="2" ParentId="527" CreationDate="2017-06-08T09:18:07.773" Score="2" Body="&lt;p&gt;For miRNA precursor predictions in species in Ensembl but not in miRBase, you can have a go to MapMi (&lt;a href=&quot;http://www.ebi.ac.uk/enright-srv/MapMi/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.ebi.ac.uk/enright-srv/MapMi/&lt;/a&gt;) . &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can easily run it locally for other species or if you have smallRNA sequencing done on something new. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let me know if you run into difficulties or have ideas to improve it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ensembl itself has its own miRNA predictions. &lt;/p&gt;&#xA;" OwnerUserId="693" LastEditorUserId="77" LastEditDate="2017-06-08T10:23:38.590" LastActivityDate="2017-06-08T10:23:38.590" CommentCount="0" />
  <row Id="573" PostTypeId="2" ParentId="570" CreationDate="2017-06-08T09:22:17.537" Score="5" Body="&lt;p&gt;There's nothing really special about RNA alignments, you can use any alignment editor, including whichever one you use for protein. That said, a classic and very useful tool for this sort of thing is &lt;a href=&quot;http://www.jalview.org/&quot; rel=&quot;noreferrer&quot;&gt;JalView&lt;/a&gt;. It can be installed locally or run as a Java webapp from your browser.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Jalview has built in DNA, RNA and protein sequence and structure visualisation and analysis capabilities. It uses Jmol to view 3D structures, and VARNA to display RNA secondary structure. The Jalview Desktop can also connect with databases and analysis services, and provides a graphical interface to the alignment and analysis services provided by the JavA Bioinformatics Analysis Web Services framework.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/I76Yq.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/I76Yq.png&quot; alt=&quot;jalview image&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-06-08T09:22:17.537" CommentCount="0" />
  <row Id="574" PostTypeId="2" ParentId="570" CreationDate="2017-06-08T09:23:18.300" Score="8" Body="&lt;p&gt;I would suggest use RALEE—RNALignment Editor in Emacs. It can get for you the consensus secondary structure, you can move left/right sequences and their secondary structures (you can't do it in JalView!), and more.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's an Emacs mode, so could be a bit hard to start off, but just try, you don't have to use all Emacs features to edit your alignments! &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The RALEE (RNA ALignment Editor in Emacs) tool provides a simple&#xA;  environment for RNA multiple sequence alignment editing, including&#xA;  structure-specific colour schemes, utilizing helper applications for&#xA;  structure prediction and many more conventional editing functions.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Sam Griffiths-Jones Bioinformatics (2005) 21 (2): 257-259. DOI: &lt;a href=&quot;https://doi.org/10.1093/bioinformatics/bth489&quot; rel=&quot;noreferrer&quot;&gt;https://doi.org/10.1093/bioinformatics/bth489&lt;/a&gt;&lt;a href=&quot;https://i.stack.imgur.com/XOWVV.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/XOWVV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/UPcwI.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/UPcwI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;Fig. You can move left/right sequences and their secondary structures (you can't do it in JalView!)&lt;/p&gt;&#xA;" OwnerUserId="640" LastEditorUserId="640" LastEditDate="2017-06-08T09:29:44.137" LastActivityDate="2017-06-08T09:29:44.137" CommentCount="2" />
  <row Id="575" PostTypeId="5" CreationDate="2017-06-08T09:35:17.007" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-08T09:35:17.007" LastActivityDate="2017-06-08T09:35:17.007" CommentCount="0" />
  <row Id="576" PostTypeId="4" CreationDate="2017-06-08T09:35:17.007" Score="0" Body="When dealing with associating sequencing reads to a matching position in a set of reference sequences (typically a genome)." OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-06-23T15:05:12.343" LastActivityDate="2017-06-23T15:05:12.343" CommentCount="0" />
  <row Id="577" PostTypeId="2" ParentId="567" CreationDate="2017-06-08T10:31:33.980" Score="5" Body="&lt;p&gt;Many interaction databases now work with &lt;a href=&quot;http://psidev.sourceforge.net/molecular_interactions/xml/doc/user/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PSI format&lt;/a&gt; files. Most of the main databases can do this and the EBI has set up &lt;a href=&quot;http://www.ebi.ac.uk/Tools/webservices/psicquic/view/main.xhtml&quot; rel=&quot;nofollow noreferrer&quot;&gt;PSICQUIC View&lt;/a&gt;, a very useful page where you can query multiple databases at once. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that it is very important to limit the results according to the detection method. There is a lot of noise in protein interaction databases. Depending on what you want to do you could limit to only experimentally verified interactions or to only direct, binary interactions &#xA;(so exclude the results of, for example, ChIP analyses which can also find complexes) etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, here's a simple example script that will query the APID database using its PSICQUIC service:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-perl prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/perl&#xA;use strict;&#xA;use warnings;&#xA;use LWP::Simple;&#xA;&#xA;my @proteins;&#xA;## Read a list of target proteins, one per line (this expects UniProt names)&#xA;open(my $fh, &quot;$ARGV[0]&quot;) or die &quot;Need a list of proteins as the 1st argument: $!\n&quot;;&#xA;while (&amp;lt;$fh&amp;gt;) {&#xA;    chomp;&#xA;    push @proteins, $_;&#xA;}&#xA;close($fh);&#xA;## Get human interactions only&#xA;my $species=&quot;9606&quot;;&#xA;&#xA;## Get the interactions for each target protein&#xA;foreach my $protein (@proteins) {&#xA;    my $queryUrl= &quot;http://cicblade.dep.usal.es/psicquic-ws/webservices/current/search/query/$protein&quot;;&#xA;    $queryUrl  .= &quot;?taxidA:$species%20ANDtaxidB:$species&quot;;&#xA;&#xA;    my $tries=1;&#xA;    my $content = get $queryUrl;&#xA;    while ($tries&amp;lt;=10) {&#xA;        if (defined($content)) {&#xA;            $tries=11;&#xA;        } else {&#xA;            print STDERR &quot;Could not retrieve $queryUrl, retrying($tries)...\n&quot;;&#xA;            $content = get $queryUrl;&#xA;        }&#xA;        $tries++;&#xA;    }&#xA;&#xA;    # Now list all interactions&#xA;    my @lines = split(/\n/, $content);&#xA;    my $LINES= @lines;&#xA;    my $count = 0;&#xA;    for my $line (@lines) {&#xA;        $count++;&#xA;        my @flds = split(/\t/, $line); # split tab delimited lines      &#xA;        # split fields of a PSIMITAB 2.5 line&#xA;        my ($idA, $idB, $altIdA, $altIdB, $aliasA, $aliasB, $detMethod, $author, $pub, $orgA, $orgB, $intType, $sourceDb, $intID, $conf) = @flds;&#xA;        ## Here you can add logic to limit the interactions by specific detection method codes ($detMethod)&#xA;        ## or database of origin etc. &#xA;&#xA;        ## Print&#xA;        print &quot;$line\n&quot;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tested it by using a file with the UniProt ID for human TP53 (P04637) and it returns a list of 3988 interactions (showing 10 randomly selected results below):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ foo.pl names.txt | shuf -n 10&#xA;uniprotkb:Q9BWC9    uniprotkb:P04637    -   -   uniprotkb:CCDC106(gene_name)    uniprotkb:TP53(gene_name)   psi-mi:&quot;MI:0007&quot;(anti tag coimmunoprecipitation)    Zhou, J. et al.(2010)   pubmed:20159018 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0469&quot;(intact)    intact:EBI-7812926  -&#xA;uniprotkb:P04637    uniprotkb:O15126    -   -   uniprotkb:TP53(gene_name)   uniprotkb:SCAMP1(gene_name) psi-mi:&quot;MI:0018&quot;(two hybrid)    Lim, J. et al.(2006)    pubmed:16713569 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0463&quot;(biogrid)   biogrid:720622  -&#xA;uniprotkb:P63165    uniprotkb:P04637    -   -   uniprotkb:SUMO1(gene_name)  uniprotkb:TP53(gene_name)   psi-mi:&quot;MI:0018&quot;(two hybrid)    Minty, A. et al.(2000)  pubmed:10961991 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0463&quot;(biogrid)   biogrid:262339  -&#xA;uniprotkb:P04637    uniprotkb:P31350    -   -   uniprotkb:TP53(gene_name)   uniprotkb:RRM2(gene_name)   psi-mi:&quot;MI:0416&quot;(fluorescence microscopy)   Xue, L. et al.(2003)    pubmed:12615712 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0465&quot;(dip)   dip:DIP-40167E  -&#xA;uniprotkb:P04637    uniprotkb:Q00987    -   -   uniprotkb:TP53(gene_name)   uniprotkb:MDM2(gene_name)   psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)    Dai, MS. et al.(2004)   pubmed:15308643 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0463&quot;(biogrid)   biogrid:478073  -&#xA;uniprotkb:Q99576    uniprotkb:P04637    -   -   uniprotkb:TSC22D3(gene_name)    uniprotkb:TP53(gene_name)   psi-mi:&quot;MI:0428&quot;(imaging technique) Ayroldi, E. et al.(2015)    pubmed:25168242 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0463&quot;(biogrid)   biogrid:1255896 -&#xA;uniprotkb:P04637    uniprotkb:Q00987    -   -   uniprotkb:TP53(gene_name)   uniprotkb:MDM2(gene_name)   psi-mi:&quot;MI:0415&quot;(enzymatic study)   Lui, K. et al.(2013)    pubmed:23572512 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0463&quot;(biogrid)   biogrid:859223  -&#xA;uniprotkb:P25685    uniprotkb:P04637    -   -   uniprotkb:DNAJB1(gene_name) uniprotkb:TP53(gene_name)   psi-mi:&quot;MI:0004&quot;(affinity chromatography technology)    Qi, M. et al.(2014) pubmed:24361594 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0463&quot;(biogrid)   biogrid:938952  -&#xA;uniprotkb:P04637    uniprotkb:Q8IW41    -   -   uniprotkb:TP53(gene_name)   uniprotkb:MAPKAPK5(gene_name)   psi-mi:&quot;MI:0424&quot;(protein kinase assay)  Sun, P. et al.(2007)    pubmed:17254968 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0469&quot;(intact)    intact:EBI-1202077  -&#xA;uniprotkb:Q13526    uniprotkb:P04637    -   -   uniprotkb:PIN1(gene_name)   uniprotkb:TP53(gene_name)   psi-mi:&quot;MI:0096&quot;(pull down) Mantovani, F. et al.(2007)  pubmed:17906639 taxid:9606(Homo sapiens)    taxid:9606(Homo sapiens)    -   psi-mi:&quot;MI:0469&quot;(intact)    intact:EBI-6112688  -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-06-09T00:07:33.717" LastActivityDate="2017-06-09T00:07:33.717" CommentCount="2" />
  <row Id="578" PostTypeId="1" AcceptedAnswerId="582" CreationDate="2017-06-08T10:38:42.810" Score="7" ViewCount="1109" Body="&lt;p&gt;I’m trying to find common names from a list of scientific names (not all will have them though).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was attempting to use &lt;code&gt;taxize&lt;/code&gt; in R but it aborts if it doesn’t find an entry in EOL and I don’t know a way around this other than manually editing the list - in which case I might as well check all this way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have a better way of doing this?&lt;/p&gt;&#xA;" OwnerUserId="691" LastEditorUserId="191" LastEditDate="2017-06-08T11:33:58.550" LastActivityDate="2017-06-08T21:44:35.360" Title="How to convert species names into common names?" Tags="&lt;r&gt;&lt;database&gt;&lt;taxonomy&gt;" AnswerCount="3" CommentCount="3" />
  <row Id="579" PostTypeId="2" ParentId="578" CreationDate="2017-06-08T10:44:47.550" Score="9" Body="&lt;p&gt;use the NCBI taxon dump under &lt;a href=&quot;ftp://ftp.ncbi.nih.gov/pub/taxonomy&quot; rel=&quot;noreferrer&quot;&gt;ftp://ftp.ncbi.nih.gov/pub/taxonomy&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;in taxdmp.zip you'll find all the names for a given NCBI taxon&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ grep -w ^9606 names.dmp &#xA;9606    |   Homo sapiens    |       |   scientific name |&#xA;9606    |   Homo sapiens Linnaeus, 1758 |       |   authority   |&#xA;9606    |   human   |       |   genbank common name |&#xA;9606    |   man |       |   common name |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="71" LastActivityDate="2017-06-08T10:44:47.550" CommentCount="0" />
  <row Id="580" PostTypeId="2" ParentId="527" CreationDate="2017-06-08T10:51:40.737" Score="2" Body="&lt;p&gt;miRbase is a database of miRNA sequences, their genomic locations and the evidence for their existence. It is not a database of their targets, so I am assuming that this is the information you are asking for. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Despite not having been updated since 2014, as far as I am aware miRBase is still the go to place for this data. For example, &lt;a href=&quot;http://blog.rnacentral.org/2015/04/new-training-course-online-resources.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;courses at the European Bioinformatics Institute&lt;/a&gt; were still teaching the use of miRBase in April this year (2017). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://rnacentral.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RNACentral&lt;/a&gt; is repository for all non-coding RNAs, but as it is a synthesis of other databases, including miRBase, I suspect data their will still be largely the same as miRBase. &lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-06-08T10:51:40.737" CommentCount="0" />
  <row Id="581" PostTypeId="2" ParentId="578" CreationDate="2017-06-08T10:57:53.833" Score="1" Body="&lt;p&gt;As Pierre &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/579/298&quot;&gt;suggested&lt;/a&gt;, you can get a dump of such names from &lt;a href=&quot;ftp://ftp.ncbi.nih.gov/pub/taxonomy&quot; rel=&quot;nofollow noreferrer&quot;&gt;NCBI&lt;/a&gt;. Then, to query it for the common name for a species using its Latin name, you can do:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-c prettyprint-override&quot;&gt;&lt;code&gt;$ awk -F'\t' -vname=&quot;Mus musculus&quot; '($7==&quot;scientific name&quot; &amp;amp;&amp;amp; $3==name){&#xA;                                      a[$1]=$3&#xA;                                    } &#xA;                                    ($1 in a &amp;amp;&amp;amp; /genbank common name/){  &#xA;                                      print $3&#xA;                                    }' names.dmp&#xA;house mouse&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-06-08T21:12:29.263" LastActivityDate="2017-06-08T21:12:29.263" CommentCount="4" />
  <row Id="582" PostTypeId="2" ParentId="578" CreationDate="2017-06-08T11:03:51.673" Score="8" Body="&lt;p&gt;As &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/579/191&quot;&gt;Pierre mentioned&lt;/a&gt;, NCBI is a good resource for this kind of transformation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can still use &lt;code&gt;taxize&lt;/code&gt; to perform the conversion:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;library(&quot;taxize&quot;)&#xA;&#xA;species &amp;lt;- c('Helianthus annuus', 'Mycobacterium bovis', 'Rattus rattus', 'XX', 'Mus musculus')&#xA;&#xA;uids &amp;lt;- get_uid(species)&#xA;&#xA;# keep only uids which you have in the database&#xA;uids.found &amp;lt;- as.uid(uids[!is.na(uids)])&#xA;# keep only species names  corresponding to your ids&#xA;species.found &amp;lt;- species[!is.na(uids)]&#xA;&#xA;common.names &amp;lt;- sci2comm(uids.found, db = 'ncbi')&#xA;names(common.names) &amp;lt;- species.found&#xA;&#xA;common.names&#xA;&#xA;## output:&#xA;## $`Helianthus annuus`&#xA;## [1] &quot;common sunflower&quot;&#xA;## &#xA;## $`Mycobacterium bovis`&#xA;## character(0)&#xA;## &#xA;## $`Rattus rattus`&#xA;## [1] &quot;black rat&quot;&#xA;## &#xA;## $`Mus musculus`&#xA;## [1] &quot;house mouse&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This works correctly if no common name is available, or if there is no species in the database. You can also try a different database, e.g. &lt;code&gt;db='itis'&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The advantage of this approach is that you do not have to parse a file yourself. The downside is that it can be slow for large list, since for every species a database request is performed.&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="298" LastEditDate="2017-06-08T21:44:35.360" LastActivityDate="2017-06-08T21:44:35.360" CommentCount="1" />
  <row Id="583" PostTypeId="1" AcceptedAnswerId="602" CreationDate="2017-06-08T12:07:20.897" Score="2" ViewCount="59" Body="&lt;p&gt;I calculated an AMOVA from a genind object, with one hierarchical factor. In the table I obtain there are SSD values (for my grouping factor,&quot;Error&quot; and total) and sigma2 values (for my grouping factor and &quot;Error&quot;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two questions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What does &quot;error&quot; stand for?&lt;/li&gt;&#xA;&lt;li&gt;How do I calculate Fst? Which values do I have to use?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;A genind object in R is an object which contains allelic information about a set of individuals.&lt;br&gt;&#xA;This is the table that I get: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Analysis of Molecular Variance&#xA;&#xA;Call: amova(formula = plantula.dist ~ g, nperm = 100)  &#xA;&#xA;           SSD         MSD  df  &#xA;g     354992.9 354992.9297   1  &#xA;Error 310814.9    693.7834 448 &#xA;Total 665807.9   1482.8683 449 &#xA;&#xA;Variance components:  &#xA;       sigma2 P.value  &#xA;g     1574.69       0  &#xA;Error  693.78         &#xA;&#xA;Variance coefficients:  &#xA;       a   &#xA;224.9956   &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="752" LastEditorUserId="775" LastEditDate="2017-06-08T21:33:04.827" LastActivityDate="2017-06-09T07:07:13.327" Title="How to calculate Fst from AMOVA" Tags="&lt;r&gt;&lt;amova&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="584" PostTypeId="1" AcceptedAnswerId="585" CreationDate="2017-06-08T12:07:22.530" Score="2" ViewCount="72" Body="&lt;p&gt;We are studying six different human macrophage/dendritic cell types isolated from healthy skin. They all differ from each other in a few cell surface markers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are interested in the characteristics of each cell type (&quot;marker&quot; genes or biological processes), the differences between them (especially cell surface proteins), and their relations (e.g. &quot;which are the closest relatives?&quot;) as can be inferred from the transcriptome, from an immunological point of view. The wider context is HIV-infection, thus HIV infection-related differences (and similarities) are of particular interest.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One naive approach is to contrast two of the most similar cell types (as inferred from previous knowledge, the sorting strategy and the PCA plot), and produce a list of differentially expressed (DE) genes.&#xA;I have now a list of ~500 DE genes between two cell types, together with fold changes, expression means etc., produced with DESeq2 on bulk RNA-seq data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the best practices to process this list to answer some of the above?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've found PPI analysis on &lt;a href=&quot;http://networkanalyst.ca&quot; rel=&quot;nofollow noreferrer&quot;&gt;NetworkAnalyst&lt;/a&gt;, and  GO and pathway analysis on &lt;a href=&quot;http://innatedb.com&quot; rel=&quot;nofollow noreferrer&quot; title=&quot;InnateDB&quot;&gt;InnateDB&lt;/a&gt; useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What other options are available?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm particularly interested in approaches using &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="208" LastEditorUserId="208" LastEditDate="2017-06-08T12:53:17.433" LastActivityDate="2017-06-08T14:37:56.240" Title="What are the ways to process a list of differentially expressed genes?" Tags="&lt;rna-seq&gt;&lt;r&gt;&lt;deseq2&gt;&lt;networks&gt;" AnswerCount="1" CommentCount="5" FavoriteCount="1" />
  <row Id="585" PostTypeId="2" ParentId="584" CreationDate="2017-06-08T12:38:57.067" Score="2" Body="&lt;p&gt;You originally had asked a very broad question, so I'll try to demonstrate why that is such a hard question to answer. I've done two fairly large differential analysis studies (and a few smaller ones) covering very different areas of research, and the approaches that other researchers used subsequent to my differential expression calculations were unsurprisingly also very different.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S221112471401047X&quot; rel=&quot;nofollow noreferrer&quot;&gt;first study&lt;/a&gt;, I did a &lt;em&gt;de-novo&lt;/em&gt; transcriptome assembly of &lt;em&gt;Schmidtea mediterranea (Smed)&lt;/em&gt; from Illumina RNASeq reads, then carried out a DE analysis (DESeq) on a &lt;em&gt;β-catenin&lt;/em&gt; knockout mutant population compared to wildtype. The other researchers had some very impressive tools available at their disposal, allowing them to subsequently visualise the positional expression of differentially-expressed genes in the knockout and wildtype populations after amputation. They found a few candidate genes that had graded expression along the anterior-posterior axis in the wildtype population, and this expression disappeared in the knockout mutants. One of these genes was a &lt;em&gt;teashirt (tsh)&lt;/em&gt; family member, and a good candidate for further discovery based on the researchers' existing knowledge of the gene pathways associated with cellular regeneration. To cut a very long story short, they experimentally validated the &lt;em&gt;teashirt&lt;/em&gt; link through further RNAi studies, generating a double-headed &lt;em&gt;Smed&lt;/em&gt; as a result. This was followed up in Zebrafish &lt;em&gt;(Danio rerio)&lt;/em&gt;, because there happened to be another building about 100m away that specialised in Zebrafish development.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;https://www.growkudos.com/publications/10.1084%252Fjem.20160470&quot; rel=&quot;nofollow noreferrer&quot;&gt;second study&lt;/a&gt; was/is a bit more similar to your current situation. We looked at sorted dendritic cell expression in mice after the application of two Th2-inducing treatment conditions (injected &lt;em&gt;Nippostrongylus brasiliensis&lt;/em&gt;; applied DBP+FITC) and one control condition (injected PBS). I did a differential expression analysis with &lt;a href=&quot;http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DESeq2&lt;/a&gt;, and developed a &lt;a href=&quot;http://shiny.rstudio.com/gallery/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Shiny App&lt;/a&gt; for result exploration after I got tired of getting asked to do a whole bunch of mechanical follow-up tasks (mostly generating graphs, lists, and heatmaps for different gene subsets). We ended up with thousands of differentially expressed genes, so it wasn't feasible to look individually at every gene for research evidence and pathways -- one scientist spent a couple of weeks looking at one interesting-looking candidate gene. The differentially-expressed genes were fed into the oh-so-expensive &lt;a href=&quot;https://www.qiagenbioinformatics.com/products/ingenuity-pathway-analysis/&quot; rel=&quot;nofollow noreferrer&quot;&gt;IPA&lt;/a&gt; (which gives results, but not necessarily good ones) and &lt;a href=&quot;https://david.ncifcrf.gov/&quot; rel=&quot;nofollow noreferrer&quot;&gt;DAVID&lt;/a&gt;, among a few others. These helped a little bit, and led to what was the central idea of our paper (i.e. that there were at least two different Th2 responses), but we still had the problem of too many &quot;significant&quot; pathways, and lots of differentially-expressed genes that were members of a whole bunch of other pathways. We tried &lt;a href=&quot;https://labs.genetics.ucla.edu/horvath/htdocs/CoexpressionNetwork/Rpackages/WGCNA/&quot; rel=&quot;nofollow noreferrer&quot;&gt;WGCNA&lt;/a&gt;; we did a bit of &lt;a href=&quot;http://software.broadinstitute.org/gsea/index.jsp&quot; rel=&quot;nofollow noreferrer&quot;&gt;GSEA&lt;/a&gt;; and we compared &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/limma.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Limma&lt;/a&gt; and DESeq2, but still couldn't find anything concrete or obvious to demonstrate what was triggering the Th2 response. There were experimental studies as well: blocking and mutant studies to investigate the two types of Th2 response (and experimentally confirm the differential expression). Our experiments and exploration are still ongoing, but we felt it was necessary to get one paper out at the intermediate stage, so just ended up taking the top 25 differentially-expressed genes in each experiment (by fold change), and finding some way of grouping some of them together (via expert knowledge and literature searches). It kind-of-sort-of worked, but we're left with a bit of frustration in that we (and the tools we used) were basically stumped by having too many biologically-relevant results.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-08T14:37:56.240" LastActivityDate="2017-06-08T14:37:56.240" CommentCount="6" />
  <row Id="586" PostTypeId="2" ParentId="565" CreationDate="2017-06-08T14:32:07.633" Score="3" Body="&lt;p&gt;The problem is that GFF, fundamentally, is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Relational_model&quot; rel=&quot;nofollow noreferrer&quot;&gt;relational&lt;/a&gt; format: it provides tags that relate individual rows via one-to-many relationships (e.g. gene–exon). This indirectly highlights the second complication: individual rows have different types, and therefore store different attributes in the 9th column.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Over the last few decades (!), we have accumulated a wealth of theory and tools to work with this kind of data. And the usual solution is to create a database schema for a relational database, and to use database drivers and database query languages (e.g. SQL, but increasingly also data-relational mappers such as LINQ and dplyr).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using a text-based format is attractive for many reasons that Devon mentions, but it is fundamentally at odds with a lot of the theory and tools for relational data. This creates an &lt;a href=&quot;https://en.wikipedia.org/wiki/Object-relational_impedance_mismatch&quot; rel=&quot;nofollow noreferrer&quot;&gt;impedance mismatch&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I’m convinced that, in the long run, the solution is going to be to revert to using relational databases for complex annotations. I say “revert” even though these databases already exist, they are simply often ignored in bioinformaticians’ day-to-day work (&lt;em&gt;I&lt;/em&gt; never use them). Because this is objectively the best technical solution, and we have the research to back this up.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-06-08T14:32:07.633" CommentCount="3" />
  <row Id="587" PostTypeId="1" AcceptedAnswerId="592" CreationDate="2017-06-08T14:54:57.103" Score="9" ViewCount="81" Body="&lt;p&gt;We're considering switching our storage format from BAM to CRAM. We work with human cancer samples, which may have very low prevalence variants (i.e. not diploid frequency).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we use lossy CRAM to save more space, how much will variants called from those CRAM files change? Which compression strategy has the lowest impact? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any other impacts on downstream tools that we're not considering?&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2017-06-09T16:28:26.360" Title="Do variant calls change when you call from CRAM?" Tags="&lt;bam&gt;&lt;file-formats&gt;&lt;variant-calling&gt;&lt;storage&gt;" AnswerCount="2" CommentCount="7" FavoriteCount="1" />
  <row Id="588" PostTypeId="2" ParentId="558" CreationDate="2017-06-08T15:00:23.893" Score="1" Body="&lt;p&gt;I would expect that without any further insights into protein folding, a quantum method would look fairly similar to existing methods, but might have a bit more capability of exploring multiple states at the same time (without multitasking).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the approach used by &lt;a href=&quot;http://boinc.bakerlab.org/rosetta/rah_graphics.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;Rosetta&lt;/a&gt;, which is used in both the &lt;a href=&quot;http://boinc.bakerlab.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Rosetta@home&lt;/a&gt; distributed computing project, and the &lt;a href=&quot;http://fold.it/portal/&quot; rel=&quot;nofollow noreferrer&quot;&gt;foldit&lt;/a&gt; distributed gaming project:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Rosetta's strategy for finding low energy shapes looks like this:&lt;/p&gt;&#xA;  &#xA;  &lt;ol&gt;&#xA;  &lt;li&gt;Start with a fully unfolded chain (like a metal chain with its ends pulled).&lt;/li&gt;&#xA;  &lt;li&gt;Move a part of the chain to create a new shape.&lt;/li&gt;&#xA;  &lt;li&gt;Calculate the energy of the new shape.&lt;/li&gt;&#xA;  &lt;li&gt;Accept or reject the move depending on the change in energy.&lt;/li&gt;&#xA;  &lt;li&gt;Repeat 2 through 4 until every part of the chain has been moved a lot of times. &lt;/li&gt;&#xA;  &lt;/ol&gt;&#xA;  &#xA;  &lt;p&gt;We call this a trajectory. The end result of a trajectory is a predicted structure.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-08T15:00:23.893" CommentCount="2" />
  <row Id="589" PostTypeId="2" ParentId="194" CreationDate="2017-06-08T15:27:39.173" Score="1" Body="&lt;p&gt;I would be concerned about the input data that produced these numbers. This dataset has a really odd shape with a dip at the start:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;data.df &amp;lt;- read.table(&quot;soft_threshold.txt&quot;, header=TRUE);&#xA;png(&quot;scale-free-194.png&quot;);&#xA;plot(data.df$SFT.R.sq);&#xA;dummy &amp;lt;- dev.off();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/F1MT9.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/F1MT9.png&quot; alt=&quot;scale-free power with an odd inflection&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's what I expect this to look like (from the supplementary information in &lt;a href=&quot;http://www.pnas.org/content/103/47/17973&quot; rel=&quot;nofollow noreferrer&quot;&gt;this&lt;/a&gt; paper):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.pnas.org/content/suppl/2006/11/03/0605938103.DC1/Image439.gif&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://www.pnas.org/content/suppl/2006/11/03/0605938103.DC1/Image439.gif&quot; alt=&quot;normal scale-free topology&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's easier to select the power/threshold when the plot looks like the second situation. With the data as presented, it doesn't look like any value would be suitable. but if I were forced to pick, then I'd choose 12 because it is the first number above the magic threshold (ignoring the weirdness at the start). However, I'll admit that I don't understand the concepts behind the power, and am just following the recommended instructions, so can't explain why it is what it is.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-08T15:27:39.173" CommentCount="1" />
  <row Id="590" PostTypeId="1" CreationDate="2017-06-08T15:46:03.133" Score="0" ViewCount="54" Body="&lt;p&gt;I want to calculate thermodynamic paremeter enthalpy change of cellulase, but I don't know what software can do this work. Please tell me, thank you very much !&lt;/p&gt;&#xA;" OwnerUserId="762" LastEditorUserId="96" LastEditDate="2017-06-08T20:07:15.413" LastActivityDate="2017-06-16T15:33:36.017" Title="Is there a software or web server to calculate thermodynamic parameter enthalpy change of cellulase?" Tags="&lt;protein-structure&gt;&lt;thermodynamics&gt;&lt;biophysics&gt;" AnswerCount="1" CommentCount="8" />
  <row Id="591" PostTypeId="2" ParentId="565" CreationDate="2017-06-08T15:49:51.480" Score="2" Body="&lt;p&gt;I quite like BED and GFF3 (I don't like GTF/GFF2, though). As text-based formats, I don't think they leave us much room for improvement. Anyway, if you want a new format, here is one. The following is a hybrid between GFF3 and BED. It is a TAB-delimited text-based format with the following fields:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;chr, required&lt;/li&gt;&#xA;&lt;li&gt;start (0-based), required&lt;/li&gt;&#xA;&lt;li&gt;end, required&lt;/li&gt;&#xA;&lt;li&gt;strand&lt;/li&gt;&#xA;&lt;li&gt;ID&lt;/li&gt;&#xA;&lt;li&gt;type (see below)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Like BED, only the first 3 columns are required, the rest are optional. What is different starts with the &quot;type&quot; field as in GFF. This &quot;type&quot;, when present, defines the columns following it. For example, if type==coding, we could have cdsStart, cdsEnd, blockCount, blockSizes and blockStarts like BED; if type==exon, column 7 could be a &quot;phase&quot;, indicating the phase of the first base. This way, &quot;type&quot; makes this format highly extensible while still relatively easy to parse in comparison to using optional tags all the way. In addition, we may have semi-colon-separated &quot;key&quot;=&quot;value&quot; pairs at the end of each line as in GFF3. Example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;chr1   10000  50000  -  x1 coding   10100   40800  2  1000,2000   10000,48000&#xA;chr1   10000  11000  -  *  exon     *       foo1=bar1;foo2=bar2&#xA;chr1   48000  50000  -  *  exon     2&#xA;chr2   10000  50000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The above only gives a bare bone of the format. There are subtle questions such as: 1) the uniqueness/scope of ID; 2) whether to have parent ID as a fixed field or a type-specific field; 3) where to put display name; 4) sorting order. These are also important in practice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: I like SQL a lot and think it should get used more often in bioinformatics, but I don't think it replaces text formats completely. Formats are useful for serialization and data transfer. They require less skill to work with and less software/hardware resource to deploy. Carefully engineered binary representations of formats can be much more efficient in a lot of use cases.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-08T15:49:51.480" CommentCount="0" />
  <row Id="592" PostTypeId="2" ParentId="587" CreationDate="2017-06-08T16:20:00.480" Score="5" Body="&lt;p&gt;By default, a CRAM you create with samtools is lossless. It typically halves the input BAM in terms of file size. If you want to compress more, you can let samtools convert most read names to integers. You won't be able to tell optical duplicates from read names, but this is a minor concern. You can also drop useless tags depending on your mapper and the downstream caller in use. For cancer data, I wouldn't reduce the resolution of base quality without comprehensive benchmarks. Unfortunately, base quality takes most of space in CRAM. Discarding the original read names and some tags probably won't save you much space.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-08T16:20:00.480" CommentCount="6" />
  <row Id="593" PostTypeId="2" ParentId="558" CreationDate="2017-06-08T17:54:28.040" Score="3" Body="&lt;p&gt;Protein folding problem can be viewed as a minimization problem. One can use &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_annealing&quot; rel=&quot;nofollow noreferrer&quot;&gt;quantum annealing&lt;/a&gt; to perform the minimization. Running this on quantum computers would improve the performance since they can perform the tunneling directly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact, quantum annealing was &lt;a href=&quot;https://dx.doi.org/10.1038/srep00571&quot; rel=&quot;nofollow noreferrer&quot;&gt;used&lt;/a&gt; (&lt;a href=&quot;http://blogs.nature.com/news/2012/08/d-wave-quantum-computer-solves-protein-folding-problem.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;blog post&lt;/a&gt;) for lattice protein folding on the the D-Wave quantum computer (128 qubits). But this was an extremely simplified model, with only 40 discrete states.&lt;/p&gt;&#xA;" OwnerUserId="191" LastActivityDate="2017-06-08T17:54:28.040" CommentCount="1" />
  <row Id="594" PostTypeId="1" AcceptedAnswerId="601" CreationDate="2017-06-08T20:21:18.697" Score="5" ViewCount="124" Body="&lt;p&gt;I have around ~3,000 short sequences of approximately ~10Kb long. What are the best ways to find the motifs among all of these sequences? Is there a certain software/method recommended?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are several ways to do this. My goal would be to:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(1) Check for motifs repeated within individual sequences&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(2) Check for motifs shared among all sequences&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(3) Check for the presence of &quot;expected&quot; or known motifs&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With respect to #3, I'm also curious if I find e.g. trinucleotide sequences, how does one check the context around these regions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for the recommendations/help!&lt;/p&gt;&#xA;" OwnerUserId="146" LastEditorUserId="146" LastEditDate="2017-06-08T21:34:24.920" LastActivityDate="2017-06-13T22:20:11.820" Title="What motif finding software is available for multiple sequences ~10Kb?" Tags="&lt;motifs&gt;&lt;software-recommendation&gt;" AnswerCount="5" CommentCount="6" />
  <row Id="595" PostTypeId="1" AcceptedAnswerId="603" CreationDate="2017-06-08T21:34:33.547" Score="7" ViewCount="150" Body="&lt;p&gt;Is there a convenient way to extract the longest isoforms from a transcriptome fasta file? I had found some &lt;a href=&quot;https://www.biostars.org/p/107759/&quot; rel=&quot;nofollow noreferrer&quot;&gt;scripts on biostars&lt;/a&gt; but none are functional and I'm having difficulty getting them to work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm aware that the longest isoforms aren't necessarily 'the best' but it will suit my purposes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The fasta was generated via Augustus. Here is what the fasta file looks like currently (sequence shortened to save space)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&#xA;atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xA;ttaatataaccaaaaacctagaaattctagccttcgatgatgttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xA;&amp;gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t2&#xA;atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgattcgtatgatcaactcatca&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The format is as such:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Gene 1 isoform 1  &#xA;Gene 1 isoform 2  &#xA;Gene 2 isoform 1  &#xA;Gene 2 isoform 2   &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and so forth. There are several genes that have more than one pair of isoforms (up to 3 or 4). There are roughly 80,000 total transcripts, probably 25,000 genes. I would like to extract the single longest isoform for each gene.&lt;/p&gt;&#xA;" OwnerUserId="779" LastEditorUserId="73" LastEditDate="2017-06-09T15:15:22.383" LastActivityDate="2017-06-13T00:51:03.047" Title="How can longest isoforms (per gene) be extracted from a FASTA file?" Tags="&lt;fasta&gt;&lt;isoform&gt;&lt;filtering&gt;" AnswerCount="5" CommentCount="8" />
  <row Id="597" PostTypeId="2" ParentId="595" CreationDate="2017-06-08T22:03:44.497" Score="1" Body="&lt;p&gt;An ex-coworker (Josep Avril) has written a couple of very useful little scripts that convert fasta to tbl (&lt;code&gt;seqID&amp;lt;TAB&amp;gt;Sequence&lt;/code&gt;) and back again. These are extremely handy for this sort of thing (and are included at the end of this answer). Using them, you can convert your fasta to a one sequence per line format, keep the longest sequence with a simple awk script and convert back to fasta.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am assuming that the part after the last &lt;code&gt;.&lt;/code&gt; in the ID line should be removed (so that &lt;code&gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&lt;/code&gt; and &lt;code&gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t2&lt;/code&gt; both map to &lt;code&gt;Doug_NoIndex_L005_R1_001_contig_2.g7&lt;/code&gt;). If that is a correct assumption, this should work for you:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ FastaToTbl file.fa | sed 's/\.[^.]*\t/\t/' | &#xA;    awk -F&quot;\t&quot; '{&#xA;                    if(length($2)&amp;gt;length(a[$1])){&#xA;                        a[$1]=$0&#xA;                    }&#xA;                }&#xA;                END{&#xA;                    for(i in a){&#xA;                        print a[i]&#xA;                    }&#xA;                }' | TblToFasta&#xA;&amp;gt;Doug_NoIndex_L005_R1_001_contig_2.g7 &#xA;atggggcataacatagagactggtgaacgtgctgaaattctacttcaaagtctacctgat&#xA;tcgtatgatcaactcatcattaatataaccaaaaacctagaaattctagccttcgatgat&#xA;gttgcagctgcggttcttgaagaagaaagtcggcgcaagaacaaagaagatagaccg&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;One possible issue with that approach is that it needs to keep one sequence per ID in memory. If you have a huge fasta file and not much memory, that might be a problem. If that's the case, you can first sort the file to ensure that similar IDs always appear together and then print each line as soon as you find the next ID:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;FastaToTbl file.fa | sed 's/\.[^.]*\t/\t/' | LC_ALL=C sort -t$'\t' -k1,1 |  &#xA;    awk -F&quot;\t&quot; '{&#xA;                    if(NR&amp;gt;1 &amp;amp;&amp;amp; prev!=$1){&#xA;                        print a[$1]; &#xA;                        prev=$1&#xA;                    } &#xA;                    if(length($2)&amp;gt;length(a[$1])){&#xA;                        a[$1]=$0&#xA;                    }&#xA;                }&#xA;                END{&#xA;                    print a[$1]&#xA;                }' | TblToFasta&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;FastaToTbl&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-c prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/awk -f&#xA;{&#xA;        if (substr($1,1,1)==&quot;&amp;gt;&quot;)&#xA;        if (NR&amp;gt;1)&#xA;                    printf &quot;\n%s\t&quot;, substr($0,2,length($0)-1)&#xA;        else &#xA;            printf &quot;%s\t&quot;, substr($0,2,length($0)-1)&#xA;        else &#xA;                printf &quot;%s&quot;, $0&#xA;}END{printf &quot;\n&quot;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;TblToFasta&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-c prettyprint-override&quot;&gt;&lt;code&gt;#! /usr/bin/awk -f&#xA;{&#xA;  sequence=$NF&#xA;&#xA;  ls = length(sequence)&#xA;  is = 1&#xA;  fld  = 1&#xA;  while (fld &amp;lt; NF)&#xA;  {&#xA;     if (fld == 1){printf &quot;&amp;gt;&quot;}&#xA;     printf &quot;%s &quot; , $fld&#xA;&#xA;     if (fld == NF-1)&#xA;      {&#xA;        printf &quot;\n&quot;&#xA;      }&#xA;      fld = fld+1&#xA;  }&#xA;  while (is &amp;lt;= ls)&#xA;  {&#xA;    printf &quot;%s\n&quot;, substr(sequence,is,60)&#xA;    is=is+60&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-06-10T07:35:23.737" LastActivityDate="2017-06-10T07:35:23.737" CommentCount="0" />
  <row Id="598" PostTypeId="2" ParentId="561" CreationDate="2017-06-08T22:04:16.040" Score="2" Body="&lt;p&gt;I wrote a command-line k-mer counter called &lt;code&gt;kmer-counter&lt;/code&gt; that will output results in a form that your Python script can consume: &lt;a href=&quot;https://github.com/alexpreynolds/kmer-counter&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/alexpreynolds/kmer-counter&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can grab, build and install it like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/alexpreynolds/kmer-counter.git&#xA;$ cd kmer-counter&#xA;$ make&#xA;$ cp kmer-counter /usr/local/bin&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Once the binary is in your path, you might use it in Python like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;k = 6&#xA;fastaFile = '/path/to/some/seqs.fa'&#xA;kmerCmd = 'kmer-counter --fasta --k=%d %s' % (k, fastaFile)&#xA;try:&#xA;    output = subprocess.check_output(kmerCmd, shell=True)&#xA;    result = {}&#xA;    for line in output.splitlines():&#xA;        (header, counts) = line.strip().split('\t')&#xA;        header = header[1:]&#xA;        kmers = dict((k,int(v)) for (k,v) in [d.split(':') for d in counts.split(' ')])&#xA;        result[header] = kmers&#xA;    sys.stdout.write(&quot;%s&quot; % (str(result)))&#xA;except subprocess.CalledProcessError as error:&#xA;    sys.stderr.write(&quot;%s&quot; % (str(error)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Given example FASTA like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;foo&#xA;TTAACG&#xA;&amp;gt;bar&#xA;GTGGAAGTTCTTAGGGCATGGCAAAGAGTCAGAATTTGAC&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For k=6, you would get an iterable Python dictionary like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{'foo': {'TTAACG': 1, 'CGTTAA': 1}, 'bar': {'GTTCTT': 1, 'AGAACT': 1, 'GAGTCA': 1, 'ATGGCA': 1, 'GAACTT': 1, 'ATTCTG': 1, 'CTAAGA': 1, 'CTTCCA': 1, 'ATTTGA': 1, 'GGAAGT': 1, 'AGGGCA': 1, 'CCTAAG': 1, 'CTCTTT': 1, 'AATTTG': 1, 'TCTGAC': 1, 'TTTGCC': 1, 'CTTAGG': 1, 'TTTGAC': 1, 'GAAGTT': 1, 'CCCTAA': 1, 'AGAATT': 1, 'AGTCAG': 1, 'CTGACT': 1, 'TCTTAG': 1, 'CGTTAA': 1, 'GTGGAA': 1, 'TGCCAT': 1, 'ACTCTT': 1, 'GGGCAT': 1, 'TTAGGG': 1, 'CTTTGC': 1, 'TGGAAG': 1, 'GACTCT': 1, 'CATGCC': 1, 'GCAAAG': 1, 'AAATTC': 1, 'GTCAAA': 1, 'TGACTC': 1, 'TAGGGC': 1, 'AAGTTC': 1, 'ATGCCC': 1, 'TCAAAT': 1, 'CAAAGA': 1, 'AACTTC': 1, 'GTCAGA': 1, 'CAAATT': 1, 'TAAGAA': 1, 'CATGGC': 1, 'AAGAAC': 1, 'AAGAGT': 1, 'TCTTTG': 1, 'TTCCAC': 1, 'TGGCAA': 1, 'GGCAAA': 1, 'AGTTCT': 1, 'AGAGTC': 1, 'TCAGAA': 1, 'GAATTT': 1, 'AAAGAG': 1, 'TGCCCT': 1, 'CCATGC': 1, 'GGCATG': 1, 'TTGCCA': 1, 'CAGAAT': 1, 'AATTCT': 1, 'GCATGG': 1, 'ACTTCC': 1, 'TTCTTA': 1, 'GCCATG': 1, 'GCCCTA': 1, 'TTCTGA': 1}}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You can use standard Python calls to manipulate this dictionary object and get sums of counts per record, for sequence, etc. which seems to answer your question. Please feel free to clarify what you're looking for if this object representation is not clear.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a fully-fleshed out demonstration, see: &lt;a href=&quot;https://github.com/alexpreynolds/kmer-counter/blob/master/test/kmer-test.py&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/alexpreynolds/kmer-counter/blob/master/test/kmer-test.py&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="776" LastEditorUserId="776" LastEditDate="2017-07-27T20:08:44.917" LastActivityDate="2017-07-27T20:08:44.917" CommentCount="1" />
  <row Id="599" PostTypeId="2" ParentId="595" CreationDate="2017-06-08T22:06:35.127" Score="1" Body="&lt;p&gt;This solution is will work for your example and probably all Augustus-derived Fastas, but mileage will vary beyond that.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/env python&#xA;from __future__ import print_function&#xA;import sys&#xA;&#xA;def parse_fasta(data):&#xA;    &quot;&quot;&quot;Stolen shamelessly from http://stackoverflow.com/a/7655072/459780.&quot;&quot;&quot;&#xA;    name, seq = None, []&#xA;    for line in data:&#xA;        line = line.rstrip()&#xA;        if line.startswith('&amp;gt;'):&#xA;            if name:&#xA;                yield (name, ''.join(seq))&#xA;            name, seq = line, []&#xA;        else:&#xA;            seq.append(line)&#xA;    if name:&#xA;        yield (name, ''.join(seq))&#xA;&#xA;isoforms = dict()&#xA;for defline, sequence in parse_fasta(sys.stdin):&#xA;    geneid = '.'.join(defline[1:].split('.')[:-1])&#xA;    if geneid in isoforms:&#xA;        otherdefline, othersequence = isoforms[geneid]&#xA;        if len(sequence) &amp;gt; len(othersequence):&#xA;            isoforms[geneid] = (defline, sequence)&#xA;    else:&#xA;        isoforms[geneid] = (defline, sequence)&#xA;&#xA;for defline, sequence in isoforms.values():&#xA;    print(defline, sequence, sep='\n')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Save as &lt;code&gt;longest.py&lt;/code&gt;, and invoke on the command line like so.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;python longest.py &amp;lt; intput.fasta &amp;gt; output.fasta&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;The line &lt;code&gt;geneid = '.'.join(defline[1:].split('.')[:-1])&lt;/code&gt; is key. Let me break it down.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;defline[1:]&lt;/code&gt;: ignore the first character of the defline (the &lt;code&gt;&amp;gt;&lt;/code&gt; symbol)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;defline[1:].split('.')&lt;/code&gt;: split the string on &lt;code&gt;.&lt;/code&gt; symbol&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;defline[1:].split('.')[:-1]&lt;/code&gt;: ignore the last value after the split (the isoform name)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;'.'.join(defline[1:].split('.')[:-1])&lt;/code&gt;: join the split values again&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;This value will be the same for all isoforms for the same gene, so we use it in the script to keep track of the longest sequence associated with each gene ID.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="191" LastEditDate="2017-06-09T08:07:00.293" LastActivityDate="2017-06-09T08:07:00.293" CommentCount="4" />
  <row Id="600" PostTypeId="2" ParentId="565" CreationDate="2017-06-08T22:09:13.897" Score="0" Body="&lt;p&gt;Just throwing this out there, as all the best stuff has already been said, but why does one need to specify a strict specification for a bioinformatic data format? As you say, what typically happens is all the action ends up in the optional fields.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's a lot to be said for the libertarian values of just letting people figure it out on their own. Take optional fields in the BAM tag spec. for example. Here you can have your own tags, but they must start with an &quot;X&quot;, a &quot;Y&quot; or a &quot;Z&quot; and be followed by &quot;[A-Za-z0-9]&quot;. Why? Why can't people use their own tag names like &quot;read is unique&quot; or &quot;edit distance to genome&quot; or whatever they want. Are we not to be trusted with the power of naming things and recalling things arbitrarily? The result is one must look up the tag name's actual meaning in the publication of the tool that produced it, or ask on certain forums, etc. And this is assuming one knows the tool that produced the reads - if not then who knows wtf 'XT' stands for.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Essentially, downstream readers of optional fields already trust that 'XT' is what it thinks it is. So if we're happy to use optional fields in any capacity, why stop at fields?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Extend this logic to the columns of the data format. Let users determine column names. You may once in a blue moon come across a file with the chromosome column as &quot;chr&quot; rather than &quot;chromosome&quot;, but most of the time you'll be looking for &quot;read is quantum-radioactive&quot;, and detecting and fixing naming-errors is not going to be as much of a problem as using a dataformat that does not store what it is you want to store. Or worse, it can store it, but in a really illogical way that results in everyone who uses the dataformat to have to ask at least 3 questions here or on other forums before they understand whats really going on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So you end up with a general solution to the problem of storing tabular information in a compatible way across many different types of software, also known as SQL (or any other kind of general-purpose database such as REDIS, Neo4J, Numpy, etc). In fact, does it even matter what the datastore is, so long as it's tabular and has the &quot;chromosome&quot; and &quot;position&quot; values for each item?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;TL;DR - We will not think of tomorrow's best data format today, because the nature of tomorrow's data is not yet known. Less policing in this area would most likely result in more robust software, where nothing is taken for granted and no assumptions about the data can be made until the schema has been parsed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In truth, the only reason we think we need to specify our dataformats is because we know if we didn't, other people will do something we didn't think of - and we, i think falsely, assume that will be bad.&lt;/p&gt;&#xA;" OwnerUserId="672" LastEditorUserId="205" LastEditDate="2017-06-09T01:59:00.970" LastActivityDate="2017-06-09T01:59:00.970" CommentCount="0" />
  <row Id="601" PostTypeId="2" ParentId="594" CreationDate="2017-06-09T02:19:01.120" Score="1" Body="&lt;p&gt;For (3), &lt;a href=&quot;http://www.geneinfinity.org/sp/sp_motif.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;this page&lt;/a&gt; has a lot of links to pattern/motif finding tools. Following through the &lt;a href=&quot;http://bio.cs.washington.edu/YMF/YMFWeb/YMFInput.pl&quot; rel=&quot;nofollow noreferrer&quot;&gt;YMF&lt;/a&gt; link on that page, I came across the University of Washington &lt;a href=&quot;http://bio.cs.washington.edu/software/motif_discovery#Motif%20Discovery&quot; rel=&quot;nofollow noreferrer&quot;&gt;Motif Discovery&lt;/a&gt; section. Of these &lt;a href=&quot;http://www1.cse.wustl.edu/~jbuhler/pgt/&quot; rel=&quot;nofollow noreferrer&quot;&gt;projection&lt;/a&gt; seemed to be the only downloadable tool. I find it interesting how old all these tools are; maybe the introduction of microarrays and NGS has made them all redundant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your sub-problem (2) seems similar to the problem I'm having with &lt;em&gt;Nippostrongylus brasiliensis&lt;/em&gt; genome sequences, where I'd like to find regions of very high homology (length 500bp to 20kb or more, 95-99% similar) that are repeated throughout the genome. These sequences are killing the assembly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The main way I can find these regions is by looking at a coverage plot of long nanopore reads mapped to the assembled genome (using GraphMap or BWA). Any regions with substantially higher than median coverage are likely to be shared repeats.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've played around in the past with chopping up the reads to smaller sizes, which works better for hitting smaller repeated regions that are such a small proportion of most reads that they are never mapped to all the repeated locations. I wrote &lt;a href=&quot;https://github.com/gringer/bioinfscripts/blob/master/normalise_seqlengths.pl&quot; rel=&quot;nofollow noreferrer&quot;&gt;my own script&lt;/a&gt; a while back to chop up reads (for a different purpose), which produces a FASTA/FASTQ file where all reads are exactly the same length. For some unknown reason I took the time to document that script &quot;properly&quot; using POD, so here's a short summary:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Converts all sequences in the input FASTA file to the same length.&#xA;     Sequences shorter than the target length are dropped, and sequences longer&#xA;     than the target length are split into overlapping subsequences covering&#xA;     the entire range. This prepares the sequences for use in an&#xA;     overlap-consensus assembler requiring constant-length sequences (such as&#xA;     edena).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And here's the syntax:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ ./normalise_seqlengths.pl -h&#xA;Usage:&#xA;    ./normalise_seqlengths.pl &amp;lt;reads.fa&amp;gt; [options]&#xA;&#xA;  Options:&#xA;    -help&#xA;      Only display this help message&#xA;&#xA;    -fraglength&#xA;      Target fragment length (in base-pairs, default 2000)&#xA;&#xA;    -overlap&#xA;      Minimum overlap length (in base-pairs, default 200)&#xA;&#xA;    -short&#xA;      Keep short sequences (shorter than fraglength)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-09T04:14:39.550" LastActivityDate="2017-06-09T04:14:39.550" CommentCount="0" />
  <row Id="602" PostTypeId="2" ParentId="583" CreationDate="2017-06-09T07:07:13.327" Score="0" Body="&lt;p&gt;Error is the estimation that can't be fitted by your parameters (in this case g). &#xA;The amova function in pegas package does the following: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;This function performs a hierarchical analysis of molecular variance as described in Excoffier et al.&#xA;  (1992). This implementation accepts any number of hierarchical levels&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And as per the definition of Fixation index, you need the variance in the frequency of the allele between different subpopulations, weighted by the sizes of the subpopulations, the variance of the allelic state in the total population. Which is not provided by amova. Thus you cannot compute the fixation index from amova results. &lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-06-09T07:07:13.327" CommentCount="2" />
  <row Id="603" PostTypeId="2" ParentId="595" CreationDate="2017-06-09T07:17:47.383" Score="4" Body="&lt;p&gt;While the solution from &lt;a href=&quot;https://bioinformatics.stackexchange.com/users/96/daniel-standage&quot;&gt;https://bioinformatics.stackexchange.com/users/96/daniel-standage&lt;/a&gt; should work (after adjusting for possible python3 incompatibility), the following is a shorter and less memory demanding method that uses biopython:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/env python&#xA;from Bio import SeqIO&#xA;import sys&#xA;&#xA;lastGene = None&#xA;longest = (None, None)&#xA;for rec in SeqIO.parse(sys.argv[1], &quot;fasta&quot;):&#xA;    gene = &quot;.&quot;.join(rec.id.split(&quot;.&quot;)[:-1])&#xA;    l = len(rec)&#xA;    if lastGene is not None:&#xA;        if gene == lastGene:&#xA;            if longest[0] &amp;lt; l:&#xA;                longest = (l, rec)&#xA;        else:&#xA;            lastGene = gene&#xA;            SeqIO.write(longest[1], sys.stdout, &quot;fasta&quot;)&#xA;            longest = (l, rec)&#xA;    else:&#xA;        lastGene = gene&#xA;        longest = (l, rec)&#xA;SeqIO.write(longest[1], sys.stdout, &quot;fasta&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you saved this as &lt;code&gt;filter.py&lt;/code&gt;, then &lt;code&gt;filter.py original.fa &amp;gt; subset.fa&lt;/code&gt; would be the command to use.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="191" LastEditDate="2017-06-09T08:06:41.350" LastActivityDate="2017-06-09T08:06:41.350" CommentCount="0" />
  <row Id="604" PostTypeId="1" CreationDate="2017-06-09T09:15:26.540" Score="4" ViewCount="72" Body="&lt;p&gt;I got a customized GRCh38.79 .gtf file (modified to have no MT genes) and I need to create a reference genome out of it (for 10xGenomics CellRanger pipeline). I suspect that the .79 part is the Ensembl number, which according this &lt;a href=&quot;http://www.ensembl.org/Help/ArchiveList&quot; rel=&quot;nofollow noreferrer&quot;&gt;ensembl archive list&lt;/a&gt; is paired with the GRCh38.p2 patch.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Should I use this patch's fasta file, or it would be fine to use any of the GRCh38 patches?&lt;/p&gt;&#xA;" OwnerUserId="263" LastEditorUserId="37" LastEditDate="2017-06-09T16:09:16.243" LastActivityDate="2017-08-10T00:29:28.937" Title="Can a customized GRCh38 .gtf file be used with any of the GRCh38 released patches?" Tags="&lt;human-genome&gt;&lt;reference-genome&gt;&lt;gtf&gt;&lt;10x-genomics&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="605" PostTypeId="2" ParentId="594" CreationDate="2017-06-09T09:41:25.517" Score="0" Body="&lt;p&gt;Most tools I know of looks for enrichment of specific motifs - but that requires that you have a set of sequences which are of special interest and a background set to test against.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is that your case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update after comments 12th June 2017.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You could try the &lt;a href=&quot;http://meme-suite.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;meme suite&lt;/a&gt; more specifically &lt;a href=&quot;http://meme-suite.org/tools/meme&quot; rel=&quot;nofollow noreferrer&quot;&gt;the motif finder&lt;/a&gt; &lt;/p&gt;&#xA;" OwnerUserId="599" LastEditorUserId="57" LastEditDate="2017-06-13T16:21:27.300" LastActivityDate="2017-06-13T16:21:27.300" CommentCount="2" />
  <row Id="606" PostTypeId="2" ParentId="594" CreationDate="2017-06-09T10:20:12.173" Score="1" Body="&lt;p&gt;it is under development, but maybe &lt;a href=&quot;https://github.com/soedinglab/BaMMmotif&quot; rel=&quot;nofollow noreferrer&quot;&gt;BaMMmotif!&lt;/a&gt; is something for you? Its main selling point is that it can look for motifs enriched in a set of sequences of equal length &lt;em&gt;de novo&lt;/em&gt;. If you can't/don't want to supply a negative set it learns one from the positive sequences. There is a wealth of options to choose from if you have more information about your sequences: there are different models for &quot;zero or one&quot;, &quot;one&quot; and &quot;multiple&quot; occurrences of the motif.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can also use it to look for known motifs, if you encode them as an XXmotif PWM. If you have a file with motifs (like binding sites) you can use that as initialization as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While I have not used the software myself, the authors are very responsive on git and the installation instructions seem pretty straightforward.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Apparently the software was developed with ChIP experiments as the usual use case scenario, it might misbehave for larger sequences or take super long to run.&lt;/p&gt;&#xA;" OwnerUserId="787" LastEditorUserId="787" LastEditDate="2017-06-12T09:44:17.427" LastActivityDate="2017-06-12T09:44:17.427" CommentCount="2" />
  <row Id="607" PostTypeId="2" ParentId="595" CreationDate="2017-06-09T10:51:42.197" Score="1" Body="&lt;p&gt;Here is a solution in R. Could get really slow with big files. Works for the example you posted.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(Biostrings)&#xA;&#xA;## read your fasta in as Biostrings object&#xA;fasta.s &amp;lt;- readDNAStringSet(&quot;sample.fa&quot;)&#xA;&#xA;## get the read names (in your case it has the isoform info)&#xA;names.fasta &amp;lt;- names(fasta.s)&#xA;&#xA;## extract only the relevant gene and isoform id (split name by the period symbol)&#xA;gene.iso &amp;lt;- sapply(names.fasta,function(j) cbind(unlist(strsplit(j,'\\.'))[2:3]))&#xA;&#xA;## convert to good data.frame = transpose result from previous step and add relevant column names&#xA;gene.iso.df &amp;lt;- data.frame(t(gene.iso))&#xA;colnames(gene.iso.df) &amp;lt;- c('gene','isoform')&#xA;&#xA;## and length of isoforms&#xA;gene.iso.df$width &amp;lt;- width(fasta.s)&#xA;&#xA;## split data.frame into list with entry for each gene&#xA;gene.iso.df.split &amp;lt;- split(gene.iso.df,gene.iso.df$gene)&#xA;&#xA;## optional to keep all the information but really you just need indices&#xA;##gene.iso.df.split.best &amp;lt;- lapply(gene.iso.df.split,function(x) x[order(x$width)[1],])&#xA;&#xA;## pull out the longest isoform ID for each gene (in case of a tie just take the first one)&#xA;best.id &amp;lt;- sapply(gene.iso.df.split,function(x) row.names(x)[order(x$width)[1]])&#xA;&#xA;## subset your original reads with the subset&#xA;fasta.s.best &amp;lt;- fasta.s[best.id]&#xA;&#xA;## export new fastafile containing longest isoform per gene&#xA;writeXStringSet(fasta.s, filepath='sample_best_isoform.fasta')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="383" LastActivityDate="2017-06-09T10:51:42.197" CommentCount="0" />
  <row Id="608" PostTypeId="1" AcceptedAnswerId="622" CreationDate="2017-06-09T15:08:40.710" Score="5" ViewCount="26" Body="&lt;p&gt;I start with a sorted and indexed bam file (&quot;mapped.bam&quot;) representing the mapping of small reads on a reference genome, and a bed file (&quot;genes.bed&quot;) containing the coordinates of a set of features of interest (let's say they are genes), for which I want to compute an average profile using programs from &lt;a href=&quot;http://deeptools.readthedocs.io/en/latest/content/list_of_tools.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;deeptools&lt;/a&gt;. &lt;strong&gt;I would like to understand the steps involved to be sure of what the vertical axis of the final profile represents.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;First step: making a bigwig file&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I create a bigwig file (&quot;mapped.bw&quot;) from the bam file using &lt;code&gt;bamCoverage&lt;/code&gt; as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bamCoverage -b mapped.bam -bs 10 -of=bigwig -o mapped.bw&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The help of &lt;code&gt;bamCoverage&lt;/code&gt; says:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The coverage is calculated as the number of reads per bin, where bins are short consecutive counting windows of a defined size.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In my case, the bins are 10 bp long. My reads are longer than that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a given bin, a given read can:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;completely overlap the bin&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;overlap the bin on n bp, n &amp;lt; 10&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;not overlap the bin at all&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Please correct me if I'm wrong: My guess it that the read is counted as 1 in cases 1. and 2., and 0 otherwise, and I also suppose that a read can be counted for several successive bins if it is long enough.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Second step: averaging over genes and plotting&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I compute a &quot;meta profile matrix&quot; (&quot;mapped_on_genes.gz&quot;) using &lt;code&gt;computeMatrix scale-regions&lt;/code&gt; as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;computeMatrix scale-regions \&#xA;    -S mapped.bw \&#xA;    -R genes.bed \&#xA;    --upstream 300 \&#xA;    --unscaled5prime 500 \&#xA;    --regionBodyLength 2000 \&#xA;    --unscaled3prime 500 \&#xA;    --downstream 300 \&#xA;    -out mapped_on_genes.gz&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(There is a &lt;code&gt;-bs&lt;/code&gt; parameter which default value is 10 according to the help of the command.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use this to plot a profile using &lt;code&gt;plotProfile&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;plotProfile -m mapped_on_genes.gz \&#xA;    -out mapped_on_genes_meta_profile.pdf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I obtain a profile in with values on the y axis. In what units are these values?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My guess is the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the upstream (300 bp) and internal 5-prime (500 bp), since the bin size was the same in &lt;code&gt;bamCoverage&lt;/code&gt; and &lt;code&gt;computeMatrix&lt;/code&gt;, each point on the x axis probably represents a 10 bp window, and its y coordinate is the average over the regions present in the bed file of the corresponding bins in the bigwig file, so it is an average number of reads overlapping a 10 bp bin.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Same thing at the 3-prime and downstream side.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the central 100 bp portion, before averaging over regions some shrinking or spreading of the bins must have been performed, I guess by averaging between neighbouring bins. So the final unit is still a &lt;strong&gt;number of reads overlapping a 10 bp bin&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And if I use larger bins, I should end up with proportionally higher values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Am I correct?&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-06-09T15:30:37.097" LastActivityDate="2017-06-09T17:04:19.710" Title="What unit do I get on the y-axis of a metagene profile plot?" Tags="&lt;hts&gt;&lt;deeptools&gt;&lt;bigwig&gt;&lt;metagene&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="609" PostTypeId="1" AcceptedAnswerId="632" CreationDate="2017-06-09T15:09:55.103" Score="5" ViewCount="38" Body="&lt;p&gt;I have two different sets of internal standards in a sample (1), and a sample (2) with these internal standards as well as about 30 peptides. Each peptide has one of each of these kind of standards. From sample 1, I can get the true ratio between standard 1 (S1) and standard 2 (S2), and from sample 2 I can get the ratio between the analyte (L) to each of the standards:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample 1: S1/S2&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample 2: L/S1, L/S2&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can thus calculate the ratio S1/S2 from the measured ratios in sample 2. Each sample is run in 5 technical replicates (that is; 5 injections into the LC-mass spectrometer), so S1/S2 ratios for one particular analyte could look like this (mock data):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;measured    calculated&#xA;0.967       0.987&#xA;1.007       0.967&#xA;1.044       1.012&#xA;1.041       1.025&#xA;1.048       1.046&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to compare the measured ratios to the calculated, to see if there are any bias in my acquisition of data. As the calculated and the measured ratio are from different samples, the replicates aren't &quot;linked&quot; so I could not use for example Bland Altman analysis. Should I use some kind of ANOVA? Or do you have any other recommendation?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would also like to make an overall assessment of the ratios of all 30 peptides to see if the bias is different for different peptides. How could I do that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any input would be greatly appreciated.&lt;/p&gt;&#xA;" OwnerUserId="796" LastEditorUserId="57" LastEditDate="2017-06-09T22:15:09.710" LastActivityDate="2017-06-10T15:40:21.290" Title="How do I compare measured ratios to calculated ratios in peptide mass spectrometry?" Tags="&lt;proteins&gt;&lt;mass-spectrometry&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="610" PostTypeId="1" CreationDate="2017-06-09T15:37:49.977" Score="4" ViewCount="38" Body="&lt;p&gt;I'm thinking that this isn't actually possible, but I'd like to check before I write it off.  I have RNA-seq data from cells under three different conditions, there are no replicates for any of the conditions.  The data has been processed with RSEM, and log2 fold changes have been calculated for each control-test pairing using the normalized expected read counts using EBseq.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If possible, I'd like to also calculate the p-value for each of these fold-changes, however, because there are no replicates I don't think that this is possible.  Is there any way to do this?&lt;/p&gt;&#xA;" OwnerUserId="506" LastActivityDate="2017-06-09T15:53:02.983" Title="Is it possible to calculate p-values for fold changes of single replicate RNA-seq samples?" Tags="&lt;ebseq&gt;&lt;rsem&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="611" PostTypeId="1" AcceptedAnswerId="618" CreationDate="2017-06-09T15:51:24.883" Score="8" ViewCount="138" Body="&lt;p&gt;The &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMv1.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;SAM specification&lt;/a&gt; indicates that each read group must have a unique ID field, but does not mark any other field as required. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have also discovered that htsjdk throws exceptions if the sample (SM) field is empty, though there is no indication in the specification that this is required. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there other read group fields that I should expect to be required by common tools? &lt;/p&gt;&#xA;" OwnerUserId="272" LastEditorUserId="73" LastEditDate="2017-06-09T16:52:01.640" LastActivityDate="2017-06-09T16:52:01.640" Title="What are the de facto required fields in a SAM/BAM read group?" Tags="&lt;file-formats&gt;&lt;sam&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="612" PostTypeId="2" ParentId="610" CreationDate="2017-06-09T15:53:02.983" Score="3" Body="&lt;p&gt;DESeq2 is able to adjust p-values for single-replicate samples by estimating shared dispersion across all samples. DESeq2 will give a warning, but try its best to carry out the analysis. More information on doing that can be found in the &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#can-i-use-deseq2-to-analyze-a-dataset-without-replicates&quot; rel=&quot;nofollow noreferrer&quot;&gt;DESeq2 vignette&lt;/a&gt; and the DESeq2 manual page.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The results will not be anywhere near the sensitivity that can be achieved when replicate information is available, but it might be good enough if only very obvious large-scale changes are of importance.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-09T15:53:02.983" CommentCount="2" />
  <row Id="613" PostTypeId="2" ParentId="611" CreationDate="2017-06-09T16:01:18.067" Score="3" Body="&lt;p&gt;The read group identifier needs to be specified in both the header lines of the BAM/SAM file and the alignment line. No other fields are required, but note that because the additional information is only stored once (i.e. in the header), it won't add much to file sizes or processing time if additional fields are included. If any of the other optional fields cause problems (either by inclusion or exclusion), it would be helpful to &lt;a href=&quot;https://github.com/samtools/htsjdk/issues&quot; rel=&quot;nofollow noreferrer&quot;&gt;report an issue&lt;/a&gt; about it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Older tools required you to specify both a header read group ID and an alignment read group ID, but most tools of the recent tools I've used seem to be intelligent enough that they will add headers for any read groups without that information.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-09T16:01:18.067" CommentCount="0" />
  <row Id="614" PostTypeId="5" CreationDate="2017-06-09T16:08:03.673" Score="0" Body="&lt;p&gt;The GFF3 formal specification is available at &lt;a href=&quot;https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-06-09T17:20:16.950" LastActivityDate="2017-06-09T17:20:16.950" CommentCount="0" />
  <row Id="615" PostTypeId="4" CreationDate="2017-06-09T16:08:03.673" Score="0" Body="Generic Feature Format version 3, a common tab-delimited plain-text format for annotating genes and other genomic features." OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-06-15T13:53:54.950" LastActivityDate="2017-06-15T13:53:54.950" CommentCount="0" />
  <row Id="616" PostTypeId="5" CreationDate="2017-06-09T16:16:17.263" Score="0" Body="&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.ensembl.org/info/website/upload/gff.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Ensembl spec&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://mblab.wustl.edu/GTF22.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;WUSTL spec&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-06-09T17:20:38.977" LastActivityDate="2017-06-09T17:20:38.977" CommentCount="0" />
  <row Id="617" PostTypeId="4" CreationDate="2017-06-09T16:16:17.263" Score="0" Body="Gene Transfer Format, a derivative of GFF, a tab-delimited plain-text format for describing gene annotations." OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-06-15T13:53:49.963" LastActivityDate="2017-06-15T13:53:49.963" CommentCount="0" />
  <row Id="618" PostTypeId="2" ParentId="611" CreationDate="2017-06-09T16:28:09.460" Score="8" Body="&lt;p&gt;The sample tag (i.e. SM) was a mandatory tag in the &lt;a href=&quot;https://sourceforge.net/p/samtools/code/HEAD/tree/trunk/sam-spec/&quot; rel=&quot;noreferrer&quot;&gt;initial SAM spec&lt;/a&gt; (see the &lt;code&gt;.pages&lt;/code&gt; file; you need a mac to open it). When transitioned to Latex, this requirement was mysteriously dropped. Picard is conforming to the initial spec. Anyway, the sample tag is important to quite a few tools. I would encourage you to add it.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-09T16:28:09.460" CommentCount="0" />
  <row Id="619" PostTypeId="2" ParentId="587" CreationDate="2017-06-09T16:28:26.360" Score="3" Body="&lt;p&gt;The main concern has always been the &quot;binning&quot; of quality scores that occurs via CRAM compression (and is also standard on the HiSeqX, HiSeq4000, and NovaSeq platforms). Anecdotally, I can report very little difference between 4-bin quality scores and full quality scores on cancer samples, though I don't know if I've seen a direct head-to-head comparison. &lt;/p&gt;&#xA;" OwnerUserId="74" LastActivityDate="2017-06-09T16:28:26.360" CommentCount="1" />
  <row Id="620" PostTypeId="1" CreationDate="2017-06-09T16:40:08.390" Score="4" ViewCount="35" Body="&lt;p&gt;Is there a computational tool for measuring what percentage of RNA is spliced in an RNAseq experiment?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not particularly interested in complicated analyses that give ratios for all possible alternative splicing variations. I'd rather have a binary classification. For example: (RNA1 = 35% spliced, 65% unspliced)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit: (Additional information)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We're very early in the exploratory stage, so a specific hypothesis is still forming. But we're interested in the sequence content of lncRNAs, many of which are poorly spliced. So we want to get a sense for how prevalent unspliced version of these transcript are. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because this is currently exploratory, estimations and noise are fairly acceptable. A very basic approach might be to calculate the ratio of reads that map to exon-to-exon junctions, verses those that map from exon-to-intro junctions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A more advanced approach I'm also considering is &lt;a href=&quot;https://combine-lab.github.io/salmon/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Salmon&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="800" LastEditorUserId="800" LastEditDate="2017-06-09T18:33:13.550" LastActivityDate="2017-06-13T19:14:44.307" Title="Spliced vs. unspliced ratios for transcripts in RNA-seq data" Tags="&lt;rna-seq&gt;&lt;rna-splicing&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="621" PostTypeId="2" ParentId="620" CreationDate="2017-06-09T16:48:33.833" Score="1" Body="&lt;p&gt;Take this first comment with a grain of salt, since this isn't an area I've worked in much, but: is binary classification possible? If a gene has 3 introns, and 2 are spliced out but 1 is retained, is this &quot;spliced&quot; or &quot;unspliced&quot;. My first impression is that an analysis would be a bit more nuanced than a binary classification.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, I'm aware of a tool called Keep Me Around for intron retention analysis (&lt;a href=&quot;https://arxiv.org/abs/1510.00696&quot; rel=&quot;nofollow noreferrer&quot;&gt;preprint&lt;/a&gt;, &lt;a href=&quot;https://github.com/pachterlab/kma&quot; rel=&quot;nofollow noreferrer&quot;&gt;code&lt;/a&gt;). I've never used this software, but it looks like it's actively maintained and was created by a research group with a lot of experience and influence in this area, for whatever that's worth. :-)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: It looks like the original post was modified since I submitted my answer, but I'll leave the text about binary classification around for posterity. :-)&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-06-09T16:48:33.833" CommentCount="0" />
  <row Id="622" PostTypeId="2" ParentId="608" CreationDate="2017-06-09T17:04:19.710" Score="2" Body="&lt;p&gt;Feel free to @ me in deepTools questions, since I'm the primary developer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a given bin, the count assigned to it is the number of reads overlapping it, regardless of whether they overlap by 1 or 10 bases. So a read overlapping only partially and one overlapping completely are treated the same.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since your bigWig file is in units of &quot;alignments&quot; (i.e., it's not 1x normalized), the resulting profile will also be in units of &quot;alignments&quot; (i.e., profiles and heatmaps are in whatever units the input files are in).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Upstream/downstream regions and unscaled regions are also 10 base bins. Note that these are then the average of the per-base value, since the bins here may not perfectly correspond to the bins in the bigWig files. The line in the profile plot is indeed the average (by default, you can choose median, max, min, etc.) of the underlying regions for each bin.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding the scaled section in the middle, the the number of genomic bases per bin is changed such that the region will have &lt;code&gt;&quot;length&quot;/(regionBodyLength/binSize)&lt;/code&gt; bases each. As above, the per-base value is then averaged (or whatever you specify) to derive the per-bin value. The &lt;code&gt;length&lt;/code&gt; here is decreased if you have unscaled regions, since otherwise bases would get counted twice.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-09T17:04:19.710" CommentCount="2" />
  <row Id="623" PostTypeId="1" CreationDate="2017-06-09T18:17:58.273" Score="6" ViewCount="59" Body="&lt;p&gt;I am using &lt;strong&gt;PythonCyc&lt;/strong&gt; API in order to write a query for metabolites in &lt;strong&gt;BioCyc&lt;/strong&gt;. The purpose of this API is to communicate with the database software of BioCyc- Pathway Tools. Pathway Tools is in lisp therefore, PythonCyc creates a bridge between python and common lisp. To use this this API one must first create a PGDB object with a specified organism ID (orgid). In the example below I create a PGDB with orgid &quot;meta&quot;. After this, I am able to call methods from PythonCyc with this object:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import sys&#xA;import pythoncyc&#xA;#this creates PGDB object associated with meta(MetaCyc)&#xA;meta = pythoncyc.select_organism('meta')&#xA;#lists pathways of specified compound&#xA;pathways = meta.pathways_of_compound('sucrose')&#xA;print pathways&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The query for the metabolite above, 'sucrose', provides a list a pathways:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[u'|PWY-7347|', u'|SUCSYN-PWY|', u'|PWY-7238|', u'|PWY-6527|', u'|PWY-5343|', u'|PWY-3801|', u'|PWY-7345|', u'|PWY-862|', u'|SUCUTIL-PWY|', u'|PWY66-373|', u'|PWY-621|', u'|PWY-822|', u'|SUCROSEUTIL2-PWY|', u'|PWY-6525|', u'|PWY-6524|', u'|PWY-5337|', u'|PWY-5384|']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However if I switch this metabolite name to a common amino acid name, such as 'valine':&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import sys&#xA;import pythoncyc&#xA;#this creates PGDB object associated with meta(MetaCyc)&#xA;meta = pythoncyc.select_organism('meta')&#xA;#lists pathways of specified compound&#xA;pathways = meta.pathways_of_compound('valine')&#xA;print pathways&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The query provides an error message that states &quot;non coercible frame&quot;, which means it cannot find this ID. This is a common metabolite, just as sucrose, which should very well have pathway entries in database, however there are none found with this method. I have also tried synonyms of valine and other methods such as &quot;all_pathways(cpds)&quot; stated in API, which give me the same error message.&#xA;What is the reason for this? Does BioCyc not list any pathways for amino acids? Or am I using an incorrect method to access amino acid information?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://pythoncyc.readthedocs.io/en/latest/&quot; rel=&quot;noreferrer&quot;&gt;Link to PythonCyc API&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="803" LastActivityDate="2017-07-17T23:46:37.833" Title="What methods should I use from PythonCyc API to query metabolites in BioCyc database?" Tags="&lt;biopython&gt;&lt;api&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="624" PostTypeId="2" ParentId="453" CreationDate="2017-06-09T20:27:11.420" Score="1" Body="&lt;p&gt;I wrote a command-line (C++14) tool called &lt;code&gt;subset&lt;/code&gt; which is up on Github: &lt;a href=&quot;https://github.com/alexpreynolds/subset&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/alexpreynolds/subset&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This should be reasonably memory efficient and fast. The &lt;code&gt;subset&lt;/code&gt; tool does not store input lines in a table, but instead streams through the file once, storing a 4 or 8k buffer chunk of the input file (depending on OS). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It stores line numbers in an array, but eight bytes per integer * 100k is 800kB, for that use case — not very much memory there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's an O(nlogn) sort penalty on the line number array, but again this list will be much smaller than the query file, and integer sorting is fairly optimized, so the hit should be small. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If your line number list is already sorted, I could add an option to skip sorting; let me know if that would be useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The filtering step walks through the line number array and input file linearly, printing lines where there are index matches, and skipping over the rest. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Indeed, &lt;code&gt;subset&lt;/code&gt; will quit early in parsing the input file if there are no more line numbers to query. So this feature is especially useful for speeding up filtering of very large query files. (If your query file has 1M rows, say, and your last line number of interest is 12345, there's no reason to read through the rest of the file.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can grab, build and install it like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/alexpreynolds/subset.git&#xA;$ cd subset&#xA;$ make&#xA;$ cp subset /usr/local/bin&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Once the binary is in your path, there are a couple ways to use it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, you can specify a start index and length value. The following grabs seven lines starting with the 33rd line (32 as a 0-indexed value):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ subset --prefix-with-indices -s 32 -n 7 -i query.txt &amp;gt; answer.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or you can specify a text file containing line numbers, each on a separate line. The following reads in a file called &lt;code&gt;line-numbers.txt&lt;/code&gt; and uses that to filter &lt;code&gt;query.txt&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ subset --prefix-with-indices -l line-numbers.txt -i query.txt &amp;gt; answer.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The indices in &lt;code&gt;line-numbers.txt&lt;/code&gt; should be positive, 0-indexed integers. The list of numbers does not need to be sorted, as &lt;code&gt;subset&lt;/code&gt; will sort the list of numbers for you. This is so that an efficient single pass through the input/query file can be done.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can leave out &lt;code&gt;--prefix-with-indices&lt;/code&gt; to leave out the debug prefix. This is there so that you can do a sanity check on the result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;test/makefile&lt;/code&gt; tests demonstrate options and usages for the two types of filtering.&lt;/p&gt;&#xA;" OwnerUserId="776" LastEditorUserId="776" LastEditDate="2017-06-10T01:29:46.010" LastActivityDate="2017-06-10T01:29:46.010" CommentCount="0" />
  <row Id="625" PostTypeId="2" ParentId="296" CreationDate="2017-06-10T03:52:26.343" Score="0" Body="&lt;p&gt;My bet is that the nanopore folks have (of course!) done a ton of optimisation on two key things:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The DNA cleanup&lt;/li&gt;&#xA;&lt;li&gt;The library preps&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I think that for most DNA extractions cleanups can vary a lot, but something like a Blue Pippin will do an exceptional job if you have access to one and can use it (they won't work for SUPER long DNA fragments which can't move through a gel of course). The Blue Pippin also does your size selection, so you don't burn out pores sequencing lots of little stuff. This should help yield too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then it's all down to the library prep, and @gringer's answer has some good tips on that. &lt;/p&gt;&#xA;" OwnerUserId="156" LastEditorUserId="73" LastEditDate="2017-06-10T05:01:59.620" LastActivityDate="2017-06-10T05:01:59.620" CommentCount="0" />
  <row Id="626" PostTypeId="1" AcceptedAnswerId="629" CreationDate="2017-06-10T03:57:26.523" Score="9" ViewCount="74" Body="&lt;p&gt;I have a large phylogenomic alignment of &gt;1000 loci (each locus is ~1000bp), and &gt;100 species. I have relatively little missing data (&amp;lt;10%). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to estimate a maximum-likelihood phylogenetic tree from this data, with measures of statistical support on each node.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many phylogenetics programs that claim to be able to analyse datasets like this (e.g. RAxML, ExaML, IQtree, FastTree, PhyML?, etc). Given that I have access to a big server (512GB RAM, 56 cores), what are the pros and cons of each program. Which is likely to give me the most accurate estimate of the ML tree for a dataset of this size?&lt;/p&gt;&#xA;" OwnerUserId="156" LastActivityDate="2017-06-12T08:49:18.100" Title="What is the best method to estimate a phylogenetic tree from a large dataset of &gt;1000 loci and &gt;100 species" Tags="&lt;phylogenetics&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="627" PostTypeId="1" CreationDate="2017-06-10T10:41:51.353" Score="1" ViewCount="25" Body="&lt;p&gt;I've been processing data for a consortium project that uses the &lt;a href=&quot;https://github.com/zhanxw/rvtests&quot; rel=&quot;nofollow noreferrer&quot;&gt;rvtests&lt;/a&gt; toolkit. Our data analysis process uses a kinship matrix and the calculation of inverse-normalised transformed statistics after covariate adjustment (for 1kG-imputed data from a SNPchip). One of the operations we have been asked to do is to analyse males and females separately for a number of different phenotypes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's an example of the command line that I'm running:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rvtest --inVcf chr${ch}.imputed.poly.vcf.gz \&#xA; --pheno ../scripts/phenotypes.raw.2017-May-01.ped \&#xA; --pheno-name ${traitName} \&#xA; --out rv_results/${traitName}_FEMALE_chr${ch} \&#xA; --kinship kinship_matrix.kinship \&#xA; --meta score,cov[windowSize=500000] \&#xA; --peopleIncludeFile ../scripts/female.iids.txt \&#xA; --covar ../scripts/covar.2017-May-11.ped \&#xA; --covar-name ${covs} --useResidualAsPhenotype --dosage DS&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, the kinship matrix doesn't interact well with the &lt;code&gt;--peopleinclude&lt;/code&gt; parameter. When I run this, the program complains about unexpected columns in the kinship matrix (starting at one column greater than the number of females in the dataset). I suspect what is happening is that rvtests is assuming that the kinship matrix applies to the sample set &lt;em&gt;after&lt;/em&gt; filtering, rather than applying to the total sample set. This sets off a few warning bells in my head, because it means that rvtest is probably ignoring the column/row labels in the kinship matrix that it has generated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I work around this problem?&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-08-09T14:17:51.377" Title="How can I combine a kinship matrix with subset individuals when using rvtests?" Tags="&lt;microarray&gt;&lt;rvtests&gt;&lt;covariate-analysis&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="628" PostTypeId="1" AcceptedAnswerId="2045" CreationDate="2017-06-10T11:14:30.313" Score="3" ViewCount="55" Body="&lt;p&gt;I used a script in R language that uses nnet library to predict promoter bacteria and i would like to know how to extract rules from this neural network results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As my input of the neural network i have n positive examples of promoter and n false examples. I use as input these examples and the FASTA file with the genome of the bacteria. As a result, i have a value of 0 to 1 for each network tested corresponding to its learning. I want to discover roles that can improve my network in future experiments in the same bacteria.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which algorithms or softwares could i use?&lt;/p&gt;&#xA;" OwnerUserId="813" LastEditorUserId="96" LastEditDate="2017-06-10T19:20:57.900" LastActivityDate="2017-07-11T15:01:38.167" Title="Rule Extraction from nnet results" Tags="&lt;r&gt;&lt;fasta&gt;&lt;machine-learning&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="629" PostTypeId="2" ParentId="626" CreationDate="2017-06-10T11:58:10.863" Score="7" Body="&lt;p&gt;&lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0027731&quot; rel=&quot;nofollow noreferrer&quot;&gt;This paper claims&lt;/a&gt; that FastTree is almost as accurate as RAxML, while being much faster. You just have to be careful, however, that the support values output by FastTree are not bootstrap values, they are &lt;a href=&quot;http://www.microbesonline.org/fasttree/&quot; rel=&quot;nofollow noreferrer&quot;&gt;based on the Shimodaira-Hasegawa test&lt;/a&gt;. (Also, &lt;a href=&quot;http://darlinglab.org/blog/2015/03/23/not-so-fast-fasttree.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;see this comment&lt;/a&gt; for the case you have very short branch lengths).   [&lt;strong&gt;update:&lt;/strong&gt; However, according to &lt;a href=&quot;http://biorxiv.org/content/early/2017/05/25/142323&quot; rel=&quot;nofollow noreferrer&quot;&gt;the recent comparison paper mentioned below&lt;/a&gt; FastTree performed quite poorly in comparison to RAxML or IQ-tree.]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I understand, you should use ExaML only if your data is too large to be handled by RAxML in a single node. ExaML should perform like RAxML but with some parallelization overhead. For all effects I treat them as the same. I don't know of relevant advantages of phyML over RAxML (for me, it's easier to use but I am very used to phyML).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not familiar with IQ-tree, but &lt;a href=&quot;https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msu300&quot; rel=&quot;nofollow noreferrer&quot;&gt;its authors claim&lt;/a&gt; that even given the same time as RAxML or phyML, IQ-tree already finds better likelihoods more often than not (although by default it takes a bit longer to converge). &lt;a href=&quot;http://biorxiv.org/content/early/2017/05/25/142323&quot; rel=&quot;nofollow noreferrer&quot;&gt;A recent comparison between all these programs&lt;/a&gt; favoured RAxML for single-gene analysis and IQ-tree for concatenation (with RAxML very close). It may also estimate branch support through a SH-like test only, but I'm not sure.   [&lt;strong&gt;update:&lt;/strong&gt; IQ-tree offers 3 measures of support, standard bootstrap, aLRT, and ultrafast bootstrap. See OP's comment below for details.]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, since you have few missing data, you might also want to try a single-locus tree inference followed by gene tree clustering (using &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/1755-0998.12676/abstract&quot; rel=&quot;nofollow noreferrer&quot;&gt;treescape&lt;/a&gt; or &lt;a href=&quot;https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msw038&quot; rel=&quot;nofollow noreferrer&quot;&gt;treeCL&lt;/a&gt;) to see how spread your data is, or to see the effect of removal of outliers, or to use ideas similar to &lt;a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0129183&quot; rel=&quot;nofollow noreferrer&quot;&gt;statistical binning&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="45" LastEditorUserId="45" LastEditDate="2017-06-12T08:49:18.100" LastActivityDate="2017-06-12T08:49:18.100" CommentCount="5" />
  <row Id="630" PostTypeId="2" ParentId="627" CreationDate="2017-06-10T13:12:47.700" Score="0" Body="&lt;p&gt;Adam Locke (a collaborator of this project) suggests that removing covariate information for the unselected individuals (i.e. setting it to NA) works around this problem:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;I believe the problem is that he is using a pre-computed kinship matrix including both males and females, and when using the “—peopleIncludeFile” it can’t properly select the right people from the kinship matrix. I don’t know that others have tried to do analysis this way. An alternative, that I think has worked for others, is to generate phenotype variables that have values only for males or females (so, e.g., BMI, BMI_females, BMI_males). For whatever reason, this appears to correctly only use the and select the samples desired without the kinship matrix.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Presumably a kinship matrix that is generated for just the subset individuals will work as well, although I don't like the idea that rvtests isn't able to do a label lookup on the kinship matrix to find the correct individuals.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-10T13:12:47.700" CommentCount="0" />
  <row Id="631" PostTypeId="2" ParentId="628" CreationDate="2017-06-10T14:14:17.493" Score="4" Body="&lt;p&gt;R's nnet package only supports fully connected neural networks with one hidden layer. This is the most primitive type of network. I doubt it will work well for promotors finding. In addition, such network won't give you useful interpretations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to explore neural networks, you should use a one-dimension convolution layer as is described in &lt;a href=&quot;http://www.nature.com/nbt/journal/v33/n8/full/nbt.3300.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;this paper&lt;/a&gt;. This layer effectively represents a positional weight matrix (PWM). You can know which motifs are used. With backtracking, you can also identify the coordinates of motifs. To deploy such a model, you need to be fairly familiar with deep learning and to learn one deep learning framework. For framework, you may start with &lt;a href=&quot;https://keras.io&quot; rel=&quot;nofollow noreferrer&quot;&gt;keras&lt;/a&gt;. Once you get used to keras, you can implement your full promotor finder in less than 100 lines of python code. Alternatively, you may try &lt;a href=&quot;http://kundajelab.github.io/dragonn/&quot; rel=&quot;nofollow noreferrer&quot;&gt;dragonn&lt;/a&gt;. It is supposed to simplify deploying models for DNA sequences. I have no experience with it, though.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For promotor finding, it is also worth trying traditional methods. It is interesting that few/no neuralNet-based publications have evaluated traditional methods, probably because many ML people know little about classical motif finding. These methods could work well, too.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-10T14:14:17.493" CommentCount="0" />
  <row Id="632" PostTypeId="2" ParentId="609" CreationDate="2017-06-10T15:40:21.290" Score="2" Body="&lt;p&gt;From a statistical perspective ratios are icky because the variance of the sampling distribution of a ratio decreases with the size of the numerator and denominator being sampled to create the ratio. If you can get at the numerator and denominator separately, then you should absolutely do that and look at count based tests like the Chi-square test, Fisher's exact test, or Poisson regression. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, a t-test would be appropriate in the two-sample case if its assumptions are met.  You need to check that both samples are normally distributed, ideally via a qq-plot. You also need to check the variance of both samples and choose a variant of the t-test that does or does not assume equal variances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the samples are not normally distributed, then the next approach would be a &lt;a href=&quot;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mann-Whitney U&lt;/a&gt; test.&lt;/p&gt;&#xA;" OwnerUserId="492" LastActivityDate="2017-06-10T15:40:21.290" CommentCount="0" />
  <row Id="633" PostTypeId="1" CreationDate="2017-06-10T20:24:19.480" Score="2" ViewCount="27" Body="&lt;p&gt;I have seen one of &lt;a href=&quot;https://discover.nci.nih.gov/cellminer/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cellminer tools&lt;/a&gt;. I am not sure how do they calculate the cross correlation of the genes, what does it actually mean? Based on what databases? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example if I take their example input (&lt;a href=&quot;http://www.genenames.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;HUGO&lt;/a&gt; format): &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;abcb1&#xA;BRCA2&#xA;CNBP&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following cross correlation matrix (based on what signals).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Identifier      abcb1   BRCA2   CNBP&#xA;abcb1           1      -0.142   0.069&#xA;BRCA2          -0.142   1       0.176&#xA;CNBP           0.069    0.176   1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="734" LastEditorUserId="734" LastEditDate="2017-06-11T05:26:04.453" LastActivityDate="2017-06-20T08:34:17.377" Title="How does Cellminer's &quot;Cross-correlations of transcripts, drugs, and microRNAs&quot; work" Tags="&lt;statistics&gt;&lt;gene&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="634" PostTypeId="2" ParentId="604" CreationDate="2017-06-10T23:09:04.270" Score="0" Body="&lt;p&gt;You can see the exact assembly by looking in the README file in Ensembl's genome download page. &lt;a href=&quot;ftp://ftp.ensembl.org/pub/release-89/fasta/homo_sapiens/dna/README&quot; rel=&quot;nofollow noreferrer&quot;&gt;ftp://ftp.ensembl.org/pub/release-89/fasta/homo_sapiens/dna/README&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see the current assembly is GCA_000001405.25. Ensembl 79 uses version GCA_000001405.17. You can expect some differences between assembly versions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you take a look at &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/assembly?term=GRCh38&amp;amp;cmd=DetailsSearch&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/assembly?term=GRCh38&amp;amp;cmd=DetailsSearch&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can see that 20,466,394 bp have been added since 17 -&gt; 25. This is only about 0.6% change so I would guess there is not that much difference between these versions. However, if I were you I would use the corresponding version of Ensembl. Even better would be using the latest version of Ensembl, as not only the genomes primary sequence but also the positions of genes can change between versions (&lt;a href=&quot;https://doi.org/10.1093/bib/bbw017&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://doi.org/10.1093/bib/bbw017&lt;/a&gt;). &lt;/p&gt;&#xA;" OwnerUserId="269" LastActivityDate="2017-06-10T23:09:04.270" CommentCount="0" />
  <row Id="637" PostTypeId="1" CreationDate="2017-06-10T23:27:13.180" Score="3" ViewCount="147" Body="&lt;p&gt;I read a lecture notes about &lt;a href=&quot;http://mcb.berkeley.edu/courses/mcb142/lecture%20topics/Dernburg/Lecture6_Chapter8_screenviewing.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;mutations&lt;/a&gt;, what kind of algorithms are there to detect mutations? How do people know if the gene is mutated or whether it's a sequencing error?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I saw &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/98/how-to-quickly-determine-mutations-in-a-read-of-a-sam-file&quot;&gt;this&lt;/a&gt; which is related, but I am not sure how to work with CIGAR, is it 100% accurate? What's the underline mechanism to detect mutation? Is there a way to predict what the mutation will cause.&lt;/p&gt;&#xA;" OwnerUserId="734" LastEditorUserId="734" LastEditDate="2017-08-05T18:04:19.953" LastActivityDate="2017-08-14T09:51:31.817" Title="How to detect a mutation and predict its consequence?" Tags="&lt;variant-calling&gt;&lt;gene&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="638" PostTypeId="2" ParentId="637" CreationDate="2017-06-10T23:58:21.667" Score="4" Body="&lt;p&gt;&lt;strong&gt;Part 1 : how to detect mutations&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The keywords you are searching for are &quot;variant calling&quot;. Basically you have to map sequencing reads to a reference genome (or gene) and then estimate for each position of the genome if the observed difference of mapped reads and the reference is more likely a sequencing error or a mutation (in genomic glossary - variant). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Popular tools for variant calling are &lt;a href=&quot;https://software.broadinstitute.org/gatk/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GATK&lt;/a&gt;, &lt;a href=&quot;https://github.com/ekg/freebayes&quot; rel=&quot;nofollow noreferrer&quot;&gt;FreeBayes&lt;/a&gt; or &lt;a href=&quot;https://samtools.github.io/bcftools/howtos/variant-calling.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;bcftools&lt;/a&gt; (previously part as Samtools package).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question you linked asks for quick alternatives to simply looking for variants. Indeed, you can just visualise the mapped reads to the reference sequence and see if the variant is there or not. CIGAR is just a notation of read alignment used in sam files (files with mapped reads), you can find good explanation of CIGAR strings &lt;a href=&quot;http://genome.sph.umich.edu/wiki/SAM&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The follow-up about &quot;How to estimate an effect of mutation&quot; is in &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/644/292&quot;&gt;@DevonRyan 's awesome answer&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="292" LastEditDate="2017-06-12T11:36:07.287" LastActivityDate="2017-06-12T11:36:07.287" CommentCount="0" />
  <row Id="639" PostTypeId="1" AcceptedAnswerId="641" CreationDate="2017-06-11T01:28:57.607" Score="12" ViewCount="384" Body="&lt;p&gt;What are the advantages of having Bioconductor, for the bioinformatics community?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've read the '&lt;a href=&quot;https://bioconductor.org/about/&quot; rel=&quot;noreferrer&quot;&gt;About&lt;/a&gt;' section and skimmed the &lt;a href=&quot;https://genomebiology.biomedcentral.com/articles/10.1186/gb-2004-5-10-r80&quot; rel=&quot;noreferrer&quot;&gt;paper&lt;/a&gt;, but still cannot really answer this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand Bioconductor is released twice a year (unlike &lt;code&gt;R&lt;/code&gt;), but if I want to use the latest version of a package, I'll have to use the dev version anyway. A stamp of &lt;em&gt;approval&lt;/em&gt; could be achieved much easier with a tag or something, so it sounds just like an extra (and unnecessary) layer to maintain.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Related to this, what are the advantages as a developer to have your package accepted into Bioconductor?&lt;/p&gt;&#xA;" OwnerUserId="208" LastActivityDate="2017-06-12T18:31:09.837" Title="Why Bioconductor?" Tags="&lt;r&gt;&lt;bioconductor&gt;" AnswerCount="4" CommentCount="3" FavoriteCount="2" />
  <row Id="641" PostTypeId="2" ParentId="639" CreationDate="2017-06-11T07:13:30.900" Score="12" Body="&lt;p&gt;&lt;strong&gt;Benefits of central repository for Community&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having a central repository for packages is very useful. For couple of reasons:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It makes very easy to resolve &lt;strong&gt;dependencies&lt;/strong&gt;. Installing all the dependencies manually would be exhausting but also dangerous (point 2).&lt;/li&gt;&#xA;&lt;li&gt;Package &lt;strong&gt;compatibility&lt;/strong&gt;! If I install package with dependencies, I would like to be sure that I install correct versions of all the dependencies.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reliability&lt;/strong&gt; thanks to unified and integrated testing. &lt;code&gt;Bioconductor&lt;/code&gt; is trying really hard to force developers to write good test, they also have people manually testing submitted packages. They also remove packages that are not maintained. Packages in &lt;code&gt;Bioconductor&lt;/code&gt; are (reasonably) reliable.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In the end, installing dev versions of R packages is in my opinion &lt;strong&gt;very bad practise&lt;/strong&gt; for reproducible science. If developers delete GitHub repo, commit hash you have used won't be enough to get the code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Benefits of central repository for developers&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I forgot about the advantages for you as developer to submit your package to &lt;code&gt;Bioconductor&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Your package will be more visible&lt;/li&gt;&#xA;&lt;li&gt;users will have a guarantee that your code was checked by third person&lt;/li&gt;&#xA;&lt;li&gt;Your package will be for users easier to install&lt;/li&gt;&#xA;&lt;li&gt;Your package will be forced to use standardized vignettes, version tags and tests -&gt; will be more accessible by community to build on your code&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Bioconductor specific advantages over CRAN&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see the big advantage in the &lt;a href=&quot;https://support.bioconductor.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;community support page&lt;/a&gt;, provided by &lt;code&gt;Bioconductor&lt;/code&gt;. &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/643/57&quot;&gt;@Llopis' comprehensive elaboration&lt;/a&gt;. &lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-12T18:31:09.837" LastActivityDate="2017-06-12T18:31:09.837" CommentCount="4" />
  <row Id="642" PostTypeId="2" ParentId="639" CreationDate="2017-06-11T10:09:41.653" Score="2" Body="&lt;p&gt;Regarding what the advantage is to you as a developer of having a bioconductor package rather than using CRAN:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;There's a hierarchy of package quality, with bioconductor on the top (followed by CRAN and then &quot;random github repos&quot;). While there are many excellent CRAN packages, the average bioconductor package is better tested and documented. So if I as a user have two different packages that I can use and one is on bioconductor and the other CRAN, then I use the bioconductor package.&lt;/li&gt;&#xA;&lt;li&gt;Higher visibility. Since bioconductor packages are held in higher regard, they also become more visible. Further, the different &quot;views&quot; (e.g., &quot;Alternative Splicing&quot; and &quot;Transcription&quot;) make it convenient to find relevant packages. You can always search CRAN, but allowing tagging like this aids in discovery.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-11T10:09:41.653" CommentCount="6" />
  <row Id="643" PostTypeId="2" ParentId="639" CreationDate="2017-06-11T10:13:24.243" Score="7" Body="&lt;p&gt;Here is a list of the advantages of having Bioconductor for the bioinformatic community:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Outreach&lt;/strong&gt;: You have a &lt;strong&gt;repository for the field&lt;/strong&gt;, in that language. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Some packages related to bioinformatics (in R) are distributed through personal repositories, CRAN, github, bitbucket, sourceforge, but they are less used and harder to find.&lt;br&gt;&#xA;There are such efforts in other languages too: Biopython, Bioperl, Biojava, ...&lt;br&gt;&#xA;Also is harder to find the repositories related to a subject in CRAN, you don't have the BiocViews, the equivalent is optional and not usually filled,  which is quite useful when looking for a method.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Quality&lt;/strong&gt;: In Bioconductor each package is tested in Linux, Windows, and iOS, to make sure it works in all major operative systems (with all the dependencies). &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In some rare cases like &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/Rsubread.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;this one&lt;/a&gt;, a packages is not supported for certain platform, but you can known it by checking the &lt;a href=&quot;http://bioconductor.org/checkResults/release/bioc-LATEST/Rsubread/&quot; rel=&quot;nofollow noreferrer&quot;&gt;build report&lt;/a&gt;.&lt;br&gt;&#xA;You are required to provide a vignette and examples in every exported element (and the vignette, examples and tests should pass). You are required to be able to install the package with stricter quality than CRAN, because there is a manual review (They pointed out a &lt;em&gt;comment&lt;/em&gt; in one of my functions!).&lt;br&gt;&#xA;They also provide docker images of the base packages. You don't need to install the latest R version to use the Bioconductor! But developers do so (at least when checked by Bioconductor servers) to ensure that the package will keep working in next R release. &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Reusing&lt;/strong&gt;: Bioconductor provides the basic elements to a big number of applications. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;For example the summarizedExperiment class is provided so that any package that needs a similar object can (should) use it. Or GSEABase is the base package to deal with GSEA enrichment analysis, providing functions, methods and classes for gene sets, and collections, making easier for anyone to create their own GSE analysis.&lt;br&gt;&#xA;It is easier to build upon the work of others if you know you are following the same quality standards. &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Support&lt;/strong&gt;: To support Q&amp;amp;A is mandatory, the package maintainer must be registered in the &lt;a href=&quot;https://support.bioconductor.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;webpage&lt;/a&gt;. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;While in CRAN usually the support is given by each package in its own way, in Bioconductor you can directly reach the maintainer and the users by posting in the same central place. &lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2017-06-11T22:00:10.837" LastActivityDate="2017-06-11T22:00:10.837" CommentCount="2" />
  <row Id="644" PostTypeId="2" ParentId="637" CreationDate="2017-06-11T10:15:56.960" Score="6" Body="&lt;p&gt;I'll follow up to the &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/638/292&quot;&gt;great answer&lt;/a&gt; from &lt;a href=&quot;https://bioinformatics.stackexchange.com/users/57/kamil-s-jaron&quot;&gt;Kamil S Jaron&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding predicting what the variant (&quot;mutation&quot; is a very loaded term) will do, there are a variety of tools. Chief among these are &lt;a href=&quot;http://annovar.openbioinformatics.org/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;annovar&lt;/a&gt; and &lt;a href=&quot;http://www.ensembl.org/info/docs/tools/vep/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;VEP&lt;/a&gt;. The general idea behind these is to classify the variants according to their overlap with genes, which codons they change (if any), how big that change is (e.g., changes in charge are more likely detrimental) and so on. One could also consider conservation, since if a position is highly conserved then changes in it are more likely to be detrimental.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you really want to predict how a variant will change a protein's function then that usually requires prior knowledge about the proteins in question. Eventually someone will use machine learning to cull the literature and provide good predictions, but I haven't seen that yet.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="48" LastEditDate="2017-08-14T09:51:31.817" LastActivityDate="2017-08-14T09:51:31.817" CommentCount="1" />
  <row Id="645" PostTypeId="2" ParentId="639" CreationDate="2017-06-11T10:18:55.697" Score="2" Body="&lt;p&gt;The most important reason is that Bioconductor has growing set of common data structures and base packages. If package X and package Y needs to work with the same type of data, having a common data structure in the core Bioconductor package Z makes our lives so much easier. I could do something in package X, take out the results and keep working on my data with package Y. This works because I'm using versions of package X and Y that are compatible with the data structure defined in package Z. The Bioconductor team makes sure that all packages use the common data structures and already existing packages where possible, so that we're all on the same page and so that people don't reinvent the wheel again and again.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, there's a sophisticated process around &lt;a href=&quot;https://www.bioconductor.org/developers/package-submission/&quot; rel=&quot;nofollow noreferrer&quot;&gt;getting your package accepted to Bioconductor&lt;/a&gt;. This ensures that packages use data structures and functions already available in other Bioconductor packages (where reasonable), it ensures that packages are well written, that they have good documentation, and that they are well tested.&lt;/p&gt;&#xA;" OwnerUserId="283" LastEditorUserId="283" LastEditDate="2017-06-12T10:22:33.963" LastActivityDate="2017-06-12T10:22:33.963" CommentCount="0" />
  <row Id="646" PostTypeId="1" AcceptedAnswerId="647" CreationDate="2017-06-11T13:49:45.317" Score="4" ViewCount="84" Body="&lt;p&gt;I have two kinds of interactions: transient and stable. We are supposed to work on stable interaction, like interactions between two monomers in a heterodimer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a heterodimer there are two chains in which there are some residues between monomers which take part in the interaction of monomers and building the structure of heterodimer, and there are other residues interacting inside each monomer for building the structure of that monomer. There are some PDB IDs related to heterodimers in PDB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now imagine that we are trying to show which residues are physically interacting between two monomers in a heterodimer, and then we are trying two show the site of these physically interacting residues in the sequence related to that special structure. For that, we need to find the sequence related to that structure and we can download it from PDB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But if we want to make a multiple sequence alignment of that sequence, first we need to find the refseq sequence of that and then we must blast the sequence and find homologues of that sequence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here there are some challenges:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;we must find residues interacting in the structure of heterodimer between two monomers.&lt;/li&gt;&#xA;&lt;li&gt;we must find the sequences of monomer chains of the heterodimer and map the interacting residues in the structure to the sequence.&lt;/li&gt;&#xA;&lt;li&gt;we know that when a structure is solved and its monomers are sequenced, may be they are not able to completely sequence the heterodimer and residue numbers are maybe different from the related refseq sequence. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I would like to know how can we find the related refseq sequence (or sequences, in the case of heterodimer) and then how can we map the physically interacting residues in the structure of PDB ID to refseq sequence (finding the sites of these interacting residues in that refseq sequence) regarding three challenges mentioned above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Imagine that I have 100 PDB IDs of 100 nonredundant heterodimers and I would like to find and download the structures, sequence and finally refseq sequence. What should I do? &lt;/p&gt;&#xA;" OwnerUserId="818" LastEditorUserId="73" LastEditDate="2017-06-13T02:55:06.973" LastActivityDate="2017-06-13T16:30:54.753" Title="Getting structure and sequence related to PDB IDs" Tags="&lt;protein-structure&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="647" PostTypeId="2" ParentId="646" CreationDate="2017-06-11T14:55:08.643" Score="6" Body="&lt;p&gt;You can download seqs and structures based on a list of PDB ids using &lt;a href=&quot;http://www.rcsb.org/pdb/download/download.do#FASTA&quot; rel=&quot;noreferrer&quot;&gt;http://www.rcsb.org/pdb/download/download.do#FASTA&lt;/a&gt; &lt;/p&gt;&#xA;" OwnerUserId="640" LastActivityDate="2017-06-11T14:55:08.643" CommentCount="0" />
  <row Id="649" PostTypeId="2" ParentId="81" CreationDate="2017-06-11T20:27:23.053" Score="0" Body="&lt;p&gt;Another next-gen read simulation tool is &lt;a href=&quot;https://popmodels.cancercontrol.cancer.gov/gsr/packages/gemsim/&quot; rel=&quot;nofollow noreferrer&quot;&gt;gemsim&lt;/a&gt;.  I haven't tested it, but I would be interested if anyone has had any experience with it.&lt;/p&gt;&#xA;" OwnerUserId="492" LastActivityDate="2017-06-11T20:27:23.053" CommentCount="0" />
  <row Id="650" PostTypeId="1" AcceptedAnswerId="670" CreationDate="2017-06-11T20:33:33.147" Score="1" ViewCount="55" Body="&lt;p&gt;I am going to describle the problem completely. First I should say that there are two kinds of interactions: transient and stable interactions. We are supposed to work on stable interaction like interactions between two monomers in a heterodimer. In a heterodimer there are two chains in which there are some residues between monomers which take part in the interaction of monomers and building the structure of heterodimer and also there are other residues interacting inside each monomer for building the structure of that monomer. There are some PDB IDs related to heterodimers in PDB. Now imagine that we are trying to show which residues are physically interacting between two monomers in a heterodimer and then we are trying two show the site of these physically interacting residues in the sequence related to that special structure. For that, we need to find the sequence related to that structure and we can download it from PDB. But if we want to make a multiple sequence alignment of that sequence, first we need to find the refseq sequence of that and then we must blast the sequence and find homologues of that sequence. Here there are some challenges: first, we must find residues interacting in the structure of heterodimer between two monomers. Second, we must find the sequences of monomer chains of the heterodimer and map the interacting residues in the structure to the sequence. Third, we know that when a structure is solved and its monomers are sequenced, may be they are not able to completely sequence the heterodimer and residue numbers are maybe different from the related refseq sequence. I would like to know how can we find the related refseq sequence (or sequences, in the case of heterodimer) and then how can we map the physically interacting residues in the structure of PDB ID to refseq sequence (finding the sites of these interacting residues in that refseq sequence) regarding three challenges mentioned above. Imagine that I have 100 PDB IDs of 100 nonredundant heterodimers and I would like to find and download the structures, sequence and finally refseq sequence. What should I do?&lt;/p&gt;&#xA;" OwnerUserId="818" LastActivityDate="2017-06-12T22:39:40.763" Title="Finding residues physically interacting in a PDB structure and sequence related to that structure" Tags="&lt;protein-structure&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="1" ClosedDate="2017-06-13T12:36:21.717" />
  <row Id="651" PostTypeId="2" ParentId="623" CreationDate="2017-06-11T21:57:03.280" Score="2" Body="&lt;p&gt;&lt;strong&gt;UPD&lt;/strong&gt;: As &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/623/what-methods-should-i-use-from-pythoncyc-api-to-query-metabolites-in-biocyc-data/651?noredirect=1#comment3035_651&quot;&gt;suggested&lt;/a&gt; by Llopis, maybe the correct identifier is VAL?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think there is no metabolite with such name in the database. I tried &lt;a href=&quot;https://biocyc.org/ECOLI/search-query?type=COMPOUND&amp;amp;name=Valine&quot; rel=&quot;nofollow noreferrer&quot;&gt;their search&lt;/a&gt;, the correct name seems to be &lt;code&gt;L-valine&lt;/code&gt;. I expect that they use similar naming for other amino acids. If you cannot find something, you can use their compound search through the website. &lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-07-13T11:02:02.967" LastActivityDate="2017-07-13T11:02:02.967" CommentCount="2" />
  <row Id="653" PostTypeId="1" CreationDate="2017-06-12T06:08:10.227" Score="2" ViewCount="73" Body="&lt;p&gt;I would like to visualize &lt;a href=&quot;http://interactome.dfci.harvard.edu/H_sapiens/index.php?page=shownetwork&quot; rel=&quot;nofollow noreferrer&quot;&gt;this&lt;/a&gt; interaction list.  Is there an online/web based way to do it? Is there a way to analyze the data?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assuming I have exported the list into lines such as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;2  A2M 348 APOE&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="734" LastEditorUserId="734" LastEditDate="2017-06-12T22:25:17.163" LastActivityDate="2017-06-13T10:32:44.133" Title="What are the standard ways to visualize protein-protein or gene-gene interactions" Tags="&lt;gene&gt;&lt;proteins&gt;&lt;interactions&gt;" AnswerCount="3" CommentCount="1" />
  <row Id="654" PostTypeId="1" CreationDate="2017-06-12T07:49:43.730" Score="2" ViewCount="74" Body="&lt;p&gt;&lt;em&gt;This is a problem I have to solve frequently, and I'd be interested in knowing what other methods people use to solve the same problem.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;About twice a year, I get asked to determine variants from Illumina reads, usually from either mouse or human. These are things that have good reference genomes (e.g. &lt;a href=&quot;http://www.ensembl.org/Homo_sapiens/Info/Index&quot; rel=&quot;nofollow noreferrer&quot;&gt;human genome from ensembl&lt;/a&gt;, &lt;a href=&quot;http://www.ensembl.org/Mus_musculus/Info/Index&quot; rel=&quot;nofollow noreferrer&quot;&gt;mouse genome from ensembl&lt;/a&gt;), and are frequently tested out on various bioinformatics tools.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have at my disposal three computers, in order of usage preference: a laptop with 4 processing threads and 8GB of memory, a desktop with 12 processing threads and 64GB of memory, and a server with 24 processing threads and 256GB of memory. Occasionally I get access to better computers when I need to do lots of processing, or to do things quickly, but these comprise my basic bioinformatics team.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've had whole-genome projects that involved processing data from 4 individuals, up to projects with about 120 individuals, with each individual having about 10-40M paired-end reads (100-125bp each). Beyond this level, I suspect other researchers will typically have their own bioinformatics team and/or capabilities, which is why I don't get any of the larger-scale jobs that I hear about from salaried bioinformaticians.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the standard approach that you would use to process these reads into reference-mapped variant data (i.e. gzipped VCF) for use in downstream analysis tools (e.g. &lt;a href=&quot;http://www.ensembl.org/Homo_sapiens/Info/Index&quot; rel=&quot;nofollow noreferrer&quot;&gt;VEP&lt;/a&gt;)?&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-13T10:01:33.920" Title="How do I generate a variant list (i.e. VCF file) using Illumina reads from a human genome?" Tags="&lt;variant-calling&gt;&lt;read-mapping&gt;&lt;human-genome&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="655" PostTypeId="2" ParentId="653" CreationDate="2017-06-12T08:34:12.630" Score="2" Body="&lt;p&gt;since I am not yet allowed to comment, I will have to pots this as an answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess Cytoscape (&lt;a href=&quot;http://cytoscape.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://cytoscape.org/&lt;/a&gt;) would be a nice way to analyse such interaction lists/networks (still depends a bit on what you plan to do).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, for this you will have to preprocess the downloadable excel sheet a bit.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;save as tab-separated file (tsv)&lt;/li&gt;&#xA;&lt;li&gt;run the below python3 script to format into an easily importable format for cytoscape. You can run the script as &lt;code&gt;python3 script.py &amp;lt;path-to-tsv-file&amp;gt; &amp;lt;delimeter for cytoscape&amp;gt;&lt;/code&gt; . The new format is written to std out on the console, therefore maybe you need to write the output to a new file. A sample call could look like &lt;code&gt;python3 testbench.py human_interactome_2017-06-12.tsv.csv &quot;;&quot;&lt;/code&gt;. Make sure to add the delimeter in quotes.&lt;/li&gt;&#xA;&lt;li&gt;you can then import the new file into cytoscape (import network as table) where the first column is your source, the third column your target and the middle column your interaction type.&lt;/li&gt;&#xA;&lt;li&gt;Within cytoscape you can then adapt your edge style to vary the line type in a discrete mapping depending on the value of the interaction.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Screenshot from cytoscape:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/FlSN5.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/FlSN5.png&quot; alt=&quot;Cytoscape screenshot&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import sys&#xA;&#xA;HIIIidx = 4&#xA;HII5idx = 5&#xA;venkatesanidx = 6&#xA;yuidx = 7&#xA;Litidx = 8&#xA;&#xA;print(sys.argv)&#xA;&#xA;if len(sys.argv) &amp;lt;= 1:&#xA;    print(&quot;Usage: script.py file delimeter&quot;)&#xA;&#xA;delim = ';'&#xA;inFile = sys.argv[1]&#xA;&#xA;if len(sys.argv) &amp;gt; 2:&#xA;    delim = sys.argv[2]&#xA;&#xA;&#xA;def printInteraction(syma, symb, inter):&#xA;    print(syma + delim + inter + delim + symb)&#xA;&#xA;&#xA;with open( inFile , 'r') as file:&#xA;&#xA;    file.readline()&#xA;&#xA;    for line in file:&#xA;&#xA;        aline = line.strip('\n').split('\t')&#xA;&#xA;        if aline[HIIIidx] == '1':&#xA;            printInteraction(aline[1], aline[3], 'HIII')&#xA;&#xA;        if aline[HII5idx] == '1':&#xA;            printInteraction(aline[1], aline[3], 'HII5')&#xA;&#xA;        if aline[venkatesanidx] == '1':&#xA;            printInteraction(aline[1], aline[3], 'VEN')&#xA;&#xA;        if aline[yuidx] == '1':&#xA;            printInteraction(aline[1], aline[3], 'YU')&#xA;&#xA;        if aline[Litidx] == '1':&#xA;            printInteraction(aline[1], aline[3], 'LIT')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="690" LastEditorUserId="690" LastEditDate="2017-06-12T12:59:17.563" LastActivityDate="2017-06-12T12:59:17.563" CommentCount="3" />
  <row Id="656" PostTypeId="2" ParentId="654" CreationDate="2017-06-12T08:41:12.713" Score="3" Body="&lt;h2&gt;1. Adapter Trimming&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;One of the first things I do after encountering a set of reads is to remove the adapter sequences from the start and end of reads. Most basecalling software includes some amount of built-in adapter trimming, but it is almost always the case that some adapter sequence will remain. Removing adapters is helpful for mapping because it reduces the effort the mapper needs to go through to make a match (which may help some borderline reads to be mapped properly).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My preferred adapter trimmer is &lt;a href=&quot;http://www.usadellab.org/cms/?page=trimmomatic&quot; rel=&quot;nofollow noreferrer&quot;&gt;Trimmomatic&lt;/a&gt;. It has the ability to search for palindromic adapter read through, and includes an adaptive sliding window option. Trimmomatic is threaded, and seems to work reasonably fast when run through files one at a time in threaded mode. Here's an example script that I might run to trim reads from some samples. This puts trimmed FASTQ files into a &lt;code&gt;trimmed&lt;/code&gt; subdirectory:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;TRIMPATH=/data/all/programs/trimmomatic/Trimmomatic-0.36;&#xA;JARPATH=&quot;${TRIMPATH}/trimmomatic-0.36.jar&quot;;&#xA;ADAPTfile=&quot;${TRIMPATH}/adapters/TruSeq3-PE-2.fa&quot;;&#xA;CMDS=&quot; ILLUMINACLIP:${ADAPTfile}:2:30:10:5:true LEADING:3 TRAILING:3  SLIDINGWINDOW:10:20 MINLEN:40 &quot;;&#xA;&#xA;mkdir -p trimmed;&#xA;&#xA;for INFILE1 in *_1.fastq.gz; do&#xA;  base=$(basename &quot;$INFILE1&quot; _1.fastq.gz);&#xA;  echo ${base};&#xA;  INFILE2=&quot;${base}_2.fastq.gz&quot;;&#xA;  OUTFILE_P1=&quot;trimmed/${base}_P1.fastq.gz&quot;;&#xA;  OUTFILE_P2=&quot;trimmed/${base}_P2.fastq.gz&quot;;&#xA;  OUTFILE_U1=&quot;trimmed/${base}_U1.fastq.gz&quot;;&#xA;  OUTFILE_U2=&quot;trimmed/${base}_U2.fastq.gz&quot;;&#xA;  java -jar &quot;${JARPATH}&quot; PE -threads 20 -phred33 \&#xA;        &quot;${INFILE1}&quot;  &quot;${INFILE2}&quot; \&#xA;        &quot;${OUTFILE_P1}&quot; &quot;${OUTFILE_U1}&quot; &quot;${OUTFILE_P2}&quot;  &quot;${OUTFILE_U2}&quot; \&#xA;        &quot;${CMDS}&quot;;&#xA;done;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Trimmomatic is somewhat sensitive to bad input data, and will loudly fail (i.e. stop running) if it sees quality strings of different length to sequence strings:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Exception in thread &quot;Thread-1&quot; java.lang.RuntimeException: Sequence and quality length don't match: 'TACATGGCCCTGAAATGACTTTCACCCAGGCAACCAGTGCCCCCTGTATAGACACATGCCTTGGGCGCTCCCCACCCTTCCTCGCGTGGCCACACCTCTGT' vs '-AAFFJFJJ7A-FJJJFJFJFJJFFA-FFAF-&amp;lt;A-&amp;lt;FFFJA-&amp;lt;-A7-F&amp;lt;&amp;lt;FFJJAJJJJJJJJJ--&amp;lt;--7-7AJ7&amp;lt;AAJA--J7&amp;lt;ACTGCTGTGGGGCACCCAGCCCCCCAGATAGCCTGGCAGAAGGATGGGGGCACAGACTTCCCAGCTGCACGGGAGAGAC'&#xA;        at org.usadellab.trimmomatic.fastq.FastqRecord.&amp;lt;init&amp;gt;(FastqRecord.java:25)&#xA;        at org.usadellab.trimmomatic.fastq.FastqParser.parseOne(FastqParser.java:89)&#xA;        at org.usadellab.trimmomatic.fastq.FastqParser.next(FastqParser.java:179)&#xA;        at org.usadellab.trimmomatic.threading.ParserWorker.run(ParserWorker.java:42)&#xA;        at java.lang.Thread.run(Thread.java:745)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;2. Read Mapping&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Read mapping finds the most likely location for a read within a target genome.&#xA;There are extremely fast approximate mappers available, but these don't yet work for variant calling, and an exact mapping approach is necessary. My current preferred mapper is &lt;a href=&quot;https://ccb.jhu.edu/software/hisat2/index.shtml&quot; rel=&quot;nofollow noreferrer&quot;&gt;HISAT2&lt;/a&gt;. I use this instead of BWA due to the double-read issue, and because of the local variant-aware mapping. HISAT2 is made by the same computing group as Bowtie2 and Tophat2 (JHUCCB), and is their recommended tool for replacing those other programs.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;HISAT2 is a successor to both HISAT and TopHat2. We recommend that HISAT and TopHat2 users switch to HISAT2.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The HISAT2 page includes a link to a &lt;a href=&quot;ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/data/grch38_snp.tar.gz&quot; rel=&quot;nofollow noreferrer&quot;&gt;genomic index file&lt;/a&gt; for the human genome, including SNP variants, which I use when mapping human reads. HISAT2 is also threaded, so I run it on the files one at a time and pipe through samtools to create a sorted BAM file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mkdir mapped&#xA;for r1 in trimmed/*_P1.fastq.gz;&#xA;  do base=$(basename &quot;${x}&quot; _P1.fastq.gz);&#xA;  r2=&quot;trimmed/${base}_P2.fastq.gz&quot;;&#xA;  echo ${base};&#xA;  hisat2 -p 20 -t -x /data/all/genomes/hsap/hisat_grch38_snp/genome_snp -1 \&#xA;    &quot;${r1}&quot; -2 &quot;${r2}&quot; 2&amp;gt;&quot;mapped/hisat2_${y}_vs_grch38_stderr.txt&quot; | \&#xA;    samtools sort &amp;gt; &quot;mapped/hisat2_${y}_vs_grch38.bam&quot;;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;3. Variant Calling&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I currently use samtools/bcftools to do this, but would be interested in other opinions as we recently had a grant reviewer response that samtools was a dinosaur program and better approaches were available. Samtools doesn't currently work with threads for variant calling, so I save the commands to a text file and then run them through &lt;a href=&quot;https://www.gnu.org/software/parallel/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GNU Parallel&lt;/a&gt;. This requires the genome files to be first downloaded and indexed via &lt;code&gt;samtools faidx&lt;/code&gt;. This produces a set of gzipped VCF files in the &lt;code&gt;variants&lt;/code&gt; directory:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mkdir -p variants;&#xA;(for x in mapped/*.bam;&#xA;   do echo samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa &quot;${x}&quot; \| \&#xA;   bcftools call --ploidy GRCh38 -v -m -O z -o variants/$(basename &quot;${x}&quot; .bam).vcf.gz; done) &amp;gt; call_jobs.txt&#xA;cat call_jobs.txt | parallel -u;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As a slow, but more accurate alternative, variants can be called for all samples at the same time:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools mpileup -v -f /data/all/genomes/hsap/hisat_grch38_snp/Homo_sapiens.GRCh38.dna.primary_assembly.fa mapped/*.bam | \&#xA;  bcftools call --ploidy GRCh38 -v -m -O z -o variants/hisat2_allCalled_vs_grch38.vcf.gz&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If a faster processing is desired, then the variant calling can be parallelised across chromosomes using &lt;code&gt;parallel&lt;/code&gt; and merged afterwards using &lt;code&gt;bcftools norm&lt;/code&gt;. The implementation of this is left as an exercise to the reader.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-13T10:01:33.920" LastActivityDate="2017-06-13T10:01:33.920" CommentCount="4" />
  <row Id="657" PostTypeId="2" ParentId="653" CreationDate="2017-06-12T09:26:50.953" Score="1" Body="&lt;p&gt;Usually interactions are represented in a graph, with edges representing the interactions (colors, thickness or values on the edges represent different properties of those edges).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I know, there aren't on line programs to represent this kind of information. For an off line representation, you can use the &lt;a href=&quot;https://cran.r-project.org/package=igraph&quot; rel=&quot;nofollow noreferrer&quot;&gt;igraph&lt;/a&gt; package in R, or &lt;a href=&quot;http://www.cytoscape.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cytoscape&lt;/a&gt; program. &lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-06-12T09:26:50.953" CommentCount="0" />
  <row Id="658" PostTypeId="1" AcceptedAnswerId="718" CreationDate="2017-06-12T11:51:35.007" Score="8" ViewCount="540" Body="&lt;p&gt;I'm looking for cloud computing services that can be used for doing bioinformatics. An example I found is &lt;a href=&quot;https://insidedna.me&quot; rel=&quot;noreferrer&quot;&gt;InsideDNA&lt;/a&gt; and there is &lt;a href=&quot;https://aws.amazon.com/health/genomics/&quot; rel=&quot;noreferrer&quot;&gt;Amazon&lt;/a&gt; of course.&#xA;A little description of these would be appreciated.&lt;/p&gt;&#xA;" OwnerUserId="208" LastActivityDate="2017-08-15T17:50:38.403" Title="What are the available cloud computing services for bioinformatics?" Tags="&lt;linux&gt;" AnswerCount="7" CommentCount="9" FavoriteCount="1" />
  <row Id="659" PostTypeId="2" ParentId="658" CreationDate="2017-06-12T12:51:03.997" Score="8" Body="&lt;p&gt;I have trialed the free version of InsideDNA, and these were my notes:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cost: \$225/month for a team of 5 with 50TB storage or \$45/month with 10TB storage for individuals (assuming 6 month package: &lt;a href=&quot;https://insidedna.me/pricing&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://insidedna.me/pricing&lt;/a&gt;).&lt;/li&gt;&#xA;&lt;li&gt;Software installed: Around 600 bioinformatic tools available and standard command line tools; some popular tools missing (like &lt;a href=&quot;http://weizhongli-lab.org/cd-hit/&quot; rel=&quot;nofollow noreferrer&quot;&gt;CD-HIT&lt;/a&gt;), but should be possible to install on request.&lt;/li&gt;&#xA;&lt;li&gt;Jobs: Maximum of 32 CPUs and 208 RAM per job submission. Test jobs generally worked, although a larger test job failed.&lt;/li&gt;&#xA;&lt;li&gt;Other points: Command line was sometimes slow, &lt;code&gt;wget&lt;/code&gt; queries were slow, and &lt;code&gt;scp&lt;/code&gt; was blocked. However, these may be resolvable issues.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Overall, I felt InsideDNA could be useful for groups without their own computational infrastructure and could be used for easily sharing resources between groups. The packages on offer seem not expensive, but I had a few issues, and I don't know how good their sys admin support would be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have not used the Amazon service, so can't comment beyond the details on their website. Also there are a few alternative companies, such as &lt;a href=&quot;https://genestack.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Genestack&lt;/a&gt; and &lt;a href=&quot;https://www.dnanexus.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;DNAnexus&lt;/a&gt;, but I haven't directly tested them either.&lt;/p&gt;&#xA;" OwnerUserId="104" LastEditorUserId="240" LastEditDate="2017-08-03T06:09:39.177" LastActivityDate="2017-08-03T06:09:39.177" CommentCount="0" />
  <row Id="660" PostTypeId="2" ParentId="658" CreationDate="2017-06-12T13:56:06.117" Score="3" Body="&lt;p&gt;Depending on your applications and uses, you might be interested in checking out &lt;a href=&quot;http://www.cyverse.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;CyVerse&lt;/a&gt;. It is an NSF funded initiative that provides you with data storage, high performance computing resources, and easy access to commonly used tools. As far as I know, it is free to use once you have an account. I also usually encounter it being used with plant and microbial genomics, so not sure how it will work with something like human genomics projects. But might be worth checking out at least. :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More information: &lt;a href=&quot;http://www.cyverse.org/about&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.cyverse.org/about&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="824" LastActivityDate="2017-06-12T13:56:06.117" CommentCount="0" />
  <row Id="661" PostTypeId="1" AcceptedAnswerId="665" CreationDate="2017-06-12T15:30:36.277" Score="3" ViewCount="53" Body="&lt;p&gt;I was wondering if I could use the gff parsing capability of bioawk to facilitate the parsing of gtf files, and I looked at the following help message:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ bioawk -c help&#xA;bed:&#xA;    1:chrom 2:start 3:end 4:name 5:score 6:strand 7:thickstart 8:thickend 9:rgb 10:blockcount 11:blocksizes 12:blockstarts &#xA;sam:&#xA;    1:qname 2:flag 3:rname 4:pos 5:mapq 6:cigar 7:rnext 8:pnext 9:tlen 10:seq 11:qual &#xA;vcf:&#xA;    1:chrom 2:pos 3:id 4:ref 5:alt 6:qual 7:filter 8:info &#xA;gff:&#xA;    1:seqname 2:source 3:feature 4:start 5:end 6:score 7:filter 8:strand 9:group 10:attribute &#xA;fastx:&#xA;    1:name 2:seq 3:qual 4:comment &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I see that there are 10 fields defined for gff parsing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, when I look at the pages for gtf and gff in the ensembl website (&lt;a href=&quot;http://www.ensembl.org/info/website/upload/gff.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;gff/gtf&lt;/a&gt; and &lt;a href=&quot;http://www.ensembl.org/info/website/upload/gff3.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;gff3&lt;/a&gt;), all have 9 fields.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm curious about these &quot;filter&quot; and &quot;group&quot; fields. What are they meant for? Are they extracted from some of the columns mentioned in the above pages?&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-06-13T07:02:39.247" Title="What kind of &quot;gff&quot; format does bioawk parse?" Tags="&lt;file-formats&gt;&lt;gff3&gt;&lt;gtf&gt;&lt;bioawk&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="662" PostTypeId="2" ParentId="594" CreationDate="2017-06-12T16:02:21.920" Score="1" Body="&lt;p&gt;Check out &lt;a href=&quot;http://homer.ucsd.edu/homer/&quot; rel=&quot;nofollow noreferrer&quot;&gt;HOMER&lt;/a&gt;. &quot;Software for motif discovery and next generation sequencing analysis&quot;, it's what my lab uses currently for finding eRNA motifs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: For @ShanZhengYang &quot;HOMER was designed as a de novo motif discovery algorithm...&quot; &#xA;&lt;a href=&quot;http://homer.ucsd.edu/homer/introduction/basics.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;HOMER De Novo Motif&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="33" LastEditorUserId="33" LastEditDate="2017-06-12T16:48:00.760" LastActivityDate="2017-06-12T16:48:00.760" CommentCount="5" />
  <row Id="663" PostTypeId="1" AcceptedAnswerId="666" CreationDate="2017-06-12T17:06:26.053" Score="5" ViewCount="116" Body="&lt;p&gt;I have a set of differentially methylated/expressed/whatever entities with p-values attached (example below).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;entity_name    p-value    magnitude&#xA;entity1        0.04459    0.68&#xA;entity2        0.02283    0.99&#xA;...&#xA;entity_n       0.78       0.025&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Typically, I apply the p.adjust function in R with the &quot;fdr&quot; (Benjamini-Hochberg) approach to leave me with p-values adjusted to control the FDR.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;adjusted &amp;lt;- p.adjust(mydata,&quot;fdr&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I am interested in showing a volcano plot with the unadjusted p-values, and two alpha levels: 0.05 and one that corresponds to the correction. What is the best way to get this alpha? Is it appropriate to set the &quot;corrected alpha&quot; to the lowest original p-value that doesn't pass FDR correction?&lt;/p&gt;&#xA;" OwnerUserId="643" LastEditorUserId="57" LastEditDate="2017-06-12T22:05:16.573" LastActivityDate="2017-06-13T11:14:15.507" Title="How to correct alpha, and not p-values themselves, for visualization purposes" Tags="&lt;r&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="664" PostTypeId="1" CreationDate="2017-06-12T17:45:52.033" Score="1" ViewCount="78" Body="&lt;p&gt;From an RNA-seq experiment I have about 17000 gene ids for 2 sample conditions arranged according to their log2 fold changes when compared to a control.  I need to annotate these, but I've never done annotation before and am wondering how to do this in R?  There seems to be multiple packages available, and I'm wondering if any of them stand out as being the best?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm primarily interested in human samples and annotated pathways.&lt;/p&gt;&#xA;" OwnerUserId="506" LastEditorUserId="77" LastEditDate="2017-06-12T19:01:53.170" LastActivityDate="2017-06-13T15:23:02.040" Title="How to perform functional analysis on a gene list in R?" Tags="&lt;r&gt;&lt;annotation&gt;" AnswerCount="2" CommentCount="7" FavoriteCount="1" />
  <row Id="665" PostTypeId="2" ParentId="661" CreationDate="2017-06-12T18:00:21.667" Score="4" Body="&lt;p&gt;I would consider the description there a bug. The &lt;code&gt;filter&lt;/code&gt; is actually the strand, &lt;code&gt;strand&lt;/code&gt; is the frame, &lt;code&gt;group&lt;/code&gt; is the attribute, and &lt;code&gt;attribute&lt;/code&gt; does nothing. These are really meant to be the 9 columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: There's a &lt;a href=&quot;https://github.com/lh3/bioawk/issues/16&quot; rel=&quot;nofollow noreferrer&quot;&gt;bug report&lt;/a&gt; related to this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit 2&lt;/strong&gt;: I've made &lt;a href=&quot;https://github.com/lh3/bioawk/pull/21&quot; rel=&quot;nofollow noreferrer&quot;&gt;a pull request&lt;/a&gt; to clarify this and fix the aforementioned bug report.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit 3&lt;/strong&gt;: I realized that I never directly answered the title of your question (&lt;em&gt;mea culpa&lt;/em&gt;). bioawk itself will work with gff, gff3, or gtf files. It really is just treating them as tab-separated files with named columns (this is surprisingly convenient, since it's a PITA to remember what column does what).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit 4&lt;/strong&gt;: The PR has been merged. If you install from github then you'll see corrected field names.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-13T07:02:39.247" LastActivityDate="2017-06-13T07:02:39.247" CommentCount="0" />
  <row Id="666" PostTypeId="2" ParentId="663" CreationDate="2017-06-12T18:22:42.213" Score="6" Body="&lt;p&gt;The only way to get the alpha levels is to determine what they will be with &lt;code&gt;p.adjust()&lt;/code&gt;, since they will depend on the distribution of your unadjusted p values. The general steps you should follow will be:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Add a column of adjusted p-values to your dataframe (&lt;code&gt;mydata$padj = p.adjust(mydata, method=&quot;BH&quot;)&lt;/code&gt;, which is the same as FDR and saves a character).&lt;/li&gt;&#xA;&lt;li&gt;Use &lt;code&gt;which&lt;/code&gt; and &lt;code&gt;max&lt;/code&gt; to determine your two alpha threshold (e.g., &lt;code&gt;max(mydata$pvalue[mydata$padj &amp;lt; 0.05])&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Then you can adjust your plots however you like (presumably with some horizontal lines at the various alphas). Whether you take the smallest non-significant value or the largest significant value is up to you, just describe what &quot;dots on the line&quot; represent.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-13T11:14:15.507" LastActivityDate="2017-06-13T11:14:15.507" CommentCount="3" />
  <row Id="667" PostTypeId="2" ParentId="658" CreationDate="2017-06-12T20:17:58.167" Score="1" Body="&lt;h3&gt;Google Genomics&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Google has an API called &lt;a href=&quot;https://cloud.google.com/genomics&quot; rel=&quot;nofollow noreferrer&quot;&gt;Google Genomics&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;SNPedia&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&quot;SNPedia is a wiki investigating human genetics.&quot;&#xA;snpedia.com&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Promethease&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&quot;Promethease is a literature retrieval system that builds a personal DNA report&quot;&#xA;promethease.com&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;DNA Land&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&quot;Compare DNA with reference data from different populations&quot;&#xA;dna.land&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;The CyDAS Project&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;And, there's the CyDAS project which has an API that can analyze ISCN formulae. Per their &lt;a href=&quot;http://www.cydas.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;web site&lt;/a&gt;: their API &quot;lets you analyze a Karyotype for virtually all information which can be extracted from karyotypes and the rearrangements therein: gains and losses of chromosomal material, break points, junctions...&quot; It's a &lt;strong&gt;free&lt;/strong&gt; service, but I don't know how up to date it is.&lt;/p&gt;&#xA;" OwnerUserId="831" LastEditorUserId="831" LastEditDate="2017-06-13T19:07:49.597" LastActivityDate="2017-06-13T19:07:49.597" CommentCount="0" />
  <row Id="668" PostTypeId="2" ParentId="663" CreationDate="2017-06-12T20:51:34.303" Score="2" Body="&lt;p&gt;When you're doing a p-value adjustment, the same unadjusted p-value in different genes can be given different adjusted p-values depending on other factors. That means that you can't directly draw a line associated with the FDR on a plot of unadjusted p-value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One possibility would be to take a range of values that are close to the FDR threshold (e.g. the 20 values closest to threshold), and draw a p-value greyzone within that region:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/Rscript&#xA;values &amp;lt;- c(rnorm(10000),rnorm(100, mean=1.5));&#xA;val.mean &amp;lt;- median(values);&#xA;val.diffs &amp;lt;- abs(values - median(values));&#xA;val.reldiffs &amp;lt;- (values - median(values));&#xA;&#xA;val.pval &amp;lt;- pnorm(val.diffs, mean = mean(val.diffs),&#xA;                   sd=sd(val.diffs), lower.tail=FALSE);&#xA;val.padj &amp;lt;- p.adjust(val.pvals, method=&quot;BH&quot;);&#xA;fdr.threshold &amp;lt;- 0.1;&#xA;close.bh &amp;lt;- order(abs(val.padj - fdr.threshold))[1:20];&#xA;&#xA;png(&quot;SE.663.png&quot;);&#xA;plot(val.reldiffs, -log10(val.pval),&#xA;     col=ifelse(1:10100 &amp;lt;= 10000,&quot;darkblue&quot;,&quot;darkgreen&quot;));&#xA;abline(h=-log10(0.05), col=&quot;red&quot;);&#xA;text(0,-log10(0.05),&quot;p=0.05&quot;, pos=1);&#xA;abline(h=range(-log10(val.pval[close.bh])), col=&quot;#00000040&quot;, lty=&quot;dashed&quot;);&#xA;rect(xleft=min(val.reldiffs)*2, xright=max(val.reldiffs)*2,&#xA;     ytop=max(-log10(val.pval[close.bh])),&#xA;     ybottom=min(-log10(val.pval[close.bh])), col=&quot;#00000020&quot;, border=NA);&#xA;text(0,min(-log10(val.pval[close.bh])),&quot;FDR=0.1&quot;, pos=1);&#xA;invisible(dev.off());&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/l8Ldk.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/l8Ldk.png&quot; alt=&quot;Volcano plot of p-values with FDR greyzone&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-13T10:21:37.260" LastActivityDate="2017-06-13T10:21:37.260" CommentCount="4" />
  <row Id="669" PostTypeId="2" ParentId="664" CreationDate="2017-06-12T21:52:41.197" Score="2" Body="&lt;p&gt;N.B., I'm avoiding discussion of &quot;best&quot;, since that's more or less impossible to answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your question can actually be divided into two:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What's a good tool for pathways analysis (ideally in R)?&lt;/li&gt;&#xA;&lt;li&gt;What are good sources of pathway information?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For (1), there are a few different possibilities, but I prefer either roast or camera from limma or goseq (a stand-alone bioconductor package). Of those, I expect what roast or goseq are doing are the most similar to what panther is doing in the sense that they're not competitive. However, since genes do correlate with each other, I prefer competitive tests like that offered by camera.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For (2), your options are basically &lt;a href=&quot;http://bioconductor.org/packages/release/data/annotation/html/KEGG.db.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;KEGG&lt;/a&gt; or &lt;a href=&quot;http://bioconductor.org/packages/release/data/annotation/html/KEGG.db.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;reactome&lt;/a&gt;. KEGG is problematic, since you need a license for anything remotely recent (there are some R packages to get around this, but I haven't a clue how kosher they are legally). Given that, reactome might be your best bet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having written all of that, the best database out there is what &lt;a href=&quot;https://www.qiagenbioinformatics.com/products/ingenuity-pathway-analysis/&quot; rel=&quot;nofollow noreferrer&quot;&gt;IPA&lt;/a&gt; uses. This is a commercial product (I have no affiliation), but you can get a demo license that will work for a couple weeks at least. If you're doing analyses relatively often then it makes sense to spend a bit of cash on a license (maybe share with neighboring groups).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regardless of what you use, you might also want to use &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/pathview.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;pathview&lt;/a&gt;, which can produce easily interpretable plots related to pathway enrichment.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-12T21:52:41.197" CommentCount="0" />
  <row Id="670" PostTypeId="2" ParentId="650" CreationDate="2017-06-12T22:39:40.763" Score="2" Body="&lt;p&gt;My reputation isn't high enough to make a comment, but have you considered a modelling software such as PyMol or Chimera? You can execute a command line script with softwares such as these and these have the functionality to predict H-bonds. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the PDB files there is information regarding the GenBank and UNIPROT info of the protein, and you can find more information here: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.wwpdb.org/documentation/file-format-content/format33/sect3.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.wwpdb.org/documentation/file-format-content/format33/sect3.html&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want RefSeq ID, you can use GenBank info you extract and convert by piping into a biomaRt (Bioconducor) script, although maybe there is a more direct way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for finding which residues in the sequence are interacting, I'm fairly certain that the software packages like PyMol or Chimera will give you information about the interacting residues. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I understand, you will want to filter out only the residues that interact and are on different chains (e.g, Chain A vs. Chain B). This process will require you to work with the syntax of the different software packages when you are scripting. For example, Chimera has a basic primer here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.cgl.ucsf.edu/chimera/current/docs/ProgrammersGuide/basicPrimer.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.cgl.ucsf.edu/chimera/current/docs/ProgrammersGuide/basicPrimer.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that I only gave a reference to Chimera because I am more comfortable with that then other packages and there is likely a more efficient way to do this. With Chimera, you can find the position within the chain of residues participating in the interaction and then you'd store these in a file to do whatever you want later.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope this helps.&lt;/p&gt;&#xA;" OwnerUserId="823" LastActivityDate="2017-06-12T22:39:40.763" CommentCount="1" />
  <row Id="671" PostTypeId="2" ParentId="595" CreationDate="2017-06-13T00:28:05.247" Score="2" Body="&lt;p&gt;Late to the party here, but I like to try to avoid writing scripts when some command line magic will do. It's good practice to index your &lt;code&gt;FASTA&lt;/code&gt; so use it.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools faidx &amp;lt;myfasta.fa&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A little bit of &lt;code&gt;awk&lt;/code&gt; and &lt;code&gt;sort&lt;/code&gt; can determine the largest isoform of each gene (and they don't even have to be sorted by name in the source &lt;code&gt;FASTA&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;awk -F'[\t.]' '{print $1,$2,$3,$4}' &amp;lt;myfasta.fa&amp;gt;.fai | sort -k4nr,4 | sort -uk1,2 | cut -f1-3 -d' '| tr ' ' '.' &amp;gt; selection.ls&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The &lt;code&gt;-F[\t.]&lt;/code&gt; specifies both the &lt;code&gt;fai&lt;/code&gt; tab delimiters, and the full stops in your contig names (&lt;code&gt;Doug_NoIndex_L005_R1_001_contig_2.g7.t1&lt;/code&gt;) as delimiters for &lt;code&gt;awk&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;For each line we print four columns. &lt;code&gt;$1&lt;/code&gt; to &lt;code&gt;$3&lt;/code&gt; are the three components of your contig name (after splitting on the &lt;code&gt;.&lt;/code&gt;), and column $4 is the column in the index for sequence length.&lt;/li&gt;&#xA;&lt;li&gt;We use sort to organise the lines by &lt;code&gt;$4&lt;/code&gt; (the contig length). Reverse sort with &lt;code&gt;-r&lt;/code&gt; so the longest genes appear first.&lt;/li&gt;&#xA;&lt;li&gt;We then subsort by columns &lt;code&gt;$1&lt;/code&gt; and &lt;code&gt;$2&lt;/code&gt; (the contig and gene name (&lt;code&gt;g1&lt;/code&gt;...)). &lt;code&gt;-u&lt;/code&gt; then drops any repeat of column &lt;code&gt;$1&lt;/code&gt; and &lt;code&gt;$2&lt;/code&gt; pairs (that is, all lines after the first pair -- the longest length).&lt;/li&gt;&#xA;&lt;li&gt;Finally, we reassemble &lt;code&gt;$1&lt;/code&gt; to &lt;code&gt;$3&lt;/code&gt; with &lt;code&gt;cut&lt;/code&gt; and &lt;code&gt;tr&lt;/code&gt; to give you a list of sequence names.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;A loop with our old friend &lt;code&gt;samtools faidx&lt;/code&gt; can put your chosen sequences out to a new &lt;code&gt;FASTA&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;while read contig;&#xA;    do samtools faidx &amp;lt;myfasta.fa&amp;gt; $contig &amp;gt;&amp;gt; selection.fa;&#xA;done &amp;lt; selection.ls&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Don't forget to index it, they are useful!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools faidx selection.fa&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="215" LastEditorUserId="215" LastEditDate="2017-06-13T00:51:03.047" LastActivityDate="2017-06-13T00:51:03.047" CommentCount="1" />
  <row Id="672" PostTypeId="1" AcceptedAnswerId="681" CreationDate="2017-06-13T01:33:49.053" Score="3" ViewCount="61" Body="&lt;p&gt;My data is a VCF file generated from an exome sequencing variant call pipeline. I'm not very familiar with the sequencing and variant calling process. I noticed that there are some missing genotypes, which are recorded as &quot;./.&quot; at the GT field. From googling I learned that they're not homozygous reference genotype (&quot;0/0&quot;), but missing calls due to some sequencing failure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to make my data &quot;cleaner&quot;, I thought it would be better to filter out the loci with missing calls if there're not too many of them. I also checked the corresponding &quot;DP&quot; of the loci with and without missing calls. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;chr1:123 GT:DP 0/0:2 1/1:4 ./.&#xA;chr1:234 GT:DP 0/0:10 1/1:11 1/1:20&#xA;chr1:345 GT:DP 0/1:40 1/1:37 0/0:78&#xA;chr1:456 GT:DP 0/1:7 0/0:23 ./.&#xA;chr1:567 GT:DP 0/1:34 1/1:39 0/0:58&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In the above toy example, there are 3 people and 5 loci. I checked the mean DP of all 5 loci and I found that the mean DP of loci with missing calls (1st and 4th loci) to be significantly lower than the mean DP of loci without missing calls (2nd, 3rd and 5th loci). This happens to my real data with a lot more loci and samples than this toy example. Is that a coincidence? Or are there any specific reason that the loci with missing calls have lower coverage than the &quot;normal&quot; loci? Thanks!&lt;/p&gt;&#xA;" OwnerUserId="832" LastEditorUserId="73" LastEditDate="2017-06-13T02:34:51.760" LastActivityDate="2017-06-13T19:21:01.190" Title="Why are there missing calls in a VCF file from exome sequencing?" Tags="&lt;variant-calling&gt;&lt;vcf&gt;&lt;sequencing&gt;&lt;exome&gt;&lt;reads&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="0" />
  <row Id="673" PostTypeId="1" CreationDate="2017-06-13T01:38:38.563" Score="5" ViewCount="85" Body="&lt;p&gt;I have a list of transcription factors and I am interested in finding out which which genes might be transcribed as a result of the formation of transcription factor complexes with that transcription factor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas on good databases? I've seen that ENCODE hosts data from CHIPseq experiments, but I'm not sure if I can find conclusions regarding interactions from this site. I'm looking for a database with known or putative transcription/gene interactions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;" OwnerUserId="823" LastEditorUserId="823" LastEditDate="2017-06-13T23:23:39.460" LastActivityDate="2017-06-14T09:50:01.067" Title="Given a transcription factor, what genes does it regulate?" Tags="&lt;chip-seq&gt;&lt;encode&gt;&lt;multi-omics&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="1" />
  <row Id="674" PostTypeId="2" ParentId="673" CreationDate="2017-06-13T01:53:41.920" Score="3" Body="&lt;p&gt;&lt;a href=&quot;http://software.broadinstitute.org/gsea/msigdb/collections.jsp#C3&quot; rel=&quot;nofollow noreferrer&quot;&gt;MSigDB&lt;/a&gt; has a collection (C3:TFT) of gene sets corresponding to transcription factor targets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://amp.pharm.mssm.edu/Harmonizome/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Harmonizome&lt;/a&gt; has functional terms for genes extracted from over a hundred publicly available resources.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-06-13T01:53:41.920" CommentCount="0" />
  <row Id="675" PostTypeId="2" ParentId="672" CreationDate="2017-06-13T02:49:37.973" Score="2" Body="&lt;p&gt;I've just been generating data like this, so can tell you about why/how missing calls are created in my dataset. There are two main reasons:&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;1. Sequencing failure&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;When reads don't map across the variant region, then it's impossible to accurately determine a genotype for that region. This will commonly happen just outside the borders of the selected regions for exome sequencing, but can also happen through natural random sampling of the genome, or through repeated sequences coupled with systematic error in either the assembled genome or the sequencer. In this case, it would be expected that called variant regions including missing data would be lower coverage. It may be helpful to plot the coverage within a particular region to work out if this is likely to be happening.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;2. Merged non-variant data&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Variant calling can be processed in two different modes, one which takes into account &lt;em&gt;all&lt;/em&gt; samples when doing calling, and another which does the calling one sample at a time, followed by subsequent merging of the results. Multi-sample variant calling is more accurate, but computational limits (or experimental design) might mean that it is more appropriate for samples to be called one-by-one. It's fairly common (for the purpose of preserving space and computational power) for calling algorithms to only report variant information so if a single sample is identical to the reference, and that sample is the only one being called, then the variant and coverage information for that sample will be lost. When samples are merged after being separately called, the declared call for the &quot;same as reference&quot; samples will be set to missing. In this case, it would be generally expected that the other called samples would have high-coverage within the variant region.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-13T19:21:01.190" LastActivityDate="2017-06-13T19:21:01.190" CommentCount="1" />
  <row Id="676" PostTypeId="1" CreationDate="2017-06-13T02:51:01.843" Score="1" ViewCount="211" Body="&lt;p&gt;Are there any advantages to learning Biopython instead of learning Bioperl?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ideally, we would learn both, but someone starting out in bioinformatics may have to choose what to learn first depending on the kind of problems actually encountered.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Are there problems for which Biopython is better than Bioperl (or vice-versa)?&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="823" LastEditorUserId="37" LastEditDate="2017-06-25T02:02:35.567" LastActivityDate="2017-06-25T02:02:35.567" Title="For what bioinformatics tasks is Biopython more adapted than Bioperl?" Tags="&lt;biopython&gt;&lt;perl&gt;" AnswerCount="4" CommentCount="3" />
  <row Id="677" PostTypeId="1" CreationDate="2017-06-13T03:03:52.503" Score="4" ViewCount="29" Body="&lt;p&gt;For my undergraduate research I'm looking for a database that gives the bound form of a particular protein structure. Is there any database that provide us with such data? So far I've found following proteins related to my work &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ascorbate peroxidase (APX)&lt;/li&gt;&#xA;&lt;li&gt;Beta-glucosidase&lt;/li&gt;&#xA;&lt;li&gt;Calcineurin b-like protein-interacting protein kinase.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've searched in PDB and UNIPROT. &lt;/p&gt;&#xA;" OwnerUserId="833" LastEditorUserId="175" LastEditDate="2017-06-13T04:08:37.613" LastActivityDate="2017-06-21T13:36:08.693" Title="how to find the bound form of an enzyme structure?" Tags="&lt;database&gt;&lt;protein-structure&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="678" PostTypeId="1" CreationDate="2017-06-13T03:48:15.283" Score="3" ViewCount="60" Body="&lt;p&gt;I have a BAM file, and I have a read ID. What is the simplest way to get mapping statistics of that read in human-readable format?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g. I might want: % identity of aligned bases; number of insertions and deletions; number of bases aligned to the reference; etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Extra points for one-liners that give detailed summary statistics, and extra-extra points if you can extend the answer to output summaries for all reads longer than some predetermined length.&lt;/p&gt;&#xA;" OwnerUserId="156" LastActivityDate="2017-06-13T14:01:53.477" Title="Get the mapping statistics of a single read from a BAM file" Tags="&lt;bam&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="679" PostTypeId="2" ParentId="677" CreationDate="2017-06-13T04:45:30.723" Score="1" Body="&lt;p&gt;Are you aware of &lt;a href=&quot;http://www.brenda-enzymes.org/index.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;BRENDA&lt;/a&gt;? I was just introduced to it today for a completely separate reason (looking at carbohydrate enzyme families in the &lt;em&gt;Nippostrongylus brasiliensis&lt;/em&gt; proteome), and it seems to be a fairly comprehensive database. There is at least a literature link there for the &lt;a href=&quot;http://www.brenda-enzymes.org/enzyme.php?ecno=1.11.1.11#Crystallization/COMMENTARY&quot; rel=&quot;nofollow noreferrer&quot;&gt;ascorbate-complexed crystalisation of Ascorbate peroxidase&lt;/a&gt; on that site. Following through the paper, I see that &lt;a href=&quot;http://www.rcsb.org/pdb/explore/explore.do?pdbId=1OAF&quot; rel=&quot;nofollow noreferrer&quot;&gt;the complex has been uploaded to PDB&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-13T04:45:30.723" CommentCount="0" />
  <row Id="680" PostTypeId="2" ParentId="678" CreationDate="2017-06-13T05:32:15.000" Score="3" Body="&lt;p&gt;&lt;code&gt;samtools stats&lt;/code&gt; seems to be able to do most of this, excluding the CIGAR-string parsing stuff (i.e. INDELs):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ samtools view -h mapped.bam | grep -e '^@' -e 'readName' |&#xA;    samtools stats | grep '^SN' | cut -f 2-&#xA;&#xA;raw total sequences:    2&#xA;filtered sequences: 0&#xA;sequences:  2&#xA;is sorted:  1&#xA;1st fragments:  2&#xA;last fragments: 0&#xA;reads mapped:   2&#xA;reads mapped and paired:    0   # paired-end technology bit set + both mates mapped&#xA;reads unmapped: 0&#xA;reads properly paired:  0   # proper-pair bit set&#xA;reads paired:   0   # paired-end technology bit set&#xA;reads duplicated:   0   # PCR or optical duplicate bit set&#xA;reads MQ0:  0   # mapped and MQ=0&#xA;reads QC failed:    0&#xA;non-primary alignments: 0&#xA;total length:   13632   # ignores clipping&#xA;bases mapped:   13632   # ignores clipping&#xA;bases mapped (cigar):   13502   # more accurate&#xA;bases trimmed:  0&#xA;bases duplicated:   0&#xA;mismatches: 3670    # from NM fields&#xA;error rate: 2.718116e-01    # mismatches / bases mapped (cigar)&#xA;average length: 6816&#xA;maximum length: 6816&#xA;average quality:    8.9&#xA;insert size average:    0.0&#xA;insert size standard deviation: 0.0&#xA;inward oriented pairs:  0&#xA;outward oriented pairs: 0&#xA;pairs with other orientation:   0&#xA;pairs on different chromosomes: 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For a length filter, an awk length check can be slotted in instead of the grep:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ samtools view -h mapped.bam |&#xA;    awk -F'\t' '{if((/^@/) || (length($10)&amp;gt;500)){print $0}}' |&#xA;    samtools stats | grep '^SN' | cut -f 2-&#xA;&#xA;raw total sequences:    131&#xA;filtered sequences: 0&#xA;sequences:  131&#xA;is sorted:  1&#xA;1st fragments:  131&#xA;last fragments: 0&#xA;reads mapped:   131&#xA;reads mapped and paired:    0   # paired-end technology bit set + both mates mapped&#xA;reads unmapped: 0&#xA;reads properly paired:  0   # proper-pair bit set&#xA;reads paired:   0   # paired-end technology bit set&#xA;reads duplicated:   0   # PCR or optical duplicate bit set&#xA;reads MQ0:  0   # mapped and MQ=0&#xA;reads QC failed:    0&#xA;non-primary alignments: 0&#xA;total length:   569425  # ignores clipping&#xA;bases mapped:   569425  # ignores clipping&#xA;bases mapped (cigar):   453884  # more accurate&#xA;bases trimmed:  0&#xA;bases duplicated:   0&#xA;mismatches: 113755  # from NM fields&#xA;error rate: 2.506257e-01    # mismatches / bases mapped (cigar)&#xA;average length: 4346&#xA;maximum length: 16909&#xA;average quality:    9.1&#xA;insert size average:    0.0&#xA;insert size standard deviation: 0.0&#xA;inward oriented pairs:  0&#xA;outward oriented pairs: 0&#xA;pairs with other orientation:   0&#xA;pairs on different chromosomes: 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-13T12:16:07.970" LastActivityDate="2017-06-13T12:16:07.970" CommentCount="4" />
  <row Id="681" PostTypeId="2" ParentId="672" CreationDate="2017-06-13T08:01:30.593" Score="3" Body="&lt;p&gt;Missing variant calls due to lack of coverage shouldn't happen in the targeted capture region and I'd think most of these would come from off-target regions where some samples had reads mapped. I'd filter out the VCF to only include on-target loci before proceeding with further analyses.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you did your variant calling on samples separately and then merged them together, you'll have missing calls at loci called variant in one sample and homozygous reference in the other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can get over this by asking the variant caller to emit all sites within the target region, which would lead to a large VCF or do multi-sample calling, which is more computationally intensive. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One other way is to do variant calling on all samples separately, generating a BED file of all variant regions in the cohort and then asking the variant caller to genotype those loci across all samples before merging them together.&lt;/p&gt;&#xA;" OwnerUserId="601" LastActivityDate="2017-06-13T08:01:30.593" CommentCount="0" />
  <row Id="682" PostTypeId="2" ParentId="676" CreationDate="2017-06-13T08:12:58.063" Score="4" Body="&lt;p&gt;Regading the perl vs python discussion, there is no final answer which language is better, but I have some advice for you:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Learn the language your colleagues or your advisor use&lt;/strong&gt;. This way you are able to discuss your code with them and also get help if you run into problems.&lt;/p&gt;&#xA;" OwnerUserId="668" LastEditorUserId="73" LastEditDate="2017-06-13T13:09:37.473" LastActivityDate="2017-06-13T13:09:37.473" CommentCount="1" />
  <row Id="683" PostTypeId="2" ParentId="676" CreationDate="2017-06-13T08:44:30.520" Score="4" Body="&lt;p&gt;This usually comes down to religious issues, so let me try and steer it back to more objective grounds:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What language do you know (better)? Use the library for that one.&lt;/li&gt;&#xA;&lt;li&gt;If you know neither and will be learning a language to use the library, the majority opinion would be that Python is easier to learn.&lt;/li&gt;&#xA;&lt;li&gt;However, some people say that they &quot;click&quot; with Perl better&lt;/li&gt;&#xA;&lt;li&gt;Python is what the majority of bioinformaticians are using at the moment (about 60%). And there is a virtue and aid in using what everyone else is.&lt;/li&gt;&#xA;&lt;li&gt;What are your colleagues / collaborators going to be using?&lt;/li&gt;&#xA;&lt;li&gt;Broadly, newer libraries tend to be written in Python.&lt;/li&gt;&#xA;&lt;li&gt;Conversely, many sing the praises of regular expressions and string handling in Perl, citing speed and ease of use.&lt;/li&gt;&#xA;&lt;li&gt;I think the numerics support in Python may be markedly better than that in Perl.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="377" LastActivityDate="2017-06-13T08:44:30.520" CommentCount="3" />
  <row Id="684" PostTypeId="2" ParentId="673" CreationDate="2017-06-13T09:06:00.697" Score="1" Body="&lt;p&gt;&lt;a href=&quot;http://iregulon.aertslab.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;iRegulon&lt;/a&gt; takes a sequence-based approach to finding transcription factor targets. There's a Cytoscape app that you can use to find the regulators of a given gene list, or the targets of a particular transcription factor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Transcription factor binding sites are predicted using a collection of position weight matrices (PWMs) from &lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003731#pcbi-1003731-t001&quot; rel=&quot;nofollow noreferrer&quot;&gt;a number of sources&lt;/a&gt;, including Jaspar and TRANSFAC.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;iRegulon: From a Gene List to a Gene Regulatory Network Using Large Motif and Track Collections PLoS Comput Biol. 2014 Jul 24;10(7):e1003731. doi: &lt;a href=&quot;http://dx,doi.org/10.1371/journal.pcbi.1003731&quot; rel=&quot;nofollow noreferrer&quot;&gt;10.1371/journal.pcbi.1003731&lt;/a&gt;. eCollection 2014 Jul.&lt;/p&gt;&#xA;" OwnerUserId="194" LastActivityDate="2017-06-13T09:06:00.697" CommentCount="0" />
  <row Id="685" PostTypeId="2" ParentId="673" CreationDate="2017-06-13T09:25:06.267" Score="2" Body="&lt;p&gt;In general, there is two options to identify targets for transcription factors: experimental (ChIP-seq) and sequence-based predictions.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;TF binding from experimental data&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;The are multiple projects that produce binding data of transcription factors and quantify their peaks across the genome. The advantage here is that you know that binding actually occurs, as opposed to motif predictions. But you need the corresponding experiments.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You already mentioned &lt;a href=&quot;https://www.encodeproject.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;ENCODE&lt;/a&gt;, which is probably the biggest producer of such data. NIH's &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/20709693&quot; rel=&quot;nofollow noreferrer&quot;&gt;Roadmap Epigenomics&lt;/a&gt; is another one, but that focusses less on TFs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have a gene list and would like to know which transcription factor was likely involved, you can do an enrichment test of your gene set in known TF targets. The &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/20709693&quot; rel=&quot;nofollow noreferrer&quot;&gt;ChEA (ChIP Enrichment Analysis) database&lt;/a&gt; does this.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Prediction using motifs&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Another possibility is to look at &lt;a href=&quot;https://en.wikipedia.org/wiki/Sequence_motif&quot; rel=&quot;nofollow noreferrer&quot;&gt;binding motifs&lt;/a&gt; and see whether your gene list is enriched in those. These will, however, be inactive in a given tissue or cell type if the chromatin is packed or the methylation state of the promotor is unfavorable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Examples for those are the &lt;a href=&quot;http://jaspar.genereg.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;JASPAR&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/TRANSFAC&quot; rel=&quot;nofollow noreferrer&quot;&gt;TRANSFAC&lt;/a&gt; databases, or the &lt;a href=&quot;http://software.broadinstitute.org/gsea/msigdb/collections.jsp#C3&quot; rel=&quot;nofollow noreferrer&quot;&gt;MSigDB motif gene set&lt;/a&gt; (as burger mentioned). You can also query those features using the Ensembl genome database (&lt;a href=&quot;http://www.ensembl.org/biomart/martview/c48f212aea63b05694341e6ead06d756&quot; rel=&quot;nofollow noreferrer&quot;&gt;BioMart&lt;/a&gt;, &lt;a href=&quot;https://rest.ensembl.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;REST&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Calculating enrichment in gene sets&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Most likely, an analysis you will want to perform is to calculate the enrichment in your gene list, e.g. that you got from differential expression in two conditions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most convenient way is to use the &lt;a href=&quot;http://amp.pharm.mssm.edu/Enrichr/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Enrichr platform&lt;/a&gt;, which is a web page that accepts a gene list and will compute enrichment in ChEA, JASPAR, TRANSFAC, etc. You can also &lt;a href=&quot;http://amp.pharm.mssm.edu/Enrichr/#stats&quot; rel=&quot;nofollow noreferrer&quot;&gt;download their gene sets&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="69" LastActivityDate="2017-06-13T09:25:06.267" CommentCount="0" />
  <row Id="686" PostTypeId="2" ParentId="676" CreationDate="2017-06-13T09:49:44.683" Score="4" Body="&lt;p&gt;Currently you could use either but a major question is which platform will others be using in the future. AFAIK Perl is only superior to Python for regex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Based on the trend I see for new programmers and new software being released: Perl is on the way out and Python is still growing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://trends.google.com/trends/explore?date=all&amp;amp;q=bioperl,biopython&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://trends.google.com/trends/explore?date=all&amp;amp;q=bioperl,biopython&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="383" LastActivityDate="2017-06-13T09:49:44.683" CommentCount="1" />
  <row Id="687" PostTypeId="1" CreationDate="2017-06-13T10:23:17.233" Score="8" ViewCount="244" Body="&lt;p&gt;I have a gene expression count matrix produced from bulk RNA-seq data. I'd like to find genes that were &lt;strong&gt;not&lt;/strong&gt; expressed in a group of samples and were expressed in another group.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem of course is that not all &lt;em&gt;effectively&lt;/em&gt; non-expressed genes will have 0 counts due to sequencing errors, or because they were expressed in a small subset of cells.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm interested in solutions using &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="208" LastActivityDate="2017-06-15T09:53:50.680" Title="What methods are available to find a cutoff value for non-expressed genes in RNA-seq?" Tags="&lt;rna-seq&gt;&lt;r&gt;" AnswerCount="4" CommentCount="0" />
  <row Id="688" PostTypeId="2" ParentId="653" CreationDate="2017-06-13T10:32:44.133" Score="2" Body="&lt;p&gt;Your list has two identifiers for the same node per line. In order to use it, you will need to change that. If you want to use the gene name (2nd and 4th fields, in your example), just run:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;awk 'print $2,$4' netw.txt &amp;gt; netw.gr&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you want to use the Entrez geneIDs instead, run:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;awk 'print $1,$3' netw.txt &amp;gt; netw.gr&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then, as others already mentioned, install &lt;a href=&quot;http://cytoscape.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;Cytoscape&lt;/code&gt;&lt;/a&gt;, launch it and import your gene list:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;In the welcome screen, select &quot;From Network File...&quot;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/EeYPU.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/EeYPU.png&quot; alt=&quot;Cytoscape welcome screen&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Then, select your network file (&lt;code&gt;netw.gr&lt;/code&gt; in the example above) and choose &quot;Advanced Options&quot;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/nDM2t.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/nDM2t.png&quot; alt=&quot;Advanced options&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Set the delimiter to SPACE and uncheck &quot;Use first line as column names&quot;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/YOnzG.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/YOnzG.png&quot; alt=&quot;select the delimiter&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Finally, click on the header of each column and set the first to &quot;source node&quot; and the second to &quot;target node&quot;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/pUBYI.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/pUBYI.png&quot; alt=&quot;source node&quot;&gt;&lt;/a&gt;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/LbSm3.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/LbSm3.png&quot; alt=&quot;target node&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Now, click &quot;OK&quot; and you will have successfully imported your network into Cytoscape. &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-06-13T10:32:44.133" CommentCount="0" />
  <row Id="689" PostTypeId="2" ParentId="687" CreationDate="2017-06-13T11:09:16.817" Score="8" Body="&lt;p&gt;A common method is to use &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3870982/&quot; rel=&quot;noreferrer&quot;&gt;zFPKMs&lt;/a&gt;, which you can find implemented in R &lt;a href=&quot;https://github.com/severinEvo/gene_expression/blob/master/zFPKM.R&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having said that, there's an inherent problem in declaring a difference between two things on either side of a given threshold. Given that, I would encourage you to use more than &quot;expressed in one and not in another&quot;, likely adding at least a &quot;with a minimal difference of X between them&quot; metric. You may also find the tau metric useful, which is implemented in R &lt;a href=&quot;https://github.com/severinEvo/gene_expression/blob/master/tau.R&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;. This is meant to measure tissue-specificity, which is more akin to what you're probably interested in doing.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-13T11:09:16.817" CommentCount="0" />
  <row Id="690" PostTypeId="2" ParentId="687" CreationDate="2017-06-13T11:13:08.053" Score="9" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;I'd like to find genes that were not expressed in a group of samples and were expressed in another group.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;This is, fundamentally, a differential expression analysis, with a twist. To solve this, you’d first use a differential expression library of your choice (e.g. DESeq2) and perform a one-tailed test of differential expression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Briefly, you’d perform the normal setup and then use&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;results(dds, altHypothesis = 'greater')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To perform a one-tailed test. This will give you only those genes that are significantly upregulated in one group. Check chapter 3.9 of the vignette for details.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course this won’t tell you that the genes are &lt;em&gt;unexpressed&lt;/em&gt; in the other group. Unfortunately I don’t know of a good value to threshold the results; I would start by plotting a histogram of the (variance stabilised) expression values in your first group, and then visually choose an expression threshold that cleanly separates genes that are clearly expressed from zeros:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vst_counts = assay(vst(dds))&#xA;dens = density(vst_counts[, replicate])&#xA;plot(dens, log = 'y')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(This merges the replicates in the group, which should be fine.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Counts follow a multimodal distribution, with one mode for unexpressed and one or more for expressed genes. The expression threshold can be set somewhere between the clearly unexpressed and expressed peaks:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/MF0vy.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/MF0vy.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here I used &lt;code&gt;identify(dens)&lt;/code&gt; to identify the threshold interactively but you could also use an analytical method:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;threshold = identify(dens)&#xA;quantile = sum(dens$x &amp;lt; dens$x[threshold]) / length(dens$x)&#xA;&#xA;# Using just one replicate here; more robust would be to use a mean value.&#xA;nonzero_counts = counts(dds, normalized = TRUE)[, replicates[1]]&#xA;nonzero_counts = nonzero_counts[nonzero_counts &amp;gt; 0]&#xA;&#xA;(expression_threshold = quantile(nonzero_counts, probs = quantile))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;26.5625%&#xA;4.112033&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-06-14T12:23:37.687" LastActivityDate="2017-06-14T12:23:37.687" CommentCount="4" />
  <row Id="691" PostTypeId="2" ParentId="687" CreationDate="2017-06-13T11:30:59.540" Score="0" Body="&lt;p&gt;Unfortunatly that is not doable - there is now way of distinguishing between features not expressed and features not expressed for technical reasons. Furthermore a lot of genes might seem lowly expressed to to technical noise.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This also means that you will have to choose a cutoff and simply use that. For choosing a cutoff looking at the replicate distribution of log10(counts) and log10(FPKM) are probably the best options.&lt;/p&gt;&#xA;" OwnerUserId="599" LastActivityDate="2017-06-13T11:30:59.540" CommentCount="2" />
  <row Id="692" PostTypeId="1" AcceptedAnswerId="698" CreationDate="2017-06-13T11:51:32.633" Score="4" ViewCount="30" Body="&lt;p&gt;I have a large collection of sequence trace files which are in &lt;a href=&quot;http://staden.sourceforge.net/manual/formats_unix_2.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;SCF file format&lt;/a&gt; (binary files that in addition to the string of DNA bases also contain the electropherogram of the sample and quality information about the base calls).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have an existing BioPython pipeline that accepts files in &lt;a href=&quot;http://www6.appliedbiosystems.com/support/software_community/ABIF_File_Format.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;AB1 format&lt;/a&gt; (pdf), so I need to convert my SCF files to AB1 files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found a question on Biostars that asks for the opposite: &lt;a href=&quot;https://www.biostars.org/p/622/&quot; rel=&quot;nofollow noreferrer&quot;&gt;converting from SCF to AB1&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The accepted answer there was to use &lt;a href=&quot;http://staden.sourceforge.net/manual/manpages_unix_1.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;convert_trace&lt;/code&gt;&lt;/a&gt; from the Staden package.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./convert_trace -out_format scf &amp;lt; trace.ab1 &amp;gt; trace.scf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So I tried the opposite:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./convert_trace -out_format abi &amp;lt; trace.scf &amp;gt; trace.ab1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This did indeed produce a file with &lt;code&gt;*.ab1&lt;/code&gt; extension, but upon importing it to my BioPython pipeline, I got the error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File should start ABIF, not '\xaeZTR'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So apparently, &lt;code&gt;convert_trace&lt;/code&gt; converted my files to &lt;a href=&quot;http://staden.sourceforge.net/manual/formats_unix_12.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;ZTR format&lt;/a&gt;, even altough I specified &lt;code&gt;-out_format abi&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Closer reading of the manual states that &lt;code&gt;[convert_exe] can write CTF, EXP, PLN, SCF and ZTR formats.&lt;/code&gt; (so indeed no ABI), but it confuses me why &lt;code&gt;ABI&lt;/code&gt; is still a valid output format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, my question is, how do I convert SCF files into ABI files?&lt;/p&gt;&#xA;" OwnerUserId="450" LastEditorUserId="298" LastEditDate="2017-06-13T12:41:26.687" LastActivityDate="2017-06-13T14:06:16.443" Title="How can I convert SCF trace files to ABI files?" Tags="&lt;scf&gt;&lt;abi&gt;&lt;staden&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="693" PostTypeId="1" AcceptedAnswerId="704" CreationDate="2017-06-13T13:03:09.127" Score="7" ViewCount="74" Body="&lt;p&gt;In single-cell RNA-seq data we have an inflated number of 0 (or near-zero) counts due to low mRNA capture rate and other inefficiencies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can we decide which genes are 0 due to gene dropout (lack of measurement sensitivity), and which are genuinely not expressed in the cell?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Deeper sequencing does not solve this problem as shown on the below saturation curve of 10x Chromium data:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/cBXmF.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/cBXmF.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="208" LastActivityDate="2017-06-13T16:03:16.817" Title="How can we distinguish between true zero and dropout-zero counts in single-cell RNA-seq?" Tags="&lt;rna-seq&gt;&lt;sequencing&gt;" AnswerCount="2" CommentCount="5" />
  <row Id="694" PostTypeId="1" AcceptedAnswerId="742" CreationDate="2017-06-13T13:29:05.017" Score="4" ViewCount="57" Body="&lt;p&gt;I'd like to test &lt;a href=&quot;http://mmtf.rcsb.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;MMTF&lt;/a&gt;, a new format for storing biomolecular structures which is promoted by RCSB as a more compact alternative to mmCIF and PDB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From &lt;a href=&quot;http://mmtf.rcsb.org/faq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;MMTF FAQ&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p&gt;How do I convert a PDBx/mmCIF file to an MMTF file?&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The BioJava library contains methods to read and write PDBx/mmCIF files and MMTF files.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Can I do such a conversion, ideally from command line, but without writing my own Java program?&lt;/p&gt;&#xA;" OwnerUserId="266" LastActivityDate="2017-06-14T22:42:30.370" Title="Converter between PDB or mmCIF and MMTF" Tags="&lt;file-formats&gt;&lt;pdb&gt;&lt;mmcif&gt;&lt;3d-structure&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="695" PostTypeId="1" AcceptedAnswerId="697" CreationDate="2017-06-13T13:43:41.210" Score="2" ViewCount="46" Body="&lt;p&gt;I have a set of genomic ranges that are potentially overlapping. I want to count the amount of ranges at certain positions using R. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm Pretty sure there are good solutions, but I seem to be unable to find them. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Solutions like &lt;a href=&quot;https://stackoverflow.com/questions/26140151/binning-two-vectors-of-different-ranges-using-r?rq=1&quot;&gt;cut&lt;/a&gt; or &lt;a href=&quot;https://stackoverflow.com/questions/11963508/generate-bins-from-a-data-frame&quot;&gt;findIntervals&lt;/a&gt; don't achieve what I want as they only count on one vector or accumulate by &lt;code&gt;all values &amp;lt;= break&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also &lt;code&gt;countMatches {GenomicRanges}&lt;/code&gt; doesn't seem to cover it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Probably one could use Bedtools, but I don't want to leave R.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could only come up with a hilariously slow solution&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# generate test data&#xA;testdata &amp;lt;- data.frame(chrom = rep(seq(1,10),10),&#xA;                       starts = abs(rnorm(100, mean = 1, sd = 1)) * 1000,&#xA;                       ends = abs(rnorm(100, mean = 2, sd = 1)) * 2000)&#xA;&#xA;# make sure that all end coordinates are bigger than start&#xA;# this is a requirement of the original data&#xA;testdata &amp;lt;- testdata[testdata$ends - testdata$starts &amp;gt; 0,]&#xA;&#xA;# count overlapping ranges on certain positions&#xA;count.data &amp;lt;- lapply(unique(testdata$chrom), function(chromosome){&#xA;    tmp.inner &amp;lt;- lapply(seq(1,10000, by = 120), function(i){&#xA;        sum(testdata$chrom == chromosome &amp;amp; testdata$starts &amp;lt;= i &amp;amp; testdata$ends &amp;gt;= i)&#xA;    })&#xA;    return(unlist(tmp.inner))&#xA;})&#xA;&#xA;# generate a data.frame containing all data&#xA;df.count.data &amp;lt;- ldply(count.data, rbind)&#xA;&#xA;# ideally the chromosome will be columns and not rows&#xA;t(df.count.data)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="248" LastEditorUserId="298" LastEditDate="2017-06-13T14:14:59.873" LastActivityDate="2017-06-13T14:14:59.873" Title="Count genomic ranges" Tags="&lt;r&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="696" PostTypeId="2" ParentId="678" CreationDate="2017-06-13T14:01:53.477" Score="1" Body="&lt;p&gt;Use &lt;a href=&quot;https://github.com/lh3/htsbox&quot; rel=&quot;nofollow noreferrer&quot;&gt;htsbox&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;htsbox samview -p in.bam | less -S&#xA;htsbox samview -pS in.sam | less -S&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It outputs mapping positions in the &lt;a href=&quot;https://github.com/lh3/miniasm/blob/master/PAF.md&quot; rel=&quot;nofollow noreferrer&quot;&gt;PAF&lt;/a&gt; format, which looks something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;read1  4983  774  4982  +  chr18  80373285  26911072  26915544  3835  4631  60 \&#xA;       mm:i:214  io:i:119  in:i:159  do:i:339  dn:i:423&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is one long line in terminal. I folded the line for display purposes. For the first 12 fixed fields, see the table in &lt;a href=&quot;https://github.com/lh3/miniasm/blob/master/PAF.md&quot; rel=&quot;nofollow noreferrer&quot;&gt;PAF page&lt;/a&gt;. They give mapping positions and identity. The optional fields tell you #mismatches (mm), #insOpens (io), #insertions (in), #delOpens (do) and #deletions (dn). Note that the alignment MUST have an &quot;NM&quot; tag; otherwise the number of matching bases (col. 11) is overestimated and &quot;mm&quot; will always be zero.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can easily compute summary statistics from PAF if you are familiar with command lines. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-shell prettyprint-override&quot;&gt;&lt;code&gt;htsbox samview -p in.bam | awk '{x+=$10;y+=$11}END{print x/y}'  # identity&#xA;htsbox samview -p in.bam | awk '$2&amp;gt;1000{x+=$10;y+=$11}END{print x/y}'  # identity for &amp;gt;1000bp&#xA;htsbox samview -p in.bam \&#xA;  | perl -ane '{$g+=$1 while /[id]n:i:(\d+)/g;$y+=$F[10]}END{print $g/$y,&quot;\n&quot;}' # gap rate&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you don't like long one-liners, feel free to write a proper script to collect all statistics in one go.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-13T14:01:53.477" CommentCount="0" />
  <row Id="697" PostTypeId="2" ParentId="695" CreationDate="2017-06-13T14:04:02.240" Score="4" Body="&lt;p&gt;&lt;a href=&quot;https://www.rdocumentation.org/packages/GenomicRanges/versions/1.24.1/topics/findOverlaps-methods&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;GenomicRanges::countOverlaps&lt;/code&gt;&lt;/a&gt; seems to be what you’re after:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;position_range = GRanges(position$chrom, IRanges(position, position, width = 1))&#xA;ranges_at_position = countOverlaps(position_ranges, granges)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-06-13T14:04:02.240" CommentCount="0" />
  <row Id="698" PostTypeId="2" ParentId="692" CreationDate="2017-06-13T14:06:16.443" Score="2" Body="&lt;p&gt;Apparently &lt;code&gt;convert_trace&lt;/code&gt; does not do a good parameter checking and silently sets ZTR as default if it does not recognise a valid output format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know of any freely available command-line converters in that specific direction. Even at the height of Sanger sequencing (late 90s, early 2000s), pipelines usually tried to reduce complexity and footprint by converting AB1 to anything else, often SCF (ZTR came too late and never got enough traction).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That being said, I dug out an SCF I had from 1997, loaded it into &lt;a href=&quot;http://www.geneious.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Geneious&lt;/a&gt;, and re-exported it as .ab1. I was a bit astonished to see that this worked. So Geneious (or similar packages from other companies) may be your best bet for a quick, one-off thing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Depending on your needs, you might want to think about other ways: either expand BioPython to read SCFs, or maybe to expand the Staden io_lib to be able to write .ab1 (James still maintains that package and is extremely helpful).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Good luck.&lt;/p&gt;&#xA;" OwnerUserId="44" LastActivityDate="2017-06-13T14:06:16.443" CommentCount="2" />
  <row Id="699" PostTypeId="2" ParentId="676" CreationDate="2017-06-13T14:59:54.690" Score="1" Body="&lt;p&gt;I agree with @story and @agapow here. As it stands right now BioPerl is still much more mature than BioPython. If there's a tool or method that only exists in the BioPerl library then you should probably use BioPerl to accomplish what you need to accomplish. But it's undeniable that BioPython is progressing very quickly and will soon overtake BioPerl. That being said there are still a lot of plus side regarding Perl in the Bioinformatics realm and it will definitely help to be somewhat competent in it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So in short my answer is to use both! I find myself mostly using Python and BioPython but if I need to accomplish something using a tool that isn't available in the BioPython library or can be accomplished more quickly using Perl I will look towards BioPerl/Perl. &lt;/p&gt;&#xA;" OwnerUserId="712" LastActivityDate="2017-06-13T14:59:54.690" CommentCount="0" />
  <row Id="700" PostTypeId="2" ParentId="693" CreationDate="2017-06-13T15:21:16.947" Score="0" Body="&lt;p&gt;Some people use imputation to differentiate between true zeros and dropout in single-cell data. Some approaches you can look into:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/VCCRI/CIDR&quot; rel=&quot;nofollow noreferrer&quot;&gt;CIDR&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/Vivianstats/scImpute&quot; rel=&quot;nofollow noreferrer&quot;&gt;scImpute&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Seurat (&lt;a href=&quot;http://satijalab.org/seurat/seurat_spatial_tutorial_part2.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;addImputedScore&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-06-13T15:21:16.947" CommentCount="0" />
  <row Id="701" PostTypeId="2" ParentId="664" CreationDate="2017-06-13T15:23:02.040" Score="0" Body="&lt;p&gt;You requested a tool similar to &lt;a href=&quot;http://pantherdb.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PANTHER&lt;/a&gt; but in R. First, PANTHER does a The PANTHER (Protein ANalysis THrough Evolutionary Relationships) tool does a classification  based on evolutionarily related proteins, gene ontologies ( molecular function, and biological process) and pathways.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;AFAIK there isn't a tool in R that integrates all these into one, but several packages do pieces of it. However, my observations after a couple of usage is that is based a lot in gene ontologies. Gene Ontologies are not pathways, but the best tool I found for it is &lt;a href=&quot;http://bioconductor.org/packages/topGO&quot; rel=&quot;nofollow noreferrer&quot;&gt;topGO&lt;/a&gt; (A bit hard to use but useful to get insights on the biology) which test over representations of a term in a given list. &lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-06-13T15:23:02.040" CommentCount="0" />
  <row Id="702" PostTypeId="1" CreationDate="2017-06-13T15:38:05.440" Score="2" ViewCount="68" Body="&lt;p&gt;I made this post in regular stack overflow but I was told about this awesome feature by @nbryans.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am a researcher (my programming knowledge is small) conducting analysis on a set of antibiotic (methicillin) resistant and a set of antibiotic (methicillin) susceptible. The sole difference between these two sets is assumed to be the resistance (R) vs susceptibility (S). I want to find if a genetic element in the resistant or susceptible genomes is correlated to resistance (positively or negatively). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have done a complete fisher exact test already and will detail how I did so below. However I would like to figure out how to prepare my data for a more complex learning algorithm such as those offered by sci-kit. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;(many of my techniques were learned from this &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/27297683&quot; rel=&quot;nofollow noreferrer&quot;&gt;article&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First I used &lt;a href=&quot;http://sun.aei.polsl.pl/REFRESH/index.php?page=projects&amp;amp;project=kmc&amp;amp;subpage=about&quot; rel=&quot;nofollow noreferrer&quot;&gt;KMC&lt;/a&gt; to split each fasta file for both R and S genomes into k-mers (I don't have enough &quot;reputation&quot; so I can't use &gt;2 links but k-mers are essentially genetic sequences of k length). The output for KMC was a dump of k-mers present in a genome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example: kmc dump&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;AAAAAAAAAACGAATGTACACAATCGAA    1&#xA;AAAAAAAAACGAATGTACACAATCGAAC    1&#xA;AAAAAAAAATCAAATCCTGACTATTTAG    1&#xA;AAAAAAAAATCGGTCAATTCATTAAAAG    1&#xA;AAAAAAAACAAACATGAAGACCTTGTTA    1&#xA;AAAAAAAACGAATGTACACAATCGAACA    1&#xA;AAAAAAAACGTGTTAAAGTGAATCACAC    1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I then merged the data into one binary matrix which had columns as genomes and rows as k-mers. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example: 1 resistant genome and 1 susceptible genome&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;kmer    R_1    S_1&#xA;AAAAAAAAAACGAATGTACACAATCGAA    1   0&#xA;AAAAAAAAACGAATGTACACAATCGAAC    1   0&#xA;AAAAAAAAATCAAATCCTGACTATTTAG    1   0&#xA;AAAAAAAAATCGGTCAATTCATTAAAAG    1   0&#xA;AAAAAAAAATTCCCTTCTAATCTTGAAT    0   1&#xA;AAAAAAAACAAAAATTATATAAAGCGAA    0   1&#xA;AAAAAAAACAAACATGAAGACCTTGTTA    1   1&#xA;AAAAAAAACAACCACCCATACATTGAGT    0   1&#xA;AAAAAAAACCCTTACAACAAATATGTAA    0   1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the data I have been working with and would like to analyze further with sci kit or other classification algorithms. Essentially each k-mer is a feature whose correlation with resistance is based on its presence in R vs S genomes. This is where I would like help transitioning this data into a dataset format suitable for sci kit. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I conducted a Fisher Exact test on each k-mer using R. For each k-mer a 2x2 matrix was created with the first column as resistant and the second column as susceptible. The two rows were present or not. So four numbers (# times k-mer was present in R, # in susceptible, # not in R, # not in S).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;      R    S&#xA;yes   #    #&#xA;no    #    #&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I used the following code in R:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;phenotype = as.numeric(grepl('^R_', colnames(raw_kmer_table)[2:ncol(raw_kmer_table)]))&#xA;fe_results = apply(kmer_pres_abs_matrix, 1, &#xA;               FUN = function(row) {&#xA;                 fe_mat = matrix(0, ncol = 2, nrow = 2)&#xA;                 fe_mat[1,1] = sum(row == 1 &amp;amp; phenotype == 1)&#xA;                 fe_mat[1,2] = sum(row == 1 &amp;amp; phenotype == 0)&#xA;                 fe_mat[2,1] = sum(row == 0 &amp;amp; phenotype == 1)&#xA;                 fe_mat[2,2] = sum(row == 0 &amp;amp; phenotype == 0)&#xA;&#xA;                 fe = fisher.test(fe_mat)&#xA;                 return(fe$p.value)&#xA;               }&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now that you have read how my data is currently formatted I would like to hear suggestions on how I can fit this data into a more complex test using sci kit or another resource. Algorithms I am interested in using are any you see fit and previously field-tested algorithms such as adaboost and forest learning algorithms. Also please ask me any clarifications or things I may have left out of this post!&lt;/p&gt;&#xA;" OwnerUserId="842" LastActivityDate="2017-06-14T15:33:46.443" Title="Preparing binary matrix data for Scikit classification algorithms" Tags="&lt;k-mer&gt;&lt;biopython&gt;&lt;machine-learning&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="703" PostTypeId="1" AcceptedAnswerId="739" CreationDate="2017-06-13T16:02:47.477" Score="2" ViewCount="33" Body="&lt;p&gt;We sort different populations of blood cells using a number of fluorescent flow cytometry markers and then sequence RNA. We want to see what the transcriptome tells us about the similarity and relation between these cells. In my experience on bulk RNA-seq data, there is a very good agreement between flow cytometry and mRNA expression for the markers. It's good to remember we sort using a few markers only, while there are hundreds if not thousands of cell surface proteins.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Should we exclude genes of these sorting (CD) marker proteins when we perform PCA or other types of clustering?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The argument is that after sorting, these genes may dominate clustering results, even if the rest of the transcriptome would tell otherwise, and thus falsely confirm similarity relations inferred from sorting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Should we at least check the influence of these genes on clustering results?&lt;/p&gt;&#xA;" OwnerUserId="208" LastActivityDate="2017-06-14T16:34:25.797" Title="Should the cell sorting marker genes be excluded during clustering?" Tags="&lt;rna-seq&gt;&lt;cell-line&gt;&lt;clustering&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="704" PostTypeId="2" ParentId="693" CreationDate="2017-06-13T16:03:16.817" Score="6" Body="&lt;p&gt;Actually this is one of the main problems you have when analyzing scRNA-seq data, and there is no established method for dealing with this. Different (dedicated) algorithms deal with it in different ways, but mostly you rely on how good the error modelling of your software is (a great read is the &lt;a href=&quot;http://dx.doi.org/10.1038/nbt.3711&quot; rel=&quot;noreferrer&quot;&gt;review&lt;/a&gt; by Wagner, Regev &amp;amp; Yosef, esp. the section on &quot;False negatives and overamplification&quot;). There are a couple of options:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;You can &lt;strong&gt;impute values&lt;/strong&gt;, i.e. fill in the gaps on technical zeros. &lt;a href=&quot;https://github.com/VCCRI/CIDR&quot; rel=&quot;noreferrer&quot;&gt;CIDR&lt;/a&gt; and &lt;a href=&quot;https://github.com/Vivianstats/scImpute&quot; rel=&quot;noreferrer&quot;&gt;scImpute&lt;/a&gt; do it directly. &lt;a href=&quot;http://biorxiv.org/content/early/2017/02/25/111591&quot; rel=&quot;noreferrer&quot;&gt;MAGIC&lt;/a&gt; and &lt;a href=&quot;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0805-z&quot; rel=&quot;noreferrer&quot;&gt;ZIFA&lt;/a&gt; project cells into a lower-dimensional space and use their similarity there to decide how to fill in the blanks.&lt;/li&gt;&#xA;&lt;li&gt;Some people straight up &lt;strong&gt;exclude genes&lt;/strong&gt; that are expressed in very low numbers. I can't give you citations off the top of my head, but many trajectory inference algorithms like &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/monocle.html&quot; rel=&quot;noreferrer&quot;&gt;monocle2&lt;/a&gt; and &lt;a href=&quot;https://github.com/jw156605/SLICER&quot; rel=&quot;noreferrer&quot;&gt;SLICER&lt;/a&gt; have heuristics to choose informative genes for their analysis.&lt;/li&gt;&#xA;&lt;li&gt;If the method you use for analysis doesn't model gene expression explicitly but &lt;strong&gt;uses some other distance method&lt;/strong&gt; to quantify similarity between cells (like cosine distance, euclidean distance, correlation), then the noise introduced by dropout can be covered by the signal of genes that are highly expressed. Note that this is dangerous, as genes that are highly expressed are not necessarily informative.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ERCC spike&lt;/strong&gt; ins can help you reduce technical noise, but I am not familiar with the Chromium protocol so maybe it doesn't apply there (?)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;since we are speaking about noise, you might consider using a protocol with unique molecular identifiers. They remove the amplification errors almost completely, at least for the transcripts that you capture...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Also, I would highly recommend using something more advanced than PCA to do the analysis. Software like the above-mentioned Monocle or &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/destiny.html&quot; rel=&quot;noreferrer&quot;&gt;destiny&lt;/a&gt; is easy to operate and increases the power of your analysis considerably.&lt;/p&gt;&#xA;" OwnerUserId="787" LastActivityDate="2017-06-13T16:03:16.817" CommentCount="0" />
  <row Id="705" PostTypeId="1" CreationDate="2017-06-13T16:24:28.293" Score="4" ViewCount="35" Body="&lt;p&gt;I am trying to understand how the MD:Z tag is used. The following is from the &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMtags.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;SAM Optional Fields Specification&lt;/a&gt;, which gives an example but is not thorough. &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The MD field aims to achieve SNP/indel calling without looking at the&#xA;  reference. For example, a string ‘10A5^AC6’ means from the leftmost&#xA;  reference base in the alignment, there are 10 matches followed by an A&#xA;  on the reference which is different from the aligned read base; the&#xA;  next 5 reference bases are matches followed by a 2bp deletion from the&#xA;  reference; the deleted sequence is AC; the last 6 bases are matches.&#xA;  The MD field ought to match the CIGAR string.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Suppose I have a read that I want to soft clip at both ends. If the read starts with CIGAR &lt;code&gt;100M&lt;/code&gt; and MD &lt;code&gt;50G49&lt;/code&gt;, and I want to change the CIGAR to &lt;code&gt;7S86M7S&lt;/code&gt;, what should the MD field become? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a more complete explanation of what appears in the MD tag field?&lt;/p&gt;&#xA;" OwnerUserId="272" LastActivityDate="2017-06-13T17:01:26.013" Title="How should the SAM MD tag match the CIGAR string?" Tags="&lt;file-formats&gt;&lt;sam&gt;&lt;cigar&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="707" PostTypeId="2" ParentId="705" CreationDate="2017-06-13T16:50:57.050" Score="4" Body="&lt;p&gt;The MD string doesn't apply to soft or hard clipped regions,. so your example read becomes &lt;code&gt;43G42&lt;/code&gt;. Since variant calling using something simplistic like this is only ever going to use the aligned portion, I guess there was never much reason to bother with adding in soft-clipping information. This also means that you need to parse the CIGAR string if you're using the MD string.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-13T17:01:26.013" LastActivityDate="2017-06-13T17:01:26.013" CommentCount="0" />
  <row Id="708" PostTypeId="1" AcceptedAnswerId="710" CreationDate="2017-06-13T17:07:48.073" Score="5" ViewCount="30" Body="&lt;p&gt;I generate bigWig files using &lt;code&gt;bamCoverage&lt;/code&gt; from deeptools, in part for my colleagues to visualize their mapped libraries in the IGV viewer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A problem is that the displayed track name is apparently the file name, which is not convenient because some files for different libraries have the same name. The reason is that the files are organized with one directory per library. I would prefer to avoid changing the way I name and organize the files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is there a way to edit my bigWig files in order to have control of the track name independently from the file name?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I understand, bigWig format derives from wiggle format, which have a track definition line that can contain a &lt;code&gt;name&lt;/code&gt; and a &lt;code&gt;definition&lt;/code&gt; attribute: &lt;a href=&quot;https://genome.ucsc.edu/goldenpath/help/customTrack.html#TRACK&quot; rel=&quot;noreferrer&quot;&gt;https://genome.ucsc.edu/goldenpath/help/customTrack.html#TRACK&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Maybe there is a way to convert a bigWig file back to wiggle, edit the track definition, and then rebuild an updated bigWig file.&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-06-13T17:41:31.517" Title="Customizing bigWig file" Tags="&lt;file-formats&gt;&lt;format-conversion&gt;&lt;bigwig&gt;&lt;visualization&gt;&lt;genome-browser&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="709" PostTypeId="1" CreationDate="2017-06-13T17:15:34.113" Score="2" ViewCount="52" Body="&lt;p&gt;When I use blastn and prokka (I will detail exactly how I did so below) on a 2.8 million bp fasta file I get output start/end numbers that do not seem to cover the entire genome. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Starting with a .fna genome such as genome.fna I ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1 - blastn&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Searched the genome for 3135 different 28-mers using BLASTn.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;makeblastdb -in genome.fna -dbtype nucl -parse_seqids -out ./output/genome&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;next command in python&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blast_tsv = NcbiblastnCommandline(query=Q, db=DB, perc_identity=100, outfmt=6, out=(OUT))&#xA;    stdout, stderr = blast_tsv()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Q is this list of k-mers. DB is the database created&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This outputs a list of search results but the important thing is that none of the start/end (columns 7 and 8) integers were greater than 100,000 yet the entire genome is 2.8 million base pairs long. I can provide the link to this file in the comments. Does this have to do with blastn stopping searching after finding one match? And if so how can I tell blastn to search for every match for each k-mer? (I'm open to using other blastn programs other than biopython)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2 - prokka&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I used prokka to create gff files for the genome. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;prokka-1.12/bin/prokka —setupdb&#xA;prokka-1.12/bin/prokka -kingdom Bacteria -rfam -outdir ./prokka_database/genome -force -prefix &quot;${genome/.fna/}&quot; genome.fna&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Th gff file produced by this command only described genes with start and end positions less than 200k. I can provide the link to this file as well. &lt;/p&gt;&#xA;" OwnerUserId="842" LastActivityDate="2017-06-13T18:22:13.263" Title="Why do BLASTn and prokka not seem to be searching the whole fasta file?" Tags="&lt;annotation&gt;&lt;genome&gt;&lt;k-mer&gt;&lt;biopython&gt;&lt;blast&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="710" PostTypeId="2" ParentId="708" CreationDate="2017-06-13T17:41:31.517" Score="4" Body="&lt;p&gt;There's no equivalent to the wiggle header in bigWig (or bigBed) files, which is why UCSC uses the file name. This is actually the reason for the track line stuff that you linked to, since you can then specify a name and just point to where the bigWig (or other format) file is on the internet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW, you can certainly convert your bigWig to wiggle, add the track line, and convert it back with UCSC tools (e.g., &lt;code&gt;bigWigToWig&lt;/code&gt; and &lt;code&gt;wigToBigWig&lt;/code&gt;). It shouldn't change the track name when you display it in UCSC (though if it does, then let me know, since I'll modify pyBigWig and libBigWig to support that then).&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-13T17:41:31.517" CommentCount="2" />
  <row Id="711" PostTypeId="2" ParentId="709" CreationDate="2017-06-13T18:22:13.263" Score="5" Body="&lt;p&gt;With a k-mer size of 28 it shouldn't be finding that many matches. And the prokka results are suspicious as well. Maybe you have multiple contigs (none larger than 100kb) in that file? What is the result of &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#xA;grep ^'&gt;' fasta_file | wc -l&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;? This would show how many contigs you have in the file.&lt;/p&gt;&#xA;" OwnerUserId="848" LastActivityDate="2017-06-13T18:22:13.263" CommentCount="6" />
  <row Id="712" PostTypeId="2" ParentId="687" CreationDate="2017-06-13T18:46:27.060" Score="3" Body="&lt;p&gt;Depending on how much effort you wish to put into this, here is one suggestion I have see used before (uses more than just R). Steps 1-4 come from &lt;a href=&quot;http://genome.cshlp.org/content/24/12/1918.long#ref-55&quot; rel=&quot;nofollow noreferrer&quot;&gt;this paper&lt;/a&gt; (see supplimentary material section &quot;Calculation of per gene Local FDR&quot;).&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;Take your gene models and collapse the introns down to maybe 100bp&lt;/em&gt;&lt;br&gt;&#xA;Thus if we have a gene with two exons and a 1kb intron eg. exons (1000, 1100) and (2000,2100), we reduce the intron size so that the exons are (1000,1100) and (1200,1300). This is because we need to find sufficient gene free space to fit the null gene in.    &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;Shift each gene into the nearest genomic space at least 5 kp from a genomic annotation and free of ESTs.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;Calculate the FPKM distribution of this shifted null set&lt;/em&gt;  &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;From these two distributions it should be possible to calculate a &lt;a href=&quot;https://cran.r-project.org/web/packages/locfdr/vignettes/locfdr-example.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;local FDR&lt;/a&gt; for expression.&lt;/em&gt;&lt;br&gt;&#xA;You will need to use a &quot;mixing proportion&quot; to do this. The above reference used &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2660870/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Qvality&lt;/a&gt; to do this.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;Carry out differential expression analysis as described in &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/690/235&quot;&gt;Konrad's answer&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;Subset the differentially expressed genes to only consider those that are not expressed in the control condition&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;You might want to also recalculate the FDRs for differential expression as you are only considering a subset of the tests you might have.&lt;/em&gt; &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="235" LastEditorUserId="235" LastEditDate="2017-06-15T09:53:50.680" LastActivityDate="2017-06-15T09:53:50.680" CommentCount="3" />
  <row Id="713" PostTypeId="2" ParentId="694" CreationDate="2017-06-13T18:55:36.833" Score="0" Body="&lt;p&gt;You looked at this &lt;a href=&quot;https://github.com/rcsb/mmtf-python&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/rcsb/mmtf-python&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;The python implementation of the MMTF API, decoder and encoder.&quot;&lt;/p&gt;&#xA;" OwnerUserId="640" LastActivityDate="2017-06-13T18:55:36.833" CommentCount="1" />
  <row Id="714" PostTypeId="2" ParentId="620" CreationDate="2017-06-13T19:14:44.307" Score="1" Body="&lt;p&gt;I don't know about tools, but i've used the following python code to calculate the ratio of reads that overlap the 5' or 3' ends of introns or that are spliced. We sum these across all introns in a gene set (we actaully use this for iCLIP analysis to see if RNA binding proteins bind pre-mRNA or spliced RNA).&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import pysam&#xA;from collections import Counter&#xA;from CGAT import GTF, IOTools&#xA;&#xA;def calculateSplicingIndex(bamfile, gtffile, outfile):&#xA;&#xA;    bamfile = pysam.AlignmentFile(bamfile)&#xA;&#xA;    counts = Counter()&#xA;&#xA;    for transcript in GTF.transcript_iterator(&#xA;            GTF.iterator(IOTools.openFile(gtffile))):&#xA;&#xA;        introns = GTF.toIntronIntervals(transcript)&#xA;&#xA;        for intron in introns:&#xA;            reads = bamfile.fetch(&#xA;                reference=transcript[0].contig,&#xA;                start=intron[0], end=intron[1])&#xA;&#xA;            for read in reads:&#xA;                if 'N' in read.cigarstring:&#xA;                    blocks = read.get_blocks()&#xA;                    starts, ends = zip(*blocks)&#xA;                    if intron[0] in ends and intron[1] in starts:&#xA;                        counts[&quot;Exon_Exon&quot;] += 1&#xA;                    else:&#xA;                        counts[&quot;spliced_uncounted&quot;] += 1&#xA;                elif (read.reference_start &amp;lt;= intron[0] - 3&#xA;                      and read.reference_end &amp;gt;= intron[0] + 3):&#xA;                    if transcript[0].strand == &quot;+&quot;:&#xA;                        counts[&quot;Exon_Intron&quot;] += 1&#xA;                    else:&#xA;                        counts[&quot;Intron_Exon&quot;] += 1&#xA;                elif (read.reference_start &amp;lt;= intron[1] - 3&#xA;                      and read.reference_end &amp;gt;= intron[1] + 3):&#xA;                    if transcript[0].strand == &quot;+&quot;:&#xA;                        counts[&quot;Intron_Exon&quot;] += 1&#xA;                    else:&#xA;                        counts[&quot;Exon_Intron&quot;] += 1&#xA;                else:&#xA;                    counts[&quot;unspliced_uncounted&quot;] += 1&#xA;&#xA;    header = [&quot;Exon_Exon&quot;,&#xA;              &quot;Exon_Intron&quot;,&#xA;              &quot;Intron_Exon&quot;,&#xA;              &quot;spliced_uncounted&quot;,&#xA;              &quot;unspliced_uncounted&quot;]&#xA;&#xA;    with IOTools.openFile(outfile, &quot;w&quot;) as outf:&#xA;&#xA;        outf.write(&quot;\t&quot;.join(header)+&quot;\n&quot;)&#xA;        outf.write(&quot;\t&quot;.join(map(str, [counts[col] for col in header]))&#xA;                   + &quot;\n&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Unfortunately this uses a bunch of libraries you may or may not have, including CGAT (for the GTF parser and IOTools package) and pysam.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you've got these statistics you can calculate the &quot;splicing index&quot; as &#xA;the log2 ratio of 2 times the number of spliced reads divided by the number of reads overlapping the 3' and 5' ends of introns. &lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-06-13T19:14:44.307" CommentCount="0" />
  <row Id="715" PostTypeId="2" ParentId="594" CreationDate="2017-06-13T19:28:19.813" Score="1" Body="&lt;p&gt;The &lt;a href=&quot;http://meme-suite.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;MEME Suite&lt;/a&gt; web site contains a collection of tools for motif analysis (I'm one of the maintainers). It contains two de novo motif discovery tools: &lt;a href=&quot;http://meme-suite.org/tools/meme&quot; rel=&quot;nofollow noreferrer&quot;&gt;MEME&lt;/a&gt; and &lt;a href=&quot;http://meme-suite.org/tools/dreme&quot; rel=&quot;nofollow noreferrer&quot;&gt;DREME&lt;/a&gt;. Public web applications are provided, but you can also download and build command line tools for a local installation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For your first goal you could use &lt;a href=&quot;http://meme-suite.org/tools/meme&quot; rel=&quot;nofollow noreferrer&quot;&gt;MEME&lt;/a&gt; and select the &quot;Any number of repetitions model&quot; (ANR). For your second goal, you'd use MEME with the &quot;Zero or One Occurrences Per Sequence&quot; (ZOOPS) model. For your third goal you could use FIMO (Find Individual Motif Occurrences), and one or more of the motif databases provided on the &lt;a href=&quot;http://meme-suite.org/doc/download.html?man_type=web&quot; rel=&quot;nofollow noreferrer&quot;&gt;software and database download page&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It sounds like your sequence data is about 30Mb. The MEME web application is limited to 60kb of sequence data, so you'd have to install a local copy of the MEME Suite. MEME would take a long time to analyze a 30Mb sequence database unless you have MPI configured, and lots of cores available. You might want to consider analyzing a randomly selected subset of your sequences. The running time of MEME grows as the cube of the number of sequences. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For short motifs, you may want to use &lt;a href=&quot;http://meme-suite.org/tools/dreme&quot; rel=&quot;nofollow noreferrer&quot;&gt;DREME&lt;/a&gt; rather than MEME. DREME is better than MEME at identifying short motifs, but is limited to motifs &amp;lt;= 8 positions wide.&lt;/p&gt;&#xA;" OwnerUserId="313" LastEditorUserId="313" LastEditDate="2017-06-13T22:20:11.820" LastActivityDate="2017-06-13T22:20:11.820" CommentCount="0" />
  <row Id="716" PostTypeId="1" AcceptedAnswerId="717" CreationDate="2017-06-13T19:39:13.280" Score="3" ViewCount="48" Body="&lt;p&gt;I am trying to calculate the mappability adjusted length of introns as described by &lt;a href=&quot;http://genesdev.cshlp.org/content/29/1/63.abstract&quot; rel=&quot;nofollow noreferrer&quot;&gt;Boutz et al&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Briefly, for each intron I wish to calculate the length minus the number of bases that are non-uniquely mappable. Mappability tracks can be downloaded as bigWigs from &lt;a href=&quot;http://genome.ucsc.edu/cgi-bin/hgFileUi?db=hg19&amp;amp;g=wgEncodeMapability&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. The the score at each base is 1/number of mapping positions, so 1 indicates a read can be uniquely mapped at this location. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am struggling to do this computation in either a reasonable amount of time, or a reasonable amount of memory.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First I tried importing the whole bigwig:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(rtracklayer)&#xA;mappability &amp;lt;- import(BigWigFile(&quot;wgEncodeCrgMapabilityAlign50mer.bigWig&quot;),&#xA;                      as=&quot;NumericList&quot;,&#xA;                      selection=BigWigSelection(intron_ranges))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;intron_ranges&lt;/code&gt; is a &lt;code&gt;GRanges&lt;/code&gt; object with the introns (about 800,000 of them).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This used &lt;strong&gt;way&lt;/strong&gt; too much memory, and soon caused by machine to fall over.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Second I tried processing one intron at a time:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;mappability_file = BigWigFile(&quot;wgEncodeCrgMapabilityAlign50mer.bigWig&quot;)&#xA;effective_length &amp;lt;- function (gr) {&#xA;  intron_selection &amp;lt;- BigWigSelection(gr)&#xA;  scores &amp;lt;- import(mappability_file, as=&quot;NumericList&quot;, selection=intron_selection)&#xA;  non_unique &amp;lt;- sum(scores[[1]] &amp;lt; 1.0)&#xA;  eff_len = width(gr)[1] - non_unique&#xA;  return(eff_len)&#xA;}    &#xA;&#xA;mappability &amp;lt;- sapply(intron_ranges, effective_length)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This has been running for hours and shows no sign of finishing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way to do this that uses less than, say 4GB of RAM, but finishes in say less than 30 minutes? It feels like there should be. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not tied to R, just what I've tried so far. Happy with answers using binary packages, common shell tools, python or R. &lt;/p&gt;&#xA;" OwnerUserId="235" LastEditorUserId="292" LastEditDate="2017-06-14T11:44:08.643" LastActivityDate="2017-06-14T11:44:08.643" Title="Time and memory efficient processing of many intervals for a bigWig file" Tags="&lt;bigwig&gt;&lt;performance&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="0" />
  <row Id="717" PostTypeId="2" ParentId="716" CreationDate="2017-06-13T20:43:29.640" Score="2" Body="&lt;p&gt;Assuming you can make a BED file of introns, you can then use the &lt;code&gt;pyBigWig&lt;/code&gt; module in python:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pyBigWig&#xA;import numpy&#xA;&#xA;bw = pyBigWig.open(&quot;some file.bw&quot;)&#xA;bed = open(&quot;introns.bed&quot;)&#xA;for line in bed:&#xA;    cols = line.strip().split(&quot;\t&quot;)&#xA;    vals = bw.values(cols[0], int(cols[1]), int(cols[2]), numpy=True)&#xA;    effLen = cols[2] - cols[1] - (vals &amp;lt; 1.0).sum()&#xA;    # Do something with this.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;One can make that parallel (with the deeptools API) should it prove too slow. This will only use an appreciable amount of memory for very large introns, since there you need to store the value at each base. One could make this much more memory efficient by using &lt;code&gt;bw.intervals()&lt;/code&gt; instead, but that'd require quite a bit more code (you'd need to determine the number of bases overlap per interval and then assume that either each base you want to query has an overlapping interval or, if not, keep track of that fact).&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-13T20:52:36.063" LastActivityDate="2017-06-13T20:52:36.063" CommentCount="6" />
  <row Id="718" PostTypeId="2" ParentId="658" CreationDate="2017-06-13T20:46:40.447" Score="4" Body="&lt;p&gt;It really depends on what you are trying to do, but here are a few services that I know of.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://cloud.google.com/genomics/v1alpha2/gatk&quot; rel=&quot;nofollow noreferrer&quot;&gt;GATK on Google Genomics Cloud&lt;/a&gt;: Google and the Broad offer a cloud instance tailored to GATK pipelines.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/health/genomics/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Genomics on Amazon Web Services&lt;/a&gt;: I don't think there is anything that makes this unique, but Amazon offers some resources to help get started with genomics/life sciences-centered cloud solutions.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.illumina.com/informatics.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Illumina Bioinformatics&lt;/a&gt;: Illumina is working on a whole suite of bioinformatics software for the cloud.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.cancergenomicscloud.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cancer Genomics Cloud&lt;/a&gt;: This is specific to cancer genomics, but I believe Seven Bridges allows you to push all sorts of data into the tool and analyze it.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="47" LastActivityDate="2017-06-13T20:46:40.447" CommentCount="0" />
  <row Id="719" PostTypeId="1" AcceptedAnswerId="726" CreationDate="2017-06-13T22:20:23.237" Score="4" ViewCount="36" Body="&lt;p&gt;From &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMtags.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;SAM Optional Fields Specification&lt;/a&gt; the NM field is &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Edit distance to the reference, including ambiguous bases but excluding clipping&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Assuming both the MD and CIGAR are present, is the edit distance simply the number of characters &lt;code&gt;[A-Z]&lt;/code&gt; appearing in the MD field plus the number of bases inserted (&lt;code&gt;xI&lt;/code&gt;, if any) from the CIGAR string? Are there any other complications? &lt;/p&gt;&#xA;" OwnerUserId="272" LastActivityDate="2017-06-14T06:43:46.623" Title="Is the optional SAM NM field strictly computable from the MD and CIGAR?" Tags="&lt;file-formats&gt;&lt;sam&gt;&lt;cigar&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="721" PostTypeId="2" ParentId="395" CreationDate="2017-06-14T01:38:01.883" Score="1" Body="&lt;p&gt;While better methods of evaluating your clusters would be to use an external dataset or a dataset with known truth, there are a variety of internal validation metrics that can be used to compare clustering solutions without another dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are a few metrics:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Davies-Bouldin Index&lt;/li&gt;&#xA;&lt;li&gt;Calinski-Harabasz Index&lt;/li&gt;&#xA;&lt;li&gt;Root-Mean-Square Standard Deviation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Many more can be found in this clustering review: &lt;a href=&quot;http://stke.sciencemag.org/content/9/432/re6&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://stke.sciencemag.org/content/9/432/re6&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These internal validation metrics grade your clustering solution based on three measures: compactness, connectedness, and separation. When using these metrics to compare clustering solutions, be sure to consider which metric is appropriate for your results as some algorithms work by optimizing certain measures.&lt;/p&gt;&#xA;" OwnerUserId="853" LastActivityDate="2017-06-14T01:38:01.883" CommentCount="0" />
  <row Id="722" PostTypeId="1" AcceptedAnswerId="825" CreationDate="2017-06-14T03:16:07.773" Score="6" ViewCount="77" Body="&lt;p&gt;I have a dataset of Oxford Nanopore cDNA reads. Many of my reads are full-length or close to full-length transcripts, and I and am interested in examining alternative splicing. For this, I would like to begin by visualising my reads and comparing variants qualitatively.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried visualising my data in both IGV and SeqMonk, but neither has given a satisfying result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;IGV&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/zM3Yn.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/zM3Yn.png&quot; alt=&quot;IGV Visualisation of NOTCH2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;IGV shows some links between aligned sections, shown as fine blue lines and thick black lines. I cannot work out what the difference between these lines is, and even more confusingly, we have numerous alignments where two aligned exons come from the same sequence but are not joined by any line. This means that, to confirm or deny a spliced variant, I need to manually examine the read ID of each exon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;SeqMonk&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/ugYDT.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ugYDT.png&quot; alt=&quot;SeqMonk Visualisation of NOTCH2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SeqMonk gives a much cleaner visualisation, but unfortunately shows no links between aligned exons, and worse, I cannot find the read IDs. This means there is functionally (as far as I can see) no way to tell which exons come from the same read.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT: Sashimi Plot&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As recommended in &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/723/163&quot;&gt;this answer&lt;/a&gt;, I have tried using IGV's Sashimi Plot. Unfortunately, I believe the high error rate and relatively low coverage is causing this to create somewhat confusing output. The Sashimi Plot shows mostly nearly every junction to have just a single read supporting it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/5ah7z.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5ah7z.png&quot; alt=&quot;IGV Sashimi Plot of NOTCH2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a visualisation tool which allows simple examination of splicing of full-length transcripts?&lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="163" LastEditDate="2017-06-20T02:13:57.857" LastActivityDate="2017-06-20T02:13:57.857" Title="Visualisation of long read RNA-Seq splicing" Tags="&lt;rna-seq&gt;&lt;nanopore&gt;&lt;read-mapping&gt;&lt;visualization&gt;" AnswerCount="3" CommentCount="2" />
  <row Id="723" PostTypeId="2" ParentId="722" CreationDate="2017-06-14T03:28:48.103" Score="2" Body="&lt;p&gt;The &lt;a href=&quot;https://software.broadinstitute.org/software/igv/Sashimi&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sashmi Plot&lt;/a&gt; feature built into IGV. It gives a nice summary of the spliced transcripts and the coverage of each exon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/6MMjw.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/6MMjw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="64" LastActivityDate="2017-06-14T03:28:48.103" CommentCount="0" />
  <row Id="724" PostTypeId="1" CreationDate="2017-06-14T04:12:13.013" Score="2" ViewCount="36" Body="&lt;p&gt;I'm working on a project where I have to assemble sequences generated by RADseq. At the end I hope to compare two species of woodpeckers in Sri Lanka by using SNPs. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to assemble it using Stacks pipeline, but it crushed on the way. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there are any other tools or pipelines that I can use for assembling my data instead?&lt;/p&gt;&#xA;" OwnerUserId="409" LastEditorUserId="57" LastEditDate="2017-06-14T09:49:13.553" LastActivityDate="2017-06-14T09:49:13.553" Title="Assembling sequence data generated by RADseq" Tags="&lt;ngs&gt;&lt;assembly&gt;&lt;radseq&gt;" AnswerCount="0" CommentCount="3" FavoriteCount="1" />
  <row Id="725" PostTypeId="2" ParentId="722" CreationDate="2017-06-14T05:23:42.993" Score="3" Body="&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/xS23U.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/xS23U.png&quot; alt=&quot;GenVision Pro Sashimi Plot&quot;&gt;&lt;/a&gt;DNASTAR's software is for purchase, but high quality. GenVision Pro does genomic visualization, including Sashimi plots.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: not sure why this answer is being downvoted, unless it's because the software isn't free. OP has tried IGV and SeqMonk, I mentioned an alternative he might not have heard of.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is a video demonstrating the use of Sashimi plots in GenVision Pro:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.dnastar.com/t-support-videos.aspx?video=YJvcERoSIsg&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.dnastar.com/t-support-videos.aspx?video=YJvcERoSIsg&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="138" LastEditorUserId="138" LastEditDate="2017-06-14T22:46:43.843" LastActivityDate="2017-06-14T22:46:43.843" CommentCount="2" />
  <row Id="726" PostTypeId="2" ParentId="719" CreationDate="2017-06-14T06:43:46.623" Score="4" Body="&lt;p&gt;Assuming both the MD and CIGAR are present &lt;strong&gt;and correct&lt;/strong&gt;, then yes, you can parse both to get the edit distance (&lt;code&gt;NM&lt;/code&gt; auxiliary tag). One big caveat to this is that there's a reason that the &lt;code&gt;samtools calmd&lt;/code&gt; command exists, since it's historically been the case that not all aligners have output correct MD strings. It's rare for the CIGAR string to be wrong and that'd be more of a catastrophic error on the part of an aligner. For what it's worth, if the &lt;code&gt;NM&lt;/code&gt; auxiliary is absent on a given alignment but present on others produced by the same aligner then it's fair to assume &lt;code&gt;NM:i:0&lt;/code&gt; for a given alignment by default (many aligners only produce &lt;code&gt;NM:i:XXX&lt;/code&gt; if the edit distance is at least 1).&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-14T06:43:46.623" CommentCount="0" />
  <row Id="727" PostTypeId="5" CreationDate="2017-06-14T09:40:29.803" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-06-14T09:40:29.803" LastActivityDate="2017-06-14T09:40:29.803" CommentCount="0" />
  <row Id="728" PostTypeId="4" CreationDate="2017-06-14T09:40:29.803" Score="0" Body="Use this tag for questions related to single-cell RNA-seq." OwnerUserId="48" LastEditorUserId="48" LastEditDate="2017-06-14T10:58:45.880" LastActivityDate="2017-06-14T10:58:45.880" CommentCount="0" />
  <row Id="729" PostTypeId="2" ParentId="673" CreationDate="2017-06-14T09:50:01.067" Score="0" Body="&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/26066708&quot; rel=&quot;nofollow noreferrer&quot;&gt;TRUSST&lt;/a&gt; is a manually curated database with around 790 TFs and their target genes. &lt;a href=&quot;http://www.grnpedia.org/trrust/&quot; rel=&quot;nofollow noreferrer&quot;&gt;TRUSST Website&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="161" LastActivityDate="2017-06-14T09:50:01.067" CommentCount="0" />
  <row Id="731" PostTypeId="1" CreationDate="2017-06-14T11:10:24.310" Score="3" ViewCount="33" Body="&lt;p&gt;I have a list of 100 genes that are called as hits in a genetic screening. I want to have a network of the interactions between the proteins of these 100 genes. I am using both &lt;a href=&quot;https://string-db.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;STRINGdb web&lt;/a&gt; and its &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/STRINGdb.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;R API&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In both situations you can select the threshold of confidence of interaction (default 0.4) for displaying the network. Changing this parameter drastically changes the network that I am generating. I have read the FAQ section of STRINGdb and they recommend to choose some arbitrary number based on the number of interactions you need for you analysis. If I use the default threshold, I have few interactions and I don't know if lowering the threshold would be correct. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, my question is, is there any established threshold based on habitual user's experience used for that, beyond the recommendation of setting an arbitrary one? &lt;/p&gt;&#xA;" OwnerUserId="678" LastEditorUserId="292" LastEditDate="2017-06-14T12:10:48.763" LastActivityDate="2017-06-15T13:55:11.447" Title="How to select a cutoff for interaction confidence in STRINGdb?" Tags="&lt;proteins&gt;&lt;public-databases&gt;&lt;networks&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="732" PostTypeId="1" AcceptedAnswerId="735" CreationDate="2017-06-14T11:26:35.647" Score="1" ViewCount="120" Body="&lt;p&gt;I've been asked to evaluate whether the MinION will be sufficient to distinguish between 20bp CRISPR guide RNAs from &lt;a href=&quot;https://www.addgene.org/pooled-library/zhang-mouse-gecko-v2/&quot; rel=&quot;nofollow noreferrer&quot;&gt;the GeCKO v2 set&lt;/a&gt;. I know that this set has some &lt;a href=&quot;https://groups.google.com/forum/#!topic/crispr/qxEi_jL7jls&quot; rel=&quot;nofollow noreferrer&quot;&gt;exactly identical sequences&lt;/a&gt;, which wouldn't be distinguishable regardless of the sequencing method used, so I would like to know whether any two non-indentical sequences differ by less than 15% of the 20bp (i.e. only 3 bases different).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are around 130k 20bp sequences in the dataset, and I'm pretty sure that an all-vs-all approach is not feasible, but perhaps there's some trick that I'm missing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I thought of doing was looking at dimer and trimer/codon counts to hunt down sequences that were most similar. I found about 100 sequences that shared the same trimer count signature (but were not identical), and within this group found [only] one pair with 85% identity; all the remainder were 70% or less.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;... but I'm having trouble ignoring the doubt I have that these two sequences are the most similar. It might be possible that there are sequences in the dataset that don't have identical trimer count signatures, but differ by less than three bases. I'll give a very obvious example (which isn't in the dataset):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1: AAAAA AAAAA AAAAA AAAAA&#xA;2: AAAAC AAAAA AAAAA AAAAA&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The trimer counts for these two sequences are different (Sequence 1 -- AAA: 18; Sequence 2 -- AAA: 15; AAC: 1; ACA: 1; CAA: 1), and yet the sequences are only 1 base different (which might be a problem for the MinION). How can I work out if any such sequence pairs exist in the dataset?&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-14T15:00:55.240" Title="How do I find the most similar sequences from a large set of short sequences?" Tags="&lt;minion&gt;&lt;crispr&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="733" PostTypeId="2" ParentId="732" CreationDate="2017-06-14T12:20:49.173" Score="1" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;I'm pretty sure that an all-vs-all approach is not feasible, but perhaps there's some trick that I'm missing.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The &quot;trick&quot; is that I hadn't actually tried this, and Bowtie2 happens to be quite good at doing this. When there are other similar matches, the MAPQ score that Bowtie2 produces are reduced, so all that is needed is to identify those sequences with less than a MAPQ of 42. I've found 2810 such sequences that share some similarity to others (2416 if I exclude reverse-complement matches):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ bowtie2-build Mouse_GeCKOv2_Library.fasta Mouse_GeCKOv2_Library.fasta&#xA;$ bowtie2 -f -x Mouse_GeCKOv2_Library.fasta -U Mouse_GeCKOv2_Library.fasta | &#xA;   samtools view | awk '{if($5 != 42){print &quot;&amp;gt;&quot;$1&quot;\n&quot;$10}}' &amp;gt; similar_seqs.fasta&#xA;125795 reads; of these:&#xA;  125795 (100.00%) were unpaired; of these:&#xA;    1 (0.00%) aligned 0 times&#xA;    122982 (97.76%) aligned exactly 1 time&#xA;    2812 (2.24%) aligned &amp;gt;1 times&#xA;100.00% overall alignment rate&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-14T12:20:49.173" CommentCount="0" />
  <row Id="734" PostTypeId="2" ParentId="732" CreationDate="2017-06-14T12:42:31.153" Score="1" Body="&lt;p&gt;For this application, you could probably also do something like calculate the Hamming distances between all of the strings in an all-vs-all approach (it should not take too long or too much overhead). You could use something like the Hamming distance tools in &lt;a href=&quot;https://julialang.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Julia&lt;/a&gt;. Here is an example of what I mean (using Julia):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;using StringDistances&#xA;&#xA;k = [&quot;AATTGGCC&quot;, &quot;AATTGGCA&quot;, &quot;AATTCCGG&quot;]&#xA;&#xA;for s in k&#xA;    for y in k&#xA;        print(s, &quot; compared to &quot;, y, &quot;:  &quot;, compare(Hamming(), s, y), &quot;\n&quot;)&#xA;    end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The output looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;AATTGGCC compared to AATTGGCC:  1.0&#xA;AATTGGCC compared to AATTGGCA:  0.875&#xA;AATTGGCC compared to AATTCCGG:  0.5&#xA;AATTGGCA compared to AATTGGCC:  0.875&#xA;AATTGGCA compared to AATTGGCA:  1.0&#xA;AATTGGCA compared to AATTCCGG:  0.5&#xA;AATTCCGG compared to AATTGGCC:  0.5&#xA;AATTCCGG compared to AATTGGCA:  0.5&#xA;AATTCCGG compared to AATTCCGG:  1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is just a simple small example but the core is there. Should not be too bad scaling it up. Hope this helps and provides another helpful option. :)&lt;/p&gt;&#xA;" OwnerUserId="824" LastEditorUserId="298" LastEditDate="2017-06-14T15:00:55.240" LastActivityDate="2017-06-14T15:00:55.240" CommentCount="0" />
  <row Id="735" PostTypeId="2" ParentId="732" CreationDate="2017-06-14T14:14:33.443" Score="3" Body="&lt;p&gt;130k * 20bp is a small data set. At this scale, SSE2 Smith-Waterman may work well:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-shell prettyprint-override&quot;&gt;&lt;code&gt;git clone https://github.com/attractivechaos/klib&#xA;cd klib &amp;amp;&amp;amp; gcc -O2 -D_KSW_MAIN ksw.c -o ksw -lz&#xA;./ksw -a1 -b1 -q1 -r1 -t14 20bp.fa 20bp.fa &amp;gt; out.tsv&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I simulated 130k 20bp reads from E. coli K-12. The last command line takes about ~2 CPU hours based on the partial output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is possible to make this much faster by using 5bp seeds. You can also try ssearch and fasta from the &lt;a href=&quot;http://fasta.bioch.virginia.edu/fasta_www2/fasta_list2.shtml&quot; rel=&quot;nofollow noreferrer&quot;&gt;fasta aligner package&lt;/a&gt;. They are slower, probably because they are not optimized for such input. Computing Hamming distance with SSE2 will be much faster, but is probably of little use as it does not allow gaps. There are also Gene Myers' edit-distance based algorithms, which can be faster than SSE2-SW.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bowtie2 (as well as most short-read mappers) won't work well at the default setting. Bowtie2 uses long exact seeds. It will miss many 1-mismatch hits, let alone 3-mismatch ones. The number you get from bowtie2 is an underestimate. You might be able to tune bowtie2, but to find 3-mismatch hits, you have to make it a lot slower.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-14T14:14:33.443" CommentCount="0" />
  <row Id="736" PostTypeId="1" AcceptedAnswerId="737" CreationDate="2017-06-14T14:22:55.960" Score="3" ViewCount="77" Body="&lt;p&gt;I'm faced with having to align many (some 100s) bacterial genomes, where the genome length is in the millions. Obviously, this is beyond normal alignment techniques and it's unclear to me what the best practice is for such circumstances:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;conventional alignment on a very powerful computer with lots of memory&lt;/li&gt;&#xA;&lt;li&gt;break up the genome into smaller fragments and align them individually&lt;/li&gt;&#xA;&lt;li&gt;some exotic different procedure&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What possible avenues of attack are there?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(I've attempted to use Mafft and Clustal with little success)&lt;/p&gt;&#xA;" OwnerUserId="377" LastActivityDate="2017-06-14T16:08:47.127" Title="Aligning many long sequences" Tags="&lt;alignment&gt;&lt;genome&gt;" AnswerCount="1" CommentCount="9" FavoriteCount="0" />
  <row Id="737" PostTypeId="2" ParentId="736" CreationDate="2017-06-14T15:12:37.680" Score="7" Body="&lt;p&gt;Whole genome aliment can be done using &lt;a href=&quot;http://darlinglab.org/mauve/user-guide/progressivemauve.html&quot; rel=&quot;noreferrer&quot;&gt;Progressive Mauve&lt;/a&gt;, &lt;a href=&quot;http://last.cbrc.jp/&quot; rel=&quot;noreferrer&quot;&gt;LAST&lt;/a&gt; or &lt;a href=&quot;https://github.com/mummer4/mummer&quot; rel=&quot;noreferrer&quot;&gt;Mummer&lt;/a&gt;. For bacteria I used Mauve since it has also very nice visualisation engine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are interested in a rough idea of the shared genome regions, you can use &lt;a href=&quot;https://github.com/zeeev/bevel&quot; rel=&quot;noreferrer&quot;&gt;bevel&lt;/a&gt;. Bevel is not really an aligner, it is more like a dot-plot, but it is super fast (even for mammalian sized genomes), therefore I would recommend to start with it.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-14T16:08:47.127" LastActivityDate="2017-06-14T16:08:47.127" CommentCount="0" />
  <row Id="738" PostTypeId="2" ParentId="702" CreationDate="2017-06-14T15:33:46.443" Score="1" Body="&lt;p&gt;Since you are using R, you probably don't want to use scikit-learn, which is for Python. However, there is a similar R library &lt;a href=&quot;https://mlr-org.github.io/&quot; rel=&quot;nofollow noreferrer&quot;&gt;mlr&lt;/a&gt; (&quot;R package to make machine learning in R easy&quot;) that provides a unified interface to all popular machine learning methods. Check their &lt;a href=&quot;https://mlr-org.github.io/mlr-tutorial/devel/html/&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt; on how to get started.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-06-14T15:33:46.443" CommentCount="0" />
  <row Id="739" PostTypeId="2" ParentId="703" CreationDate="2017-06-14T16:34:25.797" Score="2" Body="&lt;p&gt;I do not think there is a simple &quot;yes&quot; or &quot;no&quot; answer here. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A good starting point would be, as you suggest, use all the genes and assess the results in the light of the marker genes and expected results. This could both serve as as good quality control as well as give you overview of all the processes happening in the cells. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Depending on the marker genes effect and biological question you may then want to remove the marker genes with potential ordering effect, or even other genes, e.g. by GO terms or pathways. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You most likely want to account for the cell cycle phases as well. And check for other technical factors. I recommend Bioconductor pipeline to get inspired &lt;a href=&quot;https://www.bioconductor.org/help/workflows/simpleSingleCell/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.bioconductor.org/help/workflows/simpleSingleCell/&lt;/a&gt; when it comes to scRNA-seq analyses. &lt;/p&gt;&#xA;" OwnerUserId="375" LastActivityDate="2017-06-14T16:34:25.797" CommentCount="0" />
  <row Id="740" PostTypeId="2" ParentId="731" CreationDate="2017-06-14T17:09:49.490" Score="1" Body="&lt;p&gt;This depends on what you are trying to do and whether you value specificity over sensitivity. We can't tell you since it is entirely dependent on the biological question you want to answer. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I would recommend two things:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Don't use STRING. The creators of STRING made the choice to value sensitivity over all else, so they include &lt;em&gt;any&lt;/em&gt; interaction they can get their hands on. That is sometimes fine, depending on what you want to do, but is more often a problem. Personally, I tend to avoid STRING as much as possible and instead use more curated databases like &lt;a href=&quot;http://cicblade.dep.usal.es:8080/APID/init.action&quot; rel=&quot;nofollow noreferrer&quot;&gt;APID&lt;/a&gt; or &lt;a href=&quot;https://www.ebi.ac.uk/intact/&quot; rel=&quot;nofollow noreferrer&quot;&gt;IntAct&lt;/a&gt;. You can find a list of many PPI databases in the &lt;a href=&quot;http://www.ebi.ac.uk/Tools/webservices/psicquic/view/main.xhtml&quot; rel=&quot;nofollow noreferrer&quot;&gt;EBI's Psiqcuick View&lt;/a&gt; page. You might also be interested in my answer &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/577/298&quot;&gt;here&lt;/a&gt; which gives an example script using APID which can easily be modified to query other DBs. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Whatever you use, I recommend you filter by detection method. There are many different interaction detection methods ranging from those that only identify direct, binary interactions between two proteins (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Two-hybrid_screening&quot; rel=&quot;nofollow noreferrer&quot;&gt;Yeast two-hybrid&lt;/a&gt;), those that also find proteins that are in the same complex even if there is no &lt;em&gt;direct&lt;/em&gt; interaction between them (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Chromatin_immunoprecipitation&quot; rel=&quot;nofollow noreferrer&quot;&gt;ChIP&lt;/a&gt;), to various non-experimental methods which are used to &lt;em&gt;infer&lt;/em&gt; interactions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Luckily, all of these have been collected into a controlled vocabulary which you can see in the &lt;a href=&quot;http://www.ebi.ac.uk/ols/ontologies/MI/terms?obo_id=MI:0001&quot; rel=&quot;nofollow noreferrer&quot;&gt;Interaction Detection Method&lt;/a&gt; Ontology Lookup Service page. Go there, collect the interaction detection methods you consider good enough for whatever you want to do and filter your interactions using those. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For example, for a high quality human interactome I built for a project I used to work on, I was only interested in direct binary interactions, so I only kept interactions detected by the following methods:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    &quot;MI:0008&quot; =&amp;gt; &quot;array technology&quot;,&#xA;    &quot;MI:0009&quot; =&amp;gt; &quot;bacterial display&quot;,&#xA;    &quot;MI:0010&quot; =&amp;gt; &quot;beta galactosidase complementation&quot;,&#xA;    &quot;MI:0011&quot; =&amp;gt; &quot;beta lactamase complementation&quot;,&#xA;    &quot;MI:0012&quot; =&amp;gt; &quot;bioluminescence resonance energy transfer&quot;,&#xA;    &quot;MI:0013&quot; =&amp;gt; &quot;biophysical&quot;,&#xA;    &quot;MI:0014&quot; =&amp;gt; &quot;adenylate cyclase complementation&quot;,&#xA;    &quot;MI:0016&quot; =&amp;gt; &quot;circular dichroism&quot;,&#xA;    &quot;MI:0017&quot; =&amp;gt; &quot;classical fluorescence spectroscopy&quot;,&#xA;    &quot;MI:0018&quot; =&amp;gt; &quot;two hybrid&quot;,&#xA;    &quot;MI:0020&quot; =&amp;gt; &quot;transmission electron microscopy&quot;,&#xA;    &quot;MI:0030&quot; =&amp;gt; &quot;cross-linking study&quot;,&#xA;    &quot;MI:0031&quot; =&amp;gt; &quot;protein cross-linking with a bifunctional reagent&quot;,&#xA;    &quot;MI:0034&quot; =&amp;gt; &quot;display technology&quot;,&#xA;    &quot;MI:0040&quot; =&amp;gt; &quot;electron microscopy&quot;,&#xA;    &quot;MI:0041&quot; =&amp;gt; &quot;electron nuclear double resonance&quot;,&#xA;    &quot;MI:0042&quot; =&amp;gt; &quot;electron paramagnetic resonance&quot;,&#xA;    &quot;MI:0043&quot; =&amp;gt; &quot;electron resonance&quot;,&#xA;    &quot;MI:0047&quot; =&amp;gt; &quot;far western blotting&quot;,&#xA;    &quot;MI:0048&quot; =&amp;gt; &quot;filamentous phage display&quot;,&#xA;    &quot;MI:0049&quot; =&amp;gt; &quot;filter binding&quot;,&#xA;    &quot;MI:0051&quot; =&amp;gt; &quot;fluorescence technology&quot;,&#xA;    &quot;MI:0052&quot; =&amp;gt; &quot;fluorescence correlation spectroscopy&quot;,&#xA;    &quot;MI:0053&quot; =&amp;gt; &quot;fluorescence polarization spectroscopy&quot;,&#xA;    &quot;MI:0055&quot; =&amp;gt; &quot;fluorescent resonance energy transfer&quot;,&#xA;    &quot;MI:0065&quot; =&amp;gt; &quot;isothermal titration calorimetry&quot;,&#xA;    &quot;MI:0066&quot; =&amp;gt; &quot;lambda phage display&quot;,&#xA;    &quot;MI:0073&quot; =&amp;gt; &quot;mrna display&quot;,&#xA;    &quot;MI:0081&quot; =&amp;gt; &quot;peptide array&quot;,&#xA;    &quot;MI:0084&quot; =&amp;gt; &quot;phage display&quot;,&#xA;    &quot;MI:0089&quot; =&amp;gt; &quot;protein array&quot;,&#xA;    &quot;MI:0090&quot; =&amp;gt; &quot;protein complementation assay&quot;,&#xA;    &quot;MI:0091&quot; =&amp;gt; &quot;chromatography technology&quot;,&#xA;    &quot;MI:0092&quot; =&amp;gt; &quot;protein in situ array&quot;,&#xA;    &quot;MI:0095&quot; =&amp;gt; &quot;proteinchip(r) on a surface-enhanced laser desorption/ionization&quot;,&#xA;    &quot;MI:0097&quot; =&amp;gt; &quot;reverse ras recruitment system&quot;,&#xA;    &quot;MI:0098&quot; =&amp;gt; &quot;ribosome display&quot;,&#xA;    &quot;MI:0099&quot; =&amp;gt; &quot;scintillation proximity assay&quot;,&#xA;    &quot;MI:0107&quot; =&amp;gt; &quot;surface plasmon resonance&quot;,&#xA;    &quot;MI:0108&quot; =&amp;gt; &quot;t7 phage display&quot;,&#xA;    &quot;MI:0111&quot; =&amp;gt; &quot;dihydrofolate reductase reconstruction&quot;,&#xA;    &quot;MI:0112&quot; =&amp;gt; &quot;ubiquitin reconstruction&quot;,&#xA;    &quot;MI:0114&quot; =&amp;gt; &quot;x-ray crystallography&quot;,&#xA;    &quot;MI:0115&quot; =&amp;gt; &quot;yeast display&quot;,&#xA;    &quot;MI:0226&quot; =&amp;gt; &quot;ion exchange chromatography&quot;,&#xA;    &quot;MI:0227&quot; =&amp;gt; &quot;reverse phase chromatography&quot;,&#xA;    &quot;MI:0231&quot; =&amp;gt; &quot;mammalian protein protein interaction trap&quot;,&#xA;    &quot;MI:0232&quot; =&amp;gt; &quot;transcriptional complementation assay&quot;,&#xA;    &quot;MI:0255&quot; =&amp;gt; &quot;post transcriptional interference&quot;,&#xA;    &quot;MI:0369&quot; =&amp;gt; &quot;lex-a dimerization assay&quot;,&#xA;    &quot;MI:0370&quot; =&amp;gt; &quot;tox-r dimerization assay&quot;,&#xA;    &quot;MI:0397&quot; =&amp;gt; &quot;two hybrid array&quot;,&#xA;    &quot;MI:0398&quot; =&amp;gt; &quot;two hybrid pooling approach&quot;,&#xA;    &quot;MI:0399&quot; =&amp;gt; &quot;two hybrid fragment pooling approach&quot;,&#xA;    &quot;MI:0400&quot; =&amp;gt; &quot;affinity technology&quot;,&#xA;    &quot;MI:0401&quot; =&amp;gt; &quot;biochemical&quot;,&#xA;    &quot;MI:0405&quot; =&amp;gt; &quot;competition binding&quot;,&#xA;    &quot;MI:0406&quot; =&amp;gt; &quot;deacetylase assay&quot;,&#xA;    &quot;MI:0410&quot; =&amp;gt; &quot;electron tomography&quot;,&#xA;    &quot;MI:0411&quot; =&amp;gt; &quot;enzyme linked immunosorbent assay&quot;,&#xA;    &quot;MI:0415&quot; =&amp;gt; &quot;enzymatic study&quot;,&#xA;    &quot;MI:0416&quot; =&amp;gt; &quot;fluorescence microscopy&quot;,&#xA;    &quot;MI:0419&quot; =&amp;gt; &quot;gtpase assay&quot;,&#xA;    &quot;MI:0420&quot; =&amp;gt; &quot;kinase homogeneous time resolved fluorescence&quot;,&#xA;    &quot;MI:0423&quot; =&amp;gt; &quot;in-gel kinase assay&quot;,&#xA;    &quot;MI:0424&quot; =&amp;gt; &quot;protein kinase assay&quot;,&#xA;    &quot;MI:0425&quot; =&amp;gt; &quot;kinase scintillation proximity assay&quot;,&#xA;    &quot;MI:0426&quot; =&amp;gt; &quot;light microscopy&quot;,&#xA;    &quot;MI:0428&quot; =&amp;gt; &quot;imaging technique&quot;,&#xA;    &quot;MI:0432&quot; =&amp;gt; &quot;one hybrid&quot;,&#xA;    &quot;MI:0434&quot; =&amp;gt; &quot;phosphatase assay&quot;,&#xA;    &quot;MI:0435&quot; =&amp;gt; &quot;protease assay&quot;,&#xA;    &quot;MI:0437&quot; =&amp;gt; &quot;protein three hybrid&quot;,&#xA;    &quot;MI:0440&quot; =&amp;gt; &quot;saturation binding&quot;,&#xA;    &quot;MI:0508&quot; =&amp;gt; &quot;deacetylase radiometric assay&quot;,&#xA;    &quot;MI:0509&quot; =&amp;gt; &quot;phosphatase homogeneous time resolved fluorescence&quot;,&#xA;    &quot;MI:0510&quot; =&amp;gt; &quot;homogeneous time resolved fluorescence&quot;,&#xA;    &quot;MI:0511&quot; =&amp;gt; &quot;protease homogeneous time resolved fluorescence&quot;,&#xA;    &quot;MI:0512&quot; =&amp;gt; &quot;zymography&quot;,&#xA;    &quot;MI:0513&quot; =&amp;gt; &quot;collagen film assay&quot;,&#xA;    &quot;MI:0514&quot; =&amp;gt; &quot;in gel phosphatase assay&quot;,&#xA;    &quot;MI:0515&quot; =&amp;gt; &quot;methyltransferase assay&quot;,&#xA;    &quot;MI:0516&quot; =&amp;gt; &quot;methyltransferase radiometric assay&quot;,&#xA;    &quot;MI:0655&quot; =&amp;gt; &quot;lambda repressor two hybrid&quot;,&#xA;    &quot;MI:0657&quot; =&amp;gt; &quot;systematic evolution of ligands by exponential enrichment&quot;,&#xA;    &quot;MI:0678&quot; =&amp;gt; &quot;antibody array&quot;,&#xA;    &quot;MI:0695&quot; =&amp;gt; &quot;sandwich immunoassay&quot;,&#xA;    &quot;MI:0696&quot; =&amp;gt; &quot;polymerase assay&quot;,&#xA;    &quot;MI:0726&quot; =&amp;gt; &quot;reverse two hybrid&quot;,&#xA;    &quot;MI:0727&quot; =&amp;gt; &quot;lexa b52 complementation&quot;,&#xA;    &quot;MI:0728&quot; =&amp;gt; &quot;gal4 vp16 complementation&quot;,&#xA;    &quot;MI:0809&quot; =&amp;gt; &quot;bimolecular fluorescence complementation&quot;,&#xA;    &quot;MI:0813&quot; =&amp;gt; &quot;proximity enzyme linked immunosorbent assay&quot;,&#xA;    &quot;MI:0824&quot; =&amp;gt; &quot;x-ray powder diffraction&quot;,&#xA;    &quot;MI:0825&quot; =&amp;gt; &quot;x-ray fiber diffraction&quot;,&#xA;    &quot;MI:0827&quot; =&amp;gt; &quot;x-ray tomography&quot;,&#xA;    &quot;MI:0841&quot; =&amp;gt; &quot;phosphotransferase assay&quot;,&#xA;    &quot;MI:0870&quot; =&amp;gt; &quot;demethylase assay&quot;,&#xA;    &quot;MI:0872&quot; =&amp;gt; &quot;atomic force microscopy&quot;,&#xA;    &quot;MI:0879&quot; =&amp;gt; &quot;nucleoside triphosphatase assay&quot;,&#xA;    &quot;MI:0880&quot; =&amp;gt; &quot;atpase assay&quot;,&#xA;    &quot;MI:0887&quot; =&amp;gt; &quot;histone acetylase assay&quot;,&#xA;    &quot;MI:0889&quot; =&amp;gt; &quot;acetylase assay&quot;,&#xA;    &quot;MI:0892&quot; =&amp;gt; &quot;solid phase assay&quot;,&#xA;    &quot;MI:0894&quot; =&amp;gt; &quot;electron diffraction&quot;,&#xA;    &quot;MI:0895&quot; =&amp;gt; &quot;protein kinase A complementation&quot;,&#xA;    &quot;MI:0899&quot; =&amp;gt; &quot;p3 filamentous phage display&quot;,&#xA;    &quot;MI:0900&quot; =&amp;gt; &quot;p8 filamentous phage display&quot;,&#xA;    &quot;MI:0905&quot; =&amp;gt; &quot;amplified luminescent proximity homogeneous assay&quot;,&#xA;    &quot;MI:0916&quot; =&amp;gt; &quot;lexa vp16 complementation&quot;,&#xA;    &quot;MI:0920&quot; =&amp;gt; &quot;ribonuclease assay&quot;,&#xA;    &quot;MI:0921&quot; =&amp;gt; &quot;surface plasmon resonance array&quot;,&#xA;    &quot;MI:0946&quot; =&amp;gt; &quot;ping&quot;,&#xA;    &quot;MI:0947&quot; =&amp;gt; &quot;bead aggregation assay&quot;,&#xA;    &quot;MI:0949&quot; =&amp;gt; &quot;gdp/gtp exchange assay&quot;,&#xA;    &quot;MI:0953&quot; =&amp;gt; &quot;polymerization&quot;,&#xA;    &quot;MI:0968&quot; =&amp;gt; &quot;biosensor&quot;,&#xA;    &quot;MI:0969&quot; =&amp;gt; &quot;bio-layer interferometry&quot;,&#xA;    &quot;MI:0972&quot; =&amp;gt; &quot;phosphopantetheinylase assay&quot;,&#xA;    &quot;MI:0976&quot; =&amp;gt; &quot;total internal reflection fluorescence spectroscopy&quot;,&#xA;    &quot;MI:0979&quot; =&amp;gt; &quot;oxidoreductase assay&quot;,&#xA;    &quot;MI:0984&quot; =&amp;gt; &quot;deaminase assay&quot;,&#xA;    &quot;MI:0989&quot; =&amp;gt; &quot;amidase assay&quot;,&#xA;    &quot;MI:0990&quot; =&amp;gt; &quot;cleavage assay&quot;,&#xA;    &quot;MI:0991&quot; =&amp;gt; &quot;lipid cleavage assay&quot;,&#xA;    &quot;MI:0992&quot; =&amp;gt; &quot;defarnesylase assay&quot;,&#xA;    &quot;MI:0993&quot; =&amp;gt; &quot;degeranylase assay&quot;,&#xA;    &quot;MI:0994&quot; =&amp;gt; &quot;demyristoylase assay&quot;,&#xA;    &quot;MI:0995&quot; =&amp;gt; &quot;depalmitoylase assay&quot;,&#xA;    &quot;MI:0996&quot; =&amp;gt; &quot;deformylase assay&quot;,&#xA;    &quot;MI:0997&quot; =&amp;gt; &quot;ubiquitinase assay&quot;,&#xA;    &quot;MI:0998&quot; =&amp;gt; &quot;deubiquitinase assay&quot;,&#xA;    &quot;MI:0999&quot; =&amp;gt; &quot;formylase assay&quot;,&#xA;    &quot;MI:1000&quot; =&amp;gt; &quot;hydroxylase assay&quot;,&#xA;    &quot;MI:1001&quot; =&amp;gt; &quot;lipidase assay&quot;,&#xA;    &quot;MI:1002&quot; =&amp;gt; &quot;myristoylase assay&quot;,&#xA;    &quot;MI:1003&quot; =&amp;gt; &quot;geranylgeranylase assay&quot;,&#xA;    &quot;MI:1004&quot; =&amp;gt; &quot;palmitoylase assay&quot;,&#xA;    &quot;MI:1005&quot; =&amp;gt; &quot;adp ribosylase assay&quot;,&#xA;    &quot;MI:1006&quot; =&amp;gt; &quot;deglycosylase assay&quot;,&#xA;    &quot;MI:1007&quot; =&amp;gt; &quot;glycosylase assay&quot;,&#xA;    &quot;MI:1008&quot; =&amp;gt; &quot;sumoylase assay&quot;,&#xA;    &quot;MI:1009&quot; =&amp;gt; &quot;desumoylase assay&quot;,&#xA;    &quot;MI:1010&quot; =&amp;gt; &quot;neddylase assay&quot;,&#xA;    &quot;MI:1011&quot; =&amp;gt; &quot;deneddylase assay&quot;,&#xA;    &quot;MI:1016&quot; =&amp;gt; &quot;fluorescence recovery after photobleaching&quot;,&#xA;    &quot;MI:1019&quot; =&amp;gt; &quot;protein phosphatase assay&quot;,&#xA;    &quot;MI:1024&quot; =&amp;gt; &quot;scanning electron microscopy&quot;,&#xA;    &quot;MI:1026&quot; =&amp;gt; &quot;diphtamidase assay&quot;,&#xA;    &quot;MI:1030&quot; =&amp;gt; &quot;excimer fluorescence&quot;,&#xA;    &quot;MI:1031&quot; =&amp;gt; &quot;protein folding/unfolding&quot;,&#xA;    &quot;MI:1036&quot; =&amp;gt; &quot;nucleotide exchange assay&quot;,&#xA;    &quot;MI:1037&quot; =&amp;gt; &quot;Split renilla luciferase complementation&quot;,&#xA;    &quot;MI:1038&quot; =&amp;gt; &quot;silicon nanowire field-effect transistor&quot;,&#xA;    &quot;MI:1087&quot; =&amp;gt; &quot;monoclonal antibody blockade&quot;,&#xA;    &quot;MI:1088&quot; =&amp;gt; &quot;phenotype-based detection assay&quot;,&#xA;    &quot;MI:1089&quot; =&amp;gt; &quot;nuclear translocation assay&quot;,&#xA;    &quot;MI:1111&quot; =&amp;gt; &quot;two hybrid bait or prey pooling approach&quot;,&#xA;    &quot;MI:1112&quot; =&amp;gt; &quot;two hybrid prey pooling approach&quot;,&#xA;    &quot;MI:1113&quot; =&amp;gt; &quot;two hybrid bait and prey pooling approach&quot;,&#xA;    &quot;MI:1137&quot; =&amp;gt; &quot;carboxylation assay&quot;,&#xA;    &quot;MI:1138&quot; =&amp;gt; &quot;decarboxylation assay&quot;,&#xA;    &quot;MI:1142&quot; =&amp;gt; &quot;aminoacylation assay&quot;,&#xA;    &quot;MI:1145&quot; =&amp;gt; &quot;phospholipase assay&quot;,&#xA;    &quot;MI:1147&quot; =&amp;gt; &quot;ampylation assay&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you also want those that might not be direct binary but simply in the same complex, include:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;      &quot;MI:0019&quot; =&amp;gt; &quot;coimmunoprecipitation&quot;,&#xA;      &quot;MI:0006&quot; =&amp;gt; &quot;anti bait coimmunoprecipitation&quot;,&#xA;      &quot;MI:0007&quot; =&amp;gt; &quot;anti tag coimmunoprecipitation&quot;,&#xA;      &quot;MI:0858&quot; =&amp;gt; &quot;immunodepleted coimmunoprecipitation&quot;,&#xA;      &quot;MI:0096&quot; =&amp;gt; &quot;pull down&quot;,&#xA;      &quot;MI:0963&quot; =&amp;gt; &quot;interactome parallel affinity capture&quot;,&#xA;      &quot;MI:0676&quot; =&amp;gt; &quot;tandem affinity purification&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-06-14T17:09:49.490" CommentCount="0" />
  <row Id="741" PostTypeId="1" AcceptedAnswerId="744" CreationDate="2017-06-14T21:02:35.707" Score="6" ViewCount="70" Body="&lt;p&gt;Google searching for &lt;strong&gt;NM_002084&lt;/strong&gt; gives the following result:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/nuccore/NM_002084&quot; rel=&quot;nofollow noreferrer&quot;&gt;NM_002084.4&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This, I assume, is the latest version v4, hence the &lt;code&gt;.4&lt;/code&gt; suffix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Searching for previous versions I get the following results, along with notes saying it was updated or removed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/nuccore/89903006&quot; rel=&quot;nofollow noreferrer&quot;&gt;NM_002084.3&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;This sequence has been updated. See current version.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?val=NM_002084.2&quot; rel=&quot;nofollow noreferrer&quot;&gt;NM_002084.2&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?val=NM_002084.1&quot; rel=&quot;nofollow noreferrer&quot;&gt;NM_002084.1&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Record removed. This record was replaced or removed.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Using &lt;em&gt;biomaRt&lt;/em&gt; I can get the latest(&lt;strong&gt;?&lt;/strong&gt;) version as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(&quot;biomaRt&quot;)&#xA;&#xA;# define db&#xA;ensembl &amp;lt;- useMart(&quot;ensembl&quot;, dataset = &quot;hsapiens_gene_ensembl&quot;)&#xA;&#xA;# get refseqs&#xA;getBM(attributes = c('refseq_mrna',&#xA;                     'chromosome_name',&#xA;                     'transcript_start',&#xA;                     'transcript_end',&#xA;                     'strand'),&#xA;      filters = c('refseq_mrna'),&#xA;      values = list(refseq_mrna = &quot;NM_002084&quot;),&#xA;      mart = ensembl)&#xA;#  refseq_mrna chromosome_name transcript_start transcript_end strand&#xA;#1   NM_002084               5        151020438      151028992      1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But querying for specific versions gives nothing:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;getBM(attributes = c('refseq_mrna',&#xA;                     'chromosome_name',&#xA;                     'transcript_start',&#xA;                     'transcript_end',&#xA;                     'strand'),&#xA;      filters = c('refseq_mrna'),&#xA;      values = list(refseq_mrna = c(&quot;NM_002084.1&quot;, &quot;NM_002084.2&quot;, &quot;NM_002084.3&quot;, &quot;NM_002084.4&quot;)),&#xA;      mart = ensembl)&#xA;# [1] refseq_mrna      chromosome_name  transcript_start transcript_end   strand          &#xA;# &amp;lt;0 rows&amp;gt; (or 0-length row.names)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; How can I get all versions (preferably using R)?&lt;/p&gt;&#xA;" OwnerUserId="131" LastEditorUserId="292" LastEditDate="2017-06-15T09:40:25.250" LastActivityDate="2017-06-15T10:17:12.697" Title="Get RefSeq accession numbers with versions" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;biomart&gt;&lt;refseq&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="742" PostTypeId="2" ParentId="694" CreationDate="2017-06-14T22:42:30.370" Score="1" Body="&lt;p&gt;I wrote a very quick and dirty script to handle conversion between file types using BioJava.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/eedlund/Utils/tree/master/BioUtils&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/eedlund/Utils/tree/master/BioUtils&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Download the jar file here:&#xA;&lt;a href=&quot;https://github.com/eedlund/Utils/raw/master/BioUtils/BioUtils.jar&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/eedlund/Utils/raw/master/BioUtils/BioUtils.jar&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To run:&#xA;java -jar BioUtils.jar $FILE $TYPE&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $FILE is a PDB or mmCIF file you'd like to convert and $TYPE is the format of the output file [PDB, CIF, MMTF].&lt;/p&gt;&#xA;" OwnerUserId="138" LastActivityDate="2017-06-14T22:42:30.370" CommentCount="1" />
  <row Id="743" PostTypeId="2" ParentId="350" CreationDate="2017-06-14T22:56:46.020" Score="4" Body="&lt;p&gt;If you choose to perform your own culling of the PDB, resolution is probably the first thing you'll want to look at, which as Davidmh mentions is the main selection criteria for PISCES. High quality structures will also have better R-factor values. You can also give preference based on experimental technique, in descending order of quality:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Neutron diffraction, X-ray diffraction, solution/solid state NMR, electron microscopy/crystallography, fiber diffraction, solution scattering.&lt;/p&gt;&#xA;" OwnerUserId="138" LastActivityDate="2017-06-14T22:56:46.020" CommentCount="3" />
  <row Id="744" PostTypeId="2" ParentId="741" CreationDate="2017-06-15T05:40:30.137" Score="5" Body="&lt;p&gt;I don't believe this is possible using &lt;code&gt;biomaRt&lt;/code&gt;, nor using &lt;code&gt;AnnotationHub&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two suggestions, neither of them very satisfactory. First, you can specify an Ensembl archive for &lt;code&gt;biomaRt&lt;/code&gt;, for example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mart72.hs &amp;lt;- useMart(&quot;ENSEMBL_MART_ENSEMBL&quot;, &quot;hsapiens_gene_ensembl&quot;, &#xA;                      host = &quot;jun2013.archive.ensembl.org&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Of course, that requires that you have some idea of the date for each accession version and that the archives span that date - so not especially useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other option is to access EUtils using &lt;em&gt;e.g.&lt;/em&gt; &lt;code&gt;rentrez&lt;/code&gt;, which does allow search by version number:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(rentrez)&#xA;es &amp;lt;- entrez_search(&quot;nuccore&quot;, &quot;NM_002084.1 NM_002084.2&quot;)&#xA;es$ids&#xA;&#xA;[1] &quot;4504104&quot; &quot;6006000&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So knowing the accession, you could simply append 1, 2, 3... to it, run the search and see if UIDs come back, then get them using &lt;code&gt;entrez_fetch&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: here's a quick and dirty function which takes an accession as query, appends version = 1, fetches the ID, then increments the version and repeats until no more results are returned. It is not well-tested!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;getVersions &amp;lt;- function(accession) {&#xA;  require(rentrez)&#xA;  ids &amp;lt;- character()&#xA;  version &amp;lt;- 1&#xA;  repeat({&#xA;    es &amp;lt;- entrez_search(&quot;nuccore&quot;, paste0(accession, &quot;.&quot;, version))&#xA;    if(length(es$ids) == 0) {&#xA;      break&#xA;    }&#xA;    ids[version] &amp;lt;- es$ids&#xA;    version &amp;lt;- version + 1&#xA;  })&#xA;  ids&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;getVersions(&quot;NM_002084&quot;)&#xA;[1] &quot;4504104&quot;    &quot;6006000&quot;    &quot;89903006&quot;   &quot;1048339180&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="150" LastEditorUserId="150" LastEditDate="2017-06-15T10:17:12.697" LastActivityDate="2017-06-15T10:17:12.697" CommentCount="4" />
  <row Id="745" PostTypeId="1" AcceptedAnswerId="749" CreationDate="2017-06-15T06:29:13.993" Score="3" ViewCount="75" Body="&lt;p&gt;The latest version of Albacore from Oxford Nanopore Technologies calls bases from raw fast5 files. A useful piece of output is the &lt;code&gt;sequence_summary.txt&lt;/code&gt;, which is a big tab-delimited file with information on each read. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the columns in there is the &lt;code&gt;channel&lt;/code&gt;, which references the channel number from 1 to 512 on the flowcell. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;These channels obviously have a physical layout on the flowcell. What is the layout? I.e. how many channels are there per row and per column, and how do the channel numbers map to the rows and columns?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My best guess is that it's a 16*32 array, but I don't know for sure, and even if I did I wouldn't know whether the &lt;code&gt;channels&lt;/code&gt; map across the shorter or the longer axis in order (or maybe they are not in order at all, but that would be silly).&lt;/p&gt;&#xA;" OwnerUserId="156" LastEditorUserId="156" LastEditDate="2017-06-15T06:34:33.327" LastActivityDate="2017-06-19T09:06:55.287" Title="Minion channel ID's from Albacore" Tags="&lt;nanopore&gt;&lt;minion&gt;&lt;albacore&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="746" PostTypeId="2" ParentId="745" CreationDate="2017-06-15T08:09:40.980" Score="1" Body="&lt;p&gt;&lt;a href=&quot;https://github.com/mw55309/poRe_docs&quot; rel=&quot;nofollow noreferrer&quot;&gt;poRe&lt;/a&gt; has a &lt;code&gt;show.layout()&lt;/code&gt; function which shows you the 32*16 grid on which the channels are arranged. EDIT: THIS APPEARS NOT TO BE THE CORRECT PHYSICAL LAYOUT.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]&#xA; [1,]  125  121  117  113  109  105  101   97   93    89    85    81    77&#xA; [2,]  126  122  118  114  110  106  102   98   94    90    86    82    78&#xA; [3,]  127  123  119  115  111  107  103   99   95    91    87    83    79&#xA; [4,]  128  124  120  116  112  108  104  100   96    92    88    84    80&#xA; [5,]  253  249  245  241  237  233  229  225  221   217   213   209   205&#xA; [6,]  254  250  246  242  238  234  230  226  222   218   214   210   206&#xA; [7,]  255  251  247  243  239  235  231  227  223   219   215   211   207&#xA; [8,]  256  252  248  244  240  236  232  228  224   220   216   212   208&#xA; [9,]  381  377  373  369  365  361  357  353  349   345   341   337   333&#xA;[10,]  382  378  374  370  366  362  358  354  350   346   342   338   334&#xA;[11,]  383  379  375  371  367  363  359  355  351   347   343   339   335&#xA;[12,]  384  380  376  372  368  364  360  356  352   348   344   340   336&#xA;[13,]  509  505  501  497  493  489  485  481  477   473   469   465   461&#xA;[14,]  510  506  502  498  494  490  486  482  478   474   470   466   462&#xA;[15,]  511  507  503  499  495  491  487  483  479   475   471   467   463&#xA;[16,]  512  508  504  500  496  492  488  484  480   476   472   468   464&#xA;      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]&#xA; [1,]    73    69    65    61    57    53    49    45    41    37    33    29&#xA; [2,]    74    70    66    62    58    54    50    46    42    38    34    30&#xA; [3,]    75    71    67    63    59    55    51    47    43    39    35    31&#xA; [4,]    76    72    68    64    60    56    52    48    44    40    36    32&#xA; [5,]   201   197   193   189   185   181   177   173   169   165   161   157&#xA; [6,]   202   198   194   190   186   182   178   174   170   166   162   158&#xA; [7,]   203   199   195   191   187   183   179   175   171   167   163   159&#xA; [8,]   204   200   196   192   188   184   180   176   172   168   164   160&#xA; [9,]   329   325   321   317   313   309   305   301   297   293   289   285&#xA;[10,]   330   326   322   318   314   310   306   302   298   294   290   286&#xA;[11,]   331   327   323   319   315   311   307   303   299   295   291   287&#xA;[12,]   332   328   324   320   316   312   308   304   300   296   292   288&#xA;[13,]   457   453   449   445   441   437   433   429   425   421   417   413&#xA;[14,]   458   454   450   446   442   438   434   430   426   422   418   414&#xA;[15,]   459   455   451   447   443   439   435   431   427   423   419   415&#xA;[16,]   460   456   452   448   444   440   436   432   428   424   420   416&#xA;      [,26] [,27] [,28] [,29] [,30] [,31] [,32]&#xA; [1,]    25    21    17    13     9     5     1&#xA; [2,]    26    22    18    14    10     6     2&#xA; [3,]    27    23    19    15    11     7     3&#xA; [4,]    28    24    20    16    12     8     4&#xA; [5,]   153   149   145   141   137   133   129&#xA; [6,]   154   150   146   142   138   134   130&#xA; [7,]   155   151   147   143   139   135   131&#xA; [8,]   156   152   148   144   140   136   132&#xA; [9,]   281   277   273   269   265   261   257&#xA;[10,]   282   278   274   270   266   262   258&#xA;[11,]   283   279   275   271   267   263   259&#xA;[12,]   284   280   276   272   268   264   260&#xA;[13,]   409   405   401   397   393   389   385&#xA;[14,]   410   406   402   398   394   390   386&#xA;[15,]   411   407   403   399   395   391   387&#xA;[16,]   412   408   404   400   396   392   388&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="681" LastEditorUserId="681" LastEditDate="2017-06-19T09:06:55.287" LastActivityDate="2017-06-19T09:06:55.287" CommentCount="3" />
  <row Id="747" PostTypeId="1" CreationDate="2017-06-15T08:49:32.977" Score="6" ViewCount="57" Body="&lt;p&gt;Since I have seen NCBI gene names were called &quot;Entrez ID&quot; for the first time, I am wondering where that comes from. Such a weird name!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anybody know where that originates?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My hypothesis is: in French, &quot;entrez&quot; can be translated to &quot;please enter&quot; (it is &lt;code&gt;to enter&lt;/code&gt; in the imperative mood), so I'm guessing that might come from a past website/database in French where &lt;code&gt;Entrez ID&lt;/code&gt; as in &quot;Please enter the ID&quot; was written in the search bar...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I could not find any reference... I cannot be the only one to ask himself this!&lt;/p&gt;&#xA;" OwnerUserId="868" LastActivityDate="2017-06-15T14:40:09.540" Title="What is the etymology of &quot;Entrez ID&quot;?" Tags="&lt;gene&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="748" PostTypeId="2" ParentId="747" CreationDate="2017-06-15T09:03:44.387" Score="1" Body="&lt;h2&gt;Update&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/747/what-is-the-etymology-of-entrez-id/748#comment1340_747&quot;&gt;As pointed out by neilfws&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to &lt;a href=&quot;https://en.wikipedia.org/wiki/Entrez&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikipedia&lt;/a&gt; and Practical bioinformatics by Michael Agostino: The name &quot;Entrez&quot; means &quot;Come in!&quot; in French (pronounced “on-tray”); it was chosen to reflect the spirit of welcoming the public to search the content.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Old (probably incorrect) theory&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The first version of Entrez database was &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK3837/&quot; rel=&quot;nofollow noreferrer&quot;&gt;distributed&lt;/a&gt; by NCBI in 1991 (on CD-ROM). At that time, it consisted of nucleotide sequences, protein sequences and associated citations and abstracts from MEDLINE. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So even at the early stage it contained objects (entries) of very different types (sequences and abstracts). It is hard to find a word which will accurately and briefly describe all the types of data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While I don't know the correct answer and this is purely a speculation, my guess is that Entrez is an intentionally misspelled word &quot;entries&quot;; which accurately describes all the datatypes in the database.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using a misspelled word as a name is not uncommon, e.g. Google comes from misspelled &quot;googol&quot; and Flickr is a misspelled version of &quot;flicker&quot;.&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-06-15T14:40:09.540" LastActivityDate="2017-06-15T14:40:09.540" CommentCount="1" />
  <row Id="749" PostTypeId="2" ParentId="745" CreationDate="2017-06-15T10:08:22.437" Score="2" Body="&lt;p&gt;There are actually 2048 usable sequencing wells, hexagonally packed with four wells connected to the same sequencing sensor/channel via a multiplex (mux) selector. The combination of the mux and the channel number determines the physical location of the well. Unfortunately, the association between channel number and physical location is not obvious, and within each channel the muxes are also not in an obvious order, i.e. &lt;code&gt;[3,4,1,2,2,1,4,3]&lt;/code&gt; for two adjacent channels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's an &quot;official&quot; text description of the layout as it was for R7; it hasn't changed for the most recent flow cells (R9.5):&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Channels 1-64 occur at the top of the chip (the other low channel numbers are at the bottom)&lt;/li&gt;&#xA;&lt;li&gt;Channels order down the chip: 1-64, 449-512, 385-448, 321-384, 257-320, 193-256, 129-192, 65-128&lt;/li&gt;&#xA;&lt;li&gt;Muxes run from left to right in the order: 3, 4, 1, 2, 2, 1, 4, 3&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;To confirm this layout, I pulled the channel/position lookup table definitions out of MinKNOW, and it seems to be the same as this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/AGcU3.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/AGcU3.jpg&quot; alt=&quot;Current MinION channel layout&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the code to produce this image (I haven't adjusted it to have a hexagonal structure):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/Rscript&#xA;library(rjson);&#xA;&#xA;layout.fname &amp;lt;- &quot;/opt/ONT/MinKNOW/Client/resources/app/resources/channels.json&quot;;&#xA;&#xA;data.list &amp;lt;- fromJSON(file=layout.fname);&#xA;&#xA;png(&quot;MinION_FC_Layout.png&quot;, width=3072, height=1536, pointsize=24);&#xA;par(mar=c(4,4,0.5,0.5), lwd=3);&#xA;data.mat &amp;lt;- matrix(unlist(data.list), ncol=2, byrow=TRUE);&#xA;plot(data.mat, pch=21,&#xA;     bg=(colorRampPalette(c(&quot;white&quot;,&quot;grey&quot;))(2048))[1:2048],&#xA;     col=c(&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;,&quot;blue&quot;),&#xA;     xlab=&quot;locX&quot;, ylab=&quot;locY&quot;, cex=2,&#xA;     xlim=c(7,76), ylim=c(0,33));&#xA;text(x=data.mat, labels=rep(1:512, each=4), cex=0.4);&#xA;legend(&quot;top&quot;,legend=c(1,2,3,4), fill=c(&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;,&quot;blue&quot;),&#xA;       inset=0.05, title=&quot;MUX&quot;);&#xA;text(x=6, y=data.mat[data.mat[,1] == 8,2],&#xA;     labels = ceiling(which(data.mat[,1] == 8) / 4));&#xA;text(x=77, y=data.mat[data.mat[,1] == 75,2],&#xA;     labels = ceiling(which(data.mat[,1] == 75) / 4));&#xA;arrows(x0=c(8,75), x1=c(39,44), y0=33, length=0.2);&#xA;invisible(dev.off());&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-16T05:49:50.170" LastActivityDate="2017-06-16T05:49:50.170" CommentCount="3" />
  <row Id="750" PostTypeId="1" CreationDate="2017-06-15T10:31:01.647" Score="1" ViewCount="40" Body="&lt;p&gt;I saw &lt;a href=&quot;http://www.nature.com/news/us-cancer-institute-to-overhaul-tumour-cell-lines-1.19364&quot; rel=&quot;nofollow noreferrer&quot;&gt;this nature news item&lt;/a&gt;, it sounds that &lt;a href=&quot;https://discover.nci.nih.gov/cellminer/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cellminer&lt;/a&gt; is obsolete, is it right?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the new tools to analyze the &quot;&lt;a href=&quot;https://www.jax.org/jax-mice-and-services/in-vivo-pharmacology/oncology-services/pdx-tumors#&quot; rel=&quot;nofollow noreferrer&quot;&gt;new cell lines&lt;/a&gt;&quot;? Where's the PDX repository?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What about the European initiative that the article refers to? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would be happy to see correlation between drugs and genes and similar capabilities as we can see on Cellminer. &lt;/p&gt;&#xA;" OwnerUserId="734" LastEditorUserId="734" LastEditDate="2017-06-20T02:35:28.307" LastActivityDate="2017-06-20T02:35:28.307" Title="How to access the patient-derived xenografts (PDXs) repository?" Tags="&lt;cancer&gt;&lt;cell-line&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="751" PostTypeId="1" AcceptedAnswerId="753" CreationDate="2017-06-15T11:13:40.643" Score="7" ViewCount="282" Body="&lt;p&gt;I'm going through an RNA-seq pipeline in &lt;code&gt;R/Bioconductor&lt;/code&gt; and want to try multiple parameters at subsequent steps, for example, running clustering with different settings, running RegressOut or not on unwanted effects etc. That's a lot of &quot;versions&quot;, even if I don't do combinations of these steps.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I keep track of this, and my conclusions?&#xA;Not necessarily want to save the results.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Save the different scripts with git (seems overkill)&lt;/li&gt;&#xA;&lt;li&gt;Make notes in the script itself&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="208" LastEditorUserId="292" LastEditDate="2017-06-15T15:06:38.017" LastActivityDate="2017-06-15T15:57:32.463" Title="What are the ways to keep track of branches in the analysis?" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;git&gt;&lt;workflow-management&gt;&lt;best-practice&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" />
  <row Id="752" PostTypeId="1" CreationDate="2017-06-15T11:56:51.837" Score="1" ViewCount="49" Body="&lt;p&gt;Is there a vcf file on the GRCh38 assembly with common cancer mutations I can download somewhere? Maybe from one of the big international cancer genomics consortia?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By common, I mean whichever mutations have been found recurrent in different types of cancer. &lt;/p&gt;&#xA;" OwnerUserId="180" LastActivityDate="2017-06-18T17:04:43.097" Title="GRCh38 vcf file with common cancer mutations" Tags="&lt;vcf&gt;&lt;cancer&gt;" AnswerCount="3" CommentCount="2" />
  <row Id="753" PostTypeId="2" ParentId="751" CreationDate="2017-06-15T12:26:26.903" Score="4" Body="&lt;p&gt;The main purpose of git is to version code, which usually means sequential improvement of the codebase. While it is possible to use branches for multiple variants of the software, permanent branches are traditionally used for gradual integration of new features (i.e. dev/testing/master branches). Supporting multiple independent branches requires some investment, i.e. distributing common changes among branches via merge or cherry-pick. This is hard to manage when you have more than two-three branches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you compare different methods of analysis, you probably want to compare the results between methods. Having them on separate branches makes it hard.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my opinion you should integrate all methods of analysis into the master branch. To avoid copy &amp;amp; paste, it is better to put common code in a library or an independent script. You can also specify a method as a run-time parameter of your pipeline, and create a meta-script which will execute all methods of interests.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you performed benchmarking, you shouldn't remove unused methods from you master branch. Having them is important for reproducible research, and your scripts could be used in the future for new datasets.&lt;/p&gt;&#xA;" OwnerUserId="191" LastActivityDate="2017-06-15T12:26:26.903" CommentCount="2" />
  <row Id="754" PostTypeId="2" ParentId="751" CreationDate="2017-06-15T12:47:47.027" Score="8" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;Save the different scripts with git (seems overkill)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Whoa. I did an actual double take when reading this:&lt;sup&gt;1&lt;/sup&gt; it’s the &lt;em&gt;opposite&lt;/em&gt; of overkill.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Version controlling your scripts (using Git or something similar) is the absolute minimum, and should become completely automatic. For every new project I begin, one of the very first steps is to issue the &lt;code&gt;git init&lt;/code&gt; command, and to set up a remote repository (on Github).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To keep track of &lt;em&gt;different&lt;/em&gt; analyses, I use a combination of the following approaches:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Write reusable functions/scripts and parametrise. The parameters are kept either inside the script itself (that then calls the relevant function repeatedly), or in a Makefile (I recommend &lt;a href=&quot;https://snakemake.readthedocs.io/en/stable/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Snakemake&lt;/a&gt;).&lt;/li&gt;&#xA;&lt;li&gt;Document the alternative analysis approaches; once again, this could be a Makefile with different rules for alternative analyses, or a set of notebooks (via &lt;a href=&quot;http://rmarkdown.rstudio.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;R Markdown&lt;/a&gt;).&lt;/li&gt;&#xA;&lt;li&gt;Have different Git branches for mutually exclusive approaches. At the end of the analysis one of these branches gets merged into &lt;code&gt;master&lt;/code&gt;, and published. If I want to publish several analysis approaches, I merge all these branches into &lt;code&gt;master&lt;/code&gt;, and use approaches (1) or (2) enable them simultaneously.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In fact, I recommend creating a Makefile for &lt;em&gt;every&lt;/em&gt; analysis; I have found that this is the most practical, self-documenting way to run an analysis. It most closely resembles a wet-lab lab notebook. The advantage over a single R Markdown document is that rerunning just parts of the analysis can be completely automated, and dependencies in the workflow are apparent from the dependencies of the Makefile rules. This is much harder in R Markdown.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some time ago I create an &lt;a href=&quot;https://github.com/klmr/example-r-analysis&quot; rel=&quot;nofollow noreferrer&quot;&gt;example analysis workflow&lt;/a&gt; to show how this can be structured. Nowadays I would use Snakemake instead of GNU make.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding your other point:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Make notes in the script itself&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;“Notes” are a dangerous beasts: documentation is important, but experience shows that it’s sometimes very hard to keep documentation synchronised with the code. There is no mechanism that ensures that documentation and code actually agree. Differences between presumed analysis and actually executed analysis can become very problematic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;People therefore prefer using self-documenting code as much as possible; that is: writing code so that its meaning becomes immediately clear, without comments, even to somebody who hasn’t worked on the code before. Doing this well is hard and takes practice, but improves overall code quality. Once again, using a Makefile helps here because the dependencies between the rules are self-documenting the kind of analysis that was performed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Jeff Atwood has written two seminal essays on this subject:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://blog.codinghorror.com/coding-without-comments/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Coding without comments&lt;/a&gt;, and&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://blog.codinghorror.com/code-tells-you-how-comments-tell-you-why/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Code tells you how, comments tell you why&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;They are two of the best pieces of advice on programming that I could give.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; To emphasise: take a look at the &lt;a href=&quot;https://bioinformatics.stackexchange.com/revisions/754/1&quot;&gt;edit history&lt;/a&gt; of this answer.&lt;/p&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-06-15T15:57:32.463" LastActivityDate="2017-06-15T15:57:32.463" CommentCount="3" />
  <row Id="755" PostTypeId="2" ParentId="752" CreationDate="2017-06-15T12:55:47.280" Score="1" Body="&lt;p&gt;Your question doesn't give enough information for a specific answer but this should do for a start.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Get the VCF file describing all variants in Clinvar from NCBI:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Extract any variants whose VCF line contains the word &quot;cancer&quot;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;zgrep -iE '^#|CLNDBN=[^;]*cancer' clinvar.vcf.gz &amp;gt; cancer.vcf&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Now, you should probably filter that further based on variant frequency, type of cancer, etc etc but that should be a good start. &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-06-15T12:55:47.280" CommentCount="0" />
  <row Id="756" PostTypeId="1" CreationDate="2017-06-15T13:09:16.177" Score="2" ViewCount="49" Body="&lt;p&gt;I want to find all experiments in GEO that are associated with a drug (for example tolvaptan). Is there any quick and scalable way to to this? I want to query more than 100 drugs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to use bioDBnet to map Drug Bank ID to look up data sets, but I don't know what output format should I select.&lt;/p&gt;&#xA;" OwnerUserId="872" LastEditorUserId="57" LastEditDate="2017-06-15T17:48:38.077" LastActivityDate="2017-07-15T20:02:21.063" Title="How to find GEO data sets using Drug Bank ID and bioDBnet" Tags="&lt;database&gt;&lt;gene&gt;&lt;drugs&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="757" PostTypeId="2" ParentId="752" CreationDate="2017-06-15T13:22:38.700" Score="1" Body="&lt;p&gt;The trouble with looking for cancer-associated variants is that it can be difficult to tease out spurious effects (e.g. ethnicity) from causative variants. If you're interested in what &lt;em&gt;genes&lt;/em&gt; are implicated in most types of cancer, the annotations for the NanoString panels are quite good:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.nanostring.com/products/gene-expression-panels/hallmarks-cancer-gene-expression-panel-collection/pancancer-pathways-panel&quot; rel=&quot;nofollow noreferrer&quot;&gt;cancer pathways&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.nanostring.com/products/gene-expression-panels/hallmarks-cancer-gene-expression-panel-collection/pancancer-progression-panel&quot; rel=&quot;nofollow noreferrer&quot;&gt;cancer progression&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The &quot;Support documents&quot; section of those pages has a link to an Excel file with gene lists and probe sequences. While these lists don't give information about variants that disrupt function, I'm struggling a bit to think through why it would be useful in advance to know that, unless the aim is to induce cancer through CRISPR or something like that.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-15T13:22:38.700" CommentCount="0" />
  <row Id="758" PostTypeId="2" ParentId="731" CreationDate="2017-06-15T13:45:38.370" Score="1" Body="&lt;p&gt;I have used STRING pretty heavily, and have compared it to various other databases of protein interactions and signaling pathways.  I do feel like it has a lot of quality interaction annotations, but you have to sift through a lot of noise to get to them.  The simplest method I have found for doing this is to look at the individual scores for each interaction, and accept it if it passes one of the following tests:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Experiment Score &gt; 0.4&lt;/li&gt;&#xA;&lt;li&gt;Database Score &gt; 0.9&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Anything that passes one of these thresholds we consider at least an interaction of acceptable quality.  Those interactions with Experiment Scores &gt; 0.9 are high-quality, and have been experimentally validated.  The other scores represent inaccurate methods for determining signaling events, and should be ignored.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You are not going to catch every actual protein signaling event this way, but you will at least be spared a lot of false positives.  The best way to construct an actual network of signaling events is to combine interaction records from multiple databases.&lt;/p&gt;&#xA;" OwnerUserId="47" LastEditorUserId="47" LastEditDate="2017-06-15T13:55:11.447" LastActivityDate="2017-06-15T13:55:11.447" CommentCount="0" />
  <row Id="759" PostTypeId="2" ParentId="751" CreationDate="2017-06-15T15:01:31.443" Score="0" Body="&lt;p&gt;I quite agree with &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/754/292&quot;&gt;this answer&lt;/a&gt; by @Konrad Rudolph.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to emphasis that using parametrization for your scripts is what will help you avoid multiplying branches in git. So yes, use git, but you don't necessarily have to go &quot;overkill&quot; creating lots of branches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, I would command these scripts from a workflow managment tool that will somehow take care of generating the branches of your analyses. If you use Snakemake, the various options taken along the path from your data to your results will be represented by the wildcards system, and this will be visible in the structure of your folders and file names, due to the fact that Snakemake works by inferring what should be done to produce a file based on its name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This of course is not an excuse for not using other documentation approaches: Comments in the Snakefile and in the scripts, README files explaining how the workflow was run, using what configuration file. Put your scripts, the Snakefile, its configuration files and the README files under version control and document again using commit messages.&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-06-15T15:01:31.443" CommentCount="0" />
  <row Id="760" PostTypeId="2" ParentId="361" CreationDate="2017-06-15T15:47:20.327" Score="1" Body="&lt;p&gt;For FASTA files, I've implemented a relatively efficient method in &lt;a href=&quot;https://github.com/mdshw5/pyfaidx&quot; rel=&quot;nofollow noreferrer&quot;&gt;pyfaidx&lt;/a&gt; v0.4.9.1. This post made me realize that my previous code was quite slow and easy to replace:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ pip install pyfaidx&#xA;$ time faidx -i nucleotide ~/Downloads/hg38.fa&#xA;name    start   end A   T   C   G   N   others&#xA;chr1    1   248956422   67070277    67244164    48055043    48111528    18475410    &#xA;chr10   1   133797422   38875926    39027555    27639505    27719976    534460  &#xA;chr11   1   135086622   39286730    39361954    27903257    27981801    552880  &#xA;chr11_KI270721v1_random 1   100316  18375   19887   31042   31012   0   &#xA;...&#xA;chrX    1   156040895   46754807    46916701    30523780    30697741    1147866 &#xA;chrY    1   57227415    7886192 7956168 5285789 5286894 30812372    &#xA;chrY_KI270740v1_random  1   37240   8274    12978   7232    8756    0   &#xA;&#xA;real    2m28.251s&#xA;user    2m15.548s&#xA;sys 0m9.951s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="267" LastActivityDate="2017-06-15T15:47:20.327" CommentCount="0" />
  <row Id="761" PostTypeId="1" CreationDate="2017-06-15T17:51:10.583" Score="2" ViewCount="50" Body="&lt;p&gt;I am looking at the presence of viral genotypes within individual samples within an assay. Often times there is a sample whose read counts are firing off the charts and this sample tends to &quot;bleed through&quot; to the other samples. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have recently tried confronting this problem with chi square analysis of conditional probability. For instance let's say I observe with 10 sample viral genotypes A and B. I then calculate the expected number of samples having both genotypes A and B from the number of A positive samples and number of B positive samples. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far this is all I have. I do not know if anyone else has any ideas on how to determine and quantify bleed through in viral genotyping. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Slightly more information about this situation: these samples were run on Illumina HiSeq, we are planning on doing another HiSeq run with even more samples shortly so would like to take proper precautions with that, additionally the current pipeline utilizes Novobarcode as a demultiplexer so if anyone has suggestions on what I can implement during processing to minimize the effect of cross contamination that'd be helpful. &lt;/p&gt;&#xA;" OwnerUserId="712" LastEditorUserId="712" LastEditDate="2017-06-16T16:00:47.170" LastActivityDate="2017-06-16T16:00:47.170" Title="Techniques for analyzing and quantifying sample bleed through in genotyping with Illumina" Tags="&lt;genotyping&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="762" PostTypeId="2" ParentId="756" CreationDate="2017-06-15T19:10:23.020" Score="1" Body="&lt;p&gt;One of the problems I've found with GEO and even ArrayExpress is that it seems like there are tons of false positives that come up in the search. Additionally, there may be a ton of results that you miss because you didn't include the synonym of the drug. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to make sure I have all the information relating to name, etc, you can write a script which downloads the result of a search in PubChem (this can be done with drugbank as well I think) and then parse the search result, extracting important information such as synonyms of drug name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, take those synonyms and use them in an Entrez (EUtils) search for the experiments. Then try to filter the results to keep only what you're actually looking for. One bad way to do this is just by making sure the experiment summary contains the term your looking for. Another way to potentially do this, for example if there is a large experiment which tests serveral different factors is to programmatically search through associated files to ensure they contain a desired keyword. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You'll get different experiment types and so the files associated with the results will depend on the type of experiment in the search result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When you get have a list of IDs of the experiments you want, you can feed them through to GEOquery in Bioconductor.&lt;/p&gt;&#xA;" OwnerUserId="823" LastActivityDate="2017-06-15T19:10:23.020" CommentCount="0" />
  <row Id="763" PostTypeId="2" ParentId="567" CreationDate="2017-06-15T19:21:31.780" Score="2" Body="&lt;p&gt;If you run this Rscript using the gene name as an argument, you'll get a file with the pathway written to the directory.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/Rscript&#xA;args = commandArgs(trailingOnly=TRUE)&#xA;library(paxtoolsr)&#xA;id &amp;lt;- args[1]&#xA;write.table(graphPc(source=id,kind='neighborhood',&#xA;            format='BINARY_SIF',verbose=TRUE), &#xA;            file=paste(id,'pathway',sep='_'), sep='\t',quote=FALSE)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="823" LastActivityDate="2017-06-15T19:21:31.780" CommentCount="0" />
  <row Id="764" PostTypeId="2" ParentId="761" CreationDate="2017-06-15T20:17:45.203" Score="1" Body="&lt;p&gt;Are you using dual-index barcodes with different barcodes at each end? There is a known phenomena of &quot;index switching&quot; that occurs in Illumina reads. One way to control for this is to add in unamplified water samples that have unused barcode combinations where each of the indexes is shared with another sample.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More information can be found on &lt;a href=&quot;http://seqanswers.com/forums/showthread.php?p=205928&quot; rel=&quot;nofollow noreferrer&quot;&gt;this SeqAnswers thread&lt;/a&gt;. Here are some useful associated links:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.biorxiv.org/content/early/2017/04/09/125724&quot; rel=&quot;nofollow noreferrer&quot;&gt;Signal-spreading preprint&lt;/a&gt; -- detailed investigation of the phenomenom, including methods, graphs, and associated data&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://seqanswers.com/forums/showthread.php?t=73736&quot; rel=&quot;nofollow noreferrer&quot;&gt;Crossblock&lt;/a&gt; -- tool by Brian Bushnell that attempts to remove cross contamination&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://seqanswers.com/forums/showthread.php?p=205928&quot; rel=&quot;nofollow noreferrer&quot;&gt;Summary by James Hadfield&lt;/a&gt; -- mentions a few fixes suggested by Illumina, many centred around not using barcode combinations that could conflict&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.illumina.com/content/dam/illumina-marketing/documents/products/whitepapers/index-hopping-white-paper-770-2017-004.pdf?linkId=36607862&quot; rel=&quot;nofollow noreferrer&quot;&gt;Illumina's quick-fire response&lt;/a&gt; -- they've known about it for a long time, and point out that it shouldn't be a problem for most purposes&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I have suspicions that at least some of the index switching is being caused by ligation of two separate barcoded fragments during the sample prep (which definitely happens &lt;a href=&quot;https://f1000research.com/articles/6-631/v1&quot; rel=&quot;nofollow noreferrer&quot;&gt;during sample prep for nanopore&lt;/a&gt;), but I haven't done any experimentation to work out if this is also the case for Illumina (due to money/time/goal constraints). The experimental design would be to take a prepared Illumina library just prior to sequencing, ligate on ONT adapters, then sequence on both the ONT and Illumina machines to see if chimeric reads discovered via nanopore sufficiently matches the index switching observed via short-read sequencing.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-15T23:54:16.543" LastActivityDate="2017-06-15T23:54:16.543" CommentCount="1" />
  <row Id="765" PostTypeId="1" CreationDate="2017-06-16T02:59:51.210" Score="-1" ViewCount="69" Body="&lt;p&gt;At which sites on the Drosophila melanogaster CG2316 mRNA do the RNAi`s: 12170/FBti0089992, 12168/FBtp0030589, 107343/FBtp0042163 and 41984/FBst0034349 (stock numbers/flybase ID) cleave? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And how did you work this out?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This answer will be greatly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is not a homework question but simply a matter of interest. I am considering designing an experiment that requires capabilities I am yet to master. I have a decent understanding of cell, genetic and neurobiology but can't deny my weaknesses in bioinformatics, computer programming and statistics. I could not think of a more efficient way to learn it though than by watching and copying more proficient people.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I attempted the question some time ago but with limited success. I found the Fasta sequence of CG2316 and searched for the two complementary sites of the RNAi hairpin using the word doc find function. I feel this rudimentary method was flawed on a few levels, not least because I only got a single hit. As such I have posted the problem here.&lt;/p&gt;&#xA;" OwnerUserId="881" LastEditorUserId="77" LastEditDate="2017-06-16T08:47:12.393" LastActivityDate="2017-07-27T21:31:05.903" Title="Can you tell me where these RNAi’s cleave on the Drosophila melanogaster CG2316 mRNA?" Tags="&lt;fasta&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="767" PostTypeId="2" ParentId="750" CreationDate="2017-06-16T10:12:05.340" Score="2" Body="&lt;p&gt;I already see for links when I try to search. Take a look at them below. Also if you can be precise about kind of data are you trying to find, it would be better&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://pdmr.cancer.gov/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pdmr.cancer.gov/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://cdt.northwestern.edu/news/patient-derived-xenograft-repository-now-available-researchers&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://cdt.northwestern.edu/news/patient-derived-xenograft-repository-now-available-researchers&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.epo-berlin.com/epo-tumor-models-xenografts.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.epo-berlin.com/epo-tumor-models-xenografts.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.proxe.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.proxe.org/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://data-analysis.charite.de/care/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://data-analysis.charite.de/care/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.oasis-genomics.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.oasis-genomics.org/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.cbioportal.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.cbioportal.org/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hope one of these above will serve the purpose for you.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update: added some more links that might serve. As for the Cellminer the last update was 2012.&lt;/p&gt;&#xA;" OwnerUserId="451" LastEditorUserId="451" LastEditDate="2017-06-16T12:18:46.110" LastActivityDate="2017-06-16T12:18:46.110" CommentCount="2" />
  <row Id="768" PostTypeId="2" ParentId="567" CreationDate="2017-06-16T10:50:45.063" Score="1" Body="&lt;p&gt;Why not just use string-db's &lt;a href=&quot;http://string-db.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;online tool&lt;/a&gt;? There you can adjust all parameters of interest such as interaction confidence as well as pull in extra protein interactions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you make a user you can also upload a custom background gene-set set which means the analysis of go-terms and interconnectedness that sting provides is usable.&lt;/p&gt;&#xA;" OwnerUserId="599" LastActivityDate="2017-06-16T10:50:45.063" CommentCount="0" />
  <row Id="769" PostTypeId="1" AcceptedAnswerId="786" CreationDate="2017-06-16T10:53:49.660" Score="3" ViewCount="120" Body="&lt;p&gt;I'm looking to subset a standard VCF file to generate one which only includes insertions (i.e. not indels). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can get part of the way there with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bcftools view -v indels &amp;lt;vcf&amp;gt; | awk '{if(length($4) == 1) print}'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However this wouldn't catch an insertion that was part of a multi-allelic record with an insertion and deletion, where the reference length would be greater than 1 bp. Potentially then one way to go is a chain of decomposition, normalisation and then the same reference length filtering — surely there's a better way in one of the many VCF manipulation utils?&lt;/p&gt;&#xA;" OwnerUserId="61" LastEditorUserId="61" LastEditDate="2017-06-16T12:28:03.227" LastActivityDate="2017-06-16T23:03:39.673" Title="How can I extract only insertions from a VCF file?" Tags="&lt;vcf&gt;" AnswerCount="4" CommentCount="3" />
  <row Id="770" PostTypeId="2" ParentId="367" CreationDate="2017-06-16T11:01:15.400" Score="4" Body="&lt;p&gt;For the effective length part please see to Devons answer. I just have a small addition: Kallisto/Salmon/RSEM incorporate all bias estimates into the effective length meaning the effective length not only represent the length bias if you take the values from those tools (given that they were run with the bias algorithms enabled naturally).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With regards to getting gene level estimates you should not choose a specific transcript. Instead you should extract/calculate the RPKM/FPKM/TxPM (transcript per million that Kallisto/Salmon/RSEM outputs) for each transcript and sum them up to get the gene level estimate.&lt;/p&gt;&#xA;" OwnerUserId="599" LastActivityDate="2017-06-16T11:01:15.400" CommentCount="0" />
  <row Id="771" PostTypeId="1" AcceptedAnswerId="775" CreationDate="2017-06-16T11:22:34.610" Score="1" ViewCount="86" Body="&lt;p&gt;Provide an overview of 10x data analysis packages.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;10x provides Cell Ranger which prepares a count matrix from the bcl sequencer output files and other files (see bottom of page &lt;a href=&quot;https://support.10xgenomics.com/docs/license&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://support.10xgenomics.com/docs/license&lt;/a&gt; for the programs it uses).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can we do with the output files?&lt;/p&gt;&#xA;" OwnerUserId="208" LastActivityDate="2017-06-16T14:26:03.450" Title="10x Genomics Chromium single-cell RNA-seq data analysis options?" Tags="&lt;rna-seq&gt;&lt;r&gt;&lt;bioconductor&gt;&lt;scrnaseq&gt;&lt;10x-genomics&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="772" PostTypeId="2" ParentId="769" CreationDate="2017-06-16T11:24:00.487" Score="2" Body="&lt;p&gt;One method is to decompose multi-allelic records so that they're represented as one-allele, one-record using &lt;a href=&quot;https://github.com/atks/vt&quot; rel=&quot;nofollow noreferrer&quot;&gt;vt&lt;/a&gt; or similar:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bcftools view -v indels &amp;lt;vcf&amp;gt; |&#xA;  vt decompose - |&#xA;  bcftools view -H |&#xA;  awk '{if(length($5)&amp;gt;length($4)) print}'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Some decomposed alleles will come with excess reference padding. To left-shift and trim these, add a &lt;code&gt;normalize&lt;/code&gt; step (NB matching these back to your input VCF becomes non-trivial):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bcftools view -v indels &amp;lt;vcf&amp;gt; |&#xA;  vt decompose - |&#xA;  vt normalize -r &amp;lt;reference.fasta - |&#xA;  awk '{if(length($4)==1) print}'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;edit:&lt;/strong&gt; As gringer suggests, this can also be done without vt:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bcftools view -Ou -v indels &amp;lt;vcf&amp;gt; |&#xA;  bcftools norm -Ou -Nm - |&#xA;  bcftools view -H |&#xA;  awk '{if(length($5)&amp;gt;length($4)) print}'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To also include complex alleles, use &lt;code&gt;view -V snps&lt;/code&gt; (!snvs) instead of &lt;code&gt;-v indels&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="61" LastEditorUserId="61" LastEditDate="2017-06-16T14:21:01.260" LastActivityDate="2017-06-16T14:21:01.260" CommentCount="1" />
  <row Id="773" PostTypeId="2" ParentId="769" CreationDate="2017-06-16T11:24:31.330" Score="1" Body="&lt;p&gt;Using &lt;a href=&quot;http://lindenb.github.io/jvarkit/VCFFilterJS.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;vcffilterjs&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;get the length of the REF;&lt;/li&gt;&#xA;&lt;li&gt;loop over the ALT, ignore the symbolic&lt;/li&gt;&#xA;&lt;li&gt;accept the variant if it's an insertion , eq:  len(ALT)&gt;len(REF) &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java -jar dist/vcffilterjs.jar -e 'function accept(vc){var a=vc.getAlleles();var lenRef=a.get(0).length();for(i=1;i&amp;lt;a.size();++i) {var alt=a.get(i);if(alt.isSymbolic()) continue;var lenAlt=alt.length(); if(lenRef&amp;lt;lenAlt) return true; } return false; }accept(variant);' input.vcf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;" OwnerUserId="71" LastActivityDate="2017-06-16T11:24:31.330" CommentCount="0" />
  <row Id="774" PostTypeId="1" CreationDate="2017-06-16T11:31:55.983" Score="2" ViewCount="41" Body="&lt;p&gt;Q:&#xA;&quot;Recent&quot; breakthrough in bioinformatics tools for quantification (e.g.&#xA;&lt;a href=&quot;http://cole-trapnell-lab.github.io/cufflinks/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cufflinks&lt;/a&gt;/&lt;a href=&quot;http://pachterlab.github.io/kallisto/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kallisto&lt;/a&gt;/&lt;a href=&quot;https://github.com/COMBINE-lab/salmon&quot; rel=&quot;nofollow noreferrer&quot;&gt;Salmon&lt;/a&gt; etc.) and tools which can identify differential transcript usage (DTU) (e.g. &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/DRIMSeq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DRIMSeq&lt;/a&gt;, &lt;a href=&quot;http://cole-trapnell-lab.github.io/cufflinks/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cufflinks&lt;/a&gt; etc.) means that from RNA-seq data we can now relatively easy obtain a genome wide analysis of transcripts that are differentially used betwen conditions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can you use these results for? In other words what systematic analysis does this transcript level data enables?&lt;/p&gt;&#xA;" OwnerUserId="599" LastEditorUserId="599" LastEditDate="2017-06-16T11:39:46.143" LastActivityDate="2017-06-21T08:07:33.170" Title="Post analysis of differentially transcript usage (DTU)" Tags="&lt;rna-seq&gt;&lt;transcriptome&gt;&lt;software-recommendation&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="775" PostTypeId="2" ParentId="771" CreationDate="2017-06-16T11:35:51.537" Score="1" Body="&lt;h1&gt;Data preparation&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Cell Ranger uses the Illumina sequencing output (&lt;code&gt;.bcl&lt;/code&gt;) files&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/mkfastq&quot; rel=&quot;nofollow noreferrer&quot;&gt;Make fastq files&lt;/a&gt;:&#xA;&lt;code&gt;cellranger mkfastq&lt;/code&gt; ==&gt; &lt;code&gt;.fastq&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Prepare count matrix: &lt;code&gt;cellranger count&lt;/code&gt; ==&gt; &lt;code&gt;matrix.mtx, web_summary.html, cloupe.cloupe&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;Optional: combine multiple &lt;code&gt;matrix.mtx&lt;/code&gt; files (libraries): &lt;code&gt;cellranger aggr&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h1&gt;Data analysis&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/visualization/latest/what-is-loupe-cell-browser&quot; rel=&quot;nofollow noreferrer&quot;&gt;Loupe Cell Browser&lt;/a&gt; visualization of&lt;/strong&gt; &lt;code&gt;cloupe.cloupe&lt;/code&gt; &lt;strong&gt;files&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Count table&lt;/strong&gt; &lt;code&gt;matrix.mtx&lt;/code&gt; &lt;strong&gt;analysis options:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/python&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;Python&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;code&gt;R&lt;/code&gt; &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/rkit&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cell Ranger R Kit&lt;/a&gt;: &lt;code&gt;cellrangerRkit::load_cellranger_matrix()&lt;/code&gt; ==&gt; ExpressionSet&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;code&gt;R&lt;/code&gt; &lt;a href=&quot;http://hemberg-lab.github.io/scRNA.seq.course/scater-package.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Scater&lt;/a&gt;: &lt;code&gt;scater::read10XResults()&lt;/code&gt; ==&gt; SCESet object&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;code&gt;R&lt;/code&gt; &lt;a href=&quot;http://satijalab.org/seurat/pbmc-tutorial.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Seurat&lt;/a&gt;: &lt;code&gt;Seurat::Read10X()&lt;/code&gt; ==&gt; Seurat object&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="208" LastEditorUserId="208" LastEditDate="2017-06-16T14:26:03.450" LastActivityDate="2017-06-16T14:26:03.450" CommentCount="0" />
  <row Id="776" PostTypeId="2" ParentId="774" CreationDate="2017-06-16T11:38:59.103" Score="1" Body="&lt;p&gt;I will take the liberty of giving one possible answers to my own question – but I’m very interested in other answers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One analysis type that such data enables is the analysis of transcript switches with predicted potential consequences. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I myself have recently developed such a tool called &lt;a href=&quot;http://bioconductor.org/packages/IsoformSwitchAnalyzeR/&quot; rel=&quot;nofollow noreferrer&quot;&gt;IsoformSwitchAnalyzeR&lt;/a&gt;. IsoformSwitchAnalyzeR enables statistical identification (via &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/DRIMSeq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DRIMSeq&lt;/a&gt;) of isoform switches with predicted functional consequences. The consequences analyzed can be chosen from a long list which includes gain/loss of protein domains, signal peptides changes in NMD sensitivity etc. The R package also enables easy visualization of isoform switches along with their consequences and it directly supports the output of &lt;a href=&quot;http://cole-trapnell-lab.github.io/cufflinks/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cufflinks/Cuffdiff&lt;/a&gt;, &lt;a href=&quot;http://deweylab.github.io/RSEM/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RSEM&lt;/a&gt;, &lt;a href=&quot;https://github.com/COMBINE-lab/salmon&quot; rel=&quot;nofollow noreferrer&quot;&gt;Salmon&lt;/a&gt; and &lt;a href=&quot;http://pachterlab.github.io/kallisto/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kallisto&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apart from enabling identification of interesting examples switch identification also enables systematic analysis of what genes are affected. For inspiration I will recommend &lt;a href=&quot;http://mcr.aacrjournals.org/content/early/2017/06/02/1541-7786.MCR-16-0459&quot; rel=&quot;nofollow noreferrer&quot;&gt;one of my own articles&lt;/a&gt; (describing results obtained with IsoformSwitchAnalyzeR) as well Hector et al’s &lt;a href=&quot;http://biorxiv.org/content/early/2016/09/21/076653&quot; rel=&quot;nofollow noreferrer&quot;&gt;recent paper&lt;/a&gt; (which is not using IsoformSwitchAnalyzeR). Of particular interest and finesse is Hector’s analysis of how isoform switches can disrupt protein-protein interactions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looking forward to hear more ideas. &lt;/p&gt;&#xA;" OwnerUserId="599" LastEditorUserId="599" LastEditDate="2017-06-16T11:55:50.510" LastActivityDate="2017-06-16T11:55:50.510" CommentCount="0" />
  <row Id="777" PostTypeId="1" AcceptedAnswerId="778" CreationDate="2017-06-16T11:42:56.740" Score="4" ViewCount="83" Body="&lt;p&gt;I have two cancer cell lines (OCI-Ly18 &amp;amp; riva) that I want to find gene expression data for, but I'm not aware of many gene expression databases that allow searching by cell-line without searching by gene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried &lt;a href=&quot;https://genevestigator.com&quot; rel=&quot;nofollow noreferrer&quot;&gt;Genevestigator&lt;/a&gt; on the recommendation of this &lt;a href=&quot;https://www.researchgate.net/post/Is_there_a_database_to_find_out_which_genes_are_expressed_in_which_cell_lines&quot; rel=&quot;nofollow noreferrer&quot;&gt;thread&lt;/a&gt;&#xA;but found no data on either cell line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be the best way to find gene expression data for these cell lines?&lt;/p&gt;&#xA;" OwnerUserId="885" LastEditorUserId="180" LastEditDate="2017-06-16T14:49:44.300" LastActivityDate="2017-06-16T14:49:44.300" Title="Searching for gene expression data by cell line" Tags="&lt;database&gt;&lt;cancer&gt;&lt;differential-expression&gt;&lt;cell-line&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="778" PostTypeId="2" ParentId="777" CreationDate="2017-06-16T11:59:37.793" Score="5" Body="&lt;p&gt;Try the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/&quot; rel=&quot;noreferrer&quot;&gt;Gene Expression Omnibus&lt;/a&gt; - it looks like they have some datasets.&lt;/p&gt;&#xA;" OwnerUserId="599" LastActivityDate="2017-06-16T11:59:37.793" CommentCount="2" />
  <row Id="779" PostTypeId="1" AcceptedAnswerId="985" CreationDate="2017-06-16T12:17:34.883" Score="1" ViewCount="56" Body="&lt;p&gt;&lt;code&gt;cellranger aggr&lt;/code&gt; can combine multiple libraries (samples), and appends each barcode with an integer (e.g. AGACCATTGAGACTTA-1). The sample identity is not recorded in the combined &lt;code&gt;matrix.mtx&lt;/code&gt; file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate#gem_groups&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate#gem_groups&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can we keep and assign sample information to each cell after loading the data into &lt;code&gt;R&lt;/code&gt;? (e.g. &lt;code&gt;Seurat::Read10X()&lt;/code&gt;)&lt;/p&gt;&#xA;" OwnerUserId="208" LastEditorUserId="208" LastEditDate="2017-07-04T13:52:23.310" LastActivityDate="2017-07-04T14:13:22.690" Title="Handling sample identity in aggregated 10x libraries?" Tags="&lt;rna-seq&gt;&lt;r&gt;&lt;scrnaseq&gt;&lt;10x-genomics&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="780" PostTypeId="2" ParentId="777" CreationDate="2017-06-16T12:30:32.550" Score="0" Body="&lt;p&gt;Try &lt;a href=&quot;https://portals.broadinstitute.org/ccle/data/browseData?conversationPropagation=begin&quot; rel=&quot;nofollow noreferrer&quot;&gt;in this section&lt;/a&gt; of &lt;a href=&quot;https://portals.broadinstitute.org/ccle/home&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cancer Cell Line Encyclopedia&lt;/a&gt;. It has expression data for a lot of cancer cell lines. I have tried few times and found all cell lines I wanted, except HeLa. You just have to select mRNA expression data and download the file you prefer (raw data, gene-centric, etc.)&lt;/p&gt;&#xA;" OwnerUserId="678" LastActivityDate="2017-06-16T12:30:32.550" CommentCount="0" />
  <row Id="781" PostTypeId="2" ParentId="779" CreationDate="2017-06-16T13:17:22.843" Score="1" Body="&lt;p&gt;You have to keep track of the file order you used for &lt;code&gt;aggr&lt;/code&gt;.  The suffix number represents which represents the aggregated sample. You could store this information in a text file and load it into R independently of the &lt;code&gt;Read10X&lt;/code&gt; function and combine it with the cell names to get their sample names. From the page you &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate#gem_groups&quot; rel=&quot;nofollow noreferrer&quot;&gt;link&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Gem Groups&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;This is an integer that is appended to each barcode in the&#xA;  gene-barcode matrix. For example, &lt;em&gt;AGACCATTGAGACTTA-1&lt;/em&gt; and&#xA;  &lt;em&gt;AGACCATTGAGACTTA-2&lt;/em&gt; are distinct cell barcodes from different&#xA;  libraries, despite having the same nucleotide sequence.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The numbering of the GEM groups will reflect the order that the&#xA;  libraries were provided in the &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate#csv_setup&quot; rel=&quot;nofollow noreferrer&quot;&gt;Aggregation CSV&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="64" LastEditorUserId="48" LastEditDate="2017-06-16T13:44:38.527" LastActivityDate="2017-06-16T13:44:38.527" CommentCount="0" />
  <row Id="782" PostTypeId="1" AcceptedAnswerId="819" CreationDate="2017-06-16T13:39:16.387" Score="7" ViewCount="87" Body="&lt;p&gt;I am analysing 142 samples belonging to 6 batches. Additionally, those samples belong to 72 strains, which means that for most of the strains there are two samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could fit simple models (for strain and batches for instance), but when I get to the &quot;full&quot; model (~batch+strain), I get the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;so &amp;lt;- sleuth_fit(so, ~strain+batch, 'full')&#xA;Error in solve.default(t(X) %*% X) :&#xA;  system is computationally singular: reciprocal condition number = 5.2412e-19&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I should point out that of the 72 strains, only 15 have samples in distinct batches. This means that most strains (57) have both samples in the same batch.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the error due to an unknown bug or rather to the experimental design? Does it mean that the information on batches cannot be used?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT I've posted the experimental design in a &lt;a href=&quot;https://gist.github.com/mgalardini/7553e14ead89a7f020f2b0a610086805&quot; rel=&quot;nofollow noreferrer&quot;&gt;gist&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;batch   strain  replica&#xA;batch_1     strain_41   1&#xA;batch_4     strain_41   2&#xA;batch_1     strain_28   1&#xA;batch_4     strain_28   2&#xA;batch_1     strain_26   1&#xA;[...]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="123" LastEditorUserId="123" LastEditDate="2017-06-19T13:44:25.840" LastActivityDate="2017-06-20T11:00:23.183" Title="Getting a &quot;system is computationally singular&quot; error in sleuth" Tags="&lt;rna-seq&gt;&lt;r&gt;&lt;differential-expression&gt;" AnswerCount="2" CommentCount="4" FavoriteCount="0" />
  <row Id="783" PostTypeId="1" CreationDate="2017-06-16T13:56:08.720" Score="6" ViewCount="181" Body="&lt;p&gt;If we have a PDB structrure, how can we find residues physically interacting with each other in space? I know that we must find the distance between residues and if the distance is less than 5-6 Angstrom, we say that residues are physically interacting. But how can we find the distance between all residues and how can we finally determine the distances between all residues? Is there a software or webserver for that?&lt;/p&gt;&#xA;" OwnerUserId="818" LastActivityDate="2017-06-19T11:18:01.990" Title="How can we find the distance between all residues in a PDB file?" Tags="&lt;protein-structure&gt;&lt;pdb&gt;" AnswerCount="4" CommentCount="4" FavoriteCount="1" />
  <row Id="784" PostTypeId="2" ParentId="783" CreationDate="2017-06-16T14:22:17.763" Score="4" Body="&lt;p&gt;If you need to process multiple files, you could use &lt;a href=&quot;http://biopython.org/wiki/The_Biopython_Structural_Bioinformatics_FAQ&quot; rel=&quot;nofollow noreferrer&quot;&gt;Biopython&lt;/a&gt; to parse a PDB structure.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from Bio.PDB import PDBParser&#xA;&#xA;# create parser&#xA;parser = PDBParser()&#xA;&#xA;# read structure from file&#xA;structure = parser.get_structure('PHA-L', '1fat.pdb')&#xA;&#xA;model = structure[0]&#xA;chain = model['A']&#xA;&#xA;# this example uses only the first residue of a single chain.&#xA;# it is easy to extend this to multiple chains and residues.&#xA;for residue1 in chain:&#xA;    for residue2 in chain:&#xA;        if residue1 != residue2:&#xA;            # compute distance between CA atoms&#xA;            try:&#xA;                distance = residue1['CA'] - residue2['CA']&#xA;            except KeyError:&#xA;                ## no CA atom, e.g. for H_NAG&#xA;                continue&#xA;            if distance &amp;lt; 6:&#xA;                print(residue1, residue2, distance)&#xA;        # stop after first residue&#xA;        break&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you need to look at one structure, using a viewer perhaps would be easier. You could try &lt;a href=&quot;https://www.pymol.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PyMOL&lt;/a&gt;: (&lt;a href=&quot;https://pymolwiki.org/index.php/Distance&quot; rel=&quot;nofollow noreferrer&quot;&gt;how to measure distance&lt;/a&gt;). There are other PDB viewers, some of which can work even through a browser.&lt;/p&gt;&#xA;" OwnerUserId="191" LastActivityDate="2017-06-16T14:22:17.763" CommentCount="5" />
  <row Id="785" PostTypeId="1" AcceptedAnswerId="788" CreationDate="2017-06-16T14:32:43.610" Score="2" ViewCount="55" Body="&lt;p&gt;I am looking for the positions of annotated regulatory sequences (promoters, enhancers and suppressors) in the human genome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I looked at &lt;a href=&quot;http://www.ensembl.org/info/genome/funcgen/regulatory_build.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Ensembl regulatory Build&lt;/a&gt; and &lt;a href=&quot;http://www.pazar.info/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PAZAR&lt;/a&gt; but I am not used to look for datasets and I failed to find what I was looking for.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Can you help to find annotated regulatory sequences (start and end positions) in the human genome?&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="888" LastEditorUserId="888" LastEditDate="2017-06-16T14:42:31.880" LastActivityDate="2017-06-16T19:40:35.053" Title="Dataset: Locations of regulatory sequences in the human genome?" Tags="&lt;database&gt;&lt;human-genome&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="786" PostTypeId="2" ParentId="769" CreationDate="2017-06-16T14:45:22.267" Score="1" Body="&lt;p&gt;A one-liner:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;zcat my.vcf.gz |&#xA;  perl -ane '$x=0;for $y (split(&quot;,&quot;,$F[4])){$x=1 if length($y)&amp;gt;length($F[3])}print if /^#/||$x'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or equivalently&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;zcat my.vcf.gz |&#xA;  perl -ane '$x=0;map{$x=1 if length&amp;gt;length($F[3])}split(&quot;,&quot;,$F[4]);print if /^#/||$x'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For simple VCF operations, I generally recommend to write a script. This may be faster than those using heavy libraries. With a script, you only parse fields you care about; most libraries unnecessarily parse every field.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;On a related note, I recommend &lt;strong&gt;not&lt;/strong&gt; to decompose multi-allelic sites unless necessary. Decomposing is tricky, makes VCF harder to parse and to understand and may be a potential source of errors. Here is an example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#CHROM POS ID REF  ALT               QUAL FILTER  INFO  FORMAT  S1  S2  S3  S4&#xA;11     101 .  GCGT G,GCGA,GTGA,CCGT  199  PASS    .     GT      0/1 1/2 2/3 2/4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;vt decompose+normalize produces the following VCF:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#CHROM POS ID REF  ALT QUAL FILTER INFO FORMAT  S1   S2   S3   S4&#xA;11     101 .  GCGT G   199  PASS   .    GT      0/1  1/.  ./.  ./.&#xA;11     101 .  G    C   199  PASS   .    GT      0/.  ./.  ./.  ./1&#xA;11     102 .  CGT  TGA 199  PASS   .    GT      0/.  ./.  ./1  ./.&#xA;11     104 .  T    A   199  PASS   .    GT      0/.  ./1  1/.  1/.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In theory, you can reconstruct the original VCF from this output. However, it is very challenging for a program to do that. When you compute allele frequency line-by-line, this VCF will give you wrong results. &lt;code&gt;bcftools norm -m-&lt;/code&gt; replaces &quot;.&quot; with &quot;0&quot;. You can get a correct ALT allele frequency from the bcftools output, but a wrong REF allele frequency. Furthermore, vt is also imperfect in that &quot;CGT=&gt;TGA&quot; is not decomposed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My preferred output is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#CHROM POS    ID     REF    ALT    QUAL   FILTER INFO   FORMAT S1     S2     S3     S4&#xA;11     101    .      GCGT   G,&amp;lt;M&amp;gt;  0      .      .      GT     0/1    1/2    2/2    2/2&#xA;11     101    .      G      C,&amp;lt;M&amp;gt;  0      .      .      GT     0/2    2/0    0/0    0/1&#xA;11     102    .      C      T,&amp;lt;M&amp;gt;  0      .      .      GT     0/2    2/0    0/1    0/0&#xA;11     104    .      T      A,&amp;lt;M&amp;gt;  0      .      .      GT     0/2    2/1    1/1    1/0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here we use a symbolic allele &lt;code&gt;&amp;lt;M&amp;gt;&lt;/code&gt; to represent &quot;another ALT allele&quot;. You can calculate the allele frequency by looking at one line, and won't confuse other ALT alleles with REF. bgt can produce such a VCF indirectly. However, it discards all INFO, so is not a practical solution, either.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In summary, it is very difficult to decompose multi-allelic sites. When you get decomposing wrong, your downstream analyses may be inaccurate. Decomposition should be used with caution.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-16T14:45:22.267" CommentCount="0" />
  <row Id="787" PostTypeId="2" ParentId="783" CreationDate="2017-06-16T14:48:10.087" Score="2" Body="&lt;p&gt;Could you use CCP4's NCONT program? There's a GUI and a command line interface, whatever suits. You can specify which chains you want to target and interact with and set a cut off for distance. The bonus here is once you're in you have a nice suite of other structural tools to use.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're just doing it once, the GUI is friendly enough to work things out, if you're doing a batch then you can run it across several files via the command line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.ccp4.ac.uk/download/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="252" LastEditorUserId="252" LastEditDate="2017-06-16T14:57:04.067" LastActivityDate="2017-06-16T14:57:04.067" CommentCount="3" />
  <row Id="788" PostTypeId="2" ParentId="785" CreationDate="2017-06-16T14:53:13.360" Score="5" Body="&lt;p&gt;This can be done quite easily using Ensebl's &lt;a href=&quot;http://www.ensembl.org/biomart/martview&quot; rel=&quot;noreferrer&quot;&gt;BioMart&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Choose the Ensembl Regulation database:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/tm8MK.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/tm8MK.png&quot; alt=&quot;Ensembl screenshot&quot;&gt;&lt;/a&gt; &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Select the &quot;Human Regulatory Features&quot; dataset:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/6zTm8.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/6zTm8.png&quot; alt=&quot;human regulatory features&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;That's basically it right there, just click on &quot;Results&quot;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/oPctm.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/oPctm.png&quot; alt=&quot;results link&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Export to file and click &quot;Go&quot;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/SYI1k.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/SYI1k.png&quot; alt=&quot;GO link&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;This will download a file called &lt;code&gt;mart_export.txt&lt;/code&gt; which looks like this (I chose TSV for tab separated values):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ head mart_export.txt &#xA;Chromosome/scaffold name    Start (bp)  End (bp)    Feature type&#xA;18  76429380    76430144    Open chromatin&#xA;8   66405962    66406502    Open chromatin&#xA;4   61184401    61184600    CTCF Binding Site&#xA;X   40733600    40737000    Promoter&#xA;5   97407001    97407200    CTCF Binding Site&#xA;X   73946201    73946600    Promoter Flanking Region&#xA;15  19948201    19949200    CTCF Binding Site&#xA;5   11302601    11303143    Open chromatin&#xA;2   208407801   208408000   CTCF Binding Site&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see the format is pretty straightforward. The first column has the chromosome or scaffold name (it should always be a chromosome for human), the second and third are the star and end positions and the last field is the type of region. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can also choose to limit by region type by choosing a filter (click the &quot;Filters&quot; link) in BioMart before downloading or by simply parsing the file once you've downloaded it. &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-06-16T14:53:13.360" CommentCount="2" />
  <row Id="789" PostTypeId="2" ParentId="782" CreationDate="2017-06-16T15:10:27.567" Score="4" Body="&lt;p&gt;This happen when the variables (strain +batch) create a design matrix like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;batch strain&#xA;1 1 #&#xA;1 1 #&#xA;1 2&#xA;2 2&#xA;3 3&#xA;4 3&#xA;...&#xA;16 72&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which means that some of the covariates are not linearly independent (ie batch 1 and strain 1), all the strain 1 is in batch 1. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can correct for batch effects, but not in this design of the linear model (if you want to take into account the strain). You could do one batch more with those 15 strains that are in a single batch (if they are in different batch between them) that way you would get an independent design.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many strains in a single batch are in the same batch. You need to increase the number of samples (recommended anyway due to the low number of samples per strain) to avoid this problem. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;There are a lot of related question in Bioconductor &lt;a href=&quot;https://www.google.es/search?q=site%3Asupport.bioconductor.org%20%22system%20is%20computationally%20singular%22&amp;amp;rlz=1C1WPZB_enES727ES727&amp;amp;oq=site%3Asupport.bioconductor.org%20%22system%20is%20computationally%20singular%22&amp;amp;ie=UTF-8&quot; rel=&quot;nofollow noreferrer&quot;&gt;support forum&lt;/a&gt;, from where I expanded an &lt;a href=&quot;https://support.bioconductor.org/p/62058/#62059&quot; rel=&quot;nofollow noreferrer&quot;&gt;answer&lt;/a&gt;.&lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2017-06-20T07:33:09.990" LastActivityDate="2017-06-20T07:33:09.990" CommentCount="3" />
  <row Id="790" PostTypeId="2" ParentId="590" CreationDate="2017-06-16T15:33:36.017" Score="1" Body="&lt;p&gt;You could use &lt;a href=&quot;http://www.ccp4.ac.uk/MG/ccp4mg_help/pisa.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;PISA&lt;/a&gt; assuming you have a PDB file of cellulase? Sorry it's a short answer and should be a comment, I don't have the reputation, however.&lt;/p&gt;&#xA;" OwnerUserId="252" LastActivityDate="2017-06-16T15:33:36.017" CommentCount="1" />
  <row Id="791" PostTypeId="2" ParentId="785" CreationDate="2017-06-16T19:40:35.053" Score="0" Body="&lt;p&gt;Promoters and enhancers based on CAGE-seq from FANTOM5:&#xA;&lt;a href=&quot;http://fantom.gsc.riken.jp/5/data/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://fantom.gsc.riken.jp/5/data/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just choose the category and download bed-file, e.g. CAGE-peaks would be promoters. There are READMEs in each folder that explain the data. &lt;/p&gt;&#xA;" OwnerUserId="894" LastActivityDate="2017-06-16T19:40:35.053" CommentCount="0" />
  <row Id="792" PostTypeId="2" ParentId="783" CreationDate="2017-06-16T20:09:38.157" Score="0" Body="&lt;p&gt;You can use &lt;a href=&quot;http://mdtraj.org/1.8.0/&quot; rel=&quot;nofollow noreferrer&quot;&gt;MDtraj&lt;/a&gt;. The package is easy to install using Anaconda.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can get the interacting residues using the following snippet (taken from &lt;a href=&quot;http://mdtraj.org/1.6.2/examples/native-contact.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://mdtraj.org/1.6.2/examples/native-contact.html&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;heavy_pairs = np.array(&#xA;    [(i,j) for (i,j) in combinations(heavy, 2)&#xA;        if abs(native.topology.atom(i).residue.index - \&#xA;               native.topology.atom(j).residue.index) &amp;gt; 3])&#xA;&#xA;# compute the distances between these pairs in the native state&#xA;heavy_pairs_distances = md.compute_distances(native[0], heavy_pairs)[0]&#xA;# and get the pairs s.t. the distance is less than NATIVE_CUTOFF&#xA;native_contacts = heavy_pairs[heavy_pairs_distances &amp;lt; NATIVE_CUTOFF]&#xA;print(&quot;Number of native contacts&quot;, len(native_contacts))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="895" LastActivityDate="2017-06-16T20:09:38.157" CommentCount="0" />
  <row Id="793" PostTypeId="2" ParentId="769" CreationDate="2017-06-16T23:03:39.673" Score="0" Body="&lt;p&gt;If you're doing set operations, you could use &lt;a href=&quot;http://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/vcf2bed.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;vcf2bed&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ vcf2bed --insertions &amp;lt; in.vcf &amp;gt; out.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Based on the VCF v4.2 specification, &lt;code&gt;--snvs&lt;/code&gt;, &lt;code&gt;--insertions&lt;/code&gt;, and &lt;code&gt;--deletions&lt;/code&gt; are options available to filter input. In each case, the length of the reference and alternate alleles is used to determine which type of variant is being handled.&lt;/p&gt;&#xA;" OwnerUserId="776" LastActivityDate="2017-06-16T23:03:39.673" CommentCount="0" />
  <row Id="794" PostTypeId="2" ParentId="752" CreationDate="2017-06-18T17:04:43.097" Score="2" Body="&lt;p&gt;If you are looking for cancer mutations, the primary resource is COSMIC and they provide GRCh38 VCFs. The download page is here: &lt;a href=&quot;http://cancer.sanger.ac.uk/cosmic/download&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://cancer.sanger.ac.uk/cosmic/download&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It'll be up to you how you define &quot;common&quot;, but the VCF includes a lot of information you can use.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-06-18T17:04:43.097" CommentCount="0" />
  <row Id="795" PostTypeId="2" ParentId="367" CreationDate="2017-06-18T18:04:09.980" Score="7" Body="&lt;p&gt;I have a &lt;a href=&quot;http://robpatro.com/blog/?p=235&quot; rel=&quot;nofollow noreferrer&quot;&gt;blog post&lt;/a&gt; that describes the effective length (as well as these different relative abundance units).  The short explanation is that what people refer to as the &quot;effective length&quot; is actually the &lt;em&gt;expected&lt;/em&gt; effective length (i.e., the expectation, in a statistical sense, of the effective length).  The notion of effective length is actually a property of a transcript, fragment pair, and is equal to the number of potential starting locations for a fragment of this length on the given transcript.  If you take the average, over all fragments mapping to a transcript (potentially weighted by the conditional probability of this mapping), this quantity is the expected effective length of the transcript.  This is often approximated as simply $l_i - \mu$, or $l_i - \mu_{l_i}$ --- where $\mu_{l_i}$ is the mean of the &lt;em&gt;conditional&lt;/em&gt; fragment length distribution (conditioned on the fragment length being &amp;lt; $l_i$ to account for exactly the issue that you raise).&lt;/p&gt;&#xA;" OwnerUserId="491" LastEditorUserId="29" LastEditDate="2017-07-26T16:15:39.647" LastActivityDate="2017-07-26T16:15:39.647" CommentCount="0" />
  <row Id="796" PostTypeId="1" CreationDate="2017-06-19T00:24:14.877" Score="3" ViewCount="133" Body="&lt;p&gt;&lt;strong&gt;The setup&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Imagine that I work on an organism without a reference genome, and that the closest reference genome I can get is quite diverged. E.g. ~10% diverged in terms of SNVs when measured with short reads, and also has a lot of structural variants too. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now imagine I get a 1 million base-pair long-read (e.g. from Nanopore data) for my organism. The question is this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I estimate the proportion of the read that is meaningful sequence vs garbage? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Some things that probably won't work&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most standard approaches won't work here. E.g. I could try mapping the read to the reference, but &lt;em&gt;even if the read was perfectly good&lt;/em&gt; I wouldn't expect most of it to map thanks to true structural variations between the read and the reference. The same goes for standard alignment or BLAST. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Some things that might work&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The best naive method here seems to be to cut the read up into smaller pieces (either overlapping or not) and use standard approaches to map/align each of these. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, what have people tried for this? And what tools have you used and why?&lt;/p&gt;&#xA;" OwnerUserId="156" LastEditorUserId="73" LastEditDate="2017-06-19T03:42:46.103" LastActivityDate="2017-06-19T04:32:07.353" Title="How to estimate whether a long-read is meaningful sequence?" Tags="&lt;alignment&gt;&lt;nanopore&gt;&lt;read-mapping&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="797" PostTypeId="2" ParentId="796" CreationDate="2017-06-19T00:34:27.393" Score="2" Body="&lt;p&gt;As a first pass, you could check if the read is chimeric. &lt;a href=&quot;https://github.com/rrwick/Porechop&quot; rel=&quot;nofollow noreferrer&quot;&gt;Porechop&lt;/a&gt; searches for known nanopore adaptors both on the ends and through the middle of the read. This won't resolve issues around blocked or empty pores, but it will at least check if you have found two long-ish reads lumped into the same file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By default, Porechop splits chimeric reads into two (or more, I suppose!) non-chimeric reads, but the option &lt;code&gt;--discard-middle&lt;/code&gt; would be a quick and easy way to check - run it on a fasta file containing only your long read, and if the output is empty, the read is chimeric.&lt;/p&gt;&#xA;" OwnerUserId="163" LastActivityDate="2017-06-19T00:34:27.393" CommentCount="0" />
  <row Id="798" PostTypeId="1" AcceptedAnswerId="811" CreationDate="2017-06-19T00:50:23.480" Score="4" ViewCount="78" Body="&lt;p&gt;I have a refseq ID of a protein from E.coli and I want to find homologs of this protein. I ran Blast against refseq database but I got a lot of sequences most of which were from Ecoli again. I decided to run PSI-Blast to get more divergent species, but I do not exactly know if my result are real homologs or false positives. what is your idea for finding homologous protein sequences from more divergent species? And what can I do for selecting the real (but not false positive) hits?&lt;/p&gt;&#xA;" OwnerUserId="818" LastActivityDate="2017-06-19T13:54:59.920" Title="Finding homologs of a protein sequence" Tags="&lt;sequence-homology&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="799" PostTypeId="2" ParentId="796" CreationDate="2017-06-19T04:32:07.353" Score="2" Body="&lt;p&gt;If you're looking at a single organism, in the absence of a reference genome you can map other reads to suspect reads and look at coverage. Looking at the actual sequence can also be useful: real DNA usually doesn't have an abundance of two different bases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nanopore reads also give another way to see if the read looks weird by having a look at the raw signal. Our &lt;a href=&quot;https://f1000research.com/articles/6-631/v1&quot; rel=&quot;nofollow noreferrer&quot;&gt;chimeric reads paper&lt;/a&gt; gives a few examples of what DNA sequence should look like under normal circumstances. Here's the first raw signal figure from that paper:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/JOCQu.gif&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/JOCQu.gif&quot; alt=&quot;good raw signal&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If there's lots of contiguous signal with very similar current levels (i.e. it looks like the &quot;stall&quot; region, then it's not a good read and any basecalls should be ignored.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-19T04:32:07.353" CommentCount="0" />
  <row Id="800" PostTypeId="2" ParentId="798" CreationDate="2017-06-19T04:39:10.010" Score="4" Body="&lt;p&gt;It sounds like &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/smartblast/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Smart BLAST&lt;/a&gt; might do what you want. Here's the description of it's goal:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;SmartBLAST is a new and experimental NCBI tool that makes it easier to&#xA;  complete common sequence analysis tasks, such as finding a candidate&#xA;  protein name for a sequence, locating regions of high sequence&#xA;  conservation, or identifying regions covered by database sequences but&#xA;  missing from the query.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;To do this, SmartBLAST performs the following tasks in much less time&#xA;  than it takes to run a typical BLASTp search:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;a BLASTp comparison of the query with the closest matching sequences available;&lt;/li&gt;&#xA;  &lt;li&gt;a parallel BLASTp search to find the closest matches to high quality sequences from model organisms;&lt;/li&gt;&#xA;  &lt;li&gt;a multiple alignment between the query and five of the closest matching sequences (usually including two high quality sequences);&lt;/li&gt;&#xA;  &lt;li&gt;an analysis that produces a phylogenetic tree from the multiple sequence alignment.&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;[from &lt;a href=&quot;https://ncbiinsights.ncbi.nlm.nih.gov/2015/07/29/smartblast/&quot; rel=&quot;nofollow noreferrer&quot;&gt;NCBI Insights&lt;/a&gt;]&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-19T04:39:10.010" CommentCount="2" />
  <row Id="802" PostTypeId="1" CreationDate="2017-06-19T10:19:44.403" Score="2" ViewCount="66" Body="&lt;p&gt;I'm looking for the exact invocation used to generate the 16SMicrobial database that you can download from here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://ftp.ncbi.nlm.nih.gov/blast/db/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://ftp.ncbi.nlm.nih.gov/blast/db/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm hoping to create the same type of blastdb with the same type of metadata with custom sequences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Platform isn't an issue, but let's say on ubuntu 14.04 or 16.04.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to replicate the creation of the database as closely as possible. &#xA; The most important feature is the taxonomic information as can be seen &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK279688/&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The databases on the FTP site contain taxonomic information for each&#xA;  sequence, include the identifier indices for lookups, and can be up to&#xA;  four times smaller than the FASTA. The original FASTA can be generated&#xA;  from the BLAST database using blastdbcmd&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Creation of a blastdb using makeblastdb from a set of a fasta sequences is not an issue and can be achieved via:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;makeblastdb -in &amp;lt;your_file.fasta&amp;gt; -dbtype nucl -out &amp;lt;database_name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My question is specifically about the invocation NCBI uses to add the metadata that is present in the NCBI's 16SMicrobial blast database as I am keen to make sure I have replicated the process as closely as possible.&lt;/p&gt;&#xA;" OwnerUserId="302" LastEditorUserId="302" LastEditDate="2017-06-19T16:11:40.913" LastActivityDate="2017-08-21T21:28:25.223" Title="What command/invocation is used to generate NCBI 16SMicrobial blastdb" Tags="&lt;blast&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="803" PostTypeId="2" ParentId="783" CreationDate="2017-06-19T11:18:01.990" Score="0" Body="&lt;p&gt;As part of a project me and some teammates did a &lt;a href=&quot;https://github.com/llrs/PYT-SBI&quot; rel=&quot;nofollow noreferrer&quot;&gt;script&lt;/a&gt; that outputs visual maps of distances between residues. It uses Biopython. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The module contact_map.py does what you are looking for. As an example, if you want to find the residues whose CA are below 5 you can run the following command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;python3 contact_map.py pdb1cd8.ent -a CA -CA 5 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This will produce three files:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;distance_map_pdb1cd8_CA.png # Heatmap of the distance between the residues&#xA;contact_map_pdb1cd8_CA.png # Black/White heatmap: If it is at that min distance&#xA;contact_map.log  # The actions taken&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you don't have downloaded already the pdb structue you can use the main module:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;python3 cozmic.py real 1cd8 -a CA -CA 5 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-06-19T11:18:01.990" CommentCount="6" />
  <row Id="805" PostTypeId="2" ParentId="802" CreationDate="2017-06-19T12:17:45.603" Score="0" Body="&lt;p&gt;I don't know what specific properties you are considering. As far as I know, that's just a normal blast database, like any other. These are produced using the &lt;code&gt;formatdb&lt;/code&gt; command which is part of the &lt;code&gt;ncbi-blast&lt;/code&gt; software. You haven't specified what system you are using so I can't help you find and install it, but the command will be something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;formatdb -p F -i yourSeqs.fasta -n yourDBName&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;formatdb&lt;/code&gt; : the command&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-p F&lt;/code&gt; : protein: false. This is needed to make databases of nucleotide sequences. &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-i yourSeqs.fasta&lt;/code&gt; : your input file with all your sequences in fasta format. &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;-n yourDBName&lt;/code&gt; : the name (choose whatever you like) for your database.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;You can then use &lt;code&gt;yourDBName&lt;/code&gt; as a database to blast against. The above assumes yuou are &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-06-19T12:17:45.603" CommentCount="3" />
  <row Id="807" PostTypeId="1" AcceptedAnswerId="861" CreationDate="2017-06-19T13:15:22.910" Score="10" ViewCount="695" Body="&lt;p&gt;I would like to select a random record from a large set of &lt;code&gt;n&lt;/code&gt; unaligned sequencing reads in &lt;code&gt;log(n)&lt;/code&gt; time complexity (&lt;a href=&quot;https://en.wikipedia.org/wiki/Big_O_notation&quot; rel=&quot;noreferrer&quot;&gt;big O notation&lt;/a&gt;) or less. A record is defined as the equivalent of four lines in FASTQ format. The records do not fit in RAM and would need to be stored on disk. Ideally, I would like to store the reads in a compressed format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would prefer a solution that does not require any extra files such as for example a reference genome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The title of this question mentions a FASTQ only because FASTQ is a common format for storing unaligned reads on disk. I am happy with answers that require a single limited transformation of the data to another file format in time complexity order &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Update&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;A clarification: I want the random record to be selected with probability &lt;code&gt;1/n&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="492" LastEditorUserId="492" LastEditDate="2017-06-22T16:32:55.203" LastActivityDate="2017-06-22T16:32:55.203" Title="Random access on a FASTQ file" Tags="&lt;bam&gt;&lt;fastq&gt;&lt;reads&gt;&lt;benchmarking&gt;" AnswerCount="11" CommentCount="3" FavoriteCount="0" />
  <row Id="808" PostTypeId="2" ParentId="807" CreationDate="2017-06-19T13:15:22.910" Score="5" Body="&lt;p&gt;You could shuffle the FASTQ once and then read sequences off the top of the file as you need them:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;gzip -dc input.fastq.gz | paste - - - - | shuf | tr '\t' '\n'| gzip -c &amp;gt; output.fastq.gz&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would recommend &lt;a href=&quot;https://zlib.net/pigz/&quot; rel=&quot;nofollow noreferrer&quot;&gt;pigz&lt;/a&gt; as a replacement for gzip in the compression step if you have it available.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The downside of this approach is that you only get &lt;code&gt;n&lt;/code&gt; reads before you need to run the shuffle again, and &lt;a href=&quot;https://stackoverflow.com/a/24492814/528691&quot;&gt;apparently&lt;/a&gt; &lt;code&gt;shuf&lt;/code&gt; holds all data in RAM, so it would die with an out of memory error if the FASTQ file does not fit into RAM as is specified in the question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using &lt;code&gt;sort -R&lt;/code&gt; is complexity &lt;code&gt;n log(n)&lt;/code&gt; and uses temporary files, so it &lt;a href=&quot;https://vkundeti.blogspot.co.uk/2008/03/tech-algorithmic-details-of-unix-sort.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;should not&lt;/a&gt; run out of memory:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;gzip -dc input.fastq.gz | paste - - - - | nl | sort -R | perl -pe 's/\s*\d+\t//' | tr '\t' '\n'| gzip -c &amp;gt; output.fastq.gz&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;nl&lt;/code&gt; and &lt;code&gt;perl&lt;/code&gt; commands are necessary to make sure that identical records are not sorted next to each other.&lt;/p&gt;&#xA;" OwnerUserId="492" LastEditorUserId="191" LastEditDate="2017-06-22T11:06:17.097" LastActivityDate="2017-06-22T11:06:17.097" CommentCount="0" />
  <row Id="809" PostTypeId="2" ParentId="807" CreationDate="2017-06-19T13:44:42.180" Score="3" Body="&lt;p&gt;One possibility is to:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;reformat the data such that each record is a single line containing the read description, bases, and quality scores&lt;/li&gt;&#xA;&lt;li&gt;pad out each record to a maximum length in each field such that every record in the file is the same number of bytes&lt;/li&gt;&#xA;&lt;li&gt;the total number of records can now be calculated as file size / record size&lt;/li&gt;&#xA;&lt;li&gt;choose a random record number between 0 and the total number of records&lt;/li&gt;&#xA;&lt;li&gt;binary search over the reformatted file until you obtain your read&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;This would get you the log(n) lookup time you want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, the data wouldn't be compressed.  You could 2-bit encode the bases and quantize the quals to save some space, but that'd be lossy and is perhaps not what you're looking for.  Alternatively, you could block-gzip the reformatted data and keep a record of how many blocks are in the file and how many reads are in each block (since the filesize will no longer reflect the number of records in the file).  Then to obtain a specific read, you'd calculate the block number it'll appear in, decompress the block, and return the appropriate read.&lt;/p&gt;&#xA;" OwnerUserId="822" LastActivityDate="2017-06-19T13:44:42.180" CommentCount="0" />
  <row Id="810" PostTypeId="2" ParentId="807" CreationDate="2017-06-19T13:51:48.503" Score="0" Body="&lt;p&gt;I suppose I can provide code for this if needed, but keep in mind that it'd probably have to be done in C.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One could make this easier by using a bgzf compressed fastq file. Yes, BAM uses that already, but since Illumina's software now defaults to producing that there should be a reasonable amount of it already available.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Count the number of blocks in the file.&lt;/li&gt;&#xA;&lt;li&gt;Randomly select one of these blocks.&lt;/li&gt;&#xA;&lt;li&gt;Perform reservoir selection on the entries in that block.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;That won't be strictly O(log N), but it's lower on memory and IO requirements than parsing through the whole file and doing shuffling or otherwise completely munging the data into an otherwise not-terribly-useful format. This also has the benefit of allowing selecting more than one random entry a bit faster, since you can memoize things.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-19T13:51:48.503" CommentCount="0" />
  <row Id="811" PostTypeId="2" ParentId="798" CreationDate="2017-06-19T13:54:59.920" Score="4" Body="&lt;p&gt;It depends on what you're looking for. If you're just looking for sequence homology, then you can simply pick the best hits from a blast search. If, however, you are referring to &lt;em&gt;functional&lt;/em&gt; homology, if you are looking for the protein which has the same functions as your query, then it's more complicated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sequence homology is not enough to infer functional homology. For example, you can have cases of gene duplication and subsequent functional divergence. Such &lt;em&gt;paralogs&lt;/em&gt; are still &lt;em&gt;homologs&lt;/em&gt; (&lt;a href=&quot;https://biology.stackexchange.com/a/4964/1306&quot;&gt;paralogs are a subset of homologs&lt;/a&gt;), but they don't necessarily have the same function. It is also often the case that the homolog (be it orthologous or paralogous) of a protein in species B has a completely different function than its homolog in species A despite a high level of sequence similarity. This is usually very hard to determine &lt;em&gt;in silico&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To find the functionally homologous protein(s), you would ideally need to identify the essential residues that allow your protein to perform its function. This could be done using something like &lt;a href=&quot;http://pfam.xfam.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PFam&lt;/a&gt; which will identify protein domains. You can then check whether the homologs you find also have this domain. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is essentially what PSI-blast does. Although it doesn't take domains into account, each successive iteration is used to build a model of your proptein. The model is built under the reasonable assumption that highly conserved residues are important. So it will consider more diverged sequences as homologous if those residues are conserved. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you know how your protein works and what residues are important, you can use that knowledge to refine the results of your PSI-blast. If you don't, you'll have to use only &quot;good&quot; hits to make the model. One way to do this, for well studied proteins, is to only add proteins that are already annotated as homologs of what you are looking for to build your model, then use that model to search in un-annotated species. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you don't know, you could try looking for recognizable protein domains in your query protein (use PFam) and then use the HMM (hidden markov model) of the domain to identify important residues. For example, &lt;a href=&quot;http://pfam.xfam.org/family/PF01754#tabview=tab4&quot; rel=&quot;nofollow noreferrer&quot;&gt;this is&lt;/a&gt; the HMM logo for the zf-A20 zinc finger domain:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/90nPc.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/90nPc.png&quot; alt=&quot;zinc finger sequence logo&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The huge cysteine (C) residues are shown that size because they are very conserved across proteins carrying this domain and, presumably, are functionally important for the domain. So, if you pass your protein through PFam and identify domains, find the important residues and make sure all your homologs have those conserved. If using PSI-blast, only include sequences where those residues are conserved in the results you keep.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, another useful tool that works in the same way is &lt;a href=&quot;http://hmmer.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;HMMER&lt;/a&gt;. This takes a protein alignment as input, like PSI-blast builds an HMM model from it and then can use this model to query a protein database for more hits. Methods like HMMER and PSI-blast are far better than simple sequence similarity approaches when looking for homologs.  &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-06-19T13:54:59.920" CommentCount="3" />
  <row Id="812" PostTypeId="2" ParentId="807" CreationDate="2017-06-19T13:57:44.610" Score="1" Body="&lt;p&gt;To emphasise the issue (as outlined in the &lt;code&gt;1/n&lt;/code&gt; update), consider an input file with one record that is 5 million bases long, and one records that is 100 bases long. You want an equal probability of selecting either of these two records. Any random seek methods will overwhelmingly pick out the long record.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I expect that indexing the locations of record starts is really the only workable option here, particularly if multi-line records are possible. Create an index file containing the start locations of each record (as identically sized integers, e.g. 64-bit), then sample from the index file (which is an identical length for each record) to fetch the start location. I'd envisage that this file would &lt;em&gt;only&lt;/em&gt; contain the start locations; any additional metadata (including sequence name) would require seeking in the original file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once the indexing is done, the file can be compressed with bgzip, with specific offsets retrieved using the &lt;code&gt;-b&lt;/code&gt; and &lt;code&gt;-s&lt;/code&gt; options. However, I expect that compression would not be particularly efficient if multiple random records were desired.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-20T10:13:08.820" LastActivityDate="2017-06-20T10:13:08.820" CommentCount="5" />
  <row Id="813" PostTypeId="2" ParentId="807" CreationDate="2017-06-19T14:06:33.253" Score="6" Body="&lt;h1&gt;Arbitrary record access in constant time&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;To get a random record in constant time, it is sufficient to get an arbitrary record in constant time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two solutions here: One with &lt;code&gt;tabix&lt;/code&gt; and one with &lt;code&gt;grabix&lt;/code&gt;. I think the &lt;code&gt;grabix&lt;/code&gt; solution is more elegant, but I am keeping the &lt;code&gt;tabix&lt;/code&gt; solution below because &lt;code&gt;tabix&lt;/code&gt; is a more mature tool than &lt;code&gt;grabix&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks to &lt;a href=&quot;https://bioinformatics.stackexchange.com/users/37/user172818&quot;&gt;user172818&lt;/a&gt; for suggesting &lt;code&gt;grabix&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Update&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;This answer previously stated that &lt;code&gt;tabix&lt;/code&gt; and &lt;code&gt;grabix&lt;/code&gt; perform lookups in &lt;code&gt;log(n)&lt;/code&gt; time. After taking a closer look at the grabix source code and the tabix paper I am now convinced that lookups are independent of &lt;code&gt;n&lt;/code&gt; in complexity. However, both tools use an index that scales in size proportionally to &lt;code&gt;n&lt;/code&gt;. So, the loading of the index is order &lt;code&gt;n&lt;/code&gt;. However, if we consider the loading of the index as &quot;...a single limited transformation of the data to another file format...&quot;, then I think this answer is still is a valid one. If more than one record is to be retrieved, then the index needs to be stored in memory, perhaps with a framework such as pysam or htslib.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Using &lt;code&gt;grabix&lt;/code&gt;&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Compress with &lt;code&gt;bgzip&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Index the file and perform lookups with &lt;a href=&quot;https://github.com/arq5x/grabix&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;grabix&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In bash:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;gzip -dc input.fastq.gz | bgzip -c &amp;gt; output.fastq.gz&#xA;&#xA;grabix index output.fastq.gz&#xA;&#xA;# retrieve 5-th record (1-based) in log(n) time&#xA;# requires some math to convert indices (4*4 + 1, 4*4 + 4) = (17, 20)&#xA;grabix grab output.fastq.gz 17 20&#xA;&#xA;# Count the number of records for part two of this question&#xA;export N_LINES=$(gzip -dc output.fastq.gz | wc -l)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Using &lt;code&gt;tabix&lt;/code&gt;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The tabix code is more complicated and relies on the iffy assumption that &lt;code&gt;\t&lt;/code&gt; is an acceptable character for replacement of &lt;code&gt;\n&lt;/code&gt; in a FASTQ record.  If you are happy with a file format that is close to but not exactly FASTQ, then you could do the following:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Paste each record into a single line.&lt;/li&gt;&#xA;&lt;li&gt;Add a dummy chromosome and line number as the first and second column.&lt;/li&gt;&#xA;&lt;li&gt;Compress with &lt;code&gt;bgzip&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Index the file and perform lookups with &lt;a href=&quot;http://www.htslib.org/doc/tabix.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;tabix&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Note that we need to remove leading spaces introduced by &lt;code&gt;nl&lt;/code&gt; and we need to introduce a dummy chromosome column to keep &lt;code&gt;tabix&lt;/code&gt; happy:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;gzip -dc input.fastq.gz | paste - - - - | nl | sed 's/^ *//' | sed 's/^/dummy\t/' | bgzip -c &amp;gt; output.fastq.gz&#xA;tabix -s 1 -b 2 -e 2 output.fastq.gz &#xA;&#xA;# now retrieve the 5th record (1-based) in log(n) time&#xA;tabix output.fastq.gz dummy:5-5 &#xA;&#xA;# This command will retrieve the 5th record and convert it record back into FASTQ format&#xA;tabix output.fastq.gz dummy:5-5 | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&#xA;&#xA;# Count the number of records for part two of this question&#xA;export N_RECORDS=$(gzip -dc output.fastq.gz | wc -l)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h1&gt;Random record in constant time&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Now that we have a way of retrieving an arbitrary record in &lt;code&gt;log(n)&lt;/code&gt; time, retrieving a random record is simply a matter of getting a good random number generator and sampling. Here is some example code to do this in python:&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Using grabix&lt;/h2&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;# random_read.py&#xA;import os&#xA;import random&#xA;&#xA;n_records = int(os.environ[&quot;N_LINES&quot;]) // 4&#xA;rand_record_start = random.randrange(0, n_records) * 4 + 1&#xA;rand_record_end = rand_record_start + 3&#xA;os.system(&quot;grabix grab output.fastq.gz {0} {1}&quot;.format(rand_record_start, rand_record_end))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Using tabix&lt;/h2&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;# random_read.py&#xA;import os&#xA;import random&#xA;&#xA;n_records = int(os.environ[&quot;N_RECORDS&quot;])&#xA;rand_record_index = random.randrange(0, n_records) + 1&#xA;# super ugly, but works...&#xA;os.system(&#xA;    &quot;tabix output.fastq.gz dummy:{0}-{0} | perl -pe 's/^dummy\t\d+\t//' | tr '\t' '\n'&quot;.format(&#xA;        rand_record_index)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And this works for me:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;python3.5 random_read.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Please note that &lt;code&gt;os.system&lt;/code&gt; calls a system shell and is vulnerable to &lt;a href=&quot;https://en.wikipedia.org/wiki/Code_injection#Shell_injection&quot; rel=&quot;nofollow noreferrer&quot;&gt;shell injection vulnerabilities&lt;/a&gt;. If you are writing production code, then you probably want to &lt;a href=&quot;https://docs.python.org/3/library/subprocess.html#security-considerations&quot; rel=&quot;nofollow noreferrer&quot;&gt;take extra precautions&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks to &lt;a href=&quot;https://bioinformatics.stackexchange.com/users/104/chris-rands&quot;&gt;Chris_Rands&lt;/a&gt; for raising this issue.&lt;/p&gt;&#xA;" OwnerUserId="492" LastEditorUserId="191" LastEditDate="2017-06-22T11:05:54.213" LastActivityDate="2017-06-22T11:05:54.213" CommentCount="0" />
  <row Id="814" PostTypeId="2" ParentId="807" CreationDate="2017-06-19T15:02:15.580" Score="0" Body="&lt;p&gt;A quick and dirty solution could be to convert the FASTQ file to two FASTAs, one of the them storing the bases and the other one storing the qualities, and then use standard methods for random access for FASTA.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only complication is that base qualities can contain &lt;code&gt;&amp;gt;&lt;/code&gt;. However, this is a problem only when it is the first character of a line and we can fix it by prepending, e.g., &lt;code&gt;I&lt;/code&gt; to each quality sequence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If also other people think that this could be a sufficient solution, I will expand the answer and add all the commands.&lt;/p&gt;&#xA;" OwnerUserId="425" LastActivityDate="2017-06-19T15:02:15.580" CommentCount="7" />
  <row Id="815" PostTypeId="1" AcceptedAnswerId="816" CreationDate="2017-06-19T15:25:03.003" Score="3" ViewCount="35" Body="&lt;p&gt;How to extract the sequence used to create a blast database.  This is useful when you download a blastdb from somewhere else e.g. one of the &lt;a href=&quot;https://ftp.ncbi.nlm.nih.gov/blast/db&quot; rel=&quot;nofollow noreferrer&quot;&gt;databases provided by NCBI&lt;/a&gt; including the 16SMicrobial database. Or alternatively, when you want to double check which version of a sequence you have included in a blastdb.&lt;/p&gt;&#xA;" OwnerUserId="302" LastActivityDate="2017-06-19T15:25:03.003" Title="How to extract fasta from a blastdb" Tags="&lt;fasta&gt;&lt;blast&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="816" PostTypeId="2" ParentId="815" CreationDate="2017-06-19T15:25:03.003" Score="2" Body="&lt;p&gt;You can extract fasta sequence from a blastdb constructed from a fasta file using blastdbcmd which should be installed when you install blast/makeblastdb.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blastdbcmd -entry all -db &amp;lt;database label&amp;gt; -out &amp;lt;outfile&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you had a database called &lt;code&gt;my_database&lt;/code&gt; which contained the files:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;my_database.nhr&lt;/code&gt; &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;my_database.nsq&lt;/code&gt; &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;my_database.nin&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;and you wanted your fasta output file to be called &lt;code&gt;reference.fasta&lt;/code&gt; you would run the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blastdbcmd -entry all -db my_database -out reference.fasta&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="302" LastActivityDate="2017-06-19T15:25:03.003" CommentCount="0" />
  <row Id="817" PostTypeId="1" AcceptedAnswerId="846" CreationDate="2017-06-19T15:37:17.460" Score="0" ViewCount="59" Body="&lt;p&gt;I used blastn to search one a genome database for the sequences in a &lt;a href=&quot;http://ix.io/xFG&quot; rel=&quot;nofollow noreferrer&quot;&gt;file&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blastn -out output.txt -outfmt 6 -query sequence_list -db genome_database -perc_identity 100&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;These sequences are all from one contig of the genome so the output was all hits in one contig. I then &lt;strong&gt;appended&lt;/strong&gt; more sequences to the sequence file and used blastn again. However the weird thing is the output did not start with the same hits as the last blast. I expected if blastn iterated through sequences in a query file that it would output the original sequences first then go through the appended sequences. Since this was not the case, how does blast order its output?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also checked if it was sorted by which contig the hit was in and this is not the case. &lt;/p&gt;&#xA;" OwnerUserId="842" LastActivityDate="2017-06-21T18:01:18.207" Title="How does blastn sort output?" Tags="&lt;alignment&gt;&lt;blast&gt;&lt;sequence-analysis&gt;" AnswerCount="3" CommentCount="1" />
  <row Id="818" PostTypeId="1" AcceptedAnswerId="831" CreationDate="2017-06-19T17:41:36.857" Score="3" ViewCount="36" Body="&lt;p&gt;I had a protein Refseq ID and I PSI-BLASTed this sequence against Refseq database. We all know that the Refseq is a Reference sequence database and it shouldn't have redundancy. After BLASTing my sequence, at first iteration I got 1000 hits and among them there were a lot of redundant sequences! &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My sequence had 241 amino acids and I found a lot of sequences with 100% identity, 100% cover and 0 E-value exactly the same as my sequence but with different IDs. All of these IDs were from RefSeq! In other iterations and after adjusting format options, I got this redundancy with other sequences from other species. My sequence is related to a chain of a multichain protein (E.coli fumarate reductase)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, when using NP_418578 as a query, I found WP_078165098.1, WP_064226696.1, WP_062863447.1, WP_001401474.1 and other that were identical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to know what is wrong with Refseq. Is it really a Reference sequence database? If it is, where does this redundancy come from and why?&lt;/p&gt;&#xA;" OwnerUserId="818" LastEditorUserId="818" LastEditDate="2017-06-20T14:30:41.707" LastActivityDate="2017-06-20T14:30:41.707" Title="Duplicate long hits from PSI-BLAST" Tags="&lt;blast&gt;&lt;sequence-homology&gt;&lt;refseq&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="819" PostTypeId="2" ParentId="782" CreationDate="2017-06-19T19:55:37.043" Score="2" Body="&lt;p&gt;You should be able to remove any one of the following strains to end up with a rank-sufficient model matrix: 5, 10, 12, 13, 14, 15, 19, 26, 28, 3, 30, 32, 36, 39, 41, 45, 46, 49, 5, 50, 52, 53, 58, 59, 60, 69, 8. As an aside you can figure this sort of thing out as follows (I read your dataframe in a the &lt;code&gt;d&lt;/code&gt; object):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; m = model.matrix(~batch+strain, d)&#xA;&amp;gt; dim(m) # 142 row, 77 columns, so minimum rank is 77&#xA;&amp;gt; qr(m)$rank # 76, so just barely rank insufficient&#xA;&amp;gt; #see if we can remove a single column and still get rank 76&#xA;&amp;gt; colnames(m)[which(sapply(1:77, function(x) qr(m[,-x])$rank) == 76)]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You obviously don't want to remove the batch columns or the intercept. The normal tricks that you can sometimes use to get around this issue with case-control studies don't appear to help here, which is why I would just drop a strain and call it done. Keep in mind that your power is still likely terrible. I generally recommend at least 6 replicates per group (scale down the number of groups to fit your budget).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once the desired strain is removed from the model, it can be fit directly into the &lt;code&gt;sleuth_fit&lt;/code&gt; function to obtain the full model:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; m = m[, -9] # whatever column to drop to get the appropriate rank&#xA;&amp;gt; so &amp;lt;- sleuth_fit(so, m, 'full')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="77" LastEditorUserId="123" LastEditDate="2017-06-20T11:00:23.183" LastActivityDate="2017-06-20T11:00:23.183" CommentCount="1" />
  <row Id="820" PostTypeId="2" ParentId="807" CreationDate="2017-06-19T20:19:14.163" Score="1" Body="&lt;p&gt;I wrote &lt;a href=&quot;https://github.com/alexpreynolds/sample&quot; rel=&quot;nofollow noreferrer&quot;&gt;a tool called &lt;code&gt;sample&lt;/code&gt;&lt;/a&gt; that you can use to do random sampling without reading the entire file into memory. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It can be used where GNU &lt;code&gt;shuf&lt;/code&gt; fails for lack of sufficient memory. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It requires two passes through the file to do a random sample, but the second pass is generally fast(er) as it uses &lt;code&gt;mmap&lt;/code&gt; routines to do cached reads. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you do repeated samples, the repeated samples are also &lt;code&gt;mmap&lt;/code&gt;-ed (cached) and will run quickly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You might use it on a FASTQ file like so:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;$ sample -k 1234 -l 4 in.fq &amp;gt; out.fq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It parses the input file into records by every four newline characters (such as the format of a FASTQ file), reading line offset positions into memory. So the memory overhead is relatively very low.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It then applies reservoir sampling on those line offsets to write out a random sample (say, &lt;code&gt;1234&lt;/code&gt; records in this example) to standard output.&lt;/p&gt;&#xA;" OwnerUserId="776" LastEditorUserId="191" LastEditDate="2017-06-22T11:07:39.083" LastActivityDate="2017-06-22T11:07:39.083" CommentCount="2" />
  <row Id="821" PostTypeId="1" AcceptedAnswerId="823" CreationDate="2017-06-19T20:49:17.313" Score="3" ViewCount="37" Body="&lt;p&gt;My understanding is that indels are from 1bp to 10Kb, and a healthy genome has ~400K-500K Indels. Surely most of these are small. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the distribution of insertion sizes in a healthy human genome? What is the distribution of deletion sizes? What is an average ratio of insertions:deletions? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have not found a definitive reference.&lt;/p&gt;&#xA;" OwnerUserId="146" LastEditorUserId="57" LastEditDate="2017-06-20T06:03:38.873" LastActivityDate="2017-06-20T06:03:38.873" Title="What is the distribution of indel sizes in a healthy human genome? of insertion:deletion ratios?" Tags="&lt;variants&gt;&lt;indel&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="822" PostTypeId="2" ParentId="807" CreationDate="2017-06-19T21:30:20.647" Score="1" Body="&lt;p&gt;Here's another approach that doesn't require any indexing, using &lt;a href=&quot;http://bedops.readthedocs.io/en/latest/content/reference/set-operations/bedextract.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;BEDOPS &lt;code&gt;bedextract&lt;/code&gt;&lt;/a&gt; to do a &lt;code&gt;log(n)&lt;/code&gt; sample on a sorted BED file. Your sample contains random records with equal probability &lt;code&gt;1/n&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This approach requires a single &lt;code&gt;O(n)&lt;/code&gt; pass through the file to transform it to a BED file:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;$ cat records.fastq | paste - - - - | awk '{ print &quot;chrZ\t&quot;s&quot;\t&quot;(s+1)&quot;$0 }' &amp;gt; records.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Store the intervals in a separate file:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;$ cut -f1-3 records.bed &amp;gt; intervals.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To do a random sample of &lt;code&gt;k&lt;/code&gt; elements, shuffle the intervals file and preserve the order of shuffled elements. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can do this with the &lt;a href=&quot;https://github.com/alexpreynolds/sample&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;sample&lt;/code&gt; tool&lt;/a&gt; I outlined earlier:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;$ sample -k ${K} -s intervals.bed &amp;gt; intervals-sample.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or you can &lt;code&gt;shuf&lt;/code&gt; and &lt;code&gt;sort-bed&lt;/code&gt; to do the same thing:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;$ shuf -n ${K} intervals.bed | sort-bed - &amp;gt; intervals-sample.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There's an &lt;code&gt;O(klog(k))&lt;/code&gt; cost here, but if &lt;code&gt;k &amp;lt;&amp;lt;&amp;lt; n&lt;/code&gt;, i.e., you're working with whole-genome scale input, this cost is amortized over the &lt;code&gt;log(n)&lt;/code&gt; search performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next, use &lt;code&gt;bedextract&lt;/code&gt; to do a binary search on the records, and delinearize to get back to FASTQ:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;$ bedextract records.bed intervals-sample.bed | cut -f4 | tr '\t' '\n' &amp;gt; sample.fq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With Unix I/O streams, this can be done in one pass:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;$ sample -k ${K} -s intervals.bed | bedextract records.bed - | cut -f4 | tr '\t' '\n' &amp;gt; sample.fq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;By baking the sort order into &lt;code&gt;records.bed&lt;/code&gt;, you're guaranteed the ability to do a binary search, which is &lt;code&gt;log(n)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Further, by linearizing the FASTQ input to a BED file and querying on BED intervals, you have equal probability of picking any one interval (interval == FQ record). You can draw an unbiased sample without the hassle of creating and storing a separate index.&lt;/p&gt;&#xA;" OwnerUserId="776" LastEditorUserId="191" LastEditDate="2017-06-22T11:07:13.503" LastActivityDate="2017-06-22T11:07:13.503" CommentCount="0" />
  <row Id="823" PostTypeId="2" ParentId="821" CreationDate="2017-06-19T21:43:05.247" Score="3" Body="&lt;p&gt;One of the &lt;a href=&quot;https://www.nature.com/nature/journal/v526/n7571/full/nature15394.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;2015 papers from the 1000 genomes project&lt;/a&gt; has a nice figure (figure 1) showing the size distribution of medium to large sized insertions and deletions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/bFMCX.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/bFMCX.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From &lt;a href=&quot;https://www.nature.com/nature/journal/v526/n7571/full/nature15393.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;another 2015 1000 genomes paper&lt;/a&gt;, one can see that the absolute number of smaller indels is much larger, though an exact size range isn't given (as far as I saw). If you really want to know that, just download &lt;a href=&quot;ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/&quot; rel=&quot;nofollow noreferrer&quot;&gt;the most recent 1000 genomes VCF file(s)&lt;/a&gt; and compute the exact median size and/or distribution that you want.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-19T21:43:05.247" CommentCount="0" />
  <row Id="824" PostTypeId="2" ParentId="807" CreationDate="2017-06-20T00:43:57.250" Score="1" Body="&lt;p&gt;I wrote a tool called &lt;a href=&quot;https://github.com/mdshw5/strandex&quot; rel=&quot;nofollow noreferrer&quot;&gt;strandex&lt;/a&gt; that matches entire FASTQ records starting at a random offset inside a decompressed file (since non-block compressed gzip files are not seek-able). It was a response to &lt;a href=&quot;https://github.com/dib-lab/khmer/issues/1002#issuecomment-102649433&quot; rel=&quot;nofollow noreferrer&quot;&gt;this comment&lt;/a&gt; about the &lt;code&gt;screed&lt;/code&gt; FASTQ parser in &lt;a href=&quot;https://github.com/dib-lab/khmer&quot; rel=&quot;nofollow noreferrer&quot;&gt;khmer&lt;/a&gt;, and was initially just a proof of concept but it's been useful for me and a few other people. The general idea is to:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;seek to a random offset in the file&lt;/li&gt;&#xA;&lt;li&gt;read a chunk of the file&lt;/li&gt;&#xA;&lt;li&gt;test if the regex pattern &lt;code&gt;@.+[\n\r]+.+[\n\r]+\+.*?[\n\r].+[\n\r]&lt;/code&gt; matches&lt;/li&gt;&#xA;&lt;li&gt;if so, extract the matching FASTQ record using regex capture groups&lt;/li&gt;&#xA;&lt;li&gt;if no match, goto step 2&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The CLI script and python library can be installed with &lt;code&gt;pip install strandex&lt;/code&gt;, and by default the script samples &lt;code&gt;-n&lt;/code&gt; reads using file offsets starting at a random number (reproducable by setting &lt;code&gt;-s&lt;/code&gt; seed), then moves a number of bytes to uniformly sample the entire file, determined by the file size. In this way the process is not truly random, but random enough, and avoids sampling any part of the file too much. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you specify &lt;code&gt;-n&lt;/code&gt; reads less than the total number in the file you get &lt;em&gt;down&lt;/em&gt;sampling, and if &lt;code&gt;-n&lt;/code&gt; is greater than the total number of reads you get &lt;em&gt;up&lt;/em&gt;sampling. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The advantage of this method over others is - it's a one-pass method, and since it's using the python C internals regex engine it's quite fast.&lt;/p&gt;&#xA;" OwnerUserId="267" LastActivityDate="2017-06-20T00:43:57.250" CommentCount="3" />
  <row Id="825" PostTypeId="2" ParentId="722" CreationDate="2017-06-20T01:49:07.367" Score="2" Body="&lt;p&gt;&lt;a href=&quot;http://bioviz.org/igb/&quot; rel=&quot;nofollow noreferrer&quot;&gt;IGB&lt;/a&gt; gives the intended result without much hassle. It doesn't have the bells and whistles of IGV, but presents a clean and intuitive view of the individual reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In IGB, each read is presented as a single, contiguous line. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/n80lR.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/n80lR.png&quot; alt=&quot;IGB zoomed out view&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you zoom in you can see the read ID for each read.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/ntXKs.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ntXKs.png&quot; alt=&quot;IGB zoomed in view&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a more holistic view, IGV's Sashimi plot can be modified to exclude spurious junctions supported by few reads. Here is a section of the above gene with default settings:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/uuaLY.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/uuaLY.png&quot; alt=&quot;Sashimi plot with default settings&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The minimum junction coverage can be set by right-clicking on the Sashimi plot itself, or application-wide under View -&gt; Preferences -&gt; Alignments -&gt; Splice Junction Track Options. With minimum junction coverage 3, the noisy plot above now looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/A4GTP.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/A4GTP.png&quot; alt=&quot;Sashimi plot with min junction coverage 3&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The numbers are still hard to read due to numerous overlapping almost-identical junctions, but at least the plot is now readable.&lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="163" LastEditDate="2017-06-20T02:12:39.600" LastActivityDate="2017-06-20T02:12:39.600" CommentCount="0" />
  <row Id="826" PostTypeId="2" ParentId="821" CreationDate="2017-06-20T02:17:29.127" Score="2" Body="&lt;p&gt;Genome-In-A-Bottle (GIAB; version 3.3.2) contains 3163k autosomal SNPs, 278k &amp;lt;=50bp deletions and 257k &amp;lt;=50bp insertions. This is on the lower end because GIAB ignores hard regions that tend to harbor more indels. On the CHM1-CHM13 pacbio assembly (European ancestry), there are 3460k autosomal SNPs, 507k &amp;lt;=50bp deletions and 549k &amp;lt;=50 insertions. However, due to PacBio consensus errors, this 507k+549k is probably an overestimate. I would &lt;em&gt;guess&lt;/em&gt; for a non-African sample, there should be 900–1000k &amp;lt;=50 indels, though only ~80% of them can be called with ~150bp short reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For indels longer than 50bp, I would highly recommend to read &lt;a href=&quot;https://www.nature.com/nature/journal/v517/n7536/full/nature13907.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;this paper&lt;/a&gt; by Chaisson et al. This call set is constructed from the whole-genome assembly of the CHM1 genome. It is far more comprehensive and probably more accurate than all the other call sets. The following is Table 1 from the paper:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/om410.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/om410.png&quot; alt=&quot;CHM1-SVs&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have to say that the high ins-to-del ratio is surprising to me, but this is what the data tells us. I have done a similar analysis and reached a similar ratio.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: These numbers are essentially from one haplotype. They will be higher for a diploid sample.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-20T02:17:29.127" CommentCount="0" />
  <row Id="827" PostTypeId="2" ParentId="817" CreationDate="2017-06-20T03:16:04.833" Score="1" Body="&lt;p&gt;BLASTN should definitely report alignments in the order that the query sequences were provided.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Based on your description and my personal experience, I think the most likely explanation is some kind of mix up. It happens to all of us, especially when we're in a rush to get an answer, but even sometimes when we're being careful and disciplined.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you can go back and recreate the result, I would be very interested to see the two pairs of input and output files. If not, I wouldn't spend too much more time worrying about it. :-)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt; Alignments are reported in the order that the query sequences are provided, even in tabular mode. The order of each query's hits is sorted by bitscore or e-value, but that is a secondary sorting. I've included an example below that has two queries, &lt;code&gt;seq1&lt;/code&gt; and &lt;code&gt;seq2&lt;/code&gt; with &lt;code&gt;-outfmt 6&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;seq1    scaffold2       100.000 280     0       0       1       280     11131   11410   7.73e-146       518&#xA;seq1    scaffold294     88.693  283     25      4       1       279     12160   12439   6.46e-92        339&#xA;seq1    scaffold1525    85.315  286     29      9       1       277     27142   26861   3.07e-75        283&#xA;seq1    scaffold1874    83.505  291     32      10      1       280     8558    8273    1.86e-67        257&#xA;seq1    scaffold147     82.653  294     34      9       1       279     10797   10506   1.45e-63        244&#xA;seq1    scaffold2478    82.374  278     38      8       11      280     7622    7896    1.13e-59        231&#xA;seq1    scaffold3405    87.940  199     17      5       1       196     14390   14584   1.46e-58        228&#xA;seq1    scaffold395     83.060  183     23      4       106     280     12021   11839   5.40e-38        159&#xA;seq1    scaffold2853    85.965  114     13      3       1       113     20864   20753   9.16e-26        119&#xA;seq2    scaffold9       100.000 140     0       0       1       140     1121    1260    2.34e-68        259&#xA;seq2    scaffold2950    90.909  110     6       3       1       106     110     1       6.84e-34        145&#xA;seq2    scaffold3416    95.385  65      3       0       1       65      11882   11818   1.16e-21        104&#xA;seq2    scaffold3103    95.385  65      3       0       1       65      9736    9672    1.16e-21        104&#xA;seq2    scaffold5297    88.608  79      7       1       64      140     150     228     6.99e-19        95.3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-06-21T17:39:45.060" LastActivityDate="2017-06-21T17:39:45.060" CommentCount="3" />
  <row Id="828" PostTypeId="2" ParentId="633" CreationDate="2017-06-20T08:34:17.377" Score="1" Body="&lt;p&gt;In the &lt;a href=&quot;http://cancerres.aacrjournals.org/content/72/14/3499.full&quot; rel=&quot;nofollow noreferrer&quot;&gt;article&lt;/a&gt; describing Cellminer says about the cross-correlations:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;... a cross-correlation table of the resultant z-scores can be&#xA;  generated ...&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;On page 3503&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And the z-scores are defined previously as:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The tool output includes relative transcript intensity presented&#xA;  as z-scores...&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-06-20T08:34:17.377" CommentCount="0" />
  <row Id="829" PostTypeId="1" CreationDate="2017-06-20T08:58:36.057" Score="1" ViewCount="63" Body="&lt;p&gt;I would like to look if there are mutations in residues of human histones associated with any disease. For instance, if a mutation in residue K6 (lysine 6) of histone &lt;a href=&quot;http://www.uniprot.org/uniprot/Q96QV6#function&quot; rel=&quot;nofollow noreferrer&quot;&gt;H2A1A&lt;/a&gt; is associated with any human disease, (with PUBMED evidence, preferably), but in a systematic way (for all types of histones and all residues). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The purpose of this is to get some candidate residues to study. I have checked the &lt;a href=&quot;https://www.omim.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;OMIM&lt;/a&gt; database but I didn't find anything relevant. It seems that there is no systematic way to connect specific residues to diseases and search for them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any web database or API available for this purpose?&lt;/p&gt;&#xA;" OwnerUserId="678" LastEditorUserId="298" LastEditDate="2017-06-20T13:26:01.623" LastActivityDate="2017-07-14T16:12:50.580" Title="How can I find mutations associated with disease in human histone residues?" Tags="&lt;database&gt;&lt;variants&gt;&lt;api&gt;" AnswerCount="2" CommentCount="7" FavoriteCount="0" />
  <row Id="831" PostTypeId="2" ParentId="818" CreationDate="2017-06-20T12:11:46.953" Score="2" Body="&lt;p&gt;This is what is known as a feature, not a bug. Note that your identical proteins all have accessions starting with &lt;code&gt;WP_&lt;/code&gt;. These are special &quot;non-redundant proteins&quot;. Many sequences—particularly bacterial sequences—are identical between various different species so having a separate RefSeq entry for each of them would be inefficient. Therefore, RefSeq combines multiple such proteins into a single &lt;code&gt;WP_&lt;/code&gt; record. This is documented &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/refseq/about/nonredundantproteins/&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; (emphasis mine):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;A new type of RefSeq protein record which represents non-redundant protein sequences was introduced in mid-2013. This record type was introduced to address a growing issue with redundancy in the Prokaryotic RefSeq protein dataset that coincided with a significant increase in bacterial genome submissions from individual isolates and closely related bacterial strains. For example, a large number of high-quality bacterial genomes may be submitted during a disease outbreak. The submitted sequences may reflect pathogen evolution during the course of the outbreak but the majority of the encoded proteins from these genomes may be identical to each other. As RefSeq includes these genomes, per community requests, this resulted in increased redundancy. &lt;strong&gt;By representing identical proteins using a single non-redundant protein accession number (with the prefix 'WP_'), redundancy in the database is significantly reduced.&lt;/strong&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[ . . . ]&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Because a non-redundant protein sequence may be found in RefSeq genomes from multiple species, &lt;strong&gt;the organism information provided on the protein record reflects the lowest-common taxonomic node ranging from the genus species level to super-kingdom&lt;/strong&gt;. A non-redundant protein record that provides organism information at the level of a genus, family, or even super-kingdom does not mean that the protein is found in all RefSeq genomes below that taxonomic classification. It only indicates that the protein is found in more than one genome of different species for which the genus, family, or super-kingdom classification is the lowest common taxonomic node. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So, your query was &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/protein/NP_418578&quot; rel=&quot;nofollow noreferrer&quot;&gt;NP_418578.1&lt;/a&gt;, anaerobic fumarate reductase catalytic and NAD/flavoprotein subunit from &lt;em&gt;E. coli&lt;/em&gt; strain K-12, substrain MG1655. The first thing to notice is how specific that is. This is the protein found from one specific substrain of one specific strain of one specific bacterial species. It is reasonable to expect that there will be identical sequences from many, many closely related species. Both from, most probably, all other strains and substrains of &lt;em&gt;E. coli&lt;/em&gt; and from other, related bacteria. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, the specific sequences you mention are actually &lt;em&gt;slightly&lt;/em&gt; different and not 100% identical. Below is a multiple alignment of NP_418578.1 and the 4 WP_ sequences you mentioned. Note that each of the 5 entries is slightly different. Each has one residue that differs from the rest. Look for the &lt;code&gt;:&lt;/code&gt; in the identity line, there are 4 &lt;code&gt;:&lt;/code&gt; and all others are &lt;code&gt;*&lt;/code&gt; (I am only showing the relevant alignment blocks here and have removed those where all 4 sequences were identical):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;WP_001401474.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xA;WP_062863447.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xA;WP_064226696.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xA;NP_418578.1         MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xA;WP_078165098.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAIAQDH&#xA;                    *******************************************************:****&#xA;&#xA;[ . . . ]&#xA;&#xA;WP_001401474.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xA;WP_062863447.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xA;WP_064226696.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHIRGLVAMNMMEGT&#xA;NP_418578.1         KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xA;WP_078165098.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xA;                    ***********************************************:************&#xA;&#xA;[ . . . ]&#xA;&#xA;WP_001401474.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xA;WP_062863447.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKISQAFWHEWR&#xA;WP_064226696.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xA;NP_418578.1         GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xA;WP_078165098.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xA;                    **************************************************:*********&#xA;&#xA;WP_001401474.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGIDPVKEPIPVRPTAHYTMG&#xA;WP_062863447.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xA;WP_064226696.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xA;NP_418578.1         KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xA;WP_078165098.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xA;                    *****************************************:******************&#xA;&#xA;[ . . . ]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Your sequence (NP_418578.1) is only identical to one &lt;code&gt;WP_*&lt;/code&gt; multi-species sequence, WP_001192973:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;WP_001192973.1      MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xA;NP_418578.1         MQTFQADLAIVGAGGAGLRAAIAAAQANPNAKIALISKVYPMRSHTVAAEGGSAAVAQDH&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      DSFEYHFHDTVAGGDWLCEQDVVDYFVHHCPTEMTQLELWGCPWSRRPDGSVNVRRFGGM&#xA;NP_418578.1         DSFEYHFHDTVAGGDWLCEQDVVDYFVHHCPTEMTQLELWGCPWSRRPDGSVNVRRFGGM&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xA;NP_418578.1         KIERTWFAADKTGFHMLHTLFQTSLQFPQIQRFDEHFVLDILVDDGHVRGLVAMNMMEGT&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      LVQIRANAVVMATGGAGRVYRYNTNGGIVTGDGMGMALSHGVPLRDMEFVQYHPTGLPGS&#xA;NP_418578.1         LVQIRANAVVMATGGAGRVYRYNTNGGIVTGDGMGMALSHGVPLRDMEFVQYHPTGLPGS&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xA;NP_418578.1         GILMTEGCRGEGGILVNKNGYRYLQDYGMGPETPLGEPKNKYMELGPRDKVSQAFWHEWR&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xA;NP_418578.1         KGNTISTPRGDVVYLDLRHLGEKKLHERLPFICELAKAYVGVDPVKEPIPVRPTAHYTMG&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      GIETDQNCETRIKGLFAVGECSSVGLHGANRLGSNSLAELVVFGRLAGEQATERAATAGN&#xA;NP_418578.1         GIETDQNCETRIKGLFAVGECSSVGLHGANRLGSNSLAELVVFGRLAGEQATERAATAGN&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      GNEAAIEAQAAGVEQRLKDLVNQDGGENWAKIRDEMGLAMEEGCGIYRTPELMQKTIDKL&#xA;NP_418578.1         GNEAAIEAQAAGVEQRLKDLVNQDGGENWAKIRDEMGLAMEEGCGIYRTPELMQKTIDKL&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      AELQERFKRVRITDTSSVFNTDLLYTIELGHGLNVAECMAHSAMARKESRGAHQRLDEGC&#xA;NP_418578.1         AELQERFKRVRITDTSSVFNTDLLYTIELGHGLNVAECMAHSAMARKESRGAHQRLDEGC&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      TERDDVNFLKHTLAFRDADGTTRLEYSDVKITTLPPAKRVYGGEADAADKAEAANKKEKA&#xA;NP_418578.1         TERDDVNFLKHTLAFRDADGTTRLEYSDVKITTLPPAKRVYGGEADAADKAEAANKKEKA&#xA;                    ************************************************************&#xA;&#xA;WP_001192973.1      NG&#xA;NP_418578.1         NG&#xA;                    **&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, in summary, RefSeq will combine multiple identical sequences into a single &lt;code&gt;WP_*&lt;/code&gt; multi-species accession. You should therefore expect to find one 100% identical &lt;code&gt;WP_*&lt;/code&gt; sequence for your query and multiple, &lt;em&gt;almost&lt;/em&gt; identical &lt;code&gt;WP_*&lt;/code&gt; entries. And that's precisely what you see here.&lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-06-20T12:11:46.953" CommentCount="2" />
  <row Id="834" PostTypeId="2" ParentId="817" CreationDate="2017-06-20T16:44:09.773" Score="0" Body="&lt;p&gt;I have analyzed the situation more and found that blastn automatically sorts by e-value. &lt;a href=&quot;http://ix.io/xLh&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://ix.io/xLh&lt;/a&gt; (column 11)&lt;/p&gt;&#xA;" OwnerUserId="842" LastActivityDate="2017-06-20T16:44:09.773" CommentCount="1" />
  <row Id="835" PostTypeId="2" ParentId="327" CreationDate="2017-06-20T16:44:39.103" Score="0" Body="&lt;p&gt;If you are looking for NGS QC for your fastq, bam, bed and vcf files I would suggest a commercial tool called &lt;a href=&quot;https://om.euformatics.com/apex/f?p=118:1:&quot; rel=&quot;nofollow noreferrer&quot;&gt;omnomicsQ&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It automates coverage analysis and sample pass, warn fail according according to your defined SOPs. Unlike fastqc it also comes with a database so that you can compare protocols, samples, and your performance to peer organisations. It also charts performance over time, exceptions,and  correlations between metrics&lt;/p&gt;&#xA;" OwnerUserId="924" LastActivityDate="2017-06-20T16:44:39.103" CommentCount="0" />
  <row Id="836" PostTypeId="2" ParentId="807" CreationDate="2017-06-20T17:20:24.900" Score="2" Body="&lt;p&gt;One of the most thorough treatments of this question (or a similar question: grabbing a random subset of reads) was given by Jared Simpson in a blog post a few years ago. &lt;a href=&quot;http://simpsonlab.github.io/2015/05/19/io-performance/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://simpsonlab.github.io/2015/05/19/io-performance/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you just want to grab a single random read, Jared's benchmarks suggest that seeking to a random position in the file and then retrieving the next complete read should be the most performant option. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, more generally, if you want to take a random subset of reads, there are many factors that affect performance.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-06-20T17:20:24.900" CommentCount="1" />
  <row Id="837" PostTypeId="1" AcceptedAnswerId="843" CreationDate="2017-06-20T19:23:49.107" Score="2" ViewCount="74" Body="&lt;p&gt;I'm beginning with the reference genome in FASTA format, hg19. I am reading the sequence into a Python dictionary with &lt;code&gt;BioPython&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;genome_dictionary = {} &#xA;for seq_record.id in SeqIO.parse(input_fasta_file, &quot;fasta&quot;):&#xA;    genome_dictionary[seq_record.id] = seq_record&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This creates a Python dictionary with keys as chromosome names and values as strings from the FASTA. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to take the FASTA, and simulate artificial SNPs and InDels along the FASTA to benchmark various algorithms. If I was only creating SNPs in the FASTA, this would be a trivial problem. The reference FASTA has the original coordinates, and the SNPs would also be in a coordinate system which is the same. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, when I randomly insert an insertion (e.g. an insertion of 12 bp), I move all events into a new coordinate system such that &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;new coordinate system = old coordinate system + 15&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This becomes very difficult to keep track off throughout the entire genome after several interations. I therefore cannot keep track of the new and old coordinate system without running an algorithm and creating a VCF, which may have errors. This defeats the purpose of the simulation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output would be either a BED file or VCF which has keep track of the changes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What method/data structure is used to solve this problem? Would LiftOver work? Maybe CrossMap? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://genome.sph.umich.edu/wiki/LiftOver&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://genome.sph.umich.edu/wiki/LiftOver&lt;/a&gt;&#xA;&lt;a href=&quot;http://crossmap.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://crossmap.sourceforge.net/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="146" LastEditorUserId="298" LastEditDate="2017-07-05T08:32:22.120" LastActivityDate="2017-07-05T08:32:22.120" Title="After artificially creating events in a FASTA file, how do I keep track of the old coordinates?" Tags="&lt;biopython&gt;&lt;bed&gt;&lt;structural-variation&gt;&lt;indel&gt;&lt;simulated-data&gt;" AnswerCount="2" CommentCount="19" />
  <row Id="838" PostTypeId="1" AcceptedAnswerId="868" CreationDate="2017-06-20T19:26:57.350" Score="1" ViewCount="108" Body="&lt;p&gt;I have two files &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;s3.txt :&#xA;1   10  20&#xA;1   5   20&#xA;2   20  30&#xA;2   25  30&#xA;1   10  50&#xA;2   20  60&#xA;1   14  17&#xA;&#xA;s4.txt:&#xA;1   10  20&#xA;2   20  30&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am trying to match col0 of both the files and get rows that fall between range(inclusive of themselves) 10-20 and 20-30 as seen in s4 file.&#xA;file s4 has co ordinates which can be used as reference range (chrom start and end) and s3 has list of co ordinates from an experimental condition what I am trying to achieve is to which co-ordinates from my file s3 fall on or between my reference co ordinates in s4.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;code so far:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;containing_ranges = []&#xA;with open('s4.txt', 'r') as f:&#xA;    for line in f:&#xA;        fields = line.strip().split('\t')&#xA;        containing_ranges.append(fields)&#xA;&#xA;tested_ranges = []       &#xA;with open('s3.txt', 'r') as f:&#xA;    for line in f:&#xA;        fields = line.strip().split('\t')&#xA;        tested_ranges.append(fields)&#xA;&#xA;for c_range in containing_ranges:&#xA;    for t_range in tested_ranges:&#xA;        tst = int(t_range[1])&#xA;        ten = int(t_range[2])&#xA;        cst = int(c_range[1])&#xA;        cen = int(c_range[2])&#xA;        if  c_range[0] == t_range[0]:&#xA;            included = cst &amp;gt;= tst and cen &amp;lt;= ten&#xA;            if included == True:&#xA;               print t_range&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Output with missing row(1   14  17) : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;['1', '10', '20']&#xA;['1', '5', '20']&#xA;['1', '10', '50']&#xA;['2', '20', '30']&#xA;['2', '20', '60']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Desired output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1   10  20&#xA;2   20  30&#xA;2   25  30&#xA;1   14  17&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Not sure if my logic is wrong and why does it miss 14-17 as it falls between 10-20&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[EDIT] using pybedtools&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; print(s4.intersect(s3, wb=True))&#xA;1   10  20  1   10  20&#xA;1   10  20  1   5   20&#xA;1   10  20  1   10  50&#xA;1   14  17  1   14  17&#xA;2   20  30  2   20  30&#xA;2   25  30  2   25  30&#xA;2   20  30  2   20  60&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; print(s4.intersect(s3, wa=True, wb=True, F=1))&#xA;1   10  20  1   10  20&#xA;1   10  20  1   14  17&#xA;2   20  30  2   20  30&#xA;2   20  30  2   25  30&#xA;&#xA;&#xA;using bedops &#xA;bin$ less answer.bed &#xA;1       5       20&#xA;1       10      20&#xA;1       10      50&#xA;1       14      17&#xA;2       20      30&#xA;2       20      60&#xA;2       25      30&#xA;&#xA;using @bli code(on python2.7)&#xA;('1', 10, 20)&#xA;('1', 14, 17)&#xA;('2', 20, 30)&#xA;('2', 25, 30)&#xA; why can I not see the interval 1 5 20&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="926" LastEditorUserId="926" LastEditDate="2017-06-21T14:47:38.947" LastActivityDate="2017-06-22T11:22:23.687" Title="Range overlap python error with genomic regions" Tags="&lt;bed&gt;&lt;genomics&gt;&lt;python&gt;&lt;homework&gt;" AnswerCount="3" CommentCount="14" FavoriteCount="1" />
  <row Id="839" PostTypeId="2" ParentId="838" CreationDate="2017-06-20T19:35:59.390" Score="1" Body="&lt;p&gt;You could use &lt;a href=&quot;http://bedops.readthedocs.io/en/latest/#&quot; rel=&quot;nofollow noreferrer&quot;&gt;BEDOPS&lt;/a&gt;, instead:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ sort-bed s3.txt &amp;gt; s3.bed&#xA;$ sort-bed s4.txt &amp;gt; s4.bed&#xA;$ bedops --element-of 1 s3.bed s4.bed &amp;gt; answer.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you need to run it from within Python, you could use &lt;code&gt;subprocess.check_output()&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import subprocess&#xA;...&#xA;try:&#xA;    result = subprocess.check_output(&quot;bedops --element-of 1 %s %s &amp;gt; %s&quot; % (set_a_fn, set_b_fn, answer_fn), shell=True)&#xA;except subprocess.CalledProcessError as err:&#xA;    raise SystemExit(&quot;Could not run bedops\n&quot;)&#xA;# do stuff with 'result'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="776" LastActivityDate="2017-06-20T19:35:59.390" CommentCount="10" />
  <row Id="840" PostTypeId="2" ParentId="838" CreationDate="2017-06-20T19:38:59.547" Score="3" Body="&lt;p&gt;You're reinventing &lt;code&gt;bedtools intersect&lt;/code&gt; (or bedops), for which there's &lt;a href=&quot;https://daler.github.io/pybedtools/&quot; rel=&quot;nofollow noreferrer&quot;&gt;already a convenient python module&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from pybedtools import BedTool&#xA;&#xA;s3 = BedTool('s3.bed')&#xA;s4 = BedTool('s4.bed')&#xA;&#xA;print(s4.intersect(s3, wa=True, wb=True, F=1))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;wb=True&lt;/code&gt; is equivalent to &lt;code&gt;-wb&lt;/code&gt; with &lt;code&gt;bedtools intersect&lt;/code&gt; on the command line. Similarly, &lt;code&gt;F=1&lt;/code&gt; is the same as &lt;code&gt;-F 1&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-22T11:22:23.687" LastActivityDate="2017-06-22T11:22:23.687" CommentCount="6" />
  <row Id="841" PostTypeId="2" ParentId="837" CreationDate="2017-06-20T20:04:39.883" Score="4" Body="&lt;p&gt;If you look for a program which would randomly introduce SNPs + short indels and then would save everything into a VCF file, &lt;a href=&quot;https://github.com/nh13/DWGSIM&quot; rel=&quot;nofollow noreferrer&quot;&gt;DWGsim&lt;/a&gt; or &lt;a href=&quot;https://github.com/seqan/seqan/blob/master/apps/mason2/README.mason_variator&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mason Variator&lt;/a&gt; could be a good choice. Then you can create a corresponding Chain file using &lt;code&gt;bcftools consensus -c&lt;/code&gt; and transform various formats between these two coordinate systems using &lt;a href=&quot;http://crossmap.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;CrossMap&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="425" LastActivityDate="2017-06-20T20:04:39.883" CommentCount="4" />
  <row Id="842" PostTypeId="1" CreationDate="2017-06-21T02:20:33.860" Score="3" ViewCount="66" Body="&lt;p&gt;I'm currently attempting association analysis with an extremely small set of patient exomes (n=10), with no control or parental exomes available. Downloading the ExAC VCF of variant sites (&lt;a href=&quot;http://exac.broadinstitute.org/downloads&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://exac.broadinstitute.org/downloads&lt;/a&gt;) or the 1000G integrated call sets (&lt;a href=&quot;http://ftp.1000genomes.ebi.ac.uk/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://ftp.1000genomes.ebi.ac.uk/&lt;/a&gt;) and combining this with our pooled patient VCFs has not been successful (I suspect the approach of attempting to merge such large VCFs generated from different pipelines is rather naive).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looking at the primary literature, I have gathered it should be possible to use these resources to help increase statistical power for our analysis. My question is how do I take these large .vcfs with many samples and successfully merge them to our patient .vcfs, such that the combined VCF can be used downstream to run analysis packages? (PODKAT, PLINK, etc.)&lt;/p&gt;&#xA;" OwnerUserId="927" LastEditorUserId="96" LastEditDate="2017-06-21T06:35:15.377" LastActivityDate="2017-08-20T14:57:35.197" Title="What is a good pipeline for using public domain exomes as controls?" Tags="&lt;public-databases&gt;&lt;variants&gt;&lt;sequencing&gt;&lt;exome&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="843" PostTypeId="2" ParentId="837" CreationDate="2017-06-21T02:49:42.480" Score="4" Body="&lt;p&gt;I've written a handful of programs from scratch to simulate mutations and variations in real or simulated sequences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The trick has always been to sort the variants by genomic coordinate, &lt;strong&gt;apply the variant with the largest coordinate first&lt;/strong&gt;, then apply the variant with the second largest coordinate, all the way down to the variant with the smallest coordinate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Indels and other variants that affect sequence structure only affect &lt;em&gt;subsequent&lt;/em&gt; coordinates. So going in reverse order ensures variants at the beginning of the sequence don't throw off variants at the end of the sequence.&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-06-22T20:27:43.903" LastActivityDate="2017-06-22T20:27:43.903" CommentCount="4" />
  <row Id="844" PostTypeId="1" CreationDate="2017-06-21T04:38:04.053" Score="-1" ViewCount="65" Body="&lt;p&gt;I have some difficulties to understand/interpret the &lt;a href=&quot;https://en.wikipedia.org/wiki/MAPK/ERK_pathway&quot; rel=&quot;nofollow noreferrer&quot;&gt;pathway map&lt;/a&gt; and how a gene-gene interaction list or DNA sequencing can map into pathways.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition what's the difference between &lt;a href=&quot;https://en.wikipedia.org/wiki/MAPK/ERK_pathway&quot; rel=&quot;nofollow noreferrer&quot;&gt;MARK/ERK pathway&lt;/a&gt; and the output of &lt;a href=&quot;http://www.kegg.jp/kegg-bin/show_pathway?hsa05200&quot; rel=&quot;nofollow noreferrer&quot;&gt;KEGG&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="734" LastEditorUserId="734" LastEditDate="2017-06-22T03:02:52.990" LastActivityDate="2017-06-22T03:02:52.990" Title="How can I find the relevant pathway map from gene-gene or protein-protein interaction list?" Tags="&lt;gene&gt;&lt;sequencing&gt;&lt;dna&gt;&lt;pathway&gt;&lt;kegg&gt;" AnswerCount="2" CommentCount="8" ClosedDate="2017-06-23T15:02:07.927" />
  <row Id="845" PostTypeId="2" ParentId="774" CreationDate="2017-06-21T08:07:33.170" Score="1" Body="&lt;p&gt;I am not sure if this has been done, is common, or are research lines but here is what I think it can be done with transcripts differences (aside from comparing the change of the expression of each transcript):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having the the different transcripts, enable to study also the relation between transcripts, which are co-expressed together or differently co-expressed. Maybe at individual level the switch between transcripts of a gene is not related to a condition but together with other transcripts it is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Transcripts usage could be also linked to structural or genomic changes. Either short/small changes or big. At genomic level SNP, inversions or methylation..., or structural changes such as compression of a region, different distances between chromosomes/regions... &lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-06-21T08:07:33.170" CommentCount="2" />
  <row Id="846" PostTypeId="2" ParentId="817" CreationDate="2017-06-21T08:42:53.453" Score="0" Body="&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; I stand corrected. @Daniel Standage correctly pointed out that the below is only true for a single query, or for the sorting of hits within those found by a given query. Otherwise they're sorted in the order they were given. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;BLAST results are sorted in descending order by the &quot;Bit score&quot; (column 12). Sorting by E. value would not be quite the same.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;i.e. column 11 (E value) will be presented in ascending order (an E. value of 0 is as good as it gets), and as it gets higher, your hit is getting worse. Likewise, bitscores are highest at the top and decrease down the list. A hit with E. value = 0 should have a high score. How high exactly depends on the hit length, so it's common to see many E values with 0.0, but the scores differ. Here's a snippet of one of my blast results:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PVCcif_ATCC43949  PAU_06042014  100.000  24180  0     0   1      24180  2233012  2257191  0.0  43606&#xA;PVCcif_ATCC43949  PAU_06042014  78.098   5593   1164  16  2765   8323   3219956  3225521  0.0  4547&#xA;PVCcif_ATCC43949  PAU_06042014  77.549   5394   1139  16  2948   8323   2376163  2370824  0.0  4260&#xA;PVCcif_ATCC43949  PAU_06042014  74.140   5669   1342  34  8490   14100  3225688  3231290  0.0  3573&#xA;PVCcif_ATCC43949  PAU_06042014  76.835   3665   825   9   16507  20150  3234191  3237852  0.0  2764&#xA;PVCcif_ATCC43949  PAU_06042014  76.871   2646   571   11  16625  19248  2362597  2359971  0.0  2001&#xA;PVCcif_ATCC43949  PAU_06042014  70.733   4131   1005  51  9047   13063  2368869  2364829  0.0  1954&#xA;PVCcif_ATCC43949  PAU_06042014  75.040   1891   448   7   8500   10383  2370771  2368898  0.0  1272&#xA;PVCcif_ATCC43949  PAU_06042014  78.986   1361   269   7   22766  24119  890051   891401   0.0  1150&#xA;PVCcif_ATCC43949  PAU_06042014  77.819   1082   217   5   536    1610   3217983  3219048  0.0  868&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, they all have E values of 0.0, but they differ quite a lot in score, this is because of the length of the alignment, and the different percentage IDs.&lt;/p&gt;&#xA;" OwnerUserId="929" LastEditorUserId="929" LastEditDate="2017-06-21T18:01:18.207" LastActivityDate="2017-06-21T18:01:18.207" CommentCount="2" />
  <row Id="847" PostTypeId="1" AcceptedAnswerId="852" CreationDate="2017-06-21T08:45:44.693" Score="2" ViewCount="39" Body="&lt;p&gt;We believe that if after running blast, the global identity between a resulting sequence and our query is at least 30%, we can say that two sequences are homologs. what is the difference between local identity and global identity and how can we calculate them?&#xA;the following file is the global alignment result of one of my PSI-BLAST hits against query:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Aligned_sequences: 2&#xA;# 1: NP_418577.1&#xA;# 2: WP_036312822.1&#xA;# Matrix: EBLOSUM62&#xA;# Gap_penalty: 10.0&#xA;# Extend_penalty: 0.5&#xA;#&#xA;# Length: 264&#xA;# Identity:      85/264 (32.2%)&#xA;# Similarity:   125/264 (47.3%)&#xA;# Gaps:          29/264 (11.0%)&#xA;# Score: 376.5&#xA;# &#xA;#&#xA;#=======================================&#xA;&#xA;NP_418577.1        1 -----------MAEMKNLKIE-------VVRYNPEVDTAPHSAFYEVPYD     32&#xA;                                ..|..:..|:       :.|::||||..|....|:|...&#xA;WP_036312822.      1 MTIVDSGAPADTQEANDSGIQSYLVTFIIRRFDPEVDAEPRWVDYDVEMY     50&#xA;&#xA;NP_418577.1       33 ATTSLLDALGYIKDNLAPDLSYRWSCRMAICGSCGMMVNNVPKLACKTFL     82&#xA;                     .|..:||||..||.::...||:|.||...||||..|.:|...:|||||.:&#xA;WP_036312822.     51 PTDRVLDALHRIKWDVDGTLSFRRSCAHGICGSDAMRINGRNRLACKTLI    100&#xA;&#xA;NP_418577.1       83 R--DYTDGMKVEALANFPIERDLVVDMTHFIESLEAIKPYIIGNSRTADQ    130&#xA;                     :  |.:..:.|||:...|:|:||:|||..|.||...::|::...|.....&#xA;WP_036312822.    101 KDLDISKPIYVEAIKGLPLEKDLIVDMDPFFESFRDVQPFLQPKSAPEPG    150&#xA;&#xA;NP_418577.1      131 GTNIQTPAQMAKYHQFSGCINCGLCYAACPQFGLNPEFIGPAAITLAHRY    180&#xA;                     ....|:....|.|...:.||.|..|.::||.|..:.::.|||||..|||:&#xA;WP_036312822.    151 KERFQSIKDRAVYDDTTKCILCAACTSSCPVFWTDGQYFGPAAIVNAHRF    200&#xA;&#xA;NP_418577.1      181 NEDSRDHGKKERMAQLNSQNGVWSCTFVGYCSEVCPKHVDPAAAIQQGKV    230&#xA;                     ..||||.....|:..||.:.|||.|.....|:|.||:.::...||.:.|.&#xA;WP_036312822.    201 IFDSRDDAADVRLDILNDKEGVWRCRTTFNCTEACPRGIEITKAIAEVKQ    250&#xA;&#xA;NP_418577.1      231 ESSKDFLIATLKPR    244&#xA;                     ...:.         &#xA;WP_036312822.    251 AVLRG---------    255&#xA;&#xA;&#xA;#---------------------------------------&#xA;#---------------------------------------&#xA;the following file is the local alignment of that hit against query:&#xA;Aligned_sequences: 2&#xA;# 1: NP_418577.1&#xA;# 2: WP_036312822.1&#xA;# Matrix: EBLOSUM62&#xA;# Gap_penalty: 10.0&#xA;# Extend_penalty: 0.5&#xA;#&#xA;# Length: 219&#xA;# Identity:      83/219 (37.9%)&#xA;# Similarity:   119/219 (54.3%)&#xA;# Gaps:           2/219 ( 0.9%)&#xA;# Score: 390.5&#xA;# &#xA;#&#xA;#=======================================&#xA;&#xA;NP_418577.1       13 RYNPEVDTAPHSAFYEVPYDATTSLLDALGYIKDNLAPDLSYRWSCRMAI     62&#xA;                     |::||||..|....|:|....|..:||||..||.::...||:|.||...|&#xA;WP_036312822.     31 RFDPEVDAEPRWVDYDVEMYPTDRVLDALHRIKWDVDGTLSFRRSCAHGI     80&#xA;&#xA;NP_418577.1       63 CGSCGMMVNNVPKLACKTFLR--DYTDGMKVEALANFPIERDLVVDMTHF    110&#xA;                     |||..|.:|...:|||||.::  |.:..:.|||:...|:|:||:|||..|&#xA;WP_036312822.     81 CGSDAMRINGRNRLACKTLIKDLDISKPIYVEAIKGLPLEKDLIVDMDPF    130&#xA;&#xA;NP_418577.1      111 IESLEAIKPYIIGNSRTADQGTNIQTPAQMAKYHQFSGCINCGLCYAACP    160&#xA;                     .||...::|::...|.........|:....|.|...:.||.|..|.::||&#xA;WP_036312822.    131 FESFRDVQPFLQPKSAPEPGKERFQSIKDRAVYDDTTKCILCAACTSSCP    180&#xA;&#xA;NP_418577.1      161 QFGLNPEFIGPAAITLAHRYNEDSRDHGKKERMAQLNSQNGVWSCTFVGY    210&#xA;                     .|..:.::.|||||..|||:..||||.....|:..||.:.|||.|.....&#xA;WP_036312822.    181 VFWTDGQYFGPAAIVNAHRFIFDSRDDAADVRLDILNDKEGVWRCRTTFN    230&#xA;&#xA;NP_418577.1      211 CSEVCPKHVDPAAAIQQGK    229&#xA;                     |:|.||:.::...||.:.|&#xA;WP_036312822.    231 CTEACPRGIEITKAIAEVK    249&#xA;&#xA;&#xA;#---------------------------------------&#xA;#---------------------------------------&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="818" LastEditorUserId="298" LastEditDate="2017-06-21T09:58:00.247" LastActivityDate="2017-06-21T09:58:00.247" Title="what is the difference between local identity and global identity for homolog finding?" Tags="&lt;sequence-homology&gt;" AnswerCount="2" CommentCount="8" FavoriteCount="0" />
  <row Id="848" PostTypeId="2" ParentId="847" CreationDate="2017-06-21T09:15:19.583" Score="1" Body="&lt;p&gt;Defining homology isn't as simple as an identity cutoff, however, you're right that homology seems likely given a certain identity. This is where e-value comes into play. See:&#xA;&lt;a href=&quot;https://en.wikipedia.org/wiki/Expected_value&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://en.wikipedia.org/wiki/Expected_value&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To answer the question, global-global requires that the alignment account for every base in the query and the target sequence. local-global requires a the full length of the query on a part of the target. local-local allows only the highest scoring part of the alignment between the query and target to be reported. i.e. one functional domain against another. these hits are often called HSPs.&lt;/p&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-06-21T09:15:19.583" CommentCount="0" />
  <row Id="849" PostTypeId="1" CreationDate="2017-06-21T09:18:16.357" Score="3" ViewCount="38" Body="&lt;p&gt;For example, see this gene (nad1) in ENA:&#xA;&lt;a href=&quot;http://www.ebi.ac.uk/ena/data/view/ABI60879&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.ebi.ac.uk/ena/data/view/ABI60879&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you look at the XML for that gene you see the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;join(&#xA;             DQ984518.1: 324706 .. 325091 ,&#xA;  complement(DQ984518.1:  24417 ..  24498),&#xA;  complement(DQ984518.1:  22828 ..  23019),&#xA;             DQ984518.1:   3484 ..   3542 ,&#xA;  complement(DQ984518.1: 153702 .. 153960)&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which shows 5 exons joined out of phase and out of order. Is there a valid GTF representation of this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to dump a 'non-canonically spliced' gene into GTF? i.e. what's the recommendation?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cross-posted on &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/849/how-to-represent-trans-spliced-genes-in-gtf/855#855&quot;&gt;biostars&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="931" LastEditorUserId="96" LastEditDate="2017-06-21T19:46:55.577" LastActivityDate="2017-06-21T19:46:55.577" Title="How to represent trans-spliced genes in GTF?" Tags="&lt;file-formats&gt;&lt;gtf&gt;&lt;rna-splicing&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="1" />
  <row Id="850" PostTypeId="2" ParentId="844" CreationDate="2017-06-21T09:21:15.913" Score="1" Body="&lt;p&gt;See a project like Plant reactome, where they infer the pathways of all sequenced plant genomes using orthology to well annotated species.&#xA;&lt;a href=&quot;http://plantreactome.gramene.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://plantreactome.gramene.org/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-06-21T09:21:15.913" CommentCount="0" />
  <row Id="851" PostTypeId="2" ParentId="842" CreationDate="2017-06-21T09:26:47.787" Score="0" Body="&lt;p&gt;I gather that you want to use the background SNP frequency as a prior for input to your SNP calling algorithm?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure of a canned algorithm for doing this, but a quick google shows up some promising links:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3593722/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3593722/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/article.php?id=4723&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://software.broadinstitute.org/gatk/documentation/article.php?id=4723&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0489-0&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0489-0&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="931" LastEditorUserId="292" LastEditDate="2017-06-21T14:00:21.773" LastActivityDate="2017-06-21T14:00:21.773" CommentCount="2" />
  <row Id="852" PostTypeId="2" ParentId="847" CreationDate="2017-06-21T09:41:44.030" Score="3" Body="&lt;p&gt;If you want a purely alignment based approach to begin to &lt;em&gt;infer&lt;/em&gt; homology (note I say &lt;em&gt;infer&lt;/em&gt;, as we stated in the comments, this isn't sufficient to say 2 sequences are homologs), I would use a Multiple Sequence Alignment (MSA) instead of a pairwise like you've done so far, and include as many similar proteins as you can from BLAST.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The choice of algorithm for the MSA probably won't matter too much, as you will be looking for conserved regions anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have your MSA, you can easily spot conserved domains. If anything is known about this protein structurally, it's quite trivial to work out whether a conserved region is a functional domain (active site, that kind of thing).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With this information, you would be somewhat justified in calling these proteins structural homologs at least if they share domains, but, to reiterate, without doing phylogenetic analyses, you &lt;em&gt;cannot&lt;/em&gt; call them true homologs.&lt;/p&gt;&#xA;" OwnerUserId="929" LastActivityDate="2017-06-21T09:41:44.030" CommentCount="0" />
  <row Id="853" PostTypeId="1" AcceptedAnswerId="862" CreationDate="2017-06-21T10:17:57.267" Score="2" ViewCount="46" Body="&lt;p&gt;If I look at this record in GenBank I see about 6k genes:&#xA;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/nuccore/CM000760?report=gbwithparts&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/nuccore/CM000760?report=gbwithparts&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd really like to be able to dump those genes in GFF3 format, but I'm guessing I can't do that in a single step, so...&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How to dump all genes from a GenBank record in 'whatever' format?&lt;/li&gt;&#xA;&lt;li&gt;How to convert 'whatever' format to GFF3?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-06-21T18:31:20.883" Title="How to dump genes from GenBank in GFF3 format?" Tags="&lt;gff3&gt;&lt;data-download&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="1" />
  <row Id="854" PostTypeId="2" ParentId="829" CreationDate="2017-06-21T10:27:10.710" Score="1" Body="&lt;p&gt;I found &lt;a href=&quot;http://www.disgenet.org/web/DisGeNET/menu&quot; rel=&quot;nofollow noreferrer&quot;&gt;DisGeNET&lt;/a&gt; useful for my purpose, a database that associates genes with diseases and, if known, gene variants with diseases. It integrates several sources of information for computing a score of association of gene/variation with the disease ( &lt;a href=&quot;https://academic.oup.com/nar/article/45/D1/D833/2290909&quot; rel=&quot;nofollow noreferrer&quot;&gt;Piñero et al., 2016&lt;/a&gt;, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/25877637&quot; rel=&quot;nofollow noreferrer&quot;&gt;Piñero et al., 2015&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gene Disease Associations&lt;/strong&gt; from UniProt, PsyGeNET, ClinVar, Orphanet, the GWAS Catalog, CTD (human data), and Human Phenotype Ontology, RGD, MGD, and CTD (mouse and rat data) GAD, LHGDN and BeFree&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Variants Disease Associations&lt;/strong&gt; from ClinVar, the GWAS Catalog, Uniprot, GAD, and from BEFREE&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;If available, it provides the Pubmed ID of articles supporting the proposed association and the rs id from the dbSNP for variants associated to disease.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition it provides an experimental R package, &lt;a href=&quot;https://bitbucket.org/ibi_group/disgenet2r&quot; rel=&quot;nofollow noreferrer&quot;&gt;disgenet2r&lt;/a&gt;, that allows to query the database in a programmatic manner and that provides some plot functions to generate representations of the associations founded.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using this approach, I have been able to found several human histone-disease associations (not my original purpose), together with some concrete histone residues that are mutated in specific diseases. &lt;/p&gt;&#xA;" OwnerUserId="678" LastEditorUserId="678" LastEditDate="2017-06-21T12:40:37.340" LastActivityDate="2017-06-21T12:40:37.340" CommentCount="0" />
  <row Id="855" PostTypeId="2" ParentId="849" CreationDate="2017-06-21T10:33:07.927" Score="3" Body="&lt;p&gt;To my knowledge there's no defined way to deal with that in GTF. &lt;a href=&quot;https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md&quot; rel=&quot;nofollow noreferrer&quot;&gt;GFF3 handles trans-splicing&lt;/a&gt; (you'll have to scroll down to &quot;trans-spliced transcript&quot;) by giving an individual transcript multiple parents (e.g., &lt;code&gt;ID=some_transspliced_gene;Parent=gene1,gene2&lt;/code&gt;). You could use the same methodology with GTF files, but just note that it'll break most downstream programs.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-21T10:33:07.927" CommentCount="1" />
  <row Id="856" PostTypeId="1" AcceptedAnswerId="858" CreationDate="2017-06-21T10:57:29.787" Score="3" ViewCount="35" Body="&lt;p&gt;I have a VCF file with SNPs from a bacterial genome and want to find if the SNPs are located inside genes, is there some CLI-tool where you can pass a VCF file and a gff or gbk file and it returns the name of the genes?&lt;/p&gt;&#xA;" OwnerUserId="223" LastEditorUserId="223" LastEditDate="2017-06-21T11:45:19.827" LastActivityDate="2017-06-21T11:45:19.827" Title="Find gene at position from gff or gbk file" Tags="&lt;snp&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="857" PostTypeId="1" CreationDate="2017-06-21T11:08:11.633" Score="2" ViewCount="37" Body="&lt;p&gt;I have a gene expression dataset that I want to investigate. Particularly, I would like to understand whether there is any correlation between each gene's expression and some quantitative or qualtitative data (say, correlation between gene 'XPTO' , body mass index, and race). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One possible way to test this would be through logistic regression, but is this a good approach or are there caveats that I should know about using such a statistic?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is the following: which methods would you advise to measure such correlations, and why?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(this post was &lt;a href=&quot;https://www.biostars.org/p/258889/&quot; rel=&quot;nofollow noreferrer&quot;&gt;crossposted on Biostars&lt;/a&gt;)&lt;/p&gt;&#xA;" OwnerUserId="499" LastActivityDate="2017-06-21T11:44:01.653" Title="Correlating gene expression with qualitative variables" Tags="&lt;rna-seq&gt;&lt;r&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="0" />
  <row Id="858" PostTypeId="2" ParentId="856" CreationDate="2017-06-21T11:10:55.837" Score="2" Body="&lt;p&gt;Via &lt;a href=&quot;https://github.com/bedops/bedops&quot; rel=&quot;nofollow noreferrer&quot;&gt;BEDOPS&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ gff2bed &amp;lt; annotations.gff &amp;gt; annotations.bed&#xA;$ vcf2bed &amp;lt; snps.vcf &amp;gt; snps.bed&#xA;$ bedmap --echo --echo-map-id-uniq snps.bed annotations.bed &amp;gt; answer.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This can be reduced to a one-liner if you're using &lt;code&gt;bash&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ bedmap --echo --echo-map-id-uniq &amp;lt;(vcf2bed &amp;lt; snps.vcf) &amp;lt;(gff2bed &amp;lt; annotations.gff) &amp;gt; answer.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="776" LastActivityDate="2017-06-21T11:10:55.837" CommentCount="0" />
  <row Id="859" PostTypeId="2" ParentId="857" CreationDate="2017-06-21T11:44:01.653" Score="3" Body="&lt;p&gt;Logistic regression would generally be a bad choice for non-binary outcomes. In such cases, linear regression (or a GLM more generally) still works fine. You can already do that in the standard R packages for RNAseq (DESeq2, edgeR, and limma), where the fold-change is in whatever units you're measuring your quantitative trait in.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-21T11:44:01.653" CommentCount="0" />
  <row Id="860" PostTypeId="1" AcceptedAnswerId="865" CreationDate="2017-06-21T12:19:12.830" Score="2" ViewCount="48" Body="&lt;p&gt;We can calculate the distance between residues in a PDB file regarding different parameters like closest atoms, alpha carbon, beta carbon, centroid and etc. Which one of these parameters are better to show physical interaction between residues in a PDB file?&lt;/p&gt;&#xA;" OwnerUserId="818" LastActivityDate="2017-06-22T08:56:15.450" Title="Best distance parameter for estimating physical interaction between residues in a PDB file" Tags="&lt;protein-structure&gt;&lt;pdb&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="2" />
  <row Id="861" PostTypeId="2" ParentId="807" CreationDate="2017-06-21T12:36:08.063" Score="3" Body="&lt;p&gt;As &lt;a href=&quot;https://bioinformatics.stackexchange.com/users/492/wkretzsch&quot;&gt;wkretzsch&lt;/a&gt; suggested this was worthy of an actual answer, I feel the obvious solution is missing here; index the &lt;code&gt;FASTQ&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Index it&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;As much as I typically hesitate to jump to a solution that requires a script or framework (as opposed to just unix command line tools), there is sadly no &lt;code&gt;samtools fqidx&lt;/code&gt; (perhaps there should be), and existing answers suggest a lot of munging. Whilst they probably work, some appear cumbersome and have many steps in which you could make a mistake.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, to keep things simple - a quick and dirty alternative approach might be to use &lt;code&gt;biopython&lt;/code&gt;, seeing as it &lt;a href=&quot;http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc291&quot; rel=&quot;nofollow noreferrer&quot;&gt;already has this functionality implemented&lt;/a&gt; to do this, and if installed, is trivial to use:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from Bio import SeqIO&#xA;fq = SeqIO.index(&quot;myfastq.fq&quot;, &quot;fastq&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Once you've acquired an index, you'll get fast random access for any read:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;# Random access by read name (I like owls)&#xA;record = fq[&quot;HOOT&quot;]&#xA;record&#xA;#&amp;gt; SeqRecord(seq=Seq('ACGTACGT', SingleLetterAlphabet()), id='HOOT', name='HOOT', description='HOOT', dbxrefs=[])&#xA;&#xA;# We can get the sequence&#xA;record.seq&#xA;#&amp;gt; Seq('ACGTACGT', SingleLetterAlphabet())&#xA;&#xA;# and qualities&#xA;record.letter_annotations&#xA;#&amp;gt; {'phred_quality': [41, 41, 41, 41, 41, 41, 41, 41]}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you want to select arbitrary random records, you could use something like &lt;a href=&quot;https://docs.python.org/3.1/library/random.html#random.randrange&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;randrange&lt;/code&gt;&lt;/a&gt; to select between 0 and the length of the references list.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from random import randint&#xA;&#xA;# Coerce keys to a list, as `dictionary-keyiterator` has no&#xA;#   __getitem__ attribute that would allow integer indices&#xA;# Note also that this doesn't necessarily guarantee a sorted order&#xA;#  but I guess that doesn't matter if you just want random records&#xA;key_list = list(fq.keys())&#xA;&#xA;# Select a random key&#xA;# Note we use len(key_list)-1 as randint endpoints are inclusive&#xA;random_readname = key_list[ randint(0, len(key_list)-1) ]&#xA;&#xA;# Get your record&#xA;rand_record = fq.get( random_readname )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you want multiple records, you'll probably want &lt;a href=&quot;https://docs.python.org/2/library/random.html#random.sample&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;sample&lt;/code&gt;&lt;/a&gt; (to avoid replacement) instead:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from random import sample&#xA;N = 100    &#xA;random_indices = sample(xrange(len(key_list)), N)&#xA;&#xA;for key_i in random_indices:&#xA;    random_readname = key_list[ key_i ]&#xA;    rand_record = fq.get( random_readname )&#xA;    # ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For what it's worth, I think &lt;code&gt;biopython&lt;/code&gt; holds this index in RAM, so if your file is absolutely massive, you might need to be more clever. If that's the case, you could iterate through the &lt;code&gt;FASTQ&lt;/code&gt; once, and output the readname, file offset and length - akin to a &lt;code&gt;FASTA&lt;/code&gt; &lt;code&gt;fai&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="215" LastEditorUserId="191" LastEditDate="2017-06-22T11:04:45.133" LastActivityDate="2017-06-22T11:04:45.133" CommentCount="2" />
  <row Id="862" PostTypeId="2" ParentId="853" CreationDate="2017-06-21T13:27:37.420" Score="3" Body="&lt;p&gt;Turns out you can just grab the GFF3 from the NCBI's FTP site!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/003/195/GCA_000003195.3_Sorghum_bicolor_NCBIv3/&quot; rel=&quot;nofollow noreferrer&quot;&gt;ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/003/195/GCA_000003195.3_Sorghum_bicolor_NCBIv3/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/genome/doc/ftpfaq/#files&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/genome/doc/ftpfaq/#files&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Or &quot;Access the data&quot; on the right here:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/assembly/GCF_000003195.3&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/assembly/GCF_000003195.3&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-06-21T13:27:37.420" CommentCount="1" />
  <row Id="863" PostTypeId="2" ParentId="677" CreationDate="2017-06-21T13:36:08.693" Score="2" Body="&lt;p&gt;Tried looking for an explicit database? i.e.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ComSin: database of protein structures in bound (complex) and unbound (single) states in relation to their intrinsic disorder: &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2808974/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2808974/&lt;/a&gt; or&lt;/li&gt;&#xA;&lt;li&gt;LigASite: a database of biologically relevant binding sites in proteins with known apo-structures &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/17933762&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pubmed/17933762&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Else the PDB itself has a very powerful advanced search that should allow you to do this: &lt;a href=&quot;http://www.rcsb.org/pdb/search/advSearch.do&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.rcsb.org/pdb/search/advSearch.do&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-06-21T13:36:08.693" CommentCount="0" />
  <row Id="864" PostTypeId="2" ParentId="860" CreationDate="2017-06-21T13:38:50.203" Score="0" Body="&lt;p&gt;You should read my &lt;a href=&quot;https://bmcstructbiol.biomedcentral.com/articles/10.1186/1472-6807-8-53&quot; rel=&quot;nofollow noreferrer&quot;&gt;paper&lt;/a&gt;, where I did a comprehensive parameter sweep across different thresholds and definitions and benchmarked the resulting empirical potentials on a decoy ranking challenge:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The relevant phrase from the abstract is &quot;compared using 90 different definitions of residue-residue contact&quot;. However, it boils down (as always) to the question, &quot;what do you want to do&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cite me if you use my paper for your research. &lt;/p&gt;&#xA;" OwnerUserId="931" LastEditorUserId="48" LastEditDate="2017-06-22T08:56:15.450" LastActivityDate="2017-06-22T08:56:15.450" CommentCount="2" />
  <row Id="865" PostTypeId="2" ParentId="860" CreationDate="2017-06-21T13:48:17.480" Score="1" Body="&lt;p&gt;Use a distance cutoff of 12Å between Cᵦ atoms.&lt;/p&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-06-21T13:48:17.480" CommentCount="1" />
  <row Id="867" PostTypeId="2" ParentId="81" CreationDate="2017-06-21T14:46:48.633" Score="1" Body="&lt;p&gt;Simulating NGS reads while controlling sequence coverage is now easy with &lt;a href=&quot;http://karel-brinda.github.io/rnftools/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RNFtools&lt;/a&gt; (from version 0.3.1). See the &lt;a href=&quot;http://rnftools.readthedocs.io/en/latest/tutorial.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorial&lt;/a&gt;, especially section &lt;a href=&quot;http://rnftools.readthedocs.io/en/latest/tutorial/02_simulation.html#sequence-extraction&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sequence extraction&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Environment preparation&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, install &lt;a href=&quot;https://bioconda.github.io/&quot; rel=&quot;nofollow noreferrer&quot;&gt;BioConda and add the required channels&lt;/a&gt;. Then either install RNFtools in the default Conda environment&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;conda install rnftools&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or create and activate a separate Conda environment (preferable)&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;conda create -n rnftools rnftools&#xA;source activate rnftools&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Simulation&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assume that you have a reference file &lt;code&gt;ref.fa&lt;/code&gt; and a tab-separated coverage file &lt;code&gt;coverage.tsv&lt;/code&gt; (e.g., those from your example). Then the following RNFtools &lt;code&gt;Snakefile&lt;/code&gt; will do the job you want:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import rnftools&#xA;import csv&#xA;&#xA;rnftools.mishmash.sample(&quot;simulation_with_coverage_control&quot;, reads_in_tuple=1)&#xA;&#xA;fa = &quot;ref.fa&quot;&#xA;tsv = &quot;coverage.tsv&quot;&#xA;&#xA;with open(tsv) as f:&#xA;    table = csv.reader(f, delimiter='\t')&#xA;    for seqname, cov in table:&#xA;&#xA;        rnftools.mishmash.DwgSim(&#xA;            fasta=fa,&#xA;            sequences=[seqname],&#xA;            coverage=float(cov),&#xA;            read_length_1=10, # quick test with supershort reads&#xA;            read_length_2=0,&#xA;        )&#xA;&#xA;include: rnftools.include()&#xA;rule: input: rnftools.input()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When you save this file (&lt;code&gt;Snakefile&lt;/code&gt;) and run &lt;code&gt;snakemake&lt;/code&gt;, RNFtools will simulate reads using DWGsim with the coverages defined your text file, and save all the simulated reads in &lt;code&gt;simulation_with_coverage_control.fq&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can play with all the parameters. In particular, you can use a different simulator (e.g., Art-Illumina using &lt;code&gt;rnftools.mishmash.ArtIllumina&lt;/code&gt;). See the &lt;a href=&quot;http://rnftools.rtfd.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;RNFtools documentation&lt;/a&gt; for more information.&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-06-21T15:32:54.330" LastActivityDate="2017-06-21T15:32:54.330" CommentCount="0" />
  <row Id="868" PostTypeId="2" ParentId="838" CreationDate="2017-06-21T15:27:07.430" Score="1" Body="&lt;h3&gt;Regarding your python code&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If you want the &lt;strong&gt;experimental ranges that are entirely contained in one of the reference ranges&lt;/strong&gt;, you need to have the coordinates in the following order:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cst &amp;lt;= tst &amp;lt; ten &amp;lt;= cen&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If what you want are the &lt;strong&gt;experimental ranges that overlap one of the reference ranges&lt;/strong&gt;, you need to have either the start or the end of the experimental range fall within the reference range:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(cst &amp;lt;= tst &amp;lt; cen) or (cst &amp;lt; ten &amp;lt;= cen)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Your code is equivalent to neither possibilities:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cst &amp;gt;= tst and cen &amp;lt;= ten&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is equivalent to:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;tst &amp;lt;= cst and cen &amp;lt;= ten&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(Or &lt;code&gt;tst &amp;lt;= cst &amp;lt; cen &amp;lt;= ten&lt;/code&gt;, since we know that &lt;code&gt;cst &amp;lt; cen&lt;/code&gt;, by definition of a bed interval).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With this rewriting, we can more easily see that you are actually selecting the &lt;strong&gt;experimental ranges that contain a reference range&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is some (python3) code that gives you the results in the other two situations:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python3&#xA;&#xA;ref_intervals = []&#xA;with open(&quot;s4.txt&quot;, &quot;r&quot;) as f:&#xA;    for line in f:&#xA;        (chr, start, end) = line.strip().split(&quot;\t&quot;)&#xA;        ref_intervals.append((chr, int(start), int(end)))&#xA;&#xA;exp_intervals = []&#xA;with open(&quot;s3.txt&quot;, &quot;r&quot;) as f:&#xA;    for line in f:&#xA;        (chr, start, end) = line.strip().split(&quot;\t&quot;)&#xA;        exp_intervals.append((chr, int(start), int(end)))&#xA;&#xA;contained = []&#xA;overlapping = []&#xA;for (r_chr, r_start, r_end) in ref_intervals:&#xA;    for (e_chr, e_start, e_end) in exp_intervals:&#xA;        if e_chr == r_chr:&#xA;            if r_start &amp;lt;= e_start &amp;lt; r_end or r_start &amp;lt; e_end &amp;lt;= r_end:&#xA;                overlapping.append((e_chr, e_start, e_end))&#xA;            if r_start &amp;lt;= e_start &amp;lt; e_end &amp;lt;= r_end:&#xA;                contained.append((e_chr, e_start, e_end))&#xA;&#xA;print(&quot;overlapping&quot;)&#xA;for (chr, start, end) in overlapping:&#xA;    print(chr, start, end, sep=&quot;\t&quot;)&#xA;&#xA;print(&quot;contained&quot;)&#xA;for (chr, start, end) in contained:&#xA;    print(chr, start, end, sep=&quot;\t&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I run it, I obtain the following results:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ ./overlap.py &#xA;overlapping&#xA;1   10  20&#xA;1   5   20&#xA;1   10  50&#xA;1   14  17&#xA;2   20  30&#xA;2   25  30&#xA;2   20  60&#xA;contained&#xA;1   10  20&#xA;1   14  17&#xA;2   20  30&#xA;2   25  30&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is probably an good exercise to program this, but as the other answers point out, &lt;strong&gt;there are efficient tools that would be a better solution in a professional setting&lt;/strong&gt;.&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-06-21T15:34:15.073" LastActivityDate="2017-06-21T15:34:15.073" CommentCount="1" />
  <row Id="869" PostTypeId="1" AcceptedAnswerId="870" CreationDate="2017-06-21T16:30:49.260" Score="1" ViewCount="27" Body="&lt;p&gt;I am analyzing data from a quantitative polymerase chain reaction (qPCR) using R. After cleaning the raw data, it looks something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dput(x)&#xA;structure(list(Reporter = c(&quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &#xA;&quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;FAM&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &#xA;&quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &quot;VIC&quot;, &#xA;&quot;VIC&quot;), Number = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, &#xA;12L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L), A = c(22.19, &#xA;22.24, 22.5, 22.54, 22.6, 22.59, 23.07, 23.46, 22.43, 22.74, &#xA;24.09, 23.91, 24.52, 25.03, 25.25, 25.82, 25.13, 24.71, 25.34, &#xA;25.85, 25.25, 26.15, 25.81, 25.29), B = c(21.72, 21.78, 22.86, &#xA;22.73, 19.88, 20.07, 21.06, 21.06, 20.96, 21.11, 19.46, 19.43, &#xA;24.75, 24.69, 25.64, 25.19, 23.76, 23.69, 24.35, 25.05, 24.1, &#xA;23.81, 22.81, 23.13), C = c(21.37, 21.56, 20.07, 20.01, 21.17, &#xA;21.08, 20.54, 20.36, 33, NA, NA, NA, 23.91, 24.31, 23.61, 23.88, &#xA;24.33, 24.31, 23.37, 23.53, 33, NA, NA, NA), E = c(26.26, 27.33, &#xA;25.93, 26.56, 25.76, 23.03, 24.72, 25.27, 24.43, 24.31, 26.98, &#xA;23.33, 24.04, 25.02, 25.1, 25.1, 24.68, 25.48, 25.87, 26.22, &#xA;25.35, 25.36, 25.11, 25.98), F = c(25.81, 26.9, 25.58, 26.61, &#xA;25.06, 21.85, 23.59, 24.04, 23.19, 23.19, 25.17, 20.8, 24.12, &#xA;24.26, 25.32, 25.25, 24, 23.78, 24.7, 24.48, 23.52, 23.87, 23.05, &#xA;23.05), G = c(26.12, 27.02, 24.08, 25.15, 25.99, 23.18, 24.2, &#xA;24.05, 33, NA, NA, NA, 23.47, 23.45, 23.7, 23.74, 24.46, 24.19, &#xA;23.56, 23.53, 33, NA, NA, NA)), .Names = c(&quot;Reporter&quot;, &quot;Number&quot;, &#xA;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;&#xA;), row.names = c(NA, 24L))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I think that each well measures two genes: target which is 12 different genes (color FAM), and the reference or housekeeping gene, GAPDH (color VIC). Also, control is triplicate A, B, C (3 x 12 wells), and treatment is E, F, G (3 x 12 wells). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have created a function &lt;code&gt;ddCt_&lt;/code&gt; for analyzing the data with the ddCt algorithm (Livak &amp;amp; Schmittgen, 2001). It takes one argument &lt;code&gt;x&lt;/code&gt; which is a data.frame of the form exemplified above. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ddCt_ &amp;lt;- function(x) {&#xA;  # Subset x by control/treatment and target/reference&#xA;  # Then, calculate Ct averages for each triplicate&#xA;  TC &amp;lt;- x %&amp;gt;% &#xA;    select(Reporter, Number, A, B, C) %&amp;gt;% &#xA;    filter(Reporter == &quot;FAM&quot;) %&amp;gt;% &#xA;    rowwise() %&amp;gt;%&#xA;    mutate(Ct_Avg = mean(c(A, B, C), na.rm = TRUE))&#xA;&#xA;  RC &amp;lt;- x %&amp;gt;% &#xA;    select(Reporter, Number, A, B, C) %&amp;gt;% &#xA;    filter(Reporter == &quot;VIC&quot;) %&amp;gt;% &#xA;    rowwise() %&amp;gt;%&#xA;    mutate(Ct_Avg = mean(c(A, B, C), na.rm = TRUE))&#xA;&#xA;  TT &amp;lt;- x %&amp;gt;% &#xA;    select(Reporter, Number, E, F, G) %&amp;gt;% &#xA;    filter(Reporter == &quot;FAM&quot;) %&amp;gt;% &#xA;    rowwise() %&amp;gt;%&#xA;    mutate(Ct_Avg = mean(c(E, F, G), na.rm = TRUE))&#xA;&#xA;  RT &amp;lt;- x %&amp;gt;% &#xA;    select(Reporter, Number, E, F, G) %&amp;gt;% &#xA;    filter(Reporter == &quot;VIC&quot;) %&amp;gt;% &#xA;    rowwise() %&amp;gt;%&#xA;    mutate(Ct_Avg = mean(c(E, F, G), na.rm = TRUE))&#xA;&#xA;  # Normalize Ct of the target gene to the Ct of the reference gene&#xA;  dCt_control   &amp;lt;- TC$Ct_Avg - RC$Ct_Avg&#xA;  dCt_treatment &amp;lt;- TT$Ct_Avg - RT$Ct_Avg&#xA;&#xA;  # Normalize dCt of the treatment group to the dCt of the control group&#xA;  ddCt &amp;lt;- dCt_treatment - dCt_control&#xA;&#xA;  # Calculate fold change&#xA;  fc &amp;lt;- 2^(-ddCt)&#xA;&#xA;  # Calculate avg and sd&#xA;  dCt_control_avg   &amp;lt;- mean(dCt_control)&#xA;  dCt_control_sd    &amp;lt;- sd(dCt_control)&#xA;  dCt_treatment_avg &amp;lt;- mean(dCt_treatment)&#xA;  dCt_treatment_sd  &amp;lt;- sd(dCt_treatment)&#xA;&#xA;  # Create output&#xA;  df &amp;lt;- data_frame(&#xA;    Sample = 1:12, &quot;dCt Control&quot; = dCt_control, &quot;dCt Treatment&quot; = dCt_treatment, &#xA;    &quot;ddCt&quot; = ddCt, &quot;Fold Change&quot; = fc, &quot;dCt Control Avg&quot; = dCt_control_avg, &#xA;    &quot;dCt Control SD&quot; = dCt_control_sd, &quot;dCt Treatment Avg&quot; = dCt_treatment_avg, &#xA;    &quot;dCt Treatment SD&quot; = dCt_treatment_sd&#xA;    ) %&amp;gt;% round(2)&#xA;  write.csv(x = df, file = &quot;result.csv&quot;)&#xA;  saveRDS(object = df, file = &quot;result.rds&quot;)&#xA;  df&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I am not a biochemist or molecular biologist, so I am unsure of the basic concepts involved and the overall approach. So, my questions are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Is the function correct?&lt;/li&gt;&#xA;&lt;li&gt;Why is it important to calculate fold change and standard deviation after normalization?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="935" LastEditorUserId="935" LastEditDate="2017-06-21T21:05:36.370" LastActivityDate="2017-06-21T21:05:36.370" Title="qPCR: Why is fold change and standard deviation calculated after transformation?" Tags="&lt;r&gt;&lt;normalization&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="0" />
  <row Id="870" PostTypeId="2" ParentId="869" CreationDate="2017-06-21T16:43:27.950" Score="3" Body="&lt;ol&gt;&#xA;&lt;li&gt;The functions look correct, but calculate a few by hand and ensure they match. One thing I should note is that the subtraction of the Ct values usually happens before an average is made, since generally both are in the same well. In other word, subtract the reference Ct from the gene of interest Ct from the same well and then average the dCts. This order is important since one usually does a multiplexed qPCR.&lt;/li&gt;&#xA;&lt;li&gt;You have to normalize to a reference gene to control for how much cDNA was used, since that will alter the Ct values. If you calculated the fold-changes without normalization then they could be purely due to using more/less cDNA in the reaction (i.e., the output would be meaningless).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-21T19:37:17.810" LastActivityDate="2017-06-21T19:37:17.810" CommentCount="4" />
  <row Id="871" PostTypeId="2" ParentId="853" CreationDate="2017-06-21T18:31:20.883" Score="1" Body="&lt;p&gt;When accessing all of the annotated genes for a reference genome, downloading a GFF3 file directly from the &lt;a href=&quot;ftp://ftp.ncbi.nlm.nih.gov/genomes/genbank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Genbank&lt;/a&gt; or &lt;a href=&quot;ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq&quot; rel=&quot;nofollow noreferrer&quot;&gt;RefSeq&lt;/a&gt; FTP sites is definitely the way to go.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But when submitting a search query into NCBI Entrez, there is no easy way to download the results in GFF3 format. Probably the best route is to download the results in ASN.1 format, and then use &lt;code&gt;annotwriter&lt;/code&gt; from the NCBI C++ toolkit to convert to GFF3. But getting the latter to compile and run properly isn't trivial in my experience.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-06-21T18:31:20.883" CommentCount="0" />
  <row Id="872" PostTypeId="2" ParentId="844" CreationDate="2017-06-21T19:34:57.207" Score="1" Body="&lt;p&gt;As you alluded to in your question, KEGG does provide a curated set of enzyme and metabolite information. This information can be parsed and used to create a network that you can analyze to look at how gene products could be working together. Additional steps could include mapping transcriptome data to the map.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&quot;http://biorxiv.org/content/early/2016/12/07/092304&quot; rel=&quot;nofollow noreferrer&quot;&gt;this paper&lt;/a&gt; as an example of how this can be done.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope this helps. :)&lt;/p&gt;&#xA;" OwnerUserId="824" LastActivityDate="2017-06-21T19:34:57.207" CommentCount="0" />
  <row Id="873" PostTypeId="2" ParentId="802" CreationDate="2017-06-22T09:17:53.090" Score="1" Body="&lt;p&gt;Running &lt;code&gt;blastdbcmd -db foo -info&lt;/code&gt; provides a little information but I haven't seen anything which will report exactly how a blastdb was created.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A &lt;a href=&quot;https://ftp.ncbi.nlm.nih.gov/blast/documents/blastdb.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;blastdb readme&lt;/a&gt; suggests that only the &lt;code&gt;parse_seqids&lt;/code&gt; option has been added to the standard parameters.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For those from NCBI, the following makeblastdb commands are recommended:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;For nucleotide fasta file:   makeblastdb -in input_db -dbtype nucl -parse_seqids&lt;/li&gt;&#xA;  &lt;li&gt;For protein fasta file:      makeblastdb -in input_db -dbtype prot -parse_seqids&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;-parse_seqids&lt;/code&gt; requires NCBI fasta headers in the correct format (spec can be found &lt;a href=&quot;http://ncbi.github.io/cxx-toolkit/pages/ch_demo#ch_demo.id1_fetch.html_ref_fasta&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&#xA;" OwnerUserId="941" LastEditorUserId="302" LastEditDate="2017-06-22T19:56:20.457" LastActivityDate="2017-06-22T19:56:20.457" CommentCount="0" />
  <row Id="874" PostTypeId="1" AcceptedAnswerId="875" CreationDate="2017-06-22T10:17:23.743" Score="5" ViewCount="74" Body="&lt;p&gt;I am trying to find intersection of two genomic ranges (gr1 and gr2) and keep metadata from one of them&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gr1&#xA;chrI [1, 100] * | 0.1&#xA;chrI [101, 200] * | 0.2&#xA;&#xA;gr2&#xA;chrI [50, 150] + | &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;intersect(gr1, gr2)&lt;/code&gt; will remove metadata, where&#xA;&lt;code&gt;subsetByOverlaps(gr1, gr2)&lt;/code&gt; return:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;chrI [1, 100] * | 0.1&#xA;chrI [101, 200] * | 0.2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where metadata is saved, but intersection is bigger than real (the same as in initial gr1 start/end)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I get an intersection range and keep metadata?&#xA;to get the range:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;chrI [50, 100] * | 0.1&#xA;chrI [101, 150] * | 0.2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="943" LastEditorUserId="131" LastEditDate="2017-06-22T12:25:41.493" LastActivityDate="2017-06-22T12:25:41.493" Title="Intersection of two genomic ranges to keep metadata" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;granges&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="875" PostTypeId="2" ParentId="874" CreationDate="2017-06-22T11:14:42.393" Score="3" Body="&lt;p&gt;Presuming you don't have cases of multiple ovelaps&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; library(GenomicRanges)&#xA;&amp;gt; gr1 = GRanges(c(&quot;chrI&quot;, &quot;chrI&quot;), IRanges(start=c(1, 101), width=c(100, 100)), mcols=data.frame(foo=c(0.1, 0.2)))&#xA;&amp;gt; gr2 = GRanges(c(&quot;chrI&quot;), IRanges(start=c(50), width=c(101)))&#xA;&amp;gt; o = findOverlaps(gr1, gr2)&#xA;&amp;gt; grl1 = split(gr1[queryHits(o)], 1:length(o)) # You can't mendoapply on a GRanges object&#xA;&amp;gt; grl2 = split(gr2[subjectHits(o)], 1:length(o))&#xA;&amp;gt; foo = function(x, y) {&#xA;+     rv = x&#xA;+     start(rv) = max(start(x), start(y))&#xA;+     end(rv) = min(end(x), end(y))&#xA;+     return(rv)&#xA;+ }&#xA;&amp;gt; unlist(mendoapply(foo, grl1, y=grl2))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The output is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GRanges object with 2 ranges and 1 metadata column:&#xA;    seqnames     ranges strand | mcols.foo&#xA;       &amp;lt;Rle&amp;gt;  &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt; | &amp;lt;numeric&amp;gt;&#xA;  1     chrI [ 50, 100]      * |       0.1&#xA;  2     chrI [101, 150]      * |       0.2&#xA;  -------&#xA;  seqinfo: 1 sequence from an unspecified genome; no seqlengths&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It'd be more memory efficient to iterate over &lt;code&gt;o&lt;/code&gt;, but this gives you the idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In cases of multiple overlaps, one could use &lt;code&gt;intersect()&lt;/code&gt;, then &lt;code&gt;findOverlaps()&lt;/code&gt; with that and then continue on with the procedure demonstrated above (or iterate over the overlaps).&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-06-22T11:32:27.683" LastActivityDate="2017-06-22T11:32:27.683" CommentCount="0" />
  <row Id="877" PostTypeId="1" CreationDate="2017-06-22T18:05:28.307" Score="1" ViewCount="69" Body="&lt;p&gt;There are 2 new wet methods: &lt;a href=&quot;https://deainfo.nci.nih.gov/advisory/bsa/1016/3_PDX.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;Patient-derived models&lt;/a&gt;: &lt;a href=&quot;https://en.wikipedia.org/wiki/Patient-derived_tumor_xenograft&quot; rel=&quot;nofollow noreferrer&quot;&gt;Patient Derived Xenograft (PDX)&lt;/a&gt; and &lt;a href=&quot;http://www.nature.com/ng/journal/v47/n4/fig_tab/ng.3225_SF8.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Patient Derived Organoids (PDO)&lt;/a&gt; to reflect tumor biology. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any databases/resources/computational tools/previous work that use the outcomes of PDO/PDX experiments to create a predictive computational model for cancer or other diseases?&lt;/p&gt;&#xA;" OwnerUserId="734" LastEditorUserId="734" LastEditDate="2017-07-03T17:03:11.717" LastActivityDate="2017-07-03T17:03:11.717" Title="How are PDO and PDX used in computational and predicative models for tumour biology?" Tags="&lt;cancer&gt;&lt;drugs&gt;&lt;modelling&gt;" AnswerCount="1" CommentCount="15" />
  <row Id="878" PostTypeId="1" AcceptedAnswerId="880" CreationDate="2017-06-22T18:57:54.120" Score="5" ViewCount="65" Body="&lt;p&gt;This is a tough one I think: is there a publicly available, up-to-date, free, complete database for antibiotics names and classes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am specifically looking for information like, e.g., &lt;code&gt;cefpirome (is a) cephalosporin&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've looked in a couple of different places, but nothing really comes close. E.g.:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://card.mcmaster.ca/&quot; rel=&quot;nofollow noreferrer&quot;&gt;CARD&lt;/a&gt; comes closest, but is not free (companies need to pay a license) and not complete/up-to-date (e.g., cefpirome is missing)&lt;/li&gt;&#xA;&lt;li&gt;@Karel mentioned &lt;a href=&quot;http://www.kegg.jp/&quot; rel=&quot;nofollow noreferrer&quot;&gt;KEGG&lt;/a&gt; in his answer, but this also needs a license (companies for sure, academics if they download data via FTP).&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.antibiotics.or.jp/journal/database/database-top.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;Novel Antiobiotics Database&lt;/a&gt; has no class ... and stops in 2003&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://antibiotics.toku-e.com/&quot; rel=&quot;nofollow noreferrer&quot;&gt;The Antimicrobial Index&lt;/a&gt; is web only, and terms of use apparently prohibit any kind of web-scraping.&lt;/li&gt;&#xA;&lt;li&gt;A suggestion by @Pierre on Twitter was to try &lt;a href=&quot;https://en.wikipedia.org/w/api.php?action=query&amp;amp;list=categorymembers&amp;amp;cmtitle=Category:Antibiotic%20stubs&amp;amp;format=json&amp;amp;cmlimit=500&quot; rel=&quot;nofollow noreferrer&quot;&gt;Antimicrobial stubs&lt;/a&gt; on Wikipedia. Good idea, but the class is not stored in the ChemBox and must be free-text parsed. Or writing a script which evaluates the Wikipedia category tags.&lt;/li&gt;&#xA;&lt;li&gt;I drew blank at different government agencies (EMA, FDA, EFSA, NIH, etc.). The closest I found was from the WHO a list of critically important antibiotics. But not complete, and in a PDF.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Any pointer appreciated.&lt;/p&gt;&#xA;" OwnerUserId="44" LastEditorUserId="44" LastEditDate="2017-06-22T21:43:20.580" LastActivityDate="2017-06-23T15:55:14.457" Title="Publicly available, free, complete database for antibiotics names and classes?" Tags="&lt;database&gt;&lt;antibiotics&gt;&lt;open-data&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="880" PostTypeId="2" ParentId="878" CreationDate="2017-06-22T21:33:58.947" Score="4" Body="&lt;p&gt;&lt;strong&gt;ChEBI&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It should be possible to get this information from the &lt;a href=&quot;https://www.ebi.ac.uk/chebi/&quot; rel=&quot;nofollow noreferrer&quot;&gt;ChEBI database&lt;/a&gt;, see the &lt;a href=&quot;https://www.ebi.ac.uk/chebi/downloadsForward.do&quot; rel=&quot;nofollow noreferrer&quot;&gt;exported tables&lt;/a&gt;. You could download the ontologies (in OWL / OBO), parse them using some ad-hoc parser or using a dedicated library (e.g., &lt;a href=&quot;https://pypi.python.org/pypi/pronto&quot; rel=&quot;nofollow noreferrer&quot;&gt;Pronto&lt;/a&gt;), build a directed acyclic graph based on the &lt;code&gt;is_a&lt;/code&gt; relationship, and extract the antibiotic subtree.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;KEGG&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You could also use &lt;a href=&quot;http://www.genome.jp/kegg/&quot; rel=&quot;nofollow noreferrer&quot;&gt;KEGG: Kyoto Encyclopedia of Genes and Genomes&lt;/a&gt;, but a &lt;a href=&quot;http://www.kegg.jp/kegg/legal.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;subscription&lt;/a&gt; is probably required. The &lt;a href=&quot;ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug&quot; rel=&quot;nofollow noreferrer&quot;&gt;exported drug table&lt;/a&gt; should be easy to parse (e.g., in Python).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a quick verification of the completeness, you can download the table, filter out everything except antibacterial agents and check the number of records:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;curl &quot;ftp://ftp.genome.jp/pub/kegg/medicus/drug/drug&quot; &amp;gt; keg_all.txt&#xA;&#xA;cat keg_all.txt \&#xA;   | perl -pe 's/\n/BREAK/g' \&#xA;   | perl -pe 's@///BREAK@///\n@g' \&#xA;   | grep 'CLASS       Antibacterial' \&#xA;   | perl -pe 's/BREAK/\n/g' \&#xA;   &amp;gt; keg_antibacterial.txt&#xA;&#xA;cat keg_antibacterial.txt | grep &quot;///&quot; | wc -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are 646 records. However, some of them are almost identical (e.g., only 1 atom difference).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you add &lt;code&gt;| grep -i &quot;antibiotic&quot; \&lt;/code&gt; to the pipeline, then you get only 445 records.&lt;/p&gt;&#xA;" OwnerUserId="425" LastEditorUserId="425" LastEditDate="2017-06-23T15:55:14.457" LastActivityDate="2017-06-23T15:55:14.457" CommentCount="3" />
  <row Id="882" PostTypeId="1" AcceptedAnswerId="883" CreationDate="2017-06-22T23:42:46.283" Score="2" ViewCount="50" Body="&lt;p&gt;I'm contributing to a python-based project that uses &lt;code&gt;Biopython&lt;/code&gt; to analyze fastq files. It currently uses &lt;code&gt;SeqIO.parse&lt;/code&gt;, which populates various structures with all of the fastq information (including converting quality scores). There is apparently a faster (lighter-weight) parser called &lt;a href=&quot;http://biopython.org/DIST/docs/api/Bio.SeqIO.QualityIO-module.html#FastqGeneralIterator&quot; rel=&quot;nofollow noreferrer&quot;&gt;FastqGeneralIterator&lt;/a&gt; that doesn't populate all of these items.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to use &lt;code&gt;FastqGeneralIterator&lt;/code&gt;, but be able to perform some of the functions &lt;code&gt;SeqIO.parse&lt;/code&gt; offers on the fly (not for &lt;em&gt;every&lt;/em&gt; sequence). For example, I'd like to convert base quality to PHRED scores for specific sequences, but I don't see that function available.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an easy way to take a tuple output by &lt;code&gt;FastqGeneralIterator&lt;/code&gt; and convert it into a proper &lt;code&gt;Bio.SeqIO&lt;/code&gt; &lt;code&gt;SeqRecord&lt;/code&gt;? &lt;/p&gt;&#xA;" OwnerUserId="136" LastEditorUserId="492" LastEditDate="2017-06-23T15:04:59.877" LastActivityDate="2017-06-23T15:04:59.877" Title="How to maximize fastq parsing with FastqGeneralIterator (Bio.SeqIO.QualityIO)" Tags="&lt;fastq&gt;&lt;biopython&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="0" />
  <row Id="883" PostTypeId="2" ParentId="882" CreationDate="2017-06-23T13:01:54.793" Score="1" Body="&lt;p&gt;I had not come across &lt;code&gt;FastqGeneralIterator&lt;/code&gt; before, but I will start using it for some of my work!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One answer is to replicate the code in the &lt;a href=&quot;http://biopython.org/DIST/docs/api/Bio.SeqIO.QualityIO-pysrc.html#FastqPhredIterator&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;FastqPhredIterator&lt;/code&gt;&lt;/a&gt;, which does exactly what you are looking to do.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, My personal preference would be to use the biopython infrastructure as much as possible, especially when it comes to the tricky aspects of record parsing.  If the records you want extra info on are uncommon, then using &lt;code&gt;FastqPhredIterator&lt;/code&gt; for phred quality extraction might work for you:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;import io&#xA;from Bio.SeqIO.QualityIO import FastqGeneralIterator, FastqPhredIterator&#xA;&#xA;with open(&quot;input.fastq&quot;, &quot;rU&quot;) as handle:&#xA;    for title, sequence, quality in FastqGeneralIterator(handle):&#xA;        # do something with title, sequence, and quality&#xA;        # Set special_case to True when you want more info on the record&#xA;        special_case = False&#xA;        if special_case:&#xA;            record_stream = io.StringIO(&quot;\n&quot;.join([title, sequence, &quot;+&quot;, quality]))&#xA;            record = next(FastqPhredIterator(record_stream))&#xA;            # do something special with record&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What we are doing here is combining the record strings into one and dumping them into a &lt;code&gt;StringIO&lt;/code&gt; object, which implements the file handle duck-type. We then hand the in-memory handle to &lt;code&gt;FstqPhredIterator&lt;/code&gt; to parse the FASTQ record again. Make sure to set &lt;code&gt;special_case&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; only when you want extra info on a record, as the &lt;code&gt;special_case&lt;/code&gt; branch makes at least two extra copies of the FASTQ record. &lt;/p&gt;&#xA;" OwnerUserId="492" LastActivityDate="2017-06-23T13:01:54.793" CommentCount="1" />
  <row Id="884" PostTypeId="1" AcceptedAnswerId="906" CreationDate="2017-06-23T15:59:35.730" Score="7" ViewCount="68" Body="&lt;p&gt;I would like to add a text to PDB files that I'm processing with my tool, rna-pdb-tools. Someone points that the way I'm using it right now it's not correct (&lt;a href=&quot;https://github.com/mmagnus/rna-pdb-tools/issues/48&quot; rel=&quot;noreferrer&quot;&gt;https://github.com/mmagnus/rna-pdb-tools/issues/48&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use HEADER right now which is not correct.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;HEADER Generated with rna-pdb-tools&#xA;HEADER ver 37c5b4e-dirty &#xA;HEADER https://github.com/mmagnus/rna-pdb-tools &#xA;HEADER Mon Oct 10 22:18:19 2016&#xA;ATOM      1  P     C A   1     -19.687  -3.296  65.469  1.00  0.00           P &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do you happen to know which remark number to use? (&lt;a href=&quot;http://www.wwpdb.org/documentation/file-format-content/format33/remarks1.html&quot; rel=&quot;noreferrer&quot;&gt;http://www.wwpdb.org/documentation/file-format-content/format33/remarks1.html&lt;/a&gt;)?&lt;/p&gt;&#xA;" OwnerUserId="640" LastEditorUserId="96" LastEditDate="2017-07-27T04:05:00.873" LastActivityDate="2017-07-27T04:05:00.873" Title="PDB format: remark number for free text" Tags="&lt;file-formats&gt;&lt;pdb&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="885" PostTypeId="1" AcceptedAnswerId="888" CreationDate="2017-06-23T20:03:46.210" Score="6" ViewCount="189" Body="&lt;p&gt;Do you know which quality score encoding PacBio uses now? I know some of their file formats have changed in the past year or two, but I haven't found much on their quality score encoding.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most recent answer I found is from 2012, where one user states it's &quot;phred style&quot;, but doesn't state whether it's 33 or 64: &lt;a href=&quot;http://seqanswers.com/forums/archive/index.php/t-16895.html&quot; rel=&quot;noreferrer&quot;&gt;DeNovo assembly using pacBio data&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="136" LastEditorUserId="136" LastEditDate="2017-06-23T20:30:21.750" LastActivityDate="2017-06-24T20:23:05.160" Title="Which quality score encoding does PacBio use?" Tags="&lt;sequencing&gt;&lt;pacbio&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="886" PostTypeId="1" AcceptedAnswerId="889" CreationDate="2017-06-23T21:07:18.203" Score="2" ViewCount="20" Body="&lt;p&gt;I have FPKM-UQ data from COAD-TCGA.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I generated an expression set of this data using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; edata = log2(data + 1)&#xA;&amp;gt; edata[1:2,1:2]&#xA;      X01240896.3f3f.4bf9.9799.55c87bfacf36&#xA;1                                  8.540967&#xA;10                                13.528968&#xA;100                               16.296422&#xA;1000                              13.658199&#xA;10000                             15.143788&#xA;      X01ad5016.f691.4bca.82a0.910429d8d25b&#xA;1                                  9.886537&#xA;10                                16.719682&#xA;100                               17.212312&#xA;1000                              10.317842&#xA;10000                             13.166767&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Row name is entrezid, and column names are case_ids.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The batch data provided by TCGA which indicates a total batch num = 42&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; omics[1:5,]&#xA;                                      snf    boo order batch&#xA;X01240896.3f3f.4bf9.9799.55c87bfacf36   3 192807     1    22&#xA;X01ad5016.f691.4bca.82a0.910429d8d25b   2  82213     2     2&#xA;X01f493d4.229d.47a6.baa8.32a342c65d01   1 A08907     3    29&#xA;X022f39e9.57ee.4b2b.8b3a.8929e3d69a37   2 A16S13     4    32&#xA;X02f9668c.71e6.485f.88b1.b37dc8bdd2ab   3 102113     5     7&#xA;&amp;gt; batch = omics$batch&#xA;&amp;gt; batch&#xA;  [1] 22  2 29 32  7  1  8  8  7 41 26 14 26 26  1  5 22  5 15 18 28 12 12  8 19&#xA; [26] 13 38  5 27  7 41  1 22 26 28 22 33 14 13  8 10 13 11 17  5 38  5  7  7 31&#xA; [51] 22 42 11  8  7 17 15 17 12 17  2 19  2 12 18 14 22 11 21 15 17 17 33 15 19&#xA; [76] 15  6  1 10 11  2 28 36 17  9 15 12 17 15 10  6  8 13 25 11 15  1 13 24  8&#xA;[101] 13 15 17  8  2 22 17 11 13 37 19 38 13 19  5 16  5  5 22 35 35 19 13 17 22&#xA;[126]  7  8  8 41 39 14  6 19 18 38 22 14 30 19 24 11 14 13 16  7 13  8 22 12  8&#xA;[151] 22 17  2 40 13 35 15  5 24 19 41 22 22 19 17  8 13 33 10 17 29 12  9 11 24&#xA;[176] 27  5 14  7 25 13 35  9 18 16 13 31 25 18 10 35 18 14 33  5 19 17  8 11 11&#xA;[201] 42  7 10  8 39 19  8  7 11 19 31 35 28 36 13 37 10 38  3 18 13 34 39 13  3&#xA;[226] 40 31 17 19 26  7 19 12 22 19 17 12 17 13 12 28 33 37 17 35 18 40 13 20 35&#xA;[251] 17  6 15 14 12 27  5 41 31 41 18 21 19 17 40  8 41  2  5 15  5 15 15  5 39&#xA;[276]  1 22 33 13 17  7 11 15  3  7 38 15  7 14 14 22 31 14 18  3 19  7  9  6  5&#xA;[301] 22  8 11  1 13 10 19  8 14 15  6 32  1  2 19  4  5 13 18 42 11 17  8 36 19&#xA;[326] 15 22 28 15  8 11  8 15 38 19 39  7 11 42 23  5  1 22 17 35 13 40 25  7 41&#xA;[351] 38  5  8 19 31  6 38  8 36  4 15 15  7 12 22 38 29 35 15  2 10  2 19 15 25&#xA;[376]  5 39 30 19 18 15 10 10 33  1 19 32 19 19 15 20 15  7 22 35 22 30 14 12 16&#xA;[401] 21 13 15 18  7  5  6 31 12  7 15 32 33 18 14 15 15 26 31  1 14 19 17 18  1&#xA;[426]  5 10 32&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, when I run the ComBat analysis it generates all NANs, and no values:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; modcombat = model.matrix(~1, data=omics)&#xA;&amp;gt; combat_edata = ComBat(dat=edata, batch=batch, mod=modcombat, par.prior=TRUE, prior.plots=FALSE)&#xA;&amp;gt; combat_edata[1:5,1:2]&#xA;      X01240896.3f3f.4bf9.9799.55c87bfacf36&#xA;1                                       NaN&#xA;10                                      NaN&#xA;100                                     NaN&#xA;1000                                    NaN&#xA;10000                                   NaN&#xA;      X01ad5016.f691.4bca.82a0.910429d8d25b&#xA;1                                       NaN&#xA;10                                      NaN&#xA;100                                     NaN&#xA;1000                                    NaN&#xA;10000                                   NaN&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="962" LastEditorUserId="73" LastEditDate="2017-06-23T21:48:47.553" LastActivityDate="2017-06-23T22:07:12.503" Title="NaN values after ComBat analysis on TCGA COAD RNAseq" Tags="&lt;r&gt;&lt;combat&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="887" PostTypeId="1" AcceptedAnswerId="890" CreationDate="2017-06-23T21:12:05.243" Score="6" ViewCount="53" Body="&lt;p&gt;I am attempting to run a BLAST search remotely using BLAST+.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can get search to work correctly at the command line with the following commands:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blastp -query proteins.fasta -remote -db nr -out proteins_nr.txt -outfmt 6 -evalue 1e-30&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I would like to us the remote database titled &quot;Microbial proteins from nr&quot; which is what would be used for the microbial blast search accessing BLAST from the NCBI site.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm struggling to find the correct code to access this database instead of just &quot;nr&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a list with the codes of different databases?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance&lt;/p&gt;&#xA;" OwnerUserId="727" LastActivityDate="2017-06-25T07:25:07.777" Title="how to set database other than nr for remote blast+ search" Tags="&lt;database&gt;&lt;blast&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="888" PostTypeId="2" ParentId="885" CreationDate="2017-06-23T21:30:22.757" Score="7" Body="&lt;p&gt;PacBio does use PHRED 33, &lt;em&gt;but it turns out the question may be irrelevant&lt;/em&gt; for the newer PacBio Sequel Sequencer, because it reports all base qualities as PHRED 0 (ASCII &lt;code&gt;!&lt;/code&gt;). The RS-II reports PHRED 33 quality scores.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Sequel provides data in the form of &lt;em&gt;subreads&lt;/em&gt;, which are the circular consensus sequences (CCS) from a single zero-mode waveguide (ZMW). Essentially, a single circularized DNA molecule enters a ZMW well, and is sequenced as many times as possible. (Sometimes multiple molecules get into a single well, but that's not the normal case.) The number of times this single molecule can be sequenced depends on several variables, including molecule length. The consensus sequence for this single molecule is then reported out as a subread.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I started playing with the quality scores in my data, I realized the quality score for every base is &lt;code&gt;!&lt;/code&gt;, which equates to a PHRED quality of 0. I contacted someone at PacBio who informed me that they learned the base quality values were not any more useful than the bases themselves for generating a high-quality circular consensus sequence. Thus, rather than spend the computational effort to calculate the scores, they skipped it, and reported &lt;code&gt;!&lt;/code&gt; for every base. They also report an overall read quality of 0.8, but this is simply a placeholder. They are working to make this information more readily available.&lt;/p&gt;&#xA;" OwnerUserId="136" LastEditorUserId="136" LastEditDate="2017-06-24T20:23:05.160" LastActivityDate="2017-06-24T20:23:05.160" CommentCount="0" />
  <row Id="889" PostTypeId="2" ParentId="886" CreationDate="2017-06-23T22:07:12.503" Score="1" Body="&lt;p&gt;I think I might know what the problem is. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I plotted the log2 corrected values, and the distribution may not be normal because of the 0 values present in RNASeq data. If I understand correctly, this data is actually missing, whereas with my current analysis it is viewed by R as an actual value of 0. So either the missing data needs to be imputed, or the data needs to be scaled and normalized to mean of 0, then subsequently log2 transformed.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; data &amp;lt;- standardNormalization(data)&#xA;&amp;gt; data &amp;lt;- log2(data + 1)&#xA;&amp;gt; combat_data = ComBat(dat=data, batch=batch, mod=modcombat, par.prior=TRUE, prior.plots=FALSE)&#xA;&amp;gt; combat_data[1:5,1:2]&#xA;          X01240896.3f3f.4bf9.9799.55c87bfacf36&#xA;100507661                           -0.13857428&#xA;57103                               -0.06700506&#xA;22838                                0.16295096&#xA;55567                               -0.13579991&#xA;6147                                 1.64990772&#xA;          X01ad5016.f691.4bca.82a0.910429d8d25b&#xA;100507661                           -0.13789534&#xA;57103                               -0.06210232&#xA;22838                                0.01546224&#xA;55567                               -0.13443289&#xA;6147                                 1.48473072&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="962" LastActivityDate="2017-06-23T22:07:12.503" CommentCount="0" />
  <row Id="890" PostTypeId="2" ParentId="887" CreationDate="2017-06-23T23:18:32.853" Score="8" Body="&lt;p&gt;I believe you're looking for &lt;code&gt;env_nr&lt;/code&gt;? It's listed as such, under &lt;code&gt;Metagenomic proteins&lt;/code&gt; in the blastp webpage. It appears that the word within brackets should be supplied alongside the &lt;code&gt;-db&lt;/code&gt; parameter. A quick test with a dummy amino acid fasta file does turn up a result to a valid NCBI protein accession.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/hNm3k.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/hNm3k.png&quot; alt=&quot;partial screenshot from NCBI&amp;#39;s blastp webpage&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: I followed up on this with a little more digging around, and it appears the database you're looking for is called &lt;code&gt;Microbial_proteins&lt;/code&gt;. I ended up quote-searching &lt;code&gt;Microbial proteins from nr&lt;/code&gt; on Google and exactly 2 hits turn up. The first hit, I think, is what the OP posted a screenshot of in a comment to this answer. The second is a list of databases along with keywords from a Spain-based bioinformatics firm: &lt;a href=&quot;http://data.biobam.com/ncbi_blast_dbs_protein.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://data.biobam.com/ncbi_blast_dbs_protein.pdf&lt;/a&gt;. I retried the dummy example with the &lt;code&gt;Microbial_proteins&lt;/code&gt; database, and it seems to work. Again, I am not sure why I can't easily find this on an NCBI resource. Also, I am curious as to how different this database is from &lt;code&gt;env_nr&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="541" LastEditorUserId="541" LastEditDate="2017-06-25T07:25:07.777" LastActivityDate="2017-06-25T07:25:07.777" CommentCount="2" />
  <row Id="892" PostTypeId="1" AcceptedAnswerId="894" CreationDate="2017-06-24T03:12:53.440" Score="8" ViewCount="114" Body="&lt;p&gt;How do you write a &lt;code&gt;.gz&lt;/code&gt; (or &lt;code&gt;.bgz&lt;/code&gt;) &lt;code&gt;fastq&lt;/code&gt; file using &lt;code&gt;Biopython&lt;/code&gt;? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd rather avoid a separate system call.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The typical way to write an &lt;code&gt;ASCII&lt;/code&gt; &lt;code&gt;.fastq&lt;/code&gt; is done as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;for record in SeqIO.parse(fasta, &quot;fasta&quot;):&#xA;    SeqIO.write(record, fastq, &quot;fastq&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;record&lt;/code&gt; is a &lt;code&gt;SeqRecord&lt;/code&gt; object, &lt;code&gt;fastq&lt;/code&gt; is the &lt;code&gt;file handle&lt;/code&gt;, and &lt;code&gt;&quot;fastq&quot;&lt;/code&gt; is the requested file format. The file format may be &lt;code&gt;fastq&lt;/code&gt;, &lt;code&gt;fasta&lt;/code&gt;, etc., but I do not see an option for &lt;code&gt;.gz&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the &lt;a href=&quot;http://biopython.org/DIST/docs/api/Bio.SeqIO-module.html&quot; rel=&quot;noreferrer&quot;&gt;SeqIO&lt;/a&gt; API.&lt;/p&gt;&#xA;" OwnerUserId="136" LastActivityDate="2017-08-09T20:19:15.707" Title="How do you write a .gz fastq file with Biopython?" Tags="&lt;file-formats&gt;&lt;biopython&gt;&lt;python&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="3" />
  <row Id="893" PostTypeId="2" ParentId="892" CreationDate="2017-06-24T04:11:01.577" Score="0" Body="&lt;p&gt;As far as I know you can not write compressed fastq on the fly. You will have to write the entire fastq and then compress it.&lt;/p&gt;&#xA;" OwnerUserId="964" LastEditorUserId="73" LastEditDate="2017-06-24T12:41:55.300" LastActivityDate="2017-06-24T12:41:55.300" CommentCount="1" />
  <row Id="894" PostTypeId="2" ParentId="892" CreationDate="2017-06-24T06:49:27.180" Score="10" Body="&lt;p&gt;I'm not sure I'm doing it the best way, but here is an example where I read a compressed gzip fastq file and write the records in block gzip fastq:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;from Bio import SeqIO, bgzf&#xA;# Used to convert the fastq stream into a file handle&#xA;from io import StringIO&#xA;from gzip import open as gzopen&#xA;&#xA;records = SeqIO.parse(&#xA;    # There is actually simpler (thanks @peterjc)&#xA;    # StringIO(gzopen(&quot;random_10.fastq.gz&quot;).read().decode(&quot;utf-8&quot;)),&#xA;    gzopen(&quot;random_10.fastq.gz&quot;, &quot;rt&quot;),&#xA;    format=&quot;fastq&quot;)&#xA;&#xA;with bgzf.BgzfWriter(&quot;test.fastq.bgz&quot;, &quot;wb&quot;) as outgz:&#xA;    SeqIO.write(sequences=records, handle=outgz, format=&quot;fastq&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-06-26T17:18:28.973" LastActivityDate="2017-06-26T17:18:28.973" CommentCount="3" />
  <row Id="895" PostTypeId="1" AcceptedAnswerId="897" CreationDate="2017-06-24T11:19:23.527" Score="6" ViewCount="85" Body="&lt;p&gt;I want to get a .bed file with the genes' names and canonical coordinates, also I would like to have coordinates of exons, too. I can get the list from UCSC, however, if I choose UCSC Genes - knownCanonical, I can not extract coordinates of exons. If I use other options - I am getting coordinates of as many transcriptional isoforms as were detected while I need only one canonical form.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I get such BED file?&lt;/p&gt;&#xA;" OwnerUserId="349" LastEditorUserId="96" LastEditDate="2017-07-03T21:24:03.917" LastActivityDate="2017-07-04T17:42:55.687" Title="How to obtain .bed file with coordinates of all genes" Tags="&lt;bed&gt;&lt;public-databases&gt;" AnswerCount="3" CommentCount="1" FavoriteCount="0" />
  <row Id="897" PostTypeId="2" ParentId="895" CreationDate="2017-06-24T15:37:00.853" Score="8" Body="&lt;p&gt;Via Gencode and BEDOPS &lt;code&gt;convert2bed&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ wget -qO- ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_21/gencode.v21.annotation.gff3.gz \&#xA;    | gunzip --stdout - \&#xA;    | awk '$3 == &quot;gene&quot;' - \&#xA;    | convert2bed -i gff - \&#xA;    &amp;gt; genes.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You can modify the &lt;code&gt;awk&lt;/code&gt; statement to get exons, by replacing &lt;code&gt;gene&lt;/code&gt; with &lt;code&gt;exon&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BEDOPS: &lt;a href=&quot;https://github.com/bedops/bedops&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/bedops/bedops&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is based off an answer I wrote on Biostars, which includes a Perl script for generating a BED file of introns from gene and exon annotations: &lt;a href=&quot;https://www.biostars.org/p/124515/#124522&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.biostars.org/p/124515/#124522&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="776" LastEditorUserId="776" LastEditDate="2017-07-04T17:42:55.687" LastActivityDate="2017-07-04T17:42:55.687" CommentCount="1" />
  <row Id="898" PostTypeId="2" ParentId="45" CreationDate="2017-06-24T17:29:15.333" Score="1" Body="&lt;p&gt;&lt;strong&gt;&lt;em&gt;You specifically asked about FASTA files, but it's important to always consider read length and quality jointly when assessing high-error long-read data. FASTA files do not provide the quality. This information will help you determine how successful the run was, how many reads were 'high quality', etc.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I originally posted a full answer here, suggesting &lt;a href=&quot;https://github.com/conchoecia/pauvre&quot; rel=&quot;nofollow noreferrer&quot;&gt;pauvre&lt;/a&gt;, but I decided it was a little off topic since you seem to only have the FASTA files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I recommend generating FASTQ files, but I'm not sure whether you have the original base-called fast5 files. If so, generate FASTQs using &lt;code&gt;poretools&lt;/code&gt; as follows (&lt;a href=&quot;https://poretools.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;poretools doc for generating FASTQ files&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;poretools fastq fast5/&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I recommend &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/899/how-do-you-generate-read-length-vs-read-quality-plot-for-long-read-sequencing-da/900#900&quot;&gt;generating a heat map and histogram margin plot&lt;/a&gt; using &lt;a href=&quot;https://github.com/conchoecia/pauvre&quot; rel=&quot;nofollow noreferrer&quot;&gt;pauvre&lt;/a&gt; with both read length and quality like shown below.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Jl7JS.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Jl7JS.png&quot; alt=&quot;My description&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Note: I am not the original author for pauvre, but I am now contributing to this project]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="136" LastEditorUserId="136" LastEditDate="2017-06-24T18:33:29.243" LastActivityDate="2017-06-24T18:33:29.243" CommentCount="1" />
  <row Id="899" PostTypeId="1" CreationDate="2017-06-24T18:16:49.600" Score="7" ViewCount="63" Body="&lt;p&gt;How do you generate read-length vs read-quality plot (heat map with histograms in the margin) for long-read sequencing data from the Oxford Nanopore Technologies (ONT) MinION? The MinKNOW software from ONT provides a plot like this during base calling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This would also be very helpful for PacBio data.&lt;/p&gt;&#xA;" OwnerUserId="136" LastEditorUserId="136" LastEditDate="2017-06-24T18:35:31.240" LastActivityDate="2017-06-26T09:51:54.100" Title="How do you generate read-length vs read-quality plot for long-read sequencing data (e.g., MinION)?" Tags="&lt;nanopore&gt;&lt;sequencing&gt;&lt;biopython&gt;&lt;fastq&gt;&lt;python&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="2" />
  <row Id="900" PostTypeId="2" ParentId="899" CreationDate="2017-06-24T18:16:49.600" Score="3" Body="&lt;p&gt;It's important to always consider read length and quality jointly with high-error read data, and current long-read technologies (e.g., MinION and PacBio) have high error rates. &lt;strong&gt;&lt;em&gt;Considering read length and quality jointly will help you determine how successful the run was, how many reads were 'high quality', whether the longer reads are 'real' (or just pore noise), etc.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've had a recent spike of interest in similar plots and came across a project called &lt;a href=&quot;https://github.com/conchoecia/pauvre&quot; rel=&quot;nofollow noreferrer&quot;&gt;pauvre&lt;/a&gt; (french for 'poor', play on 'pore') through the Oxford Nanopore Technologies (ONT) community that I think is even better than MinKNOW's base calling plot.  Plus, you can generate these plots from a fastq file when ever you want to, unlike with MinKNOW. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;[Note: I am not the original author, but I am now contributing because I liked (and needed) it.]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Jl7JS.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Jl7JS.png&quot; alt=&quot;My description&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Pauvre will also report useful statistics:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fastq stats for fastq_runid_bb8b8ddedb22bdd6802b2bfa2b4e424c92c30d28_0.fastq&#xA;numReads: 2164829&#xA;numBasepairs: 4970615217&#xA;meanLen: 2296.077527139557&#xA;medianLen: 1495.0&#xA;minLen: 5&#xA;maxLen: 392031&#xA;N50: 3450&#xA;L50: 402786&#xA;&#xA;                               Basepairs &amp;gt;= bin by mean PHRED and length&#xA;minLen          Q0          Q5         Q10         Q15       Q17.5         Q20      Q21.5  Q25  Q25.5  Q30&#xA;     0  4970615217  4970611559  4835461771  3889995868  2900103275  1087779109  165637656  429      0    0&#xA; 50000    11531044    11531044      270324      160128       50729       50729          0    0      0    0&#xA;100000     6260554     6260554           0           0           0           0          0    0      0    0&#xA;150000     3504240     3504240           0           0           0           0          0    0      0    0&#xA;200000     2501101     2501101           0           0           0           0          0    0      0    0&#xA;250000     1609592     1609592           0           0           0           0          0    0      0    0&#xA;300000     1033423     1033423           0           0           0           0          0    0      0    0&#xA;350000      392031      392031           0           0           0           0          0    0      0    0&#xA;&#xA;                    Number of reads &amp;gt;= bin by mean Phred+Len&#xA;minLen       Q0       Q5      Q10      Q15    Q17.5     Q20  Q21.5  Q25  Q25.5  Q30&#xA;     0  2164829  2164605  2083436  1626706  1183812  435687  77341    1      0    0&#xA; 50000      109      109        5        3        1       1      0    0      0    0&#xA;100000       36       36        0        0        0       0      0    0      0    0&#xA;150000       15       15        0        0        0       0      0    0      0    0&#xA;200000        9        9        0        0        0       0      0    0      0    0&#xA;250000        5        5        0        0        0       0      0    0      0    0&#xA;300000        3        3        0        0        0       0      0    0      0    0&#xA;350000        1        1        0        0        0       0      0    0      0    0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;These plots and stats would be equally useful with PacBio, but that's not super easy (though it is possible) with current raw output from the Sequel sequencer: &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/885/which-quality-score-encoding-does-pacbio-use/888&quot;&gt;Which quality score encoding does PacBio use?&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Pauvre currently uses Biopython to parse the fastq and &lt;code&gt;matplotlib&lt;/code&gt; for the actual plot, and will let you choose the output image format (e.g., .png, .pdf, etc.). You can also choose whether the background is transparent or white (for .png output). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The parser is currently super slow because it's using &lt;code&gt;SeqIO.parse&lt;/code&gt;, but we're changing parsers to speed that up. We're also adding some extra features (e.g., choose whether to include y-axes in margin histograms, print some stats directly to the plot for documentation, etc.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Purple is currently the only color choice (which I personally love), but adding options to change that will be super easy.&lt;/p&gt;&#xA;" OwnerUserId="136" LastEditorUserId="136" LastEditDate="2017-06-24T22:07:08.513" LastActivityDate="2017-06-24T22:07:08.513" CommentCount="0" />
  <row Id="901" PostTypeId="1" AcceptedAnswerId="902" CreationDate="2017-06-24T23:33:03.743" Score="6" ViewCount="119" Body="&lt;p&gt;At the moment, the standard reference genomes (e.g. hg19, hg38) are haploid genomes. We know that the human genome is diploid. Naturally, the latter would be the respectively correct representation of the human genome. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;More and more biologists are using emerging technologies into order to capture the diploid nature of genetic information, e.g. phasing SNPs between mother and father chromosomes. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the standard way that bioinformaticians generated a standard diploid reference genome? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Actually, reference genomes are not truly haploid (by my understanding). Given that reference genomes are 5'-3' coordinated, in order to create a complementary strand, one would need to take the 3'-5' complement. In order to have a diploid genome, you need two reference genomes and two 3'-5' complements.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More importantly, how have large-scale genomic studies dealt with the fact that the haploid reference genome is a consensus-based &quot;half&quot; of a human genome? &lt;/p&gt;&#xA;" OwnerUserId="146" LastEditorUserId="37" LastEditDate="2017-06-25T02:07:50.473" LastActivityDate="2017-06-26T20:07:16.807" Title="What is the standard way to work with a diploid reference genome? Complementary strands?" Tags="&lt;human-genome&gt;&lt;genome&gt;&lt;reference-genome&gt;" AnswerCount="4" CommentCount="2" />
  <row Id="902" PostTypeId="2" ParentId="901" CreationDate="2017-06-25T02:00:16.993" Score="5" Body="&lt;p&gt;For calling small variants, the &lt;em&gt;standard way&lt;/em&gt; is to simply call diploid genotypes. You can already do a variety of research with unphased genotypes. You may further phase genotypes with imputation, pedigree or with long reads/linked reads, but not many are doing this because phasing is more difficult, may add cost and may not always give you new insight into your data. For these analyses, we use a haploid genome. For human samples, the vast majority of &quot;large-scale genomic studies&quot; are done this way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A diploid reference actually doesn't help much with reference-based analysis; it only complicates algorithms. What could help a lot is a population reference, which may be represented by a graph or a compressed full-text index or both. In theory, if you have a comprehensive population reference and a capable mapping algorithm, you may call extra variants that would not be callable with short reads. In practice, however, there are quite a few technical challenges. Handling population references is a research topic. There are no &quot;standards&quot; yet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the goal is to assemble a new reference genome from a diploid sample, we almost always prefer to produce a diploid assembly. Unfortunately, I believe there are no &quot;standard&quot; procedures, either. SuperNova from 10x genomics builds the diploid information into a graph. Falcon from PacBio uses &quot;unzip&quot;. I don't think they have got widely used and evaluated so far.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: saw your edit while writing the above. The fact that the genome only represents one strand does not mean we have to create the complement strand explicitly in analyses. We do most of reverse complement on the fly in algorithms as well as in mind.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;reference genomes are not truly haploid&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;That depends on how the reference is assembled. If you sequence a haploid sample (e.g. bacteria), your assembly will be haploid. If you sequence an inbreed lab strain that is almost homozygous (e.g. mouse and fruit fly), your assembly will be nearly haploid. If you sequence a diploid sample, your assembly is very likely to be a mosaic of the two haplotypes. In case of the human reference genome, it is more complicated. It is largely a mosaic of several humans by stitching ~150kb haplotypes from these samples.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-06-25T02:00:16.993" CommentCount="0" />
  <row Id="903" PostTypeId="5" CreationDate="2017-06-25T02:14:49.307" Score="0" Body="" OwnerUserId="37" LastEditorUserId="37" LastEditDate="2017-06-25T02:14:49.307" LastActivityDate="2017-06-25T02:14:49.307" CommentCount="0" />
  <row Id="904" PostTypeId="4" CreationDate="2017-06-25T02:14:49.307" Score="0" Body="An insertion/deletion sequence variation." OwnerUserId="37" LastEditorUserId="37" LastEditDate="2017-06-25T02:14:49.307" LastActivityDate="2017-06-25T02:14:49.307" CommentCount="0" />
  <row Id="905" PostTypeId="2" ParentId="901" CreationDate="2017-06-25T03:48:52.500" Score="2" Body="&lt;p&gt;There are some assemblers that produce assembly graphs that attempt to describe all the possible haploid paths within a set of reads. Such an assembly attempts to capture all the diploid variation (and/or population variation) in a sample at the expense of not having full-length chromosomes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Canu (for example) will produce contigs that are extended as long as consensus is maintained across different reads, but when there is a reliable break in coverage (i.e. an area where chromosomes are heterozygous) then the contigs will be broken up. Canu provides as output a GFA file (assembly graph) that can be used to determine which paths might combine together into a single chromosome.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-25T03:48:52.500" CommentCount="0" />
  <row Id="906" PostTypeId="2" ParentId="884" CreationDate="2017-06-25T22:03:05.130" Score="4" Body="&lt;p&gt;From &lt;a href=&quot;http://www.bmsc.washington.edu/CrystaLinks/man/pdb/part_31.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;this document&lt;/a&gt;, it looks like REMARK 250 is the way to go.&lt;/p&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-06-25T22:03:05.130" CommentCount="1" />
  <row Id="907" PostTypeId="2" ParentId="901" CreationDate="2017-06-26T07:23:26.087" Score="3" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;At the moment, the standard reference genomes (e.g. hg19, hg38) are haploid genomes. We know that the human genome is diploid. Naturally, the latter would be the respectively correct representation of the human genome.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The premise of your question is false.  The natural reference representation of the human genome is not diploid.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Think of a reference genome as a map, and not as a specific example of a human being's DNA.&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Not only is the human genome reference haploid, but &lt;a href=&quot;https://en.wikipedia.org/wiki/Human_genome#Human_reference_genome&quot; rel=&quot;nofollow noreferrer&quot;&gt;it is also a composite genome&lt;/a&gt;. This means that the human genome reference sequence is composed of sequences from multiple individuals.  In other words, the human reference does not correspond to any one human sequence. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any particular read from a DNA sequencer will be a read from a human genome that diverges from the reference genome. So an algorithm that tries to match the read to the reference genome will always need to handle potential discrepancies. Adding a second map against which to match a read would not change that fact.  Therefore, there is little value in providing a second haploid reference genome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Side note&lt;/strong&gt;:&#xA;There are parts of the human genome that &quot;are too complex to be represented by a single path&quot;, and the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/grc/human&quot; rel=&quot;nofollow noreferrer&quot;&gt;Genome Reference Consortium&lt;/a&gt; provides &quot;alternate loci&quot; for such regions of the genome.&lt;/p&gt;&#xA;" OwnerUserId="492" LastActivityDate="2017-06-26T07:23:26.087" CommentCount="1" />
  <row Id="908" PostTypeId="1" AcceptedAnswerId="916" CreationDate="2017-06-26T08:06:51.857" Score="6" ViewCount="113" Body="&lt;p&gt;I have three sequencing libraries of single individual mapped to a reference using &lt;code&gt;bwa-mem&lt;/code&gt;. I would like to merge the three unsorted &lt;code&gt;.sam&lt;/code&gt; files I have so, I can call variants and heterozygosity estimates using &lt;a href=&quot;https://bitbucket.org/phaentu/atlas/wiki/Home&quot; rel=&quot;nofollow noreferrer&quot;&gt;atlas&lt;/a&gt;. Atlas requires one input mapping file (&lt;code&gt;bam&lt;/code&gt;) with defined read groups because it estimates the error profiles of different libraries separately.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I merge multiple sam files? Preferably avoiding java (Picard tools).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to figure out a solution using &lt;code&gt;samtools 1.3&lt;/code&gt;. I sorted individual files using &lt;code&gt;samtools sort&lt;/code&gt;, then I used &lt;code&gt;samtools merge -r merged.bam s1.sort.sam s2.sort.sam s3.sort.sam&lt;/code&gt; to merge the sorted files. However, the read group didn't make it to the header (and the variant caller I use is complaining about it), also the read group is stupidly the file name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to define meaningful readgroup names using procedure described on &lt;a href=&quot;https://www.biostars.org/p/47487/&quot; rel=&quot;nofollow noreferrer&quot;&gt;BioStars&lt;/a&gt;, but I found that this will just change the header, it does not adjust the names of read groups defined by &lt;code&gt;samtools merge&lt;/code&gt; (the file names).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Following this related thread on &lt;a href=&quot;http://seqanswers.com/forums/showthread.php?t=4180&quot; rel=&quot;nofollow noreferrer&quot;&gt;SeqAnswers&lt;/a&gt;, I tried to define the correct header with read groups corresponding to merged file names:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools -rh rg.txt merged.bam s1.sort.sam s2.sort.sam s3.sort.sam&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;rg.txt&lt;/code&gt; is a file with header&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@RG s1.sort&#xA;@RG s2.sort&#xA;@RG s3.sort&#xA;...&#xA;output of samtools view -H s1.sort&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, the header still had not the read group, I guess because sam header accepts only tagged items to be specified (something like &lt;code&gt;@RG    XY:s1.sort&lt;/code&gt;). So, I looked into the merged &lt;code&gt;bam&lt;/code&gt; and I found that the tag of RG is &lt;code&gt;Z:&lt;/code&gt;. So I tried to just rename header of the merged file using &lt;code&gt;samtools reheader&lt;/code&gt;, but then samtools complain about the fact that a tag needs to be of length 2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Malformed key:value pair at line 123: &quot;@RG    Z:s1.sort&quot;&#xA;Segmentation fault (core dumped)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have opened an &lt;a href=&quot;https://github.com/samtools/samtools/issues/698&quot; rel=&quot;nofollow noreferrer&quot;&gt;issue&lt;/a&gt; to report this strange incompatibility of read groups generated by &lt;code&gt;samtools merge&lt;/code&gt; with &lt;code&gt;samtools reheader&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like solution to :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;create a bit more standardized read group names (&lt;code&gt;SM:Sample\tLB:library&lt;/code&gt; format)&lt;/li&gt;&#xA;&lt;li&gt;avoid pointless writing to disk like in sam -&gt; sorted.sam -&gt; merged.bam case (can be probably achieved through &quot;pipes and tees&quot;, thanks @bli)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I also know that I can specify RG to bwa, so the sam files will have read groups defined in the first place. But I do not like the idea of remapping three libraries just to create correct formatting of read groups.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-26T14:37:37.387" LastActivityDate="2017-06-27T04:10:47.523" Title="How to merge sam files together with adding read groups" Tags="&lt;sam&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="909" PostTypeId="1" AcceptedAnswerId="912" CreationDate="2017-06-26T09:14:18.923" Score="3" ViewCount="172" Body="&lt;p&gt;Note: I also posted this issue (with less context) in the bioconductor support site: &lt;a href=&quot;https://support.bioconductor.org/p/97424/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://support.bioconductor.org/p/97424/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm working on a snakemake workflow that identifies various small RNA species in &lt;em&gt;C. elegans&lt;/em&gt; small RNA-seq libraries. Some are endogenous siRNAs supposedly generated from RNA templates (through RNA-dependant RNA polymerases (RdRP)) that can be variously classified (protein-coding genes, transposons, and other repeat types).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I count such small RNA reads in a set of libraries and try to identify differentially producing sources. For this, I use DESeq2 (that I run &lt;a href=&quot;https://stackoverflow.com/q/42157335/1878788&quot;&gt;using rpy2&lt;/a&gt; from within snakemake).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure DESeq2 is always appropriate for this kind of data, but so far, the analyses would at least complete.&#xA;However, I recently added new potential types of small RNA source (simple repeats and satellites), and these happen to have low counts. I'm not 100% sure, but I suspect these low counts are the reason for failures during DESeq2 analyses (for debugging purposes, I ran this manually in R):&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; dds &amp;lt;- DESeq(dds, betaPrior=T)&#xA;estimating size factors&#xA;estimating dispersions&#xA;gene-wise dispersion estimates&#xA;mean-dispersion relationship&#xA;-- note: fitType='parametric', but the dispersion trend was not well captured by the&#xA;   function: y = a/x + b, and a local regression fit was automatically substituted.&#xA;   specify fitType='local' or 'mean' to avoid this message next time.&#xA;Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xA;  newsplit: out of vertex space&#xA;In addition: There were 12 warnings (use warnings() to see them)&#xA;&amp;gt; warnings()&#xA;Warning messages:&#xA;1: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;2: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;3: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;4: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;5: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;6: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;7: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;8: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;9: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;10: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;11: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;12: In lfproc(x, y, weights = weights, cens = cens, base = base,  ... :&#xA;  procv: no points with non-zero weight&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The count matrix indeed contains a high proportion of zeroes, the rest are mostly ones, but there are some sources that seem a little more significantly producing small RNAs:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; mean(counts_data == 0)&#xA;[1] 0.7488889&#xA;&amp;gt; mean(counts_data == 1)&#xA;[1] 0.1507407&#xA;&amp;gt; max(counts_data)&#xA;[1] 34&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How would you recommend handling such kind of data?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Should I for instance discard rows without enough counts?&#xA;If so, what would be a reasonable threshold?&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Edits: trying to get a dispersion plot:&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;The same error occurs when trying to estimate dispersion:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; dds &amp;lt;- estimateSizeFactors(dds)&#xA;&amp;gt; dds &amp;lt;- estimateDispersions(dds)&#xA;gene-wise dispersion estimates&#xA;mean-dispersion relationship&#xA;-- note: fitType='parametric', but the dispersion trend was not well captured by the&#xA;   function: y = a/x + b, and a local regression fit was automatically substituted.&#xA;   specify fitType='local' or 'mean' to avoid this message next time.&#xA;Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xA;  newsplit: out of vertex space&#xA;In addition: There were 12 warnings (use warnings() to see them)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This seems to prevent the generation of dispersion plots:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; plotDispEsts(dds)&#xA;Error in min(py[py &amp;gt; 0], na.rm = TRUE) : &#xA;  invalid 'type' (list) of argument&#xA;In addition: Warning message:&#xA;In structure(x, class = unique(c(&quot;AsIs&quot;, oldClass(x)))) :&#xA;  Calling 'structure(NULL, *)' is deprecated, as NULL cannot have attributes.&#xA;  Consider 'structure(list(), *)' instead.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Using &lt;code&gt;fitType=&quot;local&quot;&lt;/code&gt; also fails:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; dds &amp;lt;- DESeq(dds, betaPrior=T, fitType=&quot;local&quot;)&#xA;using pre-existing size factors&#xA;estimating dispersions&#xA;gene-wise dispersion estimates&#xA;mean-dispersion relationship&#xA;Error in lfproc(x, y, weights = weights, cens = cens, base = base, geth = geth,  : &#xA;  newsplit: out of vertex space&#xA;In addition: There were 12 warnings (use warnings() to see them)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But &lt;code&gt;fitType=&quot;mean&quot;&lt;/code&gt; works:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; dds &amp;lt;- DESeq(dds, betaPrior=T, fitType=&quot;mean&quot;)&#xA;estimating size factors&#xA;estimating dispersions&#xA;gene-wise dispersion estimates&#xA;mean-dispersion relationship&#xA;final dispersion estimates&#xA;fitting model and testing&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The dispersion plot then looks as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/647fF.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/647fF.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What do these &lt;code&gt;fitType&lt;/code&gt; options mean, and are there good reasons why they would have to be changed in order to accomodate for low counts?&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-06-26T09:48:42.643" LastActivityDate="2017-06-26T10:55:41.433" Title="Running differential expression analyses on count matrices with many zeroes" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;differential-expression&gt;&lt;deseq2&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="910" PostTypeId="2" ParentId="909" CreationDate="2017-06-26T09:44:14.527" Score="3" Body="&lt;p&gt;Remove low-count features in advance. This is standard for most tools including &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#pre-filtering&quot; rel=&quot;nofollow noreferrer&quot;&gt;DESeq2&lt;/a&gt; and &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;edgeR&lt;/a&gt; (see section 2.6). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This will keep you from testing a lot of features that cannot be differentially expressed (using NB model you need at least four reads - below that the uncertainty is too high to ever be DE) and you will make the false discovery correction unnecessarily hard.&lt;/p&gt;&#xA;" OwnerUserId="599" LastEditorUserId="73" LastEditDate="2017-06-26T10:48:12.570" LastActivityDate="2017-06-26T10:48:12.570" CommentCount="0" />
  <row Id="911" PostTypeId="2" ParentId="899" CreationDate="2017-06-26T09:51:54.100" Score="1" Body="&lt;p&gt;I also wrote a package to create various plots from Oxford Nanopore sequencing data and alignments: &lt;a href=&quot;https://github.com/wdecoster/NanoPlot&quot; rel=&quot;nofollow noreferrer&quot;&gt;NanoPlot&lt;/a&gt;. It can be installed through pip (see also &lt;a href=&quot;https://github.com/wdecoster/NanoPlot&quot; rel=&quot;nofollow noreferrer&quot;&gt;the README on Github&lt;/a&gt;). In addition to multiple plots also a limited NanoStats output is created (see also &lt;a href=&quot;https://github.com/wdecoster/nanostat&quot; rel=&quot;nofollow noreferrer&quot;&gt;NanoStat&lt;/a&gt;). Data can be presented using:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A fastq file (optionally compressed)&lt;/li&gt;&#xA;&lt;li&gt;A bam file&lt;/li&gt;&#xA;&lt;li&gt;The sequencing_summary.txt file generated by albacore&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Using optional flags you can:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Log transform the read lengths&lt;/li&gt;&#xA;&lt;li&gt;Use aligned reads rather than sequenced reads&lt;/li&gt;&#xA;&lt;li&gt;Downsample the reads&lt;/li&gt;&#xA;&lt;li&gt;Set a maximum read length&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've added an example below, plotting log transformed read length vs average read quality (using a kernel density estimate). More examples can be found in the &lt;a href=&quot;https://gigabaseorgigabyte.wordpress.com/2017/06/01/example-gallery-of-nanoplot/&quot; rel=&quot;nofollow noreferrer&quot;&gt;gallery on my blog&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I welcome all feedback and suggestions!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/ZwsHX.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZwsHX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="681" LastActivityDate="2017-06-26T09:51:54.100" CommentCount="0" />
  <row Id="912" PostTypeId="2" ParentId="909" CreationDate="2017-06-26T10:55:41.433" Score="4" Body="&lt;p&gt;I don't think that the issue is the low counts, but rather the number of features without any real variance (the black dots at the bottom).&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;So what the heck is the dispersion plot and why does one need to fit it anyway?&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;In a typical RNAseq experiment, one measures many thousands of genes with only a few replicates per biological group. This then leads to issues when performing testing, since your variance measurement for any given gene will be larger or smaller than reality, simply due to the lack of a large number of replicates. Consequently, at least since the &lt;code&gt;limma&lt;/code&gt; package came out, most RNAseq packages have tried to fit the mean-dispersion with some sort of line and then used that as a background distribution toward which the variance assigned to each gene should be &quot;shrunken&quot; (i.e., moved toward). Getting this at least approximately correct is important, since if you have few replicates and the final &quot;shrunken variance&quot; is significantly wrong, then your p-values will also be wrong.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;What are the various fitType options and what are they doing?&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;The three options that one can use to fit this relationship are &lt;code&gt;parametric&lt;/code&gt;, &lt;code&gt;local&lt;/code&gt;, and &lt;code&gt;mean&lt;/code&gt;. &lt;code&gt;parametric&lt;/code&gt; is the default and basically results in fitting to a bent line. This usually works quite well, but you can see that you have two clouds of data, one shaped like a bent line (at the top) and another (the black dots at the bottom) with no variance. That's causing this fit type to fail. Likewise, &lt;code&gt;local&lt;/code&gt; is doing a local fit and failing for the same reason. &lt;code&gt;mean&lt;/code&gt; is just taking the mean (average) of all of the estimates. This is fool-proof, but you're then regressing to something that really doesn't fit your data. &lt;code&gt;mean&lt;/code&gt; is really a last choice if nothing else works.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For your particular dataset, my main concerns would be figuring out (1) why the maximum mean counts are around 20 (that's really low) and (2) why so many of these features have incredibly low variance.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-06-26T10:55:41.433" CommentCount="0" />
  <row Id="913" PostTypeId="2" ParentId="908" CreationDate="2017-06-26T16:50:19.213" Score="2" Body="&lt;p&gt;&lt;strong&gt;Not so elegant but working solution&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found a solution that satisfy several of my conditions, basically I just have to assign read groups to individual mapping files, which can be just added to sorting process and then merge the sorted sam files with read groups.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-js prettyprint-override&quot;&gt;&lt;code&gt;function sort_and_assign_RG {&#xA;    # $1 input file&#xA;    # $2 read group id&#xA;    # $3 library&#xA;&#xA;    OFILE=$(basename $1 .sam).sort.RG.sam&#xA;    HEADER=$(basename $1 .sam).header.sam&#xA;&#xA;    # since I take header from unsorted file, so I need to add this information to header as well as the readgroup&#xA;    echo -e &quot;@HD\tVN:1.3\tSO:coordinate&quot; &amp;gt; $HEADER&#xA;    # add read group to header&#xA;    echo -e &quot;@RG\tID:&quot;$2&quot;\tLB:&quot;&quot;$3&quot; &amp;gt;&amp;gt; $HEADER&#xA;    # and the rest of the header&#xA;    samtools view -H $1 &amp;gt;&amp;gt; $HEADER&#xA;&#xA;    # now sort the input sam, remove header, &#xA;    # attach to each alignment read group ID (RGID) &#xA;    # and cat header and alignment together&#xA;    samtools sort -- $1 | samtools view - | \&#xA;      awk -v RGID=&quot;$2&quot; '{ printf &quot;%s\tRG:Z:%s\n&quot;, $0, RGID; }' | \&#xA;      cat $HEADER - &amp;gt; $OFILE&#xA;&#xA;    # remove temp header&#xA;    rm $HEADER&#xA;}&#xA;&#xA;sort_and_assign_RG s1.sam s1 is180&#xA;sort_and_assign_RG s2.sam s2 is350&#xA;sort_and_assign_RG s3.sam s3 is550&#xA;&#xA;samtools merge merged.bam s{1,2,3}.sort.RG.sam&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It would be nice to avoid creation of three &lt;code&gt;sam&lt;/code&gt; files on the way, but at least I get standardized read groups in a merged bam tile.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;--- edit ---&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also found where my confusion is coming from. The read group in header is in format &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@RG    ID:foo    LB:lib_foo    ...`&#xA;@RG    ID:bar    LB:lib_bar    ...`&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;ID&lt;/code&gt; is read group ID (not sample ID as I thought) and other tags are just specifications of the read group. Then individual mapping have read group assigned in different format&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{aliment1}    RG:Z:foo&#xA;{aliment2}    RG:Z:bar&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;RG&lt;/code&gt; is just a tag for read group and &lt;code&gt;Z&lt;/code&gt; is just a mark that says that this tag is just a &quot;printable string&quot;. Therefore I think that another solution would just to merge these sorted files and then just adding three &lt;strong&gt;correct&lt;/strong&gt; lines to the header.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have to say, &lt;code&gt;.sam&lt;/code&gt; is a unbelievable intuitive format.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-06-26T17:23:21.913" LastActivityDate="2017-06-26T17:23:21.913" CommentCount="1" />
  <row Id="914" PostTypeId="1" CreationDate="2017-06-26T20:06:27.333" Score="3" ViewCount="57" Body="&lt;p&gt;What tools are available to predict, based on the structure of a certain protein, its interactions within a cell?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, I am considering the split GFP protein and I am trying to predict if there will be any interactions which might hinder its binding efficiency to the other half of the GFP protein. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that I am not only looking for protein interactions, but other molecules as well.&lt;/p&gt;&#xA;" OwnerUserId="823" LastEditorUserId="823" LastEditDate="2017-06-27T15:34:30.763" LastActivityDate="2017-06-28T08:57:51.490" Title="Tool for predicting interactions in the cell" Tags="&lt;protein-structure&gt;&lt;interactions&gt;&lt;3d-structure&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="915" PostTypeId="2" ParentId="901" CreationDate="2017-06-26T20:07:16.807" Score="1" Body="&lt;p&gt;To add to all the other great answers, I would mention that the question is somewhat misleading. If the reference genome is for a single individual, then it should be diploid. However, it's a reference for all humans. It should really contain billions of copies to fully account for all the diversity. Since that is not realistic, the reference serves as a simple approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This point was addressed by the recent &lt;a href=&quot;https://www.nature.com/articles/ncomms13637&quot; rel=&quot;nofollow noreferrer&quot;&gt;Korean genome paper&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Human genomes are routinely compared against a universal reference.&#xA;  However, this strategy could miss population-specific and personal&#xA;  genomic variations, which may be detected more efficiently using an&#xA;  ethnically relevant or personal reference. ... Systematic comparison&#xA;  of human assemblies shows the importance of assembly quality,&#xA;  suggesting the necessity of new technologies to comprehensively map&#xA;  ethnic and personal genomic structure variations.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-06-26T20:07:16.807" CommentCount="0" />
  <row Id="916" PostTypeId="2" ParentId="908" CreationDate="2017-06-27T04:10:47.523" Score="5" Body="&lt;p&gt;I have used &lt;a href=&quot;https://github.com/ekg/bamaddrg&quot; rel=&quot;noreferrer&quot;&gt;https://github.com/ekg/bamaddrg&lt;/a&gt; to add read groups quickly to multiple sam files.  And then you can do a samtools merge of the tagged files.&lt;/p&gt;&#xA;" OwnerUserId="964" LastActivityDate="2017-06-27T04:10:47.523" CommentCount="1" />
  <row Id="917" PostTypeId="2" ParentId="343" CreationDate="2017-06-27T08:06:22.357" Score="3" Body="&lt;p&gt;For pre-existing reliabe TE libraries it is a bit of a mess, because not everybody deposits the species-specific TE libraries to a database like RepBase. And as far as I know DFAM contains only human resources, or am I wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for &lt;em&gt;de novo&lt;/em&gt; generation of species-specific TE libraries (which should be done for any species not already present in eg. RepBase):&#xA;There is no &quot;gold-standard&quot; how to tackle this best.&#xA;In principle one has to think about two main parts&#xA;-repeat detection&#xA;-annotation&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For &lt;strong&gt;repeat detection&lt;/strong&gt; I would recommend using a combination of two things (which is necessary, because TE copies might miss in the assemblies as repetitive regions tend to be difficult to assemble and thrown away in the final assembly).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I) Repeat detection from raw reads (as with e.g. DNApipeTE or tedna or RepeatExplorer). For me, DNAPipeTE worked quite nicely, but everything has pros and cons.&#xA;II) Repeat detection from assemblies (as with e.g. REPET or as mentioned before RepeatModeler)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then the &lt;strong&gt;annotation&lt;/strong&gt; of these repeats is tricky too, because most methods are relying on homology between the &lt;em&gt;de novo&lt;/em&gt; TEs and the TEs from some (probably distantly) related species. But some programs take also structure into account (like REPCLASS). REPET can do both detection and annotation, but is a pain to get to run. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would recommend using some programs to do de novo repeat detection on your species of interest on both the raw reads and assembly, clustering these libraries together (with e.g. uclust and 95% identity) and then run an annotation with homology and structural identification.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Probably the programs will &lt;strong&gt;not give you complete, full-length TE&lt;/strong&gt;s but rather consensus sequences of several copies from TE families.&#xA;If you want, you could search all copies of one family, extract them from the contigs plus boundaries and align them manually and curate boundaries manually. Then extend boundaries if not hitting the surrounding (non-alignable) regions or landmarks of TEs like LTRs or TIRs or so. But this is very time consuming if you only want to compare TE abundance between species for example, I would not do this and rather compare the abundance using read coverage (as in &lt;a href=&quot;https://doi.org/10.1093/molbev/msv261&quot; rel=&quot;nofollow noreferrer&quot;&gt;Bast et al. 2016&lt;/a&gt;). Depends all on the questions you want to ask.&lt;/p&gt;&#xA;" OwnerUserId="518" LastActivityDate="2017-06-27T08:06:22.357" CommentCount="0" />
  <row Id="918" PostTypeId="2" ParentId="914" CreationDate="2017-06-27T08:17:20.487" Score="4" Body="&lt;p&gt;You could try one of these tools to predict protein-protein interactions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://cb.csail.mit.edu/cb/struct2net/webserver/about.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Struct2Net&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Given two protein sequences, the structure-based interaction prediction technique threads these two sequences to all the protein complexes in the PDB and then chooses the best potential match. Based on this match, the method generates alignment scores, z-scores, and an interfacial energy for the sequence pair. Logistic regression is then used to evaluate whether a set of scores corresponds to an interaction or not. The algorithm is also extended to find all potential partners given a single protein sequence. Further details about the method are described &lt;a href=&quot;http://helix-web.stanford.edu/psb06/singh.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://cic.scu.edu.cn/bioinformatics/predict_ppi/default.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;PRED&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Pred_PPI is a web-based system that serves for predicting PPIs from different organisms. This server is freely available to any researcher wishing to use it for non-commercial purposes. Based on auto covariance (AC) and support vector machine (SVM), this tool is capable of predicting PPIs for any target protein pair only using their primary sequences, and assigning an interaction probability to each SVM prediction as well. So the user can use this tool to predict novel PPIs with high confidence.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For protein-RNA interactions, you can try:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://service.tartaglialab.com/page/catrapid_omics_group&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;cat&lt;/em&gt;Rapid&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Through the calculation of secondary structure, hydrogen bonding and van der Waals contributions, catRAPID is able predict protein-RNA interaction propensities with great accuracy (up to 89% on the ncRNA-protein interaction database, NPinter).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-06-28T08:57:51.490" LastActivityDate="2017-06-28T08:57:51.490" CommentCount="4" />
  <row Id="920" PostTypeId="1" AcceptedAnswerId="931" CreationDate="2017-06-27T10:17:45.383" Score="6" ViewCount="340" Body="&lt;p&gt;I want to compare two phylogenies and colour the association lines based on some metadata I have. I have been using ape cophyloplot but I have not had any success in getting the lines to colour accurately according to my data (&lt;a href=&quot;https://stackoverflow.com/questions/44006500/r-coloured-lines-for-tangelgram-package-ape-function-cophyloplot&quot;&gt;see previous question&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that in my actual work flow I define the colour scheme using a palette to control the colour outcome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want a means to make a tangle using phylogenies which I can format. Preferably in R. I like to get an output like this: &lt;img src=&quot;https://i.stack.imgur.com/HTDfR.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="982" LastEditorUserId="982" LastEditDate="2017-06-29T15:49:07.763" LastActivityDate="2017-06-29T15:49:07.763" Title="How do I generate a color-coded tanglegram?" Tags="&lt;r&gt;&lt;phylogeny&gt;&lt;software-recommendation&gt;&lt;phylogenetics&gt;" AnswerCount="4" CommentCount="0" />
  <row Id="921" PostTypeId="2" ParentId="920" CreationDate="2017-06-27T12:30:55.550" Score="9" Body="&lt;p&gt;I think you can try dendextend, in this &lt;a href=&quot;https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html&quot; rel=&quot;noreferrer&quot;&gt;manual&lt;/a&gt; there is an example of coloring the branches. I don't think it is exactly like your coloring, but with a little tweaking you might get your colorscheme in there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The manual mentions an argument called &lt;code&gt;color_lines&lt;/code&gt; for the function  &lt;code&gt;tanglegram()&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;# The `which` parameter allows us to pick the elements in the list to compare&#xA;iris_dendlist %&amp;gt;% dendlist(which = c(1,4)) %&amp;gt;% ladderize %&amp;gt;% &#xA;   # untangle(method = &quot;step1side&quot;, k_seq = 3:20) %&amp;gt;%&#xA;   set(&quot;rank_branches&quot;) %&amp;gt;%&#xA;   tanglegram(common_subtrees_color_branches = TRUE)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;According to the manual, the code above produces an image like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/v8kWS.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/v8kWS.png&quot; alt=&quot;example of a colored tanglegram&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Try to see if you can fit your color palette in there.&lt;/p&gt;&#xA;" OwnerUserId="939" LastEditorUserId="298" LastEditDate="2017-06-27T13:35:46.243" LastActivityDate="2017-06-27T13:35:46.243" CommentCount="1" />
  <row Id="922" PostTypeId="2" ParentId="920" CreationDate="2017-06-27T13:44:49.057" Score="1" Body="&lt;p&gt;As an alternative (though I realise this doesn't really answer the OPs question directly), &lt;a href=&quot;http://dendroscope.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Dendroscope&lt;/a&gt; and &lt;a href=&quot;https://sites.google.com/site/cophylogeny/treemap&quot; rel=&quot;nofollow noreferrer&quot;&gt;Treemap&lt;/a&gt; can also produce these types of plots. I don't know if they'll do coloured connections off the top of my head, but I think they can differentially colour the tree branches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just in case that is of use!&lt;/p&gt;&#xA;" OwnerUserId="929" LastActivityDate="2017-06-27T13:44:49.057" CommentCount="0" />
  <row Id="923" PostTypeId="1" AcceptedAnswerId="930" CreationDate="2017-06-27T16:18:53.597" Score="1" ViewCount="103" Body="&lt;p&gt;I want to convert my phylogeny into a dendrogram so I can use it with dendextend in R to produce a tanglegram. I have made some progress but I keep encountering errors, see below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(ape)&#xA;library(dendextend)&#xA;&#xA;Tree &amp;lt;- rtree(10, rooted=F)&#xA;&#xA;Tree &amp;lt;- read.tree(file=&quot;clipboard&quot;, text=NULL) &#xA;# test &amp;lt;- as.dendrogram(Tree) &#xA;## Error in ape::as.hclust.phylo(object) : the tree is not ultrametric&#xA;&#xA;Tree.ultra &amp;lt;- chronos(Tree)  &#xA;# test &amp;lt;- as.dendrogram(Tree.ultra) &#xA;## Error in ape::as.hclust.phylo(object) : the tree is not rooted&#xA;&#xA;Tree.ultra$root.edge &amp;lt;- 0&#xA;# test &amp;lt;- as.dendrogram(Tree.ultra) &#xA;## Error in ape::as.hclust.phylo(object) : the tree is not binary&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This &lt;a href=&quot;https://www.biostars.org/p/4576/&quot; rel=&quot;nofollow noreferrer&quot;&gt;thread&lt;/a&gt; seems to be the closest to answering my issues but the workflow is different and Im not sure how to address this binary issue. I have tried using as.hclust, as.hclust.phylo and hclust. &lt;/p&gt;&#xA;" OwnerUserId="982" LastEditorUserId="982" LastEditDate="2017-06-27T18:54:03.003" LastActivityDate="2017-06-28T09:23:27.727" Title="How to convert a phylogeny to a dendrogram in R" Tags="&lt;r&gt;&lt;phylogeny&gt;" AnswerCount="2" CommentCount="5" />
  <row Id="924" PostTypeId="1" AcceptedAnswerId="927" CreationDate="2017-06-27T16:55:20.077" Score="4" ViewCount="42" Body="&lt;p&gt;I have a protein of interest and I would like to now how it interacts with RNA. I have structures of both molecules.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What tool can I use?&lt;/p&gt;&#xA;" OwnerUserId="988" LastEditorUserId="77" LastEditDate="2017-06-27T21:26:30.170" LastActivityDate="2017-06-28T09:05:20.913" Title="How can I dock a protein to a nucleic acid?" Tags="&lt;proteins&gt;&lt;software-recommendation&gt;&lt;rna&gt;&lt;docking&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="925" PostTypeId="2" ParentId="923" CreationDate="2017-06-27T19:56:19.770" Score="-1" Body="&lt;p&gt;For your example to work you should replace the &lt;code&gt;rtree&lt;/code&gt; function with the &lt;code&gt;rcoal&lt;/code&gt; function, see example below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(ape)  &#xA;library(dendextend)   &#xA;Tree &amp;lt;- rtree(10, rooted=F)&#xA;is.ultrametric(Tree) &#xA;[1] FALSE &#xA;is.binary.tree(Tree) &#xA;[1] TRUE&#xA;&#xA;Tree_rcoal &amp;lt;- rcoal(10, rooted=F) &#xA;is.ultrametric(Tree_rcoal) &#xA;[1] TRUE&#xA;is.binary.tree(Tree_rcoal) &#xA;[1] TRUE&#xA;&#xA;Tree_rcoal2 &amp;lt;- rcoal(10, rooted=F)&#xA;&#xA;tanglegram(Tree_rcoal, Tree_rcoal2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To make use of a file in newick format, you can try to convert with the &lt;code&gt;chronos&lt;/code&gt; function.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;s &amp;lt;- &quot;owls(((Strix_aluco:4.2,Asio_otus:4.2):3.1,Athene_noctua:7.3):6.3,Tyto_alba:13.5);&quot;&#xA;cat(s, file = &quot;ex.tre&quot;, sep = &quot;\n&quot;)&#xA;tree.owls &amp;lt;- read.tree(&quot;ex.tre&quot;)&#xA;is.ultrametric(tree.owls)&#xA;[1] FALSE&#xA;dendrogram &amp;lt;- chronos(tree.owls)&#xA;&#xA;Setting initial dates...&#xA;Fitting in progress... get a first set of estimates&#xA;         Penalised log-lik = -24.04992 &#xA;Optimising rates... dates... -24.04992 &#xA;Optimising rates... dates... -24.04908 &#xA;&#xA;Done.&#xA;is.ultrametric(dendrogram)&#xA;[1] TRUE&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="939" LastEditorUserId="939" LastEditDate="2017-06-28T08:04:56.623" LastActivityDate="2017-06-28T08:04:56.623" CommentCount="2" />
  <row Id="926" PostTypeId="2" ParentId="765" CreationDate="2017-06-27T21:11:07.147" Score="0" Body="&lt;p&gt;Firstly, I would download some sort of editor that is built to handle nucleotide sequences. There are a handful out there, but APE is a fairly popular software designed for plasmid editing. It is much more efficient at searching for nucleotide strings as it can search for reverse complements and to a certain extent, mismatches. It's also free!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://biologylabs.utah.edu/jorgensen/wayned/ape/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://biologylabs.utah.edu/jorgensen/wayned/ape/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For your question, I'm not exactly sure what you are asking, but I would use the BLAST alignment tool to try to see similarity between your RNAi sequences and the target mRNA. You input fasta files and the program searches for any possible overlap between the two inputs. The higher the &quot;max-score&quot; the more likely the two strands (or subsections of strands) have high similarity. This program also considers the complement&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&amp;amp;PAGE_TYPE=BlastSearch&amp;amp;LINK_LOC=blasthome&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&amp;amp;PAGE_TYPE=BlastSearch&amp;amp;LINK_LOC=blasthome&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can use the results from BLAST to annotate in APE using different colors which is extremely helpful. You can also use BLAST in the user interface of APE. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also keep in mind RNAi's are not always effective, even though they are predicted to bind at a given location, that may not be a biological reality &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Good luck! &lt;/p&gt;&#xA;" OwnerUserId="470" LastActivityDate="2017-06-27T21:11:07.147" CommentCount="0" />
  <row Id="927" PostTypeId="2" ParentId="924" CreationDate="2017-06-27T23:24:38.537" Score="2" Body="&lt;p&gt;I'm not sure what you meant but you can take a look at &lt;a href=&quot;http://genesilico.pl/NPDock&quot; rel=&quot;nofollow noreferrer&quot;&gt;NPDock&lt;/a&gt; (disclaimer we wrote that tool). If you have a structure of your protein of interest, you can dock it to the structure of your DNA/RNA of interest. Mind that this is a rigid body docking which means that the structure will not change upon binding. &lt;/p&gt;&#xA;" OwnerUserId="640" LastEditorUserId="48" LastEditDate="2017-06-28T09:05:20.913" LastActivityDate="2017-06-28T09:05:20.913" CommentCount="1" />
  <row Id="928" PostTypeId="1" AcceptedAnswerId="929" CreationDate="2017-06-28T05:28:39.973" Score="4" ViewCount="58" Body="&lt;p&gt;I have a distance matrix generated by hierfstat, thusly, with a link to the fasta file &lt;a href=&quot;https://www.dropbox.com/s/z9n89y080hvl3s2/2017.06.13.strucutre_test.masked.snp_sites.v2_3_2.subsample.fa?dl=0&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(adegenet)&#xA;library(hierfstat)&#xA;snps &amp;lt;- fasta2DNAbin('test.fa', chunkSize = 50)&#xA;gi &amp;lt;- DNAbin2genind(snps)&#xA;# manually define the populations for passing to pairwise.fst&#xA;p &amp;lt;- c('Botswana', 'Botswana', 'Botswana', 'Botswana', 'France', 'France', 'Vietnam', 'Vietnam', 'Uganda', 'Uganda', 'Uganda', 'Uganda', 'Vietnam', 'Vietnam', 'Laos', 'Laos', 'Laos', 'Vietnam', 'Vietnam', 'Vietnam', 'Vietnam', 'Vietnam')&#xA;f &amp;lt;- pairwise.fst(gi, p, res.type = c('dist', 'matrix'))&#xA;as.matrix(f)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which produces&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    1   2   3   4   5&#xA;1   0   0.2189008   0.225567409 0.1821518   0.259409722&#xA;2   0.2189008   0   0.130736953 0.1648034   0.191050772&#xA;3   0.2255674   0.130737    0   0.1669077   0.006396789&#xA;4   0.1821518   0.1648034   0.166907683 0   0.203931457&#xA;5   0.2594097   0.1910508   0.006396789 0.2039315   0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is it safe to assume that 1 = Botswana, 2 = France, 3 = Vietnam, 4 = Uganda and 5 = Laos? i.e. that the distance matrix rows/columns follow the order that each population first appeared in &lt;code&gt;p&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way to determine for certain, which row/column in the distance matrix corresponds to which population?&lt;/p&gt;&#xA;" OwnerUserId="993" LastEditorUserId="993" LastEditDate="2017-06-28T08:04:01.480" LastActivityDate="2017-06-28T08:23:44.230" Title="What are the labels in my rstats distance matrix?" Tags="&lt;r&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="0" />
  <row Id="929" PostTypeId="2" ParentId="928" CreationDate="2017-06-28T08:23:44.230" Score="1" Body="&lt;p&gt;Disclaimer: I don't have adagenet and hierfstat installed, I just looked at the source code. It seems like pairwise.fst (the deprecated code from the adagenet package, which you seem to be using) already returns a matrix. Set res.type=&quot;matrix&quot; and try calling &lt;code&gt;rownames(f)&lt;/code&gt; and &lt;code&gt;colnames(f)&lt;/code&gt; afterwards.&lt;/p&gt;&#xA;" OwnerUserId="787" LastActivityDate="2017-06-28T08:23:44.230" CommentCount="2" />
  <row Id="930" PostTypeId="2" ParentId="923" CreationDate="2017-06-28T09:23:27.727" Score="0" Body="&lt;p&gt;After searching high and low I have found an answer from this &lt;a href=&quot;https://stackoverflow.com/questions/7445684/how-to-convert-a-tree-to-a-dendrogram-in-r&quot;&gt;thread&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Workflow goes like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(DECIPHER)&#xA;dend1 &amp;lt;- ReadDendrogram(file=&quot;clipboard&quot;)   &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note the newick file cannot be an existing object in the R environment. Its must be 'read in'. Available in &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/DECIPHER.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Bioconductor&lt;/a&gt;.&lt;br&gt;&#xA;This object can then be used in the dendextend package and tanglegram function. &lt;/p&gt;&#xA;" OwnerUserId="982" LastActivityDate="2017-06-28T09:23:27.727" CommentCount="5" />
  <row Id="931" PostTypeId="2" ParentId="920" CreationDate="2017-06-28T11:19:21.197" Score="1" Body="&lt;p&gt;Thanks everyone for the suggestions @b.nota answer is useful to colour according to clade groups but does not address my actual question on using metadata to colour the lines. For this answer see below:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;dendextend states for &lt;code&gt;color_lines&lt;/code&gt;&#xA;a vector of colors for the lines connected the labels. If the colors are shorter than the number of labels, they are recycled (and a warning is issued). The colors in the vector are applied on the lines from the bottom up.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# Extract labels from dendrogram on the left&#xA;labels &amp;lt;- dendA %&amp;gt;% set(&quot;labels_to_char&quot;) %&amp;gt;% labels &#xA;&#xA;#Using a metadata table with colours create a vector of colours&#xA;labels &amp;lt;- as.data.frame(labels)&#xA;labels2 &amp;lt;- merge(labels, metadata, by.x=&quot;labels&quot;, by.y=&quot;Sample.name&quot;, sort=F)&#xA;cols &amp;lt;- as.character(labels2$Colours) &#xA;&#xA;# Make tanglegram&#xA;tanglegram(dendA, dendC, color_lines = cols)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Side note if anyone with a phylogeny needs to convert it to a dendrogram to try this on their data see &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/923/how-to-convert-a-phylogeny-to-a-dendrogram-in-r/930?noredirect=1#comment1714_930&quot;&gt;my other question&lt;/a&gt;. &lt;/p&gt;&#xA;" OwnerUserId="982" LastEditorUserId="982" LastEditDate="2017-06-29T15:48:38.730" LastActivityDate="2017-06-29T15:48:38.730" CommentCount="3" />
  <row Id="932" PostTypeId="2" ParentId="920" CreationDate="2017-06-28T12:40:25.230" Score="1" Body="&lt;p&gt;To get your colors in the original order, you can make a dataframe with the labels in the right order with your metadata. You can use &lt;code&gt;merge&lt;/code&gt; for this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So from your example code it would be something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;site &amp;lt;- structure(list(name = structure(c(1L, 3L, 4L, 5L, 6L, 7L, 8L,9L, 10L, 2L), .Label = c(&quot;t1&quot;, &quot;t10&quot;, &quot;t2&quot;, &quot;t3&quot;, &quot;t4&quot;, &quot;t5&quot;,&quot;t6&quot;, &quot;t7&quot;, &quot;t8&quot;, &quot;t9&quot;), class = &quot;factor&quot;), site = c(1L, 1L,1L, 2L, 2L, 3L, 1L, 3L, 2L, 2L)), .Names = c(&quot;name&quot;, &quot;site&quot;), row.names = c(NA,10L), class = &quot;data.frame&quot;) &#xA;&#xA;library(ape)&#xA;library(dendextend)&#xA;&#xA;t1 &amp;lt;- rcoal(10)&#xA;t2 &amp;lt;- rcoal(10)&#xA;&#xA;str(site)&#xA;# The name are factors now, so make chr first&#xA;site$name &amp;lt;- as.character(site$name)&#xA;&#xA;# make a data.frame of your labels&#xA;labels_df &amp;lt;- data.frame(t1$tip.label)&#xA;&#xA;#merge the 2 data.frames in the right order (hence sort=F)&#xA;colors &amp;lt;- merge(labels_df,site,by.x=&quot;t1.tip.label&quot;, by.y=&quot;name&quot;,all.x=T, all.y=F,sort=F)&#xA;&#xA;tanglegram(t1, t2, color_lines=colors$site)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="939" LastActivityDate="2017-06-28T12:40:25.230" CommentCount="1" />
  <row Id="933" PostTypeId="1" AcceptedAnswerId="934" CreationDate="2017-06-28T12:53:59.550" Score="6" ViewCount="87" Body="&lt;p&gt;I currently find Harvard's RESTful API for &lt;a href=&quot;http://exac.hms.harvard.edu/&quot; rel=&quot;noreferrer&quot;&gt;ExAC&lt;/a&gt; extremely useful and I was hoping that a similar resource is available for &lt;a href=&quot;http://gnomad.broadinstitute.org/&quot; rel=&quot;noreferrer&quot;&gt;Gnomad&lt;/a&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know of a public access API for Gnomad or possibly any plans to integrate Gnomad into the Harvard API?&lt;/p&gt;&#xA;" OwnerUserId="195" LastEditorUserId="77" LastEditDate="2017-06-28T13:06:05.817" LastActivityDate="2017-06-28T14:20:08.960" Title="Is there public RESTful api for Gnomad?" Tags="&lt;variants&gt;&lt;gnomad&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="934" PostTypeId="2" ParentId="933" CreationDate="2017-06-28T13:04:58.663" Score="8" Body="&lt;p&gt;As far as I know, no but the vcf.gz files are behind a http server that supports &lt;em&gt;Byte-Range&lt;/em&gt;, so you can use &lt;code&gt;tabix&lt;/code&gt; or any related API:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ tabix &quot;https://storage.googleapis.com/gnomad-public/release-170228/vcf/exomes/gnomad.exomes.r2.0.1.sites.vcf.gz&quot; &quot;22:17265182-17265182&quot;&#xA;22  17265182    .   A   T   762.04  PASS    AC=1;AF=4.78057e-06;AN=209180;BaseQRankSum=-4.59400e+00;ClippingRankSum=2.18000e+00;DP=4906893;FS=1.00270e+01;InbreedingCoeff=4.40000e-03;MQ=3.15200e+01;MQRankSum=1.40000e+00;QD=1.31400e+01;ReadPosRankSum=2.23000e-01;SOR=9.90000e-02;VQSLOD=-5.12800e+00;VQSR_culprit=MQ;GQ_HIST_ALT=0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1;DP_HIST_ALT=0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0;AB_HIST_ALT=0|0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0;GQ_HIST_ALL=1591|589|120|301|650|589|1854|2745|1815|4297|5061|2921|10164|1008|6489|1560|7017|457|6143|52950;DP_HIST_ALL=2249|1418|6081|11707|16538|9514|28624|23829|7391|853|95|19|1|0|0|1|0|1|0|0;AB_HIST_ALL=0|0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0;AC_AFR=0;AC_AMR=0;AC_ASJ=0;AC_EAS=0;AC_FIN=1;AC_NFE=0;AC_OTH=0;AC_SAS=0;AC_Male=1;AC_Female=0;AN_AFR=11994;AN_AMR=31324;AN_ASJ=7806;AN_EAS=13112;AN_FIN=20076;AN_NFE=94516;AN_OTH=4656;AN_SAS=25696;AN_Male=114366;AN_Female=94814;AF_AFR=0.00000e+00;AF_AMR=0.00000e+00;AF_ASJ=0.00000e+00;AF_EAS=0.00000e+00;AF_FIN=4.98107e-05;AF_NFE=0.00000e+00;AF_OTH=0.00000e+00;AF_SAS=0.00000e+00;AF_Male=8.74386e-06;AF_Female=0.00000e+00;GC_AFR=5997,0,0;GC_AMR=15662,0,0;GC_ASJ=3903,0,0;GC_EAS=6556,0,0;GC_FIN=10037,1,0;GC_NFE=47258,0,0;GC_OTH=2328,0,0;GC_SAS=12848,0,0;GC_Male=57182,1,0;GC_Female=47407,0,0;AC_raw=1;AN_raw=216642;AF_raw=4.61591e-06;GC_raw=108320,1,0;GC=104589,1,0;Hom_AFR=0;Hom_AMR=0;Hom_ASJ=0;Hom_EAS=0;Hom_FIN=0;Hom_NFE=0;Hom_OTH=0;Hom_SAS=0;Hom_Male=0;Hom_Female=0;Hom_raw=0;Hom=0;POPMAX=FIN;AC_POPMAX=1;AN_POPMAX=20076;AF_POPMAX=4.98107e-05;DP_MEDIAN=58;DREF_MEDIAN=5.01187e-84;GQ_MEDIAN=99;AB_MEDIAN=6.03448e-01;AS_RF=9.18451e-01;AS_FilterStatus=PASS;CSQ=T|missense_variant|MODERATE|XKR3|ENSG00000172967|Transcript|ENST00000331428|protein_coding|4/4||ENST00000331428.5:c.707T&amp;gt;A|ENSP00000331704.5:p.Phe236Tyr|810|707|236|F/Y|tTc/tAc||1||-1||SNV|1|HGNC|28778|YES|||CCDS42975.1|ENSP00000331704|Q5GH77||UPI000013EFAE||deleterious(0)|benign(0.055)|hmmpanther:PTHR14297&amp;amp;hmmpanther:PTHR14297:SF7&amp;amp;Pfam_domain:PF09815||||||||||||||||||||||||||||||,T|regulatory_region_variant|MODIFIER|||RegulatoryFeature|ENSR00000672806|TF_binding_site|||||||||||1||||SNV|1||||||||||||||||||||||||||||||||||||||||||||,T|regulatory_region_variant|MODIFIER|||RegulatoryFeature|ENSR00001729562|CTCF_binding_site|||||||||||1||||SNV|1||||||||||||||||||||||||||||||||||||||||||||&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="71" LastEditorUserId="298" LastEditDate="2017-06-28T14:20:08.960" LastActivityDate="2017-06-28T14:20:08.960" CommentCount="0" />
  <row Id="935" PostTypeId="1" AcceptedAnswerId="937" CreationDate="2017-06-28T14:50:27.330" Score="8" ViewCount="203" Body="&lt;p&gt;I am looking for a tool, preferably written in C or C++, that can quickly and efficiently count the number of reads and the number of bases in a compressed fastq file. I am currently doing this using &lt;code&gt;zgrep&lt;/code&gt; and &lt;code&gt;awk&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;zgrep . foo.fasq.gz |&#xA;     awk 'NR%4==2{c++; l+=length($0)}&#xA;          END{&#xA;                print &quot;Number of reads: &quot;c; &#xA;                print &quot;Number of bases in reads: &quot;l&#xA;              }'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;zgrep .&lt;/code&gt; will print non-blank lines from the input file and the &lt;code&gt;awk 'NR%4==2&lt;/code&gt; will process every 4th line starting with the second (the sequence). &#xA;This works fine, but can take a &lt;em&gt;very&lt;/em&gt; long time when dealing with large files such as WGS data. Is there a tool I can use (on Linux) that will give me these values? Or, if not, I'm also open to suggestions for speeding up the above command.&lt;/p&gt;&#xA;" OwnerUserId="298" LastEditorUserId="73" LastEditDate="2017-06-29T23:50:12.637" LastActivityDate="2017-07-26T17:47:55.083" Title="Fast way to count number of reads and number of bases in a fastq file?" Tags="&lt;ngs&gt;&lt;fastq&gt;&lt;software-recommendation&gt;&lt;benchmarking&gt;" AnswerCount="7" CommentCount="14" />
  <row Id="936" PostTypeId="2" ParentId="935" CreationDate="2017-06-28T15:22:24.697" Score="5" Body="&lt;p&gt;The following is more than twice as fast; however, &lt;code&gt;wc&lt;/code&gt; counts newline characters as well. We thus need to subtract the line count from the base count:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fix_base_count() {&#xA;    local counts=($(cat))&#xA;    echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xA;}&#xA;&#xA;gunzip -c &quot;$file&quot; \&#xA;    | awk 'NR % 4 == 2' \&#xA;    | wc -cl \&#xA;    | fix_base_count&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, the character counts newline characters as well. We thus need to subtract the line count from it:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All the caveats from Simon’s comment apply: this assumes the “simple” FASTQ format, where each record consists of exactly four lines. I think this is true for all files produced by Illumina sequencers and downstream tools.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-06-28T15:22:24.697" CommentCount="3" />
  <row Id="937" PostTypeId="2" ParentId="935" CreationDate="2017-06-28T16:15:06.267" Score="11" Body="&lt;p&gt;It's difficult to get this to go massively quicker I think - as with &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/361/what-is-the-fastest-way-to-calculate-the-number-of-unknown-nucleotides-in-fasta&quot;&gt;this question&lt;/a&gt; working with large gzipped FASTQ files is mostly IO-bound. We could instead focus on making sure we are getting the &lt;em&gt;right&lt;/em&gt; answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;People deride them too often, but this is where a well-written parser is worth it's weight in gold. Heng Li gives us this &lt;a href=&quot;http://lh3lh3.users.sourceforge.net/parsefastq.shtml&quot; rel=&quot;noreferrer&quot;&gt;FASTQ Parser in C&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I downloaded the &lt;a href=&quot;http://lh3lh3.users.sourceforge.net/download/kseq.tar.bz2&quot; rel=&quot;noreferrer&quot;&gt;example tarball&lt;/a&gt; and modified the example code (excuse my C...):&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-c prettyprint-override&quot;&gt;&lt;code&gt;#include &amp;lt;zlib.h&amp;gt;&#xA;#include &amp;lt;stdio.h&amp;gt;&#xA;#include &quot;kseq.h&quot;&#xA;KSEQ_INIT(gzFile, gzread)&#xA;&#xA;int main(int argc, char *argv[])&#xA;{&#xA;    gzFile fp;&#xA;    kseq_t *seq;&#xA;    int l;&#xA;    if (argc == 1) {&#xA;        fprintf(stderr, &quot;Usage: %s &amp;lt;in.seq&amp;gt;\n&quot;, argv[0]);&#xA;        return 1;&#xA;    }&#xA;    fp = gzopen(argv[1], &quot;r&quot;);&#xA;    seq = kseq_init(fp);&#xA;    int seqcount = 0;&#xA;    long seqlen = 0;&#xA;    while ((l = kseq_read(seq)) &amp;gt;= 0) {&#xA;        seqcount = seqcount + 1;&#xA;        seqlen = seqlen + (long)strlen(seq-&amp;gt;seq.s);&#xA;    }&#xA;    kseq_destroy(seq);&#xA;    gzclose(fp);&#xA;    printf(&quot;Number of sequences: %d\n&quot;, seqcount);&#xA;    printf(&quot;Number of bases in sequences: %ld\n&quot;, seqlen);&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then &lt;code&gt;make&lt;/code&gt; and &lt;code&gt;kseq_test foo.fastq.gz&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For my example file (~35m reads of ~75bp) this took:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;real    0m49.670s&#xA;user    0m49.364s&#xA;sys     0m0.304s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Compared with your example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;real    0m43.616s&#xA;user    1m35.060s&#xA;sys     0m5.240s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Konrad's solution (in my hands):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;real    0m39.682s&#xA;user    1m11.900s&#xA;sys     0m5.112s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(By the way, just zcat-ing the data file to /dev/null):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;real    0m38.736s&#xA;user    0m38.356s&#xA;sys     0m0.308s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, I get pretty close in speed, but am likely to be more standards compliant. Also this solution gives you more flexibility with what you can do with the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And my horrible C can almost certainly be optimised.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Same test, with &lt;code&gt;kseq.h&lt;/code&gt; from Github, as suggested in the comments:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My machine is under different load this morning, so I've retested. Wall clock times:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;OP: 0m44.813s&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Konrad: 0m40.061s&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;zcat &amp;gt; /dev/null&lt;/code&gt;: 0m34.508s&lt;/p&gt;&#xA;&#xA;&lt;p&gt;kseq.h (Github): 0m32.909s&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So most recent version of &lt;code&gt;kseq.h&lt;/code&gt; is faster than simply zcat-ing the file (consistently in my tests...).&lt;/p&gt;&#xA;" OwnerUserId="194" LastEditorUserId="298" LastEditDate="2017-06-29T09:28:15.997" LastActivityDate="2017-06-29T09:28:15.997" CommentCount="5" />
  <row Id="938" PostTypeId="2" ParentId="895" CreationDate="2017-06-28T16:15:13.173" Score="1" Body="&lt;p&gt;You can indeed get this information from the UCSC Table Browser. Select knownGene as your primary table, make a filter, add knownCanonical to as a linked table to filter on, then in the free-form query section add &quot;1&quot; without the quotes. Then click submit and select Bed output, where you can choose &quot;exons plus&quot; as an output option. This will lead to output like the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;chr1    17368   17436   uc031tla.1_exon_0_0_chr1_17369_r    0   -&#xA;chr1    29553   30039   uc057aty.1_exon_0_0_chr1_29554_f    0   +&#xA;chr1    30563   30667   uc057aty.1_exon_1_0_chr1_30564_f    0   +&#xA;chr1    30975   31097   uc057aty.1_exon_2_0_chr1_30976_f    0   +&#xA;chr1    30365   30503   uc031tlb.1_exon_0_0_chr1_30366_f    0   +&#xA;chr1    34553   35174   uc001aak.4_exon_0_0_chr1_34554_r    0   -&#xA;chr1    35276   35481   uc001aak.4_exon_1_0_chr1_35277_r    0   -&#xA;chr1    35720   36081   uc001aak.4_exon_2_0_chr1_35721_r    0   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For more step-by-step information, please check the answer from the mailing list archives here (in particular the Exon Method 1 section):&#xA;&lt;a href=&quot;https://groups.google.com/a/soe.ucsc.edu/d/msg/genome/BJ-6DlaZNCY/grgGIpuJAwAJ&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://groups.google.com/a/soe.ucsc.edu/d/msg/genome/BJ-6DlaZNCY/grgGIpuJAwAJ&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also note that UCSC provides several mailing lists for support, if you have futher questions please send them there:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;General questions: genome@soe.ucsc.edu &lt;/li&gt;&#xA;&lt;li&gt;Questions involving private&#xA;data: genome-www@soe.ucsc.edu &lt;/li&gt;&#xA;&lt;li&gt;Questions involving mirror sites:&#xA;genome-mirror@ose.ucsc.edu&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="1000" LastActivityDate="2017-06-28T16:15:13.173" CommentCount="0" />
  <row Id="939" PostTypeId="1" CreationDate="2017-06-28T17:56:42.243" Score="4" ViewCount="40" Body="&lt;p&gt;I have a matrix of gene counts which I'm going to use as input for DESeq.  Right now, each gene is labeled by its Ensemble transcript ID, but I'd like to convert these to their HGNC symbols before I input them into DESeq for analysis. I'm attempting to do this conversion using biomaRt, however, this is my first time using the program, and I'm running into issues.  Here is my current code:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;library(&quot;biomaRt&quot;)&#xA;mart &amp;lt;- useMart(biomart = &quot;ENSEMBL_MART_ENSEMBL&quot;, dataset = &quot;mmusculus_gene_ensembl&quot;)&#xA;transcript.ids &amp;lt;- rownames(txi.kallisto$counts)&#xA;hgnc_symbols &amp;lt;- getBM(attributes = &quot;hgnc_symbol&quot;, filters = &quot;ensembl_transcript_id&quot;, values = transcript.ids, mart = mart)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Printing hgnc_symbols shows that it isn't what I want:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt;ghnc_symbols&#xA;[1] hgnc_symbol&#xA;&amp;lt;0 rows&amp;gt; (or 0-length row.names)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Lastly, here is what the input transcript IDs look like:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; head(rownames(txi.kallisto$counts))&#xA;[1] &quot;ENSMUST00000178862.1&quot; &quot;ENSMUST00000178537.1&quot; &quot;ENSMUST00000196221.1&quot;&#xA;[4] &quot;ENSMUST00000179664.1&quot; &quot;ENSMUST00000177564.1&quot; &quot;ENSMUST00000179520.1&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="506" LastEditorUserId="298" LastEditDate="2017-06-28T21:40:49.913" LastActivityDate="2017-06-29T07:18:05.463" Title="Trouble using biomaRt to retrieve hgnc symbols from Ensembl transcript ids" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;deseq2&gt;&lt;biomart&gt;&lt;ensembl&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="940" PostTypeId="2" ParentId="935" CreationDate="2017-06-28T20:00:12.073" Score="4" Body="&lt;p&gt;I get fairly quick results with my &lt;a href=&quot;https://github.com/gringer/bioinfscripts/blob/master/fastx-fetch.pl&quot; rel=&quot;nofollow noreferrer&quot;&gt;fastx-length.pl&lt;/a&gt; script, with the added bonus of being able to handle multi-line FASTQ files and displaying additional read-length QC statistics:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time zcat albacored_all.fastq.gz | /bioinf/scripts/fastx-length.pl &amp;gt; /dev/null&#xA;Total sequences: 301135&#xA;Total length: 283.902419 Mb&#xA;Longest sequence: 5.601 kb&#xA;Shortest sequence: 6 b&#xA;Mean Length: 942 b&#xA;Median Length: 999 b&#xA;N50: 111835 sequences; L50: 1.103 kb&#xA;N90: 245243 sequences; L90: 608 b&#xA;&#xA;real    0m8,802s&#xA;user    0m16,584s&#xA;sys 0m0,260s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Versus the script you have provided:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;zcat albacored_all.fastq.gz | awk 'NR%4==2{c++; l+=length($0)}&#xA;          END{&#xA;                print &quot;Number of reads: &quot;c; &#xA;                print &quot;Number of bases in reads: &quot;l&#xA;              }'&#xA;Number of reads: 301135&#xA;Number of bases in reads: 283902419&#xA;&#xA;real    0m8,382s&#xA;user    0m10,216s&#xA;sys 0m0,332s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cat to &lt;code&gt;/dev/null&lt;/code&gt; for comparison:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time zcat albacored_all.fastq.gz &amp;gt; /dev/null&#xA;&#xA;real    0m7,877s&#xA;user    0m7,856s&#xA;sys 0m0,020s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I suspect that something using &lt;a href=&quot;https://github.com/lh3/bioawk&quot; rel=&quot;nofollow noreferrer&quot;&gt;bioawk&lt;/a&gt; might be a bit faster (and similarly FASTQ-compliant).&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-28T20:00:12.073" CommentCount="1" />
  <row Id="941" PostTypeId="2" ParentId="939" CreationDate="2017-06-28T21:16:04.060" Score="6" Body="&lt;p&gt;You need to specify the number without the version. Instead of &quot;ENSMUST00000178862.1&quot; just &quot;ENSMUST00000178862&quot;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can do this with one more line:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;g &amp;lt;- gsub(&quot;\\..*&quot;, &quot;&quot;, rownames(txi.kallisto$counts))&#xA;(hgnc_symbols &amp;lt;- getBM(attributes = c(&quot;hgnc_symbol&quot;, &quot;chromosome_name&quot;, &quot;ensembl_transcript_id&quot;), filters = &quot;ensembl_transcript_id&quot;, values = g, mart = mart))&#xA;##  hgnc_symbol chromosome_name ensembl_transcript_id&#xA;##1          NA              14    ENSMUST00000177564&#xA;##2          NA               6    ENSMUST00000178537&#xA;##3          NA               6    ENSMUST00000178862&#xA;##4          NA              12    ENSMUST00000179520&#xA;##5          NA              14    ENSMUST00000179664&#xA;##6          NA              14    ENSMUST00000196221&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With these transcripts it doesn't seem that there is an equivalent hgnc_symbol. But is recognized, otherwise they are not recognized:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;(hgnc_symbols &amp;lt;- getBM(attributes = c(&quot;hgnc_symbol&quot;, &quot;chromosome_name&quot;, &quot;ensembl_transcript_id&quot;), filters = &quot;ensembl_transcript_id&quot;, values = rownames(txi.kallisto$counts), mart = mart))&#xA;## [1] hgnc_symbol           chromosome_name       ensembl_transcript_id&#xA;##&amp;lt;0 rows&amp;gt; (or 0-length row.names)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However you might be interested in doing something like &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/vignettes/biomaRt/inst/doc/biomaRt.html#given-the-human-gene-tp53-retrieve-the-human-chromosomal-location-of-this-gene-and-also-retrieve-the-chromosomal-location-and-refseq-id-of-its-homolog-in-mouse.&quot; rel=&quot;nofollow noreferrer&quot;&gt;this&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-R prettyprint-override&quot;&gt;&lt;code&gt;human = useMart(&quot;ensembl&quot;, dataset = &quot;hsapiens_gene_ensembl&quot;)&#xA;mouse = useMart(&quot;ensembl&quot;, dataset = &quot;mmusculus_gene_ensembl&quot;)&#xA;getLDS(attributes = &quot;ensembl_transcript_id&quot;,&#xA;       filters = &quot;ensembl_transcript_id&quot;, values = g,mart = mouse,&#xA;      attributesL = &quot;hgnc_symbol&quot;, martL = human)&#xA;## The query to the BioMart webservice returned an invalid result: the number of &#xA;## columns in the result table does not equal the number of &#xA;## attributes in the query. Please report this to the mailing list.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2017-06-29T07:18:05.463" LastActivityDate="2017-06-29T07:18:05.463" CommentCount="0" />
  <row Id="942" PostTypeId="1" CreationDate="2017-06-29T08:42:12.243" Score="3" ViewCount="44" Body="&lt;p&gt;I have done de novo assembly of pair end raw read sequences and resulted transcripts sequences were separated based on coding potential into two categories: long non-coding RNA transcripts and coding RNA transcripts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to find how many of long non coding can make hairpin loop structure and how can I annotate these miRNA which are generated from long non coding RNA in plants.&lt;/p&gt;&#xA;" OwnerUserId="1008" LastEditorUserId="37" LastEditDate="2017-06-30T12:04:12.420" LastActivityDate="2017-06-30T12:04:12.420" Title="How to find hairpin loop structure in a large set of long non coding RNA transcripts" Tags="&lt;bioconductor&gt;&lt;rna-structure&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="2" />
  <row Id="943" PostTypeId="1" AcceptedAnswerId="946" CreationDate="2017-06-29T08:58:04.413" Score="4" ViewCount="189" Body="&lt;p&gt;I have the following mwe for filtering a Swissprot file based on a certain feature, in this case, transmembrane regions. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from Bio import SeqIO&#xA;records=[]&#xA;for record in SeqIO.parse(&quot;Input.txt&quot;, &quot;swiss&quot;):&#xA;    transmembrane_protein=False&#xA;    print record.id&#xA;    for i, feature in enumerate(record.features):&#xA;        if feature.type == &quot;TRANSMEM&quot;:&#xA;            transmembrane_protein=True&#xA;    if transmembrane_protein==True:&#xA;        records.append(record)&#xA;SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The script works when &lt;code&gt;SeqIO.write(records, &quot;Output.txt&quot;, &quot;swiss&quot;)&lt;/code&gt; becomes &lt;code&gt;SeqIO.write(records, &quot;Output.txt&quot;, &quot;fasta&quot;)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, such a method is not yet supported.  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Reading format 'swiss' is supported, but not writing&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;From the docs, I see that &lt;a href=&quot;http://biopython.org/DIST/docs/api/Bio.SeqIO-module.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;writing is not supported for swiss&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Note that while Bio.SeqIO can read all the above file formats, it cannot write to all of them.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Is there any unofficial way of using biopython/python to write swissprot files from parsed swissprot files? &lt;/p&gt;&#xA;" OwnerUserId="747" LastEditorUserId="747" LastEditDate="2017-06-30T05:31:46.470" LastActivityDate="2017-06-30T05:31:46.470" Title="Is there any way of using biopython to write Swissprot files?" Tags="&lt;database&gt;&lt;biopython&gt;&lt;python&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="944" PostTypeId="2" ParentId="935" CreationDate="2017-06-29T09:05:43.780" Score="1" Body="&lt;p&gt;I hava implemented &lt;a href=&quot;https://github.com/jameslz/seqtk_utils/blob/master/seqtk_counts&quot; rel=&quot;nofollow noreferrer&quot;&gt;seqtk_counts&lt;/a&gt; using kseq.h from  &lt;a href=&quot;https://github.com/attractivechaos/klib&quot; rel=&quot;nofollow noreferrer&quot;&gt;klib&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just a few line of Codes:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;&#xA;#include &amp;lt;zlib.h&amp;gt;&#xA;#include &quot;kseq.h&quot;&#xA;&#xA;KSEQ_INIT(gzFile, gzread)&#xA;&#xA;int main(int argc, char *argv[]){&#xA;&#xA;gzFile fp;&#xA;kseq_t *seq;&#xA;&#xA;int l = 0;&#xA;&#xA;int64_t total = 0;&#xA;int64_t lines = 0;&#xA;&#xA;if (argc == 1) {&#xA;    fprintf(stderr, &quot;Usage: %s &amp;lt;fastq&amp;gt; &amp;lt;sample&amp;gt;\n&quot;, argv[0]);&#xA;&#xA;    return 1;&#xA;}&#xA;&#xA;fp = strcmp(argv[1], &quot;-&quot;)? gzopen(argv[1], &quot;r&quot;) : gzdopen(fileno(stdin), &quot;r&quot;);&#xA;seq = kseq_init(fp);&#xA;&#xA;while ((l = kseq_read(seq)) &amp;gt;= 0){&#xA;    total += seq-&amp;gt;seq.l;&#xA;    lines += 1;&#xA;}&#xA;&#xA;printf(&quot;%s\t%lld\t%lld\n&quot;, argv[2] ,(long long)lines, (long long)total);&#xA;kseq_destroy(seq);&#xA;gzclose(fp);&#xA;return 0;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Compile it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gcc  -O2  seqtk_counts.c  -o  seqtk_counts  -Iklib  -lz&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Usage:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;seqtk_counts foo.fasq.gz foo &#xA;or&#xA;cat foo.fasq.gz | seqtk_counts  - foo&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="652" LastEditorUserId="652" LastEditDate="2017-06-29T09:33:47.703" LastActivityDate="2017-06-29T09:33:47.703" CommentCount="1" />
  <row Id="945" PostTypeId="1" CreationDate="2017-06-29T10:21:41.013" Score="5" ViewCount="121" Body="&lt;p&gt;I'm working on a human genome project, trying to find variants that are unique to one person, but not to three other relatives of that person. Unfortunately, one of our regions of interest contains a long tandem repeat region (&lt;strong&gt;&lt;em&gt;30kb&lt;/em&gt;&lt;/strong&gt;, repeated 3 times in tandem). I'd like to be able to properly map these regions with WGS reads, but suspect that I'm coming up against a mapping preference problem with my current methods (Bowtie2 / HISAT2):&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Read A1 can potentially map to three different locations near the same locus in the genome (L1, L2, L3)&lt;/li&gt;&#xA;&lt;li&gt;L1 is most similar to A1, so is preferentially mapped. Because this similarity is greater than that of L2 or L3, the read is treated as a unique mapping&lt;/li&gt;&#xA;&lt;li&gt;Most other reads are similarly mapped to L1, possibly because it is the locus that has had the most correction applied to it in the reference genome&lt;/li&gt;&#xA;&lt;li&gt;An analysis of the mapping results indicates huge deletions at L2 and L3&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Is BWA any more resistant to this issue? If so, how can I get it to report randomly one of the three mappable loci (assuming they all appear in the results)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any other ways I can deal with this issue of distributing mapped reads throughout tandem repeats?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update: here's what this region looks like in the 1000 genomes GBR population, using the low-coverage CRAM files:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/WA4Fk.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/WA4Fk.png&quot; alt=&quot;Common deletion in GBR on Chr20&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And here is what the same region looks like when I do a self-mapping with LAST. The long repetitive chunks are very obvious:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/UTN78.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/UTN78.png&quot; alt=&quot;Self-mapping&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-07-05T23:16:42.950" LastActivityDate="2017-07-05T23:16:42.950" Title="How can I map short reads to large tandem repeat regions?" Tags="&lt;read-mapping&gt;&lt;bwa&gt;&lt;repeat-elements&gt;&lt;bowtie2&gt;" AnswerCount="0" CommentCount="18" />
  <row Id="946" PostTypeId="2" ParentId="943" CreationDate="2017-06-29T11:14:06.497" Score="5" Body="&lt;p&gt;Using &lt;code&gt;SeqIO.index&lt;/code&gt; rather than &lt;code&gt;SeqIO.parse&lt;/code&gt; lets you read all the records into a &lt;code&gt;dict&lt;/code&gt;, from which you can then extract the raw entry:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from Bio import SeqIO&#xA;&#xA;record_dict = SeqIO.index('Input.txt', 'swiss')&#xA;for key in record_dict:&#xA;    print(record_dict.get_raw(key).decode())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now you should be able to apply your test for a transmembrane protein to each entry, and write out only the ones you want to keep.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from Bio import SeqIO&#xA;&#xA;record_dict = SeqIO.index('Input.txt', 'swiss')&#xA;out = open('Output.txt', 'wb')&#xA;for key in record_dict:&#xA;    record = record_dict[key]&#xA;    for i, feature in enumerate(record.features):&#xA;    if feature.type == &quot;TRANSMEM&quot;:&#xA;        out.write(record_dict.get_raw(key).decode())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="194" LastActivityDate="2017-06-29T11:14:06.497" CommentCount="1" />
  <row Id="947" PostTypeId="2" ParentId="943" CreationDate="2017-06-29T11:26:35.603" Score="2" Body="&lt;p&gt;I don't know how to do this with biopython, but it's simple enough in Perl using &lt;a href=&quot;http://swissknife.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Swisskinfe&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-perl prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/env perl&#xA;use strict;&#xA;use warnings;&#xA;use SWISS::Entry;&#xA;&#xA;my $flat=$ARGV[0]||die(&quot;Need a flat file\n&quot;);&#xA;&#xA;## Change the line termination string so we read an &#xA;## entire entry at a time&#xA;local $/ = &quot;\n//\n&quot;;&#xA;&#xA;my $fh;&#xA;## Deal with compressed or uncompressed files&#xA;if($flat=~/\.gz$/){&#xA;    open($fh,&quot;zcat $flat |&quot;)|| die(&quot;cannot open flat file $flat : $! \n&quot;);&#xA;}&#xA;else{&#xA;    open($fh,&quot;$flat&quot;)|| die(&quot;cannot open flat file $flat : $!: $@\n&quot;);&#xA;}&#xA;## Parse the file. &#xA;entry: while (my $current_entry = &amp;lt;$fh&amp;gt;) {&#xA;    ## Read the current entry as an &quot;entry&quot; object&#xA;    my $entry = SWISS::Entry-&amp;gt;fromText($current_entry);&#xA;    ## Iterate over the entry's features&#xA;    foreach my $feature ($entry-&amp;gt;FTs-&amp;gt;elements()) {&#xA;        ## If any of the features are 'TRANSMEM', print the entry&#xA;        if ($feature-&amp;gt;[0] eq 'TRANSMEM') {&#xA;            print &quot;$current_entry\n&quot;;&#xA;            ## Skip to the next entry&#xA;            next entry;&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You need to install the Swissknife module first:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Download Swissknife_1.73.tar.gz from &lt;a href=&quot;https://sourceforge.net/projects/swissknife/files/latest/download&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; (SourceForge link).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Install&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;tar xvzf Swissknife.tar.gz&#xA;cd Swissknife_1.73/&#xA;perl Makefile.PL&#xA;sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Now, make the script above executable and run with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;foo.pl file.flat&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-06-29T11:33:38.093" LastActivityDate="2017-06-29T11:33:38.093" CommentCount="0" />
  <row Id="949" PostTypeId="1" CreationDate="2017-06-29T15:45:09.317" Score="5" ViewCount="71" Body="&lt;p&gt;I have the genotyped data from impute2 output in .gen format (imputed to 1000G P3). The file has genotype posterior probabilities (GP:3 values per variant). I have converted .gen to .vcf using qctools and the .vcf file has GT:GP format. I need to convert the .vcf file with GT:GP format to GT:DS. Genotype dosages are recommended for use in qtltools/fastqtl analysis. However, I cannot find a tool that would keep the .vcf format and convert GP to DS. Any help much appreciated!&lt;/p&gt;&#xA;" OwnerUserId="1012" LastActivityDate="2017-07-24T19:52:43.587" Title="How to convert the .vcf (imputed) file with GT:GP format to GT:DS?" Tags="&lt;file-formats&gt;&lt;vcf&gt;&lt;format-conversion&gt;&lt;impute2&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="950" PostTypeId="1" CreationDate="2017-06-29T16:20:35.530" Score="3" ViewCount="18" Body="&lt;p&gt;This is an interesting problem - I was wondering if anyone has a creative solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I have a vector of vertices representing atoms in a protein, as well as 6 variables containing the absolute minimum/maximum bound of the set at each direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to build a grid that surrounds the protein/vertices in a homogeneous layer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right now I just loop through every square of the grid bound by the minimum-maximum values and then loop through every vertex to calculate the minimum distance between my point and every atom in the protein, and see if that point is at an acceptable distance. This process is quite expensive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you guys have any creative answers as to how minimize the time?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. the list of atoms in the protein are ordered more according the the amino acid sequence, which means it has almost no relation to the location of the atom in space&lt;/p&gt;&#xA;" OwnerUserId="1016" LastActivityDate="2017-06-29T16:20:35.530" Title="Minimizing Grid Mapping time of Protein Surface" Tags="&lt;protein-structure&gt;&lt;proteins&gt;&lt;3d-structure&gt;" AnswerCount="0" CommentCount="3" FavoriteCount="0" />
  <row Id="951" PostTypeId="1" CreationDate="2017-06-29T16:24:27.427" Score="7" ViewCount="85" Body="&lt;p&gt;I have 12 human gut microbiome WGS Nextseq reads (151 bp paired end). What will be an effective strategy to assemble a metagenome?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let us say I have already filtered the fastq for quality, adapter sequence and host contamination (human, in this case). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Should I concatenate all the R1 reads as one single R1 read and one Single R2 read?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat Sample[1..12].R1 &amp;gt; Single_R1.fastq&#xA;cat Sample[1..12].R2 &amp;gt; Single_R2.fastq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and then use Diginorm to normalize the Single_R1.fastq and Single_R2.fastq. Subsequently, feed these fastq files into any metagenome assembler such as Megahit, MetaSPAdes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Normalize the output by using CD-HIT or similar tool to remove duplicates and filter by contig length. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;OR &lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Perform metagenome assembly for each of the samples individually after applying filtering, removing adapters and host contamination. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;R1=(*_R1_001.filtered.fastq)&#xA;R2=(*_R2_001.filtered.fastq)&#xA;for ((i=0;i&amp;lt;=${#R1[@]};i++)); do  &#xA;  /bin/metagenome-assembler -1 &quot;${R1[i]}&quot; -2 &quot;${R2[i]}&quot; -o ${R1[i]%.*}.contigs.fa;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Followed by combining all the contigs.fa into one mega_contigs.fa &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat *.contigs.fa &amp;gt; Mega_contigs.fa&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and use CD-HIT or similar tool to remove duplicates. &lt;/p&gt;&#xA;" OwnerUserId="626" LastEditorUserId="298" LastEditDate="2017-06-29T18:03:20.433" LastActivityDate="2017-07-31T21:12:27.573" Title="Pooling data in metagenome assembly" Tags="&lt;assembly&gt;&lt;metagenome&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="952" PostTypeId="2" ParentId="949" CreationDate="2017-06-29T16:57:57.123" Score="5" Body="&lt;p&gt;You can do this in &lt;a href=&quot;https://hail.is/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hail&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the rough code to do it (0.1 versions).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Setup:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from hail import *&#xA;hc = HailContext()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Import the .gen file. VCF works too:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataset = hc.import_gen(&#xA;    'src/test/resources/example.gen', &#xA;    'src/test/resources/example.sample')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Remap the genotype schema and export to VCF:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataset.annotate_genotypes_expr('g = {GT: g.call(), DS: g.dosage()}')\&#xA;    .export_vcf('/tmp/out.vcf.bgz')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Take a look at the &lt;a href=&quot;https://hail.is/hail/getting_started.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;getting started page&lt;/a&gt; if you want to try it out!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I should note that you may be able to do QTL analyses in Hail, depending on the method you want to use. See &lt;a href=&quot;http://discuss.hail.is/t/fast-linear-regression-for-multiple-phenotypes-eqtls/190&quot; rel=&quot;nofollow noreferrer&quot;&gt;blog post here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="1017" LastEditorUserId="1017" LastEditDate="2017-06-30T13:12:31.343" LastActivityDate="2017-06-30T13:12:31.343" CommentCount="0" />
  <row Id="953" PostTypeId="2" ParentId="942" CreationDate="2017-06-29T19:20:15.247" Score="2" Body="&lt;p&gt;For the first question: &quot;how many of long non coding can make hairpin loop structure?&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The easiest thing to would be to run your sequences through an RNA secondary structure prediction tool. There's a few tools for doing this but the most commonly used are &lt;a href=&quot;http://rna.tbi.univie.ac.at/cgi-bin/RNAWebSuite/RNAfold.cgi&quot; rel=&quot;nofollow noreferrer&quot;&gt;RNAfold&lt;/a&gt; from the &lt;a href=&quot;https://www.tbi.univie.ac.at/RNA/&quot; rel=&quot;nofollow noreferrer&quot;&gt;ViennaRNA package&lt;/a&gt; and &lt;a href=&quot;http://unafold.rna.albany.edu/?q=mfold&quot; rel=&quot;nofollow noreferrer&quot;&gt;mfold&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This will give you an output that looks like this, where the parentheses indicate which bases are paired with each other. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;GGGCUAUUAGCUCAGUUGGUUAGAGCGCACCCCUGAUAAGGGUGAGGUCGCUGAUUCGAAUUCAGCAUAGCCCA&#xA;(((((((..((((.........)))).(((((.......))))).....(((((.......)))))))))))).&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To see if there's hairpins, you'll need to annotate the elements within it. Take a look at &lt;a href=&quot;https://www.biostars.org/p/4300/&quot; rel=&quot;nofollow noreferrer&quot;&gt;this question on biostars&lt;/a&gt; for an example of how to do this. In short, you can use the &lt;a href=&quot;https://github.com/ViennaRNA/forgi&quot; rel=&quot;nofollow noreferrer&quot;&gt;forgi&lt;/a&gt; library to produce an annotation like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;(((((((..((((.........)))).(((((.......))))).....(((((.......)))))))))))).&#xA;sssssssmmsssshhhhhhhhhssssmssssshhhhhhhsssssmmmmmssssshhhhhhhsssssssssssse&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If there's an &lt;code&gt;h&lt;/code&gt; in the output annotation, you have a hairpin. The remainder of your question about annotating miRNAs will be left to somebody else.&lt;/p&gt;&#xA;" OwnerUserId="9" LastActivityDate="2017-06-29T19:20:15.247" CommentCount="1" />
  <row Id="954" PostTypeId="1" AcceptedAnswerId="968" CreationDate="2017-06-29T21:19:48.177" Score="8" ViewCount="131" Body="&lt;p&gt;I am on a Mac using UNIX. I am trying to use the kallisto quant command on all files in a directory (instead of manually entering them). Because I'm running the analysis against the same index file, I first enter the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./kallisto index -i --index --make-unique index.fa &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This successfully creates an index file. Then, I tried this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for file in *.fasta; do kallisto quant --single -l 200 -s 0.1 -o $file-aligned &#xA;&quot;$file&quot;; done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;While the $file-aligned folders are created, they're empty. I get an error that states that the index file is missing. So I assumed I would need to specify the index file in the command line. I then tried this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for file in *.fasta; do kallisto quant -i index.fa --single -l 200 -s 0.1 -o&#xA;$file-aligned  &quot;$file&quot;; done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get an error that says &quot;Error: incompatible indices. Found version 0, expected version 10. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have now also tried the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for file in *.fasta; do ./kallisto quant -i index --single - 200 -s 0.01 -o &#xA;&quot;${file}-aligned&quot; &quot;${file}&quot;; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but I get an error saying the index file is not found.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I am now running &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;or file in *.fasta; do ./kallisto quant -i index --single -l 200 -s 0.1 -o $file-aligned &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&quot;$file&quot;; done&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This sort of works. The index file error messages are gone, but it runs quant on the same file 10 times instead of each file once. If I run echo $file it spits back the file that my command is running repeatedly. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I close terminal, start a new session and run the same command, I get&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error: file not found &quot;&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="779" LastEditorUserId="779" LastEditDate="2017-06-30T18:52:35.010" LastActivityDate="2017-07-03T20:50:27.253" Title="Run kallisto iteratively across many samples" Tags="&lt;rna-seq&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="955" PostTypeId="2" ParentId="954" CreationDate="2017-06-29T23:43:58.127" Score="5" Body="&lt;p&gt;For generating the index, you should be using one of &lt;code&gt;-i&lt;/code&gt; or &lt;code&gt;--index&lt;/code&gt;, but not both:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./kallisto index --make-unique -i index.fa.idx index.fa &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You need to point kallisto to the generated index file (usually has a &lt;code&gt;.idx&lt;/code&gt; extension), rather than the fasta file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for file in *.fasta;&#xA;  do kallisto quant -i index.fa.idx --single -l 200 -s 0.1 -o &quot;${file}-aligned&quot; &quot;${file}&quot;;&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-06-30T16:13:19.153" LastActivityDate="2017-06-30T16:13:19.153" CommentCount="9" />
  <row Id="956" PostTypeId="1" CreationDate="2017-06-30T10:37:39.143" Score="3" ViewCount="58" Body="&lt;p&gt;FCS is a patented data format used for storing flow cytometry data. The most recent version is FCS3.1. There is some &lt;a href=&quot;http://isac-net.org/PDFS/90/9090600d-19be-460d-83fc-f8a8b004e0f9.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;documentation&lt;/a&gt; on the format, but there is no information on how to read these files. There are some R packages and a MATLAB code to read an FCS file, but I am looking for standard libraries developed either by the FCS consortium or any other group. I also wish to know if FCS is a subset of an existing standard data format that can be read by a standard library using any programming language.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, I would want to convert these files to an easily readable format like HDF5. &lt;/p&gt;&#xA;" OwnerUserId="345" LastEditorUserId="345" LastEditDate="2017-06-30T10:41:19.763" LastActivityDate="2017-07-04T07:01:36.560" Title="How can I read FCS files using open source libraries?" Tags="&lt;file-formats&gt;&lt;fcs&gt;" AnswerCount="2" CommentCount="8" FavoriteCount="1" />
  <row Id="957" PostTypeId="2" ParentId="956" CreationDate="2017-06-30T11:56:45.483" Score="2" Body="&lt;p&gt;R/Bioconductor has a number of different flow cytometry processing packages. One place to start for looking at cytometry data from a high level would be &lt;a href=&quot;http://opencyto.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;openCyto&lt;/a&gt; (or its &lt;a href=&quot;http://opencyto.org/openCytoVignette.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;vignette&lt;/a&gt;), which is a large set of tools for basic extraction and analysis of FCS files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have looked in the past at the FCS files as an R structure using &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/flowCore.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;flowCore&lt;/a&gt;. Loading a single FCS file is fairly straightforward and follows a familiar R pattern:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;file.name &amp;lt;- &quot;/dir/file.fcs&quot;&#xA;x &amp;lt;- read.FCS(file.name, transformation=FALSE)&#xA;summary(x)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Asking about converting FCS files into an &quot;easily readable format like HDF5&quot; doesn't seem like the right question. &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_Data_Format#HDF5&quot; rel=&quot;nofollow noreferrer&quot;&gt;HDF5&lt;/a&gt; is a container format and shares a lot of similarity to file systems. I've found it best to keep FCS files as they are, as it is a compact, standardised binary format.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-06-30T11:56:45.483" CommentCount="1" />
  <row Id="958" PostTypeId="2" ParentId="949" CreationDate="2017-06-30T12:44:22.447" Score="0" Body="&lt;p&gt;There's &lt;a href=&quot;https://samtools.github.io/bcftools/bcftools-man.html#plugin&quot; rel=&quot;nofollow noreferrer&quot;&gt;the dosage plugin for bcftools&lt;/a&gt;, but it only outputs tab separated values. It would not be too hard to extend the plugin to output a VCF with the DS tag instead, but it has not been done yet.  There's a good chance the bcftools devs would respond to a &lt;a href=&quot;https://github.com/samtools/bcftools/issues/new&quot; rel=&quot;nofollow noreferrer&quot;&gt;feature request&lt;/a&gt;...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In any case, this code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;curl https://raw.githubusercontent.com/samtools/bcftools/develop/test/convert.vcf &amp;gt; convert.vcf&#xA;bcftools +dosage convert.vcf &amp;gt; output.tsv&#xA;head -2 output.tsv &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;has the output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#[1]CHROM       [2]POS  [3]REF  [4]ALT  [5]NA00001      [6]NA00002      [7]NA00003      [8]NA00004      [9]NA00005      [10]NA00006     [11]NA00007     [12]NA00008    [13]NA00009     [14]NA00010&#xA;X       2698560 G       A       0.1     0.0     0.1     0.2     0.3     0.2     0.2     0.2     0.2     0.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is using bcftools version 1.3.1.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an excerpt from the bcftools manual for the dosage plugin:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;dosage&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;print genotype dosage. By default the plugin searches for PL, GL and GT, in that order.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="492" LastActivityDate="2017-06-30T12:44:22.447" CommentCount="0" />
  <row Id="959" PostTypeId="2" ParentId="956" CreationDate="2017-06-30T12:51:15.207" Score="2" Body="&lt;p&gt;A few years ago, I wrote a python script to convert FCS files into tab-separated format. It was far from handling all the possibilities that the format description offers, but at least it worked for some of the files produced on one of our machine: &lt;a href=&quot;http://www.igh.cnrs.fr/equip/Seitz/en_equipe-programmes.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.igh.cnrs.fr/equip/Seitz/en_equipe-programmes.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The format documentation I found enabled decoding (see section 3 of the pdf you mention), but it requires reading data in binary mode.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The general idea of this format (and, I guess, many other binary formats), is that there is a header zone at the beginning of the file with a defined number of fields encoding numbers indicating how the rest of the file is structured. So a first phase is to parse this header, following the description given in the documentation of the format. The information extracted from the header tells where to find the data and how it is encoded, still according to rules described in the format documentation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In case this may be useful, and for the record, here is the code from the above-mentioned script (after stripping comments, some of which are merely copied from the format documentation, and adding a few ones):&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/env python&#xA;&quot;&quot;&quot;This script tries to read FCS flow cytometry data.&#xA;Format parsing inspired by information found here:&#xA;http://isac-net.org/Resources-for-Cytometrists/Data-Standards/Data-File-Standards/Flow-Cytometry-Data-File-Format-Standards.aspx&#xA;&quot;&quot;&quot;&#xA;&#xA;import re&#xA;# To decode binary-encoded data&#xA;import struct&#xA;import sys&#xA;&#xA;class Parameter(object):&#xA;    &quot;&quot;&quot;This object represents one of the parameter types that are present in a DATA segment of a FCS file.&quot;&quot;&quot;&#xA;    __slots__ = (&quot;p_name&quot;, &quot;p_bits&quot;, &quot;p_range&quot;, &quot;p_ampl&quot;, &quot;parser&quot;)&#xA;    def __init__(self, p_name, p_bits, p_range, p_ampl):&#xA;        self.p_name = p_name&#xA;        self.p_bits = p_bits&#xA;        self.p_range = p_range&#xA;        self.p_ampl = p_ampl&#xA;        # Function for parsing a value of the parameter in the data segment&#xA;        self.parser = None&#xA;&#xA;##############################################&#xA;# Here starts the parsing of the header part #&#xA;# which tells where the other parts are.     #&#xA;##############################################&#xA;&#xA;f = open(sys.argv[1], &quot;rb&quot;)&#xA;# The format name is encoded in 6 letters&#xA;# An ASCII letter is coded with one octet&#xA;file_format = &quot;&quot;.join([f.read(1) for __ in range(6)])&#xA;sys.stdout.write(&quot;Format: %s\n&quot; % file_format)&#xA;# The format descriptions reserves 4 octets that we skip&#xA;skip = f.read(4)&#xA;# 8 octet chunks encode the start and end positions&#xA;# of different parts of the data&#xA;text_start = int(f.read(8).strip(&quot; &quot;))&#xA;text_end = int(f.read(8).strip(&quot; &quot;))&#xA;data_start = int(f.read(8).strip(&quot; &quot;))&#xA;data_end = int(f.read(8).strip(&quot; &quot;))&#xA;analysis_start = int(f.read(8).strip(&quot; &quot;))&#xA;analysis_end = int(f.read(8).strip(&quot; &quot;))&#xA;&#xA;if (analysis_start and analysis_end):&#xA;    sys.stderr.write(&quot;Cannot deal with ANALYSIS segment of an FCS file.\n&quot;)&#xA;&#xA;####################################################&#xA;# Here starts the parsing of the &quot;TEXT&quot; portion    #&#xA;# which describes how the data proper is organized #&#xA;####################################################&#xA;f.seek(text_start)&#xA;# The first character in the primary TEXT segment is the ASCII delimiter character.&#xA;sep = f.read(1)&#xA;if sep not in [&quot;_&quot;, &quot;@&quot;]:&#xA;    alt_sep = &quot;_@_&quot;&#xA;elif sep not in [&quot;_&quot;, &quot;|&quot;]:&#xA;    alt_sep = &quot;_|_&quot;&#xA;else:&#xA;    assert sep not in [&quot;+&quot;, &quot;|&quot;]&#xA;    alt_sep = &quot;+|+&quot;&#xA;text_segment = f.read(text_end - text_start)&#xA;&#xA;fields = text_segment.split(sep)&#xA;&#xA;info = {}&#xA;&#xA;i = 0&#xA;while i &amp;lt; len(fields) - 1:&#xA;    key = fields[i]&#xA;    i += 1&#xA;    val = fields[i]&#xA;    i += 1&#xA;    # Keywords are case insensitive, they may be written in a file in lower case, upper case, or a&#xA;    # mixture of the two. However, an FCS file reader must ignore keyword case. A keyword value may&#xA;    # be in lower case, upper case or a mixture of the two. Keyword values are case sensitive.&#xA;    info[key.upper()] = val&#xA;print &quot;%s events were detected.&quot; % info[&quot;$TOT&quot;]&#xA;print &quot;Each event is characterized by %s parameters&quot; % info[&quot;$PAR&quot;]&#xA;&#xA;if info[&quot;$NEXTDATA&quot;] != &quot;0&quot;:&#xA;    sys.stderr.write(&quot;Some other data exist in the file but hasn't been parsed.\n&quot;)&#xA;&#xA;# L - List mode. For each event, the value of each parameter is stored in the order in which the&#xA;# parameters are described. The number of bits reserved for parameter 1 is described using the&#xA;# $P1B keyword. There can be only one set of list mode data per data set. The $DATATYPE&#xA;# keyword describes the data format. This is the most versatile mode for the storage of flow&#xA;# cytometry data because mode C and mode U data can be created from mode L data.&#xA;assert info[&quot;$MODE&quot;] == &quot;L&quot;&#xA;&#xA;parameters = []&#xA;&#xA;# indices of the parameters&#xA;p_indices = range(1, int(info[&quot;$PAR&quot;]) + 1)&#xA;for i in p_indices:&#xA;    p_name = info[&quot;$P%dN&quot; % i]&#xA;    p_bits =  info[&quot;$P%dB&quot; % i]&#xA;    p_range = info[&quot;$P%dR&quot; % i]&#xA;    p_ampl =  info[&quot;$P%dE&quot; % i]&#xA;    parameters.append(Parameter(p_name, p_bits, p_range, p_ampl))&#xA;&#xA;sys.stdout.write(&quot;The parameters are:\n%s\n&quot; % &quot;\t&quot;.join([par.p_name for par in parameters]))&#xA;&#xA;# How are 32 bit words organized&#xA;if info[&quot;$BYTEORD&quot;] == &quot;4,3,2,1&quot;:&#xA;    endianness = &quot;&amp;gt;&quot;&#xA;else:&#xA;    endianness = &quot;&amp;lt;&quot;&#xA;    assert info[&quot;$BYTEORD&quot;] == &quot;1,2,3,4&quot;&#xA;&#xA;    # I stripped a long comment which is just a copy of the documentation&#xA;# Type of data:&#xA;if info[&quot;$DATATYPE&quot;] == &quot;I&quot;:&#xA;    for par in parameters:&#xA;        nb_bits = int(par.p_bits)&#xA;        assert nb_bits % 8 == 0&#xA;        nb_bytes = nb_bits / 8&#xA;        # Determine format string for unpacking (see https://docs.python.org/2/library/struct.html)&#xA;        if nb_bytes == 1:&#xA;            c_type = &quot;B&quot; # unsigned char&#xA;        elif nb_bytes == 2:&#xA;            c_type = &quot;H&quot; # unsigned short&#xA;        elif nb_bytes == 4:&#xA;            c_type = &quot;L&quot; # unsigned long&#xA;        elif nb_bytes == 8:&#xA;            c_type = &quot;Q&quot; # unsigned long long&#xA;        else:&#xA;            raise ValueError, &quot;Number of bytes (%d) not valid for an integer (see https://docs.python.org/2/library/struct.html#byte-order-size-and-alignment).&quot; % nb_bytes&#xA;        fmt = &quot;%s%s&quot; % (endianness, c_type)&#xA;        p_range = int(par.p_range)&#xA;        def parser(data):&#xA;            value = struct.unpack(fmt, data.read(nb_bytes))[0]&#xA;            try:&#xA;                assert value &amp;lt; p_range&#xA;            except AssertionError:&#xA;                print &quot;Value %s higher than %d&quot; % (str(value), p_range)&#xA;            return value&#xA;        par.parser = parser&#xA;    pass&#xA;else:&#xA;    raise NotImplementedError, &quot;Only the parsing of integer value has been implemented so far.&quot;&#xA;&#xA;&#xA;out_file = open(sys.argv[2], &quot;w&quot;)&#xA;out_file.write(&quot;#amplification_types\t&quot; + &quot;\t&quot;.join([par.p_ampl for par in parameters]) + &quot;\n&quot;)&#xA;out_file.write(&quot;parameters\t&quot; + &quot;\t&quot;.join([par.p_name for par in parameters]) + &quot;\n&quot;)&#xA;i = 1&#xA;##############################################&#xA;# Here starts the parsing of the data proper #&#xA;##############################################&#xA;f.seek(data_start)&#xA;while f.tell() &amp;lt; data_end:&#xA;    values = []&#xA;    for par in parameters:&#xA;        values.append(par.parser(f))&#xA;    out_file.write(&quot;%d\t&quot; % i + &quot;\t&quot;.join(map(str, values)) + &quot;\n&quot;)&#xA;    i += 1&#xA;out_file.close()&#xA;f.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-07-04T07:01:36.560" LastActivityDate="2017-07-04T07:01:36.560" CommentCount="2" />
  <row Id="960" PostTypeId="1" CreationDate="2017-06-30T13:19:17.627" Score="6" ViewCount="59" Body="&lt;p&gt;I have some protein sequences and I want to build a position-specific scoring matrix (PSSM) for them and then upload this PSSM to &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE=Proteins&amp;amp;PROGRAM=blastp&amp;amp;RUN_PSIBLAST=on&quot; rel=&quot;nofollow noreferrer&quot;&gt;NCBI PSI-BLAST&lt;/a&gt;. I used &lt;a href=&quot;http://fasta.bioch.virginia.edu/fasta_www2/chaps.cgi&quot; rel=&quot;nofollow noreferrer&quot;&gt;CHAPS&lt;/a&gt; program for this pupose but uploading the output PSSM gave me an error in NCBI PSI-BLAST. Do you know any tool or webserver for getting PSSMs for a group of protein sequences which then can work in NCBI PSI-BLAST?&#xA;This is a part of my PSSM file: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PssmWithParameters ::= {&#xA;pssm {&#xA;    isProtein TRUE ,&#xA;    numRows 28 ,&#xA;    numColumns 131 ,&#xA;    byRow FALSE ,&#xA;    query&#xA;    seq {&#xA;        id {&#xA;            other {&#xA;                accession &quot;WP_000208753&quot; } } ,&#xA;        inst {&#xA;            repr raw ,&#xA;            mol aa ,&#xA;            length 131 ,&#xA;            seq-data&#xA;            ncbieaa &quot;MTTKRKPYVRPMTSTWWKKLPFYRFYMLREGTAVPAVWFSIELIFGLFALKNGPEAW&#xA;AGFIDFLQNPVIVIINLITLAAALLHTKTWFELAPKAANIIVKDEKMGPEPIIKSLWAVTVVATIVILFVALYW&quot; } } ,&#xA;    intermediateData {&#xA;        freqRatios {&#xA;            { 0, 10, 0 } ,&#xA;            { 564418841, 10, -10 } ,&#xA;            { 0, 10, 0 } ,&#xA;            { 11768571, 10, -9 } ,&#xA;            { 185838265, 10, -10 } ,&#xA;            { 31496547, 10, -9 } ,&#xA;            { 3872857, 10, -8 } ,&#xA;            { 291750464, 10, -10 } ,&#xA;            { 128450763, 10, -10 } ,&#xA;            { 759856221, 10, -10 } ,&#xA;            { 359173937, 10, -10 } ,&#xA;            { 179865517, 10, -9 } ,&#xA;            { 14537895, 10, -8 } ,&#xA;            { 212921456, 10, -10 } ,&#xA;            { 220554141, 10, -10 } ,&#xA;            { 368516324, 10, -10 } ,&#xA;            { 319343525, 10, -10 } ,&#xA;            { 426173953, 10, -10 } ,&#xA;            { 463659523, 10, -10 } ,&#xA;            { 817322186, 10, -10 } ,&#xA;            { 811693964, 10, -11 } ,&#xA;            { 0, 10, 0 } ,&#xA;            { 227810064, 10, -10 } ,&#xA;            { 0, 10, 0 } ,&#xA;            { 0, 10, 0 } ,&#xA;            { 0, 10, 0 } ,&#xA;            { 0, 10, 0 } ,&#xA;            { 0, 10, 0 } ,&#xA;            { 0, 10, 0 } ,&#xA;            { 768325726, 10, -10 } ,&#xA;            { 0, 10, 0 } ,&#xA;            { 1425562, 10, -8 } ,&#xA;            { 372685285, 10, -10 } ,&#xA;            { 466727421, 10, -10 } ,&#xA;            { 185741084, 10, -10 } ,&#xA;            { 427328646, 10, -10 } ,&#xA;            { 122594965, 10, -10 } , &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="818" LastEditorUserId="240" LastEditDate="2017-08-02T15:43:07.487" LastActivityDate="2017-08-02T15:43:07.487" Title="A tool or webserver for building PSSM matrix" Tags="&lt;blast&gt;&lt;phylogeny&gt;" AnswerCount="1" CommentCount="8" FavoriteCount="1" />
  <row Id="961" PostTypeId="2" ParentId="935" CreationDate="2017-06-30T13:50:55.503" Score="0" Body="&lt;h3&gt;Using pyGATB&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;(I use the same file as in &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/400/292&quot;&gt;https://bioinformatics.stackexchange.com/a/400/292&lt;/a&gt;, same workstation as in &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/380/292&quot;&gt;https://bioinformatics.stackexchange.com/a/380/292&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time python3 -c &quot;from gatb import Bank; seq_lens = [len(seq) for seq in Bank('SRR077487_2.filt.fastq.gz')]; print('Number of reads: %d' % len(seq_lens), 'Number of bases in reads: %d' % sum(seq_lens), sep='\n')&quot;&#xA;Number of reads: 23861612&#xA;Number of bases in reads: 2386161200&#xA;&#xA;real    0m41.122s&#xA;user    0m40.788s&#xA;sys     0m0.312s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is quite faster than bioawk:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time bioawk -c fastx '{nb_seq+=1; nb_char+=length($seq)} END {print &quot;Number of reads: &quot;nb_seq&quot;\nNumber of bases in reads: &quot;nb_char}' SRR077487_2.filt.fastq.gz&#xA;Number of reads: 23861612&#xA;Number of bases in reads: 2386161200&#xA;&#xA;real    1m3.182s&#xA;user    1m2.916s&#xA;sys     0m0.268s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But not so much than the OP example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time zgrep . SRR077487_2.filt.fastq.gz | awk 'NR%4==2{c++; l+=length($0)} END{print &quot;Number of reads: &quot;c; print &quot;Number of bases in reads: &quot;l}'&#xA;Number of reads: 23861612&#xA;Number of bases in reads: 2386161200&#xA;&#xA;real    0m47.127s&#xA;user    1m36.292s&#xA;sys     0m6.796s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or than &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/936/292&quot;&gt;the &lt;code&gt;wc&lt;/code&gt; based solution&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ fix_base_count() {&#xA;&amp;gt;     local counts=($(cat))&#xA;&amp;gt;     echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xA;&amp;gt; }&#xA;$ time gunzip -c SRR077487_2.filt.fastq.gz | awk 'NR % 4 == 2' | wc -cl | fix_base_count&#xA;23861612 2386161200&#xA;&#xA;real    0m44.915s&#xA;user    1m12.000s&#xA;sys     0m6.972s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I didn't compare with C-based solutions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/937/292&quot;&gt;&lt;code&gt;zcat&lt;/code&gt; to &lt;code&gt;/dev/null&lt;/code&gt; reference&lt;/a&gt; is the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ time zcat SRR077487_2.filt.fastq.gz &amp;gt; /dev/null&#xA;&#xA;real    0m39.745s&#xA;user    0m39.464s&#xA;sys     0m0.252s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm still impressed by pyGATB speed&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-06-30T13:50:55.503" CommentCount="0" />
  <row Id="962" PostTypeId="1" AcceptedAnswerId="964" CreationDate="2017-06-30T16:21:57.023" Score="3" ViewCount="35" Body="&lt;p&gt;For a study in GEO, I would like to obtain the data table header descriptions, specifically the &quot;VALUE&quot; column for all samples in the study.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you go here: &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE99511&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE99511&lt;/a&gt;)&#xA;and then scroll down and click one of the samples: let's choose &quot;GSM2644971&quot;. Then scroll down and you should see &quot;Data table header descriptions&quot; and below that you should see &quot;&lt;strong&gt;VALUE&lt;/strong&gt; Normalized (provided the normalization method) Average Beta&quot;. That information is what I want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried using assayData() from the Biobase package but I didn't know whether the method takes a sample, a matrix of samples, or something else as a parameter. Apparently it takes a S4 and then returns some random text but I don't know how to use that random text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDITED:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I do:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(Biobase)&#xA;library(GEOquery)&#xA;&#xA;getgeo&amp;lt;-getGEO(&quot;GSE99511&quot;)&#xA;assayData(getgeo$GSE99511_series_matrix.txt.gz)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I use assayData() I would want it to return &quot;Normalized (provided the normalization method) Average Beta&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Instead it returns:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;environment: 0x110674178&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Please let me know if the question doesn't make sense or if there's another method that I should be using.&lt;/p&gt;&#xA;" OwnerUserId="1025" LastEditorUserId="1025" LastEditDate="2017-06-30T23:45:53.233" LastActivityDate="2017-07-02T12:54:12.990" Title="Obtaining data table headers from GEO using GEOquery" Tags="&lt;r&gt;&lt;database&gt;&lt;bioconductor&gt;&lt;text&gt;" AnswerCount="1" CommentCount="6" FavoriteCount="0" />
  <row Id="964" PostTypeId="2" ParentId="962" CreationDate="2017-07-02T12:54:12.990" Score="2" Body="&lt;p&gt;The short answer is that if you are seeing information in a GEO sample, then it is the sample that you need to download and access using the appropriate method. In this case, that method is &lt;code&gt;Columns()&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(GEOquery)&#xA;gsm &amp;lt;- getGEO(&quot;GSM2644971&quot;)&#xA;Columns(gsm)&#xA;&#xA;          Column      Description&#xA;1         ID_REF                                                            &#xA;2         VALUE       Normalized (provided the normalization method) Average Beta&#xA;3         Detection   Pval&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The longer answer: you will really benefit from sitting down with the GEOquery documentation and trying to understand the objects that it uses and their methods. It is not something you can work out by trial and error for every case-by-case basis. &lt;a href=&quot;https://www.bioconductor.org/packages/release/bioc/vignettes/GEOquery/inst/doc/GEOquery.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;The tutorial&lt;/a&gt; is a really good starting point (you will find &lt;code&gt;Columns()&lt;/code&gt; in there, for example).&lt;/p&gt;&#xA;" OwnerUserId="150" LastActivityDate="2017-07-02T12:54:12.990" CommentCount="0" />
  <row Id="965" PostTypeId="2" ParentId="960" CreationDate="2017-07-02T21:04:35.520" Score="2" Body="&lt;p&gt;If you take a look at my answer in this BioStars post, you can generate a PSSM using &lt;code&gt;AlignIO&lt;/code&gt; in Biopython:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.biostars.org/p/259190/#259265&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.biostars.org/p/259190/#259265&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="929" LastActivityDate="2017-07-02T21:04:35.520" CommentCount="0" />
  <row Id="966" PostTypeId="2" ParentId="935" CreationDate="2017-07-02T21:56:20.447" Score="4" Body="&lt;h1&gt;pigz | awk | wc is the fastest method&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;First off for benchmarks with FASTQ it's best to use a specific real-world example with a known answer.  I've chosen this file:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG01815/sequence_read/ERR047740_1.filt.fastq.gz&quot; rel=&quot;nofollow noreferrer&quot;&gt;ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG01815/sequence_read/ERR047740_1.filt.fastq.gz&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;as my test file, the correct answers being:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Number of reads: 67051220&#xA;Number of bases in reads: 6034609800&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Next we want to find the fastest way possible to count these, all timings are the average wall-clock time (real) of 10 runs collected with the bash &lt;code&gt;time&lt;/code&gt; on an otherwise unloaded system:&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;zgrep&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;zgrep . ERR047740_1.filt.fastq.gz |&#xA;     awk 'NR%4==2{c++; l+=length($0)}&#xA;          END{&#xA;                print &quot;Number of reads: &quot;c; &#xA;                print &quot;Number of bases in reads: &quot;l&#xA;              }'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the slowest method with an average run-time of 125.35 seconds&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;gzip awk&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Using &lt;code&gt;gzip&lt;/code&gt; we gain about another 10 seconds:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gzip -dc ERR047740_1.filt.fastq.gz |&#xA;     awk 'NR%4==2{c++; l+=length($0)}&#xA;          END{&#xA;                print &quot;Number of reads: &quot;c; &#xA;                print &quot;Number of bases in reads: &quot;l&#xA;              }'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Average run-time is 116.69 seconds&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Konrad's gzip awk wc variant&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fix_base_count() {&#xA;    local counts=($(cat))&#xA;    echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xA;}&#xA;&#xA;gzip -dc ERR047740_1.filt.fastq.gz \&#xA;    | awk 'NR % 4 == 2' \&#xA;    | wc -cl \&#xA;    | fix_base_count&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This runs slower on this test file than the &lt;code&gt;gzip awk&lt;/code&gt; variant of the solution, average run-time is 122.28 seconds.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;kseq_test using latest &lt;code&gt;kseq.h&lt;/code&gt; from &lt;a href=&quot;https://github.com/attractivechaos/klib&quot; rel=&quot;nofollow noreferrer&quot;&gt;klib&lt;/a&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Code compiled with: &lt;code&gt;gcc -O2 -o kseq_test kseq_test.c -lz&lt;/code&gt; where &lt;code&gt;kseq_test.c&lt;/code&gt; is Simon's adaptation of Heng Li's FASTQ parser. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;kseq_test ERR047740_1.filt.fastq.gz&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Average run-time is 99.14 seconds, which is better than the &lt;code&gt;gzip&lt;/code&gt; core utilities based solution so far, but we can do better! &lt;/p&gt;&#xA;&#xA;&lt;h3&gt;piz awk&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Using Mark Adler's &lt;a href=&quot;https://zlib.net/pigz/&quot; rel=&quot;nofollow noreferrer&quot;&gt;pigz&lt;/a&gt; as a drop-in replacement for &lt;code&gt;gzip&lt;/code&gt;, note that &lt;code&gt;pigz&lt;/code&gt; gives us a speed gain as on top of &lt;code&gt;gzip&lt;/code&gt; as in addition to the main deflate thread it uses another 3 threads for reading, writing and checksum calculations, see the &lt;a href=&quot;https://zlib.net/pigz/pigz.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;man page&lt;/a&gt; for details.    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pigz -dc ERR047740_1.filt.fastq.gz |&#xA;     awk 'NR%4==2{c++; l+=length($0)}&#xA;          END{&#xA;                print &quot;Number of reads: &quot;c; &#xA;                print &quot;Number of bases in reads: &quot;l&#xA;              }'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Average run-time is now 93.86 seconds, &lt;strong&gt;this is ~5 seconds faster than the kseq based C code but we can further improve the benchmark&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;pigz awk wc&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Next we use &lt;code&gt;pigz&lt;/code&gt; as a drop in replacment for Konrad's &lt;code&gt;wc&lt;/code&gt; variant of the &lt;code&gt;awk&lt;/code&gt; based solution.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fix_base_count() {&#xA;    local counts=($(cat))&#xA;    echo &quot;${counts[0]} $((${counts[1]} - ${counts[0]}))&quot;&#xA;}&#xA;&#xA;gzip -dc ERR047740_1.filt.fastq.gz \&#xA;    | awk 'NR % 4 == 2' \&#xA;    | wc -cl \&#xA;    | fix_base_count&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Average run-time is now down to 83.03 seconds, this is ~16 seconds faster than the kseq based solution and ~42 seconds faster than the OPs &lt;code&gt;zgrep&lt;/code&gt; based solution.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next as a baseline lets see just how much of this run-time is due to decompression of the input &lt;code&gt;fastq.gz&lt;/code&gt; file.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;gzip alone&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;gzip -dc ERR047740_1.filt.fastq.gz &amp;gt; /dev/null&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Average run-time: 105.95 seconds, so the &lt;code&gt;gzip&lt;/code&gt; based solutions (which also includes &lt;code&gt;zcat&lt;/code&gt; and &lt;code&gt;zgrep&lt;/code&gt; as these are provided by &lt;code&gt;gzip&lt;/code&gt;) are never going to be faster than &lt;code&gt;kseq_test&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;pigz alone&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;pigz -dc ERR047740_1.filt.fastq.gz &amp;gt; /dev/null&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Average run-time: 77.66 seconds, so quite clearly the additional three threads for read, write and checksum calculation offer a useful advantage&lt;/strong&gt;.  What's more this speed-up is greater when leveraging the &lt;code&gt;awk | wc&lt;/code&gt; based solution, it's not clear why, but I expect this is due to the extra write thread.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Interestingly average CPU usage across all threads is quite revealing for the various answers, I've collated these stats using GNU time &lt;code&gt;/usr/bin/time --verbose&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;zgrep&lt;/code&gt; based solution 133% - must be more than one thread somehow&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;gzip | awk&lt;/code&gt; based solution 99% - all &lt;code&gt;gzip&lt;/code&gt; based solutions run single-threaded at 99% CPU usage&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;pigz | awk&lt;/code&gt; 147% &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;gzip | awk | wc&lt;/code&gt; 99% as with &lt;code&gt;gzip&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;pgiz | awk | wc&lt;/code&gt; 155%&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;kseq_test&lt;/code&gt; 99%&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;gzip &amp;gt; dev/null&lt;/code&gt; 99%&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;pigz &amp;gt; dev/null&lt;/code&gt; 155%&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Whilst the main deflate thread in &lt;code&gt;pigz&lt;/code&gt; will run at 100% CPU load the extra 3 don't quite fully occupy additional cores to 100% (as is evidenced by average CPU usage of ~150%) they do however clearly result in reduced run-time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using Ubuntu 16.04.2 LTS**, my &lt;code&gt;gzip&lt;/code&gt;, &lt;code&gt;zcat&lt;/code&gt;, &lt;code&gt;zgrep&lt;/code&gt; versions are all gzip 1.6 and &lt;code&gt;pigz&lt;/code&gt; is version 2.3.1.  &lt;code&gt;gcc&lt;/code&gt; is version 5.4.0 &lt;/p&gt;&#xA;&#xA;&lt;p&gt;** I think my patch level is actually 16.04.4 but I've not rebooted for 170 days :p&lt;/p&gt;&#xA;" OwnerUserId="532" LastEditorUserId="532" LastEditDate="2017-07-02T22:46:28.427" LastActivityDate="2017-07-02T22:46:28.427" CommentCount="3" />
  <row Id="967" PostTypeId="2" ParentId="877" CreationDate="2017-07-02T23:22:34.500" Score="1" Body="&lt;p&gt;&lt;strong&gt;Potential pitfall!&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure about predictive models, but you need to be aware of a potential pitfall in blindly aligning PDX or PDO based sequencing data without first removing contaminating host organism reads, as otherwise these will lead to a lot of false positive variants caused by miss alignment.  In my experience even a small mount of host material can lead to a ten fold increase in called variants due to miss aligned host reads looking like true variants.  I'd recommend using &lt;a href=&quot;https://academic.oup.com/bioinformatics/article/28/12/i172/269972&quot; rel=&quot;nofollow noreferrer&quot;&gt;Xenome&lt;/a&gt;, &lt;a href=&quot;https://github.com/data61/gossamer/blob/master/docs/xenome.md&quot; rel=&quot;nofollow noreferrer&quot;&gt;source&lt;/a&gt;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note Xenome was designed for DNA sequencing it's not splice-aware, so for RNA-Seq there is no optimal solution other than removing reads which align to the host organisms genome.  Although the issue here is that in conjunction with a splice-aware read aligner synteny between the host and grafted genomes might create some interesting problems.  However for RNA-Seq provided &lt;a href=&quot;https://en.wikipedia.org/wiki/Flow_cytometry#Fluorescence-activated_cell_sorting_.28FACS.29&quot; rel=&quot;nofollow noreferrer&quot;&gt;FACS&lt;/a&gt; or similar confirms low-levels of host contamination I expect levels of expression will not be badly affected.  Although this really needs investigating.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally and rather annoyingly Xenome produces none-standard FASTQ so you'll need to fix it's output with:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;awk '{if (NR % 4 == 1) print \&quot;@\&quot;$0; else if (NR % 4 == 3) print \&quot;+\&quot;$0; else print $0 }'&lt;/code&gt; as reported &lt;a href=&quot;http://seqanswers.com/forums/showthread.php?t=43872&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; on seqanswers.&lt;/p&gt;&#xA;" OwnerUserId="532" LastActivityDate="2017-07-02T23:22:34.500" CommentCount="0" />
  <row Id="968" PostTypeId="2" ParentId="954" CreationDate="2017-07-03T09:02:32.610" Score="5" Body="&lt;p&gt;&lt;strong&gt;Running the jobs in parallel&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Iteratively is one solution, but since you're not using bootstrapping, which can be run multithread (and is not needed if you're taking the popular &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;tximport&lt;/a&gt; route) you can do better using &lt;a href=&quot;https://www.gnu.org/software/parallel/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GNU parallel&lt;/a&gt;, this will enable you to run as many jobs as you have execution threads on your system simultaneously.  Note that Kalliso only runs multithreaded with &lt;code&gt;-t&lt;/code&gt; if you're using bootstrapping.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;You'll need to make the index first:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;kallisto index --make-unique -I index.fa.idx index.fa&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Next lets define the input FASTQ files and the sample IDs you want to use using a tab-delimited input file like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sample_1    sample_1_R1.fastq.gz&#xA;sample_2    sample_2_R1.fastq.gz&#xA;sample_3    sample_3_R1.fastq.gz&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here the first column is the sample ID which will be used for output and the second separated by a tab character is the input file. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can produce this using a text editor but as a short-cut you can feed the list of fastq.gz you have into a list via:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ls -1 *.fastq.gz &amp;gt; editme&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Next we create a list of jobs to run on our system using a utility script &lt;code&gt;Make_job_list_kallisto.sh&lt;/code&gt; which consumes our tab delimited input file list above.  I've used your &lt;code&gt;-l&lt;/code&gt; and &lt;code&gt;-s&lt;/code&gt; parameters below, note that with paired end data normally these are estimated for you.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/bin/bash -eu&#xA;&#xA;[ $# -ne 2 ] &amp;amp;&amp;amp; { echo -en &quot;\n*** This script generates jobs for GNU parallel. *** \n\n Error Nothing to do, usage: &amp;lt; input tab delimited list &amp;gt; &amp;lt; output run list file &amp;gt;\n\n&quot; ; exit 1; }&#xA;set -o pipefail&#xA;&#xA;# Get command-line args&#xA;INPUT_LIST=$1&#xA;OUTPUT=$2&#xA;&#xA;# Set counter&#xA;COUNT=1&#xA;END=$(wc -l $INPUT_LIST | awk '{print $1}')&#xA;&#xA;echo &quot; &quot;&#xA;echo &quot; * Input file is: $INPUT_LIST&quot;&#xA;echo &quot; * Number of runs: $END&quot;&#xA;echo &quot; * Output job list for GNU parallel saved to: $OUTPUT&quot;&#xA;echo &quot; &quot;&#xA;&#xA;# Main bit of command-line for job&#xA;CMD=&quot;kallisto quant -i index.fa.idx --single -l 200 -s 0.1&quot;&#xA;&#xA;# Main Loop&#xA;[ -e $OUTPUT ] &amp;amp;&amp;amp; rm $OUTPUT&#xA;while [ $COUNT -le $END ];&#xA;do&#xA;    LINE=( $(awk &quot;NR==$COUNT&quot; $INPUT_LIST) )&#xA;    # Make file list&#xA;    echo &quot;Working on $COUNT of $END Sample ID: ${LINE[0]}, Files ${LINE[@]:1}&quot;&#xA;    echo &quot;$CMD -o ${LINE[0]} ${LINE[@]:1}&quot; &amp;gt;&amp;gt; $OUTPUT&#xA;    ((COUNT++))&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We use the script like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; Make_job_list_kallisto.sh input_file_list.txt job_list.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Now we have a job list for GNU parallel we can run our jobs in parallel like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;parallel --progress --jobs 4 --joblog kallisto_joblog.txt &amp;lt; job_list.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This will run 4 of the kallisto jobs simultaneously (Increase this if you have more threads - don't mind consuming all the IO/CPU threads on your system) producing a nice progress report of those completed, additionally a job log will be written to the file &lt;code&gt;kallisto_joblog.txt&lt;/code&gt;.  &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For paired end data I already have these scripts + similar instructions on &lt;a href=&quot;https://github.com/MattBashton/grolar&quot; rel=&quot;nofollow noreferrer&quot;&gt;GitHub&lt;/a&gt; which people might find useful. &lt;/p&gt;&#xA;" OwnerUserId="532" LastEditorUserId="532" LastEditDate="2017-07-03T20:50:27.253" LastActivityDate="2017-07-03T20:50:27.253" CommentCount="7" />
  <row Id="969" PostTypeId="1" AcceptedAnswerId="1005" CreationDate="2017-07-03T09:09:33.200" Score="4" ViewCount="47" Body="&lt;p&gt;I would like to ask if anyone has experience in running a subset of the PASA pipeline, in particular for the reconciliation of some experimental 'transcripts' with the reference annotation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In more detail, I am working with RNA-seq data from &lt;em&gt;D. melanogaster&lt;/em&gt;. I have reconstructed the 'transcripts' using Trinity. I have aligned these 'transcripts' to the reference genome using GMAP. Now I would like to match these 'experimental transcripts' with the reference annotation, to see how they compare.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering if I can run just a subset of the whole PASA pipeline, so basically skipping the step of alignment to the reference and using the alignment file that I generated externally.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, I would be glad to receive more general redirecting to any released software allowing the comparison of experimental transcripts to the reference annotation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cross-posted on the &lt;a href=&quot;https://groups.google.com/forum/#!topic/pasapipeline-users/cyG9e8-HjG8&quot; rel=&quot;nofollow noreferrer&quot;&gt;PASA Google Group pasapipeline-users&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cross-posted &lt;a href=&quot;https://www.biostars.org/p/260594/&quot; rel=&quot;nofollow noreferrer&quot;&gt;on biostars&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="294" LastEditorUserId="191" LastEditDate="2017-07-05T13:47:56.627" LastActivityDate="2017-07-06T07:35:34.347" Title="PASA pipeline: compare experimental transcripts to the reference annotation" Tags="&lt;rna-seq&gt;&lt;assembly&gt;&lt;transcriptome&gt;&lt;isoform&gt;&lt;pasa&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="0" />
  <row Id="970" PostTypeId="1" AcceptedAnswerId="971" CreationDate="2017-07-03T11:49:21.247" Score="1" ViewCount="31" Body="&lt;p&gt;For a diet analysis of an insect-eating animal, all species in a sample shall be identified. For this a sequencing of the metagenomic sample was done, where the COI/COX region was used as a barcode and amplified. This resulted in 20 million reads (after quality control). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;While looking for an apropriate workflow, I am faced with these issues:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;For many tools 20 million reads are to much to handle (BLAST for&#xA;instance)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;For the majority of metagenomic tools the focus is on micro-organisms, not insects&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The database to search against must contain COI/COX data, mainly from insects, however, the animal could also eat snails or frog eggs...&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Since the reads are amplicons some analysis are not recommended&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Leaving me with these questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What are the typical analysis steps in a workflow like this?&lt;/li&gt;&#xA;&lt;li&gt;Which tools could I use for my purpose?&lt;/li&gt;&#xA;&lt;li&gt;Where could I get a database of COI/COX data?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="1037" LastActivityDate="2017-07-03T15:34:15.717" Title="Workflow of metabarcoding analysis" Tags="&lt;metagenome&gt;&lt;barcode&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="971" PostTypeId="2" ParentId="970" CreationDate="2017-07-03T12:13:22.150" Score="0" Body="&lt;p&gt;I'm working on something similar as a side project. I'll detail the steps that I'm using, but I'm curious if others can suggest a better way.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;You don't mention if your data are paired-end, but in my case I have paired-end sequences that overlap in the middle. I used &lt;a href=&quot;https://ccb.jhu.edu/software/FLASH&quot; rel=&quot;nofollow noreferrer&quot;&gt;FLASh&lt;/a&gt; to merge the reads and &lt;a href=&quot;https://github.com/lh3/seqtk&quot; rel=&quot;nofollow noreferrer&quot;&gt;seqtk&lt;/a&gt; to convert the reads to fasta format.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I used the cluster_fast algorithm in &lt;a href=&quot;https://github.com/torognes/vsearch&quot; rel=&quot;nofollow noreferrer&quot;&gt;vsearch&lt;/a&gt; to cluster sequences at 100% identity, then filtered out rare sequence types (&amp;lt;10 sequences) to account for sequencing errors.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;This resulted in a manageable number of unique sequences, which I searched against a custom database using vsearch, but which could also have been searched against the nr database using blast.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="581" LastActivityDate="2017-07-03T12:13:22.150" CommentCount="1" />
  <row Id="972" PostTypeId="1" AcceptedAnswerId="993" CreationDate="2017-07-03T14:25:39.637" Score="4" ViewCount="86" Body="&lt;p&gt;I want to get the alignment of chain A of 1kf6 (PDB ID) from the pfam database &lt;a href=&quot;http://pfam.xfam.org/protein/FRDA_ECOLI&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. This protein chain has two main domains (FAD_binding_2 and    Succ_DH_flav_C). In pfam there is a link to one of these domains and after clicking in one of the mentioned domains in the table below the page, another page comes at the top of which there is a link to architectures. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if we click on &quot;FAD_binding_2&quot; in the table, there are 220 architectures that have this domain and after clicking on that button (at the top of page), a page opens &lt;a href=&quot;http://pfam.xfam.org/family/Succ_DH_flav_C#tabview=tab1&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. In this page 7471 sequences with the following architecture: FAD_binding_2, Succ_DH_flav_C, resembles most to my chain (because it has both FAD_binding_2 and Succ_DH_flav_C domains). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we want to see all 7471 sequences with that architecture we can click on the show button under that architecture and finally we can see all of that 7471 architecture resembling our chain A sequence. Down the page we can see other architectures that all have FAD_binding_2, but some of them do not have both FAD_binding_2 and Succ_DH_flav_C domains (or some of them have other extra domains that we are not interested in) and so we do not need their sequences and their alignments. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;When we want to get the alignment of full sequences at the left side of the page, we can see that we can just get the alignment of all 15696 sequences and it seems that there is no way to get the alignment of our favourite 7471 sequences. I would like to know if there is any way to get the alignment or at least the fasta file of our favourite 7471 sequences (which have both of our favourite domains not just one of them)?&lt;/p&gt;&#xA;" OwnerUserId="818" LastEditorUserId="298" LastEditDate="2017-07-03T16:20:39.590" LastActivityDate="2017-07-18T13:01:54.007" Title="Obtaining all protein sequences with a particular domain architecture from Pfam" Tags="&lt;sequence-homology&gt;&lt;pfam&gt;&lt;domains&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="2" />
  <row Id="973" PostTypeId="2" ParentId="970" CreationDate="2017-07-03T14:28:22.810" Score="0" Body="&lt;p&gt;Regarding your third question (&quot;Where could I get a database of COI/COX data?&quot;), you can apparently search and download fasta sequences from &lt;a href=&quot;http://www.boldsystems.org/index.php/Public_SearchTerms&quot; rel=&quot;nofollow noreferrer&quot;&gt;BOLD&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I don't see a &quot;select all&quot; button to select all records found by one search in a single fasta file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There seems to exist an R API: &lt;a href=&quot;https://cran.r-project.org/web/packages/bold/bold.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://cran.r-project.org/web/packages/bold/bold.pdf&lt;/a&gt; which refers to this: &lt;a href=&quot;http://www.boldsystems.org/index.php/resources/api#sequenceParameters&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.boldsystems.org/index.php/resources/api#sequenceParameters&lt;/a&gt;&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From this I seem to understand that one can directly query using urls:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;wget -O Gastropoda.fa http://www.boldsystems.org/index.php/API_Public/sequence?taxon=Gastropoda&amp;amp;marker=COI&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I get a fasta file containing COI sequences for Gastropoda. There may be ways to refine the query geographically, using the &lt;code&gt;geo&lt;/code&gt; parameter.&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-07-03T15:34:15.717" LastActivityDate="2017-07-03T15:34:15.717" CommentCount="1" />
  <row Id="974" PostTypeId="1" AcceptedAnswerId="975" CreationDate="2017-07-03T14:41:40.387" Score="7" ViewCount="139" Body="&lt;p&gt;I have high-depth variant calling created using the HaplotypeCaller with &lt;code&gt;--output_mode EMIT_ALL_SITES&lt;/code&gt; I'm interested in finding all sites (regardless of genotype call heterozygous or homozygous) where at least one of the &lt;strong&gt;alternative&lt;/strong&gt; alleles have an &lt;code&gt;AD&lt;/code&gt; value (Allelic Depth) greater than 10, &lt;em&gt;I&lt;/em&gt;.&lt;em&gt;e&lt;/em&gt;. are supported by more than 10 reads. Also ideally I want back more than just the first alternative allele.  Note that I don't want back lines of VCF were we only see an AD count for the ref allele only.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So in the example VCF snippet below I'm wanting to select lines: 6,7,8,12,13 and 14, which have GT:AD values &lt;code&gt;1/1:1,988:989&lt;/code&gt; &lt;code&gt;0/1:116,92&lt;/code&gt; &lt;code&gt;0/1:220,234&lt;/code&gt; &lt;code&gt;0/1:62,611&lt;/code&gt; &lt;code&gt;1/1:0,109&lt;/code&gt; respectively.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  12908_DIAG&#xA;3   187446740   .   T   .   Infinity    .   AN=2;DP=1095;MQ=60.00   GT:AD:DP    0/0:1095:1095&#xA;3   187446741   .   C   .   Infinity    .   AN=2;DP=1117;MQ=60.00   GT:AD:DP    0/0:1117:1117&#xA;3   187446752   .   A   .   Infinity    .   AN=2;DP=1297;MQ=60.00   GT:AD:DP    0/0:1297:1297&#xA;3   187446763   .   C   .   Infinity    .   AN=2;DP=1494;MQ=60.00   GT:AD:DP    0/0:1494:1494&#xA;3   187451574   .   C   .   Infinity    .   AN=2;DP=1493;MQ=60.00   GT:AD:DP    0/0:1493:1493&#xA;3   187451609   rs1880101   A   G   39794.03    .   AC=2;AF=1.00;AN=2;BaseQRankSum=1.859;ClippingRankSum=0.000;DB;DP=995;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;MQRankSum=0.000;QD=24.56;ReadPosRankSum=0.406;SOR=8.234  GT:AD:DP:GQ:PL  1/1:1,988:989:99:39808,2949,0&#xA;4   1803279 .   T   G   0   .   AC=0;AF=0.00;AN=2;BaseQRankSum=-6.652;ClippingRankSum=0.000;DP=245;ExcessHet=3.0103;FS=89.753;MLEAC=0;MLEAF=0.00;MQ=59.97;MQRankSum=0.000;ReadPosRankSum=-2.523;SOR=6.357   GT:AD:DP:GQ:PL  0/0:211,23:234:99:0,364,6739&#xA;4   1803307 rs2305183   T   C   2486.60 .   AC=1;AF=0.500;AN=2;BaseQRankSum=-5.049;ClippingRankSum=0.000;DB;DP=215;ExcessHet=3.0103;FS=1.110;MLEAC=1;MLEAF=0.500;MQ=59.97;MQRankSum=0.000;QD=11.95;ReadPosRankSum=-0.045;SOR=0.809  GT:AD:DP:GQ:PL  0/1:116,92:208:99:2494,0,3673&#xA;4   1803671 .   C   A   0   .   AC=0;AF=0.00;AN=2;BaseQRankSum=-0.880;ClippingRankSum=0.000;DP=450;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.953;SOR=0.572    GT:AD:DP:GQ:PL  0/0:445,2:447:99:0,1272,15958&#xA;4   1803681 .   T   C   0   .   AC=0;AF=0.00;AN=2;BaseQRankSum=-1.654;ClippingRankSum=0.000;DP=483;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=-0.422;SOR=0.664    GT:AD:DP:GQ:PL  0/0:479,2:481:99:0,1408,18538&#xA;4   1803703 .   A   G   0   .   AC=0;AF=0.00;AN=2;BaseQRankSum=-1.704;ClippingRankSum=0.000;DP=458;ExcessHet=3.0103;FS=0.000;MLEAC=0;MLEAF=0.00;MQ=60.00;MQRankSum=0.000;ReadPosRankSum=0.299;SOR=0.497 GT:AD:DP:GQ:PL  0/0:454,2:456:99:0,1325,18095&#xA;4   1803704 rs2234909   T   C   6676.60 .   AC=1;AF=0.500;AN=2;BaseQRankSum=-2.605;ClippingRankSum=0.000;DB;DP=456;ExcessHet=3.0103;FS=1.753;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=14.71;ReadPosRankSum=0.324;SOR=0.849   GT:AD:DP:GQ:PL  0/1:220,234:454:99:6684,0,6366&#xA;4   1803824 rs2305184   C   G   2030.60 .   AC=1;AF=0.500;AN=2;BaseQRankSum=8.083;ClippingRankSum=0.000;DB;DP=124;ExcessHet=3.0103;FS=6.128;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=16.51;ReadPosRankSum=0.180;SOR=0.096    GT:AD:DP:GQ:PL  0/1:62,61:123:99:2038,0,1766&#xA;4   1805296 rs3135883   G   A   3876.03 .   AC=2;AF=1.00;AN=2;DB;DP=110;ExcessHet=3.0103;FS=0.000;MLEAC=2;MLEAF=1.00;MQ=60.00;QD=29.22;SOR=9.401    GT:AD:DP:GQ:PL  1/1:0,109:109:99:3890,326,0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.dropbox.com/s/gm4pftuk9i1gx6p/test.vcf?dl=1&quot; rel=&quot;nofollow noreferrer&quot;&gt;A dropbox link for file&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd initially considered using GATK's SelectVariants but I'm not sure JEXL has the ability to select out what I want specifically other than a blanket AD &gt; 10 which will give me both ref and alt alleles with AD &gt; 10.  Perhaps there is a bioawk solution or something more elaborate with coreutils which could successfully return sites with an alt AD count &gt; 10? &lt;/p&gt;&#xA;" OwnerUserId="532" LastEditorUserId="532" LastEditDate="2017-07-05T16:02:23.770" LastActivityDate="2017-07-14T15:18:35.790" Title="Selecting sites from VCF which have an alt AD &gt; 10" Tags="&lt;variant-calling&gt;&lt;vcf&gt;&lt;bioawk&gt;&lt;coreutils&gt;&lt;selectvariants&gt;" AnswerCount="3" CommentCount="1" FavoriteCount="2" />
  <row Id="975" PostTypeId="2" ParentId="974" CreationDate="2017-07-03T15:41:14.367" Score="5" Body="&lt;p&gt;using &lt;a href=&quot;http://lindenb.github.io/jvarkit/VCFFilterJS.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;vcfilterjs&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and the following script:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;function accept(vc)&#xA;    {&#xA;    var i,j;&#xA;    for(i=0;i&amp;lt; vc.getNSamples();++i)&#xA;        {&#xA;        var genotype = vc.getGenotype(i);&#xA;        if(!genotype.hasAD()) continue;&#xA;        var ad = genotype.getAD();&#xA;        /* loop over AD starting from '1' =  first ALT */&#xA;        for(j=1  ;j&amp;lt; ad.length ;++j)&#xA;            {&#xA;            if(ad[j]&amp;gt;10) return true;&#xA;            }&#xA;        }&#xA;    return false;&#xA;    }&#xA;accept(variant);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;usage:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java -jar dist/vcffilterjs.jar -f script.js Test.vcf &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="71" LastEditorUserId="71" LastEditDate="2017-07-05T16:24:32.853" LastActivityDate="2017-07-05T16:24:32.853" CommentCount="4" />
  <row Id="977" PostTypeId="2" ParentId="951" CreationDate="2017-07-03T21:16:19.757" Score="1" Body="&lt;p&gt;If your goal is to bin the resulting contigs into genomes, then you should do option #1, pooling the reads and assembling into one set of contigs. &lt;/p&gt;&#xA;" OwnerUserId="1044" LastActivityDate="2017-07-03T21:16:19.757" CommentCount="0" />
  <row Id="979" PostTypeId="1" AcceptedAnswerId="980" CreationDate="2017-07-04T11:53:41.607" Score="3" ViewCount="220" Body="&lt;p&gt;I have a reference database with contains 100s of sequences in fasta format. Some of these sequences have duplicate names like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;1_uniqueGeneName&#xA;atgc&#xA;&amp;gt;1_anotherUniqueGeneName&#xA;atgc&#xA;&amp;gt;1_duplicateName&#xA;atgc&#xA;&amp;gt;1_duplicateName&#xA;atgc&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is is possible to run through a large file like this and change the names of only the duplicates?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;1_uniqueGeneName&#xA;atgc&#xA;&amp;gt;1_anotherUniqueGeneName&#xA;atgc&#xA;&amp;gt;1_duplicateName_1&#xA;atgc&#xA;&amp;gt;1_duplicateName_2&#xA;atgc&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="982" LastEditorUserId="48" LastEditDate="2017-07-05T17:52:29.090" LastActivityDate="2017-07-05T17:52:29.090" Title="How to append numbers only on duplicates sequence names?" Tags="&lt;fasta&gt;&lt;text&gt;" AnswerCount="3" CommentCount="3" />
  <row Id="980" PostTypeId="2" ParentId="979" CreationDate="2017-07-04T11:58:43.597" Score="5" Body="&lt;p&gt;Sure, this little Perl snippet should do it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ perl -pe 's/$/_$seen{$_}/ if ++$seen{$_}&amp;gt;1 and /^&amp;gt;/; ' file.fa &#xA;&amp;gt;1_uniqueGeneName&#xA;atgc&#xA;&amp;gt;1_anotherUniqueGeneName&#xA;atgc_2&#xA;&amp;gt;1_duplicateName&#xA;atgc_3&#xA;&amp;gt;1_duplicateName_2&#xA;atgc_4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or, to make the changes in the original file, use &lt;code&gt;-i&lt;/code&gt;:    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;perl -i.bak -pe 's/$/_$seen{$_}/ if ++$seen{$_}&amp;gt;1 and /^&amp;gt;/; ' file.fa &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note that the first occurrence of a duplicate name isn't changed, the second will become &lt;code&gt;_2&lt;/code&gt;, the third &lt;code&gt;_3&lt;/code&gt; etc. &lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Explanation&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;perl -pe&lt;/code&gt; : &lt;strong&gt;p&lt;/strong&gt;rint each input line after applying the script given by &lt;code&gt;-e&lt;/code&gt; to it. &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;++$seen{$_}&amp;gt;1&lt;/code&gt; : increment the current value stored in the hash &lt;code&gt;%seen&lt;/code&gt; for this line (&lt;code&gt;$_&lt;/code&gt;) by 1 and compare it to &lt;code&gt;1&lt;/code&gt;. &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;s/$/_$seen{$_}/ if ++$seen{$_}&amp;gt;1 and /^&amp;gt;/&lt;/code&gt; : if the current line starts with a &lt;code&gt;&amp;gt;&lt;/code&gt; and the  value stored in the hash &lt;code&gt;%seen&lt;/code&gt; for this line is greater than 1 (if this isn't the first time we see this line), replace the end of the line (&lt;code&gt;$&lt;/code&gt;) with a &lt;code&gt;_&lt;/code&gt; and the current value in the hash&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Alternatively, here's the same idea in &lt;code&gt;awk&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ awk '(/^&amp;gt;/ &amp;amp;&amp;amp; s[$0]++){$0=$0&quot;_&quot;s[$0]}1;' file.fa &#xA;&amp;gt;1_uniqueGeneName&#xA;atgc&#xA;&amp;gt;1_anotherUniqueGeneName&#xA;atgc&#xA;&amp;gt;1_duplicateName&#xA;atgc&#xA;&amp;gt;1_duplicateName_2&#xA;atgc&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To make the changes in the original file (assuming you are using GNU &lt;code&gt;awk&lt;/code&gt; which is the default on most Linux versions), use &lt;code&gt;-i inplace&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;awk -iinplace '(/^&amp;gt;/ &amp;amp;&amp;amp; s[$0]++){$0=$0&quot;_&quot;s[$0]}1;' file.fa &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3&gt;Explanation&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;In awk, the special variable &lt;code&gt;$0&lt;/code&gt; is the current line. &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;(/^&amp;gt;/ &amp;amp;&amp;amp; s[$0]++)&lt;/code&gt; : if this line starts with a &lt;code&gt;&amp;gt;&lt;/code&gt; and incrementing the value stored in the array &lt;code&gt;s&lt;/code&gt; for this line by 1 evaluates to true (is greater than 0).&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;$0=$0&quot;_&quot;s[$0]&lt;/code&gt; :  make the current line be itself with a &lt;code&gt;_&lt;/code&gt; and the value from &lt;code&gt;s&lt;/code&gt; appended.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;1;&lt;/code&gt; : this is just shorthand for &quot;print this line&quot;. If an expression evaluates to true, awk will print the current line. Since &lt;code&gt;1&lt;/code&gt; is always true, this will print every line. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;If you want all of the duplicates to be marked, you need to read the file twice. Once to collect the names and a second to mark them:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ awk '{&#xA;    if (NR==FNR){&#xA;        if(/^&amp;gt;/){&#xA;            s[$0]++&#xA;        }&#xA;        next;&#xA;    }&#xA;    if(/^&amp;gt;/){&#xA;        k[$0]++;&#xA;        if(s[$0]&amp;gt;1){&#xA;            $0=$0&quot;_&quot;k[$0]&#xA;        }&#xA;    }&#xA;    print&#xA;}' file.fa file.fa&#xA;&amp;gt;1_uniqueGeneName&#xA;atgc&#xA;&amp;gt;1_anotherUniqueGeneName&#xA;atgc&#xA;&amp;gt;1_duplicateName_1&#xA;atgc&#xA;&amp;gt;1_duplicateName_2&#xA;atgc&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;: note that all of these approaches assume you don't already have sequence names ending with &lt;code&gt;_N&lt;/code&gt; where &lt;code&gt;N&lt;/code&gt; is a number. If your input file has 2 sequences called &lt;code&gt;foo&lt;/code&gt; and one called &lt;code&gt;foo_2&lt;/code&gt;, then you will end up with two &lt;code&gt;foo_2&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ cat test.fa&#xA;&amp;gt;foo_2&#xA;actg&#xA;&amp;gt;foo&#xA;actg&#xA;&amp;gt;foo&#xA;actg&#xA;$ perl -pe 's/$/_$seen{$_}/ if ++$seen{$_}&amp;gt;1 and /^&amp;gt;/; ' test.fa &#xA;&amp;gt;foo_2&#xA;actg&#xA;&amp;gt;foo&#xA;actg&#xA;&amp;gt;foo_2&#xA;actg&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If this can be an issue for you, use one of the more sophisticated approaches suggested by the other answers.&lt;/p&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-07-04T13:59:59.517" LastActivityDate="2017-07-04T13:59:59.517" CommentCount="2" />
  <row Id="981" PostTypeId="2" ParentId="979" CreationDate="2017-07-04T12:02:17.610" Score="6" Body="&lt;p&gt;Sure, using biopython:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python&#xA;from Bio import SeqIO&#xA;&#xA;records = set()&#xA;of = open(&quot;output.fa&quot;, &quot;w&quot;)&#xA;for record in SeqIO.parse(&quot;foo.fa&quot;, &quot;fasta&quot;):&#xA;    ID = record.id&#xA;    num = 1&#xA;    while ID in records:&#xA;        ID = &quot;{}_{}&quot;.format(record.id, num)&#xA;        num += 1&#xA;    records.add(ID)&#xA;    record.id = ID&#xA;    record.name = ID&#xA;    record.description = ID&#xA;    SeqIO.write(record, of, &quot;fasta&quot;)&#xA;of.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Change &lt;code&gt;output.fa&lt;/code&gt; and &lt;code&gt;foo.fa&lt;/code&gt;. One doesn't need to explicitly change the &lt;code&gt;.name&lt;/code&gt; and &lt;code&gt;.description&lt;/code&gt;, but that's handy to prevent the original ID from not appearing still (after a space).&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Explanation&lt;/h1&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;records = set()&lt;/code&gt; This will create a lookup table of all IDs &lt;strong&gt;written&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;for record in SeqIO.parse(&quot;foo.fa&quot;, &quot;fasta&quot;):&lt;/code&gt; Open &lt;code&gt;foo.fa&lt;/code&gt; as a fasta file and iterate over the &lt;strong&gt;entries&lt;/strong&gt; in it. These are objects with an ID, name, description, and sequence attribute (the name and description are the same as the ID if not present).&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;ID = record.id&lt;/code&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Memoization&quot; rel=&quot;nofollow noreferrer&quot;&gt;Memoize&lt;/a&gt; the entry ID (e.g., &lt;code&gt;1_duplicateName&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;while ID in records:&lt;/code&gt; As long as the ID has already been seen, keep looping.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;ID = &quot;{}_{}&quot;.format(record.id, num)&lt;/code&gt; Start adding increasing numbers after the ID, such as &lt;code&gt;1_duplicateName_1&lt;/code&gt; and &lt;code&gt;1_duplicateName_2&lt;/code&gt;. This will continue until the ID has not been seen.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;records.add(ID)&lt;/code&gt; Add the unseen ID to the set.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;record.id = ID&lt;/code&gt; Update the ID, the &lt;code&gt;.name&lt;/code&gt; and &lt;code&gt;.description&lt;/code&gt; are the same. If you don't do that, then you get output like &lt;code&gt;&amp;gt;1_duplicateName_1 1_duplicateName&lt;/code&gt;. That's not really a problem, but it's excessive.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;SeqIO.write(record, of, &quot;fasta&quot;)&lt;/code&gt; Write the record to the output file in fasta format.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;A benefit of biopython is the easy ability to change formats, so instead of fasta one could have substituted Genbank or another format if needed.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-07-05T15:49:58.337" LastActivityDate="2017-07-05T15:49:58.337" CommentCount="0" />
  <row Id="982" PostTypeId="1" CreationDate="2017-07-04T12:53:14.190" Score="4" ViewCount="70" Body="&lt;p&gt;I was wondering how I can calculate the charge of a protein peptide (e.g. &quot;RKTTLVPNTQTASPR&quot;) computationally in R or another tool.&lt;/p&gt;&#xA;" OwnerUserId="704" LastEditorUserId="298" LastEditDate="2017-07-04T14:56:02.313" LastActivityDate="2017-07-18T22:47:05.673" Title="Calculating the charge of a peptide computationally" Tags="&lt;r&gt;&lt;proteins&gt;" AnswerCount="3" CommentCount="1" FavoriteCount="0" />
  <row Id="983" PostTypeId="2" ParentId="982" CreationDate="2017-07-04T13:05:35.823" Score="2" Body="&lt;p&gt;A quick google search turns up &lt;a href=&quot;http://protcalc.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;protcalc&lt;/a&gt;, which is able to give a nice pH-dependent table of peptide charges (yours ranges from 3.1 at pH 4 to 1.5 at pH 10). It's on sourceforge, so hopefully the source code (and maybe a publication) is somewhere in there. Granted, this isn't R, but it's not clear from your post how necessary that really is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW, in general &quot;best and most accurate&quot; turns out to be highly subjective (at least the &quot;best&quot; part of that).&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-04T13:05:35.823" CommentCount="4" />
  <row Id="984" PostTypeId="2" ParentId="979" CreationDate="2017-07-04T13:10:58.163" Score="6" Body="&lt;p&gt;And here’s a solution using R (with the Bioconductor):&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;fa = ShortRead::readFasta(infile)&#xA;ids = as.character(ShortRead::id(fa))&#xA;fa@id = Biostrings::BStringSet(make.unique(ids, sep = '_'))&#xA;ShortRead::writeFasta(fa, outfile)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="29" LastEditorUserId="29" LastEditDate="2017-07-04T17:14:57.440" LastActivityDate="2017-07-04T17:14:57.440" CommentCount="0" />
  <row Id="985" PostTypeId="2" ParentId="779" CreationDate="2017-07-04T14:13:22.690" Score="0" Body="&lt;h3&gt;An example solution for Seurat:&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Retrieve sample IDs from the .csv used with &lt;code&gt;cellranger&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samples &amp;lt;- read.csv(file.path(&quot;/path/to/csv&quot;, &quot;nameof.csv&quot;), stringsAsFactors=F)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Load the 10x dataset and initialize the Seurat object:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cells.data &amp;lt;- Read10X(&quot;path/to/filtered_gene_bc_matrices&quot;)&#xA;&#xA;cells &amp;lt;- new(&quot;seurat&quot;, raw.data=cells.data)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Get barcodes and suffix:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cellcodes &amp;lt;- as.data.frame(cells@raw.data@Dimnames[[2]])&#xA;colnames(cellcodes) &amp;lt;- &quot;barcodes&quot;&#xA;rownames(cellcodes) &amp;lt;- cellcodes$barcodes&#xA;&#xA;cellcodes$libcodes &amp;lt;- as.factor(gsub(pattern=&quot;.+-&quot;, replacement=&quot;&quot;, cellcodes$barcodes))&#xA;cellcodes$samples &amp;lt;- as.vector(samples$library_id[cellcodes$libcodes])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Create dataframe for &lt;code&gt;meta.data&lt;/code&gt; argument and set up object:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sampleidentity &amp;lt;- cellcodes[&quot;samples&quot;]&#xA;&#xA;cells &amp;lt;- Setup(cells,&#xA;               meta.data=sampleidentity,&#xA;               min.cells=3,&#xA;               min.genes=200,&#xA;               do.logNormalize=T, total.expr=1e4, project=&quot;projectname&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Use the &lt;code&gt;group.by&lt;/code&gt; argument of various functions.&lt;/p&gt;&#xA;" OwnerUserId="208" LastActivityDate="2017-07-04T14:13:22.690" CommentCount="1" />
  <row Id="986" PostTypeId="2" ParentId="895" CreationDate="2017-07-04T16:06:20.003" Score="0" Body="&lt;p&gt;You can download a list of transcript annotations as a flat file from UCSC:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refGene.txt.gz&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is not exactly a BED file, but it does contain information about the known transcripts for this assembly (hg19, in this case):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;585 NR_046018   chr1    +   11873   14409   14409   14409   3   11873,12612,13220,  12227,12721,14409,  0   DDX11L1 unk unk -1,-1,-1,&#xA;585 NR_024540   chr1    -   14361   29370   29370   29370   11  14361,14969,15795,16606,16857,17232,17605,17914,18267,24737,29320,  14829,15038,15947,16765,17055,17368,17742,18061,18366,24891,29370,  0   WASH7P  unk unk -1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The gene name can be found in column 13. The exon start and end positions are in columns 10 and 11, respectively. More information can be found in the other columns:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;bin&lt;/li&gt;&#xA;&lt;li&gt;name&lt;/li&gt;&#xA;&lt;li&gt;chrom&lt;/li&gt;&#xA;&lt;li&gt;strand&lt;/li&gt;&#xA;&lt;li&gt;txStart&lt;/li&gt;&#xA;&lt;li&gt;txEnd&lt;/li&gt;&#xA;&lt;li&gt;cdsStart&lt;/li&gt;&#xA;&lt;li&gt;cdsEnd&lt;/li&gt;&#xA;&lt;li&gt;exonCount&lt;/li&gt;&#xA;&lt;li&gt;exonStarts&lt;/li&gt;&#xA;&lt;li&gt;exonEnds&lt;/li&gt;&#xA;&lt;li&gt;score&lt;/li&gt;&#xA;&lt;li&gt;name2&lt;/li&gt;&#xA;&lt;li&gt;cdsStartStat&lt;/li&gt;&#xA;&lt;li&gt;cdsEndStat&lt;/li&gt;&#xA;&lt;li&gt;exonFrames&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;(The column information was obtained from the &lt;a href=&quot;http://hgdownload.cse.ucsc.edu/goldenpath/hg19/database/refGene.sql&quot; rel=&quot;nofollow noreferrer&quot;&gt;sql file here&lt;/a&gt;)&lt;/p&gt;&#xA;" OwnerUserId="9" LastActivityDate="2017-07-04T16:06:20.003" CommentCount="0" />
  <row Id="987" PostTypeId="2" ParentId="974" CreationDate="2017-07-04T16:24:13.710" Score="2" Body="&lt;p&gt;This now works with the development version of Bcftools v1.5 (commit 4f134df). Thanks to Petr Danecek for &lt;a href=&quot;https://github.com/samtools/bcftools/issues/639&quot; rel=&quot;nofollow noreferrer&quot;&gt;adding the feature&lt;/a&gt;. I expect this feature to make its way into the next release of Bcftools:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;git clone git://github.com/samtools/htslib.git&#xA;git clone git://github.com/samtools/bcftools.git&#xA;(cd bcftools; make)&#xA;&#xA;bgzip Test.vcf&#xA;./bcftools/bcftools index Test.vcf.gz&#xA;./bcftools/bcftools filter -i 'AD[1-] &amp;gt; 10' Test.vcf.gz &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Output without header (I have modified the second line to be tri-allelic to demonstrate the filtering works):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;3   187451609   rs1880101   A   G   39794   PASS    AC=2;AF=1;AN=2;BaseQRankSum=1.859;ClippingRankSum=0;DB;DP=995;ExcessHet=3.0103;FS=0;MLEAC=2;MLEAF=1;MQ=60;MQRankSum=0;QD=24.56;ReadPosRankSum=0.406;SOR=8.234   GT:AD:DP:GQ:PL  1/1:1,988:989:99:39808,2949,0&#xA;4   1803279 .   T   G   0   PASS    AC=0;AF=0;AN=2;BaseQRankSum=-6.652;ClippingRankSum=0;DP=245;ExcessHet=3.0103;FS=89.753;MLEAC=0;MLEAF=0;MQ=59.97;MQRankSum=0;ReadPosRankSum=-2.523;SOR=6.357 GT:AD:DP:GQ:PL  0/0/0:211,3,34:234:99:0,364,6739&#xA;4   1803307 rs2305183   T   C   2486.6  PASS    AC=1;AF=0.5;AN=2;BaseQRankSum=-5.049;ClippingRankSum=0;DB;DP=215;ExcessHet=3.0103;FS=1.11;MLEAC=1;MLEAF=0.5;MQ=59.97;MQRankSum=0;QD=11.95;ReadPosRankSum=-0.045;SOR=0.809   GT:AD:DP:GQ:PL  0/1:116,92:208:99:2494,0,3673&#xA;4   1803704 rs2234909   T   C   6676.6  PASS    AC=1;AF=0.5;AN=2;BaseQRankSum=-2.605;ClippingRankSum=0;DB;DP=456;ExcessHet=3.0103;FS=1.753;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=14.71;ReadPosRankSum=0.324;SOR=0.849  GT:AD:DP:GQ:PL  0/1:220,234:454:99:6684,0,6366&#xA;4   1803824 rs2305184   C   G   2030.6  PASS    AC=1;AF=0.5;AN=2;BaseQRankSum=8.083;ClippingRankSum=0;DB;DP=124;ExcessHet=3.0103;FS=6.128;MLEAC=1;MLEAF=0.5;MQ=60;MQRankSum=0;QD=16.51;ReadPosRankSum=0.18;SOR=0.096    GT:AD:DP:GQ:PL  0/1:62,61:123:99:2038,0,1766&#xA;4   1805296 rs3135883   G   A   3876.03 PASS    AC=2;AF=1;AN=2;DB;DP=110;ExcessHet=3.0103;FS=0;MLEAC=2;MLEAF=1;MQ=60;QD=29.22;SOR=9.401 GT:AD:DP:GQ:PL  1/1:0,109:109:99:3890,326,0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="492" LastEditorUserId="492" LastEditDate="2017-07-14T15:18:35.790" LastActivityDate="2017-07-14T15:18:35.790" CommentCount="0" />
  <row Id="989" PostTypeId="1" CreationDate="2017-07-05T06:55:02.657" Score="2" ViewCount="29" Body="&lt;p&gt;I'd like to simulate 10% sequencing error using &lt;code&gt;art_illumina&lt;/code&gt;. The simulator doesn't have a parameter that I can just give the 10%, but it has this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;-qs  --qShift   the amount to shift every first-read quality score by&#xA;-qs2 --qShift2  the amount to shift every second-read quality score by&#xA;                NOTE: For -qs/-qs2 option, a positive number will shift up &#xA;                quality scores (the max is 93) that reduce substitution sequencing &#xA;                errors and a negative number will shift down quality scores that &#xA;                increase sequencing errors. If shifting scores by x, the error&#xA;                rate will be 1/(10^(x/10)) of the default profile.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Heng Li's &lt;code&gt;wgsim&lt;/code&gt; has an option for base error rate. Can I do the same for &lt;code&gt;art_illumina&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Does the &quot;error rate&quot; in the description mean sequencing error? If I set both &lt;code&gt;qShift&lt;/code&gt; and &lt;code&gt;qShift2&lt;/code&gt; to 10, &lt;code&gt;1/(10^(x/10)) == 0.1, where x is 10&lt;/code&gt;, does that mean 10 is the value I should give to the simulator?&lt;/p&gt;&#xA;" OwnerUserId="174" LastEditorUserId="298" LastEditDate="2017-07-05T08:31:34.743" LastActivityDate="2017-07-05T08:31:34.743" Title="How to simulate &quot;base error rate&quot; in art_illumina?" Tags="&lt;ngs&gt;&lt;simulated-data&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="990" PostTypeId="1" CreationDate="2017-07-05T08:56:46.243" Score="1" ViewCount="40" Body="&lt;p&gt;I am interested in collecting the reference genomes of marine protists and would like to know which databases can be used for such tasks, and is there a single, centralized database of protistological genomic data?&lt;/p&gt;&#xA;" OwnerUserId="369" LastEditorUserId="369" LastEditDate="2017-07-05T09:24:30.257" LastActivityDate="2017-07-05T09:28:22.160" Title="Genome databases of microbial eukaryotes" Tags="&lt;database&gt;&lt;genome&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="991" PostTypeId="2" ParentId="990" CreationDate="2017-07-05T09:28:22.160" Score="1" Body="&lt;p&gt;You can find many protist genomes on ensembl&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://protists.ensembl.org/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://protists.ensembl.org/index.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="939" LastActivityDate="2017-07-05T09:28:22.160" CommentCount="0" />
  <row Id="992" PostTypeId="2" ParentId="972" CreationDate="2017-07-05T10:01:28.437" Score="1" Body="&lt;p&gt;That does not appear to be possible using PFAM. I would suggest trying a different database. I managed to perform the same query using SMART (&lt;a href=&quot;http://smart.embl-heidelberg.de/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://smart.embl-heidelberg.de/&lt;/a&gt;), via entering &quot;Pfam:FAD_binding_2 AND Pfam:Succ_DH_flav_C&quot; under &quot;Domain selection&quot; in the &quot;Architecture Analysis&quot; section of the search screen, which resulted in 9921 protein sequences with those two domains as you can see &lt;a href=&quot;http://smart.embl-heidelberg.de/smart/selective.cgi?domains=Pfam%3AFAD_binding_2+AND+Pfam%3ASucc_DH_flav_C&amp;amp;terms=&amp;amp;taxon_text=&amp;amp;input=Architecture+query&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the results screen you can select all the sequences and under &quot;Action&quot; at the top of the screen choose &quot;Download Protein Sequences as FASTA file&quot;.&lt;/p&gt;&#xA;" OwnerUserId="369" LastActivityDate="2017-07-05T10:01:28.437" CommentCount="3" />
  <row Id="993" PostTypeId="2" ParentId="972" CreationDate="2017-07-05T12:00:40.840" Score="1" Body="&lt;p&gt;I believe you can do this using SQL files of the database.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First you need to get the integer accession of this architecture. &lt;a href=&quot;ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/architecture.txt.gz&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;architecture&lt;/code&gt; table&lt;/a&gt; has the following columns:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;auto_architecture&lt;/code&gt; (integer id, &lt;strong&gt;this is what we need&lt;/strong&gt;)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;architecture&lt;/code&gt; (a string describing the architecture)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;type_example&lt;/code&gt; (example sequence)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;no_seqs&lt;/code&gt; (number of sequences)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;architecture_acc&lt;/code&gt; (accessions of the architectures)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;You need the following line:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;3900719357      FAD_binding_2~Succ_DH_flav_C    Z9JRB3  7471    PF00890 PF02910&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now you need to get the sequences with this architecture id. First, download the &lt;a href=&quot;ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/database_files/pfamseq.txt.gz&quot; rel=&quot;nofollow noreferrer&quot;&gt;file with all the sequences&lt;/a&gt; (&lt;strong&gt;warning&lt;/strong&gt;, this one is more than 7Gb). With this one you need column #1 (pfam sequence accesion), #12 (sequence itself) and #16 (architecture integer id).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you do not want to import files in the database, you can use &lt;code&gt;awk&lt;/code&gt; to export all the sequences to &lt;code&gt;fasta&lt;/code&gt; format.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-bash prettyprint-override&quot;&gt;&lt;code&gt;zcat pfamseq.txt.gz | awk -F\\t '{ if ($16==&quot;3900719357&quot;) print &quot;&amp;gt;&quot; $1 &quot;\n&quot; $12}' &amp;gt; sequences.fasta&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Of couse, if you need to perform multiple queries like this, it is much easier to import tables into the database. Then you can perform both queries in a single SQL operation (assuming you know the architecture string):&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-sql prettyprint-override&quot;&gt;&lt;code&gt;SELECT pfamseq_acc, sequence&#xA;    FROM architecture&#xA;    JOIN pfamseq ON pfamseq.auto_architecture=architecture.auto_architecture&#xA;    WHERE architecture=&quot;FAD_binding_2~Succ_DH_flav_C&quot;;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(I haven't tried this SQL query, so maybe you need to correct it a bit)&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-07-05T14:35:18.587" LastActivityDate="2017-07-05T14:35:18.587" CommentCount="2" />
  <row Id="994" PostTypeId="1" CreationDate="2017-07-05T14:20:21.857" Score="3" ViewCount="79" Body="&lt;p&gt;I have 446 whole Klebsiella Pneumoniae genomes I want to build a phylogenetic tree from. After reading about constructing phylogenetic trees it seems the only option for large numbers of genomes is to isolate a gene with low variability from generation to generation and use this gene to build a tree. For example Lars Jensen recommends using &quot;16S rRNA [or] all ribosomal-protein-coding genes&quot; &lt;a href=&quot;https://www.biostars.org/p/1930/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.biostars.org/p/1930/&lt;/a&gt;. What program isolates these genes of interest from the whole genome fasta files and can put them into a multiple alignment file? Or outputs them in a formate ready for a multiple alignment program such as Muave? The reason I say a multiple alignment file is because this it the type of file most phylogenetic tree programs take (I.E. clonalframe).&lt;/p&gt;&#xA;" OwnerUserId="842" LastActivityDate="2017-07-05T20:39:21.993" Title="How to isolate genes from whole genomes for phylogenetic tree analysis?" Tags="&lt;gene&gt;&lt;phylogeny&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="1" />
  <row Id="995" PostTypeId="2" ParentId="994" CreationDate="2017-07-05T14:50:16.667" Score="1" Body="&lt;p&gt;There are a lot of ways to do this. I suggest using Prokka/Roary to produce a core genome alignment. There's a useful tutorial on the &lt;a href=&quot;https://sanger-pathogens.github.io/Roary/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Roary&lt;/a&gt; website:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for file in *&#xA;do&#xA;    prokka --kingdom Bacteria --outdir &quot;${file%%.*}&quot;  --genus Listeria --locustag &quot;${file%%.*}&quot; &quot;$file&quot;&#xA;    mv &quot;${file%%.*}&quot;/PROKKA_07052017.gff GFF/&quot;${file%%.*}&quot;.gff # use current date&#xA;done&#xA;roary -f Alignment -e -n -v GFF/*.gff&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Alignment/core_gene_alignment.aln can be used as input for phylogenetic analyses&lt;/p&gt;&#xA;" OwnerUserId="581" LastEditorUserId="298" LastEditDate="2017-07-05T20:39:21.993" LastActivityDate="2017-07-05T20:39:21.993" CommentCount="2" />
  <row Id="996" PostTypeId="2" ParentId="994" CreationDate="2017-07-05T14:54:52.480" Score="1" Body="&lt;p&gt;Extract desired gene sequences using &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi&quot; rel=&quot;nofollow noreferrer&quot;&gt;standalone blast&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simply provide a reference database with your desired output. &#xA;Set up your command and away you go. You can set the search up with a for loop for a batch of sequences. Command may look like &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for f in *.fasta; do&#xA;   f=$(basename $f .fasta)&#xA;   blastn \&#xA;   -outfmt &quot;6 sseqid qseq %&quot; \&#xA;   -query $f.fasta \&#xA;   -subject reference.fna \&#xA;   &amp;gt; out/$f.fas&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Watch the output as blast will spit out the gene as detected in + or - sense. If you want to only gather positive sense use &lt;code&gt;-strand&lt;/code&gt;option. &#xA;The default output I have here is tab output which requires a few sed commands to make into fasta.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sed -i \&#xA;-e 's/\s*$//g' \&#xA;-e 's/^/&amp;gt;/g'  \&#xA;-e 's/\s\+/\n/g' \&#xA;*.fas &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Online alignment servers are an easy way to align small datasets e.g. &lt;a href=&quot;http://www.ebi.ac.uk/Tools/msa/&quot; rel=&quot;nofollow noreferrer&quot;&gt;EBI&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="982" LastEditorUserId="982" LastEditDate="2017-07-05T20:13:16.533" LastActivityDate="2017-07-05T20:13:16.533" CommentCount="11" />
  <row Id="997" PostTypeId="1" CreationDate="2017-07-05T14:55:12.183" Score="6" ViewCount="90" Body="&lt;p&gt;Sometimes it useful to perform a nucleotide protein coding gene sequence alignment based on codons, not on individual nucleotides. For example for further codon model analysis it is important to have full codons.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A widely used approach here is to perform a protein sequence alignment first and then impose this alignment to the nucleotide sequences using &lt;a href=&quot;http://www.bork.embl.de/pal2nal/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PAL2NAL&lt;/a&gt;, &lt;a href=&quot;http://wbiomed.curtin.edu.au/bioinf/CodonAlign.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;CodonAlign&lt;/a&gt; or something similar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is how &lt;a href=&quot;https://dx.doi.org/10.1186/1471-2105-6-156&quot; rel=&quot;nofollow noreferrer&quot;&gt;transAlign&lt;/a&gt; or &lt;a href=&quot;http://guidance.tau.ac.il/ver2/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GUIDANCE&lt;/a&gt; (in codon mode) work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem here is that you are discarding part of the information which could be potentially used for the sequence alignment. E.g. if you have slowly evolving low-complexity region adjacent to a quickly evolving one, the amino acid induced alignment could be wrong, while incorporating nucleotide sequence potentially allows to make the alignment more accurate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm aware of two programs which can do true codon alignment. First, &lt;a href=&quot;http://wasabiapp.org/software/prank/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PRANK&lt;/a&gt; has a dedicated codon model, but it is rather slow and using it is overkill for certain problems. Second, &lt;a href=&quot;http://www.bioinformatics.org/sms2/pairwise_align_codons.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sequence Manipulation Suite&lt;/a&gt; can perform codon alignments, but only for a pair of sequences; also it's javascript based, therefore it is hard to run it for a large number of sequences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you recommend any software for multiple codon sequence alignment? Preferably available for offline use.&lt;/p&gt;&#xA;" OwnerUserId="191" LastEditorUserId="272" LastEditDate="2017-07-15T20:47:23.833" LastActivityDate="2017-07-22T11:05:35.870" Title="Which sequence alignment tools support codon alignment?" Tags="&lt;codon&gt;&lt;sequence-alignment&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="1" />
  <row Id="998" PostTypeId="1" AcceptedAnswerId="2284" CreationDate="2017-07-05T15:23:26.553" Score="4" ViewCount="50" Body="&lt;p&gt;I'm looking for a lightweight knowledgebase describing the human body to annotate disease sites. &#xA;I do not need a great level of detail, I just need a kind of basic organs/sub-organs taxonomy. I checked out the &lt;a href=&quot;http://si.washington.edu/projects/fma&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;Foundational Model of Anatomy ontology&lt;/em&gt;&lt;/a&gt;, but it's over-kill for my purposes and really cumbersome (Protege fails when it tries to load FMA.owl).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a simple online knowlegebase available for this?&lt;/p&gt;&#xA;" OwnerUserId="1063" LastEditorUserId="298" LastEditDate="2017-07-05T15:28:58.790" LastActivityDate="2017-08-12T21:00:41.833" Title="Human body sites knowledge base" Tags="&lt;database&gt;&lt;ontology&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="0" />
  <row Id="999" PostTypeId="2" ParentId="998" CreationDate="2017-07-05T15:33:55.510" Score="3" Body="&lt;p&gt;search in &lt;a href=&quot;https://bioportal.bioontology.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;bioportal&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example:  &lt;strong&gt;SNOMED&lt;/strong&gt;: &lt;a href=&quot;https://bioportal.bioontology.org/ontologies/SNOMEDCT?p=summary&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://bioportal.bioontology.org/ontologies/SNOMEDCT?p=summary&lt;/a&gt; , &lt;strong&gt;UBERON&lt;/strong&gt; : &lt;a href=&quot;https://bioportal.bioontology.org/ontologies/UBERON?p=summary&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://bioportal.bioontology.org/ontologies/UBERON?p=summary&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;please, note that &lt;strong&gt;FMA&lt;/strong&gt; &lt;a href=&quot;https://bioportal.bioontology.org/ontologies/FMA?p=summary&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://bioportal.bioontology.org/ontologies/FMA?p=summary&lt;/a&gt; is also available as RDF, csv , etc...  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;you could also try to convert FMA to obo and try to open it with &lt;a href=&quot;http://oboedit.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://oboedit.org/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="71" LastActivityDate="2017-07-05T15:33:55.510" CommentCount="3" />
  <row Id="1000" PostTypeId="2" ParentId="982" CreationDate="2017-07-05T20:28:07.993" Score="1" Body="&lt;p&gt;I've used PropKa in the past to get isoelectric points. Pretty simple to use:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/jensengroup/propka-3.1&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/jensengroup/propka-3.1&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="929" LastActivityDate="2017-07-05T20:28:07.993" CommentCount="0" />
  <row Id="1001" PostTypeId="1" CreationDate="2017-07-05T20:36:23.447" Score="3" ViewCount="51" Body="&lt;p&gt;I have a core genome dataset of approx 2530 genes for 149 taxa. I have run an unpartitioned phylogenetic analysis using iqtree. But I am unhappy with the resolution in some of the phylogeny. I want to check if partitioning the alignment  will improve resolution. My alignment is from roary output and MAFFT alignment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ive tried partitioning by loci and using iqtree (beta 1.6.4) to determine a model of best fit for each partition but this is apparently a very long demanding process as our HPC here keeps killing the analysis. What are other good methods/ means to partition DNA data?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My next course of action is to remove uninformative sites. iqtree does a quick analysis of each partition and reveals which are parsimony uninformative sites. Would I be just as well to delete these partitions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My current iqtree command is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;iqtree -nt AUTO  -s core_gene_alignment.aln -spp loci.nex  -m TESTMERGE -rcluster-max 100&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="982" LastEditorUserId="982" LastEditDate="2017-07-20T13:35:38.607" LastActivityDate="2017-07-20T13:35:38.607" Title="How to optimise core genome data for phylogenetic analysis [partition vs remove uninformative sites]" Tags="&lt;phylogeny&gt;&lt;software-recommendation&gt;&lt;phylogenetics&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="1002" PostTypeId="1" AcceptedAnswerId="1009" CreationDate="2017-07-05T22:18:06.357" Score="2" ViewCount="182" Body="&lt;p&gt;I was wondering if there is a tutorial or a small code snippet to understand how to write bioinformatics pipeline using python, for example&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;use a aligner (say hisat) &lt;/li&gt;&#xA;&lt;li&gt;get the output and process it using samtools &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I was able to use subprocess from python2.7 for this purpose using samtools but i am not able to link both the processes.i.e given path(which I can use argparse) for directory with fastq files output would be processed bam. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;sample code for samtools sam to bam :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import subprocess&#xA;subprocess.run(['samtools','view', '-bS',&#xA;                '../some.sam',&#xA;                '&amp;gt;',&#xA;                '../some.bam'])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="926" LastEditorUserId="931" LastEditDate="2017-07-06T11:22:58.393" LastActivityDate="2017-07-06T11:22:58.393" Title="using python to write bioinformatics pipelines tutorial" Tags="&lt;rna-seq&gt;&lt;samtools&gt;&lt;python&gt;&lt;genomics&gt;" AnswerCount="5" CommentCount="10" FavoriteCount="1" />
  <row Id="1003" PostTypeId="2" ParentId="1002" CreationDate="2017-07-05T22:30:23.793" Score="5" Body="&lt;p&gt;BioPython has some good tools for processing reads and alignments.&#xA;&lt;a href=&quot;http://biopython.org/DIST/docs/tutorial/Tutorial.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://biopython.org/DIST/docs/tutorial/Tutorial.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a python library wrapping samtools so many of the samtools calls can be used directly as python objects and calls&#xA;&lt;a href=&quot;https://pysam.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://pysam.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would use subprocess to call the aligner and specify the output to a bam file that you have named and then read that bam file with pysam to do the analysis that you are interested in.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you provide more specifics about the analysis or aligners I can help you more but you are in the right track just need to connect the two calls together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import subprocess&#xA;import pysam&#xA;import os &#xA;&#xA;for root, dirs, files in os.walk(rootPath):&#xA;    for filename in fnmatch.filter(files, &quot;.fastq&quot;):&#xA;&#xA;        subprocess.run([`hisat`, `-f`, filename `-o`, `some.bam`])&#xA;&#xA;        #get depth&#xA;        depth = pysam.depth('some.bam')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="964" LastEditorUserId="964" LastEditDate="2017-07-06T02:35:12.113" LastActivityDate="2017-07-06T02:35:12.113" CommentCount="1" />
  <row Id="1004" PostTypeId="2" ParentId="1002" CreationDate="2017-07-06T00:25:58.437" Score="3" Body="&lt;p&gt;If you're just doing alignment and conversion to a sorted BAM file, there's no need to run it through python. A simple pipe on the unix command line works just as well (and probably runs faster):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;hisat2 -x genome.index -1 reads_R1.fq.gz -2 reads_R2.fq.gz | &#xA;  samtools sort &amp;gt; reads_vs_genome.bam&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-07-06T02:51:18.897" LastActivityDate="2017-07-06T02:51:18.897" CommentCount="2" />
  <row Id="1005" PostTypeId="2" ParentId="969" CreationDate="2017-07-06T07:35:34.347" Score="2" Body="&lt;p&gt;Answer from the authors: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;PASA ends up using gff3 format instead of sam format for uploading the&#xA;  alignments.  If somebody has the gmap gff3 output, then can be done to&#xA;  just upload that directly using the custom alignment importer, but not&#xA;  with sam. They suggest just having PASA rerun GMAP as part of its&#xA;  regular routine.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="294" LastActivityDate="2017-07-06T07:35:34.347" CommentCount="0" />
  <row Id="1006" PostTypeId="2" ParentId="1002" CreationDate="2017-07-06T09:00:02.707" Score="0" Body="&lt;p&gt;Have a look at &lt;a href=&quot;https://github.com/spotify/luigi&quot; rel=&quot;nofollow noreferrer&quot;&gt;luigi&lt;/a&gt; for writing pipelines in python. You can found BioLuigi if you like :-P&lt;/p&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-07-06T09:00:02.707" CommentCount="3" />
  <row Id="1007" PostTypeId="1" AcceptedAnswerId="2050" CreationDate="2017-07-06T09:03:18.220" Score="2" ViewCount="128" Body="&lt;p&gt;I have a list of about 50k 'Protein IDs' from Reactome. Is there a simple way to get all the corresponding 'Pathway IDs' for each protein? What is the best service to use? (I'm guessing I can use the Reactome API, but I don't necessarily want to hit that 50k times...).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Protein IDs from reactome look like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;R-HSA-49155&#xA;R-HSA-199420&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The corresponding Reactome 'Pathway IDs' for those Protein IDs would be:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;R-HSA-49155  R-HSA-110331&#xA;R-HSA-49155  R-HSA-110330&#xA;R-HSA-49155  R-HSA-110357&#xA;R-HSA-199420 R-HSA-1660499&#xA;R-HSA-199420 R-HSA-202424&#xA;R-HSA-199420 R-HSA-199418&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="931" LastEditorUserId="931" LastEditDate="2017-07-07T09:00:10.593" LastActivityDate="2017-07-13T09:04:28.000" Title="Convert Reactome Protein IDs to Pathway IDs?" Tags="&lt;conversion&gt;&lt;identifiers&gt;&lt;reactome&gt;" AnswerCount="2" CommentCount="5" FavoriteCount="1" />
  <row Id="1008" PostTypeId="2" ParentId="1002" CreationDate="2017-07-06T09:12:57.450" Score="11" Body="&lt;p&gt;Taking a different tack from other answers, there's lots of tools for pipelines in Python. Note: there was a time when people would use &quot;pipeline&quot; to refer to a shell script. I'm talking about something more sophisticated that helps you decompose an analysis into parts and  runs it robustly.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Snakemake&lt;/strong&gt; is my favourite. It's (nearly) pure Python and can generate reports.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Nextflow&lt;/strong&gt; is growing in popularity and is pretty straightforward&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Ruffus&lt;/strong&gt; used to be reasonably popular, seemed fine when I used it&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Bpipe&lt;/strong&gt; is for bioinformatics&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Airflow&lt;/strong&gt; is a more industrial &quot;big&quot; solution but in wide use&lt;/li&gt;&#xA;&lt;li&gt;See this &lt;a href=&quot;https://github.com/pditommaso/awesome-pipeline&quot; rel=&quot;noreferrer&quot;&gt;big list of pipeline systems&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I'm sure you can find something there.&lt;/p&gt;&#xA;" OwnerUserId="377" LastActivityDate="2017-07-06T09:12:57.450" CommentCount="0" />
  <row Id="1009" PostTypeId="2" ParentId="1002" CreationDate="2017-07-06T09:53:12.087" Score="2" Body="&lt;p&gt;I &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/1008/191&quot;&gt;agree&lt;/a&gt; that using a specialized tool is probably a good idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nevertheless if you want to stick with Python, I suggest using &lt;a href=&quot;https://plumbum.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;plumbum&lt;/code&gt;&lt;/a&gt; instead of &lt;code&gt;subprocess&lt;/code&gt;. It has a very nice syntax for this kind of problems.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from plumbum import local&#xA;&#xA;# load a command&#xA;samtools = local['samtools']&#xA;&#xA;# you can then easily redirect&#xA;(samtools['view', '-bS', '../some.sam'] &amp;gt; '../some.bam')()&#xA;&#xA;# it is also easy to pipe &#xA;hisat2 = local['hisat2']&#xA;chain = hisat2['-x', 'genome.index', '-1', 'reads_R1.fq.gz', '-2', 'reads_R2.fq.gz'] | samtools['sort'] &amp;gt; 'reads_vs_genome.bam'&#xA;&#xA;chain()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="191" LastEditorUserId="191" LastEditDate="2017-07-06T11:16:07.240" LastActivityDate="2017-07-06T11:16:07.240" CommentCount="0" />
  <row Id="1010" PostTypeId="1" AcceptedAnswerId="1011" CreationDate="2017-07-07T10:02:31.870" Score="4" ViewCount="36" Body="&lt;p&gt;I have this read in my BAM file. It maps on chromosome 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I open this BAM file in IGV, and I can see the alignment on chromosome 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But when I open this file in R with Rsamtools:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bamContigsCel &amp;lt;- Rsamtools::scanBam('output/alignment/pacbio/bwa/ref     /bristolAssemblySorted.bam', param = Rsamtools::ScanBamParam(what = Rsamtools::scanBamWhat(), flag = Rsamtools::scanBamFlag(isMinusStrand = FALSE), tag = bamTags))[[1]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I then check if the read maps to chromosome 1 in my R object but I cannot find it.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bamContigsCel$rname[bamContigsCel$qname == '000000F|arrow']&#xA;[1] II II IV&#xA;Levels: I II III IV MtDNA V X&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But if I look at the BAM file, it's there. Why isn't Rsamtools importing my read into R?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ samtools view bristolAssemblySorted.bam | grep -n '000000F' | head  -c80&#xA;000000F|arrow   2064    I   336331  7   2926310H260M1774267H    *   0 0   GAAGCTGTCTAAACTTTGGC&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Cross-posted on &lt;a href=&quot;https://www.biostars.org/p/261365/&quot; rel=&quot;nofollow noreferrer&quot;&gt;biostars&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1075" LastEditorUserId="77" LastEditDate="2017-07-07T11:46:03.383" LastActivityDate="2017-07-09T20:07:48.103" Title="scanBam from Rsamtools is not importing one of my reads into R" Tags="&lt;r&gt;&lt;bam&gt;&lt;samtools&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1011" PostTypeId="2" ParentId="1010" CreationDate="2017-07-07T10:35:44.040" Score="3" Body="&lt;p&gt;Note the flag; that read is mapped in a reverse-complemented manner, so &lt;code&gt;isMinusStrand = FALSE&lt;/code&gt; is filtering it out. I tested this by making a BAM file with only that read:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@SQ SN:I    LN:1000000&#xA;000000F|arrow   2064    I   336331  7   2926310H260M1774267H    *   00  *   *&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then in R:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; library(Rsamtools)&#xA;&amp;gt; blah = scanBam(&quot;foo.bam&quot;, param=ScanBamParam(what=scanBamWhat(), flag=scanBamFlag(isMinusStrand = TRUE)))[[1]]&#xA;&amp;gt; blah$qname&#xA;[1] &quot;000000F|arrow&quot;&#xA;&amp;gt; blah = scanBam(&quot;foo.bam&quot;, param=ScanBamParam(what=scanBamWhat(), flag=scanBamFlag(isMinusStrand = FALSE)))[[1]]&#xA;&amp;gt; blah$qname&#xA;character(0)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="77" LastEditorUserId="48" LastEditDate="2017-07-09T20:07:48.103" LastActivityDate="2017-07-09T20:07:48.103" CommentCount="2" />
  <row Id="1012" PostTypeId="1" AcceptedAnswerId="1013" CreationDate="2017-07-07T12:43:54.240" Score="3" ViewCount="75" Body="&lt;p&gt;I am dealing with a list of genes which have been selected from a gene enrichment analysis. In order to see what kind of genes are overrepresented, I ran &lt;a href=&quot;https://github.com/jhcepas/eggnog-mapper&quot; rel=&quot;nofollow noreferrer&quot;&gt;eggnog-mapper&lt;/a&gt; to do an orthology assignment of each gene against a bacterial database (provided in eggnog). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;After running eggnog you get a .csv file containing for each gene which is the closest ortholog, and a list of GO, KEGG and COG terms associated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From there, I would like to do a Fisher-test (also considering Bonferroni correction) to highlight COG categories overrepresented in our dataset compared to our reference genome. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally my question: How do you deal with genes assigned to multiple COG categories (same would apply for GO terms). You count +1 to each of the COG categories assigned? Do you just choose one category per gene? &lt;/p&gt;&#xA;" OwnerUserId="446" LastEditorUserId="298" LastEditDate="2017-07-07T12:45:38.550" LastActivityDate="2017-07-07T12:48:31.647" Title="COG Annotation - Dealing with genes assigned to two or more COG categories" Tags="&lt;sequence-annotation&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="1013" PostTypeId="2" ParentId="1012" CreationDate="2017-07-07T12:48:31.647" Score="3" Body="&lt;p&gt;You add 1 to the count of each COG category. You are looking for over represented &lt;em&gt;COG categories&lt;/em&gt; so you must count them all. Many genes will be assigned to multiple categories. In fact, a majority of human genes are given multiple GO cellular function annotations. I have worked considerably in this field and one of the surprises I found is how common protein multifunctionality is. But I digress. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The point here is that you want to see whether any COG categories are over represented in your test set. Whether a gene is annotated to one or to a hundred categories is irrelevant. You are counting genes per COG, so you need to count each of the COGs assigned to a gene separately. &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-07-07T12:48:31.647" CommentCount="0" />
  <row Id="1014" PostTypeId="1" AcceptedAnswerId="1015" CreationDate="2017-07-07T12:52:14.280" Score="4" ViewCount="43" Body="&lt;p&gt;I'm working with scATAC-Seq data on the K562 cell line, which is supposed to be derived from a female patient. &lt;a href=&quot;https://www.nature.com/article-assets/npg/nature/journal/v523/n7561/extref/nature14590-s1.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;While following the scATAC-seq data analysis pipeline&lt;/a&gt;, after performing bowtie alignment they recommend filtering out all reads aligned to mitochondrial DNA and on the Y chromosome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Out of curiosity, I decided to count how many reads aligned to chrY and found that it can be quite high - chrY has approximately 10% as many reads as a similarly-sized chromosome (chr19 in hg19).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it normal to find reads mapping to chrY when the cell line is female-derived, or did I mess up somehow?&lt;/p&gt;&#xA;" OwnerUserId="1076" LastActivityDate="2017-07-07T12:58:00.847" Title="Y Chromosome Aligned Reads in scATAC-seq data from a female-derived cell line?" Tags="&lt;alignment&gt;&lt;bowtie2&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1015" PostTypeId="2" ParentId="1014" CreationDate="2017-07-07T12:58:00.847" Score="6" Body="&lt;p&gt;There are homologous regions between X an Y chromosomes: &lt;a href=&quot;https://en.wikipedia.org/wiki/Pseudoautosomal_region&quot; rel=&quot;noreferrer&quot;&gt;https://en.wikipedia.org/wiki/Pseudoautosomal_region&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is therefore normal to have some female-derived reads mapping in Y chromosome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You should probably check what proportion of such reads fall in other parts of the Y chromosome than pseudoautosomal regions.&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-07-07T12:58:00.847" CommentCount="1" />
  <row Id="1016" PostTypeId="1" CreationDate="2017-07-07T19:10:32.453" Score="1" ViewCount="24" Body="&lt;p&gt;Does anyone know if there is a program/library/script in R or Python that takes as input a list of proteins/peptides and a list of post-translational modifications (PTMs; like oxidation of methionine and acetylation of cysteine), and as output returns a matrix/list with the imported peptides and all possible locations of the selected modifications?&lt;/p&gt;&#xA;" OwnerUserId="704" LastEditorUserId="96" LastEditDate="2017-07-08T11:43:12.400" LastActivityDate="2017-07-08T12:50:08.357" Title="Software to produce a table of post-translational modifications from a peptide list" Tags="&lt;r&gt;&lt;proteins&gt;&lt;python&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1017" PostTypeId="2" ParentId="1016" CreationDate="2017-07-08T12:50:08.357" Score="1" Body="&lt;p&gt;I don't know of a tool that can do exactly what you want. However, there are various tools that can predict specific classes of post translational modifications. For example:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Phosphorylation: &lt;a href=&quot;http://gps.biocuckoo.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GPS&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Computational prediction of phosphorylation sites with their cognate protein kinases (PKs) is greatly helpful for further experimental design. Although ~10 online predictors were developed, the PK classification and control of false positive rate (FPR) were not well addressed. Here we adopted a well-established rule to classify PKs into a hierarchical structure with four levels. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Mitochondrial import: &lt;a href=&quot;http://ihg.gsf.de/ihg/mitoprot.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mitoprot&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;MitoProt calculates the N-terminal protein region that can support a Mitochondrial Targeting Sequence and the cleavage site. A complete description of the method to make the prediction is available in: M.G. Claros, P. Vincens. Computational method to predict mitochondrially imported proteins and their targeting sequences. Eur. J. Biochem. 241, 779-786 (1996). &lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Acetylation: &lt;a href=&quot;http://bdmpail.biocuckoo.org/prediction.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;&quot;PAIL: Prediction of Acetylation on Internal Lysines&quot;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In this work, we present a novel online predictor for protein acetylation sites prediction of PAIL, Prediction of Acetylation on Internal Lysines. We have manually mined scientific literature to collect 249 experimentally verified acetylation sites of 92 distinct proteins. Then the BDM (Bayesian Discriminant Method) algorithm has been employed.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;You can use tools like the above and then combine the results to get the sort of matrix you are asking for. &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-07-08T12:50:08.357" CommentCount="0" />
  <row Id="1018" PostTypeId="1" CreationDate="2017-07-08T13:35:12.137" Score="3" ViewCount="34" Body="&lt;p&gt;I'm using velvet to align given reads of RNA to given CDSs (i.e. coding areas and genes) of an organism, so I can generate gene-expression profiles. But after using &lt;code&gt;velvetg out-dir/ -alignment yes&lt;/code&gt;, velvet produces &lt;code&gt;contig-alignment.psa&lt;/code&gt; which is a file in a strange format. &#xA;The file for every contig contains zero or more records of 7 integers, and I cannot understand their meaning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following is an excerpt of the file, and you can see the entire file &lt;a href=&quot;https://paste.ofcode.org/d9599cGH3aXnbdGqtWqrMs&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;contig_1&#xA;1    193    1939    1939    6   194 2&#xA;195 244 1   1   6   22  71&#xA;&amp;gt;contig_2&#xA;1   84  1   1   6   59  142&#xA;86  170 1935    1935    6   22  106&#xA;172 285 1935    1935    6   108 221&#xA;&amp;gt;contig_3&#xA;1   98  2   2   6   1   98&#xA;100 321 2   2   6   100 321&#xA;334 415 1204    1204    6   1   82&#xA;&amp;gt;contig_15&#xA;1   23  3   3   6   84  106&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="872" LastEditorUserId="298" LastEditDate="2017-07-08T15:44:04.793" LastActivityDate="2017-07-11T11:45:54.160" Title="How to interpret contig-alignment.psa produced by velvet" Tags="&lt;rna-alignment&gt;&lt;read-alignment&gt;&lt;velvet&gt;" AnswerCount="0" CommentCount="6" />
  <row Id="1019" PostTypeId="1" AcceptedAnswerId="2019" CreationDate="2017-07-08T14:24:13.377" Score="3" ViewCount="89" Body="&lt;p&gt;I'm trying to compute a gene expression profile for an organism. I have gene nucleotide sequences of the mentioned organism stored in a fasta file and a set of paired reads stored in two separate files with the same fasta format. Now I want to compute the coverage of every gene by the given reads. That is, on average how many reads cover a given segment of the sequence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any tool for this specific purpose?&#xA;I'm trying to use the velvet sequencer but I have encountered a &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/1018/how-to-interpret-contig-alignment-psa-produced-by-velvet&quot;&gt;few problems&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="872" LastEditorUserId="872" LastEditDate="2017-07-08T19:08:23.327" LastActivityDate="2017-07-08T21:50:31.803" Title="How can I compute gene expression for a set of RNA reads?" Tags="&lt;rna-seq&gt;&lt;rna-alignment&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="0" />
  <row Id="2019" PostTypeId="2" ParentId="1019" CreationDate="2017-07-08T16:22:43.643" Score="6" Body="&lt;p&gt;It's unclear if your paired-end reads are actually in fasta format, I'll presume that they're in fastq instead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The easiest tool to use is &lt;a href=&quot;http://salmon.readthedocs.io/en/latest/&quot; rel=&quot;noreferrer&quot;&gt;salmon&lt;/a&gt;, which nicely deals with things like multimapping. If you're trying to judge the quality of an assembly, then I recommend having a look at &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/27252236&quot; rel=&quot;noreferrer&quot;&gt;transrate&lt;/a&gt;, which uses some related methodology for assessing contig quality.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-08T16:22:43.643" CommentCount="3" />
  <row Id="2020" PostTypeId="2" ParentId="1019" CreationDate="2017-07-08T19:17:51.123" Score="1" Body="&lt;p&gt;In addition to salmon that was already mentioned, you can also try &lt;a href=&quot;https://pachterlab.github.io/kallisto/starting&quot; rel=&quot;nofollow noreferrer&quot;&gt;kallisto&lt;/a&gt; or &lt;a href=&quot;https://github.com/bli25ucb/RSEM_tutorial&quot; rel=&quot;nofollow noreferrer&quot;&gt;RSEM&lt;/a&gt;, which are also fairly popular/respectable and will work with a transcriptome FASTA.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-07-08T19:17:51.123" CommentCount="0" />
  <row Id="2021" PostTypeId="2" ParentId="1019" CreationDate="2017-07-08T21:50:31.803" Score="2" Body="&lt;p&gt;It's not possible to compute absolute expression from RNASeq reads if they are processed in the usual way, where a sequencer produces the same number of reads regardless of the input RNA amount. At best, RNASeq will give you an indication of &lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004075&quot; rel=&quot;nofollow noreferrer&quot;&gt;proportional expression&lt;/a&gt; within a single sample. For this reason, relative expression (i.e. that used by differential expression tests) is easier to determine than absolute expression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The closest approximation to absolute expression is to generate an expression relative to the average expression of a set of housekeeping genes, but I don't think there's a universal set that has been decided on. Gene expression, even for common housekeeping genes, can vary depending on the environmental conditions of the cell. For example, GAPDK is involved in &lt;a href=&quot;http://www.nature.com/nri/journal/v16/n9/full/nri.2016.70.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;immune cell activation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, as long as experimental conditions are similar, and you're not planning on looking for statistical significance, the proportional expression can still give qualitative insights into how cell populations behave in relation to other populations. DESeq2 provides a &lt;a href=&quot;https://rdrr.io/bioc/DESeq2/man/varianceStabilizingTransformation.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;variance-stabilising transformation function&lt;/a&gt; that minimises variation for small-count genes, assuming that each sample has roughly the same total expression. I have found that I get better outcomes / comparisons from this transformation when carrying out a further adjustment to account for gene length (i.e. divide by the length of the longest transcript for each gene). See our &lt;a href=&quot;http://jem.rupress.org/content/early/2016/12/01/jem.20160470#materials-methods&quot; rel=&quot;nofollow noreferrer&quot;&gt;Th2 paper&lt;/a&gt;, section &quot;Read mapping and differential expression analysis&quot; for more information. The &quot;transcripts-per-million&quot; values produced by &lt;a href=&quot;https://pachterlab.github.io/kallisto/about.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kallisto&lt;/a&gt; and &lt;a href=&quot;https://combine-lab.github.io/salmon/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Salmon&lt;/a&gt; provide measures similar to this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If, on the other hand, you were able to modify the experimental design, single cell sequencing (or &quot;known cell count&quot; sequencing) can be used for determining absolute expression: use a spiked-in transcript that is added in proportion to the &lt;em&gt;cell count&lt;/em&gt;, so that results can be compared in proportion to the expression for that transcript.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-07-08T21:50:31.803" CommentCount="0" />
  <row Id="2022" PostTypeId="1" AcceptedAnswerId="2030" CreationDate="2017-07-09T03:06:16.293" Score="2" ViewCount="78" Body="&lt;p&gt;I have been searching for a long time and the furthest I got is some database with the functional description for genes. Then, I have to parse these descriptions manually to figure out the association. Moreover, for some species, there are not even any functional descriptions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need this information for mice, rice, and Arabidopsis Thaliana. Can anyone help me? &lt;/p&gt;&#xA;" OwnerUserId="1082" LastEditorUserId="57" LastEditDate="2017-07-09T16:27:02.363" LastActivityDate="2017-07-11T09:10:06.243" Title="Where can I find a database that has phenotype information together with associated SNPs?" Tags="&lt;gene&gt;&lt;gwas&gt;" AnswerCount="3" CommentCount="2" FavoriteCount="0" />
  <row Id="2023" PostTypeId="2" ParentId="974" CreationDate="2017-07-09T19:15:01.990" Score="4" Body="&lt;p&gt;You can do this in &lt;a href=&quot;https://hail.is&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hail&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from hail import *&#xA;hc = HailContext()&#xA;(hc.import_vcf('test.vcf')&#xA; .filter_variants_expr('gs.exists(g =&amp;gt; g.ad[1:].exists(d =&amp;gt; d &amp;gt; 10))')&#xA; .export_vcf('filtered.vcf'))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This works with any number of samples and will keep the variants where at least one sample has a genotype with an alternate allele support by more than 10 reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To verify we got the expected 6 variants:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; hc.import_vcf('filtered.vcf').count()&#xA;(1L, 6L)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://hail.is/hail/hail.VariantDataset.html#hail.VariantDataset.count&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;count&lt;/code&gt;&lt;/a&gt; returns the number of samples (1) and number of variants (6).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Take a look at the &lt;a href=&quot;https://hail.is/hail/getting_started.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;getting started page&lt;/a&gt; or &lt;a href=&quot;https://hail.is/hail/tutorials-landing.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;tutorials&lt;/a&gt; if you want to try it out!&lt;/p&gt;&#xA;" OwnerUserId="1024" LastActivityDate="2017-07-09T19:15:01.990" CommentCount="0" />
  <row Id="2024" PostTypeId="1" CreationDate="2017-07-09T21:31:41.673" Score="0" ViewCount="51" Body="&lt;p&gt;Are there any examples of researchers data mining articles and papers (e.g., from pubmed or google scholar) in order to derive relations between genes and diseases?  &lt;/p&gt;&#xA;" OwnerUserId="734" LastEditorUserId="77" LastEditDate="2017-07-09T21:59:22.660" LastActivityDate="2017-07-09T22:50:16.190" Title="Using data mining of papers in order to derive genomic connections" Tags="&lt;gene&gt;&lt;genome&gt;&lt;cancer&gt;&lt;data-mining&gt;" AnswerCount="2" CommentCount="9" FavoriteCount="1" ClosedDate="2017-07-09T23:00:13.187" />
  <row Id="2025" PostTypeId="2" ParentId="2024" CreationDate="2017-07-09T22:32:46.213" Score="2" Body="&lt;p&gt;There are many databases that have used publication scraping for oncogenic gene fusions. There are publications for the individual methods they used for their scraping and aggregation.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;COSMIC - &lt;a href=&quot;http://cancer.sanger.ac.uk/cosmic&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://cancer.sanger.ac.uk/cosmic&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;TICdb - &lt;a href=&quot;http://www.unav.es/genetica/TICdb/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.unav.es/genetica/TICdb/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;ChimerDB - &lt;a href=&quot;http://ercsb.ewha.ac.kr/fusiongene/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://ercsb.ewha.ac.kr/fusiongene/&lt;/a&gt; -&#xA;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/27899563&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/pubmed/27899563&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="964" LastActivityDate="2017-07-09T22:32:46.213" CommentCount="0" />
  <row Id="2026" PostTypeId="2" ParentId="2024" CreationDate="2017-07-09T22:50:16.190" Score="2" Body="&lt;p&gt;This existed as a closed silo, at least in 2015. Qiagen has a team of hired students and Post-Docs for collating research papers into their &lt;a href=&quot;https://www.qiagenbioinformatics.com/products/qiagen-knowledge-base/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Knowledge Base&lt;/a&gt;, an extensive database that is integrated into a few of their commercial products. Qiagen's claim is that by providing a consistently-structured and well-formatted database, the process of research discovery is accelerated. I've got a few notes from a seminar they gave us a couple of years ago:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The curation is better than a free / public database because free databases typically have wrong or inconsistent data&lt;/li&gt;&#xA;&lt;li&gt;Their knowledge base is an army of hired MDs and PhDs, 5.3M &quot;findings&quot; as of Nov 05 2015&lt;/li&gt;&#xA;&lt;li&gt;Scientific articles have unstructured data, so expert knowledge is required to process the data into a &quot;finding&quot; for their knowledge base&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I was already doubtful about their approach, and this doubt was magnified when they started getting into the details of their process. I started to lose trust and write my own thoughts about it:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The knowledge base they have created is private&lt;/li&gt;&#xA;&lt;li&gt;The approach is to use a function (i.e. MDs/PhDs) to convert research articles into scientifically-validated associations. However, the function is effectively a black box; Qiagen allow the experts to decide for themselves how a particular article should be converted into a finding.&lt;/li&gt;&#xA;&lt;li&gt;The findings in the knowledge base have no strength or likelihood.&lt;/li&gt;&#xA;&lt;li&gt;The Experts were encouraged to only enter findings that were &quot;generally accepted by the scientific community. but it seemed like only a single expert was required to get a finding into the knowledge base (i.e. findings weren't curated by other people).&lt;/li&gt;&#xA;&lt;li&gt;When using their software (e.g. IPA), p-values of 10^-15 and 10^-22 are reported; these numbers are meaningless.&lt;/li&gt;&#xA;&lt;li&gt;The team didn't look at methylation, phosphorylation, etc.; plans are in place to hire additional experts to look at this at a later date.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;After listening to their seminar, I realised that Qiagen's idea of a general database that attempts to capture everything isn't going to work well. Experts will disagree about whether a particular association is valid, relevance (or correctness) will differ depending on the area of interest, and the established correctness of a piece of information can change over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be possible to create something that might work by adding a lot more metadata to all the &quot;findings&quot; in such a database, but it would need to be a huge, public, collaborative effort. Something on the scale of SRA/ENA, with encouragement from the scientific community to enter details into the database prior to publications being accepted/indexed. If the people who wrote papers entered their findings in such a database, other researchers wouldn't need to be hired to try to understand papers and guess at the findings of published research.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-07-09T22:50:16.190" CommentCount="1" />
  <row Id="2027" PostTypeId="2" ParentId="2022" CreationDate="2017-07-10T06:56:52.493" Score="2" Body="&lt;p&gt;A few days ago, I was trying to find some GWAS datasets to download. I hope this site for 3000 rice will help: &lt;a href=&quot;http://snp-seek.irri.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://snp-seek.irri.org/&lt;/a&gt;. You can click &lt;a href=&quot;http://snp-seek.irri.org/_snp.zul&quot; rel=&quot;nofollow noreferrer&quot;&gt;Genotype&lt;/a&gt; in the index page for &quot;Query for SNPs from the 3000 genome project&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; With the help of user manual, I can get IRIS ID from &lt;a href=&quot;http://snp-seek.irri.org/_snp.zul&quot; rel=&quot;nofollow noreferrer&quot;&gt;Genotype&lt;/a&gt;. According to this &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/12855438&quot; rel=&quot;nofollow noreferrer&quot;&gt;paper&lt;/a&gt;, iris id may associate with some phenotype information. But the website of IRIS is under maintenance, I can not get more information about phenotype.  &lt;/p&gt;&#xA;" OwnerUserId="1085" LastEditorUserId="1085" LastEditDate="2017-07-11T06:26:50.940" LastActivityDate="2017-07-11T06:26:50.940" CommentCount="2" />
  <row Id="2028" PostTypeId="1" AcceptedAnswerId="2033" CreationDate="2017-07-10T09:40:34.390" Score="1" ViewCount="18" Body="&lt;p&gt;MEDIPS is an established tool with functions for the quality control and analysis of data derived from immunoprecipitation (IP)-seq samples, like Methylation IP sequencing datasets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to know if there are any other tools I should consider nowadays that could do a better job and hence replace MEDIPS for the analysis of MeDIP datasets.&lt;/p&gt;&#xA;" OwnerUserId="180" LastActivityDate="2017-07-11T02:29:51.997" Title="alternatives to MEDIPS to analyse MeDIP datasets" Tags="&lt;methylation&gt;&lt;medips&gt;&lt;medip-seq&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2029" PostTypeId="1" CreationDate="2017-07-10T12:02:01.193" Score="2" ViewCount="35" Body="&lt;p&gt;I'm looking for a Linked Open Data approach to annotate a dataset with human genome and drugs information.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to the &lt;a href=&quot;http://lod-cloud.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Linked Open Data cloud&lt;/a&gt;, there are a lot of interconnected &lt;code&gt;RDF&lt;/code&gt; vocabularies devoted to life sciences but, unfortunately, a lot of them seem out of date.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I recently found &lt;a href=&quot;https://www.wikidata.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikidata&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Wikidata is a free and open knowledge base that can be read and edited&#xA;  by both humans and machines.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It seems exactly what Linked Data should be and it seems continuously updated. At a first sight, it covers very well genomic and drugs knowledge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does Anyone know if Wikidata is a reliable source for genomic and drugs annotation?&lt;/p&gt;&#xA;" OwnerUserId="1063" LastEditorUserId="1063" LastEditDate="2017-07-10T22:59:23.797" LastActivityDate="2017-07-10T22:59:23.797" Title="Is Wikidata a reliable KB for genomic and drugs annotations?" Tags="&lt;annotation&gt;&lt;public-databases&gt;&lt;data-management&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2030" PostTypeId="2" ParentId="2022" CreationDate="2017-07-10T16:25:32.420" Score="5" Body="&lt;p&gt;&lt;a href=&quot;http://www.mousephenotype.org/&quot; rel=&quot;noreferrer&quot;&gt;International Mouse Phenotyping Consortium&lt;/a&gt; is building a database of phenotypes and knock-outs of mouse. I believe that this database will be fairly complete (20000 knock-outs), but these are knock-outs, not SNPs... There are several mouse GWAS studies, but I am not aware of a database that would pull all the results together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Arabidopsis&lt;/em&gt; big GWAS project is called &lt;a href=&quot;http://1001genomes.org/tools/&quot; rel=&quot;noreferrer&quot;&gt;1001 genomes project&lt;/a&gt;. A lot more information about &lt;em&gt;Arabidopsis&lt;/em&gt; from individual experiments is collected and maintained by &lt;a href=&quot;https://www.arabidopsis.org/&quot; rel=&quot;noreferrer&quot;&gt;The Arabidopsis Information Resource&lt;/a&gt;, however this resource is subscription based, therefore what you need might not be available for free.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As mentioned, SNPs of rice were identified and linked to different rice groups in &lt;a href=&quot;https://gigascience.biomedcentral.com/articles/10.1186/2047-217X-3-7&quot; rel=&quot;noreferrer&quot;&gt;The 3,000 rice genomes project&lt;/a&gt;. There is also &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/27322545&quot; rel=&quot;noreferrer&quot;&gt;GWAS&lt;/a&gt; done on 512 individuals, not sure if it is sufficient for reliable conclusions (even they claim that they found supersigificant links).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this what you were searching for?&lt;/p&gt;&#xA;" OwnerUserId="57" LastActivityDate="2017-07-10T16:25:32.420" CommentCount="1" />
  <row Id="2031" PostTypeId="1" CreationDate="2017-07-10T17:05:03.750" Score="2" ViewCount="42" Body="&lt;p&gt;I need to install &lt;a href=&quot;http://www2.ub.es/dnasp/download.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DnaSp&lt;/a&gt;,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but its not working on my mac. I'm using wine to install it using &lt;a href=&quot;http://www2.ub.es/dnasp/download.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;these instructions for wine&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;it successfully installs but then I get this error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Mollies-MacBook-Air:dnasp51001 molliepassacantando$ WINEPREFIX=~/.wine64 wine regsvr32.exe scrrun.dll mfc40.dll threed32.ocx&#xA;regsvr32: Successfully registered DLL 'scrrun.dll'&#xA;regsvr32: Successfully registered DLL 'mfc40.dll'&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xA;err:typelib:sltg_get_typelib_ref Unable to find reference&#xA;regsvr32: Successfully registered DLL 'threed32.ocx'&#xA;Mollies-MacBook-Air:dnasp51001 molliepassacantando$ winedevice.exe(30828,0x405ed000) malloc: *** error for object 0x401cea08: pointer being freed was not allocated&#xA;*** set a breakpoint in malloc_error_break to debug&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and cannot run it. has anyone successfully installed DnaSp on a Mac or know of an alternate program I could use? I need to be able to calculate haplotype frequencies.&lt;/p&gt;&#xA;" OwnerUserId="1088" LastEditorUserId="1088" LastEditDate="2017-07-10T19:20:42.660" LastActivityDate="2017-07-10T19:20:42.660" Title="How to install DnaSP on a Mac" Tags="&lt;genome&gt;&lt;dna&gt;&lt;dnasp&gt;&lt;software-installation&gt;" AnswerCount="0" CommentCount="7" />
  <row Id="2032" PostTypeId="2" ParentId="2029" CreationDate="2017-07-10T17:15:42.777" Score="1" Body="&lt;p&gt;The question is what you mean with &quot;reliable&quot;. For bacterial genome annotations there is one Wikidata based platform called &lt;a href=&quot;http://wikigenomes.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;WikiGenomes&lt;/a&gt; (&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/28365742&quot; rel=&quot;nofollow noreferrer&quot;&gt;publications&lt;/a&gt;).&lt;/p&gt;&#xA;" OwnerUserId="579" LastActivityDate="2017-07-10T17:15:42.777" CommentCount="2" />
  <row Id="2033" PostTypeId="2" ParentId="2028" CreationDate="2017-07-11T02:29:51.997" Score="1" Body="&lt;p&gt;I guess QSEA should be an obvious answer:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;QSEA (quantitative sequencing enrichment analysis) was developed as&#xA;  the successor of the MEDIPS package for analyzing data derived from&#xA;  methylated DNA immunoprecipitation (MeDIP) experiments followed by&#xA;  sequencing (MeDIP-seq).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/vignettes/qsea/inst/doc/qsea_tutorial.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://bioconductor.org/packages/release/bioc/vignettes/qsea/inst/doc/qsea_tutorial.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-07-11T02:29:51.997" CommentCount="0" />
  <row Id="2034" PostTypeId="1" AcceptedAnswerId="2046" CreationDate="2017-07-11T07:03:13.770" Score="2" ViewCount="36" Body="&lt;p&gt;I am trying to reorder scaffolds of a rice species, but no genetic map is available right now. &lt;em&gt;Oryza sativa Japonica&lt;/em&gt; is a close relative of this rice species. Mummer was used to do a whole genome alignment, and I am trying to reorder scaffolds according the mummer result. But I found it's too complicated to do this by myself, many scaffolds may align to multiple Oryza sativa Japonica chromosomes or multiple loci in the same chromosome.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What software can I use to handle this task?&lt;/p&gt;&#xA;" OwnerUserId="1085" LastEditorUserId="298" LastEditDate="2017-07-11T08:49:31.670" LastActivityDate="2017-07-11T15:15:50.387" Title="Reordering scaffolds according to a reference without a genetic map" Tags="&lt;alignment&gt;&lt;assembly&gt;&lt;software-recommendation&gt;&lt;scaffold&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2035" PostTypeId="2" ParentId="2034" CreationDate="2017-07-11T07:17:43.397" Score="4" Body="&lt;p&gt;Have you tried &lt;a href=&quot;http://darlinglab.org/mauve/mauve.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mauve&lt;/a&gt; alignment? Its pretty easy once you become familiar and has a GUI for further ease of use. Additionally there are a few online tutorials on how to re-order contigs/ scaffolds using this software. Heres &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2723005/&quot; rel=&quot;nofollow noreferrer&quot;&gt;one I use&lt;/a&gt; when in need.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mauve contains a function called &lt;a href=&quot;http://darlinglab.org/mauve/user-guide/reordering.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mauve Contig Mover (MCM)&lt;/a&gt; which can be used to a) compare an assembly to a reference and/or b) rearrange contigs to improve assembly quality.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use the tutorial as above which uses the GUI for Mauve. I&lt;/p&gt;&#xA;" OwnerUserId="982" LastEditorUserId="982" LastEditDate="2017-07-11T09:55:52.823" LastActivityDate="2017-07-11T09:55:52.823" CommentCount="2" />
  <row Id="2036" PostTypeId="1" AcceptedAnswerId="2039" CreationDate="2017-07-11T07:47:50.980" Score="2" ViewCount="166" Body="&lt;p&gt;Looking for tools to reconcile alignment file of experimental transcripts mapped to genome (SAM/BAM) with the reference transcriptome annotation (GTF) from Ensembl (organism: &lt;em&gt;D. melanogaster&lt;/em&gt;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The aim would be to check which transcripts reported in the alignment file are also reported in the reference annotation, generating a new annotation (e.g., GTF) including the new transcripts as seen in the alignment file (experimental transcripts to genome).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any idea?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-- Edit: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Up to now, I tried an in-house script that does the following: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;generate a BED file from the BAM file with the exon coordinates (start/end) per transcript - using &lt;a href=&quot;https://github.com/bedops/bedops&quot; rel=&quot;nofollow noreferrer&quot;&gt;BEDOPS v2p4p27&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;convert the reference GTF to BED to retrieve the coordinates per annotated feature &lt;/li&gt;&#xA;&lt;li&gt;compare the experimental vs the reference BED intervals&lt;/li&gt;&#xA;&lt;li&gt;infer the suite of annotated exons per experimental transcript, thus, ideally, the isoform it corresponds to&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;However, with this simple approach, I only get around 20% of the experimental transcripts that can be identified in all their length using reference annotation. This might be due either to a too simple approach (e.g. if the start/end of the exon in the experimental transcript is shifted by 1 base as compared to the reference, it won't be detected), or to a real biological signal (since I am also using data from long-read sequencing technologies).&lt;/p&gt;&#xA;" OwnerUserId="294" LastEditorUserId="294" LastEditDate="2017-07-11T08:30:36.360" LastActivityDate="2017-07-11T09:01:26.063" Title="tools to reconcile experimental transcripts with reference annotation" Tags="&lt;alignment&gt;&lt;transcriptome&gt;&lt;software-recommendation&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="2038" PostTypeId="2" ParentId="2036" CreationDate="2017-07-11T08:44:12.137" Score="3" Body="&lt;p&gt;As per my answer to @_julien_roux on twitter:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Trying to find novel transcripts within the context of an existing annotation is much less straightforward. You probably need to do a &quot;genome-guided assembly&quot; with Trinity and PASA:&#xA;&lt;a href=&quot;http://pasapipeline.github.io/#A_ComprehensiveTranscriptome&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://pasapipeline.github.io/#A_ComprehensiveTranscriptome&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We did something similar in much simpler organisms in our recent paper: &#xA;&lt;a href=&quot;http://dx.doi.org/10.1186/s12864-017-3505-0&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://dx.doi.org/10.1186/s12864-017-3505-0&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your biggest problems are going to be alternative spliced gene models, the large amount of fragmentation inherent in de novo transcriptome assembly and largely underreported non-coding RNAs in most genomes. It was easier for us as the dictyostelids have few and small introns - fly is going to be a different ball-game. Be prepared to spend a lot of time optimising parameters before finding a result you're happy with (maybe) ;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You may think the fly genome pretty good and complete, but be prepared to find lots of errors, corrections and novelty. We were very surprised at the amount of new annotation we were able to find in &lt;em&gt;D. discoideum&lt;/em&gt; - an organism where every single gene had been manually curated!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Good luck!&lt;/p&gt;&#xA;" OwnerUserId="1092" LastEditorUserId="1092" LastEditDate="2017-07-11T09:01:26.063" LastActivityDate="2017-07-11T09:01:26.063" CommentCount="2" />
  <row Id="2039" PostTypeId="2" ParentId="2036" CreationDate="2017-07-11T08:44:31.813" Score="4" Body="&lt;p&gt;I've never tried this myself, so I don't know how easy this is...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One option would be to start with &lt;a href=&quot;http://research-pub.gene.com/gmap/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GMAP&lt;/a&gt;, which is meant to align whole transcripts against the genome. The really nice thing about this is that it can directly produce GFF3 files. You can then use that with your Ensembl GTF with &lt;code&gt;cuffcompare&lt;/code&gt; or whatever the equivalent is in stringTie. You should then be able to get the information you want from the &lt;a href=&quot;http://cole-trapnell-lab.github.io/cufflinks/cuffcompare/#transfrag-class-codes&quot; rel=&quot;nofollow noreferrer&quot;&gt;transfrag codes&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-11T08:44:31.813" CommentCount="1" />
  <row Id="2040" PostTypeId="2" ParentId="2022" CreationDate="2017-07-11T09:10:06.243" Score="1" Body="&lt;p&gt;Have you looked at Ensembl? At least for mice, they have pretty rich data for SNPs e.g.&#xA;&lt;a href=&quot;http://www.ensembl.org/Mus_musculus/Variation/Explore?db=core;r=2:3225281-3226281;v=rs27096498;vdb=variation;vf=802773&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.ensembl.org/Mus_musculus/Variation/Explore?db=core;r=2:3225281-3226281;v=rs27096498;vdb=variation;vf=802773&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assumes you have known dbSNP ids (or 'rs' numbers) for your SNPs. If not you could use their VEP tool to annotate them:&#xA;&lt;a href=&quot;http://www.ensembl.org/Homo_sapiens/Tools/VEP?db=core&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.ensembl.org/Homo_sapiens/Tools/VEP?db=core&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;VEP is also available for rice and Arabidopsis:&#xA;&lt;a href=&quot;http://plants.ensembl.org/Oryza_sativa/Tools/VEP?db=core&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://plants.ensembl.org/Oryza_sativa/Tools/VEP?db=core&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1092" LastActivityDate="2017-07-11T09:10:06.243" CommentCount="1" />
  <row Id="2041" PostTypeId="2" ParentId="488" CreationDate="2017-07-11T09:18:11.997" Score="1" Body="&lt;p&gt;By far my preferred option for doing this manually is PICR:&#xA;&lt;a href=&quot;http://www.ebi.ac.uk/Tools/picr/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.ebi.ac.uk/Tools/picr/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW it is not &quot;ridiculous&quot; to get different numbers of genes reported for a given set of proteins. For several reasons:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Uniprot IDs can disappear, merge or be split&lt;/li&gt;&#xA;&lt;li&gt;not all uniprot and gene IDs have a 1-to-1 relationship&lt;/li&gt;&#xA;&lt;li&gt;depending on species some gene symbols can be ambiguous or synonymous.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="1092" LastActivityDate="2017-07-11T09:18:11.997" CommentCount="0" />
  <row Id="2043" PostTypeId="1" AcceptedAnswerId="2044" CreationDate="2017-07-11T13:10:56.437" Score="9" ViewCount="121" Body="&lt;p&gt;Biopython's &lt;code&gt;.count()&lt;/code&gt; methods, like Python's &lt;code&gt;str.count()&lt;/code&gt;, perform a non-overlapping count, how can I do an overlapping one?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, these code snippets &lt;code&gt;return 2&lt;/code&gt;, but I want the answer &lt;code&gt;3&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from Bio.Seq import Seq&#xA;&amp;gt;&amp;gt;&amp;gt; Seq('AAAA').count('AA')&#xA;2&#xA;&amp;gt;&amp;gt;&amp;gt; 'AAAA'.count('AA')&#xA;2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="104" LastEditorUserId="104" LastEditDate="2017-08-10T14:41:41.410" LastActivityDate="2017-08-14T08:58:49.933" Title="How can I do an overlapping sequence count in Biopython?" Tags="&lt;biopython&gt;&lt;python&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="2044" PostTypeId="2" ParentId="2043" CreationDate="2017-07-11T13:10:56.437" Score="17" Body="&lt;p&gt;For Biopython 1.70, there is a new &lt;code&gt;Seq.count_overlap()&lt;/code&gt; method, which includes optional &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; arguments:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from Bio.Seq import Seq&#xA;&amp;gt;&amp;gt;&amp;gt; Seq('AAAA').count_overlap('AA')&#xA;3&#xA;&amp;gt;&amp;gt;&amp;gt; Seq('AAAA').count_overlap('AA', 1, 4)&#xA;2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This method is also implemented for the &lt;code&gt;MutableSeq&lt;/code&gt; and &lt;code&gt;UnknownSeq&lt;/code&gt; classes:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from Bio.Seq import MutableSeq, UnknownSeq&#xA;&amp;gt;&amp;gt;&amp;gt; MutableSeq('AAAA').count_overlap('AA')&#xA;3&#xA;&amp;gt;&amp;gt;&amp;gt; UnknownSeq(4, character='A').count_overlap('AA')&#xA;3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Disclaimer: I co-contributed the &lt;code&gt;.count_overlap()&lt;/code&gt; methods with Peter Cock, see &lt;a href=&quot;https://github.com/biopython/biopython/commit/97709cc7a4b8591e794b442796d41e4f7b855a0f&quot; rel=&quot;noreferrer&quot;&gt;&lt;code&gt;97709cc&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="104" LastActivityDate="2017-07-11T13:10:56.437" CommentCount="0" />
  <row Id="2045" PostTypeId="2" ParentId="628" CreationDate="2017-07-11T15:01:38.167" Score="2" Body="&lt;p&gt;Short answer: you can't.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Neural networks use positive and negative examples to add weights to the neural network architecture that is provided to it. Trying to deconvolve the meaning behind the weights is nigh-on impossible except for the simplest perceptron.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is common to many machine learning algorithms: they work very much like black boxes. One exception is decision trees. They will report what features are used to classify the positive vs negative datasets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, as with all ML methods, you need to be very careful of your training datasets. This is especially true of motif searching and especially for negative datasets. It's quite easy to find a negative dataset which is completely inappropriate for learning and give you misleading accuracies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like @user172818, I would try traditional methods as they can work well, again, if given appropriate data. The &lt;a href=&quot;http://meme-suite.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;MEME-suite&lt;/a&gt; would be a good start.&lt;/p&gt;&#xA;" OwnerUserId="1092" LastActivityDate="2017-07-11T15:01:38.167" CommentCount="2" />
  <row Id="2046" PostTypeId="2" ParentId="2034" CreationDate="2017-07-11T15:15:50.387" Score="2" Body="&lt;p&gt;I've been using &lt;a href=&quot;http://kmer.sourceforge.net/wiki/index.php/Getting_Started_with_ATAC&quot; rel=&quot;nofollow noreferrer&quot;&gt;ATAC&lt;/a&gt; to successfully align different versions of the rice genome assembly. There is a chance it will work for you between two closely related species (potato and tomato work well for example).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's not massively user friendly, but it's very fast, and it produces long, 'clean' alignments (i.e. almost all 1 to 1 alignments). The output format is a bit cryptic (zero-based start position and length instead of the more usual 1-based start and end positions), but as I said, it should basically output the order for you in about 30 minutes.&lt;/p&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-07-11T15:15:50.387" CommentCount="1" />
  <row Id="2048" PostTypeId="1" CreationDate="2017-07-11T20:06:56.393" Score="1" ViewCount="42" Body="&lt;p&gt;UPDATE: I found the solution. I was using normalized values and GEO was using raw beta values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to link GEOquery and minfi. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically I want to obtain beta values from the idat files on GEOquery. I was following this guide: &lt;a href=&quot;https://kasperdanielhansen.github.io/genbioconductor/html/minfi.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://kasperdanielhansen.github.io/genbioconductor/html/minfi.html&lt;/a&gt;, up until the preprocessing part. Meaning that I was able to obtain the RGset. Then I used my own preprocessing code to obtain the beta values. However, I cross checked them with the beta values that were on GEO and they weren't consistent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, the accession number I used was GSE68777. So I went to that study on GEO and clicked on the first sample: &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM1681154&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM1681154&lt;/a&gt;. Then I scrolled down and clicked &quot;Download full table&quot; to download the samples and beta values in a text file. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I typed head(beta) and chose the first sample. Then I did command F in the text file for that sample and it's value there wasn't the same as the value from the beta data table. Hopefully you can help find the error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the code I'm using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(GEOquery)&#xA;library(minfi)&#xA;library(&quot;IlluminaHumanMethylation450kanno.ilmn12.hg19&quot;)&#xA;library(&quot;IlluminaHumanMethylation450kmanifest&quot;)&#xA;&#xA;######## Code copied from 450k Guide ########&#xA;&#xA;getGEOSuppFiles(&quot;GSE68777&quot;)&#xA;untar(&quot;GSE68777/GSE68777_RAW.tar&quot;, exdir = &quot;GSE68777/idat&quot;)&#xA;head(list.files(&quot;GSE68777/idat&quot;, pattern = &quot;idat&quot;))&#xA;idatFiles &amp;lt;- list.files(&quot;GSE68777/idat&quot;, pattern = &quot;idat.gz$&quot;, full = TRUE)&#xA;sapply(idatFiles, gunzip, overwrite = TRUE)&#xA;rgSet &amp;lt;- read.metharray.exp(&quot;GSE68777/idat&quot;)&#xA;geoMat &amp;lt;- getGEO(&quot;GSE68777&quot;)&#xA;pD.all &amp;lt;- pData(geoMat[[1]])&#xA;pD &amp;lt;- pD.all[, c(&quot;title&quot;, &quot;geo_accession&quot;, &quot;characteristics_ch1.1&quot;, &quot;characteristics_ch1.2&quot;)]&#xA;names(pD)[c(3,4)] &amp;lt;- c(&quot;group&quot;, &quot;sex&quot;)&#xA;pD$group &amp;lt;- sub(&quot;^diagnosis: &quot;, &quot;&quot;, pD$group)&#xA;pD$sex &amp;lt;- sub(&quot;^Sex: &quot;, &quot;&quot;, pD$sex)&#xA;sampleNames(rgSet) &amp;lt;- sub(&quot;.*_5&quot;, &quot;5&quot;, sampleNames(rgSet))&#xA;rownames(pD) &amp;lt;- pD$title&#xA;pD &amp;lt;- pD[sampleNames(rgSet),]&#xA;pData(rgSet) &amp;lt;- pD&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1025" LastEditorUserId="1025" LastEditDate="2017-07-17T18:58:28.627" LastActivityDate="2017-07-17T18:58:28.627" Title="Minfi returning incorrect beta values" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;normalization&gt;&lt;methylation&gt;&lt;data-mining&gt;" AnswerCount="0" CommentCount="5" FavoriteCount="0" />
  <row Id="2049" PostTypeId="1" AcceptedAnswerId="2075" CreationDate="2017-07-11T21:14:20.717" Score="1" ViewCount="73" Body="&lt;p&gt;My goal is to make a conservation plot between bacterial sequences and a human protein.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, I have a FASTA file of the protein, and a FASTA file with the sequences of the proteins from the BLAST results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my initial attempts, I have tried to do this using msa software:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have downloaded clustalx and run a profile alignment using the single protein sequence as Profile 1 and the sequences from the BLAST results as Profile 2, and selected the option &quot;Align Sequences to Profile 1&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm having difficulty interpreting and dealing with the results. Since the majority of the sequences aren't aligned, the results are a total mess, with a huge number of dashes added.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to visualize the number of BLAST hits per amino acid in my protein of interest. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;ie: &lt;img src=&quot;https://i.stack.imgur.com/L8jL5.jpg&quot; alt=&quot;alignment&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to have a graph with my protein on the x-axis and a plot like the regions of high conservation for the multiple alignments, except with the spikes corresponding to a high number of BLAST hits. This would allow me to identify regions of my protein that have higher sequence similarity to bacteria than others.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a better way to achieve this or to salvage the results from the profile alignment?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;" OwnerUserId="727" LastEditorUserId="272" LastEditDate="2017-07-15T20:47:11.947" LastActivityDate="2017-07-16T13:37:45.500" Title="Profile of conservation between bacterial sequences and human protein" Tags="&lt;sequence-alignment&gt;" AnswerCount="1" CommentCount="9" />
  <row Id="2050" PostTypeId="2" ParentId="1007" CreationDate="2017-07-11T21:19:03.517" Score="6" Body="&lt;p&gt;If you don't mind hitting it 50k times and are OK with python3...&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;from urllib import request&#xA;import json&#xA;&#xA;def getPathways(proteinID):&#xA;    baseURL = 'http://reactome.org/ContentService/data/query'&#xA;    PathwayIDs = set()&#xA;    try:&#xA;        response = request.urlopen('{}/{}'.format(baseURL, proteinID)).read().decode()&#xA;        data = json.loads(response)&#xA;        if 'consumedByEvent' in data:&#xA;            for event in data['consumedByEvent']:&#xA;                PathwayIDs.add(event['stId'])&#xA;        if 'producedByEvent' in data:&#xA;            for event in data['producedByEvent']:&#xA;                PathwayIDs.add(event['stId'])&#xA;    except:&#xA;        pass&#xA;    return PathwayIDs&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Usage would then be something like:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;l = ['R-HSA-49155', 'R-HSA-199420', '']&#xA;for rid in l:&#xA;    ids = getPathways(rid)&#xA;    for _ in ids:&#xA;        print(&quot;{}\t{}&quot;.format(rid, _))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which would produce:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;R-HSA-49155 R-HSA-110239&#xA;R-HSA-49155 R-HSA-110240&#xA;R-HSA-49155 R-HSA-110238&#xA;R-HSA-49155 R-HSA-110356&#xA;R-HSA-199420    R-HSA-8948800&#xA;R-HSA-199420    R-HSA-6807106&#xA;R-HSA-199420    R-HSA-6807206&#xA;R-HSA-199420    R-HSA-6807126&#xA;R-HSA-199420    R-HSA-8847968&#xA;R-HSA-199420    R-HSA-8850997&#xA;R-HSA-199420    R-HSA-2321904&#xA;R-HSA-199420    R-HSA-8948775&#xA;R-HSA-199420    R-HSA-8944497&#xA;R-HSA-199420    R-HSA-6807134&#xA;R-HSA-199420    R-HSA-8850945&#xA;R-HSA-199420    R-HSA-8873946&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note that this will silently ignore invalid or missing IDs such as &lt;code&gt;''&lt;/code&gt; (that's the &lt;code&gt;try&lt;/code&gt; and &lt;code&gt;except&lt;/code&gt; above. Note also that these are different pathway IDs than what you provided in your example. The main reason is that the protein IDs you showed are not always involved in the pathways IDs you showed (in my example, they always are).&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-11T21:19:03.517" CommentCount="1" />
  <row Id="2051" PostTypeId="1" AcceptedAnswerId="2052" CreationDate="2017-07-11T22:01:38.863" Score="1" ViewCount="26" Body="&lt;p&gt;I'm looking at Reduced representation bisulfite sequencing (RRBS) data from ENCODE, and to align the FASTQ files I've used Bismark with Bowtie 1. When I load the resulting BAM file into &lt;code&gt;R&lt;/code&gt; with &lt;code&gt;Rsamtools&lt;/code&gt; and &lt;code&gt;GenomicRanges&lt;/code&gt;, I get something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; reads&#xA;GRanges object with 2 ranges and 5 metadata columns:&#xA;      seqnames         ranges strand |                     seq                                   XM          XR          XG        NM&#xA;         &amp;lt;Rle&amp;gt;      &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt; |          &amp;lt;DNAStringSet&amp;gt;                          &amp;lt;character&amp;gt; &amp;lt;character&amp;gt; &amp;lt;character&amp;gt; &amp;lt;integer&amp;gt;&#xA;  [1]     chrM [16523, 16558]      - | ATAAAACCTA...CCCTTAAATA .....h........h.......z.............          CT          GA         3&#xA;  [2]     chrM [16524, 16559]      + | TAAAGTTTAA...TTTTAAATAA .....hh.......hhh.h.z...hhhh........          CT          CT        11&#xA;  -------&#xA;  seqinfo: 25 sequences from an unspecified genome&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For the read on the - strand (which is aligned to the &quot;G-&gt;A&quot;-converted reference), how should I compare methylation calls to those on the + strand, since they don't line up?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, the &lt;code&gt;z&lt;/code&gt; in the methylation string of the - strand is one position ahead of the &lt;code&gt;z&lt;/code&gt; in the + strand (which makes sense because of symmetric CpG methylation). But how should I determine whether these two methylation calls are &quot;essentially the same&quot; or not?&lt;/p&gt;&#xA;" OwnerUserId="240" LastEditorUserId="57" LastEditDate="2017-07-14T21:06:46.883" LastActivityDate="2017-07-14T21:06:46.883" Title="How to interpret methylation calls from Bismark on opposite strands?" Tags="&lt;r&gt;&lt;samtools&gt;&lt;methylation&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2052" PostTypeId="2" ParentId="2051" CreationDate="2017-07-11T22:22:01.657" Score="3" Body="&lt;p&gt;The strand the bismark reports is related to the strand from which the read originated, not necessarily how it's aligned. So, alignments on the + strand shouldn't have calls overlapping those on the - strand, since you can't have a C in the same place on the same strand. One should often see Z/z next to each other on opposite strand, like in your example, since these are CpG (so it should be a Z/z on the + strand and then a Z/z on the following base on the - strand). Thus, in your example you have two reads supporting unmethylation for the CpG as a whole (1 read for each of the Cs).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most confusing thing about BSseq is that reads (or paired-end reads) are only ever informative for a single strand. This is actually true for all sequencing with Illumina instruments (single-stranded fragments are loaded, after all), but since strands are almost always complementary we can usually ignore this fact.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an aside, you might find &lt;a href=&quot;https://github.com/dpryan79/MethylDackel&quot; rel=&quot;nofollow noreferrer&quot;&gt;MethylDackel&lt;/a&gt; useful (full disclosure, I wrote it). It'll be much faster at extracting methylation calls than bismark and supports nice things like excluding regions of bias methylation and likely variant positions.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-11T22:22:01.657" CommentCount="2" />
  <row Id="2053" PostTypeId="2" ParentId="2043" CreationDate="2017-07-12T03:18:23.897" Score="8" Body="&lt;p&gt;I've encountered this problem before, and used python &lt;code&gt;re&lt;/code&gt; module to solve this problem.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import re&#xA;all = re.findall(r'(?=(AA))','AAAA')&#xA;counts = len(all)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You can get more details in this &lt;a href=&quot;https://stackoverflow.com/questions/11430863/how-to-find-overlapping-matches-with-a-regexp&quot;&gt;thread&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1085" LastEditorUserId="1085" LastEditDate="2017-07-12T05:17:53.587" LastActivityDate="2017-07-12T05:17:53.587" CommentCount="1" />
  <row Id="2054" PostTypeId="1" CreationDate="2017-07-12T08:14:19.597" Score="5" ViewCount="35" Body="&lt;p&gt;I have recently generated a genome-guided transcriptome with &lt;a href=&quot;https://github.com/trinityrnaseq/trinityrnaseq/wiki&quot; rel=&quot;noreferrer&quot;&gt;Trinity&lt;/a&gt;, and would like to apply an additional filter to exclude transcripts that don't have good support from the RNASeq reads. This is with the goal of trying to reduce the initial dataset down to something a bit more manageable (I have about 300k transcripts covering 250Mb in total, but would prefer about 1/10 of that number).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see that Trinity has a &lt;a href=&quot;https://github.com/trinityrnaseq/trinityrnaseq/wiki/RNA-Seq-Read-Representation-by-Trinity-Assembly&quot; rel=&quot;noreferrer&quot;&gt;workflow using Bowtie2&lt;/a&gt; for evaluating the quality of assembled transcripts, but Bowtie2 is getting a bit old in the mapping world now. Are any of the super-fast transcript mappers (e.g. &lt;a href=&quot;https://combine-lab.github.io/salmon/&quot; rel=&quot;noreferrer&quot;&gt;Salmon&lt;/a&gt; or &lt;a href=&quot;https://pachterlab.github.io/kallisto/about&quot; rel=&quot;noreferrer&quot;&gt;Kallisto&lt;/a&gt;) appropriate to use for transcript filtering?&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-07-13T08:44:04.673" Title="Filter Trinity transcriptome based on RNASeq reads" Tags="&lt;rna-seq&gt;&lt;read-mapping&gt;" AnswerCount="2" CommentCount="4" FavoriteCount="1" />
  <row Id="2055" PostTypeId="2" ParentId="2054" CreationDate="2017-07-12T08:41:00.213" Score="4" Body="&lt;p&gt;I used a combination of BUSCO and Salmon to filter transcripts based on their abundance in the RNASeq read dataset. The approach was roughly as follows:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Run BUSCO in short/transcript mode on the Trinity-generated sequences&lt;/li&gt;&#xA;&lt;li&gt;Run Salmon on the Trinity-generated sequences, using the RNASeq reads that were used to generate the transcriptome&lt;/li&gt;&#xA;&lt;li&gt;Merge the BUSCO full results table with the Salmon results table to determine distribution of the number of reads mapped for BUSCO genes&lt;/li&gt;&#xA;&lt;li&gt;Choose an appropriate &quot;real&quot; read count threshold based on the read count distribution for BUSCO genes&lt;/li&gt;&#xA;&lt;li&gt;Sub-select the transcripts based on this threshold, using the Salmon results table&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h2&gt;Step 1 -- BUSCO&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;python ~/install/busco/BUSCO.py -i ../Trinity-GG.fasta -o BUSCO_Trinity-GG_nematodes -l ~/install/busco/nematoda_odb9 -m tran -c 10&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Step 2 -- Salmon&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;~/install/salmon/Salmon-0.8.2_linux_x86_64/bin/salmon index -t Trinity-GG.fasta -i Trinity-GG.fasta.sai&#xA;~/install/salmon/Salmon-0.8.2_linux_x86_64/bin/salmon quant -i Trinity-GG.fasta.sai -1 reads-all_R1_trimmed.fastq.gz -2 reads-all_R2_trimmed.fastq.gz -p 10 -o quant/reads-all_quant -l A&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Step 3 -- Results merge (using R)&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/Rscript&#xA;busco.table &amp;lt;-&#xA;    read.delim(&quot;BUSCO/run_BUSCO_Trinity-GG_nematodes/full_table_BUSCO_T-BNOCFED_nematodes.tsv&quot;, comment.char=&quot;#&quot;,&#xA;               header=FALSE, stringsAsFactors = FALSE,&#xA;               col.names=c(&quot;Busco id&quot;,&quot;Status&quot;,&quot;Sequence&quot;,&quot;Score&quot;,&quot;Length&quot;));&#xA;busco.table &amp;lt;- subset(busco.table, Status != &quot;Missing&quot;);&#xA;&#xA;count.table &amp;lt;- read.delim(&quot;quant/reads-all_quant_ISR/quant.sf&quot;,&#xA;                          stringsAsFactors=FALSE);&#xA;&#xA;busco.expr.df &amp;lt;- merge(busco.table, count.table,&#xA;                       by.x=&quot;Sequence&quot;, by.y=&quot;Name&quot;, all.x=TRUE);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Step 4 -- Count thresholding (visual exploration)&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pdf(&quot;busco_NumReads.pdf&quot;, paper=&quot;a4r&quot;, width=11, height=8);&#xA;options(scipen=10);&#xA;busco.numReads &amp;lt;- sort(tapply(busco.expr.df$NumReads, busco.expr.df$Busco.id,&#xA;                              function(x){&#xA;                                  exp(mean(log(x[x&amp;gt;0])))}), decreasing=TRUE);&#xA;par(mfrow=c(1,2));&#xA;plot(x=seq(0,1,length.out=length(busco.numReads)),&#xA;     busco.numReads, log=&quot;y&quot;, ylab=&quot;Number of Mapped Reads&quot;,&#xA;     yaxt=&quot;n&quot;,&#xA;     xlab=&quot;Proportion of Complete BUSCO sequences&quot;);&#xA;axis(2, at=rep(c(1,2,5),each=6) * 10^(0:5), las=2, cex.axis=0.71);&#xA;abline(col=&quot;#00000020&quot;, h=rep(1:9,each=6) * 10^(0:5), lwd=0.1);&#xA;abline(col=&quot;#80808080&quot;, h=rep(1,each=6) * 10^(0:5), lwd=2);&#xA;rect(ybottom=10^(par(&quot;usr&quot;)[3]), ytop=10^(par(&quot;usr&quot;)[4]),&#xA;     xleft=0.9, xright=par(&quot;usr&quot;)[2],&#xA;     col=&quot;#80808020&quot;, border=NA);&#xA;plot(x=seq(0,1,length.out=length(busco.numReads)), xlim=c(0.9,1),&#xA;     busco.numReads, log=&quot;y&quot;, ylab=&quot;Number of Mapped Reads&quot;, yaxt=&quot;n&quot;,&#xA;     xlab=&quot;Proportion of Complete BUSCO sequences&quot;);&#xA;axis(2, at=rep(c(1,2,5),each=6) * 10^(0:5), las=2, cex.axis=0.71);&#xA;abline(col=&quot;#00000020&quot;, h=rep(1:9,each=6) * 10^(0:5), lwd=0.1);&#xA;abline(col=&quot;#80808080&quot;, h=rep(1,each=6) * 10^(0:5), lwd=2);&#xA;invisible(dev.off());&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/7iw47.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/7iw47.png&quot; alt=&quot;choosing read count threshold from BUSCO genes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looks like a value of 50 will be suitable.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Step 5 -- Sub-select the transcripts&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat(file=&quot;HighCount_Transcripts_Num50.txt&quot;,&#xA;    subset(count.table, NumReads &amp;gt;= 50)$Name, sep=&quot;\n&quot;);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then filter using a suitable &lt;a href=&quot;https://github.com/gringer/bioinfscripts/blob/master/fastx-fetch.pl&quot; rel=&quot;nofollow noreferrer&quot;&gt;FASTA selection utility&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pv Trinity-GG.fasta | ~/scripts/fastx-fetch.pl -i HighCount_Transcripts_Num50.txt &amp;gt; HighCount50_Trinity-GG.fasta&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-07-12T08:54:27.503" LastActivityDate="2017-07-12T08:54:27.503" CommentCount="2" />
  <row Id="2056" PostTypeId="1" CreationDate="2017-07-12T09:36:21.243" Score="2" ViewCount="32" Body="&lt;p&gt;For DNA/RNA quantification machines like the Bioanalyzer or TapeStation, the DNA Integrity Number (DIN) or RNA Integrity Number (RIN) numbers are quoted as a measure of the fragmentation of the material.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How is the DNA Integrity Number (DIN) calculated in Bioanalyzer/TapeStation? Is there any open-source software to parse/collate the data from the machines?&lt;/p&gt;&#xA;" OwnerUserId="180" LastActivityDate="2017-07-17T02:10:55.023" Title="how is the DNA Integrity Number (DIN) calculated in Bioanalyzer/TapeStation?" Tags="&lt;dna&gt;&lt;quantification&gt;&lt;bioanalyzer&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="2057" PostTypeId="2" ParentId="2056" CreationDate="2017-07-12T09:38:45.987" Score="0" Body="&lt;p&gt;The best I could find by looking at github.com is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/brianfleharty/Agilent-BioAnalyzer-RIN-R-code-for-Fly/blob/master/RNA_RIN.R&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/brianfleharty/Agilent-BioAnalyzer-RIN-R-code-for-Fly/blob/master/RNA_RIN.R&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;#set the working directory&#xA;dir&amp;lt;- &quot;H:\\B_RIN\\emd&quot;&#xA;setwd(dir)&#xA;myfiles&amp;lt;-list.files()&#xA;which(substr(myfiles,69,79)==&quot;Results.csv&quot;)&#xA;results&amp;lt;-myfiles[which(substr(myfiles,69,79)==&quot;Results.csv&quot;)]&#xA;n &amp;lt;- which(substr(myfiles,69,79)==&quot;Results.csv&quot;)&#xA;targets &amp;lt;- read.delim(myfiles[n[1]], sep=&quot;,&quot;,header=F,skip=14,as.is=T)&#xA;length(n)&#xA;targets2&amp;lt;- read.delim(myfiles[n[1]], sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xA;str1&amp;lt;-substr(targets2[1,2],1,67)&#xA;str1&#xA;str2&amp;lt;-&quot;_S&quot;&#xA;str3&amp;lt;-paste(str1,str2,sep=&quot;&quot;)&#xA;str3&#xA;chip_num&amp;lt;-length(n)&#xA;samp_num_chip&amp;lt;-length(grep(str3,myfiles))&#xA;samp_num&amp;lt;-length(which(substr(myfiles,69,74)==&quot;Sample&quot;))&#xA;samples1&amp;lt;- matrix(NA, ncol=chip_num, nrow=samp_num)&#xA;for(i in 1:chip_num)&#xA;{&#xA;#mod chip numb&#xA;targets2&amp;lt;- read.delim(myfiles[n[i]], sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xA;str1&amp;lt;-substr(targets2[1,2],1,67)&#xA;str2&amp;lt;-&quot;_S&quot;&#xA;str3&amp;lt;-paste(str1,str2,sep=&quot;&quot;)&#xA;samp_num_chip&amp;lt;-length(grep(str3,myfiles))&#xA;#mod sample numb&#xA;str1&amp;lt;-substr(targets2[1,2],1,67)&#xA;str4&amp;lt;-&quot;_Sample&quot;&#xA;for(j in 1:samp_num_chip)&#xA;{&#xA;iii&amp;lt;-j&#xA;end.st&amp;lt;-&quot;.csv&quot;&#xA;str5&amp;lt;-paste(str1,str4,iii,end.st,sep=&quot;&quot;)&#xA;samples1[j,i]&amp;lt;-str5&#xA;}&#xA;}&#xA;samples1&#xA;grep(&quot;2100&quot;,samples1)&#xA;samples1&amp;lt;-samples1[grep(&quot;2100&quot;,samples1)]&#xA;samples1&#xA;write.table(samples1,file=paste(dir,&quot;\\ba_lane.txt&quot;,sep=&quot;&quot;), sep=&quot;\t&quot;,col.names=NA)&#xA;#RIN Calculator#######refined peakfinder####################&#xA;#read bioanalyzer data into a matrix called dta&#xA;#since the total RNA and mRNA assays run differently&#xA;#skip more lines for the total RNA assay&#xA;ramp &amp;lt;- colorRamp(c(&quot;darkmagenta&quot;,&quot;white&quot;))&#xA;elec &amp;lt;- read.delim(&quot;ba_lane.txt&quot;, sep=&quot;\t&quot;,header=T,as.is=T)&#xA;qc.mat &amp;lt;- matrix(NA, ncol=1, nrow=nrow(elec))&#xA;dta &amp;lt;- matrix(NA, nrow=1060, ncol=nrow(elec))&#xA;for(i in 1:nrow(elec))&#xA;{&#xA;x &amp;lt;- read.csv(as.character(elec[i,2]), header=F, skip=18, nrows=1060)&#xA;dta[,i] &amp;lt;- x[,2]&#xA;}&#xA;time&amp;lt;- x[,1]&#xA;#plot ladder and define peak threshold in HD&#xA;par(mfrow=c(3,4))&#xA;for(samples in 1:length(samples1))&#xA;#for(samples in 1:nrow(elec))&#xA;{&#xA;#which(dta[,lad] &amp;gt; 5)&#xA;#draw some segments and store max fluor in max.peaks matrix&#xA;lad&amp;lt;-samples&#xA;ti1&amp;lt;-17&#xA;ti2&amp;lt;-65&#xA;p.row&amp;lt;-length(dta[which(time==ti1):which(time==ti1+1),lad])*(ti2-ti1+1)&#xA;peaks &amp;lt;- matrix(NA, nrow=p.row, ncol=1)&#xA;max.peaks &amp;lt;- matrix(NA, nrow=length(seq(ti1,ti2,.05)), ncol=1)&#xA;for (i in 1:length(seq(ti1,ti2,.05)))&#xA;{&#xA;ii&amp;lt;-seq(ti1,ti2,.05)[i]&#xA;x0&amp;lt;- ii&#xA;x0&amp;lt;-round(x0,digits=3)&#xA;y0&amp;lt;- ii&#xA;x&amp;lt;- ii+.05&#xA;x&amp;lt;-round(x,digits=3)&#xA;y&amp;lt;- ii&#xA;#segments(x0,y0,x,y)&#xA;max.peaks[i]&amp;lt;- max(dta[which(time==x0):which(time==x),lad])&#xA;}    &#xA;#human or mouse&#xA;cent&amp;lt;-max(max.peaks)*.6&#xA;#flatworm&#xA;#cent&amp;lt;-max(max.peaks)*.05&#xA;plot(time,dta[,lad],type=&quot;h&quot;,col=&quot;dodgerblue&quot;,xlab=&quot;seconds&quot;,ylab=&quot;Fluorescence Units&quot;,xlim=c(17,75),ylim=c(0,(max(max.peaks)*1.2)),main=elec[lad,1])&#xA;abline(h=cent,col=&quot;grey&quot;)&#xA;#define the middle of the segments&#xA;max.avg&amp;lt;-seq(ti1+.025,ti2+.025,.05)&#xA;#plot the maximum value of each segment&#xA;max.peaks&#xA;lines(max.avg,max.peaks,type=&quot;b&quot;,col=&quot;deepskyblue&quot;)&#xA;#lines(max.avg,min.peaks,type=&quot;b&quot;,col=&quot;blue&quot;)&#xA;#find time associated with peaks &amp;gt; 5 fu and fill seconds matrix with them&#xA;#12.5% of max fu incase chip is fu&#xA;p1&amp;lt;-which(max.avg==38)&#xA;p2&amp;lt;-which(max.avg==55)&#xA;sizes &amp;lt;- which(max.peaks&amp;gt;cent)&#xA;max.avg[sizes]&#xA;gt35&amp;lt;-which(max.avg[sizes]&amp;gt;35)&#xA;lt55&amp;lt;-which(max.avg[sizes]&amp;lt;55)&#xA;#as.numeric(duplicated(c(gt35,lt55)))&#xA;tfr&amp;lt;-which(duplicated(c(gt35,lt55))==TRUE)&#xA;c(gt35,lt55)[tfr]&#xA;max.avg[sizes][c(gt35,lt55)[tfr]]&#xA;max.peaks[sizes][c(gt35,lt55)[tfr]]&#xA;points(max.avg[sizes+1][c(gt35,lt55)[tfr]],max.peaks[sizes+1][c(gt35,lt55)[tfr]],col=&quot;lawngreen&quot;,pch=19)&#xA;points(max.avg[sizes-1][c(gt35,lt55)[tfr]],max.peaks[sizes-1][c(gt35,lt55)[tfr]],col=&quot;lawngreen&quot;,pch=19)&#xA;points(max.avg[sizes][c(gt35,lt55)[tfr]],max.peaks[sizes][c(gt35,lt55)[tfr]],col=&quot;deeppink3&quot;,pch=19)&#xA;#colnms2&amp;lt;-c(&quot;max.avg[sizes]&quot;,&quot;max.peaks[sizes]&quot;,&quot;max.avg[sizes+1]&quot;,&quot;max.peaks[sizes+1]&quot;,&quot;max.avg[sizes-1]&quot;)&#xA;max.avg[sizes]&#xA;max.peaks[sizes]&#xA;max.avg[sizes+1]&#xA;max.peaks[sizes+1]&#xA;max.avg[sizes-1]&#xA;max.peaks[sizes-1]&#xA;area&amp;lt;-cbind(max.peaks[sizes-1][c(gt35,lt55)[tfr]],max.peaks[sizes][c(gt35,lt55)[tfr]],max.peaks[sizes+1][c(gt35,lt55)[tfr]])&#xA;time2&amp;lt;-cbind(max.avg[sizes-1][c(gt35,lt55)[tfr]],max.avg[sizes][c(gt35,lt55)[tfr]],max.avg[sizes+1][c(gt35,lt55)[tfr]])&#xA;p1&amp;lt;-area[which(area[,1]&amp;lt;cent),1][1]&#xA;t1&amp;lt;-time2[which(area[,1]&amp;lt;cent),1][1]&#xA;p2&amp;lt;-area[which(area[,1]&amp;lt;cent),1][2]&#xA;t2&amp;lt;-time2[which(area[,1]&amp;lt;cent),1][2]&#xA;#area[which(area[,2]&amp;lt;cent),2]&#xA;#area[which(area[,2]&amp;lt;cent),2]&#xA;p3&amp;lt;-area[which(area[,3]&amp;lt;cent),3][1]&#xA;t3&amp;lt;-time2[which(area[,3]&amp;lt;cent),3][1]&#xA;p4&amp;lt;-area[which(area[,3]&amp;lt;cent),3][2]&#xA;t4&amp;lt;-time2[which(area[,3]&amp;lt;cent),3][2]&#xA;#abline(v=area[which(area[,3]&amp;lt;cent),3])&#xA;#abline(h=11)&#xA;#abline(v=42.75)&#xA;v1&amp;lt;-t1&#xA;v2&amp;lt;-t2&#xA;v3&amp;lt;-t3&#xA;v4&amp;lt;-t4&#xA;abline(v=v1)&#xA;abline(v=v2)&#xA;abline(v=v3)&#xA;abline(v=v4)&#xA;#time[area[which(area[,1]&amp;lt;cent),1][1]==dta[,lad]]&#xA;#time[area[which(area[,3]&amp;lt;cent),3][1]==dta[,lad]]&#xA;#time[area[which(area[,1]&amp;lt;cent),1][2]==dta[,lad]]&#xA;#time[area[which(area[,3]&amp;lt;cent),3][2]==dta[,lad]]&#xA;#abline(v=time[area[which(area[,1]&amp;lt;cent),1][1]==dta[,lad]])&#xA;#abline(v=time[area[which(area[,3]&amp;lt;cent),3][1]==dta[,lad]])&#xA;#abline(v=time[area[which(area[,1]&amp;lt;cent),1][2]==dta[,lad]])&#xA;#abline(v=time[area[which(area[,3]&amp;lt;cent),3][2]==dta[,lad]])&#xA;#abline(h=area[which(area[,3]&amp;lt;cent),3][1])&#xA;#abline(h=area[which(area[,3]&amp;lt;cent),3][2]+3)&#xA;s18a&amp;lt;-round(t1,digits=1)&#xA;s18b&amp;lt;-round(t3,digits=1)&#xA;s28a&amp;lt;-round(t2,digits=1)&#xA;s28b&amp;lt;-round(t4,digits=1)&#xA;if(is.na(s18a)){s18a&amp;lt;-30}&#xA;if(is.na(s18b)){s18b&amp;lt;-35.05}&#xA;if(is.na(s28a)){s28a&amp;lt;-30}&#xA;if(is.na(s28b)){s28b&amp;lt;-30.05}&#xA;eta&amp;lt;-which(time==s18a)&#xA;etb&amp;lt;-which(time==s18b)&#xA;tea&amp;lt;-which(time==s28a)&#xA;teb&amp;lt;-which(time==s28b)&#xA;#dta[eta:etb,lad]&#xA;#dta[tea:teb,lad]&#xA;s18&amp;lt;-sum(dta[eta:etb,lad])&#xA;s28&amp;lt;-sum(dta[tea:teb,lad])&#xA;s28/s18&#xA;qc.mat[samples]&amp;lt;-  (-1*exp((s28/s18)*-1)+1)*10&#xA;text(60,cent*.75,c(format(qc.mat[samples],digits=2),&quot;\n\nB-RIN&quot;))&#xA;}&#xA;qc.mat&#xA;###end of ratio calc&#xA;###&#xA;###&#xA;#barplot(qc.mat,beside=T,ylim=c(0,11))&#xA;#set the working directory&#xA;#results &amp;lt;- read.delim(&quot;results-files.txt&quot;, sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xA;#targets &amp;lt;- read.delim(results[1,], sep=&quot;,&quot;,header=F,skip=14,as.is=T)&#xA;#targets[1:5,]&#xA;tbl&amp;lt;-which(targets[,1]==&quot;Overall Results:&quot;)&#xA;which(targets[,1]==&quot;Sample Name&quot;)&#xA;length(which(targets[,1]==&quot;Sample Name&quot;))&#xA;samples&amp;lt;-as.character(targets[which(targets[,1]==&quot;Sample Name&quot;),2])&#xA;#read in the targets file                               &#xA;colnms&amp;lt;-as.character(targets[(tbl[1]+1):(tbl[1]+4),1])&#xA;mat1 &amp;lt;- matrix(NA, ncol=length(colnms), nrow=length(samples)*length(results))&#xA;colnames(mat1)&amp;lt;-colnms&#xA;rownames(mat1)&amp;lt;-rep(samples,length(results))&#xA;for(i in 1:length(results)) &#xA;{&#xA;targets &amp;lt;- read.delim(results[i], sep=&quot;,&quot;,header=F,skip=14,as.is=T)&#xA;targets[1:5,]&#xA;which(targets[,1]==&quot;Sample Name&quot;)&#xA;length(which(targets[,1]==&quot;Sample Name&quot;))&#xA;samples&amp;lt;-as.character(targets[which(targets[,1]==&quot;Sample Name&quot;),2])&#xA;tbl&amp;lt;-which(targets[,1]==&quot;Overall Results:&quot;)&#xA;length(which(targets[,1]==&quot;Overall Results:&quot;))&#xA;tbl[1]&#xA;tbl[1]&#xA;colnms&amp;lt;-as.character(targets[(tbl[1]+1):(tbl[1]+4),1])&#xA;for (j in 1:length(samples))&#xA;{&#xA;mat1[((length(samples)*(i-1))+j),]&amp;lt;-as.matrix(targets[(tbl[j]+1):(tbl[j]+4),2])&#xA;}&#xA;}&#xA;mat1&#xA;cbind(mat1,qc.mat)&#xA;colnames(qc.mat)&amp;lt;-&quot;B-RIN&quot;&#xA;write.table(cbind(mat1,qc.mat),file=paste(dir,&quot;\\RIN_VS_B-RIN.txt&quot;,sep=&quot;&quot;), sep=&quot;\t&quot;,col.names=NA)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="180" LastEditorUserId="298" LastEditDate="2017-07-12T10:23:21.807" LastActivityDate="2017-07-12T10:23:21.807" CommentCount="1" />
  <row Id="2059" PostTypeId="1" CreationDate="2017-07-13T08:24:21.383" Score="1" ViewCount="54" Body="&lt;p&gt;I am looking for advice on how to best approach a problem I am faced with. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a dataset of numerous degradative biomarkers, clinical information and various other measures (from clinical trials). I would like to see how different biomarkers relate to each other, and effect their levels (high/low etc).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have thought about implementing something similar to a protein-protein interaction network, although this will of course not involve physical interactions, but more pathway interactions for further hypothesis driven research. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wanted to firstly: validate if this project actually makes sense and secondly: ask for some advice around how to best implement this. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have experience implementing neural/deep nets, directed and undirected networks (Bayes) in python, but not sure exactly how to go about this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT (from below): My goal is to gain an understanding of how these biomarker levels fluctuate in different disease profiles - and understand if there is a relationship between each of them (co/independent). The limitation is that I do not have access to genetic or environmental data so this will be a &quot;random effect&quot;. There are around 1000 marker samples and 10's of biomarkers mixed between clinical and biochemical. There are also numerous time points to be incorporated&lt;/p&gt;&#xA;" OwnerUserId="367" LastEditorUserId="367" LastEditDate="2017-07-14T11:24:28.650" LastActivityDate="2017-07-14T11:24:28.650" Title="Building network for biomarker interaction analysis" Tags="&lt;proteins&gt;&lt;networks&gt;&lt;pathway&gt;&lt;interactions&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="0" />
  <row Id="2060" PostTypeId="2" ParentId="2054" CreationDate="2017-07-13T08:35:41.727" Score="3" Body="&lt;p&gt;I would recommend &lt;a href=&quot;http://hibberdlab.com/transrate/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Transrate&lt;/a&gt;. It generates a score for each transcript based on many metrics (read their paper &lt;a href=&quot;http://dx.doi.org/10.1101/gr.196469.115&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;) and, using that score, gives a predicted cut-off to produce the optimal transcript set.&lt;/p&gt;&#xA;" OwnerUserId="1092" LastEditorUserId="1092" LastEditDate="2017-07-13T08:44:04.673" LastActivityDate="2017-07-13T08:44:04.673" CommentCount="0" />
  <row Id="2061" PostTypeId="2" ParentId="1007" CreationDate="2017-07-13T09:04:28.000" Score="3" Body="&lt;p&gt;On Reactome website they have at the download &lt;a href=&quot;http://reactome.org/pages/download-data/&quot; rel=&quot;nofollow noreferrer&quot;&gt;page&lt;/a&gt; mapping files (uniprot, ensembl, etc.), but unfortunately not for the protein IDs you are using (stable identifiers).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I had contact with their helpdesk, and they sent me a file containing all protein IDs to the pathways. Exactly what you need. I have asked them if they wanted to put it on their download page as well, but not sure if they want to do this. Meanwhile you can ask for the file as well, or get it from my google &lt;a href=&quot;https://drive.google.com/open?id=0ByVMqlqqt462enVaNkZZSU9iMjg&quot; rel=&quot;nofollow noreferrer&quot;&gt;drive&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume you know how to get your Protein IDs and pathways from this file, using e.g., R?&lt;/p&gt;&#xA;" OwnerUserId="939" LastActivityDate="2017-07-13T09:04:28.000" CommentCount="0" />
  <row Id="2062" PostTypeId="1" CreationDate="2017-07-13T12:22:24.687" Score="1" ViewCount="54" Body="&lt;p&gt;I would like to be able to batch download FASTA files from ENSEMBL. I normally would use the API to download them from the ENSEMBL gene IDs. However, the IDs aren't ones I know, only ones I would get from searching on the ENSEMBL website and collecting the IDs manually.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way I can use the API to query search terms and collect all results that are returned?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So for example, if I use the ENSEMBL website to search &quot;HLA&quot; I could collect the list of IDs (ENSG00000204252 etc. etc.) manually. HLA works okay using the API to query gene IDs because there's only a few terms (A, B, C plus non-classical for class I) but I was wondering if there was a way to directly access the search query one the homepage programmatically, as I may need to do it for some messier examples.&lt;/p&gt;&#xA;" OwnerUserId="252" LastEditorUserId="252" LastEditDate="2017-07-14T08:59:31.373" LastActivityDate="2017-07-14T08:59:31.373" Title="Is there a way to retrieve ENSEMBL IDs from a search query?" Tags="&lt;database&gt;&lt;reference-genome&gt;&lt;ensembl&gt;&lt;data-mining&gt;" AnswerCount="1" CommentCount="7" FavoriteCount="0" />
  <row Id="2063" PostTypeId="2" ParentId="2062" CreationDate="2017-07-13T17:47:10.213" Score="1" Body="&lt;p&gt;I would download the complete geneset FASTA file from &lt;a href=&quot;http://www.ensembl.org/info/data/ftp/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;ensembl&lt;/a&gt; and then parse out the genes you want with a script.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively, you could do similarly with API. Get all the gene names, select the ones you want, and then pull down each sequence.&lt;/p&gt;&#xA;" OwnerUserId="1092" LastActivityDate="2017-07-13T17:47:10.213" CommentCount="0" />
  <row Id="2065" PostTypeId="1" CreationDate="2017-07-14T10:46:42.500" Score="1" ViewCount="50" Body="&lt;p&gt;I'm new to bioinformatics and Im starting a new microbial sequencing project and I'd like to record all of the qc data correctly. This is research and I'm not sure what I'll need later. Is there any consensus on useful qc metrics to record in a database for each sequencing run?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;No rnaseq, running FastQC, assembling with a defined reference for a single species. &lt;/p&gt;&#xA;" OwnerUserId="167" LastEditorUserId="167" LastEditDate="2017-07-14T10:55:05.473" LastActivityDate="2017-07-14T10:55:05.473" Title="What sequencing data metrics should I record?" Tags="&lt;database&gt;&lt;ngs&gt;&lt;qc&gt;" AnswerCount="0" CommentCount="9" />
  <row Id="2066" PostTypeId="2" ParentId="2059" CreationDate="2017-07-14T10:54:33.113" Score="1" Body="&lt;p&gt;You can start by looking at the correlation (if data is non-parametric) of each variable with each variable of each data point you have. One thing that might change is the correlation between variables along the disease.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next you can determine which relationships are constant along the disease progression/clinical trial. Identify also the ones that don't show a constant relationship. To do so you can use WGCNA (although with just 10 variables is too low to expect a scale free network you can use to correlate the variables and group them by correlation profile)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then the question is which variables are more important, those who kept the same relationship or those which change? &#xA;Use the eigengenes of those variables (separately by those that have a constant relationship and those which don't) to estimate the dependency with the parametric variables. Or you simply can do an ANOVA of all the variables with each non-parametric variable. &lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-07-14T10:54:33.113" CommentCount="6" />
  <row Id="2067" PostTypeId="1" CreationDate="2017-07-14T11:13:42.640" Score="4" ViewCount="126" Body="&lt;p&gt;I want to perform a genome comparison on a group of isolates. I want to look into two broad groups of taxa and compare the accessory genome in each group. I have been using prokka (v1.12) and roary (v3.8.2) to do this but it appears the accessory_binary_genes.fa file is actually an untrue representation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; gene_presence_absence.Rtab does contain all the full presence/ absence for accessory gene sets. Despite this Im still unhappy with the nomenclature of the gene groups [issue for another day] &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;a href=&quot;https://github.com/sanger-pathogens/Roary/issues/335&quot; rel=&quot;nofollow noreferrer&quot;&gt;[github issue 335]&lt;/a&gt; Your best to ignore the accessory_binary_genes.fa file. It is just for creating a quick and dirty tree with FastTree. The file itself is filtered to remove very common and not common variation to speedup the tree generation, hence the difference in numbers.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The top 5% and bottom 5% are excluded. It is truncated at 4000 genes.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I've been looking into alternative pipelines and a new software &lt;a href=&quot;https://www.nature.com/articles/srep24373&quot; rel=&quot;nofollow noreferrer&quot;&gt;BPGA&lt;/a&gt; looks promising. Does anyone have experience with this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I essentially want a tool which will give me the core and accessory gene sets, without the noise from partial gene hits.&lt;/p&gt;&#xA;" OwnerUserId="982" LastEditorUserId="982" LastEditDate="2017-07-20T11:05:50.983" LastActivityDate="2017-07-20T11:05:50.983" Title="What tools can I use for a bacterial core/pan genome pipeline?" Tags="&lt;genome&gt;&lt;software-recommendation&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="0" />
  <row Id="2068" PostTypeId="1" AcceptedAnswerId="2072" CreationDate="2017-07-14T13:40:47.287" Score="4" ViewCount="92" Body="&lt;p&gt;I would like to convert a file in &lt;code&gt;gff3&lt;/code&gt; format to a &lt;code&gt;gtf2.2&lt;/code&gt; format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reason why I would like to do this is: I have a set of transcripts assembled by a bunch of different software (and using RNA-seq data from different sequencing technologies) and I would like to compare each of those sets with the reference transcriptome annotation for that species (&lt;em&gt;D. melanogaster&lt;/em&gt;). I &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/2036/tools-to-reconcile-experimental-transcripts-with-reference-annotation&quot;&gt;already asked for the community advice&lt;/a&gt; about how to proceed on that, but to run the suggested software (&lt;a href=&quot;https://tacorna.github.io/&quot; rel=&quot;nofollow noreferrer&quot;&gt;TACO&lt;/a&gt; and &lt;a href=&quot;http://cole-trapnell-lab.github.io/cufflinks/cuffmerge/&quot; rel=&quot;nofollow noreferrer&quot;&gt;cuffmerge&lt;/a&gt;) I need to have a GTF file containing the experimental transcripts to compare to the reference transcripts (GTF).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Up to now, I &lt;strong&gt;unsuccessfully&lt;/strong&gt; tried: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;gffread&lt;/code&gt; utility in the &lt;a href=&quot;http://cole-trapnell-lab.github.io/cufflinks/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cufflinks&lt;/a&gt; package (&lt;code&gt;gffread input.gff3 -T -o output.gtf&lt;/code&gt;): this results in an empty &lt;code&gt;output.gtf&lt;/code&gt; file and an empty &lt;code&gt;log&lt;/code&gt; file (used &lt;code&gt;Cufflinks&lt;/code&gt; v.2.2.1) - I contacted the authors via their &lt;a href=&quot;https://groups.google.com/d/msg/tuxedo-tools-users/T-qE4P67-HU/O_up6Yh6BgAJ&quot; rel=&quot;nofollow noreferrer&quot;&gt;Google group&lt;/a&gt; but haven't heard of them yet &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;gff3_to_gtf&lt;/code&gt; utility in the &lt;a href=&quot;http://genometools.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GenomeTools&lt;/a&gt; (&lt;code&gt;gt&lt;/code&gt;) package (&lt;code&gt;gt gff3_to_gtf input.gff3 -o output.gtf&lt;/code&gt;): the output is not created and the log file is not informative - I contacted the authors via their &lt;a href=&quot;http://gt-users@genometools.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;mailing list&lt;/a&gt; but haven't heard of them yet &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;GFF3_to_GTF&lt;/code&gt; utility in the &lt;a href=&quot;https://testtoolshed.g2.bx.psu.edu/repository/display_tool?repository_id=afcb6456d8e300ed&amp;amp;tool_config=database%2Fcommunity_files%2F000%2Frepo_21%2Ffml_gff_converter_programs%2Fgalaxy%2Fgff3_to_gtf.xml&amp;amp;changeset_revision=4c459744cab1&quot; rel=&quot;nofollow noreferrer&quot;&gt;FML&lt;/a&gt; package (&lt;code&gt;./gff3_to_gtf_converter.pl input.gff3 output.gtf&lt;/code&gt;): the output just contains a header (&lt;code&gt;##gff-version 2.5&lt;/code&gt;) and the &lt;code&gt;log&lt;/code&gt; is empty&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;gff3&lt;/code&gt; file was created as output of &lt;a href=&quot;https://wiki.gacrc.uga.edu/wiki/Gmap&quot; rel=&quot;nofollow noreferrer&quot;&gt;GMAP&lt;/a&gt;, and contains the transcripts as found by alignment to the reference (specifying option &lt;code&gt;-f gff3_match_cdna&lt;/code&gt;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;[Edit: what I found out a poteriori, is that this format is not a standard gff3, thus the conversion is not trivial...]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the beginning of the gff3 file I tried to convert: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ head r9_gmap_match-cdna.gff&#xA;##gff-version   3&#xA;# Generated by GMAP version 2017-04-24 using call:  /home/aechchik/software/gmap-2017-04-24/bin/gmap.sse42 -d gmapidx -D /scratch/beegfs/monthly/aechchik/isoforms/ref/chromosomes/ -f gff3_match_cdna -n 0 -t 20 r9_2d.fasta&#xA;2L    gmapidx    cDNA_match    18442664    18443024    79    -    . ID=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1;Name=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d;Target=ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d 141 489;Gap=M13 I10 M8 D3 M8 D3 M32 D1 M4 D2 M5 D3 M34 I3 M6 D1 M7 D1 M5 D1 M30 D2 M16 I1 M8 D3 M10 D1 M5 D1 M6 I1 M8 I1 M8 D1 M33 D3 M33 I1 M28 D3 M25&#xA;###&#xA;3R    gmapidx    cDNA_match    15853880    15855465    96    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 80 1645;Gap=M46 D2 M12 D2 M157 D3 M4 D1 M50 I2 M3 I1 M12 I1 M68 I1 M66 I1 M53 D2 M47 D1 M16 D1 M35 D1 M155 D1 M28 D1 M166 D2 M47 D1 M8 D4 M69 D1 M28 D1 M5 D1 M12 D1 M202 I1 M115 D1 M61 I2 M7 D1 M7 I1 M36 D2 M41&#xA;3R    gmapidx    cDNA_match    15855529    15855742    97    +    . ID=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1;Name=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d;Target=dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d 1646 1856;Gap=M21 D1 M68 D1 M85 D1 M37&#xA;###&#xA;X    gmapidx    cDNA_match    14837810    14838142    93    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 74 406;Gap=M13 D1 M182 I1 M14 I2 M56 I1 M30 D2 M21 D1 M13&#xA;X    gmapidx    cDNA_match    14837470    14837753    92    -    . ID=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1;Name=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d;Target=960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d 407 688;Gap=I2 M5 I1 M64 I1 M44 D1 M9 D5 M31 D3 M23 I1 M19 I1 M25 I1 M26 D1 M19 I1 M9&#xA;###&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="294" LastEditorUserId="294" LastEditDate="2017-07-18T16:41:08.203" LastActivityDate="2017-07-18T16:41:08.203" Title="How to convert GFF3 to GTF2" Tags="&lt;software-recommendation&gt;&lt;format-conversion&gt;&lt;gff3&gt;&lt;gtf&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="0" />
  <row Id="2069" PostTypeId="2" ParentId="2068" CreationDate="2017-07-14T13:46:53.463" Score="0" Body="&lt;p&gt;The &lt;a href=&quot;http://genometools.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;GenomeTools&lt;/a&gt; package has lots of small applications that let you massage GFF3. It may have a converter, or it may let you tweak your existing GFF3 so that gffread works&lt;/p&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-07-14T13:46:53.463" CommentCount="5" />
  <row Id="2070" PostTypeId="2" ParentId="829" CreationDate="2017-07-14T16:12:50.580" Score="2" Body="&lt;p&gt;&lt;strong&gt;Ensembl&lt;/strong&gt; contains this information: When you &lt;a href=&quot;http://www.ensembl.org/Homo_sapiens/Gene/Phenotype?db=core;g=ENSG00000164508;r=6:25726132-25726527;t=ENST00000297012#ALL_tablePanel&quot; rel=&quot;nofollow noreferrer&quot;&gt;go to the “phenotype” menu item of a given gene&lt;/a&gt;, you will see a list of variants (potentially after clicking on “ALL associated variants”) with their associated genomic position.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can subtract the transcript’s start position from that position to find out the residue (of the translated sequence, and be careful to subtract the lengths of all preceding introns!). Unfortunately this seems to require some manual work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively you can &lt;a href=&quot;http://www.ensembl.org/Homo_sapiens/Variation/Phenotype?db=core;g=ENSG00000164508;r=6:25726132-25726527;t=ENST00000297012;v=rs2275906;vdb=variation;vf=1677930&quot; rel=&quot;nofollow noreferrer&quot;&gt;query the individual SNPs&lt;/a&gt;, which will tell you their position in all transcripts (in this case: Leu 298/68/244):&#xA;&lt;a href=&quot;https://i.stack.imgur.com/zr2Q4.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/zr2Q4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-07-14T16:12:50.580" CommentCount="0" />
  <row Id="2072" PostTypeId="2" ParentId="2068" CreationDate="2017-07-15T11:30:13.013" Score="5" Body="&lt;p&gt;The following bit of python code should work:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/env python&#xA;import sys&#xA;&#xA;lastTranscript = [None, None, None, []]  # ID, chrom, strand, [(start, end, score), ...]&#xA;&#xA;&#xA;def getID(s):&#xA;    &quot;&quot;&quot;Parse out the ID attribute&quot;&quot;&quot;&#xA;    s = s.split(&quot;;&quot;)&#xA;    for k in s:&#xA;        if k.startswith(&quot;ID=&quot;):&#xA;            return k[3:]&#xA;    return None&#xA;&#xA;&#xA;def dumpLastTranscript():&#xA;    &quot;&quot;&quot;Print the last transcript&quot;&quot;&quot;&#xA;    bounds = sorted(lastTranscript[3])&#xA;    print(&quot;{}\tgmapidx\tgene\t{}\t{}\t.\t{}\t.\tgene_id \&quot;{}\&quot;; transcript_id \&quot;{}\&quot;;&quot;.format(lastTranscript[1], bounds[0][0], bounds[-1][1], lastTranscript[2], lastTranscript[0], lastTranscript[0]))&#xA;    print(&quot;{}\tgmapidx\ttranscript\t{}\t{}\t.\t{}\t.\tgene_id \&quot;{}\&quot;; transcript_id \&quot;{}\&quot;;&quot;.format(lastTranscript[1], bounds[0][0], bounds[-1][1], lastTranscript[2], lastTranscript[0], lastTranscript[0]))&#xA;    for start, end, score in bounds:&#xA;        print(&quot;{}\tgmapidx\texon\t{}\t{}\t{}\t{}\t.\tgene_id \&quot;{}\&quot;; transcript_id \&quot;{}\&quot;;&quot;.format(lastTranscript[1], start, end, score, lastTranscript[2], lastTranscript[0], lastTranscript[0]))&#xA;&#xA;&#xA;def handleLine(cols):&#xA;    &quot;&quot;&quot;Handle a single line, appending the exon bounds to the previous if relevant&quot;&quot;&quot;&#xA;    ID = getID(cols[8])&#xA;    assert(ID is not None)&#xA;    if lastTranscript[0] is not None and lastTranscript[0] != ID:&#xA;        dumpLastTranscript()&#xA;        lastTranscript[3] = []&#xA;    lastTranscript[0] = ID&#xA;    lastTranscript[1] = cols[0]&#xA;    lastTranscript[2] = cols[6]&#xA;    lastTranscript[3].append((int(cols[3]), int(cols[4]), cols[5]))&#xA;&#xA;&#xA;f = open(sys.argv[1])&#xA;for line in f:&#xA;    if line.startswith(&quot;#&quot;):&#xA;        continue&#xA;    cols = line.strip().split(&quot;\t&quot;)&#xA;    handleLine(cols)&#xA;&#xA;dumpLastTranscript()&#xA;f.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you saved that as &lt;code&gt;gff2gtf.py&lt;/code&gt; and made it executable, the usage would be &lt;code&gt;./gff2gtf.py foo.gff3 &amp;gt; foo.gtf&lt;/code&gt;. With the example that you provided in your post, the result is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;2L  gmapidx gene    18442664    18443024    .   -   .   gene_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;; transcript_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;;&#xA;2L  gmapidx transcript  18442664    18443024    .   -   .   gene_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;; transcript_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;;&#xA;2L  gmapidx exon    18442664    18443024    79  -   .   gene_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;; transcript_id &quot;ae6a7818-85b5-4739-8031-e58f4462ad41_Basecall_2D_2d.path1&quot;;&#xA;3R  gmapidx gene    15853880    15855742    .   +   .   gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xA;3R  gmapidx transcript  15853880    15855742    .   +   .   gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xA;3R  gmapidx exon    15853880    15855465    96  +   .   gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xA;3R  gmapidx exon    15855529    15855742    97  +   .   gene_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;; transcript_id &quot;dd2444cf-34d6-4cd3-87ab-0ae1f3cb1f96_Basecall_2D_2d.path1&quot;;&#xA;X   gmapidx gene    14837470    14838142    .   -   .   gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xA;X   gmapidx transcript  14837470    14838142    .   -   .   gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xA;X   gmapidx exon    14837470    14837753    92  -   .   gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xA;X   gmapidx exon    14837810    14838142    93  -   .   gene_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;; transcript_id &quot;960b50cd-945e-4c12-b9bc-367f965575bb_Basecall_2D_2d.path1&quot;;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That at least looks like correct gtf 2.2 format to me.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-07-17T07:29:29.317" LastActivityDate="2017-07-17T07:29:29.317" CommentCount="0" />
  <row Id="2073" PostTypeId="1" CreationDate="2017-07-15T15:02:59.803" Score="2" ViewCount="64" Body="&lt;p&gt;I'm currently learning about Gene Set Enrichment Analysis (&lt;a href=&quot;http://software.broadinstitute.org/gsea/index.jsp&quot; rel=&quot;nofollow noreferrer&quot;&gt;GSEA&lt;/a&gt;) in the hopes of using it in my analysis of differentially expressed genes, and I just had a few questions about the program, specifically about GSEAPreranked, which I need cleared up.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;On the ranked list needed for GSEA input, should the list include all genes, or only those that pass a certain threshold of significance (i.e. fold change higher than 2, p value less than 0.05, etc.)? Ideally I'd like to sort the genes by fold change alone as I don't trust my p values as much, so should I only include genes with high fold changes?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I am comparing multiple conditions of disease with different treatments. Am I correct that GSEA only compares two conditions? If this is the case should I run GSEA for each control/treatment comparison? Would this be conventional?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="506" LastEditorUserId="57" LastEditDate="2017-07-17T15:17:56.247" LastActivityDate="2017-07-17T15:17:56.247" Title="Basic questions about GSEA" Tags="&lt;gene&gt;&lt;annotation&gt;&lt;rna&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2074" PostTypeId="2" ParentId="2073" CreationDate="2017-07-15T15:46:52.880" Score="4" Body="&lt;p&gt;You seem to refer to the GSEA provided by the &lt;a href=&quot;http://software.broadinstitute.org/gsea/index.jsp&quot; rel=&quot;nofollow noreferrer&quot;&gt;Broad institute&lt;/a&gt;, (there are other GSEA algorithms). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) You can provide whatever you wish, but if you want to know if those gene sets in which side of the ordered list are they, then provide all the list (of genes) you have.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) GSEA analyize if the order of a given list distributes in certain way the elements of another (unordered) list. In your case you would need to compute the GSEA for each treatment vs control comparison.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-07-15T15:46:52.880" CommentCount="0" />
  <row Id="2075" PostTypeId="2" ParentId="2049" CreationDate="2017-07-16T13:37:45.500" Score="2" Body="&lt;p&gt;I'm not sure what you mean by it being a mess? That looks like a pretty good alignment to me, and it most certainly has aligned all the sequences. You are nearly always going to have gaps (see my MSA below, which are all paralogs from the &lt;em&gt;same genome&lt;/em&gt; so they couldn't really be more closely related, and yet, there are gaps). That comes with its own set of problems mind you, as you need to decide how you're going to deal with them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You also don't necessarily have to do a profile alignment. Those sequences don't look massively divergent to me, so straight forward sequence alignment would probably give you more or less the same result.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;My goal is to visualize the number of BLAST hits per amino acid in my protein of interest.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;This doesn't really make sense. You should really be searching for the number of BLAST hits with a particular domain, if you want to infer homology.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want a per-position visualisation of the conservation, you could look at the shannon entropy for each column and plot that. I wrote a script to do just that a little while ago: &lt;a href=&quot;https://github.com/jrjhealey/bioinfo-tools/blob/master/Shannon.py&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/jrjhealey/bioinfo-tools/blob/master/Shannon.py&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just beware it's not super well tested yet. Feed an MSA in with as many sequences as you want to analyse, but you'll have to have identified the sequences and done the alignment first.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, given this MSA:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    16    149&#xA;PAU_02775  MSTTPEQIAV EYPIPTYRFV VSLGDEQIPF NSVSGLDISH DVIEYKDGTG&#xA;PLT_01696  MSTTPEQIAV EYPIPTYRFV VSIGDEQIPF NSVSGLDISH DVIEYKDGTG&#xA;PAK_02606  MSTTPEQIAV EYPIPTYRFV VSIGDEQVPF NSVSGLDISH DVIEYKDGTG&#xA;PLT_01736  MSTTPEQIAV EYPIPTYRFV VSIGDEKVPF NSVSGLDISH DVIEYKDGTG&#xA;PAK_01896  MTTTT----V DYPIPAYRFV VSVGDEQIPF NNVSGLDITY DVIEYKDGTG&#xA;PAU_02074  MATTT----V DYPIPAYRFV VSVGDEQIPF NSVSGLDITY DVIEYKDGTG&#xA;PLT_02424  MSVTTEQIAV DYPIPTYRFV VSVGDEQIPF NNVSGLDITY DVIEYKDGTG&#xA;PLT_01716  MTITPEQIAV DYPIPAYRFV VSVGDEKIPF NNVSGLDVHY DVIEYKDGTG&#xA;PLT_01758  MAITPEQIAV EYPIPTYRFV VSVGDEQIPF NNVSGLDVHY DVIEYKDGIG&#xA;PAK_03203  MSTSTSQIAV EYPIPVYRFI VSIGDDQIPF NSVSGLDINY DTIEYRDGVG&#xA;PAU_03392  MSTSTSQIAV EYPIPVYRFI VSVGDEKIPF NSVSGLDISY DTIEYRDGVG&#xA;PAK_02014  MSITQEQIAA EYPIPSYRFM VSIGDVQVPF NSVSGLDRKY EVIEYKDGIG&#xA;PAU_02206  MSITQEQIAA EYPIPSYRFM VSIGDVQVPF NSVSGLDRKY EVIEYKDGIG&#xA;PAK_01787  MSTTADQIAV QYPIPTYRFV VTIGDEQMCF QSVSGLDISY DTIEYRDGVG&#xA;PAU_01961  MSTTADQIAV QYPIPTYRFV VTIGDEQMCF QSVSGLDISY DTIEYRDGVG&#xA;PLT_02568  MSTTVDQIAV QYPIPTYRFV VTVGDEQMSF QSVSGLDISY DTIEYRDGIG&#xA;&#xA;           NYYKMPGQRQ AINISLRKGV FSGDTKLFDW INSIQLNQVE KKDISISLTN&#xA;           NYYKMPGQRQ AINISLRKGV FSGDTKLFDW INSIQLNQVE KKDISISLTN&#xA;           NYYKMPGQRQ AINISLRKGV FSGDTKLFDW INSIQLNQVE KKDISISLTN&#xA;           NYYKMPGQRQ AINITLRKGV FSGDTKLFDW LNSIQLNQVE KKDISISLTN&#xA;           NYYKMPGQRQ LINITLRKGV FPGDTKLFDW LNSIQLNQVE KKDVSISLTN&#xA;           NYYKMPGQRQ LINITLRKGV FPGDTKLFDW LNSIQLNQVE KKDVSISLTN&#xA;           NHYKMPGQRQ LINITLRKGV FPGDTKLFDW LNSIQLNQVE KKDVSISLTN&#xA;           NYYKMPGQRQ SINITLRKGV FPGDTKLFDW INSIQLNQVE KKDIAISLTN&#xA;           NYYKMPGQRQ SINITLRKGV FPGDTKLFDW INSIQLNQVE KKDIAISLTN&#xA;           NWFKMPGQSQ LVNITLRKGV FPGKTELFDW INSIQLNQVE KKDITISLTN&#xA;           NWFKMPGQSQ STNITLRKGV FPGKTELFDW INSIQLNQVE KKDITISLTN&#xA;           NYYKMPGQIQ RVDITLRKGI FSGKNDLFNW INSIELNRVE KKDITISLTN&#xA;           NYYKMPGQIQ RVDITLRKGI FSGKNDLFNW INSIELNRVE KKDITISLTN&#xA;           NWLQMPGQRQ RPTITLKRGI FKGQSKLYDW INSISLNQIE KKDISISLTD&#xA;           NWLQMPGQRQ RPTITLKRGI FKGQSKLYDW INSISLNQIE KKDISISLTD&#xA;           NWLQMPGQRQ RPSITLKRGI FKGQSKLYDW INSISLNQIE KKDISISLTD&#xA;&#xA;           EAGTEILMTW SVANAFPTSL TSPSFDATSN EVAVQEITLT ADRVTIQAA&#xA;           EAGTEILMTW SVANAFPTSL ISPSFDATSN EVAVQEITLT ADRVTIQAA&#xA;           EAGTEILMTW SVANAFPTSL TSPSFDATSN EVAVQEITLT ADRVTIQAA&#xA;           EAGTEILMTW SVANAFPTSL TAPAFDATSN EVAVQEISLT ADRVTIQAA&#xA;           ETGTEILMSW SVANAFPTSL TSPSFDATSN DIAVQEIKLT ADRVTIQAA&#xA;           EVGTEILMTW SVANAFPTSL TSPSFDATSN DIAVQEIKLT ADRVTIQAA&#xA;           EAGTEILMSW SVANAFPTSL TSPSFDATSN DIAVQEIKLT ADRVMIQAA&#xA;           ETGSQILMTW NVANAFPTSF TSPSFDAASN DIAIQEIALV ADRVTIQAP&#xA;           EAGTEILMTW NVANAFPTSF TSPSFDATSN EIAVQEIALT ADRVTIQAA&#xA;           DAGTELLMTW NVSNAFPTSL TSPSFDATSN DIAVQEITLT ADRVIMQAV&#xA;           DAGTELLMTW NVSNAFPTSL TSPSFDATSN DIAVQEITLM ADRVIMQAV&#xA;           DTGSEVLMSW VVSNAFPSSL TAPSFDASSN EIAVQEISLV ADRVTIQVP&#xA;           DTGSKVLMSW VVSNAFPSSL TAPSFDASSN EIAVQEISLV ADRVTIQVP&#xA;           ETGSNLLITW NIANAFPEKL TAPSFDATSN EVAVQEMSLK ADRVTVEFH&#xA;           ETGSNLLITW NIANAFPEKL TAPSFDATSN EVAVQEISLK ADRVTVEFH&#xA;           ETGSNLLITW NIANAFPEKL TAPSFDATSN EVAVQEISLK ADRVTVEFH&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You'd get this plot:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/zCFTz.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/zCFTz.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;I would like to have a graph with my protein on the x-axis and a plot&#xA;  like the regions of high conservation for the multiple alignments,&#xA;  except with the spikes corresponding to a high number of BLAST hits.&#xA;  This would allow me to identify regions of my protein that have higher&#xA;  sequence similarity to bacteria than others.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I'm not sure your logic is quite right here though. Blast won't give you hits depending on a particular position. It's a local aligner, so it'll just return you hits where at least some part of your query matches at least some part of another.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What you could do is take the logic in the script above, and just use a different metric. For example, perhaps you could count the proportion of sequences which have the most common amino acid at a given position within your MSA. That would be fairly crude though.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you say in the comments,&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;My final goal is to produce a plot visualizing regions of high bacterial sequence similarity to my human protein of interest.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;your original alignment will show you this intrinsically, if only you include the sequences of all the BLAST hits in the first place. Thus your work flow will be:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Blast sequence of interest.&lt;/li&gt;&#xA;&lt;li&gt;Download all/as many hits as you want (bear in mind the E-value/bitscore and the number of hits you get. It might only be a few dozen, in which case you can use the lot, but if not, just take all the hits below a certain cut-off.)&lt;/li&gt;&#xA;&lt;li&gt;Align all the sequences.&lt;/li&gt;&#xA;&lt;li&gt;Look at the column scores for the whole MSA. You can use whatever metric of conservation you like really. Might be as simple as proportion of sequences with the most common residue, or something more complex like Shannon entropy (though as you can see in the graph above Shannon entropy can be kinda noisy) etc.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="929" LastActivityDate="2017-07-16T13:37:45.500" CommentCount="5" />
  <row Id="2076" PostTypeId="2" ParentId="997" CreationDate="2017-07-17T00:31:47.860" Score="1" Body="&lt;p&gt;I don't know of any transcript-to-transcript aligners that are able to do this, but &lt;a href=&quot;http://last.cbrc.jp/doc/lastal.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;LAST&lt;/a&gt; can align transcript queries to protein reference sequences using a specified frameshift cost. Here's the specific documentation for that option:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;-F COST   &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Align DNA queries to protein reference sequences, using the specified&#xA;  frameshift cost. A value of 15 seems to be reasonable. (As a special&#xA;  case, -F0 means DNA-versus-protein alignment without frameshifts,&#xA;  which is faster.) The output looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;a score=108 s prot 2  40 + 649&#xA;FLLQAVKLQDP-STPHQIVPSP-VSDLIATHTLCPRMKYQDD s dna  8 117 + 999&#xA;FFLQ-IKLWDP\STPH*IVSSP/PSDLISAHTLCPRMKSQDN&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;The \ indicates a forward shift by one nucleotide, and the / indicates&#xA;  a reverse shift by one nucleotide. The * indicates a stop codon. The&#xA;  same alignment in tabular format looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;108 prot 2 40 + 649 dna 8 117 + 999 4,1:0,6,0:1,10,0:-1,19&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;The &quot;-1&quot; indicates the reverse frameshift.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I sent an email to the &lt;a href=&quot;https://groups.google.com/forum/#!forum/last-align&quot; rel=&quot;nofollow noreferrer&quot;&gt;LAST mailing list&lt;/a&gt; about adding a frameshift penalty for transcript-to-transcript matching; I've been pleasantly surprised with the requested features that Martin Frith has added to LAST in the past. Unfortunately, in this case the problem is too difficult to sort out due to all the possible combinations that could happen, so it's unlikely to be implemented in LAST in the forseeable future (unless someone else writes that code).&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-07-22T11:05:35.870" LastActivityDate="2017-07-22T11:05:35.870" CommentCount="0" />
  <row Id="2077" PostTypeId="2" ParentId="2056" CreationDate="2017-07-17T02:10:55.023" Score="2" Body="&lt;p&gt;Note that DIN (DNA Integrity Number) and RIN (RNA Integrity Number) are different scores. I've had trouble in the past finding code or formulas for calculating both of these scores, which is very frustrating considering how frequently they are used in research papers and for NGS QC. The closest that I have been able to get to this are demonstrative graphs for various &lt;a href=&quot;http://bmcmolbiol.biomedcentral.com/articles/10.1186/1471-2199-7-3&quot; rel=&quot;nofollow noreferrer&quot;&gt;RIN numbers&lt;/a&gt; and &lt;a href=&quot;http://www.agilent.com/cs/library/applications/5991-5258EN.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;DIN numbers&lt;/a&gt;, with a description of the variables that are included in the model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code that @719016 provided appears to attempt to calculate the RIN, not the DIN.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just in case it's helpful, here is an extract of that code, only including lines that seem to be directly involved in calculating the actual B_RIN values. This is not runnable, because it depends on particular input files, and it's still pretty unintelligible:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;myfiles&amp;lt;-list.files()&#xA;results&amp;lt;-myfiles[which(substr(myfiles,69,79)==&quot;Results.csv&quot;)]&#xA;n &amp;lt;- which(substr(myfiles,69,79)==&quot;Results.csv&quot;)&#xA;targets &amp;lt;- read.delim(myfiles[n[1]], sep=&quot;,&quot;,header=F,skip=14,as.is=T)&#xA;targets2&amp;lt;- read.delim(myfiles[n[1]], sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xA;str3&amp;lt;-paste0(substr(targets2[1,2],1,67),&quot;_S&quot;)&#xA;chip_num&amp;lt;-length(n)&#xA;samp_num_chip&amp;lt;-length(grep(str3,myfiles))&#xA;samp_num&amp;lt;-length(which(substr(myfiles,69,74)==&quot;Sample&quot;))&#xA;samples1&amp;lt;- matrix(NA, ncol=chip_num, nrow=samp_num)&#xA;for(i in 1:chip_num)&#xA;{&#xA;    ##mod chip numb&#xA;    targets2&amp;lt;- read.delim(myfiles[n[i]], sep=&quot;,&quot;,header=F,skip=0,as.is=T)&#xA;    str3&amp;lt;-paste0(substr(targets2[1,2],1,67),&quot;_S&quot;)&#xA;    samp_num_chip&amp;lt;-length(grep(str3,myfiles))&#xA;    ##mod sample numb&#xA;    str1&amp;lt;-substr(targets2[1,2],1,67)&#xA;    for(j in 1:samp_num_chip)&#xA;    {&#xA;        iii&amp;lt;-j&#xA;        str5&amp;lt;-paste0(str1,&quot;_Sample&quot;,iii,&quot;.csv&quot;)&#xA;        samples1[j,i]&amp;lt;-str5&#xA;    }&#xA;}&#xA;samples1&amp;lt;-samples1[grep(&quot;2100&quot;,samples1)]&#xA;##RIN Calculator#######refined peakfinder####################&#xA;##read bioanalyzer data into a matrix called dta&#xA;##since the total RNA and mRNA assays run differently&#xA;##skip more lines for the total RNA assay&#xA;elec &amp;lt;- read.delim(&quot;ba_lane.txt&quot;, sep=&quot;\t&quot;,header=T,as.is=T)&#xA;qc.mat &amp;lt;- matrix(NA, ncol=1, nrow=nrow(elec))&#xA;dta &amp;lt;- matrix(NA, nrow=1060, ncol=nrow(elec))&#xA;for(i in 1:nrow(elec))&#xA;{&#xA;    x &amp;lt;- read.csv(as.character(elec[i,2]), header=F, skip=18, nrows=1060)&#xA;    dta[,i] &amp;lt;- x[,2]&#xA;}&#xA;time&amp;lt;- x[,1]&#xA;for(samples in 1:length(samples1))&#xA;{&#xA;    ##store max fluor in max.peaks matrix&#xA;    lad&amp;lt;-samples&#xA;    ti1&amp;lt;-17&#xA;    ti2&amp;lt;-65&#xA;    p.row&amp;lt;-length(dta[which(time==ti1):which(time==ti1+1),lad])*(ti2-ti1+1)&#xA;    peaks &amp;lt;- matrix(NA, nrow=p.row, ncol=1)&#xA;    max.peaks &amp;lt;- matrix(NA, nrow=length(seq(ti1,ti2,.05)), ncol=1)&#xA;    for (i in 1:length(seq(ti1,ti2,.05)))&#xA;    {&#xA;        ii&amp;lt;-seq(ti1,ti2,.05)[i]&#xA;        x0&amp;lt;- ii&#xA;        x0&amp;lt;-round(x0,digits=3)&#xA;        y0&amp;lt;- ii&#xA;        x&amp;lt;- ii+.05&#xA;        x&amp;lt;-round(x,digits=3)&#xA;        y&amp;lt;- ii&#xA;        max.peaks[i]&amp;lt;- max(dta[which(time==x0):which(time==x),lad])&#xA;    }    &#xA;    ##human or mouse&#xA;    cent&amp;lt;-max(max.peaks)*.6&#xA;    ##flatworm&#xA;    ##cent&amp;lt;-max(max.peaks)*.05&#xA;    ##define the middle of the segments&#xA;    max.avg&amp;lt;-seq(ti1+.025,ti2+.025,.05)&#xA;    ##find time associated with peaks &amp;gt; 5 fu and fill seconds matrix with them&#xA;    ##12.5% of max fu incase chip is fu&#xA;    sizes &amp;lt;- which(max.peaks&amp;gt;cent)&#xA;    gt35&amp;lt;-which(max.avg[sizes]&amp;gt;35)&#xA;    lt55&amp;lt;-which(max.avg[sizes]&amp;lt;55)&#xA;    tfr&amp;lt;-which(duplicated(c(gt35,lt55))==TRUE)&#xA;    area&amp;lt;-cbind(max.peaks[sizes-1][c(gt35,lt55)[tfr]],&#xA;                max.peaks[sizes][c(gt35,lt55)[tfr]],&#xA;                max.peaks[sizes+1][c(gt35,lt55)[tfr]])&#xA;    time2&amp;lt;-cbind(max.avg[sizes-1][c(gt35,lt55)[tfr]],&#xA;                 max.avg[sizes][c(gt35,lt55)[tfr]],&#xA;                 max.avg[sizes+1][c(gt35,lt55)[tfr]])&#xA;    t1&amp;lt;-time2[which(area[,1]&amp;lt;cent),1][1]&#xA;    t2&amp;lt;-time2[which(area[,1]&amp;lt;cent),1][2]&#xA;    t3&amp;lt;-time2[which(area[,3]&amp;lt;cent),3][1]&#xA;    t4&amp;lt;-time2[which(area[,3]&amp;lt;cent),3][2]&#xA;    s18a&amp;lt;-ifelse(is.na(t1,-30,round(t1,digits=1)))&#xA;    s18b&amp;lt;-ifelse(is.na(t3,-35.05,round(t3,digits=1)))&#xA;    s28a&amp;lt;-ifelse(is.na(t2,-30,round(t2,digits=1)))&#xA;    s28b&amp;lt;-ifelse(is.na(t4,-30.05,round(t4,digits=1)))&#xA;    eta&amp;lt;-which(time==s18a)&#xA;    etb&amp;lt;-which(time==s18b)&#xA;    tea&amp;lt;-which(time==s28a)&#xA;    teb&amp;lt;-which(time==s28b)&#xA;    s18&amp;lt;-sum(dta[eta:etb,lad])&#xA;    s28&amp;lt;-sum(dta[tea:teb,lad])&#xA;    qc.mat[samples]&amp;lt;-  (-1*exp((s28/s18)*-1)+1)*10&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Mostly based on that last line, the RIN seems to be associated with the ratio of the 18s peak to the 28s peak.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-07-17T02:10:55.023" CommentCount="0" />
  <row Id="2078" PostTypeId="1" AcceptedAnswerId="2112" CreationDate="2017-07-16T20:23:10.543" Score="4" ViewCount="44" Body="&lt;p&gt;I've got a decent knowledge of programming (incl. bash scripting) but I fail to understand how Arlequin works. Could you please help me with a very simple reproducible example on how to use Arlequin via the command line?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As the .zip file comes with its pre-compiled version, I'll just note that I'm on Mac OS X but of course, a reproducible example with Linux (or at worst with Windows) would also be helpful.&lt;/p&gt;&#xA;" OwnerUserId="888" OwnerDisplayName="Remi.b" LastEditorUserId="888" LastEditDate="2017-07-20T23:10:33.560" LastActivityDate="2017-07-20T23:10:33.560" Title="How can I use Arlequin via the command line?" Tags="&lt;genome&gt;&lt;statistics&gt;&lt;positive-selection&gt;" AnswerCount="1" CommentCount="7" />
  <row Id="2079" PostTypeId="2" ParentId="2073" CreationDate="2017-07-17T06:35:19.213" Score="4" Body="&lt;p&gt;Use the following R package for Gene Set Enrichment analysis of RNA-seq data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.bioconductor.org/packages/release/bioc/html/SeqGSEA.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.bioconductor.org/packages/release/bioc/html/SeqGSEA.html&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is another R package recently published called &quot;Fast Gene Set Enrichment Analysis&quot; by Alexey Sergushichev.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/fgsea.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://bioconductor.org/packages/release/bioc/html/fgsea.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="206" LastActivityDate="2017-07-17T06:35:19.213" CommentCount="0" />
  <row Id="2080" PostTypeId="2" ParentId="2067" CreationDate="2017-07-17T11:21:39.053" Score="2" Body="&lt;p&gt;Roary also takes into account paralogs, so sometimes two core genes are split into different groups based on their neighbour genes and they end up with different nomenclature (group_*...). As suggested by Andrew Page in the github issue I would consider the gene_presence_absence.Rtab (this contains all the orthologous genes) and remove rows corresponding to vectors only containing 1s (core genes). In this way you will have a matrix of 1 and 0 corresponding to presence/absence of a particular accessory gene in your isolates. &lt;/p&gt;&#xA;" OwnerUserId="446" LastActivityDate="2017-07-17T11:21:39.053" CommentCount="3" />
  <row Id="2081" PostTypeId="1" CreationDate="2017-07-17T12:22:15.753" Score="3" ViewCount="70" Body="&lt;p&gt;When doing Illumina 2x150bp sequencing of genomic DNA, and after aligning the reads to GRCh38, what percentage of the non-N fraction of the human genome is MAPQ=0? This is, what part corresponds to regions that can't be uniquely mapped with 2x150bp reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And, how many genes are affected by MAPQ=0 regions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I presume the numbers will dance around depending on fragment sizes, read quality, etc. but I am happy with some starting numbers.&lt;/p&gt;&#xA;" OwnerUserId="180" LastEditorUserId="57" LastEditDate="2017-07-20T12:56:04.567" LastActivityDate="2017-07-20T14:51:07.630" Title="what percentage of the human genome is MAPQ=0?" Tags="&lt;genome&gt;&lt;read-mapping&gt;&lt;human&gt;&lt;illumina&gt;&lt;mapq&gt;" AnswerCount="3" CommentCount="3" FavoriteCount="1" />
  <row Id="2082" PostTypeId="2" ParentId="2067" CreationDate="2017-07-17T14:19:35.680" Score="2" Body="&lt;p&gt;LS-BSR should be able to give you what you're looking for:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://peerj.com/articles/332/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://peerj.com/articles/332/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/jasonsahl/LS-BSR&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/jasonsahl/LS-BSR&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After you run the primary analysis, there is a simple documented workflow for splitting the pangenome into core and accessory, based on a user defined threshold. I'm the developer, so can help if you run into any problems.&lt;/p&gt;&#xA;" OwnerUserId="1126" LastActivityDate="2017-07-17T14:19:35.680" CommentCount="4" />
  <row Id="2083" PostTypeId="1" CreationDate="2017-07-17T15:57:38.820" Score="6" ViewCount="86" Body="&lt;p&gt;I am trying to translate (lift over) bed files describing genomic regions from hg37 to hg38. I have tried both UCSC's &lt;a href=&quot;http://genome.ucsc.edu/cgi-bin/hgLiftOver&quot; rel=&quot;noreferrer&quot;&gt;LiftOver&lt;/a&gt; tool and &lt;a href=&quot;http://crossmap.sourceforge.net/&quot; rel=&quot;noreferrer&quot;&gt;CrossMap&lt;/a&gt; but saw that they give me different results. I therefore need a way of assessing how correct the results are. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I decided to test by downloading a bed file of the RefSeq genes for hg37 and one for hg38 from UCSC. Then, I run &lt;code&gt;liftOver&lt;/code&gt; to translate the hg37 file to hg39 coordinates and now I want to compare the results of the liftover to the bed file I downloaded for hg38.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I usefully compare these files? I am hoping for a tool that can report how similar the two are. Ideally, I would like to see three things in the output:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Matching regions: those that are shown with identical positions in the liftover results and the UCSC bed file. &lt;/li&gt;&#xA;&lt;li&gt;Non-matching regions: those whose coordinates do not match between the two files. &lt;/li&gt;&#xA;&lt;li&gt;(if possible) fuzzy matches: those that are not identical but pretty close (off by a few nucleotides).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I thought it would be easy to do with a little awk script but it gets quite complicated because certain RefSeq genes are mapped to multiple positions (e.g. &lt;a href=&quot;http://www.genenames.org/cgi-bin/gene_symbol_report?hgnc_id=41553&quot; rel=&quot;noreferrer&quot;&gt;MIR4454&lt;/a&gt;). And the 3rd requirement (fuzzy) is not trivial to do in awk. I can live without that though, if a tool can give me the other two. &lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-07-17T19:37:29.100" Title="How can I compare two bed files?" Tags="&lt;bed&gt;&lt;liftover&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="2084" PostTypeId="2" ParentId="2083" CreationDate="2017-07-17T18:09:10.553" Score="2" Body="&lt;p&gt;I wrote &lt;a href=&quot;http://aegean.readthedocs.io/en/stable/parseval.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;ParsEval&lt;/a&gt; to handle these types of comparisons. ParsEval reports a variety of similarity statistics at both the nucleotide level and feature (exon) level. It doesn't explicitly support &quot;fuzzy&quot; matches, but it does report scaled values between 0.0 and 1.0 for similarity, so you could get a similar feel by definig a similarity cutoff for filtering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Caveat: ParsEval doesn't handle BED files, only GFF3.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-07-17T18:09:10.553" CommentCount="4" />
  <row Id="2085" PostTypeId="2" ParentId="2083" CreationDate="2017-07-17T19:06:47.940" Score="3" Body="&lt;p&gt;From a set operations viewpoint, consider your hg37 regions as a &quot;reference&quot; set and hg38 as a &quot;map&quot; set. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have these sets in BED format, &lt;a href=&quot;http://bedops.readthedocs.io/en/latest/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;BEDOPS&lt;/a&gt; lets you do set operations on them with various overlap criteria.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically, you can use &lt;a href=&quot;http://bedops.readthedocs.io/en/latest/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;BEDOPS &lt;code&gt;bedmap&lt;/code&gt;&lt;/a&gt; to map hg38 regions to hg37 regions. The &lt;code&gt;bedmap&lt;/code&gt; tool lets you assign overlap criteria from as little as one base to as much as full, mutual overlap. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you do mapping, you can do operations, like counting the number of regions that meet the overlap criteria between reference and map sets, and calculate percentages.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, use &lt;code&gt;--exact&lt;/code&gt; to get exactly-matching regions between hg37 and hg38:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ bedmap --count --exact hg37.bed hg38.bed | awk '{s+=$1}END{print s;}' &amp;gt; exact_counts.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You can do fractional (&quot;fuzzy&quot;) overlaps, where (for example) at least 50% of an hg38 region has to overlap an hg37 region:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ bedmap --count --fraction-map 0.5 hg37.bed hg38.bed | awk '{s+=$1}END{print s;}' &amp;gt; fuzzy_0p5_counts.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Getting a percentage out of a &quot;fuzzy&quot; result can be complicated by overlaps between hg38 regions that overlap a reference hg37 region. A more complicated procedure can be implemented with the &lt;code&gt;--echo-map&lt;/code&gt; option and some command-line work to implement logic to deal with that case more stringently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To get a straight-up count of no overlaps, build the inverse — or mask — of your hg38 regions, using &lt;a href=&quot;https://github.com/ENCODE-DCC/kentUtils&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kent tools&lt;/a&gt; &lt;code&gt;fetchChromSizes&lt;/code&gt; and BEDOPS &lt;code&gt;bedops --difference&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ fetchChromSizes hg38 | awk '{print $1&quot;\t0\t&quot;$2}' | sort-bed - &amp;gt; hg38.bounds.bed&#xA;$ bedops --difference hg38.bounds.bed hg38.bed &amp;gt; hg38.masked.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The regions in &lt;code&gt;hg38.masked.bed&lt;/code&gt; make up all of the genomic space in hg38, except for your lift-over regions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apply &lt;code&gt;--count --fraction-ref 1.0&lt;/code&gt; to count the number of reference (hg37) elements which completely overlap these masked hg38 regions:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ bedmap --count --fraction-ref 1.0 hg37.bed hg38.masked.bed | awk '{s+=$1}END{print s;}' &amp;gt; no_counts.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;http://bedops.readthedocs.io/en/latest/content/reference/statistics/bedmap.html#overlap-criteria&quot; rel=&quot;nofollow noreferrer&quot;&gt;documentation for &lt;code&gt;bedmap&lt;/code&gt;&lt;/a&gt; describes these overlap criteria and operations in more detail.&lt;/p&gt;&#xA;" OwnerUserId="776" LastEditorUserId="776" LastEditDate="2017-07-17T19:37:29.100" LastActivityDate="2017-07-17T19:37:29.100" CommentCount="0" />
  <row Id="2086" PostTypeId="2" ParentId="623" CreationDate="2017-07-17T23:46:37.833" Score="2" Body="&lt;p&gt;Pathway-tools has a unique identifier (called a Frame-ID) for each metabolite in its database.  You can get the list of all compounds with this api call:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;all_cpds = meta.get_class_all_instances('|Compounds|')&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;for each compound, you can also get its common name, or all its synonyms:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;cpd_names = {}&#xA;for cpd_frame_id in all_cpds:&#xA;      cpd_names[cpd_frame_id] = meta.get_name_string(cpd_frame_id)&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="1131" LastActivityDate="2017-07-17T23:46:37.833" CommentCount="1" />
  <row Id="2087" PostTypeId="2" ParentId="972" CreationDate="2017-07-18T13:01:54.007" Score="1" Body="&lt;p&gt;An alternative way is using the Hmmer website, and do a hmmsearch with the Pfam accession, forex. (PF02910). Navigate to the domain tab and find the domain architecture that you are interested in and press view scores. That way, only the sequences you are interested in will be downloadable under the tab downloads. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Step-by-Step guide:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1- Go to &lt;a href=&quot;http://www.ebi.ac.uk/Tools/hmmer/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.ebi.ac.uk/Tools/hmmer/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2- Search with hmmsearch &lt;a href=&quot;http://www.ebi.ac.uk/Tools/hmmer/search/hmmsearch&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.ebi.ac.uk/Tools/hmmer/search/hmmsearch&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3- Use Accession search and type in PF02910&lt;/p&gt;&#xA;&#xA;&lt;p&gt;4- Navigate to Domain&lt;/p&gt;&#xA;&#xA;&lt;p&gt;5- To the right side of &#xA;9742 sequences with domain architecture: FAD_binding_2, Succ_DH_flav_C &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Press View scores.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;6- It should show (Your results have been filtered) and Query matches (9742)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;7- Navigate to Download and download the sequences in the format of interest. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;8- Repeat for domain architectures of interest. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope you find this helpful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Kind Regards,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sara&lt;/p&gt;&#xA;" OwnerUserId="1049" LastActivityDate="2017-07-18T13:01:54.007" CommentCount="0" />
  <row Id="2088" PostTypeId="1" CreationDate="2017-07-18T13:26:44.693" Score="0" ViewCount="50" Body="&lt;p&gt;Suppose I have single-end RNA-seq data for which the reads in the fastq file are reversed with respect to the original extracted RNAs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I have the following workflow:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I map the reads on the reference genome, sort and index the resulting sam output.&lt;/li&gt;&#xA;&lt;li&gt;I use the bam file to make a bigwig file showing reads mapping on the plus and minus strands of the chromosomes.&lt;/li&gt;&#xA;&lt;li&gt;I use the bam file and an annotation file to quantify the gene expression.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Step 2. and 3. depend on step 1., but are not dependant on one another.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm sure that strandedness needs to be taken into account no later than at step 3.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The questions are the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is it common to have mapping tools with an option to &quot;correct&quot; for strandedness ?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assuming the bam file has been built ignoring library strandedness information, &lt;strong&gt;is it good practice to correct for strandedness when building bigwig files?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding this second point, I'm split between two attitudes:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;There should be no correction, the bigwig files are there to represent the &quot;raw&quot; mapping results.&lt;/li&gt;&#xA;&lt;li&gt;The bigwig files are there to give an idea of the origin and abundance of the originally extracted RNAs, so a strandedness correction should be used if necessary.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Which one is the most common attitude?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the second one is preferred, then wouldn't it be &quot;safer&quot; / &quot;cleaner&quot; to handle strandedness as early as possible? Possibly at the mapping, or even using a pre-processing step?&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-07-18T17:59:44.230" Title="At what processing step should library strandedness type be taken into account?" Tags="&lt;read-mapping&gt;&lt;strandedness&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="2089" PostTypeId="2" ParentId="2088" CreationDate="2017-07-18T15:07:12.493" Score="1" Body="&lt;p&gt;I usually take into account library strandness during the mapping to the reference step (but I never worked with bigwig files myself). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assuming RNA-seq data from Illumina, you can use &lt;a href=&quot;https://ccb.jhu.edu/software/hisat2/manual.shtml&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hisat2&lt;/a&gt; for alignment to the reference genome, and you have the option &lt;code&gt;--rna-strandness&lt;/code&gt; to specify the strand-specificity information (default unstranded). This is particularly important when you wish to have XS strand attribute in your bam file for easier compatibility with downstream software (especially important in my experience if you are planning to use some utility from &lt;a href=&quot;http://cole-trapnell-lab.github.io/cufflinks/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cufflinks&lt;/a&gt; for your step 3).&lt;/p&gt;&#xA;" OwnerUserId="294" LastActivityDate="2017-07-18T15:07:12.493" CommentCount="4" />
  <row Id="2090" PostTypeId="1" AcceptedAnswerId="2094" CreationDate="2017-07-18T15:34:49.860" Score="2" ViewCount="24" Body="&lt;p&gt;It seems that in snakemake the script specified after the &lt;code&gt;--jobscript&lt;/code&gt; cannot be used properly if a &lt;code&gt;workdir:&lt;/code&gt; is specified in the snakefile. &#xA;The path of the script specified becomes relative to the workdir defined in the snakefile instead of being relative to the current working directory. I do not know if this is a feature or a bug !!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nevertheless, it becomes quite painful to workaround this problem. The only way I found was to parse the command line in the snakefile, for example:&#xA;&lt;code&gt;&#xA;snakemake -c &quot;qsub&quot; -j 30 --js ./sge.sh --latency-wait 30 -rp&#xA;&lt;/code&gt;&#xA;in order to copy &lt;code&gt;./sge.sh&lt;/code&gt; in the output directory defined by the keyword &lt;code&gt;workdir&lt;/code&gt; in the snakefile.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a side effect, if you specify option for qsub on the command line, for example:&#xA;&lt;code&gt;&#xA;snakemake -c &quot;qsub -e ./logs/ -o ./logs/&quot; -j 30 --js ./sge.sh --latency-wait 30 -rp&#xA;&lt;/code&gt;&#xA;the &lt;code&gt;logs&lt;/code&gt; folder must be created under the workdir directory.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there something I don't understand with the &lt;code&gt;--jobscript&lt;/code&gt; option? Am I not using it the right way? Am I going against the best practice?&lt;/p&gt;&#xA;" OwnerUserId="373" LastActivityDate="2017-07-18T18:44:51.447" Title="combining the use of workdir and option --jobscript in snakemake" Tags="&lt;snakemake&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="2091" PostTypeId="2" ParentId="2088" CreationDate="2017-07-18T16:14:56.310" Score="0" Body="&lt;p&gt;Firstly, right at the start if the experiment it's important that your RNA samples are processed with a strand-specific protocol (e.g. Illumina's TruSeq Stranded) in order to produce stranded libraries for sequencing. If the samples haven't been treated as such, then there's nothing you can do to correct it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Secondly, I'm not sure stranded protocols are compatible with single-end reads. The aligners I'm aware of use the direction of aligned read pairs to assess the strand from which the RNA fragment has come from. How that works with single-end reads, I don't know?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, to answer your specific question, essentially step 1 is where you need to start with strand-specific options and do so at all subsequent steps as appropriate.&lt;/p&gt;&#xA;" OwnerUserId="1092" LastActivityDate="2017-07-18T16:14:56.310" CommentCount="3" />
  <row Id="2092" PostTypeId="1" CreationDate="2017-07-18T17:21:35.507" Score="2" ViewCount="30" Body="&lt;p&gt;I have found a blog post with a script that I would like to use for my current research project: &lt;a href=&quot;https://bcbio.wordpress.com/2009/02/07/automated-protein-conservation-display-from-blast-alignments/&quot; rel=&quot;nofollow noreferrer&quot;&gt;link&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The script is incredibly fast and produces a smooth conservation plot. In the blog post, the author mentions that it would be totally possible to change the remote blast function to a local database. Instead of making a local database, I have opted to try to change the function to still use a remote blast, but with different parameters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found the following resource for learning about the NCBIWWW module: &lt;a href=&quot;http://biopython.org/DIST/docs/api/Bio.Blast.NCBIWWW-module.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;link&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using this, I rewrote the line in the script:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blast_handle = NCBIWWW.qblast(blast_method, &quot;nr&quot;, search_gi)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;to:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blast_handle = NCBIWWW.qblast(program=&quot;blastp&quot;, &#xA;                                      database=&quot;refseq_protein&quot;, &#xA;                                      sequence=search_gi, &#xA;                                      entrez_query=&quot; txid2 [ORGN] OR txid4751 [ORGN] &quot;, &#xA;                                      expect=20000.0, &#xA;                                      alignments=10000, &#xA;                                      descriptions=10000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Strangely, I get the exact same graph as an output even with the different parameters. Am I doing something about this search incorrectly?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I got my BLAST to match the webpage. my current parameters are:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blast_handle = NCBIWWW.qblast(program=&quot;blastp&quot;, database=&quot;refseq_protein&quot;, sequence=&quot;P02686.3&quot;, word_size=6, expect=20000.0, hitlist_size=10000, gapcosts=&quot;11 1&quot;, matrix_name=&quot;BLOSUM62&quot;, filter=&quot;F&quot;, genetic_code=1, threshold=21, composition_based_statistics=2, alignments=10000, descriptions=10000, entrez_query=&quot; txid2 [ORGN] OR txid4751 [ORGN] &quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With this new blast_handle, the script no longer works. I get the following error codes:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;line 191, in &#xA;    main(sys.argv[1])&lt;/p&gt;&#xA;&#xA;&lt;p&gt;line 27, in main&#xA;    blast_rec = ncbi_manager.remote_blast(protein_gi, &quot;blastp&quot;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;line 184, in remote_blast&#xA;    return rec_it.next()&lt;/p&gt;&#xA;&#xA;&lt;p&gt;line 575, in parse&#xA;    raise ValueError(&quot;Your XML file was empty&quot;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;ValueError: Your XML file was empty&lt;/p&gt;&#xA;" OwnerUserId="727" LastEditorUserId="727" LastEditDate="2017-07-20T21:46:00.317" LastActivityDate="2017-07-20T21:46:00.317" Title="changing blast parameters using NCBIWWW module" Tags="&lt;biopython&gt;&lt;blast&gt;&lt;python&gt;" AnswerCount="0" CommentCount="3" FavoriteCount="1" />
  <row Id="2093" PostTypeId="2" ParentId="2088" CreationDate="2017-07-18T17:59:44.230" Score="1" Body="&lt;ol&gt;&#xA;&lt;li&gt;If your aligner has a strandedness option then go ahead and use it. The general idea here being that you'll preferentially align correctly to genes.&lt;/li&gt;&#xA;&lt;li&gt;Whether to bother with strandedness here depends on your goal. Are you interested in anti-sense transcription? Do you need to use the bigWig track to accurately quantify translational pausing or some other strand-specific phenomenon? If yes, then go ahead and make different bigWig files for each strand. If you just need the bigWig file for convenience in IGV or creating heatmaps (e.g., with deepTools) then you'll generally be fine without accounting for strand. For well-annotated species, the rate of anti-sense transcription is generally low and the rate of genes overlapping isn't absurdly high. Consequently, you can usually get away with using an unstranded bigWig.&lt;/li&gt;&#xA;&lt;li&gt;If you're using featureCounts you'll certainly want to use the appropriate strandedness setting.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;It's relatively common for RNAseq aligners/mappers to offer strandedness options. Certainly tophat2 and hisat(2) do, but also things like salmon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again, for bigWig files the question becomes whether you really need two files to get the information you need. You're not going to delete the BAM files, so you can always make them if you need them.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-18T17:59:44.230" CommentCount="0" />
  <row Id="2094" PostTypeId="2" ParentId="2090" CreationDate="2017-07-18T18:17:26.237" Score="2" Body="&lt;p&gt;To quote from the &lt;a href=&quot;https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html#configure-working-directory&quot; rel=&quot;nofollow noreferrer&quot;&gt;snakemake documentation&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;All paths in the snakefile are interpreted relative to the directory snakemake is executed in. This behaviour can be overridden by specifying a workdir in the snakefile:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;code&gt;workdir: &quot;path/to/workdir&quot;&lt;/code&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Usually, it is preferred to only set the working directory via the command line, because above directive limits the portability of Snakemake workflows.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It's not explicitly stated, but it's sort of implied that all relative paths then become relative to the working directory. I would expect that specifying an absolute path would get around this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As an aside, in my mind setting the working directory is usually only needed on clusters without a shared file system (presumably shared between the worker nodes, but not with the head node), since there you can't &lt;code&gt;cd $WORKDIR&lt;/code&gt; before running snakemake. This is then normally done with your scheduler, in such cases.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-07-18T18:44:51.447" LastActivityDate="2017-07-18T18:44:51.447" CommentCount="3" />
  <row Id="2095" PostTypeId="1" AcceptedAnswerId="2099" CreationDate="2017-07-18T22:32:49.527" Score="1" ViewCount="32" Body="&lt;p&gt;I used prokka to create a database for some fasta files but I noticed a strange difference between the prokka and fasta files. Normally the seqid for the gff3 output for prokka has seqid's that match the sequence id's of the fasta files. The files I just produced with prokka have a different seqid than my fasta files. My fasta files look like this &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;NODE_108_length_645_cov_0.679537_ID_215&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However the prokka output gff3 file for the same fasta file uses a seqid like this&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gnl|X|JHIOGLGG_28&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What would cause this difference? And do the last digits still correlate to the same seqid's even though the string before it has changed?&lt;/p&gt;&#xA;" OwnerUserId="842" LastEditorUserId="298" LastEditDate="2017-07-18T23:24:55.483" LastActivityDate="2017-07-19T08:31:11.713" Title="Does Prokka or gff3 change the sequence / subject ID of fasta files?" Tags="&lt;fasta&gt;&lt;gff3&gt;&lt;sequence-analysis&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2096" PostTypeId="2" ParentId="982" CreationDate="2017-07-18T22:47:05.673" Score="2" Body="&lt;p&gt;Another quick Google search reveals the &lt;a href=&quot;https://cran.r-project.org/web/packages/Peptides/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Peptides R package&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(Peptides)&#xA;s &amp;lt;- &quot;RKTTLVPNTQTASPR&quot;&#xA;charge(s)&#xA;&#xA;[1] 2.997683&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Optional arguments to &lt;code&gt;charge()&lt;/code&gt; are &lt;code&gt;pH&lt;/code&gt; (default = 7) and &lt;code&gt;pKscale&lt;/code&gt; (choice of nine, default = &quot;Lehninger&quot;). See &lt;code&gt;?charge&lt;/code&gt; for details.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another (non-R) option is the &lt;a href=&quot;http://emboss.sourceforge.net/apps/release/6.6/emboss/apps/iep.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;EMBOSS suite program iep&lt;/a&gt;. I highly recommend installing EMBOSS, it provides a suite of useful command-line tools with output that can be easily read and processed by other sofware, including R.&lt;/p&gt;&#xA;" OwnerUserId="150" LastActivityDate="2017-07-18T22:47:05.673" CommentCount="0" />
  <row Id="2097" PostTypeId="1" CreationDate="2017-07-19T03:09:25.747" Score="3" ViewCount="29" Body="&lt;p&gt;I want to survival analysis using the subset of TCGA LUAD dataset, which identifier are located at &lt;a href=&quot;https://xenabrowser.net/datapages/?host=https%3A%2F%2Ftcga.xenahubs.net&amp;amp;dataset=TCGA.LUAD.sampleMap%2FLUAD_clinicalMatrix&amp;amp;label=Phenotypes&amp;amp;allIdentifiers=true&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://xenabrowser.net/datapages/?host=https%3A%2F%2Ftcga.xenahubs.net&amp;amp;dataset=TCGA.LUAD.sampleMap%2FLUAD_clinicalMatrix&amp;amp;label=Phenotypes&amp;amp;allIdentifiers=true&lt;/a&gt; .&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; head(LUAD_clinicalMatrix[,c(&quot;X_EVENT&quot;,&quot;X_OS_IND&quot;)])&#xA;  X_EVENT X_OS_IND&#xA;1      NA       NA&#xA;2       0        0&#xA;3       1        1&#xA;4       0        0&#xA;5       0        0&#xA;6       0        0&#xA;&#xA;&amp;gt; all(LUAD_clinicalMatrix$X_EVENT==LUAD_clinicalMatrix$X_OS_UNIT)&#xA;[1] FALSE&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to know which identifier means the sample is alive or dead, and &quot;1&quot; means alive or dead?&lt;/p&gt;&#xA;" OwnerUserId="1133" LastActivityDate="2017-07-19T08:55:52.523" Title="What does the &quot;X_OS_IND&quot; column mean?" Tags="&lt;r&gt;&lt;file-formats&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2098" PostTypeId="1" CreationDate="2017-07-19T04:55:16.090" Score="4" ViewCount="40" Body="&lt;p&gt;My goal is to work on GWAS method development as a research project. However, human genome data are usually confidential because of the identification problem, so it's very hard to get them. (application takes a long time and I may not get it at the end.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, maybe I can try to play with the data of other species first. So I wonder where I can find some GWAS data for Zebrafish and rice. (The reason I am asking for these two is that I find researchers tend to perform genome analysis to mice, Arabidopsis Thaliana, zebrafish, and rice, and I already found where to download the GWAS data for the first two.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can find several data sets (like &lt;a href=&quot;https://zfin.org/downloads&quot; rel=&quot;nofollow noreferrer&quot;&gt;ZFIN&lt;/a&gt;) that have the annotated genome information, but that's only one genome. Where can I download the raw data (like genotypes of 500 zebrafishes and corresponding phenotypes) to start my own analysis? &lt;/p&gt;&#xA;" OwnerUserId="1082" LastEditorUserId="1082" LastEditDate="2017-07-19T16:14:25.300" LastActivityDate="2017-08-18T19:42:09.680" Title="Where can I find a GWAS data set about genotypes and phenotypes for Zebrafish and rice?" Tags="&lt;database&gt;&lt;gwas&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="0" />
  <row Id="2099" PostTypeId="2" ParentId="2095" CreationDate="2017-07-19T08:31:11.713" Score="3" Body="&lt;p&gt;Prokka is an annotation tool. It takes your contigs file with fasta headers &lt;code&gt;&amp;gt;NODE_108...&lt;/code&gt;and searchs for ORFs and their putative functions. Prokka produces submission ready output, the new fasta headers you see &lt;code&gt;&amp;gt;JHIOGLGG_28...&lt;/code&gt;are locus tags and gene description from the annotation and are not related to contig node data. Please see the &lt;a href=&quot;https://github.com/tseemann/prokka#output-files&quot; rel=&quot;nofollow noreferrer&quot;&gt;github information page&lt;/a&gt; for more info.&lt;/p&gt;&#xA;" OwnerUserId="982" LastActivityDate="2017-07-19T08:31:11.713" CommentCount="0" />
  <row Id="2100" PostTypeId="2" ParentId="2097" CreationDate="2017-07-19T08:55:52.523" Score="3" Body="&lt;p&gt;According to &lt;a href=&quot;https://groups.google.com/forum/#!topic/ucsc-cancer-genomics-browser/YvKnWZSsw1Q&quot; rel=&quot;nofollow noreferrer&quot;&gt;this link&lt;/a&gt;, 1=death 0=censor and null=no data.&lt;/p&gt;&#xA;" OwnerUserId="931" LastActivityDate="2017-07-19T08:55:52.523" CommentCount="0" />
  <row Id="2101" PostTypeId="1" AcceptedAnswerId="2108" CreationDate="2017-07-19T13:41:05.330" Score="3" ViewCount="30" Body="&lt;p&gt;I have surprisingly low counts when running &lt;code&gt;featureCounts&lt;/code&gt; on some (single-end) RNA-seq data mapped on &lt;em&gt;C. elegans&lt;/em&gt; genome using &lt;code&gt;hisat2&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To more easily show the problem, I generated a small subset of the bam file and of the annotation file I'm using. Here is what I can see when loading these two files on IGV:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/iijez.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/iijez.png&quot; alt=&quot;View of the his-16 -&amp;gt; his-12 region in IGV&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The coverage for the &quot;his-11&quot; and &quot;his-15&quot; genes peak at around 3600. For &quot;his-12&quot; and &quot;his-16&quot;, it peaks at more than 1000&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the content of my annotation file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ cat his_11-16.gtf &#xA;II  ensembl transcript  13817759    13818143    .   -   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001890&quot;; gene_name &quot;his-16&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P8354&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.10&quot;; transcript_name &quot;ZK131.10&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS11539&quot;;&#xA;II  ensembl transcript  13818371    13818739    .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001889&quot;; gene_name &quot;his-15&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P8185&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.9&quot;; transcript_name &quot;ZK131.9&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS52436&quot;;&#xA;II  ensembl transcript  13819626    13819937    .   -   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001888&quot;; gene_name &quot;his-14&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P21890&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.8&quot;; transcript_name &quot;ZK131.8&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS50308&quot;;&#xA;II  ensembl transcript  13820210    13820620    .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001887&quot;; gene_name &quot;his-13&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P2149&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.7&quot;; transcript_name &quot;ZK131.7&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS35106&quot;;&#xA;II  ensembl transcript  13821198    13821581    .   -   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001886&quot;; gene_name &quot;his-12&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P26082&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.6&quot;; transcript_name &quot;ZK131.6&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS23693&quot;;&#xA;II  ensembl transcript  13821778    13822314    .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00001885&quot;; gene_name &quot;his-11&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P10535&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;ZK131.5&quot;; transcript_name &quot;ZK131.5&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS52792&quot;;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;featureCounts&lt;/code&gt; run:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ featureCounts -a his_11-16.gtf -o his_11-16_counts.txt -t transcript -g gene_name -O his_11-16_sorted.bam&#xA;&#xA;        ==========     _____ _    _ ____  _____  ______          _____  &#xA;        =====         / ____| |  | |  _ \|  __ \|  ____|   /\   |  __ \ &#xA;          =====      | (___ | |  | | |_) | |__) | |__     /  \  | |  | |&#xA;            ====      \___ \| |  | |  _ &amp;lt;|  _  /|  __|   / /\ \ | |  | |&#xA;              ====    ____) | |__| | |_) | | \ \| |____ / ____ \| |__| |&#xA;        ==========   |_____/ \____/|____/|_|  \_\______/_/    \_\_____/&#xA;      v1.5.2&#xA;&#xA;//========================== featureCounts setting ===========================\\&#xA;||                                                                            ||&#xA;||             Input files : 1 BAM file                                       ||&#xA;||                           S his_11-16_sorted.bam                           ||&#xA;||                                                                            ||&#xA;||             Output file : his_11-16_counts.txt                             ||&#xA;||                 Summary : his_11-16_counts.txt.summary                     ||&#xA;||              Annotation : his_11-16.gtf (GTF)                              ||&#xA;||      Dir for temp files : ./                                               ||&#xA;||                                                                            ||&#xA;||                 Threads : 1                                                ||&#xA;||                   Level : meta-feature level                               ||&#xA;||              Paired-end : no                                               ||&#xA;||         Strand specific : no                                               ||&#xA;||      Multimapping reads : not counted                                      ||&#xA;|| Multi-overlapping reads : counted                                          ||&#xA;||   Min overlapping bases : 1                                                ||&#xA;||                                                                            ||&#xA;\\===================== http://subread.sourceforge.net/ ======================//&#xA;&#xA;//================================= Running ==================================\\&#xA;||                                                                            ||&#xA;|| Load annotation file his_11-16.gtf ...                                     ||&#xA;||    Features : 6                                                            ||&#xA;||    Meta-features : 6                                                       ||&#xA;||    Chromosomes/contigs : 1                                                 ||&#xA;||                                                                            ||&#xA;|| Process BAM file his_11-16_sorted.bam...                                   ||&#xA;||    Single-end reads are included.                                          ||&#xA;||    Assign reads to features...                                             ||&#xA;||    Total reads : 35037                                                     ||&#xA;||    Successfully assigned reads : 1849 (5.3%)                               ||&#xA;||    Running time : 0.00 minutes                                             ||&#xA;||                                                                            ||&#xA;||                         Read assignment finished.                          ||&#xA;||                                                                            ||&#xA;|| Summary of counting results can be found in file &quot;his_11-16_counts.txt&quot;    ||&#xA;||                                                                            ||&#xA;\\===================== http://subread.sourceforge.net/ ======================//&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And the resulting counts file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ cat his_11-16_counts.txt&#xA;# Program:featureCounts v1.5.2; Command:&quot;featureCounts&quot; &quot;-a&quot; &quot;his_11-16.gtf&quot; &quot;-o&quot; &quot;his_11-16_counts.txt&quot; &quot;-t&quot; &quot;transcript&quot; &quot;-g&quot; &quot;gene_name&quot; &quot;-O&quot; &quot;his_11-16_sorted.bam&quot; &#xA;Geneid  Chr Start   End Strand  Length  his_11-16_sorted.bam&#xA;his-16  II  13817759    13818143    -   385 869&#xA;his-15  II  13818371    13818739    +   369 3&#xA;his-14  II  13819626    13819937    -   312 953&#xA;his-13  II  13820210    13820620    +   411 23&#xA;his-12  II  13821198    13821581    -   384 423&#xA;his-11  II  13821778    13822314    +   537 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Why such a low assignment rate, why so little counts?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see that &lt;code&gt;hisat2&lt;/code&gt; decided to split an important proportion of the reads between orthologs, like &quot;his-16&quot; and &quot;his-12&quot;, but my understanding is that the default values for the &lt;code&gt;minOverlap&lt;/code&gt; and &lt;code&gt;fracOverlap&lt;/code&gt; parameters should ensure that such split reads will be counted:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  --minOverlap &amp;lt;int&amp;gt;  Minimum number of overlapping bases in a read that is&#xA;                      required for read assignment. 1 by default. Number of&#xA;                      overlapping bases is counted from both reads if paired&#xA;                      end. If a negative value is provided, then a gap of up&#xA;                      to specified size will be allowed between read and the&#xA;                      feature that the read is assigned to.&#xA;&#xA;  --fracOverlap &amp;lt;float&amp;gt; Minimum fraction of overlapping bases in a read that is&#xA;                      required for read assignment. Value should be within range&#xA;                      [0,1]. 0 by default. Number of overlapping bases is&#xA;                      counted from both reads if paired end. Both this option&#xA;                      and '--minOverlap' option need to be satisfied for read&#xA;                      assignment.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What did I get wrong?&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-07-19T21:16:01.000" Title="Counts obtained by featureCounts seem much less than observed coverage" Tags="&lt;read-mapping&gt;&lt;featurecounts&gt;&lt;coverage&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="2102" PostTypeId="2" ParentId="2081" CreationDate="2017-07-19T15:02:52.070" Score="1" Body="&lt;p&gt;I would suggest you to run some simulation. You can use &lt;a href=&quot;https://github.com/lh3/wgsim&quot; rel=&quot;nofollow noreferrer&quot;&gt;wgsim&lt;/a&gt; to simulate reads that are as much similar to what you want an answer for.&#xA;I don't think there are faster methods.&lt;/p&gt;&#xA;" OwnerUserId="1140" LastActivityDate="2017-07-19T15:02:52.070" CommentCount="1" />
  <row Id="2103" PostTypeId="2" ParentId="2081" CreationDate="2017-07-19T19:20:48.830" Score="5" Body="&lt;p&gt;Mapping quality is determined by the repetitiveness of the genome, the sequencing error rate, insert size, the capability of the mapper and the nasty heuristics behind the mapper. MAPQ=0 to one mapper is not necessarily MAPQ=0 to another.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, I get what you mean. You want to know the uniqueness/repetitiveness. It is still hard if you want to get a useful answer. For 150bp reads, each reference position is covered by 150 reads. What if 50 of them have no other exact hits elsewhere, but the rest 100 have? Is this position a repeat or not? In addition, what if the 50 are unique only because one mismatch? If there is a variant at that mismatch, the 50 would become repeats or mapped elsewhere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My preference is to say a position is &quot;unique&quot; under k-long reads if over k/2 reads overlapping the position have no other perfect or 1-mismatch/1-gap hits elsewhere in the genome. Under this definition, 79.3% of human genome are unique for 35bp reads. 92.4% for 75bp reads. I don't have the number for 150bp reads. I guess it will be around 95%. Empirically, 94-95% of human genome is callable with 100bp paired-end reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As to other measurements, the most common one is the fraction of reads that has an exact hit elsewhere. You can use Fabio's method. It gives a good enough estimate once you simulate over 1 million reads. This fraction is around 86% for 35bp reads and 95% for 75bp reads, as I remember. The problem with this approach is this fraction is not very informative to variant calling due to the issues I talked about. Another way is to use RepeatMasker. It is worse. RepeatMasker masks 50% of human genome, but excludes segmental duplications where short reads can't be confidently mapped.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-07-19T19:20:48.830" CommentCount="3" />
  <row Id="2104" PostTypeId="2" ParentId="2098" CreationDate="2017-07-19T19:39:13.950" Score="0" Body="&lt;p&gt;I think it will be more productive for you if you choose one of the public human datasets and practise your GWAS methods on that. A couple of examples examples:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.internationalgenome.org/data&quot; rel=&quot;nofollow noreferrer&quot;&gt;1000 genomes&lt;/a&gt; -- use &quot;population&quot; as your trait of association. You'll find that you get association across the whole genome, so the analysis method would change from &quot;what bits are associated&quot; to &quot;what are the bits that are most consistently associated&quot;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;NCI &lt;a href=&quot;https://portal.gdc.cancer.gov/repository&quot; rel=&quot;nofollow noreferrer&quot;&gt;Genomic Data Commons&lt;/a&gt; -- open access (and controlled access) data associated with cancer&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-07-19T19:39:13.950" CommentCount="3" />
  <row Id="2106" PostTypeId="1" AcceptedAnswerId="2107" CreationDate="2017-07-19T20:49:38.947" Score="3" ViewCount="18" Body="&lt;p&gt;Could some one please help me with understanding how to generate the ribosomal interval list that is required to when using picard metrics this step&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java -jar picard.jar CollectRnaSeqMetrics \&#xA;      I=input.bam \&#xA;      O=output.RNA_Metrics \&#xA;      REF_FLAT=ref_flat.txt \&#xA;      STRAND=SECOND_READ_TRANSCRIPTION_STRAND \&#xA;      RIBOSOMAL_INTERVALS=ribosomal.interval_list&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have a couple of bam files that were aligned using mm9 ensembl gtf as annotation, i did a lot of search and found this link Ribosomal Intervals For Collectrnaseqmetrics but this link or any other source does not explain how to produce or find ribo interval list from ensembl annotation&lt;/p&gt;&#xA;" OwnerUserId="926" LastActivityDate="2017-07-19T21:20:24.753" Title="ribo interval list for picard ensembl mm9 gtf" Tags="&lt;rna-seq&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2107" PostTypeId="2" ParentId="2106" CreationDate="2017-07-19T21:08:28.883" Score="3" Body="&lt;p&gt;At least with Ensembl annotations, you can &lt;code&gt;grep rRNA foo.gtf &amp;gt; rRNA.gtf&lt;/code&gt; to get the annotated rRNAs. Note however, that the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/gene/100861531&quot; rel=&quot;nofollow noreferrer&quot;&gt;45S cassette&lt;/a&gt; isn't in the mouse reference genome, so you'll never have alignments to 18S, 5.8S, or 28S rRNA. I always get the feeling that the Collect*Metrics programs are made with human sequences in mind, since the cassette is included in the human reference genome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyway, while you can use the annotation and reference sequence to make picard happy, make sure to append the Rn45S sequence to your genome if you want to actually measure rRNA levels. In my experience this is only useful for rRNA-depleted samples, since the rates can vary wildly there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: To make a file compatible with picard's &quot;interval list&quot;, one can do the following with the GTF file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;grep rRNA foo.gtf | awk 'BEGIN{OFS=&quot;\t&quot;}{print $1, $4-1, $5}' &amp;gt; foo.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;According to the &lt;a href=&quot;https://gatkforums.broadinstitute.org/gatk/discussion/1319/collected-faqs-about-interval-lists&quot; rel=&quot;nofollow noreferrer&quot;&gt;picard documentation&lt;/a&gt; a BED file can be used as input and the above &lt;code&gt;awk&lt;/code&gt; code extracts the appropriate columns and switches to 0-based coordinates used in the BED format.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-07-19T21:20:24.753" LastActivityDate="2017-07-19T21:20:24.753" CommentCount="4" />
  <row Id="2108" PostTypeId="2" ParentId="2101" CreationDate="2017-07-19T21:16:01.000" Score="4" Body="&lt;p&gt;Given the high level of multimapping in this region, you'll need to use the &lt;code&gt;-M --primary&lt;/code&gt; options if you want to keep many of the alignments. I would be very hesitant to use these numbers as input for DESeq2 or similar programs, since it's fairly questionable whether one should fully trust the &quot;randomness&quot; of the aligner's assignments. I'm more comfortable using something like &lt;a href=&quot;http://salmon.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;salmon&lt;/a&gt; for such cases.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-19T21:16:01.000" CommentCount="0" />
  <row Id="2109" PostTypeId="1" CreationDate="2017-07-20T12:38:01.380" Score="1" ViewCount="13" Body="&lt;p&gt;NCI-60 has additional &lt;a href=&quot;https://dtp.cancer.gov/discovery_development/nci-60/cell_list.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;cell lines&lt;/a&gt;, but when I get results from &lt;a href=&quot;https://discover.nci.nih.gov/cellminer/&quot; rel=&quot;nofollow noreferrer&quot;&gt;CellMiner&lt;/a&gt; they are not included in there. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Namely, the genes such as:  &lt;code&gt;DLD-1&lt;/code&gt;(Colon), &lt;code&gt;KM20L2&lt;/code&gt;    (Colon), &lt;code&gt;M19-MEL&lt;/code&gt; (Melanoma), don't show in the results of CellMiner. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there away to include these additional lines in the correlation calculations?&lt;/p&gt;&#xA;" OwnerUserId="734" LastEditorUserId="734" LastEditDate="2017-07-20T13:08:03.397" LastActivityDate="2017-07-20T13:08:03.397" Title="Working with the additional cell lines in NCI-60" Tags="&lt;cancer&gt;&lt;cell-line&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="0" />
  <row Id="2110" PostTypeId="2" ParentId="2081" CreationDate="2017-07-20T14:51:07.630" Score="3" Body="&lt;p&gt;As has been said before, mappability to the 'human genome' depends on a number of factors, among these the reference version and type of reads, for which you are interested in GRCh38 and 2x150bp reads. Although I am not aware of numbers accounting for these particular reference and type of Illumina reads, the 1000 genomes project has provided the community with a similar and close estimation that you might be interested in considering regarding your inquiry.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similar to your question, the 1K project estimated 'the proportion of the human genome that is less accessible to short reads'. In these estimates the human genome is GRCh37 and the types of reads in question are mostly 2x Illumina with a mixture of lengths with the longest being up to 250bp. In these estimates each base in the human genome is considered (and marked) 'less accesible' according to these criteria:  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;L - depth of coverage is much lower than average&lt;/p&gt;&#xA;&#xA;&lt;p&gt;H - depth of coverage is much higher than average&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Z - too many reads with zero mapping quality overlap this position&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Q - the average mapping quality at the position is too low&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Each of these criteria has &quot;standard&quot; and &quot;strict&quot; thresholds for a base to be considered - or not - in each category. You can read more in the link below: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/accessible_genome_masks/README.accessible_genome_mask.20140520&quot; rel=&quot;nofollow noreferrer&quot;&gt;ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/accessible_genome_masks/README.accessible_genome_mask.20140520&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to the strict thresholds, the human genome has about 16.8% of &quot;Z&quot; and 3.1% of &quot;Q&quot; bases, respectively. Considering the &quot;Z&quot; and &quot;Q&quot; criteria as a proxy for ~ mapq=0, about 19.9% of the human genome can not be uniquely mapped. &lt;/p&gt;&#xA;" OwnerUserId="475" LastActivityDate="2017-07-20T14:51:07.630" CommentCount="0" />
  <row Id="2111" PostTypeId="1" CreationDate="2017-07-20T15:53:54.547" Score="1" ViewCount="37" Body="&lt;p&gt;I'm using the Peptides package in R to calculate the mass of peptides with the mw() method. The think is that i want to calculate them in different pH values and thus I wrote this little function to take in mind the pH value according this image &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/iA96t.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/iA96t.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code is this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;phmw = function(pept_seq , unimod_location, pH ){&#xA;&#xA;  monoisotopic_hydrogen_mass = 1.00794&#xA;&#xA;  if(pH &amp;lt; 7){&#xA;&#xA;    # mol_weight = mol_weight_of_peptide + mol_weight_of_Hydrogen&#xA;    mol_weight = mw(pept_seq, monoisotopic = T) + monoisotopic_hydrogen_mass&#xA;&#xA;  }else if(pH &amp;gt; 7){&#xA;&#xA;    # mol_weight = mol_weight_of_peptide - mol_weight_of_Hydrogen&#xA;    mol_weight = mw(pept_seq, monoisotopic = T) - monoisotopic_hydrogen_mass&#xA;&#xA;  }else if(pH == 7){&#xA;&#xA;    # mol_weight = mol_weight_of_peptide &#xA;    mol_weight = mw(pept_seq, monoisotopic = T) &#xA;  }&#xA;&#xA;  return(mol_weight)&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do you think that this approach is correct or not ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;" OwnerUserId="704" LastActivityDate="2017-07-20T15:53:54.547" Title="Influence of pH on protein peptide mass" Tags="&lt;r&gt;&lt;proteins&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="2112" PostTypeId="2" ParentId="2078" CreationDate="2017-07-20T23:10:02.150" Score="2" Body="&lt;p&gt;For MAC OSX, the executable found in &lt;code&gt;arlsumstat_macosx&lt;/code&gt; (&lt;code&gt;arlsumstatmac_64bit&lt;/code&gt;) seem to work appropriately. The directory for &lt;code&gt;arlsumstat_macosx&lt;/code&gt; does not contain any examples but you can use example files from &lt;code&gt;arlecore_macosx/Example files_linux&lt;/code&gt; using this syntax.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The complication is that &lt;code&gt;arlsumstatmac_64bit&lt;/code&gt; will look in the current directory for the file &lt;code&gt;arl_run.ars&lt;/code&gt;. There is a file with the same name in &lt;code&gt;arlecore_macosx&lt;/code&gt; but it is not compatible. So you have to be in the &lt;code&gt;arlsumstat_macosx&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In short, &lt;code&gt;cd&lt;/code&gt; to &lt;code&gt;arlsumstat_macosx&lt;/code&gt; and do&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./arlsumstatmac_64bit path/to/arlecore_macosx/Example\ files_linux/Freqncy/cohen.ars path/to/arlecore_macosx/Example\ files_linux/Freqncy/cohen.arp&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;, where the &lt;code&gt;.arp&lt;/code&gt; file contains the data and the &lt;code&gt;.ars&lt;/code&gt; file contains the settings (the description of what you want Arlequin to do).&lt;/p&gt;&#xA;" OwnerUserId="888" LastActivityDate="2017-07-20T23:10:02.150" CommentCount="0" />
  <row Id="2113" PostTypeId="1" CreationDate="2017-07-21T12:33:07.187" Score="8" ViewCount="110" Body="&lt;p&gt;A different sort of problem: even a small 'omics lab generates a lot of data, raw, intermediate and processed. What (software) solutions exist for managing this data, such that &quot;old&quot; data can be retrieved and checked or re-analysed, even after people have left the lab? Important points would be:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ease of installation&lt;/li&gt;&#xA;&lt;li&gt;ease of putting data in, in an appropriately tagged / labeled fashion (it's no good if the repository is just a centralised bad mess)&lt;/li&gt;&#xA;&lt;li&gt;useful search and exploration&lt;/li&gt;&#xA;&lt;li&gt;security (i.e. restricted to members of the lab)&lt;/li&gt;&#xA;&lt;li&gt;previews / summaries of the data&lt;/li&gt;&#xA;&lt;li&gt;can accommodate any dataset&lt;/li&gt;&#xA;&lt;li&gt;local, not SaaS&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;It seems from my explorations of the topic that there's very little between the primitive (e.g. handrolled Access databases) and big industrial solutions. Things I have looked at include:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Dataverse: very popular, installation seems complex and unclear if uploading is that easy&lt;/li&gt;&#xA;&lt;li&gt;DSpace: mostly for publications and documents&lt;/li&gt;&#xA;&lt;li&gt;CKAN&lt;/li&gt;&#xA;&lt;li&gt;OSF: used this for a while, integrates with a lot of services but uploading data seemed awkward&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="377" LastActivityDate="2017-07-25T22:01:40.587" Title="Solutions for managing data in a small bioinformatics / 'omics lab?" Tags="&lt;database&gt;&lt;data-management&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="2114" PostTypeId="1" AcceptedAnswerId="2115" CreationDate="2017-07-21T13:03:59.900" Score="4" ViewCount="136" Body="&lt;p&gt;I have a binary matrix of gene presence or absence which looks like: [roary output]&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Gene    sample1 sample2 sample3 sample4&#xA;fliI        1       1       1        1&#xA;patB_1      1       1       1        1&#xA;pgpA        1       1       1        1&#xA;osmB        1       1       1        1&#xA;cspA        1       0       1        1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can I convert to fasta so it looks like and technically aligned:  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;sample1&#xA;11111&#xA;&amp;gt;sample2&#xA;11110&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is it possible to do this in R? &lt;/p&gt;&#xA;" OwnerUserId="982" LastActivityDate="2017-07-22T10:53:27.133" Title="How to convert a binary matrix of gene presence or absence into a fasta sequence" Tags="&lt;r&gt;&lt;fasta&gt;&lt;format-conversion&gt;" AnswerCount="3" CommentCount="2" FavoriteCount="1" />
  <row Id="2115" PostTypeId="2" ParentId="2114" CreationDate="2017-07-21T13:20:17.730" Score="6" Body="&lt;p&gt;It's not a fasta file, but:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;&amp;gt; m&#xA;       sample1 sample2 sample3 sample4&#xA;fliI         1       1       1       1&#xA;patB_1       1       1       1       1&#xA;pgpA         1       1       1       1&#xA;osmB         1       1       1       1&#xA;cspA         1       0       1       1&#xA;&amp;gt; # Collapse to labeled strings&#xA;&amp;gt; blah = apply(m, 2, function(x) paste(x, collapse=''))&#xA;&amp;gt; blah&#xA;sample1 sample2 sample3 sample4 &#xA;&quot;11111&quot; &quot;11110&quot; &quot;11111&quot; &quot;11111&quot; &#xA;&amp;gt; # Write that to a file with the appropriate format&#xA;&amp;gt; cat(paste(mapply(function(x, y) sprintf(&quot;&amp;gt;%s\n%s\n&quot;, x, y), names(blah), blah), collapse=&quot;&quot;), file=&quot;some file.fa&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Make sure to change &lt;code&gt;some file.fa&lt;/code&gt;. I have no idea why you would want to do all of this, but this will give you the output you want:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ cat blah.txt &#xA;&amp;gt;sample1&#xA;11111&#xA;&amp;gt;sample2&#xA;11110&#xA;&amp;gt;sample3&#xA;11111&#xA;&amp;gt;sample4&#xA;11111&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-21T13:20:17.730" CommentCount="1" />
  <row Id="2116" PostTypeId="2" ParentId="2114" CreationDate="2017-07-21T18:05:08.483" Score="3" Body="&lt;p&gt;One could also use &lt;code&gt;cut&lt;/code&gt; and a pair of Python scripts:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;transpose.py&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python                                                                                                                                                                                                                                                                                                                       &#xA;&#xA;import sys&#xA;&#xA;for c in zip(*(l.split() for l in sys.stdin.readlines() if l.strip())):&#xA;    sys.stdout.write(&quot;%s\n&quot; % ('\t'.join(c)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;mock_fasta.py&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python                                                                                                                                                                                                                                                                                                                       &#xA;&#xA;import sys&#xA;&#xA;for e in (l.split() for l in sys.stdin.readlines() if l.strip()):&#xA;    sys.stdout.write(&quot;&amp;gt;%s\n%s\n&quot; % (e[0], ''.join(e[1:])))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ cut -f2- in.mtx | transpose.py | mock_fasta.py &amp;gt; in.fa&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which gives:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ more in.fa&#xA;&amp;gt;sample1&#xA;11111&#xA;&amp;gt;sample2&#xA;11110&#xA;&amp;gt;sample3&#xA;11111&#xA;&amp;gt;sample4&#xA;11111&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="776" LastActivityDate="2017-07-21T18:05:08.483" CommentCount="0" />
  <row Id="2117" PostTypeId="1" CreationDate="2017-07-21T18:15:17.510" Score="3" ViewCount="33" Body="&lt;p&gt;I have identified a genomic region (CpG island) that is enriched for a characteristic of interest in the vast majority of my samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After viewing it in the UCSC genome browser, I have confirmed that it does not overlap any annotated gene promoters, gene bodies, or retrotransposons, but am convinced that it is functionally interacting with something long range.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a publicly available visualization tool where I can input my genomic region and get a list of topologically associated domains from 3C or other chromatin-chromatin interaction data?&lt;/p&gt;&#xA;" OwnerUserId="643" LastActivityDate="2017-07-21T18:47:06.840" Title="Visualization tools for 3C/Hi-C long-range interaction data?" Tags="&lt;software-recommendation&gt;&lt;visualization&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="2118" PostTypeId="2" ParentId="2117" CreationDate="2017-07-21T18:47:06.840" Score="1" Body="&lt;p&gt;Depending on the genome you want, you may be able to use the &lt;a href=&quot;http://chorogenome.ie-freiburg.mpg.de/&quot; rel=&quot;nofollow noreferrer&quot;&gt;chorogeneome navigator&lt;/a&gt;, which is from some of my colleagues. This uses a number of public HiC datasets and allows you to view interactions.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-21T18:47:06.840" CommentCount="0" />
  <row Id="2119" PostTypeId="1" AcceptedAnswerId="2120" CreationDate="2017-07-21T19:45:06.567" Score="3" ViewCount="43" Body="&lt;p&gt;I have a problem here with my rna seq data:&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Sequencing details:&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;rRNA was removed, followed by cDNA preparation and generation of stranded libraries using the TruSeq Stranded Total RNA Sample Prep Kit. Sequencing performed on the HiSeq2500 platform (Illumina) to generate 2 × 125 bp paired-end reads&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Alignment and preprocessing&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;reads were aligned using tophat2(std aligner in pipeline at core (&lt;code&gt;--library-type fr-firststrand&lt;/code&gt;), unfortunately the original unaligned files were purged and i only have access to this aligned file which i converted to fastq using following steps&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;samtools sort -n sample.bam -o sample_sorted.bam&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;bedtools bamtofastq -i sample_sorted.bam -fq sample_1.fq -fq2 sample_2.fq&lt;/code&gt; (get a lot of mate skipping errors here)&lt;/li&gt;&#xA;&lt;li&gt;check for these reads for adapters and trim them and again use hisat2 (with &lt;code&gt;--rna-strandness RF&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h1&gt;trimmed reads&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Reads were trimmed by passing parameters &lt;code&gt;--adapters adapters.file --adapter-trim-end RIGHT --length-dist --threads 12 --adapter-min-overlap 7 --max-uncalled 250 --min-read-length 25 --&lt;/code&gt; to FLEXBAR version 2.4&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;adapters for all samples using multiqc&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/IsVW7.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/IsVW7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;fastqc sequence length pre trimming&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/fPONe.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/fPONe.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;fastqc sequence length post trimming&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/3nY5A.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/3nY5A.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;rmats error&lt;/h2&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Incorrect readLength. sample.bam has a read length of 114, while readLength param is 125&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h2&gt;picard error on aligned bam from hisat2)&lt;/h2&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;ProcessExecutor   &lt;a href=&quot;https://i.stack.imgur.com/fPONe.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;2&lt;/a&gt; &quot;Not creating insert size PDF as there are duplicated header names: All_Reads&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h2&gt;snapshot of picard insertsize metrics file&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/lgeHk.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/lgeHk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Questions&lt;/h1&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Is my approach flawed at some step?&lt;/li&gt;&#xA;&lt;li&gt;why do i get different read lengths error for each sample when i process it through rmats when i specify len as 125?&lt;/li&gt;&#xA;&lt;li&gt;why is no histogram being produced when using picard insertsize metrics?&lt;/li&gt;&#xA;&lt;li&gt;is normal to see the graph shift to the right after trimming?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h1&gt;insertsize for all samples (data from picard collectinsertsize complied usingmultiqc)&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/l3A9I.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/l3A9I.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="926" LastEditorUserId="29" LastEditDate="2017-07-24T16:27:41.447" LastActivityDate="2017-07-24T16:27:41.447" Title="insert size pre and post trimming" Tags="&lt;rna-seq&gt;&lt;rna-splicing&gt;&lt;trimming&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="2120" PostTypeId="2" ParentId="2119" CreationDate="2017-07-21T22:19:44.113" Score="3" Body="&lt;ol&gt;&#xA;&lt;li&gt;It's not an unreasonable approach. rMATs is rather picky about its input, but it seems you've noticed that.&lt;/li&gt;&#xA;&lt;li&gt;rMATs can't handle trimmed reads. It also can't handle soft-clipped reads. You might as well not trim. You'll get a lower alignment rate, but your only other choice would be to either exclude the trimmed reads or trim everything down to the same length.&lt;/li&gt;&#xA;&lt;li&gt;You have a mixture of RF and FR alignments and the plotting script can't handle that.&lt;/li&gt;&#xA;&lt;li&gt;Assuming you mean the read length distribution, it's actually shifted to the left, not the right. You had a single length before and a long left tail after trimming.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-21T22:19:44.113" CommentCount="2" />
  <row Id="2121" PostTypeId="2" ParentId="2114" CreationDate="2017-07-22T10:53:27.133" Score="3" Body="&lt;p&gt;A mixture of &lt;code&gt;apply&lt;/code&gt;, &lt;code&gt;paste&lt;/code&gt; and &lt;code&gt;cat&lt;/code&gt; will do this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; data.mat&#xA;       sample1 sample2 sample3 sample4&#xA;fliI         1       1       1       1&#xA;patB_1       1       1       1       1&#xA;pgpA         1       1       1       1&#xA;osmB         1       1       1       1&#xA;cspA         1       0       1       1&#xA;&amp;gt; data.conv &amp;lt;- apply(data.mat,2,paste,collapse=&quot;&quot;) # aggregate along 2nd dim&#xA;&amp;gt; cat(paste0(&quot;&amp;gt;&quot;, names(data.conv), &quot;\n&quot;, data.conv), sep=&quot;\n&quot;)&#xA;&amp;gt;sample1&#xA;11111&#xA;&amp;gt;sample2&#xA;11110&#xA;&amp;gt;sample3&#xA;11111&#xA;&amp;gt;sample4&#xA;11111&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If writing to a file is required, just add a &lt;code&gt;file&lt;/code&gt; parameter to the &lt;code&gt;cat&lt;/code&gt; command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat(paste0(&quot;&amp;gt;&quot;, names(data.conv), &quot;\n&quot;, data.conv), &#xA;    sep=&quot;\n&quot;, file=&quot;out_fastaLike.txt&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-07-22T10:53:27.133" CommentCount="0" />
  <row Id="2124" PostTypeId="2" ParentId="884" CreationDate="2017-07-24T15:56:04.047" Score="2" Body="&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Actually, after having a more thorough look into the documentation; &lt;code&gt;REMARK 3&lt;/code&gt; might be the more relevant to what you need. Other tools are reported in that section. &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;REMARK 3 presents information on refinement program(s) used and related statistics. For non-diffraction studies, REMARK 3 is used to describe any refinement done, but its format is mostly free text.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.wwpdb.org/documentation/file-format-content/format33/remark3.html&quot; rel=&quot;nofollow noreferrer&quot; title=&quot;More on `Remark 3`&quot;&gt;More on &lt;code&gt;Remark 3&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;From the same source as you reference yourself &lt;a href=&quot;http://www.wwpdb.org/documentation/file-format-content/format33/remarks1.html&quot; rel=&quot;nofollow noreferrer&quot; title=&quot;v 3.3&quot;&gt;v 3.3&lt;/a&gt;, I would argue that &#xA;&lt;code&gt;REMARK 0&lt;/code&gt; would be more accurate than &lt;code&gt;REMARK 250&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;REMARK 0 (updated), Re-refinement notice&#xA;  REMARK 0 identifies entries in which a re-refinement has been performed using the data from an existing entry. This remark also describes the PDB code and the journal records for the original data set.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;While when you consider the definition of &lt;code&gt;REMARK 250&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;-- which follows directly:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;REMARK 205&lt;/code&gt; (specific to Fiber diffraction experiment), &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;REMARK 201&lt;/code&gt;(and &lt;code&gt;REMARK 215/217&lt;/code&gt; all specific to NMR experiment),&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;REMARK 230&lt;/code&gt; (neutron diffraction study), &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;REMARK 240&lt;/code&gt;(electron crystallography study) and &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;REMARK 245&lt;/code&gt;(and &lt;code&gt;REMARK 247&lt;/code&gt;: specific to EM study)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;REMARK 250, Other Type of Experiment Details&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;REMARKs specific to other kinds of studies, not listed above. &#xA;  REMARK 250 is mandatory if other than X-ray, NMR, neutron, or electron study. The format of the date in this remark is DD-MMM-YY. DD is the day of the month (a number 01 through 31), MMM is the English 3-letter abbreviation for the month, and YY is the year.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and then &lt;code&gt;REMARK 265&lt;/code&gt; is also about the crystallography experiment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence, I know that personally, I usually assume that whatever is annotated in &lt;code&gt;REMARK 250&lt;/code&gt;is relevant to the first acquisition experiment to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another reason I am more inclined to think &lt;code&gt;REMARK 0&lt;/code&gt;is more accurate, it is due that the definition of &lt;code&gt;REMARK 0&lt;/code&gt; was &lt;strong&gt;updated&lt;/strong&gt; while &lt;code&gt;REMARK 6-99&lt;/code&gt; are &lt;em&gt;are no longer for use of free text annotation&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW, I am not sure how &lt;a href=&quot;http://pdb-extract.wwpdb.org/&quot; rel=&quot;nofollow noreferrer&quot; title=&quot;Pdb_extract&quot;&gt;Pdb_extract&lt;/a&gt; would do with the remarks when converting the PDB file to CIF files. It would probably worth perusing  &lt;a href=&quot;http://mmcif.wwpdb.org/docs/pdb_to_pdbx_correspondences.html#XPLOR&quot; rel=&quot;nofollow noreferrer&quot; title=&quot;PDB to PDBx/mmCIF Data Item Correspondences&quot;&gt;PDB to PDBx/mmCIF Data Item Correspondences&lt;/a&gt; page.&lt;/p&gt;&#xA;" OwnerUserId="203" LastEditorUserId="203" LastEditDate="2017-07-26T23:01:09.873" LastActivityDate="2017-07-26T23:01:09.873" CommentCount="0" />
  <row Id="2125" PostTypeId="2" ParentId="949" CreationDate="2017-07-24T19:52:43.587" Score="2" Body="&lt;p&gt;Hm, I didn't know that the plugin existed so I wrote my own script to convert GP to minor allele dosage on github. Maybe someone else will find it useful :) &lt;a href=&quot;https://github.com/7methylg/VCF-GP-to-DS&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/7methylg/VCF-GP-to-DS&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1183" LastActivityDate="2017-07-24T19:52:43.587" CommentCount="0" />
  <row Id="2128" PostTypeId="1" CreationDate="2017-07-25T09:34:43.760" Score="4" ViewCount="47" Body="&lt;p&gt;I would like to gather proteins FASTA sequence from Entrez with python 2.7. I am looking for any proteins that have the keywords: &quot;terminase&quot; and &quot;large&quot; in their name. So far I got this code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from Bio import Entrez&#xA;Entrez.email = &quot;example@example.org&quot;&#xA;&#xA;&#xA;searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=&quot;terminase large&quot;, retmax=1000)&#xA;searchResult = Entrez.read(searchResultHandle)&#xA;ids = searchResult[&quot;IdList&quot;]&#xA;&#xA;handle = Entrez.efetch(db=&quot;protein&quot;, id=ids, rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xA;record = handle.read()&#xA;&#xA;out_handle = open('myfasta.fasta', 'w')&#xA;out_handle.write(record.rstrip('\n'))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, it can get me several terminases from various organisms, while I need only terminase form bacteriophages (specificly Viruses [taxid 10239], host: bacteria). I've managed to get the nuccore accession ids from NCBI of the viruses I am intersted in, but I don't know how to combine those two informations. The id file looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;NC_001341&#xA;NC_001447&#xA;NC_028834&#xA;NC_023556&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do I need to access every gb file of every ID and search for my desired protein in it?&lt;/p&gt;&#xA;" OwnerUserId="1193" LastActivityDate="2017-07-25T12:28:13.403" Title="Getting protein FASTA sequence based on keyword with python" Tags="&lt;biopython&gt;&lt;python&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2130" PostTypeId="1" AcceptedAnswerId="2132" CreationDate="2017-07-25T11:11:26.397" Score="2" ViewCount="16" Body="&lt;p&gt;Can anyone explain me, why I don't find a specific protein with a blast that was took before from the NCBI refseq database?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically, I was trying to blast the protein with the accession number &quot;NP_420767&quot; and its sequence, respectively, however that protein does not show up in the results. It not only happens when the standard options are chosen, but also, when &quot;Reference proteins (refseq_protein)&quot; as database in the blast options is selected.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am really puzzled by that.. Shouldn't blast show the initial refseq entry in the results list as it is part of the refseq database and has the same sequence?&lt;/p&gt;&#xA;" OwnerUserId="1191" LastActivityDate="2017-07-25T11:27:52.257" Title="blasting a refseq protein does not show the protein in the result set" Tags="&lt;blast&gt;&lt;refseq&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2131" PostTypeId="1" CreationDate="2017-07-25T11:26:33.690" Score="3" ViewCount="30" Body="&lt;p&gt;Is there a way to filter bases in BAM files based on phred quallities through python's pysam ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a code here that&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Takes the nucleobases per position from a BAM file using pysam's pileup function&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Saves it in ReverseList and ForwardList based on both strands (i.e forward and reverse), I want to reject those bases that have phred quality below 25 to be not stored in the ForwardList and ReverseList lists so that they are not used for further analysis.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; samfile = pysam.Samfile( filename, &quot;rb&quot; )&#xA; ReverseList = [''] * lenref&#xA; ForwardList = [''] * lenref&#xA; for pileupcolumn in samfile.pileup() :&#xA;     for pileupread in pileupcolumn.pileups:&#xA;         if (pileupread.alignment.mapping_quality &amp;lt;= 15):&#xA;               continue      &#xA;         if not pileupread.is_del and not pileupread.is_refskip:&#xA;               if pileupread.alignment.is_reverse: #negative&#xA;                      ReverseList[pileupcolumn.pos] += pileupread.alignment.query_sequence[pileupread.query_position]&#xA;               else:&#xA;                      ForwardList[pileupcolumn.pos] += pileupread.alignment.query_sequence[pileupread.query_position]&#xA;&#xA; samfile.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Where lenref = 16569(length of mitochondrial genome) and filename is the name of BAM file. I want to filter based of phred qualities of bases.&lt;/p&gt;&#xA;" OwnerUserId="1194" LastActivityDate="2017-07-25T11:42:57.597" Title="Filtering bases based on phred qualities with pysam" Tags="&lt;bam&gt;&lt;python&gt;&lt;pysam&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="2132" PostTypeId="2" ParentId="2130" CreationDate="2017-07-25T11:27:52.257" Score="2" Body="&lt;p&gt;NP_420767 is represented by the non-redundant refSeq protein WP_010919826, which has the same amino acid sequence. This is not very clearly annotated, but if you scroll down to the sequence in the GenPept entry for NP_420767, you'll see the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CONTIG      join(WP_010919826.1:1..799)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/refseq/about/nonredundantproteins&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; for more info.&lt;/p&gt;&#xA;" OwnerUserId="581" LastActivityDate="2017-07-25T11:27:52.257" CommentCount="1" />
  <row Id="2133" PostTypeId="2" ParentId="2131" CreationDate="2017-07-25T11:42:57.597" Score="3" Body="&lt;p&gt;The PileupRead object as a query_position attribute, which you can use for this:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;for pileupcolumn in samfile.pileup() :&#xA;    for pileupread in pileupcolumn.pileups:&#xA;         if (pileupread.alignment.mapping_quality &amp;lt;= 15):&#xA;             continue      &#xA;         if not pileupread.is_del and not pileupread.is_refskip:&#xA;             if pileupread.alignment.query_qualities[pileupread.query_position] &amp;lt; 10:&#xA;                 # Skip entries with base phred scores &amp;lt; 10&#xA;                 continue&#xA;             if pileupread.alignment.is_reverse: #negative&#xA;                   ReverseList[pileupcolumn.pos] += pileupread.alignment.query_sequence[pileupread.query_position]&#xA;             else:&#xA;                   ForwardList[pileupcolumn.pos] += pileupread.alignment.query_sequence[pileupread.query_position]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note the 6th line, which implements a filter with a threshold of 10.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-25T11:42:57.597" CommentCount="0" />
  <row Id="2134" PostTypeId="2" ParentId="2113" CreationDate="2017-07-25T11:43:31.083" Score="4" Body="&lt;p&gt;I don't know of any prebuilt products, but I can describe how we managed this in my postdoc lab, and how I plan to manage it in my newly-started group. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rule 1: All work happens in the projects project directory on the central &#xA;filestore, not on your desktop or laptop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rule 2: Heavy computational work is done by the groups standard analysis pipelines. Interpretation is done in jupyter notebooks or Rmarkdown.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A project has a directory on the group's filestore. That directory has a fixed structure:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;proj001----raw_data             *&#xA;       |&#xA;        --external_datasets&#xA;       |&#xA;        --src                  *&#xA;       |&#xA;        --notebooks            *&#xA;       |&#xA;        --pipeline1&#xA;       |&#xA;        --pipeline2&#xA;       | &#xA;        -- etc&#xA; * link to a seperate, backed up filesystem.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Pipelines are where heavy duty analysis happens and everyone uses the same standard pipelines, producing the same standard output files and database structure. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So common pipelines might be the mapping pipeline, the readqc pipeline, the differential expression pipeline, the exome pipeline, the chip-seq pipeline etc...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Pipelines have three important outputs: an automatically generated html report, an sqlite database and files in the export directory. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;When we archive a project, this is what we save, along with the pipeline's configuration file and log file. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So if I know that Jane Bloggs did an RNAseq 5 years ago on a cell type i'm interested in, if I konw that that was project 5, I know that in the project 5 directory there will be a &lt;code&gt;diff_expression_pipeline&lt;/code&gt; directory, and that it will contain an sqlite database called &lt;code&gt;csvdb&lt;/code&gt; and that that will have a table called &lt;code&gt;refcoding_deseq_gene_diff&lt;/code&gt; and that table will follow a known format. There will be bigwigs in the &lt;code&gt;export&lt;/code&gt; directory. Or the BAMs will be in the &lt;code&gt;mapping_pipeline&lt;/code&gt; directory. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course the problem remains knowing that Jane Bloggs did this RNAseq and what the project was called. We use a Wiki for this, but its not ideal. &lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-07-25T11:43:31.083" CommentCount="0" />
  <row Id="2135" PostTypeId="2" ParentId="2128" CreationDate="2017-07-25T12:28:13.403" Score="3" Body="&lt;p&gt;Found what I was looking for. In:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=&quot;terminase large&quot;, retmax=1000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've added:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;searchterm = &quot;(terminase large subunit AND viruses[Organism]) AND Caudovirales AND refseq[Filter]&quot;&#xA;searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=searchterm, retmax=6000)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which norrowed down my searches to the desired viruses. Granted it's not filtered by host, but by a taxonomy group, but it is enough for my work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you @Llopis for additional help&lt;/p&gt;&#xA;" OwnerUserId="1193" LastActivityDate="2017-07-25T12:28:13.403" CommentCount="1" />
  <row Id="2136" PostTypeId="2" ParentId="658" CreationDate="2017-07-25T13:07:46.147" Score="5" Body="&lt;p&gt;I'm not sure what kinds of bioinformatics tasks you would like to perform, therefore it is difficult to give a good recommendation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're specifically working on statistical genetics, I can recommend &lt;a href=&quot;https://hail.is&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hail&lt;/a&gt; [1]. Hail is an open-source tool for analyzing genetics data at the tens of terabyte scale. Most of Hail's users do their science in Jupyter notebooks that are backed by Google Cloud Platform Dataproc clusters. Hail permits you to perform a variety of statistical genetics tasks including:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;filtering and aggregation for quality control&lt;/li&gt;&#xA;&lt;li&gt;subsetting, linear regression, linear mixed model regression, and linear burden testing&lt;/li&gt;&#xA;&lt;li&gt;utilities for computing various measures of relatedness&lt;/li&gt;&#xA;&lt;li&gt;principal components analysis&lt;/li&gt;&#xA;&lt;li&gt;variant splitting&lt;/li&gt;&#xA;&lt;li&gt;import/export from a variety of formats including PLINK, VCF, and BGEN, and&lt;/li&gt;&#xA;&lt;li&gt;a python API which enables the use of libraries like matplotlib for plotting analysis results&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;To learn specifically about using Hail with the Google Cloud Platform and Jupyter notebooks, I strongly recommend &lt;a href=&quot;http://discuss.hail.is/t/using-hail-with-jupyter-notebooks-on-google-cloud/196/2&quot; rel=&quot;nofollow noreferrer&quot;&gt;Liam's Hail forum post about his cloud-tools repository&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's an example, from the &lt;a href=&quot;https://hail.is/hail/tutorials/hail-overview.html#Quality-Control&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hail tutorial&lt;/a&gt;, of using Hail to perform some quality control and display a scatter plot of the first two principal components of the individuals:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;from hail import *&#xA;import matplotlib.pyplot as plt&#xA;import matplotlib.patches as mpatches&#xA;&#xA;hc = HailContext()&#xA;&#xA;table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample')&#xA;common_vds = (hc.read('data/1kg.vds')&#xA;              .annotate_samples_table(table, root='sa')&#xA;              .sample_qc()&#xA;              .filter_samples_expr('sa.qc.dpMean &amp;gt;= 4 &amp;amp;&amp;amp; sa.qc.callRate &amp;gt;= 0.97')&#xA;              .filter_genotypes('''let ab = g.ad[1] / g.ad.sum() in&#xA;                         ((g.isHomRef &amp;amp;&amp;amp; ab &amp;lt;= 0.1) ||&#xA;                          (g.isHet &amp;amp;&amp;amp; ab &amp;gt;= 0.25 &amp;amp;&amp;amp; ab &amp;lt;= 0.75) ||&#xA;                          (g.isHomVar &amp;amp;&amp;amp; ab &amp;gt;= 0.9))''')&#xA;              .variant_qc()&#xA;              .filter_variants_expr('va.qc.AF &amp;gt; 0.01')&#xA;              .ld_prune(memory_per_core=512, num_cores=4))&#xA;&#xA;pca = common_vds.pca('sa.pca', k=5, eigenvalues='global.eigen')&#xA;pca_table = pca.samples_table().to_pandas()&#xA;&#xA;colors = {'AFR': 'green', 'AMR': 'red', 'EAS': 'black', 'EUR': 'blue', 'SAS': 'cyan'}&#xA;plt.scatter(pca_table[&quot;sa.pca.PC1&quot;], pca_table[&quot;sa.pca.PC2&quot;],&#xA;            c = pca_table[&quot;sa.SuperPopulation&quot;].map(colors),&#xA;            alpha = .5)&#xA;plt.xlim(-0.6, 0.6)&#xA;plt.xlabel(&quot;PC1&quot;)&#xA;plt.ylabel(&quot;PC2&quot;)&#xA;legend_entries = [mpatches.Patch(color=c, label=pheno) for pheno, c in colors.items()]&#xA;plt.legend(handles=legend_entries, loc=2)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;[1] Disclaimer: I work on Hail&lt;/p&gt;&#xA;" OwnerUserId="1087" LastEditorUserId="1087" LastEditDate="2017-08-15T17:50:38.403" LastActivityDate="2017-08-15T17:50:38.403" CommentCount="0" />
  <row Id="2137" PostTypeId="1" AcceptedAnswerId="2138" CreationDate="2017-07-25T15:56:54.430" Score="3" ViewCount="87" Body="&lt;p&gt;I have a gtf file from Ensembl, and I noticed that several &quot;transcript&quot; annotations have the exact same coordinates. See for instance the third and fourth transcripts (&quot;Y74C9A.2b.1&quot; and &quot;Y74C9A.2b.4&quot;) for this gene:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ grep &quot;WBGene00022276&quot; genes.gtf | awk '$3 == &quot;transcript&quot; {print}'&#xA;I   ensembl transcript  10413   16842   .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P24752&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2a.2&quot;; transcript_name &quot;Y74C9A.2a.2&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS7921&quot;;&#xA;I   ensembl transcript  11495   16793   .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P22572&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2b.2&quot;; transcript_name &quot;Y74C9A.2b.2&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS9220&quot;;&#xA;I   ensembl transcript  11499   16842   .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P22572&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2b.1&quot;; transcript_name &quot;Y74C9A.2b.1&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS9215&quot;;&#xA;I   ensembl transcript  11499   16842   .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P22572&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2b.4&quot;; transcript_name &quot;Y74C9A.2b.4&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS9215&quot;;&#xA;I   ensembl transcript  11505   16842   .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P24752&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2a.3&quot;; transcript_name &quot;Y74C9A.2a.3&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS16820&quot;;&#xA;I   ensembl transcript  11618   16842   .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P24752&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2a.1&quot;; transcript_name &quot;Y74C9A.2a.1&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS43845&quot;;&#xA;I   ensembl transcript  11623   16842   .   +   .   gene_biotype &quot;protein_coding&quot;; gene_id &quot;WBGene00022276&quot;; gene_name &quot;nlp-40&quot;; gene_source &quot;ensembl&quot;; gene_version &quot;1&quot;; p_id &quot;P22572&quot;; transcript_biotype &quot;protein_coding&quot;; transcript_id &quot;Y74C9A.2b.3&quot;; transcript_name &quot;Y74C9A.2b.3&quot;; transcript_source &quot;ensembl&quot;; transcript_version &quot;1&quot;; tss_id &quot;TSS49800&quot;;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What does it mean? Do these two &quot;transcripts&quot; actually correspond to an exon, belonging to two different isoforms?&lt;/p&gt;&#xA;" OwnerUserId="292" LastActivityDate="2017-07-25T17:14:56.770" Title="Same transcript coordinates in gtf file, different transcript ID" Tags="&lt;annotation&gt;&lt;gtf&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2138" PostTypeId="2" ParentId="2137" CreationDate="2017-07-25T16:48:03.920" Score="6" Body="&lt;p&gt;&lt;code&gt;transcript&lt;/code&gt; objects cover the co-ordinates from the start of the first exon to the end of the last exon of a transcript (i.e. an isoform). If two different isoforms share the same first and last exons, but have a different set of internal exons, then their &lt;code&gt;transcript&lt;/code&gt; entries will be the same, but the set of &lt;code&gt;exon&lt;/code&gt; entires associated with each transcript will be different.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, consider the two transcripts below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                      1         2&#xA;             12345678901234567890123 &#xA;transcript_1 |&amp;gt;&amp;gt;&amp;gt;|----|&amp;gt;&amp;gt;|----|&amp;gt;&amp;gt;&amp;gt;&amp;gt;|&#xA;&#xA;transcript_2 |&amp;gt;&amp;gt;|--|&amp;gt;&amp;gt;|------|&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;These two transcripts would give the following GTF entires&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;chr1   ensembl   transcript    1   23   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_1&quot;;&#xA;chr1   ensembl   exon    1   5   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_1&quot;;&#xA;chr1   ensembl   exon    10   13   .   +   .   gene_id &quot;gene_1&quot;; gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_1&quot;;   &#xA;chr1   ensembl   exon    18   23   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_1&quot;;   &#xA;chr1   ensembl   transcript    1   23   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_2&quot;;&#xA;chr1   ensembl   exon    1   4   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_2&quot;;&#xA;chr1   ensembl   exon    7   10   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_2&quot;;   &#xA;chr1   ensembl   exon    17   23   .   +   .   gene_id &quot;gene_1&quot;; transcript_id &quot;transcript_2&quot;;   &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-07-25T16:48:03.920" CommentCount="0" />
  <row Id="2139" PostTypeId="2" ParentId="2137" CreationDate="2017-07-25T17:07:37.253" Score="7" Body="&lt;p&gt;As Ian &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/2138/298&quot;&gt;explained&lt;/a&gt;, these are different transcripts which happen to have the same start and end positions. You have no information on their exonic structure in that file. However, if you look them up at EnsEMBL, you will see:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://May2017.archive.ensembl.org/Caenorhabditis_elegans/Transcript/Summary?db=core;g=WBGene00022276;r=I:11499-16842;t=Y74C9A.2b.1&quot; rel=&quot;noreferrer&quot;&gt;Transcript Y74C9A.2b.1&lt;/a&gt; :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/yYXlV.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/yYXlV.png&quot; alt=&quot;exonic structure of Y74C9A.2b.1&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1   Y74C9A.2b.1.e1  11,499  11,561  -   -   63&#xA;    Intron 1-2      11,562  11,617          56&#xA;2   Y74C9A.2b.1.e2  11,618  11,689  -   -   72&#xA;    Intron 2-3      11,690  14,950          3,261&#xA;3   Y74C9A.2b.1.e3  14,951  15,160  -   1   210&#xA;    Intron 3-4      15,161  16,472          1,312&#xA;4   Y74C9A.2a.1.e3  16,473  16,842  1   -   370&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This has 4 exons, two of which are non-coding (UTR). The second starts at &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://May2017.archive.ensembl.org/Caenorhabditis_elegans/Transcript/Summary?db=core;g=WBGene00022276;r=I:11499-16842;t=Y74C9A.2b.4&quot; rel=&quot;noreferrer&quot;&gt;Transcript Y74C9A.2b.4&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/uZbYI.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/uZbYI.png&quot; alt=&quot;exonic structure of Y74C9A.2b.4&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1   Y74C9A.2b.4.e1  11,499  11,557  -   -   59&#xA;    Intron 1-2      11,558  11,617          60&#xA;2   Y74C9A.2b.1.e2  11,618  11,689  -   -   72&#xA;    Intron 2-3      11,690  14,950          3,261&#xA;3   Y74C9A.2b.1.e3  14,951  15,160  -   1   210&#xA;    Intron 3-4      15,161  16,472          1,312&#xA;4   Y74C9A.2a.1.e3  16,473  16,842  1   -   370&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Granted, the two transcripts are very simlar, they both have 4 exons, the first 2 of which are non coding (UTR). However, the 1st exon of transcript &#xA;Y74C9A.2b.1 is 63 nt long and ends at 11,561 while the 1st exon of transcript Y74C9A.2b.4 is 59 nts and ends at 11,557.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So there you go: two (slightly) different transcripts. &lt;/p&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-07-25T17:14:56.770" LastActivityDate="2017-07-25T17:14:56.770" CommentCount="0" />
  <row Id="2140" PostTypeId="1" AcceptedAnswerId="2148" CreationDate="2017-07-25T17:23:52.373" Score="3" ViewCount="107" Body="&lt;p&gt;We have an experimental design as seen below &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where we administered drug at 0 min for each mouse genotype and took them down at given below intervals. wt and ko mouse models  were administered only with the vehicle which would be our negative control.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Performed rnaseq (total rna) truseq stranded protocol would like to see differences between each time point compared to the vehicle, used star aligner to align to mm9 , htseq for gene counts . &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sample  type    time&#xA;WT  Vehicle     30 min&#xA;KO  Vehicle     30 min&#xA;WT  Drug    30min&#xA;KO  Drug    30min&#xA;WT  Drug    1 hour&#xA;KO  Drug    1 hour&#xA;WT  Drug    2 hour&#xA;KO  Drug    2hour&#xA;WT  Drug    3 hour&#xA;KO  Drug    3hour&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We do not have any replicates and hence cannot use any of the differential expression statistical tools (asked this question at bioconductor where Dr. Love himself replied &lt;a href=&quot;https://support.bioconductor.org/p/98429/#98456&quot; rel=&quot;nofollow noreferrer&quot;&gt;link to question asked&lt;/a&gt;) where he suggested a linear model. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;we were wondering if someone could help us with any such design (papers/previously performed analysis) what we would like to see is comparison/differences (significant) between these groups &#xA;(veh vs 30min, veh vs 1 hour, beh vs 2 hours, veh vs 3 hours) &lt;/p&gt;&#xA;" OwnerUserId="926" LastEditorUserId="926" LastEditDate="2017-07-25T19:41:07.320" LastActivityDate="2017-07-26T14:28:53.783" Title="differential gene expression complex design no replicates" Tags="&lt;rna-seq&gt;&lt;deseq2&gt;&lt;differential-expression&gt;" AnswerCount="2" CommentCount="5" FavoriteCount="0" />
  <row Id="2141" PostTypeId="2" ParentId="2113" CreationDate="2017-07-25T22:01:40.587" Score="2" Body="&lt;p&gt;We have been organizing data on a &quot;per project&quot; basis, using a GitHub repository for each project. Each project ends up being a paper (written in R Markdown), so you could have multiple &quot;projects&quot; based on the same datasets. See this as an example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/SchlossLab/new_project&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/SchlossLab/new_project&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The analyses are all managed with a Makefile that goes from loading the sequence data from the SRA, to performing analyses, to rendering the final manuscript. This way it is always clear how things were done. Here is an example of what that can look like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/SchlossLab/new_project/blob/master/Makefile&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/SchlossLab/new_project/blob/master/Makefile&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as organizing the raw data and whatnot, I have just used a simple MySQL database hosted on MAMP (Mac version, Linux is LAMP, Windows is WAMP). It has a GUI and everything, and is pretty easy to use. Here is a resource for getting started on that:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://microbiology.github.io/PDFs/MySQL-Intro-Workshop-Hannigan.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://microbiology.github.io/PDFs/MySQL-Intro-Workshop-Hannigan.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All of this stuff is free to use, which is awesome! Hope this helps! :)&lt;/p&gt;&#xA;" OwnerUserId="824" LastActivityDate="2017-07-25T22:01:40.587" CommentCount="0" />
  <row Id="2142" PostTypeId="1" AcceptedAnswerId="2143" CreationDate="2017-07-26T01:44:09.730" Score="11" ViewCount="79" Body="&lt;p&gt;I am doing a research project involving calculating k-mer frequencies and I am wondering if there is any standard file format for storing k-mer counts.&lt;/p&gt;&#xA;" OwnerUserId="1196" LastActivityDate="2017-07-28T01:33:46.580" Title="Is there a standard k-mer count file format?" Tags="&lt;file-formats&gt;&lt;k-mer&gt;&lt;dna&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2143" PostTypeId="2" ParentId="2142" CreationDate="2017-07-26T05:13:34.800" Score="5" Body="&lt;p&gt;Not as far as I am aware. The Ray assembler used to (and possibly still does) store the kmers as FASTA files where the header was the count of the sequence, which I thought was a pretty neat bastardisation of the FASTA file format. It looks like this format is also used by Jellyfish when reporting kmer frequencies by the &lt;code&gt;dump&lt;/code&gt; command (but its default output format is a custom binary format):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The dump subcommand outputs a list of all the k-mers in the file associated with their count. By default, the output is in FASTA format, where the header line contains the count of the k-mer and the sequence part is the sequence of the k-mer. This format has the advantage that the output contains the sequence of k-mers and can be directly fed into another program expecting the very common FASTA format. A more convenient column format (for human beings) is selected with the -c  switch.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.genome.umd.edu/jellyfish.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Jellyfish&lt;/a&gt; changed their internal format between v1 and v2 (both not FASTA), because they changed to doing counts based on bloom filters. Jellyfish2 has an optional two-pass method that sets up a bloom filter intermediate file to record kmers, and multiple different final reporting formats.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://khmer.readthedocs.io/en/v2.1.2/introduction.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Khmer&lt;/a&gt; also uses bloom filters, but in a slightly different way. It also has been extended to be useful for partitioning and comparing datasets.&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-07-28T01:33:46.580" LastActivityDate="2017-07-28T01:33:46.580" CommentCount="0" />
  <row Id="2144" PostTypeId="2" ParentId="2140" CreationDate="2017-07-26T07:38:16.753" Score="5" Body="&lt;p&gt;Although it is not recommended to use no replicates, in the edgeR manual they give some advice on how to go on with no replicates design. See page 21 of user &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf&quot; rel=&quot;noreferrer&quot;&gt;guide&lt;/a&gt;. You can e.g. estimate a BCV value. Of course it is still a trick and not sound statistics.&lt;/p&gt;&#xA;" OwnerUserId="939" LastActivityDate="2017-07-26T07:38:16.753" CommentCount="1" />
  <row Id="2145" PostTypeId="1" AcceptedAnswerId="2147" CreationDate="2017-07-26T09:40:18.733" Score="4" ViewCount="269" Body="&lt;p&gt;Low coverage MinION reads should be useful to close gaps and resolve repeats left by short-read assemblers. However, I haven't had any success with the software I know about. I'm aware of the following packages, either for scaffolding or closing gaps in short-read assemblies using long reads:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/warrenlr/LINKS/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;LINKS&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://sourceforge.net/projects/operasf&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;OPERA-LG&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/mdcao/npScarf&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;npScarf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://sourceforge.net/projects/pb-jelly&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;PBJelly&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.baseclear.com/genomics/bioinformatics/basetools/SSPACE-longread&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;SSPACE-longread&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I've tried &lt;code&gt;npScarf&lt;/code&gt; and &lt;code&gt;LINKS&lt;/code&gt;, and they ran successfully but didn't resolve the gaps in my assembly. I couldn't get &lt;code&gt;PBJelly&lt;/code&gt; and &lt;code&gt;OPERA-LG&lt;/code&gt; to run with the current versions of &lt;code&gt;BLASR&lt;/code&gt; and &lt;code&gt;samtools&lt;/code&gt;, and both packages seem not to be maintained. I have not tried &lt;code&gt;SSPACE-longread&lt;/code&gt; because it's not open source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What software can I use to fill gaps and resolve repeats in a short-read assembly with low-coverage Nanopore data?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h3&gt;More information&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I'm finishing a mitochondrial genome. I have a 17 kb short read assembly with one gap. This was made from 2x150b paired-end reads from a TruSeq PCR-free library with a 400 b insert size. Molecular data suggests the genome may be around 32 kb.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After mapping short reads against the short read assembly, &lt;code&gt;Pilon&lt;/code&gt; identified a tandem repeat of 156 bp in an AT-rich region, but wasn't able to close the gap.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have 1.4 Gb of MinION rapid reads with an L50 of 8808 b. I mapped these reads against the short read assembly, and I can see reads that span the gap. I seem to have the information required to close the gap, but I don't know how to do it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The organism has a nuclear genome &gt; 600 Mb, so long read coverage is low. Despite this, I tried a &lt;em&gt;de novo&lt;/em&gt; &lt;code&gt;Canu&lt;/code&gt; assembly of the Nanopore reads, and I pulled out a 39 kb contig containing mitochondrial genes. Most genes on this contig are duplicated and fragmented, and &lt;code&gt;Pilon&lt;/code&gt; wasn't able to improve it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for reading!&lt;/p&gt;&#xA;" OwnerUserId="1021" LastEditorUserId="1021" LastEditDate="2017-07-26T10:34:48.093" LastActivityDate="2017-07-26T10:34:48.093" Title="How can I use Nanopore reads to close gaps or resolve repeats in a short-read assembly?" Tags="&lt;nanopore&gt;&lt;long-reads&gt;&lt;repeat-elements&gt;&lt;scaffold&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="2147" PostTypeId="2" ParentId="2145" CreationDate="2017-07-26T10:25:29.833" Score="4" Body="&lt;p&gt;You might want to look into &lt;a href=&quot;https://github.com/rrwick/Unicycler&quot; rel=&quot;nofollow noreferrer&quot;&gt;Unicycler&lt;/a&gt; (manuscript with more information &lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005595&quot; rel=&quot;nofollow noreferrer&quot;&gt;can be found here&lt;/a&gt;); even though it is supposed to be used with bacterial genomes only, it might work well with a small genome such as a mitochondrion's.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/wq02F.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/wq02F.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Beware that if you happen to have very long reads, you might end up with an assembly with multiple copies of the circular genome: you might want to look into &lt;a href=&quot;https://github.com/sanger-pathogens/circlator&quot; rel=&quot;nofollow noreferrer&quot;&gt;circlator&lt;/a&gt; then.&lt;/p&gt;&#xA;" OwnerUserId="123" LastEditorUserId="298" LastEditDate="2017-07-26T10:30:40.230" LastActivityDate="2017-07-26T10:30:40.230" CommentCount="3" />
  <row Id="2148" PostTypeId="2" ParentId="2140" CreationDate="2017-07-26T11:42:11.777" Score="6" Body="&lt;p&gt;Mike Love is right. &lt;strong&gt;If&lt;/strong&gt; the response you are looking for is linear in terms of change per minute, the most productive approach is likely to be fitting a linear model. You &lt;strong&gt;might&lt;/strong&gt; get something out of this because the difference between successive time points represents multiple measurements of the rate of change over time. The biggest problem is that the time points aren't independent. You would need to fit a model that estimated that rate for both WT and KO and then test the difference in rates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see three ways forward, in all cases you want to test the last term in the model &lt;code&gt;~time + genotype + time:genotype&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Ideally you would like to fit a negative binomial linear model to the read counts. But that is going to be difficult as you have no replicates from which to assess the dispersion. Thus you could generate read counts using your favorite read count pipeline (STAR+featureCounts/Salmon/Kallisto) and take the approach in the edgeR guide noted by &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/2144/235&quot; title=&quot;b.nota&quot;&gt;b.nota&lt;/a&gt; and then use edgeR to test the fitted model.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;You could use something like salmon or kallisto to estimate the TPMs assume that log TPMs are normally distributed and model them with plain &lt;code&gt;lm&lt;/code&gt; in R, using something like &lt;code&gt;broom&lt;/code&gt; to model each gene separately. A tutorial by David Robinson, the author of &lt;code&gt;broom&lt;/code&gt; on doing this sort of thing can be found &lt;a href=&quot;http://varianceexplained.org/r/tidy-genomics-broom/&quot; rel=&quot;nofollow noreferrer&quot; title=&quot;here&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;You could use the Salmon or Kallisto bootstraps to estimate &lt;em&gt;technical&lt;/em&gt; variation (but not biological) and then I suspect you could use Sleuth to fit the model.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In any case these results should be treated with scepticism andmostly treated as hypothesis generating. You might find genes that you want to go back and confirm and you might see some patterns of interest if you look at some enrichments etc. which you can then design experiments to test. &lt;/p&gt;&#xA;" OwnerUserId="235" LastEditorUserId="235" LastEditDate="2017-07-26T14:28:53.783" LastActivityDate="2017-07-26T14:28:53.783" CommentCount="3" />
  <row Id="2149" PostTypeId="1" CreationDate="2017-07-26T11:54:47.203" Score="3" ViewCount="31" Body="&lt;h3&gt;Question&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;How can I extract reads from a &lt;code&gt;bam&lt;/code&gt; file (produced by &lt;code&gt;bwa-mem&lt;/code&gt;) to &lt;code&gt;fastq&lt;/code&gt; given a list of reference sequences to filter out?&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Potential difficulties&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;maintaining FR orientation of pair end reads (in &lt;code&gt;bam&lt;/code&gt; all the sequences are reference sequences)&lt;/li&gt;&#xA;&lt;li&gt;keeping R1 and R2 reads &lt;/li&gt;&#xA;&lt;li&gt;keeping quality scores in the same encoding as original fastq (default illumina phred scores in my case)&lt;/li&gt;&#xA;&lt;li&gt;bam can be (ana usually is) sorted by coordinates&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Almost there solutions&lt;/h3&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/blaxterlab/blobology/blob/master/bowtie2_extract_reads_mapped_to_specific_contigs.pl&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sujai's perl solution&lt;/a&gt; in &lt;a href=&quot;https://github.com/blaxterlab/blobology&quot; rel=&quot;nofollow noreferrer&quot;&gt;blobology&lt;/a&gt; does exact opposite - getting reads from list of references (so I could just reverse the list). The disadvantage is that the script outputs an interleaved &lt;code&gt;fq&lt;/code&gt; file; requires unique names of mates, otherwise R1/R2 information is lost.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;samtools + grep them all from fastq files&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;create a list of read names that do not map to filtered scaffolds. (cut will extract just read names, uniq will collapse pair end read names if they are the same). Then grep read names from fastq files and remove -- separator between hits&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view foo.bam | grep -vf list_of_scaffols_filter \&#xA;  | cut -f 1 | uniq &amp;gt; list_of_reads_to_keep&#xA;&#xA;grep -A 3 -f list_of_reads_to_keep foo_R1.fq | grep -v &quot;^--$&quot; &amp;gt; foo_R1_filtered_bash.fq&#xA;grep -A 3 -f list_of_reads_to_keep foo_R2.fq | grep -v &quot;^--$&quot; &amp;gt; foo_R1_filtered_bash.fq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ol start=&quot;3&quot;&gt;&#xA;&lt;li&gt;filter bam &amp;amp; picard tools&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Or I could do just the filtering part and use &lt;a href=&quot;http://broadinstitute.github.io/picard/command-line-overview.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Picard-tools&lt;/a&gt; (Picard.SamToFastq), but as usual I am avoiding java as much as I can. I guess&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;samtools view foo.bam | grep -vf list_of_scaffols_filter \&#xA;  | java -jar picard.jar SamToFastq INPUT=/dev/stdin \&#xA;  FASTQ=foo_R1_filtered_bash.fq SECOND_END_FASTQ=foo_R2_filtered_bash.fq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;The first solution does not really work for me since I do not want to rename all the reads in bam file and I want to keep the R1/R2 information (since R1 and R2 have different error profiles). Both solutions 2 and 3 I find bit clumsy and I am not sure if they are general, I might get some unexpected behaviour if one of reads is mapped second is not... They both relay on the same filtering step.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering about some pysam solution. I guess it will be much slower, but at least it will be much clearer and perhaps more general. Something like in &lt;a href=&quot;https://www.biostars.org/p/6970/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Convert Bam File To Fasta File&lt;/a&gt; - there is pysam solution for fasta (not fastq), almost there...&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Story&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I have very fragmented reference genome. Some of scaffolds them are too small to works with, and some of them are contaminants (identified using blobtools).&#xA;I want to separate reads that are mapping to different groups to separate contaminants, short scaffolds and scaffold that will be used for downstream analysis. The reason is that if we remap all the reads to filtered reference (0.7 - 0.8 of original genome), the most of them (0.95 - 0.99) will still find a place where they map, therefore there is 0.2 - 0.3 of misplaced reads that will obviously have to affect downstream analysis, like variant calling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This filtering idea is based logic that if the filtered duplicated genomic region will contain some small differences, they will attract their reads (and if I filter them I will improve variant calling) and if they will be exactly same, they will get reads assigned at random, so there is no harm in doing that.&lt;/p&gt;&#xA;" OwnerUserId="57" LastActivityDate="2017-07-26T13:17:23.220" Title="How to safely and efficiently convert subset of bam to fastq?" Tags="&lt;bam&gt;&lt;fastq&gt;&lt;format-conversion&gt;&lt;filtering&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="2150" PostTypeId="2" ParentId="2149" CreationDate="2017-07-26T13:13:32.230" Score="1" Body="&lt;p&gt;I'm not aware of any pre-made program to do this, so I &lt;a href=&quot;https://github.com/dpryan79/Answers/tree/master/bioinfoSE_2149&quot; rel=&quot;nofollow noreferrer&quot;&gt;wrote one for you&lt;/a&gt;. This will take a BAM file with any ordering and produce properly ordered gzipped fastq files with the filtering as you requested. Internally, this iterates over all of the entries in the BAM file (ignoring secondary/supplemental entries and those where both mates map to your filter list), store the properly oriented sequence/quality/read name in a buffer, and then dumps that buffer entry to disk once the mate is found. This should be reasonably performant (hey, it's python, to don't expect too much), though if you happen to have indexed BAM files then one could think of ways to make this run faster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do check the output, since I've only run one test.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-26T13:13:32.230" CommentCount="2" />
  <row Id="2151" PostTypeId="2" ParentId="2149" CreationDate="2017-07-26T13:17:23.220" Score="0" Body="&lt;p&gt;Ok, I wrote a bit brute pysam / BioPython parser that uses index of bam to get proper order of read pair for R1 / R2 files and bitwise flag. It should not be too difficult to add more sophisticated filtering rules now.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python3&#xA;# 1. arg - indexed bam file&#xA;# 2. arg - list of headers to filter&#xA;# 3. arg - name pattern for the output reads&#xA;&#xA;import os&#xA;import sys&#xA;import pysam&#xA;from Bio import SeqIO, Seq, SeqRecord&#xA;&#xA;samfile = pysam.AlignmentFile(sys.argv[1], &quot;rb&quot;)&#xA;header_set = set(line.strip() for line in open(sys.argv[2]))&#xA;base = sys.argv[3]&#xA;&#xA;out_R1 = base + 'R1_filtered.fq'&#xA;out_R2 = base + 'R2_filtered.fq'&#xA;&#xA;with open(out_R1, mode='w') as R1, open(out_R2, mode='w') as R2:&#xA;    for entry in samfile:&#xA;        if entry.is_read1 and not entry.reference_name in header_set:&#xA;            # pait pair&#xA;            entry_R2 = samfile.mate(entry)&#xA;            # do some sequence gymnastics with R1&#xA;            seq_R1 = Seq.Seq(entry.seq)&#xA;            if entry.is_reverse :&#xA;                seq_R1 = seq_R1.reverse_complement()&#xA;            # do some sequence gymnastics with R2&#xA;            seq_R2 = Seq.Seq(entry_R2.seq)&#xA;            if entry_R2.is_reverse :&#xA;                seq_R2 = seq_R2.reverse_complement()&#xA;            R1.write('@' + entry.qname + '\n' + str(seq_R1) + '\n+\n' + entry.qqual + '\n')&#xA;            R2.write('@' + entry_R2.qname + '\n' + str(seq_R2) + '\n+\n' + entry_R2.qqual + '\n')&#xA;&#xA;&#xA;samfile.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are some ugly bits, please feel free to make it nicer.&lt;/p&gt;&#xA;" OwnerUserId="57" LastActivityDate="2017-07-26T13:17:23.220" CommentCount="0" />
  <row Id="2152" PostTypeId="1" AcceptedAnswerId="2162" CreationDate="2017-07-26T13:35:42.953" Score="7" ViewCount="76" Body="&lt;p&gt;Let $C$ be base coverage, $R$ is the length of reads and $K$ is the length of $k$-mer. Then $k$-mer coverage $C_k$ can be computed as $C_k = C\cdot(R - K + 1)/R$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could someone please explain why is this equation valid (I'm mostly confused as why it is divided by $R$)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Source: &lt;a href=&quot;http://www.ebi.ac.uk/~zerbino/velvet/Manual.pdf&quot; rel=&quot;noreferrer&quot;&gt;Velvet manual&lt;/a&gt; Section 5.1&lt;/p&gt;&#xA;" OwnerUserId="1199" LastEditorUserId="29" LastEditDate="2017-07-26T16:32:17.023" LastActivityDate="2017-07-27T10:54:57.303" Title="Formula for k-mer coverage" Tags="&lt;k-mer&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2153" PostTypeId="1" AcceptedAnswerId="2154" CreationDate="2017-07-26T15:20:57.187" Score="3" ViewCount="30" Body="&lt;p&gt;I am interested in finding a database that takes a gene or protein name as input (possibly with the option to specify transcript) and gives information about the protein's functional domains in terms of either specific residue ranges, genomic coordinates, etc. Ideally, the database would include 3D conformational information as well as the functional domains. Does such a database exist? It seems that this information can be inferred for a given protein using various publications and references but I have yet to find a single collection containing these data all in one place.&lt;/p&gt;&#xA;" OwnerUserId="1201" LastEditorUserId="203" LastEditDate="2017-07-27T09:11:20.577" LastActivityDate="2017-07-27T09:11:20.577" Title="Do any publicly available databases detail protein structure and functional domains?" Tags="&lt;database&gt;&lt;gene&gt;&lt;proteins&gt;&lt;protein-structure&gt;&lt;rna-structure&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="2154" PostTypeId="2" ParentId="2153" CreationDate="2017-07-26T16:24:03.780" Score="4" Body="&lt;p&gt;Some of this information (at least some domains, active sites, etc) is available from &lt;a href=&quot;http://www.uniprot.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;UniProt&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to download their whole database, you can search without specifying any terms and then click the Download button.&lt;/p&gt;&#xA;" OwnerUserId="306" LastEditorUserId="29" LastEditDate="2017-07-26T17:12:17.537" LastActivityDate="2017-07-26T17:12:17.537" CommentCount="1" />
  <row Id="2156" PostTypeId="1" AcceptedAnswerId="2158" CreationDate="2017-07-26T17:21:23.227" Score="4" ViewCount="108" Body="&lt;p&gt;I have been analyzing some virus DNA from the NCBI databases. The fasta sequences that I recieve from them have header lines that look like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;gi|61393989|gb|AY848686.1|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I think that the second number is the GenBank ID, but I have several questions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) What does the &quot;gi&quot; in the first ID stand for?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Is this a standard format for storing a series of IDs associated with sequences? That is, should I expect to see this format elsewhere and be prepared to parse fasta headers like this in code that I write?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) It seems that &lt;code&gt;AY848686&lt;/code&gt; is also a valid GenBank ID, and I see the &lt;code&gt;.1&lt;/code&gt; missing/present in GenBank IDs in various places. What does the &lt;code&gt;.1&lt;/code&gt; mean at the end of GenBank IDs and why is it there/does it &lt;em&gt;need&lt;/em&gt; to be there?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance for you answers.&lt;/p&gt;&#xA;" OwnerUserId="1196" LastActivityDate="2017-07-26T17:33:05.717" Title="Fasta Sequence Identifier format?" Tags="&lt;fasta&gt;&lt;identifiers&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="2157" PostTypeId="2" ParentId="2153" CreationDate="2017-07-26T17:23:16.920" Score="1" Body="&lt;p&gt;&lt;a href=&quot;http://www.ensembl.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;EnsEMBL&lt;/a&gt; also has this. Search for your gene of interest, choose your transcript, go to the page of its protein product(s), and select &quot;Domains &amp;amp; Features&quot; from the right-hand menu (using human p53 as an example):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Domain source       Start End  Description                                            Accession InterPro  &#xA;PANTHER             3     331  -                                                      PTHR11447:SF6-         &#xA;Pfam                5     29   p53 transactivation domain                             PF08563   IPR013872 &#xA;PANTHER             3     331  p53 tumour suppressor family                           PTHR11447 IPR002117 &#xA;PRINTS              116   142  p53 tumour suppressor family                           PR00386   IPR002117 &#xA;PRINTS              158   179  p53 tumour suppressor family                           PR00386   IPR002117 &#xA;PRINTS              213   234  p53 tumour suppressor family                           PR00386   IPR002117 &#xA;PRINTS              236   258  p53 tumour suppressor family                           PR00386   IPR002117 &#xA;Prosite_patterns    237   249  p53 tumour suppressor family                           PS00348   IPR002117 &#xA;PRINTS              264   286  p53 tumour suppressor family                           PR00386   IPR002117 &#xA;Pfam                95    288  p53 DNA-binding domain                                 PF00870   IPR011615 &#xA;Superfamily         97    287  p53-like transcription factor DNA-binding              SSF49417  IPR008967 &#xA;Gene3D              94    297  p53/RUNT-type transcription factor DNA-binding domain  2.60.40.720IPR012346 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-07-26T17:23:16.920" CommentCount="0" />
  <row Id="2158" PostTypeId="2" ParentId="2156" CreationDate="2017-07-26T17:25:01.930" Score="4" Body="&lt;ol&gt;&#xA;&lt;li&gt;The &lt;code&gt;gi&lt;/code&gt; is an abbreviation for &quot;Genbank identifier&quot;.&lt;/li&gt;&#xA;&lt;li&gt;This is a pretty standard convention used by data stored in NCBI databases. There used to be a pretty comprehensive description of the conventions used at NCBI (I wouldn't say it was a standard or specification, just convention) &lt;a href=&quot;ftp://ftp.ncbi.nih.gov/blast/documents/formatdb.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;, but this page is no longer available it seems.&lt;/li&gt;&#xA;&lt;li&gt;The &lt;code&gt;AY848686&lt;/code&gt; label is an accession value, and the appended number is a version number.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/Sitemap/sequenceIDs.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;this page&lt;/a&gt; for more info.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The short answer is: these are all internal conventions. None of it &lt;em&gt;needs&lt;/em&gt; to be there, except perhaps to ensure NCBI's internal tools work as expected.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: I found a non-official copy of the old docs &lt;a href=&quot;http://structure.usc.edu/blast/formatdb.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. Look for the section starting with &quot;FASTA Defline Format&quot;. I've copied it here to prevent further problems with bitrot in the future.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;FASTA Defline Format&#xA;--------------------&#xA;The syntax of FASTA Deflines used by the NCBI BLAST server depends on&#xA;the database from which each sequence was obtained.  The table below lists&#xA;the identifiers for the databases from which the sequences were derived.&#xA;&#xA;&#xA;  Database Name                         Identifier Syntax&#xA;&#xA;  GenBank                               gb|accession|locus&#xA;  EMBL Data Library                     emb|accession|locus&#xA;  DDBJ, DNA Database of Japan           dbj|accession|locus&#xA;  NBRF PIR                              pir||entry&#xA;  Protein Research Foundation           prf||name&#xA;  SWISS-PROT                            sp|accession|entry name&#xA;  Brookhaven Protein Data Bank          pdb|entry|chain&#xA;  Patents                               pat|country|number &#xA;  GenInfo Backbone Id                   bbs|number &#xA;  General database identifier           gnl|database|identifier&#xA;  NCBI Reference Sequence               ref|accession|locus&#xA;  Local Sequence identifier             lcl|identifier&#xA;&#xA;&#xA;For example, an identifier might be &quot;gb|M73307|AGMA13GT&quot;, where the &quot;gb&quot; tag&#xA;indicates that the identifier refers to a GenBank sequence, &quot;M73307&quot; is its&#xA;GenBank ACCESSION, and &quot;AGMA13GT&quot; is the GenBank LOCUS.  &#xA;&#xA;&quot;gi&quot; identifiers are being assigned by NCBI for all sequences contained&#xA;within NCBI's sequence databases.  The 'gi' identifier provides a uniform&#xA;and stable naming convention whereby a specific sequence is assigned&#xA;its unique gi identifier.  If a nucleotide or protein sequence changes,&#xA;however, a new gi identifier is assigned, even if the accession number&#xA;of the record remains unchanged. Thus gi identifiers provide a mechanism&#xA;for identifying the exact sequence that was used or retrieved in a&#xA;given search.&#xA;&#xA;For searches of the nr protein database where the sequences are derived&#xA;from conceptual translations of sequences from the nucleotide databases&#xA;the following syntax is used:&#xA;&#xA;                     gi|gi_identifier&#xA;&#xA;An example would be:&#xA;&#xA;        gi|451623           (U04987) env [Simian immunodeficiency...&quot;&#xA;&#xA;where '451623' is the gi identifier and the 'U04987' is the accession&#xA;number of the nucleotide sequence from which it was derived.&#xA;&#xA;Users are encouraged to use the '-I' option for Blast output which will&#xA;produce a header line with the gi identifier concatenated with the database&#xA;identifier of the database from which it was derived, for example, from a&#xA;nucleotide database:&#xA;&#xA;        gi|176485|gb|M73307|AGMA13GT&#xA;&#xA;And similarly for protein databases: &#xA;&#xA;        gi|129295|sp|P01013|OVAX_CHICK&#xA;&#xA;The gnl ('general') identifier allows databases not on the above list to&#xA;be identified with the same syntax.  An example here is the PID identifier:&#xA;&#xA;        gnl|PID|e1632&#xA;&#xA;PID stands for Protein-ID; the &quot;e&quot; (in e1632) indicates that this ID&#xA;was issued by EMBL.  As mentioned above, use of the &quot;-I&quot; option&#xA;produces the NCBI gi (in addition to the PID) which users can also&#xA;retrieve on.&#xA;&#xA;The bar (&quot;|&quot;) separates different fields as listed in the above table.&#xA;In some cases a field is left empty, even though the original&#xA;specification called for including this field.  To make&#xA;these identifiers backwards-compatiable for older parsers&#xA;the empty field is denoted by an additional bar (&quot;||&quot;).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-07-26T17:33:05.717" LastActivityDate="2017-07-26T17:33:05.717" CommentCount="3" />
  <row Id="2159" PostTypeId="2" ParentId="2156" CreationDate="2017-07-26T17:31:21.397" Score="1" Body="&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;That is a GenBank gene ID and it (&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/Sitemap/sequenceIDs.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;apparently&lt;/a&gt;) stands for &quot;GenInfo Identifier&quot;. The full GenBank ID is &lt;code&gt;gi|61393989&lt;/code&gt;. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Yes, this is the standard format for sequences provided by NCBI. However, you can have all sorts of extra details appended to that. They (NCBI) generally use &lt;code&gt;|&lt;/code&gt; as a field separator, so you could try to write code that parse that, but I really wouldn't recommend it. The FASTA headers have traditionally been free. People can put anything they like there apart from a &lt;code&gt;\n&lt;/code&gt;, so I really wouldn't put any effort into writing code to parse these headers. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you need a short name for the sequence, just take the first N characters or, if it matches &lt;code&gt;^..|.+?|&lt;/code&gt;, take the characters until the second &lt;code&gt;|&lt;/code&gt;. Seriously though, don't bother parsing this it is very rarely helpful and is almost certain to break as soon as you use a different database. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The AY848686.1 is a GenBank accession. The &lt;code&gt;gb&lt;/code&gt; indicates that it's a GenBank accession and the &lt;code&gt;.1&lt;/code&gt; means it is the 1st version of that accession. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;As for why there are two (&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/Sitemap/sequenceIDs.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;source&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Unlike the gi number system, in which sequence identification numbers were not necessarily consistent across the databases (e.g., GenBank and EMBL could each assign their own gi number to a sequence), the new system is designed to ensure consistency. It is also designed to show a relationship between a sequence identification number and the accession number of the record in which it is found. In contrast, GI numbers are assigned consecutively and bear no resemblance to the accession number. Finally, the new system allows the assignment of alphanumeric protein IDs to proteins translations within nucleotide sequence records. The protein IDs contain three letters followed by five digits, a period, and a version number.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-07-26T17:31:21.397" CommentCount="0" />
  <row Id="2160" PostTypeId="2" ParentId="2142" CreationDate="2017-07-26T17:39:51.333" Score="3" Body="&lt;p&gt;Based on my experience reviewing k-mer counting software and as a core contributor to the khmer project, I can confidently say that there is no widely used standard format.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-07-26T17:39:51.333" CommentCount="0" />
  <row Id="2161" PostTypeId="2" ParentId="935" CreationDate="2017-07-26T17:47:55.083" Score="0" Body="&lt;p&gt;if the data is in SRA, there is sra-stat utility that returns reads,bases and quality distribution. these are stored in the SRA file. use --quick to get the stored stats or --statistics to calculate additional values broken down per readgroup/barcode.&#xA;sra-stat --quick SRR077487&lt;/p&gt;&#xA;" OwnerUserId="1203" LastActivityDate="2017-07-26T17:47:55.083" CommentCount="0" />
  <row Id="2162" PostTypeId="2" ParentId="2152" CreationDate="2017-07-26T20:45:29.493" Score="2" Body="&lt;p&gt;$C_k$ is defined as the number of reads containing a k-mer. The fraction of a read available to contain a k-mer is $(R-K+1)/R$, which is the number of k-mers in the read divided by its length. That times the nucleotide coverage ($C$) is then the expected coverage of the k-mer.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-26T20:45:29.493" CommentCount="0" />
  <row Id="2163" PostTypeId="1" CreationDate="2017-07-26T20:50:10.633" Score="3" ViewCount="52" Body="&lt;p&gt;I have a code below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def FilterReads(in_file, out_file):&#xA;&#xA;def read_ok(read):&#xA;    &quot;&quot;&quot;&#xA;    read_ok - reject reads with a low quality (&amp;lt;5) base call&#xA;    read - a PySam AlignedRead object&#xA;    returns: True if the read is ok&#xA;    &quot;&quot;&quot;&#xA;    if any([ord(c)-33 &amp;lt; _BASE_QUAL_CUTOFF for c in list(read.qual)]):&#xA;        return False&#xA;    else:&#xA;        return True&#xA;&#xA;_BASE_QUAL_CUTOFF = 30&#xA;&#xA;bam_in = pysam.Samfile(in_file, 'rb')&#xA;bam_out = pysam.Samfile(out_file, 'wb', template=bam_in)&#xA;&#xA;&#xA;for read in bam_in.fetch():&#xA;    if read_ok(read):&#xA;        bam_out.write(read)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This code works fine by &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rejecting reads having a base with phred quality score below five&lt;/li&gt;&#xA;&lt;li&gt;But it first takes a BAM file &lt;/li&gt;&#xA;&lt;li&gt;And then creates a filtered BAM file for further analysis but this takes a lot of time. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So is there a way to reject these reads using pileup in pysam so that I may not have to create a file then sort it and again read it? &#xA;Or can I modify &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/2131/filtering-bases-based-on-phred-qualities-with-pysam&quot;&gt;this code&lt;/a&gt; to perform the same function&lt;/p&gt;&#xA;" OwnerUserId="1194" LastActivityDate="2017-08-22T10:33:41.657" Title="Reject reads with low quality bases from a Bam file through pysam" Tags="&lt;bam&gt;&lt;ngs&gt;&lt;alignment&gt;&lt;python&gt;&lt;pysam&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="2164" PostTypeId="1" AcceptedAnswerId="2167" CreationDate="2017-07-26T22:03:37.427" Score="3" ViewCount="53" Body="&lt;p&gt;I'm attempting to visualize the results of my BLAST search in a way similar to the graphical display of the distribution of blast hits from the web BLAST.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, from my BLAST search:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/zVWjr.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/zVWjr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;except I would like the graph to contain information including information for the distribution of all 6139 hits across my query sequence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since this is a large number of sequences to visualize a distribution, I am considering using a score of the number of hits on a certain region, and having an output more similar to:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/HhPYg.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/HhPYg.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I were to run my blast using the BLAST+ command line tool, and read the results into R, which parts of the output should I plot in order to recreate the graphical display of the distribution? Ideally I would like to make a reusable object that can make an equivalent plot for any BLAST input.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;" OwnerUserId="727" LastEditorUserId="96" LastEditDate="2017-07-27T03:45:52.203" LastActivityDate="2017-07-28T19:18:37.343" Title="creating graph of distribution of blast hits on the query sequence" Tags="&lt;r&gt;&lt;blast&gt;&lt;visualization&gt;" AnswerCount="1" CommentCount="5" FavoriteCount="1" />
  <row Id="2165" PostTypeId="1" AcceptedAnswerId="2166" CreationDate="2017-07-27T02:14:57.477" Score="7" ViewCount="60" Body="&lt;p&gt;I have RNA seq data which I've quantified using Kallisto.  I'd like to use tximport to transform the read count data into input for EdgeR, following the R code supplied in the tximport documentation:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;cts &amp;lt;- txi$counts&#xA;normMat &amp;lt;- txi$length&#xA;normMat &amp;lt;- normMat/exp(rowMeans(log(normMat)))&#xA;library(edgeR)&#xA;o &amp;lt;- log(calcNormFactors(cts/normMat)) + log(colSums(cts/normMat))&#xA;y &amp;lt;- DGEList(cts)&#xA;y$offset &amp;lt;- t(t(log(normMat)) + o)&#xA;# y is now ready for estimate dispersion functions see edgeR User's Guide&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would ideally follow this up with EdgeR's exactTest.  However, I'd also like to remove genes with low read counts using code such as this:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;filterCounts &amp;lt;- function(counts) {&#xA;  cpms &amp;lt;- cpm(counts)&#xA;  keep &amp;lt;- rowSums(cpms &amp;gt; 1) &amp;gt;= 1&#xA;  counts[keep,]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The only issue is that I can't tell whether I should remove the genes with low read counts before or after I normality the counts while importing with tximport in the first code.  I'm thinking that it would be best to do this before, editing out the abundance, counts, and length rows of the txi data frame which correspond to lowly expressed genes, but I would like this to be confirmed before I proceed.&lt;/p&gt;&#xA;" OwnerUserId="506" LastEditorUserId="203" LastEditDate="2017-07-28T11:18:40.363" LastActivityDate="2017-07-28T11:18:40.363" Title="When performing differential expression analysis, should genes with low read counts be removed before or after normalization?" Tags="&lt;rna-seq&gt;&lt;statistics&gt;&lt;edger&gt;&lt;best-practice&gt;&lt;preprocessing&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="2166" PostTypeId="2" ParentId="2165" CreationDate="2017-07-27T06:37:14.703" Score="6" Body="&lt;p&gt;The more genes you have the more robust the scaling factor is (among the reason why one doesn't normalize to ERCC spike-ins without a compelling reason), so I suppose in theory it's better to filter after determining the scale factor. Having said that, I'd be surprised if the results changed much either way. Unless you end up filtering out a LOT of genes the scaling factors should be fairly robust to the change.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-27T06:37:14.703" CommentCount="2" />
  <row Id="2167" PostTypeId="2" ParentId="2164" CreationDate="2017-07-27T08:55:09.647" Score="2" Body="&lt;p&gt;You probably want to include query start (qstart) and query end (qend) in your blast output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blastn -query your.fasta -out blast.out.txt -db your.db -outfmt '6 qseqid sseqid qstart qend length evalue'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In R you can take the &quot;qstart:qend&quot; from each line for density plot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many ways in R to plot the densities of these start and end amino acids.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let me show an example with a small data frame:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;qstart &amp;lt;- c(200, 300, 250, 400, 500)&#xA;qend &amp;lt;- c(300, 450, 400, 600, 650)&#xA;&#xA;df &amp;lt;- as.data.frame(cbind(qstart,qend))&#xA;&#xA;aa &amp;lt;- vector()&#xA;i=1&#xA;for(i in 1:5){&#xA;aa &amp;lt;- append(aa, c(df[i,1]:df[i,2]))&#xA;i+1&#xA;}&#xA;&#xA;hist(aa)&#xA;dens &amp;lt;- density(aa)&#xA;plot(dens)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="939" LastEditorUserId="939" LastEditDate="2017-07-28T19:18:37.343" LastActivityDate="2017-07-28T19:18:37.343" CommentCount="3" />
  <row Id="2168" PostTypeId="2" ParentId="2152" CreationDate="2017-07-27T10:02:08.870" Score="5" Body="&lt;p&gt;I was still puzzled from the answers, so I tried to calculate with all the steps. I take this definition &quot;$C_k$ is the number of reads containing a k-mer.&quot; and corresponding definition for coverage ($C$): &quot;$C$ is the number of reads covering a base&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Coverage is $C = \frac{T \cdot R}{L}$, where $T$ is total number of reads, $R$ is read length and $L$ is length of genome. Given the $C_k$ definition, $C_k = \frac{T (R - K + 1)}{L-K+1}$, where $R - K + 1$ is just number of kmers in a read, and $L-K+1$ is number of kmers in a genome. Then,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$C_k = \frac{T (R - K + 1)}{L-K+1} = \frac{T (R - K + 1)}{L-K+1} \cdot \frac{R}{R} = \frac{R - K + 1}{R} \cdot \frac{T \cdot R}{L - K + 1}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;since $L &amp;gt;&amp;gt; K$, we can approximate $L - K + 1 \approx L$, then we reduce the expression to&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\frac{R - K + 1}{R} \cdot \frac{T \cdot R}{L} = \frac{R - K + 1}{R} \cdot C$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;which is the formula for $C_k$.&lt;/p&gt;&#xA;" OwnerUserId="57" LastEditorUserId="57" LastEditDate="2017-07-27T10:54:57.303" LastActivityDate="2017-07-27T10:54:57.303" CommentCount="0" />
  <row Id="2169" PostTypeId="1" AcceptedAnswerId="2171" CreationDate="2017-07-27T10:52:01.277" Score="2" ViewCount="116" Body="&lt;p&gt;I am a student trying to analyze GEO2r datas for one of my courses.&#xA;The IDs given in the output are different for different series. I need to convert all of them to a similar format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this process I encountered the following type of ID which I don't know its origin:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;”ID&quot;    &quot;logFC&quot;&#xA;&quot;SP_v2 4634&quot;    &quot;-0.9897758&quot;&#xA;&quot;SP_v2 3382&quot;    &quot;-0.8391782&quot;&#xA;&quot;SP_v2 4210&quot;    &quot;-1.1693583&quot;&#xA;&quot;SP_v2 2117&quot;    &quot;-1.0504727&quot;&#xA;&quot;SP_v2 3488&quot;    &quot;-0.9756444&quot;&#xA;&quot;SP_v2 1128&quot;    &quot;-0.8289103&quot;&#xA;&quot;SP_v2 2735&quot;    &quot;-0.8629999&quot;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Each one of the rows represent a single gene.&#xA;My question is that what is this ID? The GEO accession is GSE97750.&lt;/p&gt;&#xA;" OwnerUserId="872" LastEditorUserId="298" LastEditDate="2017-07-27T13:03:53.463" LastActivityDate="2017-07-27T13:03:53.463" Title="What database do gene IDs starting with sp_v2 refer to?" Tags="&lt;identifiers&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="0" />
  <row Id="2170" PostTypeId="1" AcceptedAnswerId="2172" CreationDate="2017-07-27T12:21:49.380" Score="1" ViewCount="21" Body="&lt;p&gt;Is there any extensive documentation or description for each class of Transfrag class codes as reported by the cuffcompare tool in the Cufflinks package?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://cole-trapnell-lab.github.io/cufflinks/cuffcompare/#transfrag-class-codes&quot; rel=&quot;nofollow noreferrer&quot;&gt;Official doc&lt;/a&gt; might not be the best. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g. what does &lt;em&gt;contained&lt;/em&gt; mean (class code: c)? Or, what is a &lt;em&gt;generic exonic overlap&lt;/em&gt; (class code: o)?&lt;/p&gt;&#xA;" OwnerUserId="294" LastActivityDate="2017-07-27T12:47:53.153" Title="Documentation for Transfrag class codes (cuffcompare)" Tags="&lt;transcriptome&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2171" PostTypeId="2" ParentId="2169" CreationDate="2017-07-27T12:27:29.010" Score="5" Body="&lt;p&gt;This is possibly a wild goose chase, but a lot of searching led me to this sample: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM2536852&quot; rel=&quot;noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM2536852&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which appears to exhibit accessions of the right format. This in turn leads to the platform:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL22166&quot; rel=&quot;noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL22166&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which is a custom spotted cDNA array, which the metadata seems to suggest is a human platform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The full annotation table gives gene symbols and Entrez IDs for most of the probes on the array:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?view=data&amp;amp;acc=GPL22166&amp;amp;id=53508&amp;amp;db=GeoDb_blob144&quot; rel=&quot;noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?view=data&amp;amp;acc=GPL22166&amp;amp;id=53508&amp;amp;db=GeoDb_blob144&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To take the example of the first line of your results:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID          ORF     Entrez gene SEQUENCE&#xA;SP_v2 4634  ETNK1   55500       AAAGCAGCTTCATCTTTCAAAATTGATTTGCTCTGGTTTT&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The Entrez Gene record for that ID matches up:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/gene/?term=55500&quot; rel=&quot;noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/gene/?term=55500&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="194" LastActivityDate="2017-07-27T12:27:29.010" CommentCount="0" />
  <row Id="2172" PostTypeId="2" ParentId="2170" CreationDate="2017-07-27T12:47:53.153" Score="1" Body="&lt;p&gt;You can get a bit more information by &lt;a href=&quot;https://github.com/cole-trapnell-lab/cufflinks/blob/b55bb214ad4abeb58a1435d8f7a7f0dd8bed8b76/src/gff_utils.cpp&quot; rel=&quot;nofollow noreferrer&quot;&gt;looking at the source code comments&lt;/a&gt;. &lt;a href=&quot;https://github.com/cole-trapnell-lab/cufflinks/blob/b55bb214ad4abeb58a1435d8f7a7f0dd8bed8b76/src/gff_utils.cpp#L140-L141&quot; rel=&quot;nofollow noreferrer&quot;&gt;Contained&lt;/a&gt; is for single exon transcripts that are primarily contained (&lt;a href=&quot;https://github.com/cole-trapnell-lab/cufflinks/blob/b55bb214ad4abeb58a1435d8f7a7f0dd8bed8b76/src/gff_utils.cpp#L145&quot; rel=&quot;nofollow noreferrer&quot;&gt;&gt;80% overlap&lt;/a&gt;) within an exon of an already annotated transcript. &lt;code&gt;Generic exonic overlap&lt;/code&gt; is for cases where a transfrag exon overlaps that of an annotated transcript. The strand doesn't seem to matter in such cases.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-07-27T12:47:53.153" CommentCount="0" />
  <row Id="2173" PostTypeId="1" CreationDate="2017-07-28T01:49:03.500" Score="0" ViewCount="62" Body="&lt;p&gt;Take for instance, this hypothetical example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bam &amp;lt;- system.file(&quot;extdata&quot;,package = &quot;SomePackage&quot;,&quot;snps.bam&quot;)&#xA;reads &amp;lt;- readGAlignments(bam)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How do I display &lt;code&gt;reads&lt;/code&gt; in a plot with the second and fourth graph of this picture (mismatch and reads) &lt;a href=&quot;https://i.stack.imgur.com/23dbd.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/23dbd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;, and the first graph of this picture? &lt;a href=&quot;https://i.stack.imgur.com/ZFZAO.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZFZAO.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've tried getting additional columns from &lt;code&gt;readGAlignments&lt;/code&gt;, but it still seems to break.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also tried using Gviz, but that doesn't support indels, which is crucial to my work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, is there a way to see individual mismatches on the reads like in the first graph of the second picture, or like in Gviz? I've looked at the varying graphs for ggbio and the karyogram seems like the closest thing.&lt;/p&gt;&#xA;" OwnerUserId="1218" LastEditorUserId="203" LastEditDate="2017-07-29T16:12:54.553" LastActivityDate="2017-07-29T16:12:54.553" Title="Using the ggbio package in R, how do you display a GenomicAlignments object as a mismatch plot and reads plot using autoplot?" Tags="&lt;r&gt;&lt;visualization&gt;&lt;genome-browser&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2174" PostTypeId="1" AcceptedAnswerId="2175" CreationDate="2017-07-28T08:30:21.540" Score="4" ViewCount="88" Body="&lt;p&gt;I have a fastq file from minION (albacore) that contains information on the read ID and the start time of the read. I want to extract these two bits of information into a single csv file. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been trying to figure out a grep/awk/sed solution, but without success. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;E.g.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@93a12f52-95e5-40c7-8c3e-70bf94ed0720 runid=17838b1d08f30a031bf60afabb146a8b0fba7486 read=12217 ch=492 start_time=2017-07-04T06:42:43Z&#xA;CTATTGTCCCCTGCCGCTGCCCCTCCTGCTACGCCCCACTGCCTCACCAGCCGTTACGGTCGCCCCCCATCGCATGCCTTTACACACACACTTCTTTACACATGCTATCTTCCC&#xA;+&#xA;&quot;&quot;*)&amp;amp;$-.,-(#&quot;&amp;amp;'$%''+16#&quot;$##&amp;amp;)%%/&quot;+*(*(&amp;amp;#&quot;&amp;amp;'%1&quot;+)#)&quot;&quot;%$$#&amp;amp;&amp;amp;'1%&quot;'8&amp;gt;MJ&amp;lt;#'&amp;amp;%'.2'.$(&amp;amp;#'()'&amp;amp;&amp;amp;%'$('%&quot;%%%..$#&quot;&amp;amp;&quot;#+&amp;amp;,*$%&quot;#&quot;&#xA;@ff37e422-a25f-404c-8314-ef1733f9c30c runid=17838b1d08f30a031bf60afabb146a8b0fba7486 read=8432 ch=200 start_time=2017-07-04T06:56:41Z&#xA;CGATGGCCGTATGCTTTTGTTATGAAGCGAAAAGCTGCTCGCTTCTCTAGATAATAATGATGTGGCGAAAACGCTATGCGATTCGTTGACATACATGATGGCGGATTTATCTACCACTTTGTGGCATGCTTTTCTCGCCAGATAATGGAATGTTTCTCTGGCGGTAATGGATAGTATCAAATCTCACTAGCCCATTCTATAAGCGCATCCGCATGCACTAGTTCTTGATTCGATCGTCCTCTAGCATGTTCGAAGAAGATAGCATTCACTATCATCATCGCCTCAGGTAAGTTTATTCGGTTGGGGCGTGTGAAGGCAAACACCTTGTTGTCCAGTAAGTTTTCAGTTACTATAACTTAAAGTCGCACATGAATCTAGTCTCCTATTCCCCACCCATGATCCCACTCACACATTTCTACAGAGATGTGGTTAGAAATTTTCATATTAGGTCAGCTTTGACTCAATAAGACATAATTCTTCACTGAATGACTTTTTAAGAACCACCAGGACCAGAGAGAACCAAGAGAGTGGTACCTCTTAAAACACAATAAAGTGATTCAGCCTTAGCCATTGGATTCTGGAGGACCTTGAACCATGTGGGAAGCAGCTCAGGGTGGCCATGTACTATACTGGCGGGTAAGCTTCTGGAGTGCTAGGTTCTTTTTGTCTTTTCTTAAGCATTGCCGCCAGTTGATTGGGTTTTGAACATAAAATAATGCGCCACCAGCAATTCCAGATTTGTTCCTACGGGATAGATTTGTTCAGTTCTAGCATTATGCTTCACTAACCAGATGCGGGCCCTAAGTCCTTCACTTGGAATATTGGATTGGATCATGAGAATATTCTGTCTGAAGCTCGTCATTAATTTTGTTACAAAATAGAGCTTTTTGACTGGAAGTACCACCATACGTGTTCTCAAACTTCAGCATTTTTAGAACTTCCCACGGCATCTTGACCCTTTTCACAGCATGGATAGTCAGGCAGCAGTGAACTTTGTGACTCTTTAATGCCTTCACTTTTCTCTCAGTTTCCCCGCCTTGCGTTATCTTTACTCGTCTTGGGACTTTTATCCCAATGCCAGCCTTCTACCCTGAGACCTCAGTGGGTCATCATCCCAGCCCGGGACATCTCATCCCATCATTTATGGGCTGTTGTGTTTTTTTCAAAACCTAGCCCTCTCAGGAGGAGGAGGAGTGGGAGTCAGTTCAGTGAGGAGGATTAGGATGATCTGAAATGTAAGCACATATAAGCGAAGCACTTATTTTGGGTTGGGTCCTCACGGTGGACATAAGATCGCCTTATGTGTTTAGTAAGCCATTCCTAGCTCTCAATGGCGTGATTACATAGAAGCGTGAGGGATCAGTCCTATGGAAGACTAGGAAGTAAATGAACAAAATATATTAACCATAGAAGTCTCATGGGTCGCTGTAGCCAAAAGATTAACACTTTTGACTACATTGTGGTTTTAGGCATTGAAACAAAAACTTTGAGTCTCCTAAACAAATGAATGGAAAATAGTAGCGAACTTCGATTCCTAACATTAAATCTAGAAATAGCAAGTTAGTTTAAAGACTTTATTTAGCTTTGCTTGCTATAATGAAAACCTTGCCTCCCGGTCGGGGCCATTGTGCCTGAAGCTAGCTTATTGTCTCCTCGAGCTCCCAGCTTCAGCAACTCCTTTTGAAGCGTTTGTCTCAGCTTGGATCTTCAGCAGCTCTTGGTGGCTCTTTTGAGCTAGCTCCTCTGAGATCTTGTATTTGGTAGGTCGCTTAGTCATAGTACTTTTCTTTTAACACCCTTCAGCTCTACGATTACATTTGGTTTTGTGGATATCATAATGATTGATGTGAAGATACATTGTACATGTG&#xA;+&#xA;&quot;#&quot;&quot;#&quot;##&quot;'&quot;&amp;amp;%&amp;amp;?JP7+80)'&amp;amp;*+&amp;amp;&amp;amp;(,.&amp;gt;3&amp;amp;*(#%(')*&amp;amp;'*#(-$)&quot;&amp;amp;,63;?844&amp;amp;&amp;amp;#$'3+.&amp;gt;@9;-/259...&amp;amp;:&quot;$&quot;/*&quot;#*#%(.&amp;amp;%)+,/76/+'4B93:;70.1)'1:)*#&amp;amp;()(+'03589/++&amp;amp;)'$-:;@,B5(9+.JAU;7-+&amp;amp;,1212.+329NPSDHRHH6),&amp;amp;)%)+).-*$,&amp;lt;1(.+&amp;amp;$$2&amp;amp;,-0+.&amp;amp;'//&quot;4(%&quot;+#,/)&amp;lt;56%*%*$&amp;lt;4*/;DI+6&amp;gt;((-%*).+--;2-'04-062&amp;lt;@-&amp;lt;&amp;gt;6CP)#.0/+,#(*2+#&quot;$$-2'5(%%&amp;amp;'5+)$$%8+0;&amp;gt;.@9*(,((,*-2/49G9/1/3./.0CQ93E+/-D-'67J*)($-2+'*/&amp;amp;*)+%&amp;amp;'092&amp;lt;C9/5()$&amp;amp;%(.,+-'**)'2/.(//6&amp;lt;59-5-)&quot;**40:A4+834)-(*+;K0-,&amp;amp;)+%)(#-&amp;amp;+&amp;amp;&amp;amp;))+$&amp;gt;9.6&amp;lt;?A;3'/85$/--..)/3DP99HJ9.&quot;2;I?@C/5%.4&amp;lt;0+0+&amp;amp;$-05C46208AFQE80&quot;%356+02*9I73//(-*/9706192*/)%)(&amp;amp;'($1749C*KKA*/#&quot;/*)5($'#+7@=$%)/&amp;amp;+%##*&amp;amp;&amp;amp;;.),#(4.,2/'-G03%&quot;&quot;#,(2)./,%%1;7#+%$'(03'5)+&amp;amp;.%1)&amp;amp;(%'#*,$3#''-3,),$&quot;),'+,,*$%'%$33ILADPPY@JW\O2-8'9[Y@--*.,&quot;&quot;$%$&quot;%#&quot;65',0/4(6.*43B1.#&amp;amp;)+),(6BI729=9&amp;amp;#$.)#,(,(/-286D&amp;amp;),'C;P=)'-'#$)*91*$%(.3LH31,($/(+)'$$*#+'((('*)00:7C1E24(%+$&amp;amp;-,$'++)&amp;lt;)'**&quot;&amp;amp;''&quot;(*(.14++$5&amp;amp;*,%?.F1&amp;lt;10&quot;&amp;amp;%$,)&amp;amp;&amp;amp;*(&amp;amp;$%$/.&quot;&quot;+3&quot;#&quot;#&quot;*%5*-&amp;gt;87FQ&amp;gt;AKY&amp;lt;9855,&amp;amp;(*@&amp;gt;HAZRQ)'%$&quot;--&amp;amp;/#%.+,/A9F8-1'+&quot;6--*34P-9;=997;7&amp;lt;SUU8((-+.:784/%).&amp;amp;)#*'(*/%7H(1JI-+&amp;amp;+)&amp;amp;9.,/N)&quot;%&quot;&amp;amp;))22/&amp;amp;,)%)'+*/17:8$''(*.*..(/.(%+*&amp;amp;22(+)7GM99A@9)0*1%/470((&amp;amp;((%(937'15084#,($&quot;&amp;amp;'%&quot;'4.,+5&amp;lt;7($*0+*291$*&amp;amp;$%%('+/)'.1-(-.7=K/'%&quot;'*)03&amp;lt;:/&amp;lt;=1(&amp;gt;.((-9-'+&amp;gt;&amp;amp;&amp;amp;&amp;amp;%';,)69V&amp;lt;'1:&amp;gt;FNXA+01:7*#&quot;$'$.%&amp;amp;4BQT7/&amp;amp;03++)'&amp;amp;'*3E.+0(*6..*&amp;lt;.)9*)))'%#(*.&amp;amp;&amp;amp;'&quot;*-IU=&amp;amp;,-))-.,-&amp;gt;'G--6..90067%(,0/*%9)&amp;amp;#'&quot;&quot;#$,5H/$/#-/&amp;amp;2(.0.+;&amp;gt;9--7.3@+2(/4.*0&amp;amp;,-&amp;amp;'/*%0-**-#,/2&amp;amp;-$*)'+#&quot;*)-*##&amp;amp;$3):C:D6H+',./2?&amp;lt;0&amp;gt;)%#&amp;amp;&amp;amp;3::9;8$#%#('$#$',,,*-346@=.%&amp;amp;&amp;amp;(&amp;amp;%('*+&amp;amp;*&amp;amp;1)'&amp;amp;-..#'+2*)/6499=-2*,0,82,$)($:2%*##%$,.5=-5/((#)2./('($@=4,)%1+176:AHQT,(,)0(()@2'+(18G?($)(*'.)&amp;amp;&amp;amp;+*12SD7,18I2)()*+C:01ETZIPDUOB+.55K7B:A5)*/('')/-1=7E&amp;lt;IGR=43@A68;K/:8.'--)BGEE5-0/5+,2''DXBLW7)#DHTC5AL82+--,&amp;amp;#);+).,'3)/.336/&amp;amp;'&amp;amp;(,+8E346-/%1-)1*$+),@-&amp;amp;-%'$+)5.&amp;amp;&amp;amp;%&amp;amp;#(,2&amp;amp;03&amp;amp;/((,(-5%%)?7I4*').+/7+&amp;amp;***845(*/.)(.0))&amp;lt;?YK3=GHH2&amp;amp;*)%$%#%KLY&amp;lt;9&amp;amp;&amp;amp;+,8;/2*./;+))-)/03$&amp;amp;*$+0'36&amp;lt;J+5/+&amp;amp;+(HGFFM57,59)-,0)/:**&quot;%$&amp;amp;#$'&amp;amp;$%#)*%33VX=@FOL00(''4IGIYOG0=1,,#(*$)-0CFM?8+31E8)(+)(%'&quot;'/&quot;&quot;%+3/,1;+7-*''&amp;amp;-%98.$$/'&amp;amp;)&amp;amp;+&amp;amp;(&amp;amp;&amp;amp;&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Should produce&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="156" LastEditorUserId="73" LastEditDate="2017-08-01T22:27:22.980" LastActivityDate="2017-08-01T22:27:22.980" Title="Extract nanopore read ID &amp; start times from fastq file" Tags="&lt;nanopore&gt;&lt;fastq&gt;&lt;csv&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="0" />
  <row Id="2175" PostTypeId="2" ParentId="2174" CreationDate="2017-07-28T08:37:41.497" Score="7" Body="&lt;pre&gt;&lt;code&gt;awk '{if(NR%4==1) print $1, $5}' file.fastq | sed -e &quot;s/ start_time=/, /&quot; -e &quot;s/^@//&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;awk&lt;/code&gt; command gets the first of every 4 lines, printing the first and fifth &quot;word&quot;. &lt;code&gt;sed&lt;/code&gt; is then used to strip the initial &lt;code&gt;@&lt;/code&gt; and replace &lt;code&gt;start_time=&lt;/code&gt; with &lt;code&gt;,&lt;/code&gt;. The output on your example file is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="77" LastEditorUserId="298" LastEditDate="2017-07-28T15:15:43.600" LastActivityDate="2017-07-28T15:15:43.600" CommentCount="1" />
  <row Id="2176" PostTypeId="2" ParentId="2174" CreationDate="2017-07-28T15:10:39.743" Score="5" Body="&lt;p&gt;Since the string &lt;code&gt;start_time&lt;/code&gt; will only appear on the header line, or else you don't have a valid fastq file, you can simply do:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ perl -ne '/^@(\S+).*start_time=(.*)/ &amp;amp;&amp;amp; print &quot;$1, $2\n&quot;' file.fastq &#xA;93a12f52-95e5-40c7-8c3e-70bf94ed0720,2017-07-04T06:42:43Z&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c,2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Alternatively, since you mentioned &lt;code&gt;awk&lt;/code&gt; and &lt;code&gt;sed&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ awk -v OFS=&quot;, &quot; '/start_time/{print $1,$NF}' file.fastq | sed 's/start_time=//'&#xA;@93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xA;@ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or, doing the whole thing in &lt;code&gt;awk&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ awk 'sub(/start_time=/,&quot;&quot;){print $1&quot;, &quot;$NF}' file.fastq &#xA;@93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xA;@ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And if the &lt;code&gt;@&lt;/code&gt; annoy you:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ awk 'sub(/^@/,&quot;&quot;) &amp;amp;&amp;amp; sub(/ .*start_time=/,&quot;, &quot;)' file.fastq &#xA;93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And in &lt;code&gt;sed&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ sed -n 's/^@\([^ ]*\).*start_time=\(.*\)/\1, \2/p' file.fastq &#xA;93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or, if your sed supports it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ sed -En 's/^@(\S+).*start_time=(.*)/\1, \2/p' file.fastq &#xA;93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Finally, since &lt;code&gt;grep&lt;/code&gt; can't do replacements, to use it you would have to do something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ grep -oP '^@\K\S+|start_time=\K.*' file.fastq | paste - - &#xA;93a12f52-95e5-40c7-8c3e-70bf94ed0720    2017-07-04T06:42:43Z&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c    2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And to get the commas:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ grep -oP '^@\K\S+|start_time=\K.*' file.fastq | paste - - | sed 's/\t/, /'&#xA;93a12f52-95e5-40c7-8c3e-70bf94ed0720, 2017-07-04T06:42:43Z&#xA;ff37e422-a25f-404c-8314-ef1733f9c30c, 2017-07-04T06:56:41Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-07-28T15:10:39.743" CommentCount="0" />
  <row Id="2177" PostTypeId="1" AcceptedAnswerId="2178" CreationDate="2017-07-28T17:57:12.517" Score="4" ViewCount="44" Body="&lt;p&gt;GFF3 formats are tabular files with 9 fields per line, separated by tabs.First 8 fields share almost same data structure. But the 9th field varies a lot depending on the features. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;strong&gt;last field(9th field)&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;varies&lt;/em&gt;&lt;/strong&gt; a lot from one gene prediction algorithm to another.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Presently I am trying to build my own gff module that will parse one gff file and return one structured data structure( &lt;strong&gt;python dictionary structure&lt;/strong&gt;). For that, I need a good understanding of the 9th field of the gff format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So can any one please help me by providing various gff files.&lt;/p&gt;&#xA;" OwnerUserId="211" LastEditorUserId="96" LastEditDate="2017-07-28T19:19:29.480" LastActivityDate="2017-07-28T19:19:29.480" Title="How 9th column of a gff file varies from one gene prediction algorithm to another?" Tags="&lt;file-formats&gt;&lt;python&gt;&lt;gff3&gt;&lt;sequence-annotation&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2178" PostTypeId="2" ParentId="2177" CreationDate="2017-07-28T19:18:45.167" Score="6" Body="&lt;p&gt;The first place to start is the &lt;a href=&quot;https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md&quot; rel=&quot;noreferrer&quot;&gt;GFF3 specification&lt;/a&gt;. This is the official word on what is and is not allowed in a GFF3 file. For example, users can define arbitrary attribute keys, so long as they do not begin with an uppercase letter (these are reserved for &quot;official&quot; use).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But your question doesn't seem to be about what is &lt;em&gt;allowed&lt;/em&gt;, but what is &lt;em&gt;commonly used&lt;/em&gt;. I have a question for you: which gene predictors are YOU using? Or are you using gene annotations produced by others?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are a few examples.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;NCBI RefSeq&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;If you're using GFF3 from NCBI, chances are it looks something like this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;NC_007070.3     Gnomon  gene    329235  331223  .       +       .       ID=gene14;Dbxref=BEEBASE:GB42168,GeneID:551678;Name=LOC551678;gbkey=Gene;gene=LOC551678;gene_biotype=protein_coding&#xA;NC_007070.3     Gnomon  mRNA    329235  331223  .       +       .       ID=rna27;Parent=gene14;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;Name=XM_624067.4;gbkey=mRNA;gene=LOC551678;model_evidence=Supporting evidence includes similarity to: 66 ESTs%2C 24 Proteins%2C and 99%25 coverage of the annotated genomic feature by RNAseq alignments%2C including 113 samples with support for all annotated introns;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xA;NC_007070.3     Gnomon  exon    329235  329459  .       +       .       ID=id117;Parent=rna27;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;gbkey=mRNA;gene=LOC551678;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xA;NC_007070.3     Gnomon  exon    329850  330082  .       +       .       ID=id118;Parent=rna27;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;gbkey=mRNA;gene=LOC551678;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xA;NC_007070.3     Gnomon  exon    330166  330301  .       +       .       ID=id119;Parent=rna27;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;gbkey=mRNA;gene=LOC551678;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xA;NC_007070.3     Gnomon  exon    330376  331223  .       +       .       ID=id120;Parent=rna27;Dbxref=GeneID:551678,Genbank:XM_624067.4,BEEBASE:GB42168;gbkey=mRNA;gene=LOC551678;product=receptor expression-enhancing protein 5-like%2C transcript variant X1;transcript_id=XM_624067.4&#xA;NC_007070.3     Gnomon  CDS     329333  329459  .       +       0       ID=cds8;Parent=rna27;Dbxref=GeneID:551678,Genbank:XP_624070.1,BEEBASE:GB42168;Name=XP_624070.1;gbkey=CDS;gene=LOC551678;product=receptor expression-enhancing protein 5-like isoform X1;protein_id=XP_624070.1&#xA;NC_007070.3     Gnomon  CDS     329850  330082  .       +       2       ID=cds8;Parent=rna27;Dbxref=GeneID:551678,Genbank:XP_624070.1,BEEBASE:GB42168;Name=XP_624070.1;gbkey=CDS;gene=LOC551678;product=receptor expression-enhancing protein 5-like isoform X1;protein_id=XP_624070.1&#xA;NC_007070.3     Gnomon  CDS     330166  330301  .       +       0       ID=cds8;Parent=rna27;Dbxref=GeneID:551678,Genbank:XP_624070.1,BEEBASE:GB42168;Name=XP_624070.1;gbkey=CDS;gene=LOC551678;product=receptor expression-enhancing protein 5-like isoform X1;protein_id=XP_624070.1&#xA;NC_007070.3     Gnomon  CDS     330376  330416  .       +       2       ID=cds8;Parent=rna27;Dbxref=GeneID:551678,Genbank:XP_624070.1,BEEBASE:GB42168;Name=XP_624070.1;gbkey=CDS;gene=LOC551678;product=receptor expression-enhancing protein 5-like isoform X1;protein_id=XP_624070.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;MAKER&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The MAKER annotation workflow (&lt;a href=&quot;http://dx.doi.org/10.1101/gr.6743907&quot; rel=&quot;noreferrer&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;http://www.yandell-lab.org/software/maker.html&quot; rel=&quot;noreferrer&quot;&gt;software&lt;/a&gt;) is a pretty commonly used gene annotation tool, and produces GFF3 output like this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scaffold_12     maker   gene    652527  655343  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959;Name=maker-scaffold_12-augustus-gene-0.959&#xA;scaffold_12     maker   mRNA    652527  655343  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1;Parent=maker-scaffold_12-augustus-gene-0.959;Name=maker-scaffold_12-augustus-gene-0.959-mRNA-1;_AED=0.24;_eAED=0.18;_QI=0|0|0|0.66|0.5|0.33|3|0|218&#xA;scaffold_12     maker   exon    652527  652817  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:exon:1203;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xA;scaffold_12     maker   exon    654877  655170  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:exon:1204;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xA;scaffold_12     maker   exon    655272  655343  .       +       .       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:exon:1205;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xA;scaffold_12     maker   CDS     652527  652817  .       +       0       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:cds;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xA;scaffold_12     maker   CDS     654877  655170  .       +       0       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:cds;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xA;scaffold_12     maker   CDS     655272  655343  .       +       0       ID=maker-scaffold_12-augustus-gene-0.959-mRNA-1:cds;Parent=maker-scaffold_12-augustus-gene-0.959-mRNA-1&#xA;scaffold_12     maker   gene    941547  943897  .       +       .       ID=snap-scaffold_12-processed-gene-0.851;Name=snap-scaffold_12-processed-gene-0.851&#xA;scaffold_12     maker   mRNA    941547  943897  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1;Parent=snap-scaffold_12-processed-gene-0.851;Name=snap-scaffold_12-processed-gene-0.851-mRNA-1;_AED=0.22;_eAED=0.22;_QI=0|0|0.25|0.25|1|1|4|661|95&#xA;scaffold_12     maker   exon    941547  941631  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:exon:1206;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xA;scaffold_12     maker   exon    942343  942367  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:exon:1207;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xA;scaffold_12     maker   exon    942780  942920  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:exon:1208;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xA;scaffold_12     maker   exon    943200  943897  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:exon:1209;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xA;scaffold_12     maker   CDS     941547  941631  .       +       0       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:cds;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xA;scaffold_12     maker   CDS     942343  942367  .       +       2       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:cds;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xA;scaffold_12     maker   CDS     942780  942920  .       +       1       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:cds;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xA;scaffold_12     maker   CDS     943200  943236  .       +       1       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:cds;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xA;scaffold_12     maker   three_prime_UTR 943237  943897  .       +       .       ID=snap-scaffold_12-processed-gene-0.851-mRNA-1:three_prime_utr;Parent=snap-scaffold_12-processed-gene-0.851-mRNA-1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;GeneMark, SNAP, Augustus&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;If we look at the output of several common &lt;em&gt;ab initio&lt;/em&gt; gene prediction tools, none of them actually produces GFF3 by default. Here is some output from GeneMark (&lt;a href=&quot;https://doi.org/10.1093/nar/26.4.1107&quot; rel=&quot;noreferrer&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;http://exon.gatech.edu/GeneMark/&quot; rel=&quot;noreferrer&quot;&gt;software&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;543   1   +  Initial         799051     799097         47          1 2 - -&#xA;543   2   +  Terminal        799236     799266         31          3 3 - -&#xA;&#xA;544   3   -  Terminal        802357     802514        158          3 2 - -&#xA;544   2   -  Internal        802607     802685         79          1 1 - -&#xA;544   1   -  Initial         802829     802843         15          3 1 - -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is some output from SNAP (&lt;a href=&quot;http://dx.doi.org/10.1186/1471-2105-5-59&quot; rel=&quot;noreferrer&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;http://korflab.ucdavis.edu/software.html&quot; rel=&quot;noreferrer&quot;&gt;software&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Einit   8230    8239    +       9.329   0       1       0       scaffold_12-snap.4&#xA;Exon    8848    8869    +       2.484   2       2       2       scaffold_12-snap.4&#xA;Exon    10121   10208   +       15.302  1       0       2       scaffold_12-snap.4&#xA;Exon    11361   11420   +       5.969   0       0       2       scaffold_12-snap.4&#xA;Exon    11471   11535   +       0.921   0       2       1       scaffold_12-snap.4&#xA;Eterm   12169   12187   +       18.163  1       0       1       scaffold_12-snap.4&#xA;Einit   14569   14668   +       -6.918  0       1       0       scaffold_12-snap.5&#xA;Exon    15029   15203   +       -1.023  2       2       0       scaffold_12-snap.5&#xA;Exon    16171   16307   +       -9.230  1       1       1       scaffold_12-snap.5&#xA;Eterm   16667   16698   +       9.829   2       0       0       scaffold_12-snap.5&#xA;Einit   17809   17898   -       -5.390  0       0       0       scaffold_12-snap.6&#xA;Exon    17299   17350   -       6.978   0       1       2       scaffold_12-snap.6&#xA;Eterm   17029   17129   -       -2.857  2       0       0       scaffold_12-snap.6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Both of these formats appear to be tab-delimited, but that's where the similarity with GFF3 ends.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output of Augustus (&lt;a href=&quot;https://doi.org/10.1093/nar/gkl200&quot; rel=&quot;noreferrer&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;http://bioinf.uni-greifswald.de/augustus/&quot; rel=&quot;noreferrer&quot;&gt;software&lt;/a&gt;) looks like GTF (a variant of the GFF format) once you lose all the lines beginning with a &lt;code&gt;#&lt;/code&gt; symbol, but Augustus also has a GFF3 output mode.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# Predicted genes for sequence number 1 on both strands&#xA;# start gene g1&#xA;scaffold_12     AUGUSTUS        gene    2841    3400    1       +       .       g1&#xA;scaffold_12     AUGUSTUS        transcript      2841    3400    1       +       .       g1.t1&#xA;scaffold_12     AUGUSTUS        start_codon     2841    2843    .       +       0       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xA;scaffold_12     AUGUSTUS        intron  3027    3097    1       +       .       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xA;scaffold_12     AUGUSTUS        CDS     2841    3026    1       +       0       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xA;scaffold_12     AUGUSTUS        CDS     3098    3400    1       +       0       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xA;scaffold_12     AUGUSTUS        stop_codon      3398    3400    .       +       0       transcript_id &quot;g1.t1&quot;; gene_id &quot;g1&quot;;&#xA;# protein sequence = [MAIKNAEHDLRVIVDAIEGLGLKVAPHKTEAMAFPASALCGRRGAAPPKIRLGGSSILVGSRSRWYISGHSENSSKSP&#xA;# RTEGKETTPLQQRDPLDAPLWVSGVVAHCCGGPEGQEGCPGLAAQGSDQGVLRIRDGLLCGYDGCGDHRPRPSDSSAGGGLCRP]&#xA;# end gene g1&#xA;###&#xA;# start gene g2&#xA;scaffold_12     AUGUSTUS        gene    4712    15229   0.21    +       .       g2&#xA;scaffold_12     AUGUSTUS        transcript      4712    15229   0.21    +       .       g2.t1&#xA;scaffold_12     AUGUSTUS        start_codon     4712    4714    .       +       0       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xA;scaffold_12     AUGUSTUS        intron  4858    5591    0.74    +       .       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xA;scaffold_12     AUGUSTUS        intron  5686    15028   0.23    +       .       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xA;scaffold_12     AUGUSTUS        CDS     4712    4857    0.72    +       0       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xA;scaffold_12     AUGUSTUS        CDS     5592    5685    0.42    +       1       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xA;scaffold_12     AUGUSTUS        CDS     15029   15229   0.43    +       0       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xA;scaffold_12     AUGUSTUS        stop_codon      15227   15229   .       +       0       transcript_id &quot;g2.t1&quot;; gene_id &quot;g2&quot;;&#xA;# protein sequence = [MGRNSHRSCCVVNCKITSAKSDCKFYKFPTAKWKINQRKMWVAAVKRQKYIKDEISHAETQTEITEVTGATKVNYANK&#xA;# KYICLLFVRTYVLRMLVDVALSNLRFSLFGIRKSLEIFGQSEKADQTRWRLPSCEMEWIESRKGKMRE]&#xA;# end gene g2&#xA;###&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Parsing attributes&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;So...how should YOU handle attributes in GFF3's 9th column? That depends a lot on what you want to do with the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most important attributes are &lt;code&gt;ID&lt;/code&gt; and &lt;code&gt;Parent&lt;/code&gt;, which are used to define relationships between features and subfeatures. (These relationships implicitly define a directed acyclic graph of features, although most GFF3 parsers don't directly support traversal of this graph.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But not only do you have to handle the pre-defined attributes discussed in the GFF3 specification, you also have to be able to handle any number of arbitrary attributes whose keys you may not know beforehand.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The simplest way to handle this would be to parse the attribute column into a dictionary of key/value pairs. Once it's in this form, it's trivial to see what attributes are there and how to access them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if we have a feature with the following attributes...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID=mRNA42;Parent=gene19;integrity=0.95;foo=bar&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;...we would want it in a dictionary like so.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;attributes = {&#xA;    'ID': 'mrna42',&#xA;    'Parent': 'gene19',&#xA;    'integrity': '0.95',&#xA;    'foo': 'bar',&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Python code to parse that might look something like this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;attributes = dict()&#xA;for keyvaluepair in attributestring.split(';'):&#xA;    for key, value in keyvaluepair.split('='):&#xA;        attributes[key] = value&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;One thing to consider is that it's possible for an attribute to have multiple values (separated by commas). It's not commonly used, but it is valid and something that should be handled or at least checked for.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-07-28T19:18:45.167" CommentCount="2" />
  <row Id="2179" PostTypeId="1" CreationDate="2017-07-28T23:41:39.557" Score="-2" ViewCount="52" Body="&lt;pre&gt;&lt;code&gt;use Bio::DB::GenBank;&#xA;use Bio::DB::Query::GenBank;&#xA;&#xA;$query = &quot;LEGK&quot;; &#xA;$query_obj = Bio::DB::Query::GenBank-&amp;gt;new(-db =&amp;gt; 'protein', &#xA;                                          -query =&amp;gt; $query );&#xA;&#xA;$gb_obj = Bio::DB::GenBank-&amp;gt;new;&#xA;&#xA;$stream_obj = $gb_obj-&amp;gt;get_Stream_by_query($query_obj);&#xA;&#xA;while ($seq_obj = $stream_obj-&amp;gt;next_seq) {&#xA;    # do something with the sequence object    &#xA;    print &quot;&amp;gt;$query&quot;,' ', $seq_obj-&amp;gt;display_id, ' ', $seq_obj-&amp;gt;desc,&quot;\n&quot;, $seq_obj-&amp;gt;seq[,'\n';&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can I print first occurrence of protein sequence?&lt;/p&gt;&#xA;" OwnerUserId="1224" LastEditorUserId="48" LastEditDate="2017-08-01T12:57:03.253" LastActivityDate="2017-08-01T12:57:03.253" Title="Bioperl - how can i print first result of search sequence per iteration?" Tags="&lt;database&gt;&lt;public-databases&gt;&lt;perl&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="0" />
  <row Id="2180" PostTypeId="1" AcceptedAnswerId="2181" CreationDate="2017-07-29T00:06:37.310" Score="3" ViewCount="86" Body="&lt;p&gt;I'm very new to bioinformatics in general, and I'm trying to understand some basic concepts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have RNAseq data, and bioinformatics people tell me that intensities cannot be compared across patients. So there are all of these pipelines to compare intensities to Z scores--are those as simple as just plugging data into a bioconductor package?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be great to get some overview description of why Z values are important/why you can't compare intensities across patients, and/or some pointers towards any resources that I can read to learn more.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks much for your time!&lt;/p&gt;&#xA;" OwnerUserId="1226" LastActivityDate="2017-07-31T09:09:58.577" Title="RNAseq: Z score, Intensity, and Resources" Tags="&lt;rna-seq&gt;&lt;bioconductor&gt;&lt;python&gt;&lt;normalization&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="0" />
  <row Id="2181" PostTypeId="2" ParentId="2180" CreationDate="2017-07-30T08:57:43.057" Score="3" Body="&lt;p&gt;It depends on what test or analysis you want to do, whether you need intensities (expression values) or z-scores.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to do statistical analysis, such as finding differentially expressed genes between groups of patients (e.g., with limma), you don't want to use z-scores. But you use normalized intensities (for microarrays) or start with raw counts for RNAseq. Then follow the user guide (limma has a great user guide, also for starters).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For visualization in heatmaps or for other clustering (e.g., k-means, fuzzy) it is useful to use z-scores. Z-scores are a form of transformation (scaling), where every genes is sort of &quot;reset&quot; to the mean of all samples, using also the standard deviation. If you want to know exactly what a z-score is, a simple google search can tell you the details. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In R you can use the scale function for z-score transformation. Be aware that the function works on columns though. Which means you have to transpose your matrix first if genes are in the rows, and then transpose them back after transformation.&lt;/p&gt;&#xA;" OwnerUserId="939" LastActivityDate="2017-07-30T08:57:43.057" CommentCount="2" />
  <row Id="2182" PostTypeId="2" ParentId="2179" CreationDate="2017-07-30T15:28:33.087" Score="3" Body="&lt;p&gt;In the simplest case, if you just want to stop after the first record was printed, you can just add &lt;code&gt;exit&lt;/code&gt; (I also corrected the syntax errors you had and added &lt;code&gt;use strict&lt;/code&gt; and &lt;code&gt;use warnings&lt;/code&gt;; I suggest you get into the habit of using those two, they save you from a lot of grief in the long run):&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-perl prettyprint-override&quot;&gt;&lt;code&gt;#!/usr/bin/env perl&#xA;use Bio::DB::GenBank;&#xA;use Bio::DB::Query::GenBank;&#xA;use strict;&#xA;use warnings; &#xA;my $query = &quot;LEGK&quot;; &#xA;my $query_obj = Bio::DB::Query::GenBank-&amp;gt;new(-db =&amp;gt; 'protein', &#xA;                                          -query =&amp;gt; $query );&#xA;my $gb_obj = Bio::DB::GenBank-&amp;gt;new;&#xA;my $stream_obj = $gb_obj-&amp;gt;get_Stream_by_query($query_obj);&#xA;while (my $seq_obj = $stream_obj-&amp;gt;next_seq) {&#xA;	print &quot;&amp;gt;$query&quot;,' ', $seq_obj-&amp;gt;display_id, ' ', $seq_obj-&amp;gt;desc,&quot;\n&quot;, $seq_obj-&amp;gt;seq, &quot;\n&quot;;&#xA;    exit;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If instead, you want to only process the first result for a list of multiple queries:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env perl&#xA;use Bio::DB::GenBank;&#xA;use Bio::DB::Query::GenBank;&#xA;use strict;&#xA;use warnings; &#xA;my @queries = qw(LEGK TP53 ACT1); &#xA;query:foreach my $query (@queries) {&#xA;	my $query_obj = Bio::DB::Query::GenBank-&amp;gt;new(-db =&amp;gt; 'protein', &#xA;                                          -query =&amp;gt; $query );&#xA;	my $gb_obj = Bio::DB::GenBank-&amp;gt;new;&#xA;	my $stream_obj = $gb_obj-&amp;gt;get_Stream_by_query($query_obj);&#xA;	while (my $seq_obj = $stream_obj-&amp;gt;next_seq) {&#xA;		print &quot;&amp;gt;$query&quot;,' ', $seq_obj-&amp;gt;display_id, ' ', $seq_obj-&amp;gt;desc,&quot;\n&quot;, $seq_obj-&amp;gt;seq, &quot;\n&quot;;&#xA;        ## Move to the next query&#xA;        next query;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or, to print only the first sequence if multiple quries have identical sequences:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env perl&#xA;use Bio::DB::GenBank;&#xA;use Bio::DB::Query::GenBank;&#xA;use strict;&#xA;use warnings; &#xA;my @queries = qw(LEGK TP53 ACT1); &#xA;foreach my $query (@queries) {&#xA;	my %seen;&#xA;	my $query_obj = Bio::DB::Query::GenBank-&amp;gt;new(-db =&amp;gt; 'protein', &#xA;                                          -query =&amp;gt; $query );&#xA;	my $gb_obj = Bio::DB::GenBank-&amp;gt;new;&#xA;	my $stream_obj = $gb_obj-&amp;gt;get_Stream_by_query($query_obj);&#xA;	while (my $seq_obj = $stream_obj-&amp;gt;next_seq) {&#xA;		if ($seen{$seq_obj-&amp;gt;seq}) {&#xA;			## Here, since we're not using an argument (next query), this will move&#xA;			## to the next iteration of the current loop and not the `foreach`. &#xA;			next;&#xA;		}&#xA;		print &quot;&amp;gt;$query&quot;,' ', $seq_obj-&amp;gt;display_id, ' ', $seq_obj-&amp;gt;desc,&quot;\n&quot;, $seq_obj-&amp;gt;seq, &quot;\n&quot;;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-07-30T15:28:33.087" CommentCount="0" />
  <row Id="2183" PostTypeId="1" CreationDate="2017-07-31T06:45:17.843" Score="-2" ViewCount="42" Body="&lt;p&gt;I have the following Chip-Seq data and could not found a description of it. Can you help me with it? More, I would like to find out the order of the nucleotides in binding regions; that is, what number they are.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@GWZHISEQ01:319:HA1NPADXX:1:1101:1182:2196 1:N:0:ACTGAT&#xA;GGAGGAAGTGTGTTGCTGCAGACAGTAGCAACCAGACCCACACTGCGCGTA&#xA;+&#xA;CCCFFFFFHHFHHIJJJJIJJIIIIHGJJJJJJJCGHIJJIJJFHIIJJII&#xA;@GWZHISEQ01:319:HA1NPADXX:1:1101:1272:2109 1:N:0:ACTGAT&#xA;NCCACGCTAGGCTCAGCTTGTCGGCCTGGCTAAGCAGTTGCGAAAGTGCGC&#xA;+&#xA;#1=DDDFFHHHHHJJIJJJJJJIGEHGGJIGHJFHGJIJJJJJJJ@CHCBG&#xA;@GWZHISEQ01:319:HA1NPADXX:1:1101:1418:2141 1:N:0:ACTGAT&#xA;GCACGCACTACCCAGAGATCATCCAAAGCCTGAAGCCACAGGGCGCACTCG&#xA;+&#xA;CCCFFFFFGHGHHJJJIIJJJJIIHGGGIJIIJJIIIJJJIJJJIJIJIJH&#xA;@GWZHISEQ01:319:HA1NPADXX:1:1101:1302:2160 1:N:0:ACTGAT&#xA;ACACTCTTTCCCTACACGACGCTCTTCCGAGATCGGAAGAGCACACGTCTG&#xA;+&#xA;CCCFFFFFHDHHHJJJJIJJJJIIIIIJIHJIJJJIIJHIIJJJJJJJJHH&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1230" LastActivityDate="2017-08-04T10:20:55.950" Title="Chip-Seq data description" Tags="&lt;chip-seq&gt;" AnswerCount="1" CommentCount="4" ClosedDate="2017-07-31T08:31:28.373" />
  <row Id="2184" PostTypeId="2" ParentId="2183" CreationDate="2017-07-31T07:23:14.257" Score="4" Body="&lt;p&gt;That's a &lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format&quot; rel=&quot;nofollow noreferrer&quot;&gt;fastq&lt;/a&gt; file, you will want to align it to the genome, call peaks, and then use something like &lt;a href=&quot;http://meme-suite.org/index.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;MEME&lt;/a&gt; to determine binding motifs.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="292" LastEditDate="2017-08-04T10:20:55.950" LastActivityDate="2017-08-04T10:20:55.950" CommentCount="5" />
  <row Id="2185" PostTypeId="2" ParentId="2180" CreationDate="2017-07-31T09:09:58.577" Score="1" Body="&lt;p&gt;In cancer genomics experiments often involve sequencing a large number of tumour samples, but few or no matched normals. This is partly due to financial constraints, but also due to ethical ones: it is easy to get ethical approval for tumour tissue, which is removed as a matter of course in a patient's care anyway, whereas it is hard to get approval for normal tissue, which would not normally be removed and would require an additional procedure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, this presents a problem. Most of the procedures we use in standard gene expression analysis involve comparing one group to another after suitable normalisations of the data. But here we have no suitable comparison group. To get around this problem in cancer RNA-seq analysis we do something called outlier detection. That is, we assume that for a given gene, the majority of the patients have &quot;normal&quot; expression, and look for the few patients that don't look like the rest. We do this with the Z-score.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To calculate the Z-score for an observation we subtract the mean of all observations and divide by the standard deviation. Thus the Z score of an observation is how many standard deviations an observation is from the mean of all observations - or how unusual it is. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus for Gene A in Patient 1, we calculate how many standard deviations Gene A is from the mean of Gene A across all patients. A very big (or small/negative) value tells us that the expression of Gene A is unusual in patient 1 compared to the other patients. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;(NB: in some cancer studies they use the median and median absolute deviation rather than the mean and standard deviation to compute a &quot;robust Z-score&quot; as the data is unlikely to be normally distributed). &lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-07-31T09:09:58.577" CommentCount="1" />
  <row Id="2186" PostTypeId="1" AcceptedAnswerId="2187" CreationDate="2017-07-31T09:43:42.450" Score="4" ViewCount="111" Body="&lt;p&gt;I have a problem: I've managed to download a massive fasta file of 1500 sequences, but now I want to split them into seperate fasta files based on the genus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT&#xA;The fasta file looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;terminase_large.fasta&#xA;&amp;gt;YP_009300697.1 terminase large subunit [Arthrobacter phage Mudcat]&#xA;MGLSNTATPLYYGQF...&#xA;&#xA;&amp;gt;YP_009208724.1 hypothetical protein ADP65_00072 [Achromobacter phage phiAxp-3]&#xA;MSNVLLKQELDEWL...&#xA;&#xA;&amp;gt;YP_009148449.1 large terminase subunit [Delftia phage RG-2014]&#xA;MSEPRKLVKKTLD...&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would like to end up with something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mycobacterium_terminase.fasta&#xA;&amp;gt;Mycobacterium phage JAMaL&#xA;MVRKKPPPELE...&#xA;&#xA;&amp;gt;Mycobacterium phage Bruin&#xA;MEVCGYTLDDI...&#xA;&#xA;&amp;gt;Mycobacterium phage Zaka&#xA;MSLDNHLPELA...&#xA;&#xA;salmonella_terminase.fasta&#xA;&amp;gt;Salmonella virus SETP7&#xA;MNVDITATEPQ...&#xA;&#xA;&amp;gt;Salmonella virus SE2&#xA;MEGGSDSLIAM...&#xA;and so on...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So that the label on the alignment would have the name of the phage and not the protein's ID&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've mamanged to exctract the genus names of my organism with this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;outfile = open('species.txt', 'w')&#xA;with open('terminase_large.fasta') as fd:&#xA;    for line in fd:&#xA;        if line.startswith('&amp;gt;'):&#xA;            if '[' in line:&#xA;                name=line.split('[')[-1]&#xA;                name=name.split(' ', 1)[0]&#xA;                outfile.write(name[:] + &quot;\n&quot;)&#xA;outfile.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And got to extract only the unique names with this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;lines_seen = set()&#xA;outfile = open('species2.txt', &quot;w&quot;)&#xA;for line in open(&quot;species.txt&quot;, &quot;r&quot;):&#xA;    if line not in lines_seen:  # not a duplicate&#xA;        outfile.write(line)&#xA;        lines_seen.add(line)&#xA;outfile.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(Can I merge those two scripts together?)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, my genus names look like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Arthrobacter&#xA;Achromobacter&#xA;Delftia&#xA;....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried automating my script to get the Entrez data, but it gives me the 'Supplied id parameter is empty' message&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from Bio import Entrez&#xA;Entrez.email = &quot;example@example.org&quot;&#xA;&#xA;for line in open(&quot;species2.txt&quot;, &quot;r&quot;):&#xA;    searchterm = &quot;(terminase large subunit AND viruses[Organism]) AND&quot; +line+ &quot;AND refseq[Filter]&quot;&#xA;    searchResultHandle = Entrez.esearch(db=&quot;protein&quot;, term=searchterm, retmax=1000)&#xA;    searchResult = Entrez.read(searchResultHandle)&#xA;    ids = searchResult[&quot;IdList&quot;]&#xA;&#xA;    handle = Entrez.efetch(db=&quot;protein&quot;, id=ids, rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xA;    record = handle.read()&#xA;&#xA;    out_handle = open('terminase_large_'+str(line[:-1])+'.fasta', 'w')&#xA;    out_handle.write(record.rstrip('\n'))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Can someone help me with it?&lt;/p&gt;&#xA;" OwnerUserId="1193" LastEditorUserId="73" LastEditDate="2017-08-01T05:57:48.453" LastActivityDate="2017-08-11T13:19:48.687" Title="Subset FASTA file by species name" Tags="&lt;fasta&gt;&lt;python&gt;&lt;biopython&gt;" AnswerCount="2" CommentCount="10" FavoriteCount="2" />
  <row Id="2187" PostTypeId="2" ParentId="2186" CreationDate="2017-07-31T10:35:37.210" Score="6" Body="&lt;p&gt;Splitting into multiple files and changing the IDs can be easily done:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;perl -pe 'if(/&amp;gt;/){/\[(.*?)\]\s*$/; $_=&quot;&amp;gt; $1\n&quot;}' file.fa | &#xA;    awk '(/^&amp;gt;/){name=$2} {print &amp;gt;&amp;gt; name&quot;.fa&quot;}'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That assumes all your FASTA headers have &lt;code&gt;[foo bar baz]&lt;/code&gt; as the last element of a line. It will create a file called &lt;code&gt;foo.fa&lt;/code&gt; (the bacterium's name) with all sequences saved there. &lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Explanation&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;perl -pe&lt;/code&gt; : run the script given by &lt;code&gt;-e&lt;/code&gt; on each line of the input file, and print the resulting line. &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;if(/&amp;gt;/)&lt;/code&gt; : if this line starts with a &lt;code&gt;&amp;gt;&lt;/code&gt;. \&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;/\[(.*?)\]\s*$/&lt;/code&gt; : match an opening bracket (&lt;code&gt;\[&lt;/code&gt;), then capture (that's what the parentheses do, they capture a pattern so we can refer to it as &lt;code&gt;$1&lt;/code&gt;) everything until the first &lt;code&gt;]&lt;/code&gt; (&lt;code&gt;.*?\]&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;$_=&quot;&amp;gt; $1\n&quot;&lt;/code&gt; : the &lt;code&gt;$_&lt;/code&gt; special variable in Perl is (in this case) the current line. So, &lt;code&gt;$_=foo&lt;/code&gt; means &quot;make the current line read &lt;code&gt;foo&lt;/code&gt;. Since the &lt;code&gt;-p&lt;/code&gt; prints each input line, changing the value of &lt;code&gt;$_&lt;/code&gt; means the changed value will be printed. So here, we are printing &lt;code&gt;&amp;gt;&lt;/code&gt;, whatever was in the square brackets (&lt;code&gt;$1&lt;/code&gt;) and a newline character. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The output of the perl command alone on your example input file is:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;$ perl -pe 'if(/&amp;gt;/){/\[(.*)\]\s*$/; $_=&quot;&amp;gt; $1\n&quot;}' file.fa&#xA;&amp;gt; Arthrobacter phage Mudcat&#xA;MGLSNTATPLYYGQF...&#xA;&#xA;&amp;gt; Achromobacter phage phiAxp-3&#xA;MSNVLLKQELDEWL...&#xA;&#xA;&amp;gt; Delftia phage RG-2014&#xA;MSEPRKLVKKTLD...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, we now pass it through &lt;code&gt;awk&lt;/code&gt; which does:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;(/^&amp;gt;/){a=$2}&lt;/code&gt; : if this line starts with an &lt;code&gt;&amp;gt;&lt;/code&gt;, save the second field (the bacterial species) as the variable &lt;code&gt;name&lt;/code&gt;. &lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;{print &amp;gt;&amp;gt; name&quot;.fa&quot;}&lt;/code&gt; : print each line into a file whose name is the current value of the variable &lt;code&gt;name&lt;/code&gt; with a &lt;code&gt;.fa.&lt;/code&gt; extension. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;If you prefer python scripts to the one-liner approach, you can do the same thing with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python&#xA;import re&#xA;outFile = None&#xA;with open(&quot;file.fa&quot;, &quot;r&quot;) as inFile, open(species + &quot;.fa&quot;, 'a') as outFile:&#xA;    for line in inFile:&#xA;        line = line.rstrip()&#xA;        if line.startswith('&amp;gt;'):&#xA;            regex = re.compile('.*\[((.+?)\s+.*?)\].*')&#xA;            matches = regex.search(line)&#xA;            species = matches[2]&#xA;            outFile.write('&amp;gt;%s\n' % matches[1])&#xA;        else:&#xA;            outFile.write(&quot;%s\n&quot; % line)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;As for your script, you've got the right idea, but have a small bug. You forgot to remove the &lt;code&gt;\n&lt;/code&gt; from your input file, so it looks for &lt;code&gt;Arthrobacter\n&lt;/code&gt; instead of &lt;code&gt;Arthrobacter&lt;/code&gt;. The golden rule of debugging is &quot;print all the things&quot;. If you add &lt;code&gt;print(&quot;Searchterm: &quot;,searchterm)&lt;/code&gt;  you will see:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Searchterm:  (terminase large subunit AND viruses[Organism]) ANDArthrobacter  &#xA;AND refseq[Filter]&#xA;Searchterm:  (terminase large subunit AND viruses[Organism]) ANDAchromobacter  &#xA;AND refseq[Filter]&#xA;Searchterm:  (terminase large subunit AND viruses[Organism]) ANDDelftia  &#xA;AND refseq[Filter]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, you need to remove the newline characters and add a space like so (I also made it a bit more &quot;pythonic&quot; and conforming to the Python syntax guidelines):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python&#xA;from Bio import Entrez&#xA;Entrez.email = &quot;example@example.org&quot;&#xA;&#xA;with open(&quot;species2.txt&quot;, &quot;r&quot;) as in_handle:&#xA;    for line in in_handle:&#xA;        line = line.rstrip()&#xA;        searchterm = (&quot;(terminase large subunit AND viruses[Organism]) &quot; +&#xA;                      &quot;AND %s AND refseq[Filter]&quot; % line)&#xA;        print(&quot;Searchterm: &quot;, searchterm)&#xA;        searchResultHandle = Entrez.esearch(db=&quot;protein&quot;,&#xA;                                            term=searchterm, retmax=1000)&#xA;        searchResult = Entrez.read(searchResultHandle)&#xA;        ids = searchResult[&quot;IdList&quot;]&#xA;&#xA;        handle = Entrez.efetch(db=&quot;protein&quot;, id=ids,&#xA;                               rettype=&quot;fasta&quot;, retmode=&quot;text&quot;)&#xA;        record = handle.read()&#xA;&#xA;        with open('terminase_large_' + str(line[:-1]) + '.fasta', 'w') as out_handle:&#xA;            out_handle.write(record.rstrip('\n'))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastEditorUserId="450" LastEditDate="2017-08-11T13:19:48.687" LastActivityDate="2017-08-11T13:19:48.687" CommentCount="5" />
  <row Id="2188" PostTypeId="2" ParentId="951" CreationDate="2017-07-31T21:12:27.573" Score="1" Body="&lt;p&gt;I also think pooling is the better option, followed by partitioning by coverage/taxonomy of contigs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Maybe check out &lt;a href=&quot;https://github.com/DRL/blobtools&quot; rel=&quot;nofollow noreferrer&quot;&gt;BlobTools&lt;/a&gt;, which helps with filtering read-pairs by taxonomy of contigs they contribute to and also does nice visualisations of assemblies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://blobtools.readme.io/docs/the-blobtools-workflows&quot; rel=&quot;nofollow noreferrer&quot;&gt;Workflow B&lt;/a&gt; seems to be what you want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Disclaimer: I am the developer of this tool.&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="1234" LastActivityDate="2017-07-31T21:12:27.573" CommentCount="0" />
  <row Id="2189" PostTypeId="5" CreationDate="2017-08-01T10:33:20.660" Score="0" Body="&lt;p&gt;RNA-seq (RNA sequencing) is a widely used approach to study gene expression. Questions should include this tag if they pertain to issues related to bioinformatics analysis of RNA-seq data. &lt;/p&gt;&#xA;" OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-01T10:47:49.007" LastActivityDate="2017-08-01T10:47:49.007" CommentCount="0" />
  <row Id="2190" PostTypeId="4" CreationDate="2017-08-01T10:33:20.660" Score="0" Body="RNA-seq (RNA sequencing) is a widely used approach to study gene expression. Questions should include this tag if they pertain to issues related to bioinformatics analysis of RNA-seq data. " OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-01T10:47:55.073" LastActivityDate="2017-08-01T10:47:55.073" CommentCount="0" />
  <row Id="2192" PostTypeId="5" CreationDate="2017-08-01T13:53:26.140" Score="0" Body="&lt;p&gt;bwa (Burrows-Wheeler Aligner) is a software for aligning reads obtained with Next Generation Sequencing. Questions related to the functionalities of bwa should include this tag. Please consider specifying if you are using bwa aln or bwa mem. Fro more info: &lt;a href=&quot;http://bio-bwa.sourceforge.net/bwa.shtml&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://bio-bwa.sourceforge.net/bwa.shtml&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-01T14:02:58.737" LastActivityDate="2017-08-01T14:02:58.737" CommentCount="0" />
  <row Id="2193" PostTypeId="4" CreationDate="2017-08-01T13:53:26.140" Score="0" Body="bwa (Burrows-Wheeler Aligner) is a software for aligning reads obtained with Next Generation Sequencing. " OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-01T14:02:54.490" LastActivityDate="2017-08-01T14:02:54.490" CommentCount="0" />
  <row Id="2194" PostTypeId="1" CreationDate="2017-08-01T16:24:34.650" Score="9" ViewCount="81" Body="&lt;p&gt;I’ve got an RNA-seq dataset with a large proportion of environmental RNA “contamination”. BLASTing random reads reveals that much of the data comes from bacterial, plant and viral RNA. My target organism only accounts for ~5% of the RNA-seq read data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to get a more or less (more, if possible) comprehensive overview of what species are found in my sample — bacteria, plants, animals (?), but also viruses. How can I perform this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been unable to find a “standard” way of performing a metagenome screen. BLAST online services all seem to be severely rate limited (certainly unable to handle uploading ~80 M reads). Installing BLAST (or a similar tool) locally is of course not an issue, but I can’t find a comprehensive database that spans all phyla — the best I’ve been able to find are databases, such as &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/refseq/about/nonredundantproteins/&quot; rel=&quot;noreferrer&quot;&gt;NCBI-NR&lt;/a&gt;, that are restricted to single phyla or classes, or to bacteria.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ideally I’d like a workflow that I feed with my RNA-seq data, and get out a list of species with % of reads mapped.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-08-03T21:38:09.977" Title="How do I efficiently perform a metagenome screen of “all” species?" Tags="&lt;rna-seq&gt;&lt;metagenome&gt;" AnswerCount="4" CommentCount="3" FavoriteCount="1" />
  <row Id="2195" PostTypeId="2" ParentId="2194" CreationDate="2017-08-01T17:36:43.390" Score="3" Body="&lt;p&gt;I think you're just looking for &lt;code&gt;nr&lt;/code&gt;. It is absolutely not limited to prokaryotes, far from it. It is, according to the blurb on the NCBI's blast page:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The nucleotide collection consists of GenBank+EMBL+DDBJ+PDB+RefSeq sequences, but excludes EST, STS, GSS, WGS, TSA, patent sequences as well as phase 0, 1, and 2 HTGS sequences. The database is non-redundant. Identical sequences have been merged into one entry, while preserving the accession, GI, title and taxonomy information for each entry.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The link you gave in your question doesn't describe the &lt;code&gt;nr&lt;/code&gt; database, but the collection of RefSeq non-redundant sequences. The two are not the same thing at all. The nr databases contains sequences from all available species so it should be perfect for what you're after. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, I admit I haven't tried this with millions of redas, but I did used to run many blast searches with several thousand sequences using the command line blast tools and the remote nr database. I just checked the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/books/NBK279690/&quot; rel=&quot;nofollow noreferrer&quot;&gt;blast+ docs&lt;/a&gt; now and see no mention of a rate limit. Which isn't to say there isn't one, for sure, but since each query is run sequentially when you give a multifasta file, there seems to reason to limit since the servers should be able to queue the jobs as needed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, don't use the API or roll your own, just &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE_TYPE=BlastDocs&amp;amp;DOC_TYPE=Download&quot; rel=&quot;nofollow noreferrer&quot;&gt;download NCBI's blast client&lt;/a&gt; and try that:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;blastn -query query.fa -db nr -task blastn &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If that complains, you'll have to download the &lt;code&gt;nr&lt;/code&gt; DB (well, since you want nucleotide blast, you probably want its nucleotide cousin, &lt;code&gt;nt&lt;/code&gt;) which you can find here: &lt;a href=&quot;ftp://ftp.ncbi.nlm.nih.gov/blast/db/&quot; rel=&quot;nofollow noreferrer&quot;&gt;ftp://ftp.ncbi.nlm.nih.gov/blast/db/&lt;/a&gt;. Just get all the &lt;code&gt;foo.nt*&lt;/code&gt; files.&lt;/p&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-08-01T17:36:43.390" CommentCount="2" />
  <row Id="2196" PostTypeId="2" ParentId="2194" CreationDate="2017-08-01T21:54:51.507" Score="6" Body="&lt;p&gt;As you point out, BLAST is not going to work well for reads from a typical RNASeq experiment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From an RNASeq dataset, you might be able to get away with just a ribosomal screen using &lt;a href=&quot;https://www.arb-silva.de/projects/ssu-ref-nr/&quot; rel=&quot;noreferrer&quot;&gt;Silva&lt;/a&gt;, because the ribosome is the most abundantly transcribed gene. This will usually be the case even if polyA selection or ribosomal depletion was carried out on a sample.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, bear in mind that there will be a lot of false results due to database incompleteness. I used &lt;a href=&quot;http://ccb.jhu.edu/software/kraken/&quot; rel=&quot;noreferrer&quot;&gt;Kraken&lt;/a&gt; when I did something like this a few years ago, and ended up with one sequence annotated as a fruit bat (where my subject was a water-living planarian flatworm). &lt;a href=&quot;https://github.com/infphilo/centrifuge/&quot; rel=&quot;noreferrer&quot;&gt;Centrifuge&lt;/a&gt; would be a better option now; the authors even claim that &lt;a href=&quot;http://genome.cshlp.org/content/26/12/1721.full&quot; rel=&quot;noreferrer&quot;&gt;they can store the entirety of nr in a 70 GB index&lt;/a&gt; with Centrifuge (which would be better / more comprehensive than just a ribosomal screen).&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-08-01T21:54:51.507" CommentCount="0" />
  <row Id="2197" PostTypeId="2" ParentId="52" CreationDate="2017-08-01T22:16:10.137" Score="2" Body="&lt;p&gt;We have added ERCC spike-ins to all our RNASeq data, just in case other people might find it useful in the future. However, I have never used it in my own analyses because I can't think of a reasonable way that it could be used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The typical recommendation for ERCC is to add it in proportion to the input RNA amount, but that makes an assumption that total cell RNA counts are similar across different cells (which is demonstrably false by looking at single cell RNASeq results).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have yet to think of a situation where ERCC would provide better results than a &quot;housekeeping&quot; gene set sampled from the original reads.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-08-01T22:16:10.137" CommentCount="5" />
  <row Id="2198" PostTypeId="2" ParentId="2174" CreationDate="2017-08-01T22:25:13.990" Score="2" Body="&lt;p&gt;Albacore produces a &lt;code&gt;sequencing_summary.txt&lt;/code&gt; file (actually TSV, not CSV) in the same directory as the &lt;code&gt;workspace&lt;/code&gt; folder that might have the data that you want in it. Here are the fields present in that file:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;filename&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;read_id&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;run_id&lt;/li&gt;&#xA;&lt;li&gt;channel&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;start_time&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;duration&lt;/li&gt;&#xA;&lt;li&gt;num_events&lt;/li&gt;&#xA;&lt;li&gt;template_start&lt;/li&gt;&#xA;&lt;li&gt;num_events_template&lt;/li&gt;&#xA;&lt;li&gt;template_duration&lt;/li&gt;&#xA;&lt;li&gt;num_called_template&lt;/li&gt;&#xA;&lt;li&gt;sequence_length_template&lt;/li&gt;&#xA;&lt;li&gt;mean_qscore_template&lt;/li&gt;&#xA;&lt;li&gt;strand_score_template&lt;/li&gt;&#xA;&lt;li&gt;complement_start&lt;/li&gt;&#xA;&lt;li&gt;num_events_complement&lt;/li&gt;&#xA;&lt;li&gt;complement_duration&lt;/li&gt;&#xA;&lt;li&gt;num_called_complement&lt;/li&gt;&#xA;&lt;li&gt;sequence_length_complement&lt;/li&gt;&#xA;&lt;li&gt;mean_qscore_complement&lt;/li&gt;&#xA;&lt;li&gt;strand_score_complement&lt;/li&gt;&#xA;&lt;li&gt;sequence_length_2d&lt;/li&gt;&#xA;&lt;li&gt;mean_qscore_2d&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-08-01T22:25:13.990" CommentCount="1" />
  <row Id="2199" PostTypeId="1" CreationDate="2017-08-02T05:27:40.667" Score="1" ViewCount="46" Body="&lt;p&gt;I have a question regarding the RNA-Seq analysis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We have a time course data regarding a mouse model wt and mutant treated with a drug (10uM) and the taken down at different time points: 8 time points and did RNA-seq on these samples currently we have tpm values and raw gene counts from rna seq. Also we have only one sample for each time point. we would like to see for changes in genes across different time point but not sure statistical method should we use to determine this, should we consider for example a single time point wt vs mt, or see trends across wt and mt individually &#xA;I know that there are some bioc packages for NGS analysis (edgeR,&#xA;DESeq,etc). Is any package or method good for one-sample-per-time-point&#xA;data?&lt;/p&gt;&#xA;" OwnerUserId="315" LastEditorUserId="57" LastEditDate="2017-08-02T08:56:11.040" LastActivityDate="2017-08-02T08:56:11.040" Title="ngs time course experiment" Tags="&lt;rna-seq&gt;&lt;software-recommendation&gt;&lt;statistics&gt;" AnswerCount="0" CommentCount="3" ClosedDate="2017-08-02T11:48:59.337" />
  <row Id="2200" PostTypeId="1" CreationDate="2017-08-02T17:24:24.340" Score="2" ViewCount="34" Body="&lt;p&gt;I have a PDB file, and I need to extract its residue sequence numbers (resSeq's). Based on manual inspection of the first few lines of the PDB file (pasted below), I would think that resSeq's should begin with 22, 23. However, Biopython's PDB module suggests otherwise (output attached below as well). I wonder if it's a Biopython bug or if I have problems understanding the PDB format. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ATOM      1  N   GLY A  22      78.171  89.858  59.231  1.00 21.24           N  &#xA;ATOM      2  CA  GLY A  22      79.174  88.827  58.999  1.00 20.87           C  &#xA;ATOM      3  C   GLY A  22      80.438  89.415  58.391  1.00 21.89           C  &#xA;ATOM      4  O   GLY A  22      80.362  90.202  57.440  1.00 23.18           O  &#xA;ATOM      5  N   LEU A  23      81.588  89.069  58.972  1.00 21.51           N  &#xA;ATOM      6  CA  LEU A  23      82.895  89.555  58.527  1.00 20.80           C  &#xA;ATOM      7  C   LEU A  23      83.288  89.020  57.162  1.00 22.41           C  &#xA;ATOM      8  O   LEU A  23      82.889  87.923  56.788  1.00 22.93           O  &#xA;ATOM      9  CB  LEU A  23      83.973  89.232  59.560  1.00 20.97           C  &#xA;ATOM     10  CG  LEU A  23      84.225  87.818  60.062  1.00 13.32           C  &#xA;ATOM     11  CD1 LEU A  23      85.448  87.888  60.939  1.00 15.24           C  &#xA;ATOM     12  CD2 LEU A  23      83.035  87.258  60.829  1.00 12.21           C  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;for i in chain:&#xA;    print i.get_full_id()&#xA;&#xA;OUT:('pdb', 0, 'A', (' ', 2, ' '))&#xA;    ('pdb', 0, 'A', (' ', 3, ' '))&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1249" OwnerDisplayName="Alex Mayorov" LastEditorUserId="298" LastEditDate="2017-08-03T08:28:43.720" LastActivityDate="2017-08-03T13:45:40.647" Title="Biopython: resseq doesn't match pdb file" Tags="&lt;pdb&gt;&lt;biopython&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2201" PostTypeId="1" CreationDate="2017-08-02T20:14:06.153" Score="0" ViewCount="82" Body="&lt;p&gt;This is a cross-post from &lt;a href=&quot;https://math.stackexchange.com/q/2380215&quot;&gt;Mathematics forum&lt;/a&gt;. As no one has answered it yet, I fIgured post here as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to describe a transform I used to rank my data points. I have recoded my variable columns from $10$ measurements with different range to $n$ (for each measurement) where $n \in \{0,0.5,1\}$.    &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For columns where measurements were expressed as categorical values I simply recoded the categorical value to either $0$, $0.5$ or $1$ and for continuous variables I recoded the lower quartile as $0$, interquartile as $0.5$ and upper quartile as $1$ and finally I summed the recoded values to produce a single score $\sum n_{i1,..i10} $ for each row. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to write my method and I'd like to have it in mathematical notation. I was wondering if anyone could help me with this!  &lt;/p&gt;&#xA;" OwnerUserId="1250" LastEditorUserId="77" LastEditDate="2017-08-18T13:29:16.580" LastActivityDate="2017-08-18T17:39:08.600" Title="Mathematical notation for formulating a rank score" Tags="&lt;modelling&gt;" AnswerCount="1" CommentCount="11" FavoriteCount="0" />
  <row Id="2202" PostTypeId="1" AcceptedAnswerId="2211" CreationDate="2017-08-02T21:03:29.337" Score="3" ViewCount="21" Body="&lt;p&gt;I am interested in identifying indels in whole genome bisulfite sequencing data (76bp paired end). Currently, I do this by setting the &lt;code&gt;-rfg&lt;/code&gt; and &lt;code&gt;-rdg&lt;/code&gt; affine gap penalty scores for bowtie2 to more permissive values than the default 5+3N and mapping using the bisulfite sequencing alignment wrapper, bismark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is, what values of &lt;code&gt;-rfg&lt;/code&gt; and &lt;code&gt;-rdg&lt;/code&gt; will allow me to identify longest possible indels without sacrificing alignment quality? Is it better to set the affine penalty to zero with a high penalty for initially opening an indel (ex. 8+0N)? Or is it better to keep the initial penalty low and having a nonzero penalty for extension (ex. 2+1N)?&lt;/p&gt;&#xA;" OwnerUserId="643" LastEditorUserId="643" LastEditDate="2017-08-04T03:39:26.213" LastActivityDate="2017-08-04T03:39:26.213" Title="Best way to detect long insertions in bisulfite sequencing data?" Tags="&lt;ngs&gt;&lt;alignment&gt;&lt;read-mapping&gt;&lt;methylation&gt;&lt;indel&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="2203" PostTypeId="1" CreationDate="2017-08-03T08:54:48.673" Score="3" ViewCount="57" Body="&lt;p&gt;I want to incorporate in our Shiny app. the possibility to select different nucleotide sequences and do a multiple sequence alignment in real-time. The user would define as input different nt. sequences and then a tree will pop up in the app.. Do you think it is feasible? Any recommendations and feedback are really appreciated :)&lt;/p&gt;&#xA;" OwnerUserId="446" LastEditorUserId="446" LastEditDate="2017-08-03T09:30:25.013" LastActivityDate="2017-08-03T09:31:47.057" Title="Multiple sequence alignment with R" Tags="&lt;r&gt;&lt;sequence-alignment&gt;" AnswerCount="1" CommentCount="7" />
  <row Id="2204" PostTypeId="1" AcceptedAnswerId="2262" CreationDate="2017-08-03T09:29:03.723" Score="3" ViewCount="31" Body="&lt;p&gt;Is the &lt;code&gt;MUMmer&lt;/code&gt; suite capable of calculating reference sequence coverage statistics for all query sequences collectively? It would be possible to achieve by parsing the output of &lt;code&gt;nucmer&lt;/code&gt; / &lt;code&gt;show-coords&lt;/code&gt; / &lt;code&gt;show-tiling&lt;/code&gt; but it seems like there should be a better way. I currently do this using a sensitive read mapper, &lt;code&gt;samtools depth&lt;/code&gt; and some scripting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To clarify, I'd like to know the reference coverage achieved using all of the query sequences (i.e. the whole &lt;em&gt;de novo&lt;/em&gt; assembly, in my case).&lt;/p&gt;&#xA;" OwnerUserId="1258" LastEditorUserId="1258" LastEditDate="2017-08-03T12:23:46.210" LastActivityDate="2017-08-10T13:14:13.037" Title="How to calculate overall reference coverage with MUMmer?" Tags="&lt;assembly&gt;&lt;coverage&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2206" PostTypeId="2" ParentId="2203" CreationDate="2017-08-03T09:31:47.057" Score="2" Body="&lt;p&gt;The &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/msa.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;‹msa› package on Bioconductor&lt;/a&gt; does exactly that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It doesn’t hand the results on a silver platter, though: you’ll need to &lt;a href=&quot;https://bioconductor.org/packages/devel/bioc/vignettes/msa/inst/doc/msa.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;read the vignette&lt;/a&gt; carefully to learn how to use it. But after that it’s pretty powerful.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-08-03T09:31:47.057" CommentCount="1" />
  <row Id="2207" PostTypeId="2" ParentId="2200" CreationDate="2017-08-03T13:45:40.647" Score="2" Body="&lt;p&gt;Let's check it. I copied your file fragment into &lt;code&gt;a.pdb&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; from Bio import PDB&#xA;&amp;gt;&amp;gt;&amp;gt; for res in PDB.PDBParser().get_structure('pdb', 'a.pdb')[0].get_residues():&#xA;...   print(res.get_full_id())&#xA;... &#xA;('pdb', 0, 'A', (' ', 22, ' '))&#xA;('pdb', 0, 'A', (' ', 23, ' '))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So BioPython (ver. 1.66) gives 22 and 23 as expected.&lt;/p&gt;&#xA;" OwnerUserId="266" LastActivityDate="2017-08-03T13:45:40.647" CommentCount="1" />
  <row Id="2208" PostTypeId="5" CreationDate="2017-08-03T14:10:09.633" Score="0" Body="&lt;p&gt;python (&lt;a href=&quot;https://www.python.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.python.org/&lt;/a&gt;) is a programming language, widely used in bioinformatics. Use this tag for questions about programming in python.&lt;/p&gt;&#xA;" OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-03T18:11:21.103" LastActivityDate="2017-08-03T18:11:21.103" CommentCount="0" />
  <row Id="2209" PostTypeId="4" CreationDate="2017-08-03T14:10:09.633" Score="0" Body="python is a programming language, widely used in bioinformatics" OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-03T18:11:15.430" LastActivityDate="2017-08-03T18:11:15.430" CommentCount="0" />
  <row Id="2210" PostTypeId="2" ParentId="658" CreationDate="2017-08-03T15:23:45.557" Score="1" Body="&lt;p&gt;DNAnexus -- &lt;a href=&quot;http://dnanexus.com&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://dnanexus.com&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BaseSpace -- &lt;a href=&quot;http://basespace.illumina.com&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://basespace.illumina.com&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Seven Bridges Genomics -- &lt;a href=&quot;http://www.sbgenomics.com&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.sbgenomics.com&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Curoverse &lt;a href=&quot;http://curoverse.com&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://curoverse.com&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;InsideDNA &lt;a href=&quot;http://insidedna.me/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://insidedna.me/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="180" LastActivityDate="2017-08-03T15:23:45.557" CommentCount="0" />
  <row Id="2211" PostTypeId="2" ParentId="2202" CreationDate="2017-08-03T18:30:18.183" Score="3" Body="&lt;p&gt;You'd be best off by starting with &lt;code&gt;-rfg&lt;/code&gt; and &lt;code&gt;-rdg&lt;/code&gt; as is and reverting bismark's change of &lt;code&gt;--score-min&lt;/code&gt; back to the default for bowtie2. That alone will allow for much longer indels. If that still doesn't suffice, then I'd play around more with &lt;code&gt;--score-min&lt;/code&gt; before messing with the gap open/extend penalties. If you do need to play with those, then increase the gap open penalty and decrease the gap extension to 1. Do check that the resulting alignments aren't nonsense though!&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-03T18:30:18.183" CommentCount="1" />
  <row Id="2212" PostTypeId="2" ParentId="2194" CreationDate="2017-08-03T18:55:25.983" Score="1" Body="&lt;p&gt;I have had good success using the MEGAN software to detect and visualize contamination.  My input data was WGS not RNA-seq But I would check it out.  It can create a very nice weight phylogenetic tree for all the detected organisms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://ab.inf.uni-tuebingen.de/software/megan6&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://ab.inf.uni-tuebingen.de/software/megan6&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/ttT6l.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ttT6l.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="964" LastActivityDate="2017-08-03T18:55:25.983" CommentCount="1" />
  <row Id="2213" PostTypeId="2" ParentId="2194" CreationDate="2017-08-03T21:38:09.977" Score="1" Body="&lt;p&gt;You might want to look into &lt;a href=&quot;https://en.wikipedia.org/wiki/MinHash&quot; rel=&quot;nofollow noreferrer&quot;&gt;minihashes&lt;/a&gt; as a more efficient way of clustering and comparing your sequences to known species and samples. &lt;a href=&quot;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0997-x&quot; rel=&quot;nofollow noreferrer&quot;&gt;In the mash paper&lt;/a&gt; the authors show how it would be feasible to create minihash sketches that you could search against from large metagenomics datasets (check the &quot;Clustering massive metagenomic datasets&quot; section in the results):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For a large-scale test, samples from the Human Microbiome Project [36]&#xA;  (HMP) and Metagenomics of the Human Intestinal Tract [37] (MetaHIT)&#xA;  were combined to create a ~10 TB 888-sample dataset. Importantly, the&#xA;  size of a Mash sketch is independent of the input size, requiring only&#xA;  70 MB to store the combined sketches (s = 10,000, k = 21) for these&#xA;  datasets.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Moreover, the creators of &lt;a href=&quot;https://sourmash.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;sourmash&lt;/a&gt; (a tool using the same approach) provide &lt;a href=&quot;https://sourmash.readthedocs.io/en/latest/databases.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;precomputed sketches for the whole of NCBI's RefSeq database&lt;/a&gt; (includes bacterial, viral and fungal genomes). Creating a similar sketch for the whole of ENA/SRA (or a subset of it) could require some time but would allow for very fast and accurate searches.&lt;/p&gt;&#xA;" OwnerUserId="123" LastActivityDate="2017-08-03T21:38:09.977" CommentCount="0" />
  <row Id="2215" PostTypeId="1" AcceptedAnswerId="2218" CreationDate="2017-08-04T13:17:53.547" Score="5" ViewCount="43" Body="&lt;p&gt;I have a set of RNA-seq samples from different experiments (Single and Paired End, depending on the experiment). I ran &lt;a href=&quot;https://www.bioinformatics.babraham.ac.uk/projects/fastqc/&quot; rel=&quot;noreferrer&quot;&gt;FASTQC&lt;/a&gt; in all the samples and found overrepresented adapter sequences:&lt;a href=&quot;https://i.stack.imgur.com/RlZlZ.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/RlZlZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I removed the adaptors (TruSeq adaptors) using &lt;a href=&quot;http://cutadapt.readthedocs.io/en/stable/guide.html&quot; rel=&quot;noreferrer&quot;&gt;Cutadapt&lt;/a&gt; (in addition, I removed low quality and N bases from the 3' end of the reads). After that, I ran again FASTQC and the output is the following (representative example) :&#xA;&lt;a href=&quot;https://i.stack.imgur.com/tpgVR.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/tpgVR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know what is happening? Now I have an overrepresented sequence for which no sequence is provided. What does this mean? &lt;/p&gt;&#xA;" OwnerUserId="678" LastActivityDate="2017-08-04T17:36:59.603" Title="FASTQC overrepresented sequences after trimming" Tags="&lt;ngs&gt;&lt;fastq&gt;&lt;trimming&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="0" />
  <row Id="2216" PostTypeId="1" CreationDate="2017-08-04T15:10:40.400" Score="18" ViewCount="1133" Body="&lt;p&gt;A bit of a historical question on a number, 30 times coverage, that's become so familiar in the field: why do we sequence the human genome at 30x coverage?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question has two parts:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Who came up with the 30x value and why?&lt;/li&gt;&#xA;&lt;li&gt;Does the value need to be updated to reflect today's state-of-the-art?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In summary, if the 30x value is a number that was based on the old Solexa GAIIx 2x35bp reads and error rates, and the current standard Illumina sequencing is 2x150bp, does the 30x value need updating?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/GGkq0.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/GGkq0.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="180" LastEditorUserId="180" LastEditDate="2017-08-10T10:08:24.777" LastActivityDate="2017-08-10T18:12:40.087" Title="Why sequence the human genome at 30x coverage?" Tags="&lt;genome&gt;&lt;sequencing&gt;&lt;dna&gt;&lt;next-generation&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="3" />
  <row Id="2217" PostTypeId="2" ParentId="2216" CreationDate="2017-08-04T15:46:51.593" Score="21" Body="&lt;p&gt;The earliest mention of the 30x paradigm I could find is in the original Illumina whole-genome sequencing paper: &lt;a href=&quot;https://www.nature.com/nature/journal/v456/n7218/full/nature07517.html&quot; rel=&quot;noreferrer&quot;&gt;Bentley, 2008&lt;/a&gt;. Specifically, in Figure 5, they show that most SNPs have been found, and that there are few uncovered/uncalled bases by the time you reach 30x: &lt;a href=&quot;https://i.stack.imgur.com/42eHQ.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/42eHQ.png&quot; alt=&quot;30xSequencingDepth&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These days, 30x is still a common standard, but large-scale germline sequencing projects are often pushing down closer to 25x and finding it adequate. Every group doing this seriously has done power calculations based on specifics of their machines and prep (things like error rates and read lengths matter!).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cancer genomics is going in the other direction. When you have to contend with purity, ploidy, and subclonal populations, much more coverage than 30x is needed. Our group showed in &lt;a href=&quot;http://www.cell.com/cell-systems/abstract/S2405-4712(15)00113-1&quot; rel=&quot;noreferrer&quot;&gt;this 2015 paper&lt;/a&gt; that even 300x whole-genome coverage of a tumor was likely missing real rare variants in a tumor. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the whole, the sequence coverage you need really depends on what questions you're asking, and I'd recommend that anyone designing a sequencing experiment consult with both a sequencing expert and a statistician beforehand (and it's even better if those are the same person!)&lt;/p&gt;&#xA;" OwnerUserId="74" LastActivityDate="2017-08-04T15:46:51.593" CommentCount="2" />
  <row Id="2218" PostTypeId="2" ParentId="2215" CreationDate="2017-08-04T17:36:59.603" Score="5" Body="&lt;p&gt;As @AaronBerlin mentioned, you didn't remove reads that were completely trimmed. Next time use the &lt;code&gt;--minimum-length&lt;/code&gt; option and set it to something reasonable, like 20. Alternatively, use &quot;Trim Galore!&quot;, which is a wrapper around cutadapt that has more reasonable defaults.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-04T17:36:59.603" CommentCount="1" />
  <row Id="2219" PostTypeId="2" ParentId="97" CreationDate="2017-08-04T19:08:28.877" Score="0" Body="&lt;p&gt;&lt;a href=&quot;https://github.com/bede/kindel&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kindel&lt;/a&gt; can infer consensus from low quality alignments of short reads to viral references, and extending it to work with single molecule reads and larger genomes is on my to-do list, though I imagine this will require some redesign. Presumably you're dealing with a bacterial or fungal genome in this case ? I do also have a basic C++ version, but it's a long way away from being user friendly. Anyway, it may be worth a look – feel free to get in touch with any issues you encounter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/bede/kindel&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/bede/kindel&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: I'd use Pilon as mentioned above&lt;/p&gt;&#xA;" OwnerUserId="1258" LastEditorUserId="1258" LastEditDate="2017-08-04T19:24:34.243" LastActivityDate="2017-08-04T19:24:34.243" CommentCount="0" />
  <row Id="2220" PostTypeId="2" ParentId="2216" CreationDate="2017-08-04T20:51:38.387" Score="3" Body="&lt;p&gt;30 times coverage is not unique to this problem, but number &lt;strong&gt;30&lt;/strong&gt; has its empirical role in statistics:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In statistical analysis, the rule of three states that if a certain event did not occur in a sample with n subjects, the interval from 0 to 3/n is a 95% confidence interval for the rate of occurrences in the population. &lt;strong&gt;When n is greater than 30, this is a good approximation to results from more sensitive tests.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Rule_of_three_(statistics)&quot; rel=&quot;nofollow noreferrer&quot;&gt;Wikipedia: Rule of three (statistics)&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similarly, you can search for related questions like this one:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/post/What_is_the_rationale_behind_the_magic_number_30_in_statistics&quot; rel=&quot;nofollow noreferrer&quot;&gt;What is the rationale behind the magic number 30 in statistics?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In line with this, I have seen data processing in other disciplines which required &lt;strong&gt;n ≥ 30&lt;/strong&gt; for sufficient reliability of results.&lt;/p&gt;&#xA;" OwnerUserId="1267" LastActivityDate="2017-08-04T20:51:38.387" CommentCount="0" />
  <row Id="2222" PostTypeId="2" ParentId="2216" CreationDate="2017-08-06T03:27:17.950" Score="5" Body="&lt;p&gt;Solexa Inc. sequenced NA12878 chrX to ~30x in early 2007, which later became part of &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/18987734&quot; rel=&quot;noreferrer&quot;&gt;Bentley (2008)&lt;/a&gt;. This, I believe, was the first time that 30x showed up. I don't recall they had a particular reason for that. Figure 5 in the published paper was just aftermath. It does not really explain why not 25x or 35x, given that the curves between 25x and 35x in that figure are about linear.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the abstract of &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/21771779&quot; rel=&quot;noreferrer&quot;&gt;Ajay et al (2011)&lt;/a&gt;, the authors argued &quot;the current recommendation of ~30x coverage is not adequate&quot;. Nonetheless, the discussion section seems to suggest 50–60x would be necessary with GAIIx, but 35x was adequate with HiSeq2000 plus better recent chemistry. Overall, this paper provides a more thorough analysis. The data quality at that time is also closer to data we produce today.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The required coverage is largely determined by two factors: read placement bias (e.g. GC bias) and base/mapping error rate. While GC bias has been reduced with the PCR-free protocol, base error rate has been going the downward since HiSeq2500. I guess 30x coverage would be necessary if you want to achieve the sensitivity with the older 30x data. Illumina, as a sequencing service provider, and our sequencing facility still insist on the 30x threshold.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-08-06T03:27:17.950" CommentCount="1" />
  <row Id="2223" PostTypeId="1" AcceptedAnswerId="2228" CreationDate="2017-08-06T19:26:16.787" Score="6" ViewCount="127" Body="&lt;p&gt;I have some software which takes fastas as the input. I need to include SNVs and InDels from a VCF into the reference hg38 and then use this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is, I don't know of an algorithmically sound way to do this.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Are there any existing software packages which could do this efficiently? Is it easier to output a FASTA, or a bam (and then convert to a FASTA)? &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;What about if I wanted to do the same with a bedpe of germline structural variants? &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="146" LastEditorUserId="96" LastEditDate="2017-08-07T21:31:56.017" LastActivityDate="2017-08-07T21:31:56.017" Title="How to manipulate a reference FASTA or bam to include variants from a VCF?" Tags="&lt;bam&gt;&lt;fasta&gt;&lt;vcf&gt;&lt;variants&gt;&lt;reference-genome&gt;" AnswerCount="3" CommentCount="3" />
  <row Id="2224" PostTypeId="1" AcceptedAnswerId="2225" CreationDate="2017-08-06T19:47:22.410" Score="0" ViewCount="16" Body="&lt;p&gt;I'm trying to use Annovar to annotate some variants with their CADD and FATHMM scores.  I've downloaded the latest versions of the software and the databases but when I run it I get an error saying the index is out of date.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;WARNING: Your index file annovar_latest/humandb/hg19_cadd.txt.idx is out of date and will not be used. ANNOVAR can still generate correct results without index file.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It still runs without the index file but it is extremely slow. Does anyone know how to update the index file?&lt;/p&gt;&#xA;" OwnerUserId="59" LastActivityDate="2017-08-06T20:36:58.513" Title="Annovar index out of date" Tags="&lt;annovar&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="2225" PostTypeId="2" ParentId="2224" CreationDate="2017-08-06T20:36:58.513" Score="1" Body="&lt;p&gt;Your safest bet is to just &lt;a href=&quot;http://annovar.openbioinformatics.org/en/latest/user-guide/download/#additional-databases&quot; rel=&quot;nofollow noreferrer&quot;&gt;redownload the annotation&lt;/a&gt;. I've not seen official documentation of the index used by annovar, but apparently it's just a text file with a single line header and chromosome-bin coordinates. Over on SEQanswers, &lt;a href=&quot;http://seqanswers.com/forums/showthread.php?t=23535&quot; rel=&quot;nofollow noreferrer&quot;&gt;there's a thread discussing this issue&lt;/a&gt; with a single &lt;a href=&quot;http://seqanswers.com/forums/attachment.php?attachmentid=3818&amp;amp;d=1432814490&quot; rel=&quot;nofollow noreferrer&quot;&gt;perl script&lt;/a&gt; that allegedly recreates the index.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-06T20:36:58.513" CommentCount="2" />
  <row Id="2226" PostTypeId="2" ParentId="2223" CreationDate="2017-08-06T20:38:17.907" Score="4" Body="&lt;p&gt;GATK has a solution that might work for you.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_gatk_tools_walkers_fasta_FastaAlternateReferenceMaker.php&quot; rel=&quot;nofollow noreferrer&quot;&gt;FastaAlternateReferenceMaker&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="59" LastActivityDate="2017-08-06T20:38:17.907" CommentCount="1" />
  <row Id="2227" PostTypeId="2" ParentId="2223" CreationDate="2017-08-06T21:23:14.093" Score="3" Body="&lt;p&gt;There's a &lt;code&gt;vcf2fq&lt;/code&gt; &lt;a href=&quot;https://github.com/lh3/samtools/blob/master/bcftools/vcfutils.pl&quot; rel=&quot;nofollow noreferrer&quot;&gt;sub-program&lt;/a&gt; that was written as part of vcfutils to convert a VCF file into a fastq file given a reference sequence. Unfortunately this doesn't work properly with INDELs (it will just mask them, rather than actually converting them), so I wrote &lt;a href=&quot;https://github.com/gringer/bioinfscripts/blob/master/vcf2fq.pl&quot; rel=&quot;nofollow noreferrer&quot;&gt;a modification&lt;/a&gt; to implement INDEL correction as well:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./vcf2fq.pl -f &amp;lt;input.fasta&amp;gt; &amp;lt;all-site.vcf&amp;gt; &amp;gt; &amp;lt;output.fastq&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-08-06T21:23:14.093" CommentCount="0" />
  <row Id="2228" PostTypeId="2" ParentId="2223" CreationDate="2017-08-06T21:47:56.003" Score="3" Body="&lt;p&gt;You could &lt;a href=&quot;http://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/vcf2bed.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;convert VCF to BED&lt;/a&gt; via &lt;code&gt;vcf2bed --snvs&lt;/code&gt;, &lt;code&gt;vcf2bed --insertions&lt;/code&gt;, and &lt;code&gt;vcf2bed --deletions&lt;/code&gt;, and then use &lt;code&gt;samtools faidx&lt;/code&gt; by way of a &lt;a href=&quot;https://gist.github.com/alexpreynolds/fa9b0f90e181e3b4f640&quot; rel=&quot;nofollow noreferrer&quot;&gt;wrapper script&lt;/a&gt; to convert BED to FASTA, e.g.:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ vcf2bed --snvs &amp;lt; variants.vcf | bed2faidxsta.pl &amp;gt; snvs.fa&#xA;$ vcf2bed --insertions &amp;lt; variants.vcf | bed2faidxsta.pl &amp;gt; insertions.fa&#xA;$ vcf2bed --deletions &amp;lt; variants.vcf | bed2faidxsta.pl &amp;gt; deletions.fa&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You need FASTA files for your reference genome, which have been indexed with &lt;code&gt;samtools faidx&lt;/code&gt;, e.g., for &lt;code&gt;hg38&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ cd /foo/bar/baz&#xA;$ wget ftp://hgdownload.cse.ucsc.edu/goldenPath/hg38/chromosomes/*.fa.gz&#xA;$ for fn in `ls *.fa.gz`; do gunzip $fn; done&#xA;$ for fn in `ls *.fa`; do samtools faidx $fn; done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Once you have indexed FASTA files somewhere on your file system, you can pipe BED to the &lt;code&gt;bed2faidxsta.pl&lt;/code&gt; script, to get out FASTA sequences.&lt;/p&gt;&#xA;" OwnerUserId="776" LastEditorUserId="776" LastEditDate="2017-08-07T16:41:45.307" LastActivityDate="2017-08-07T16:41:45.307" CommentCount="0" />
  <row Id="2229" PostTypeId="2" ParentId="658" CreationDate="2017-08-06T22:47:16.617" Score="0" Body="&lt;p&gt;There are tonnes of them. On top of the excellent ones everybody mentioned&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;iRods&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Arvados&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Galaxy&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="1280" LastActivityDate="2017-08-06T22:47:16.617" CommentCount="1" />
  <row Id="2230" PostTypeId="1" AcceptedAnswerId="2231" CreationDate="2017-08-07T09:44:43.227" Score="4" ViewCount="91" Body="&lt;p&gt;Given a read ID, I want to edit a single basecall (e.g. the 12th base) for just that read from within a large FASTQ file containing millions of reads.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;example, i want to change the 12th base ('C') in read 31027 to a 'T':&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;@70630 1:N:0:ATCACG&#xA;GAAGGTCCATGGATAATACTCAATTTTCCACAACAGCTTTTGTACTCTAGATCATTGATATTTACCAAAAGTCACTCAAACTCATCCTATGCATAATTCTAGTCCACCAATCATGATATGATGGAGAACATGGTTGTAATCAGGAAGACAG&#xA;+&#xA;DDDDDHIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIHIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIGHHIIIIIIIIIIIIIIIIIIIIIIHIIHIHIIIHHIGIIIIHHHHIIH@&#xA;@31027 1:N:0:ATCACG&#xA;CAAAAGTCACTCAAACTCATCCTATGCATAATTCTAGTCCACCAATCATGATATGATGGAGAACATGGTTGTAATCAGGAAGACAGATAAAGCAGCAGACCAAAAGTAATCTGAGAAATTATATTTGAATCACTCAGATATACATCAAATA&#xA;+&#xA;DDBDDHC@GHFHHIIE@CEHHIHEFFCGFCH?HHEGFCCEHHCGH@HFHHHCHHIIIHEHIIHII@CECHIHIIIEECCDGEHFFHHHHEHHFHHGGIHHHDGHFHIIIGHHHHHHEHHIIHIIIGGHHHCHHHCHIIIHHIEH@GHIHIC&#xA;@87319 1:N:0:ATCACG&#xA;CAATTAAGCTTTGGCAACGGTGGTCAAGATGAGATGCATATGGAGATAATAACTAAAAGTCAATCGAGACTCATCGTATGCATATTTCTAGTCCATCGATCATGAAATGATAGGATAGCTAGAATGAAAAGTAAATTTCCAGAAGGTCCAT&#xA;+&#xA;DDDDDIIIIIIIIIIIIIHIHHIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIHHIIIIIIIIIIIIIIIIIHIIHIIIIIIIIIIIIHIIIIIIIIIHIHIH&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Of course I could stream through the entire FASTQ file rewriting everything to a new file until i get to the read of interest, do the edits to that read, write it out to the new file, and then keep going until the end. BioPython would work fine for this as would other approaches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, is there an efficient way to do this without the full read/write stream? Could this be done with sed/awk ? What about an indexed fastq file?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;cheers&#xA;David&lt;/p&gt;&#xA;" OwnerUserId="1281" LastActivityDate="2017-08-07T11:58:19.077" Title="How can I edit a specific FASTQ read in place, given the read ID?" Tags="&lt;ngs&gt;&lt;biopython&gt;&lt;fastq&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="2231" PostTypeId="2" ParentId="2230" CreationDate="2017-08-07T09:52:10.770" Score="4" Body="&lt;p&gt;You'll first need to determine the appropriate line number, which you can do with &lt;code&gt;grep -m1 -nw &quot;@31027&quot; foo.fastq&lt;/code&gt;. After that, note that you can provide a line number to &lt;code&gt;sed&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sed -i '123456s/CAAAAGTCACTCA/CAAAAGTCACTTA/' foo.fastq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That will do the replacement only on line 123456 and edit the file in place (the &lt;code&gt;-i&lt;/code&gt; option).&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="298" LastEditDate="2017-08-07T11:48:56.240" LastActivityDate="2017-08-07T11:48:56.240" CommentCount="2" />
  <row Id="2232" PostTypeId="5" CreationDate="2017-08-07T11:06:35.623" Score="0" Body="&lt;p&gt;FASTQ (&lt;a href=&quot;https://en.wikipedia.org/wiki/FASTQ_format&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://en.wikipedia.org/wiki/FASTQ_format&lt;/a&gt;) is a file format use to store short reads and their quality values. Use this tag for questions related to fastq file format.&lt;/p&gt;&#xA;" OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-07T11:37:41.333" LastActivityDate="2017-08-07T11:37:41.333" CommentCount="0" />
  <row Id="2233" PostTypeId="4" CreationDate="2017-08-07T11:06:35.623" Score="0" Body="FASTQ is a file format use to store short reads and their quality values" OwnerUserId="1140" LastEditorUserId="1140" LastEditDate="2017-08-07T11:37:44.543" LastActivityDate="2017-08-07T11:37:44.543" CommentCount="0" />
  <row Id="2234" PostTypeId="1" AcceptedAnswerId="2236" CreationDate="2017-08-06T20:33:05.040" Score="2" ViewCount="151" Body="&lt;p&gt;I wish to adapt the r language function fgseaL, &lt;a href=&quot;https://github.com/ctlab/fgsea&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/ctlab/fgsea&lt;/a&gt; , to perform rapidGSEA, &lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1244-x&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1244-x&lt;/a&gt; , computation of inter-class  deviation per gene and the subsequent gene rank sorting operation on 9 different phenotype labels as illustrated in the diagram immediately below:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/kXvmr.gif&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/kXvmr.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought of applying the R-language rank() function on the Expression Data Matrix D. If that is not correct, what sequence of R language commands should we&#xA;apply to the Expression Data Matrix D to calculate a key value sorted deviation measure  across 8 labeled human leukemia groups and a healthy labeled normal control group prior to running fgseaL? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I show below how fgseaL finds the correlation matrix between the R language variable , mat, which corresponds to the Expression Data Matrix D and the R language variable , labels , which is a vector of gene phenotype labels&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    tmatSc &amp;lt;- scale(t(mat))&#xA;    labelsSc &amp;lt;- scale(labels)[, 1]&#xA;&#xA;    minSize &amp;lt;- max(minSize, 1)&#xA;&#xA;    pathwaysFiltered &amp;lt;- lapply(pathways, function(p) { as.vector(na.omit(fmatch(p, rownames(mat)))) })&#xA;    pathwaysSizes &amp;lt;- sapply(pathwaysFiltered, length)&#xA;&#xA;    toKeep &amp;lt;- which(minSize &amp;lt;= pathwaysSizes &amp;amp; pathwaysSizes &amp;lt;= maxSize)&#xA;    m &amp;lt;- length(toKeep)&#xA;&#xA;    if (m == 0) {&#xA;        return(data.table(pathway=character(),&#xA;                          pval=numeric(),&#xA;                          padj=numeric(),&#xA;                          ES=numeric(),&#xA;                          NES=numeric(),&#xA;                          nMoreExtreme=numeric(),&#xA;                          size=integer(),&#xA;                          leadingEdge=list()))&#xA;    }&#xA;&#xA;    pathwaysFiltered &amp;lt;- pathwaysFiltered[toKeep]&#xA;    pathwaysSizes &amp;lt;- pathwaysSizes[toKeep]&#xA;&#xA;    corRanks &amp;lt;- var(tmatSc, labelsSc)[,1]&#xA;    ranksOrder &amp;lt;- order(corRanks, decreasing=T)&#xA;    ranksOrderInv &amp;lt;- invPerm(ranksOrder)&#xA;    stats &amp;lt;- corRanks[ranksOrder]&#xA;&#xA;    pathwaysReordered &amp;lt;- lapply(pathwaysFiltered, function(x) ranksOrderInv[x])&#xA;&#xA;    gseaStatRes &amp;lt;- do.call(rbind,&#xA;                           lapply(pathwaysReordered, calcGseaStat,&#xA;                                  stats=stats,&#xA;                                  returnLeadingEdge=TRUE))&#xA;&#xA;I found a problem with the algorithm shown immediately below.&#xA;correcttest &amp;lt;- data.frame(names = row.names(normal))&#xA;correcttest &amp;lt;- cbind(correcttest3, normal)&#xA;correcttest &amp;lt;- cbind(correcttest3, ALL3m)&#xA;rownames(correcttest) &amp;lt;- correcttest$names&#xA;correcttest$names &amp;lt;- NULL&#xA;correctlabelnormal &amp;lt;- rep(0:0, 73)&#xA;correctlabelALL3m &amp;lt;- rep(1:1, 122)&#xA;correctlabel &amp;lt;- as.vector(c(correctlabelnormal,correctlabelALL3m))&#xA;s &amp;lt;- apply(correcttest, 1, function(x) coef(lm(x~correctlabel))[2])&#xA;o &amp;lt;- rank(s)&#xA;o &amp;lt;- max(o) - o + 1&#xA;res &amp;lt;- fgseaL(df,o,correctlabel,nperm = 2000,minSize = 1, maxSize=50000)&#xA;empty data table (0 rows) of 8 columns:   pathway,pval,padj,ES,NES,nMoreExtreme,size&#xA;&#xA;I found the binary phenotype labeled group fgseaL test results below looked satisfactory.    &#xA;correcttest &amp;lt;- data.frame(names = row.names(normal))&#xA;correcttest &amp;lt;- cbind(correcttest3, normal)&#xA;correcttest &amp;lt;- cbind(correcttest3, ALL3m)&#xA;rownames(correcttest) &amp;lt;- correcttest$names&#xA;correcttest$names &amp;lt;- NULL&#xA;correctlabelnormal &amp;lt;- rep(0:0, 73)&#xA;correctlabelALL3m &amp;lt;- rep(1:1, 122)&#xA;correctlabel &amp;lt;- as.vector(c(correctlabelnormal,correctlabelALL3m))&#xA;fgseaL(df,correcttest,correctlabel,nperm = 2000,minSize = 1, maxSize=50000)&#xA;       pathway        pval        padj         ES       NES nMoreExtreme  size&#xA;1: Gene.Symbol 0.003940887 0.003940887 -0.2460126 -1.180009            3 45714&#xA;                                 leadingEdge&#xA;1: AKIRIN2,LRRC20,HSPA5,HSPA5,DTWD2,ZFYVE28,&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Thank you for your consideration.&lt;/p&gt;&#xA;" OwnerUserId="1283" OwnerDisplayName="Frank" LastEditorUserId="1283" LastEditDate="2017-08-08T00:28:04.583" LastActivityDate="2017-08-23T16:16:10.837" Title="How to adapt the fgseaL function to perform rapidGSEA computation of gene ranks across 9 different phenotype labels?" Tags="&lt;r&gt;&lt;genomics&gt;&lt;gsea&gt;" AnswerCount="3" CommentCount="12" FavoriteCount="0" />
  <row Id="2235" PostTypeId="2" ParentId="2230" CreationDate="2017-08-07T11:50:46.357" Score="2" Body="&lt;p&gt;If your fastq file is simple, if each of your reads only has a single line of DNA, you could do:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;awk '{&#xA;        if($1 == &quot;@31027&quot;){&#xA;            a=NR+1&#xA;        } &#xA;        if(NR==a){&#xA;            split($0,s,&quot;&quot;); &#xA;            s[12]=&quot;T&quot;; &#xA;            for(i in s){&#xA;                seq = sprintf(&quot;%s%s&quot;,seq,s[i]);&#xA;            } &#xA;            $0=seq&#xA;        }&#xA;    }1;' file.fastq &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-08-07T11:58:19.077" LastActivityDate="2017-08-07T11:58:19.077" CommentCount="0" />
  <row Id="2236" PostTypeId="2" ParentId="2234" CreationDate="2017-08-07T12:14:39.873" Score="1" Body="&lt;p&gt;You rank the fit coefficient rather than the original score matrix. So, given a score matrix, &lt;code&gt;D&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-r prettyprint-override&quot;&gt;&lt;code&gt;D = matrix(c(22,20,9,8,46,22,18,10,3,18,3,29,2,1,5,45,43,47,17,5,14,44,21,36), byrow=T, ncol=6)&#xA;cl = c(0,0,0,1,1,1)&#xA;s = apply(m, 1, function(x) coef(lm(x~cl))[2]) # [1]&#xA;o = rank(s)&#xA;o = max(o) - o + 1 # [2]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;o&lt;/code&gt; is then the rank of each row.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[1] This fits each row as a linear model of &lt;code&gt;cl&lt;/code&gt; and extracts the &lt;code&gt;cl&lt;/code&gt; coefficient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[2] This converts the ranking to be the same as shown in the figure. I don't know if this is important, but I would assume so given how GSEA methods tend to work.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-07T12:14:39.873" CommentCount="17" />
  <row Id="2237" PostTypeId="1" AcceptedAnswerId="2239" CreationDate="2017-08-07T14:57:32.910" Score="2" ViewCount="41" Body="&lt;p&gt;I am using the following command to get all refseq genes from UCSC:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/bin/mysql --user=genomep --password=password --host=genome-mysql.cse.ucsc.edu \&#xA;    -A -D hg38 -e 'select concat(t.name, &quot;.&quot;, i.version) name, \&#xA;    k.locusLinkId as &quot;EntrezId&quot;, t.chrom, t.strand, t.txStart, \&#xA;    t.txEnd, t.cdsStart, t.cdsEnd, t.exonCount, t.exonStarts, \&#xA;    t.exonEnds, t.score, t.name2 from refGene t join hgFixed.gbCdnaInfo i \&#xA;    on t.name = i.acc join hgFixed.refLink k on t.name = k.mrnaAcc'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That returns data in the following format (showing the 1st 5 lines):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-------------+-----------+-------+--------+---------+-------+----------+--------+-----------+--------------------+--------------------+-------+-----------+&#xA;| name        | EntrezId  | chrom | strand | txStart | txEnd | cdsStart | cdsEnd | exonCount | exonStarts         | exonEnds           | score | name2     |&#xA;+-------------+-----------+-------+--------+---------+-------+----------+--------+-----------+--------------------+--------------------+-------+-----------+&#xA;| NR_046018.2 | 100287102 | chr1  | +      |   11873 | 14409 |    14409 |  14409 |         3 | 11873,12612,13220, | 12227,12721,14409, |     0 | DDX11L1   |&#xA;| NR_106918.1 | 102466751 | chr1  | -      |   17368 | 17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-1 |&#xA;| NR_107062.1 | 102465909 | chr1  | -      |   17368 | 17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-2 |&#xA;| NR_107063.1 | 102465910 | chr1  | -      |   17368 | 17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-3 |&#xA;| NR_128720.1 | 103504738 | chr1  | -      |   17368 | 17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-4 |&#xA;+-------------+-----------+-------+--------+---------+-------+----------+--------+-----------+--------------------+--------------------+-------+-----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I also want to find the accession of the canonical transcript of each of the genes returned by the command above. Those seem to be stored in the &lt;code&gt;knownCanonical&lt;/code&gt; table:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ /usr/bin/mysql --user=genomep --password=password --host=genome-mysql.cse.ucsc.edu -A -D hg38 -e 'select * from knownCanonical limit 5'&#xA;+-------+------------+-----------+-----------+------------+--------------------+&#xA;| chrom | chromStart | chromEnd  | clusterId | transcript | protein            |&#xA;+-------+------------+-----------+-----------+------------+--------------------+&#xA;| chrX  |  100628669 | 100636806 |         1 | uc004ega.3 | ENSG00000000003.14 |&#xA;| chrX  |  100584801 | 100599885 |         2 | uc004efy.5 | ENSG00000000005.5  |&#xA;| chr20 |   50934866 |  50958550 |         3 | uc002xvw.2 | ENSG00000000419.12 |&#xA;| chr1  |  169853073 | 169893959 |         4 | uc001ggs.5 | ENSG00000000457.13 |&#xA;| chr1  |  169795048 | 169854080 |         5 | uc001ggp.4 | ENSG00000000460.16 |&#xA;+-------+------------+-----------+-----------+------------+--------------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, there seems to be no obvious way to link the &lt;code&gt;knownCanonical&lt;/code&gt; table to the &lt;code&gt;refGene&lt;/code&gt;, &lt;code&gt;hgFixed.gbCdnaInfo&lt;/code&gt; and &lt;code&gt;hgFixed.refLink&lt;/code&gt; tables used above. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, how can I modify my 1st query (or, if necessary write a new one) so that my results also include the accession of the gene's canonical transcript? &lt;/p&gt;&#xA;" OwnerUserId="298" LastEditorUserId="96" LastEditDate="2017-08-08T03:13:26.597" LastActivityDate="2017-08-08T03:13:26.597" Title="Get canonical transcript from UCSC" Tags="&lt;public-databases&gt;&lt;identifiers&gt;&lt;ucsc&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2238" PostTypeId="2" ParentId="2186" CreationDate="2017-08-07T15:06:06.557" Score="1" Body="&lt;h2&gt;A (bio)awk-based solution&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;Generate the list of genus&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bioawk -c fastx '{print $comment}' terminase_large.fasta \&#xA;    | sed -r 's/.*\[(\w+).*/\1/' \&#xA;    | sort -u &amp;gt; genus_names.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is how it works:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/lh3/bioawk&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;bioawk&lt;/code&gt;&lt;/a&gt; is like awk but with extra parsing capabilities. &lt;code&gt;-c fastx&lt;/code&gt; parses the fields in fasta or fastq records. &lt;code&gt;{print $comment}&lt;/code&gt; will print the &lt;code&gt;$comment&lt;/code&gt; field which contain the genus names you want to extract.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The &lt;code&gt;sed&lt;/code&gt; command captures (with parentheses) the word (&lt;code&gt;\w+&lt;/code&gt;) that is just after the opening square bracket (which should be the genus name, if there is only one opening square bracket on the line), and substitutes the whole line (that is, everything before the bracket (&lt;code&gt;.*&lt;/code&gt;), the bracket (&lt;code&gt;\[&lt;/code&gt;), the word, and everything after the word (&lt;code&gt;.*&lt;/code&gt; again)) with just the captured word (&lt;code&gt;\1&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;All this is sorted, and the &lt;code&gt;-u&lt;/code&gt; option of &lt;code&gt;sort&lt;/code&gt; ensures there is only one occurrence of each genus name in the output, which we put in the &lt;code&gt;genus_names.txt&lt;/code&gt; file.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h3&gt;Extract the fasta records for each genus&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for genus in $(cat genus_names.txt)&#xA;do&#xA;    bioawk -c fastx '{print}' terminase_large.fasta \&#xA;        | grep ${genus} \&#xA;        | awk -F &quot;\t&quot; '{print &quot;&amp;gt;&quot;$4&quot;\n&quot;$2}' \&#xA;        | sed -r 's/^&amp;gt;[^\[]+\[(.*)\]/&amp;gt;\1/' &amp;gt; ${genus}.fasta&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is how it works:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;We loop on the content of the previously established list of genus names.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;For each value of &lt;code&gt;${genus}&lt;/code&gt;, we parse again the fasta file with &lt;code&gt;bioawk&lt;/code&gt;. This time, for a given fasta record, we print all the elements that have been parsed. They appear on one line, in the following order (according to &lt;code&gt;bioawk -c help&lt;/code&gt;): &lt;code&gt;1:name 2:seq 3:qual 4:comment&lt;/code&gt; (we actually only need &lt;code&gt;seq&lt;/code&gt; and &lt;code&gt;comment&lt;/code&gt; for the following steps, but there are less risks of making errors at the later &lt;code&gt;awk&lt;/code&gt; step if we keep the four fields).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;We use &lt;code&gt;grep&lt;/code&gt; to select the resulting lines that contain the genus name we're currently dealing with.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The lines that have been successfully selected are then parsed by awk and reformatted into fasta format using only the comment part in the header. The &lt;code&gt;-F &quot;\t&quot;&lt;/code&gt; is to indicate that the field delimiters are tabulations (which is how bioawk has written the records when we did &lt;code&gt;{print}&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;We use &lt;code&gt;sed&lt;/code&gt; to keep only the part inside the square brackets (again using a pattern capture between parentheses, that we use in the substitution part with &lt;code&gt;\1&lt;/code&gt;), and the output is written in a file named using the current value of &lt;code&gt;${genus}&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;More details on the last &lt;code&gt;sed&lt;/code&gt; command:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We match the whole header lines. They start with &quot;&gt;&quot; (&lt;code&gt;^&amp;gt;&lt;/code&gt;), then there are some non-&quot;[&quot; characters (&lt;code&gt;[^\[]+&lt;/code&gt;), then a &quot;[&quot; (&lt;code&gt;\[&lt;/code&gt;), then something we capture (&lt;code&gt;(.*)&lt;/code&gt;), then a &quot;]&quot; (&lt;code&gt;\]&lt;/code&gt;). We replace them with the captured part. The sequence lines are unaffected because they don't match the above regular expression.&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-08-09T12:18:26.183" LastActivityDate="2017-08-09T12:18:26.183" CommentCount="0" />
  <row Id="2239" PostTypeId="2" ParentId="2237" CreationDate="2017-08-07T21:02:46.693" Score="3" Body="&lt;p&gt;You can link them with the &lt;code&gt;kgXref&lt;/code&gt; table, since &lt;code&gt;kgXref.refseq == refGene.name&lt;/code&gt; and &lt;code&gt;kgXref.kgID == knownCanonical.transcript&lt;/code&gt;. Since it seems that &lt;code&gt;knownCanonical.transcript&lt;/code&gt; is what you want anyway, you don't even need to join on it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mysql --user=genomep --password=password --host=genome-mysql.cse.ucsc.edu \&#xA;-A -D hg38 -e 'select concat(t.name, &quot;.&quot;, i.version) name, x.kgID, \&#xA;k.locusLinkId as &quot;EntrezId&quot;, t.chrom, t.strand, t.txStart, \&#xA;t.txEnd, t.cdsStart, t.cdsEnd, t.exonCount, t.exonStarts, \&#xA;t.exonEnds, t.score, t.name2 from refGene t join (hgFixed.gbCdnaInfo i, hgFixed.refLink k, kgXref x) \&#xA;on (t.name = i.acc and t.name = k.mrnaAcc and t.name = x.refseq) limit 10'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The output is then:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-------------+------------+-----------+-------+--------+---------+--------+----------+--------+-----------+--------------------+--------------------+-------+------------+&#xA;| name        | kgID       | EntrezId  | chrom | strand | txStart | txEnd  | cdsStart | cdsEnd | exonCount | exonStarts         | exonEnds           | score | name2      |&#xA;+-------------+------------+-----------+-------+--------+---------+--------+----------+--------+-----------+--------------------+--------------------+-------+------------+&#xA;| NR_106918.1 | uc031tla.1 | 102466751 | chr1  | -      |   17368 |  17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-1  |&#xA;| NR_107062.1 | uc031tlm.1 | 102465909 | chr1  | -      |   17368 |  17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-2  |&#xA;| NR_107063.1 | uc032cta.1 | 102465910 | chr1  | -      |   17368 |  17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-3  |&#xA;| NR_128720.1 | uc032dmn.1 | 103504738 | chr1  | -      |   17368 |  17436 |    17436 |  17436 |         1 | 17368,             | 17436,             |     0 | MIR6859-4  |&#xA;| NR_036051.1 | uc031tlb.1 | 100302278 | chr1  | +      |   30365 |  30503 |    30503 |  30503 |         1 | 30365,             | 30503,             |     0 | MIR1302-2  |&#xA;| NR_036266.1 | uc033cjs.1 | 100422831 | chr1  | +      |   30365 |  30503 |    30503 |  30503 |         1 | 30365,             | 30503,             |     0 | MIR1302-9  |&#xA;| NR_036267.1 | uc032csz.1 | 100422834 | chr1  | +      |   30365 |  30503 |    30503 |  30503 |         1 | 30365,             | 30503,             |     0 | MIR1302-10 |&#xA;| NR_036268.1 | uc032hiw.1 | 100422919 | chr1  | +      |   30365 |  30503 |    30503 |  30503 |         1 | 30365,             | 30503,             |     0 | MIR1302-11 |&#xA;| NR_026822.1 | uc001aak.4 |    654835 | chr1  | -      |   34610 |  36081 |    36081 |  36081 |         3 | 34610,35276,35720, | 35174,35481,36081, |     0 | FAM138C    |&#xA;| NR_106918.1 | uc031tla.1 | 102466751 | chr1  | -      |  187890 | 187958 |   187958 | 187958 |         1 | 187890,            | 187958,            |     0 | MIR6859-1  |&#xA;+-------------+------------+-----------+-------+--------+---------+--------+----------+--------+-----------+--------------------+--------------------+-------+------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you want the protein ID then join &lt;code&gt;knownCanonical&lt;/code&gt; on &lt;code&gt;x.kgID&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-07T21:02:46.693" CommentCount="0" />
  <row Id="2240" PostTypeId="2" ParentId="525" CreationDate="2017-08-07T21:13:05.843" Score="3" Body="&lt;p&gt;Just for reference, there is a not very in depth, but first-hand reasoning (Jim Kent) given here about why bedGraphToBigWig does not support streaming &lt;a href=&quot;http://genome.soe.ucsc.narkive.com/2S1Z3VpG/bedgraphtobigwig-reading-in-from-stdin&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://genome.soe.ucsc.narkive.com/2S1Z3VpG/bedgraphtobigwig-reading-in-from-stdin&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: as noted wigToBigWig allows bedgraph streaming input via stdin but takes (possibly much) more memory than bedGraphToBigWig itself does&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat file.bedgraph | wigToBigWig stdin chrom.sizes output.bigwig&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note: You can also use &lt;code&gt;wiggletools bin 10&lt;/code&gt; for example to bin rows and downsample&lt;/p&gt;&#xA;" OwnerUserId="60" LastEditorUserId="60" LastEditDate="2017-08-09T16:13:38.660" LastActivityDate="2017-08-09T16:13:38.660" CommentCount="2" />
  <row Id="2241" PostTypeId="1" CreationDate="2017-08-07T23:00:43.070" Score="4" ViewCount="47" Body="&lt;p&gt;I am looking the secretome profile and the membrane receptor profile for a given cell type. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my specific case, this should be the secretome and outer membrane receptor profiles of dorsal root ganglion. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I've done in the past is taken proteomics datasets or RNAseq datasets and used this as a reference cell surface proteome or secretome. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wasn't sure what the best way to filter secreted proteins or membrane receptors were, so I used the Human Protein Atlas and this turned out to be a big headache. A possible alternative method might be to use DAVID, but there must be a more efficient method via the command line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My questions are these:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Is there a &quot;consensus&quot; database containing protein expression&#xA;profiles for different tissue types in human?&lt;/li&gt;&#xA;&lt;li&gt;If not, what is the best way to filter out the &lt;strong&gt;secreted&lt;/strong&gt; and &lt;strong&gt;transmembrane&lt;/strong&gt; proteins by gene symbol?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="823" LastActivityDate="2017-08-10T15:10:04.033" Title="Secretome and membrane receptor profiles" Tags="&lt;public-databases&gt;&lt;networks&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2242" PostTypeId="1" AcceptedAnswerId="2244" CreationDate="2017-08-08T09:59:08.317" Score="8" ViewCount="259" Body="&lt;p&gt;I would like to convert a &lt;a href=&quot;https://genome.ucsc.edu/FAQ/FAQformat#format1&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;BED&lt;/code&gt;&lt;/a&gt; format to &lt;a href=&quot;http://www.ensembl.org/info/website/upload/gff3.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;GFF3&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only useful tool that I could find via a &lt;a href=&quot;https://www.google.ch/search?q=bed%20to%20gff3&quot; rel=&quot;nofollow noreferrer&quot;&gt;google search&lt;/a&gt; seems to be &lt;a href=&quot;https://galaxy.inf.ethz.ch/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Galaxy&lt;/a&gt;, and I do not feel very comfortable with online tools, plus the webserver is currenlty under maintenance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone knows about a command-line tool that can handle this conversion?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: here are some lines of my BED file: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ head -4 last_minion-r7_sort.bed&#xA;211000022278137 175 211 8e5d0959-7cdb-49cf-9298-94ed3b2aedb5_Basecall_2D_000_2d 42  +&#xA;211000022279134 0   503 e8a9c6b8-bad2-4a7e-97d8-ca4acb34ff70_Basecall_2D_000_2d 69  -&#xA;211000022279134 24  353 e258783d-95a3-41f5-9ad5-bb12311dbaf4_Basecall_2D_000_2d 45  -&#xA;211000022279134 114 429 26601afb-581a-41df-b42b-b366148ea06f_Basecall_2D_000_2d 100 -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The bed file thus has 6 columns as for now: chromosome, start coordinate, end coordinate, read name, score, strand. This file was obtained from conversion of &lt;code&gt;MAF&lt;/code&gt; format (as output of alignment of RNA-seq reads to reference genome, using &lt;a href=&quot;http://last.cbrc.jp/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;LAST&lt;/code&gt;&lt;/a&gt;) converted to &lt;code&gt;SAM&lt;/code&gt; using &lt;a href=&quot;http://last.cbrc.jp/doc/maf-convert.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;maf-convert&lt;/code&gt;&lt;/a&gt;, then to &lt;code&gt;BAM&lt;/code&gt; using &lt;a href=&quot;http://samtools.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;samtools&lt;/code&gt;&lt;/a&gt;, finally to &lt;code&gt;BED&lt;/code&gt; using &lt;a href=&quot;http://bedtools.readthedocs.io/en/latest/content/bedtools-suite.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;bedtools&lt;/code&gt;&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The aim of my conversion is basically to convert &lt;code&gt;SAM&lt;/code&gt; -&gt; &lt;code&gt;GTF&lt;/code&gt;, for &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/2036/tools-to-reconcile-experimental-transcripts-with-reference-annotation&quot;&gt;post-processing&lt;/a&gt;. Since there is no straightforward way to do this, I am going through steps, the only way to do this in my knowledge is : &lt;code&gt;SAM&lt;/code&gt; -&gt; &lt;code&gt;BAM&lt;/code&gt; -&gt; &lt;code&gt;BED&lt;/code&gt; -&gt; &lt;code&gt;GFF3&lt;/code&gt; -&gt; &lt;code&gt;GTF&lt;/code&gt; but for now I am stuck in the &lt;code&gt;BED&lt;/code&gt; -&gt; &lt;code&gt;GFF3&lt;/code&gt; part. &lt;/p&gt;&#xA;" OwnerUserId="294" LastEditorUserId="294" LastEditDate="2017-08-08T12:40:39.843" LastActivityDate="2017-08-10T13:39:09.303" Title="How to convert BED to GFF3" Tags="&lt;software-recommendation&gt;&lt;bed&gt;&lt;format-conversion&gt;&lt;gff3&gt;" AnswerCount="4" CommentCount="8" FavoriteCount="1" />
  <row Id="2243" PostTypeId="2" ParentId="27" CreationDate="2017-08-08T11:02:02.630" Score="0" Body="&lt;p&gt;Just to point out that if you want to follow @Devon Ryan's answer for a different organism/assembly, that is not in his very useful linked resource, you can download NCBI to UCSC contig to chromosome number mappings from &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/assembly&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.ncbi.nlm.nih.gov/assembly&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To the site and search for your assembly name. At the bottom of the page is a box called &quot;Global assembly definition&quot; containing a link titled &quot;Download full sequence report&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The downloaded file contains a table with:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Chromosome numbers in &quot;Sequence-Name&quot;/&quot;Assigned-molecule&quot;  &lt;/li&gt;&#xA;&lt;li&gt;NCBI names in Refseq-Accn  &lt;/li&gt;&#xA;&lt;li&gt;UCSC contig name in &quot;UCSC-style-name&quot;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-08-08T11:02:02.630" CommentCount="0" />
  <row Id="2244" PostTypeId="2" ParentId="2242" CreationDate="2017-08-08T11:42:33.840" Score="4" Body="&lt;p&gt;To answer the question as asked,  for people googling.&#xA;For BED6, in python:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;#contigs.tsv contians chromosome names and lengths in two columns&#xA;for line in open(&quot;contigs.tsv&quot;):&#xA;   fields = line.strip().split(&quot;\t&quot;)&#xA;   print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;1&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xA;&#xA;for line in open(&quot;my_bed_file.bed&quot;):&#xA;   fields = line.strip().split(&quot;\t&quot;)&#xA;&#xA;   # note: BED is 0-based, half-open, GFF is 1-based, closed&#xA;   start = str(int(fields[1]) + 1)&#xA;   print fields[0], &quot;bed&quot;, &quot;interval&quot;, fields[1], fields[2], fields[4], fields[5], &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (fields[3], fields[0])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For bed12, in python:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;#contigs.tsv contians chromosome names and lengths in two columns&#xA;for line in open(&quot;contigs.tsv&quot;):&#xA;   fields = line.strip().split(&quot;\t&quot;)&#xA;   print fields[0], &quot;.&quot;, &quot;contig&quot;,&quot;1&quot;,str(fields[1]), &quot;.&quot;, &quot;+&quot;, &quot;.&quot;, &quot;ID=%s&quot; % fields[0]&#xA;&#xA;for line in open(&quot;my_bed12.bed&quot;):&#xA;&#xA;   fields = line.strip().split(&quot;\t&quot;)&#xA;   contig = fields[0]&#xA;   # note: BED is 0-based, half-open, GFF is 1-based, closed&#xA;   start= int(fields[1]) + 1)&#xA;   end=fields[2]&#xA;   name=fields[3]&#xA;   score=fields[4]&#xA;   strand=fields[5]&#xA;   print contig, &quot;bed&quot;, &quot;interval&quot;, str(start), end, score, strand, &quot;.&quot;, &quot;ID=%s;parent=%s&quot; % (name, contig)&#xA;&#xA;   block_starts = map(int,fields[11].split(&quot;,&quot;))&#xA;   block_sizes = map(int, fields[10].split(&quot;,&quot;))&#xA;&#xA;   for (block, (bstart, blen)) in enumerate(zip(block_starts, block_sizes)):&#xA;      bend = start + bstart + blen&#xA;      print contig, &quot;bed&quot;, &quot;block&quot;, str(start + bstart), str(bend), score, strand, &quot;.&quot;, &quot;ID=%s_%i;parent=%s&quot; %(name, block, name)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="235" LastEditorUserId="235" LastEditDate="2017-08-08T12:06:37.843" LastActivityDate="2017-08-08T12:06:37.843" CommentCount="0" />
  <row Id="2245" PostTypeId="2" ParentId="2242" CreationDate="2017-08-08T12:00:46.733" Score="3" Body="&lt;p&gt;To convert BAM to GTF, which is the best way to get a file to compare with cuffcompare:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import pysam&#xA;&#xA;bamfile=pysam.AlignmentFile(&quot;my_bam_file.bam&quot;)&#xA;&#xA;for alignment in bamfile.fetch():&#xA;&#xA;    if alignment.is_unmapped:&#xA;        continue&#xA;&#xA;    contig = bamfile.get_reference_name(alignment.reference_id)&#xA;    name = alignment.query_name&#xA;&#xA;    if alignment.is_reverse:&#xA;        strand = &quot;-&quot;&#xA;    else: &#xA;        strand = &quot;+&quot;&#xA;&#xA;    for start, end in alignment.getblocks():&#xA;        # note: BED is 0-based, half-open, GFF is 1-based, closed&#xA;        print contig, &quot;BAM&quot;, &quot;exon&quot;, str(start + 1), str(end), &quot;0&quot;, strand, &quot;.&quot;, 'gene_id &quot;%s&quot;; transcript_id &quot;%s&quot;;' % (name, name)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;NB, this will also work with a SAM file as long as it is headered.&lt;/p&gt;&#xA;" OwnerUserId="235" LastEditorUserId="235" LastEditDate="2017-08-10T13:39:09.303" LastActivityDate="2017-08-10T13:39:09.303" CommentCount="2" />
  <row Id="2246" PostTypeId="1" CreationDate="2017-08-08T14:47:08.813" Score="0" ViewCount="44" Body="&lt;p&gt;I'm currently developing a program that determines protein structure, and I'm in need of creating a kind of &quot;profile&quot; for each amino acid. I've tried searching over the past several months (here and there), but haven't been able to find any such library (the programming language doesn't matter). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm specifically looking for something along the lines of an adjacency (bond) matrix for each atom within an amino acid, and in particular which atom within the amino acid attaches to the protein backbone.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I should also specify that in the adjacency (bond) matrix, I am only concerned with T/F values. Which is to say, I don't care for the bond length to be stored in the matrix, just the information of if two atoms are connected by a bond, denoted with a TRUE or FALSE value. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're interested in seeing some details on how my algorithm works, I'm basing a great deal of my calculatory procedures on the research done by &lt;a href=&quot;http://scholar.google.com/citations?user=xu3AWpAAAAAJ&amp;amp;hl=en&quot; rel=&quot;nofollow noreferrer&quot;&gt;Carlile Lavor&lt;/a&gt;. See &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/j.1475-3995.2007.00622.x/full&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; for the publication I frequently reference.&lt;/p&gt;&#xA;" OwnerUserId="1286" LastEditorUserId="298" LastEditDate="2017-08-08T16:00:09.153" LastActivityDate="2017-08-08T16:00:09.153" Title="Looking for an amino acid library" Tags="&lt;protein-structure&gt;&lt;public-databases&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2247" PostTypeId="2" ParentId="2246" CreationDate="2017-08-08T15:45:48.750" Score="0" Body="&lt;p&gt;The amino acids attach to the protein backbone thorough the &lt;a href=&quot;https://en.wikipedia.org/wiki/Peptide_bond&quot; rel=&quot;nofollow noreferrer&quot;&gt;peptide bond&lt;/a&gt;. The carbon which links with other amino acids receive the name of C'.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The bonds between atoms (of amino acids) are flexible, so the distance and the position (due to flexible angles between bonds) changes in each structure. Using PDB you can find the distances of the amino acids in known structures. Computing the distance between each atom of each aminoacid could be done with &lt;a href=&quot;http://biopython.org&quot; rel=&quot;nofollow noreferrer&quot;&gt;Biopython&lt;/a&gt;. You can &lt;a href=&quot;http://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc151&quot; rel=&quot;nofollow noreferrer&quot;&gt;read the PDB&lt;/a&gt; files and calculate the distance between atoms. Specifically you can look &lt;a href=&quot;http://biopython.org/wiki/The_Biopython_Structural_Bioinformatics_FAQ&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; for &quot;How do I measure distances?&quot;. Here I reproduce the example:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre&gt;&lt;code&gt;# Get some atoms&#xA;ca1 = residue1['CA']&#xA;ca2 = residue2['CA']&#xA;# Simply subtract the atoms to get their distance&#xA;distance = ca1 - ca2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-08-08T15:45:48.750" CommentCount="4" />
  <row Id="2250" PostTypeId="2" ParentId="2242" CreationDate="2017-08-08T20:27:37.457" Score="5" Body="&lt;p&gt;Galaxy has API and API-consuming libraries (such as BioBlend) that will allow you to interactively script against it without opening the graphical interface at all.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However you can also take almost any tool out of Galaxy and use it independently since everything is open source. The converter you mentioned is available as a Python script &lt;a href=&quot;https://github.com/galaxyproject/galaxy/blob/dev/tools/filters/bed_to_gff_converter.py&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt; and the tool 'wrapper' which you can use to understand how to invoke the python script is &lt;a href=&quot;https://github.com/galaxyproject/galaxy/blob/dev/tools/filters/bed2gff.xml&quot; rel=&quot;noreferrer&quot;&gt;next to it&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="34" LastActivityDate="2017-08-08T20:27:37.457" CommentCount="0" />
  <row Id="2251" PostTypeId="1" CreationDate="2017-08-09T15:32:05.220" Score="2" ViewCount="14" Body="&lt;p&gt;I am trying to install &lt;a href=&quot;https://github.com/gtamazian/chromosomer&quot; rel=&quot;nofollow noreferrer&quot;&gt;chromosomer&lt;/a&gt; but I fail. Can anybody help me, please?&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;$ pip install chromosomer&#xA;&#xA;Collecting chromosomer&#xA;  Could not find a version that satisfies the requirement chromosomer (from versions: )&#xA;No matching distribution found for chromosomer&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The output &lt;code&gt;python&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;$ python&#xA;&#xA;Python 3.6.0 |Anaconda custom (x86_64)| (default, Dec 23 2016, 13:19:00) &#xA;[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1075" LastEditorUserId="298" LastEditDate="2017-08-09T16:39:29.187" LastActivityDate="2017-08-09T16:39:29.187" Title="Cannot install chromosomer" Tags="&lt;python&gt;&lt;software-installation&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="0" />
  <row Id="2252" PostTypeId="2" ParentId="2242" CreationDate="2017-08-09T15:56:32.420" Score="1" Body="&lt;p&gt;Bioconductor makes this so easy. It does the coordinate conversion on import.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(rtracklayer)&#xA;&#xA;## import the bed file&#xA;bed.ranges &amp;lt;- import.bed('regions.bed')&#xA;&#xA;## export as a gff3 file&#xA;export.gff3(bed.ranges,'regions.gff3')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And people wonder why R is so popular for bioinformatics...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also if needed, you could go straight from &lt;strong&gt;BAM&lt;/strong&gt; file to &lt;strong&gt;gff3&lt;/strong&gt; in R as well.&lt;/p&gt;&#xA;" OwnerUserId="383" LastActivityDate="2017-08-09T15:56:32.420" CommentCount="1" />
  <row Id="2253" PostTypeId="2" ParentId="2251" CreationDate="2017-08-09T16:21:04.900" Score="5" Body="&lt;p&gt;Chromosomer only exists for Python 2. You should thus be able to install it via&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;pip2 install chromosomer&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;But:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your Python installation is a bit screwed up: there are Python 2 and Python 3, which are unfortunately incompatible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On your system &lt;code&gt;python&lt;/code&gt; and &lt;code&gt;pip&lt;/code&gt; seem to be aliases for Python 3, which I’d strongly recommend against (lots of tools will break). Instead, they should alias Python 2. Python 3 should be invoked via &lt;code&gt;python3&lt;/code&gt;/&lt;code&gt;pip3&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-08-09T16:21:04.900" CommentCount="4" />
  <row Id="2254" PostTypeId="2" ParentId="892" CreationDate="2017-08-09T20:19:15.707" Score="1" Body="&lt;p&gt;Here is how I was able to export to a standard gzipped FASTQ file using Biopython. Basically, instead of using &lt;code&gt;SeqIO.write&lt;/code&gt;, I directly called the &lt;code&gt;.format&lt;/code&gt; method of the SeqRecord object. The example code below imports a gzipped FASTQ file, removes reads that do not contain a G in positions 7, 8, and 9, and writes the results to a gzipped FASTQ file.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from Bio import SeqIO&#xA;import gzip&#xA;&#xA;path_in = &quot;path/to/in.fastq.gz&quot;&#xA;path_out = &quot;path/to/out.fastq.gz&quot;&#xA;handle_in = gzip.open(path_in, &quot;rt&quot;)&#xA;handle_out = gzip.open(path_out, &quot;wt&quot;)&#xA;&#xA;fq = SeqIO.parse(handle_in, &quot;fastq&quot;)&#xA;for read in fq:&#xA;    # Only export reads that have a G in positions 7, 8,&#xA;    # and 9&#xA;    if read.seq[6:9] == &quot;GGG&quot;:&#xA;        handle_out.write(read.format(&quot;fastq&quot;))&#xA;&#xA;handle_in.close()&#xA;handle_out.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Using the code example from the original question that converts a FASTA file to a FASTQ file, it would look something like the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fastq = gzip.open(&quot;path/to/out.fastq.gz&quot;, &quot;wt&quot;)&#xA;for record in SeqIO.parse(fasta, &quot;fasta&quot;):&#xA;    fastq.write(record.format(&quot;fastq&quot;))&#xA;fastq.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Caveat:&lt;/strong&gt; This only works for Python 3. Python 3 differs from Python 2 in how it imports data from binary files (&lt;a href=&quot;http://python3porting.com/problems.html?highlight=decode#reading-from-files&quot; rel=&quot;nofollow noreferrer&quot;&gt;source&lt;/a&gt;). In fact, ideally you should be using Python 3.4 or greater since Biopython has compatibility issues with Python 3.3 (&lt;a href=&quot;https://github.com/biopython/biopython/pull/723&quot; rel=&quot;nofollow noreferrer&quot;&gt;source&lt;/a&gt;).&lt;/p&gt;&#xA;" OwnerUserId="1302" LastActivityDate="2017-08-09T20:19:15.707" CommentCount="0" />
  <row Id="2257" PostTypeId="1" AcceptedAnswerId="2258" CreationDate="2017-08-09T23:26:19.593" Score="3" ViewCount="38" Body="&lt;p&gt;I'm running blast+ searches for a list of proteins using specific parameters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is my search&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; blastp -query protein.fasta -remote -db refseq_protein -out &#xA;protein_refseq_blast.txt -outfmt '6 qseqid sseqid qstart qend length evalue' -&#xA;entrez_query 'txid2 [ORGN] or txid4751 [ORGN]' -evalue 200000 -max_target_seqs 10000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For some proteins, this runs moderately quickly, ~5-10 mins. For other proteins, this command never stops running, even after an hour or so. I'm wondering if this variability in speed is due to any part of this choice in parameters, is simply variability in speed for the BLAST servers. Is it common for BLAST searches to take this long? Is there a way to run all of them in parallel using these parameters if this is the case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;" OwnerUserId="727" LastActivityDate="2017-08-10T10:12:11.917" Title="Major variability in speed of BLAST between proteins" Tags="&lt;blast&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="2258" PostTypeId="2" ParentId="2257" CreationDate="2017-08-10T08:35:02.367" Score="4" Body="&lt;p&gt;Try changing you &lt;code&gt;evalue&lt;/code&gt; threshold to something like &lt;code&gt;0.001&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you blasted a sequence against a database containing 10,000 sequences and got a hit with an evalue of &lt;code&gt;1&lt;/code&gt;, this would mean that, given the size of your database and length of protein, you would expect to find a match as good as (or better than) the hit returned once purely by chance. If the &lt;code&gt;evalue&lt;/code&gt; threshold in your query was set to &lt;code&gt;0.01&lt;/code&gt; (for example) then this hit would not be returned.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you got a hit with an &lt;code&gt;evalue&lt;/code&gt; of 200,000 this would mean you would expect to see 200,000 matches with a similar score simply by chance. If your &lt;code&gt;evalue&lt;/code&gt; threshold were set to 200000, then all of the hits with an evalue &amp;lt;=200000 would be considered significant. If you set it to &lt;code&gt;0.001&lt;/code&gt; then only those hits with &lt;code&gt;evalues&lt;/code&gt; &amp;lt;= &lt;code&gt;0.001&lt;/code&gt; would be considered significant. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This means that (depending on the size of the data base) almost every sequence in your query set will be considered as significant given your threshold, which will slow down your search hugely. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;See the the section on E values &lt;a href=&quot;https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE_TYPE=BlastDocs&amp;amp;DOC_TYPE=FAQ#expect&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1148" LastEditorUserId="1148" LastEditDate="2017-08-10T10:12:11.917" LastActivityDate="2017-08-10T10:12:11.917" CommentCount="0" />
  <row Id="2259" PostTypeId="1" AcceptedAnswerId="2261" CreationDate="2017-08-10T08:52:33.817" Score="3" ViewCount="128" Body="&lt;p&gt;I'm looking to generate data based on the DNA composition of a region of my genomes (data is incomplete genomes from HiSeq runs in fasta format). I'm looking for software which will give me sliding windows for GC content, GC skew, codon bias etc. I would like to use something which I can use with or plug into R. &lt;/p&gt;&#xA;" OwnerUserId="982" LastEditorUserId="77" LastEditDate="2017-08-10T12:36:33.297" LastActivityDate="2017-08-10T12:36:33.297" Title="Software recommendations - DNA composition" Tags="&lt;r&gt;&lt;software-recommendation&gt;&lt;dna&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2260" PostTypeId="2" ParentId="2257" CreationDate="2017-08-10T09:23:08.983" Score="3" Body="&lt;p&gt;The evalue associated with a blast HSP (High-scoring Segment Pair) is a measure of how often you would expect to find such an HSP in a database of this size by pure chance. For instance, if you blast the sequence &lt;code&gt;ACTG&lt;/code&gt; against the entirety of the NCBI's &lt;code&gt;nr&lt;/code&gt; database, you would expect several hundred thousand hits by pure chance. That is, several hundred thousand sequences that can be aligned to your query but are in no way related to it (not actual homologs). Those hits don't actually mean anything, so you use the evalue to limit the results to those that are unlikely to be found by chance and, therefore, more likely to have biological meaning if hits are found. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, setting the evalue to 200000 means you are interested even in HSPs whose characteristics (score, length, etc) mean you would expect to find &amp;lt;=200000 HSPs with such characteristics by pure chance. This evalue is pointless. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because your e-value is so high, BLAST will report absolutely anything and everything it finds, and the vast majority of your results will be meaningless and irrelevant. If you want a permissive e-value, you could use &lt;code&gt;1&lt;/code&gt; or even &lt;code&gt;10&lt;/code&gt; but 200000 is way too much. You usually only want negative e-values, cases where you wouldn't expect to find even one such HSP by chance. For highly dissimilar sequences, I would recommend you use 0.1 or something of that scale. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Changing the evalue will speed things up considerably. That said, you should always expect different times for different queries since the time it takes to find hits will always depend on the number of hits that can be found. If your sequence can be aligned to many of the sequences in the database, it will take longer to fetch all results. &lt;/p&gt;&#xA;" OwnerUserId="298" LastEditorUserId="298" LastEditDate="2017-08-10T10:05:48.027" LastActivityDate="2017-08-10T10:05:48.027" CommentCount="0" />
  <row Id="2261" PostTypeId="2" ParentId="2259" CreationDate="2017-08-10T09:58:50.333" Score="4" Body="&lt;p&gt;There are more R packages available that calculate GC content, for example Ape's &lt;code&gt;GC.content()&lt;/code&gt; function.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; library(ape)&#xA;&amp;gt; data(woodmouse)&#xA;&amp;gt; GC.content(woodmouse)&#xA;[1] 0.3873347&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With a sliding window is in &lt;code&gt;Biostrings&lt;/code&gt; package.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; library(Biostrings) &#xA;&amp;gt; DNA &amp;lt;- DNAString(&quot;ACTGAAACCGTGGCAGTTTGAC&quot;)&#xA;&amp;gt; letterFrequencyInSlidingView(DNA, view.width=4,letters=&quot;CG&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For codon usage bias, I think &lt;code&gt;cubfits&lt;/code&gt; or &lt;code&gt;sscu&lt;/code&gt; packages might be useful. &lt;/p&gt;&#xA;" OwnerUserId="939" LastEditorUserId="939" LastEditDate="2017-08-10T11:24:02.910" LastActivityDate="2017-08-10T11:24:02.910" CommentCount="3" />
  <row Id="2262" PostTypeId="2" ParentId="2204" CreationDate="2017-08-10T12:19:49.280" Score="1" Body="&lt;p&gt;I believe all you need to do is to run &lt;code&gt;dnadiff&lt;/code&gt; from MUMmer. That will run a comparison and output a number of useful metrics.&lt;/p&gt;&#xA;" OwnerUserId="1305" LastEditorUserId="77" LastEditDate="2017-08-10T13:14:13.037" LastActivityDate="2017-08-10T13:14:13.037" CommentCount="1" />
  <row Id="2263" PostTypeId="1" CreationDate="2017-08-10T12:24:36.120" Score="1" ViewCount="15" Body="&lt;p&gt;I noticed remote bigWig files (http URL) take sometimes about 1 minute to load on IGV. Once they loaded, jumping from one region to another can also take several seconds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My understanding is that bigWig files have an internal index for different chunks of different sizes. Can the index be modified so that the speed of loading on IGV is improved? Does this need to happen at the point of creation of the bigWig file, or can the index of an existing bigWig be updated to speed up loading?&lt;/p&gt;&#xA;" OwnerUserId="180" LastActivityDate="2017-08-10T12:33:11.853" Title="IGV faster loading remote bigWig files" Tags="&lt;bigwig&gt;&lt;deeptools&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2264" PostTypeId="2" ParentId="2263" CreationDate="2017-08-10T12:33:11.853" Score="1" Body="&lt;p&gt;You would need to do this when creating the bigWig file. Note that I'm not aware of anything that allows manually setting the various zoom levels (libBigWig/pyBigWig will allow you to set a max number, but that's it).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This then raises the question of whether IGV is using an appropriate zoom level to begin with. I've never looked at how it chooses which of the zoom levels to use, but I know it uses them since if you don't have any it will fail to load bigWig files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Honestly, though, if it's taking a minute to jump to a new region then I suspect that the network is just slow or the remote server is overtaxed. Changing the internal indexing isn't going to fix that.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-10T12:33:11.853" CommentCount="0" />
  <row Id="2265" PostTypeId="1" AcceptedAnswerId="2271" CreationDate="2017-08-10T12:46:40.243" Score="2" ViewCount="258" Body="&lt;p&gt;I generated a file starting with the following bed lines:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ head -6 /tmp/bed_with_gene_ids.bed&#xA;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   4118    4220    &quot;WBGene00022277&quot;    .   -&#xA;I   4118    4358    &quot;WBGene00022277&quot;    .   -&#xA;I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xA;I   4220    4223    &quot;WBGene00022277&quot;    .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would like to merge them based on the name field (the 4-th column), taking the min for the start and the max for the end. Other fields are expected to be the same for all records having the same name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Expected result:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I found a potential solution based on &lt;code&gt;bedtools groupby&lt;/code&gt; here: &lt;a href=&quot;https://www.biostars.org/p/145751/#145775&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://www.biostars.org/p/145751/#145775&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Sample data:&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat genes.bed&#xA;chr14    49894259    49895806    ENSMUST00000053290    0.000000    ...&#xA;chr14    49894873    49894876    ENSMUST00000053290    0.000000    ...&#xA;chr14    49894876    49895800    ENSMUST00000053291    0.000000    ...&#xA;chr14    49895797    49895800    ENSMUST00000053291    0.000000    ...&#xA;chr14    49901908    49901941    ENSMUST00000053291    0.000000    ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Example output:&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sort -k4,4 genes.bed \&#xA;| groupBy -g 1,4 -c 4,2,3 -o count,min,max \&#xA;| awk -v OFS='\t' '{print $1, $4, $5, $2, $3}'&#xA;&#xA;chr14    49894259    49895806    ENSMUST00000053290    2&#xA;chr14    49894876    49901941    ENSMUST00000053291    3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;However:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;I don't understand the groupBy behaviour (Why &lt;code&gt;-g 1,4&lt;/code&gt; and not just &lt;code&gt;-g 4&lt;/code&gt;?, Why &lt;code&gt;-c 4,2,3&lt;/code&gt; in this order and then rearrange things using &lt;code&gt;awk&lt;/code&gt;?)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;This code doesn't work for me.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Here is what happens when I try the solution given above:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ head -3 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 1,4 -c 4,2,3 -o count,min,max | awk -v OFS='\t' '{print $1, $4, $5, $2, $3}'&#xA;3           3746    4220&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here are attempt based on what I thought could work according to &lt;a href=&quot;http://bedtools.readthedocs.io/en/latest/content/tools/groupby.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;the documentation&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 4 -c 1,2,3,4,5,6 -o first,min,max,distinct,first,first&#xA;I   3746    10230   &quot;WBGene00022277&quot;,&quot;WBGene00023193&quot;   .   -&#xA;&#xA;$ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 4 -c 1,2,3,4,5,6 -o first,min,max,last,first,first&#xA;I   3746    10230   &quot;WBGene00022277&quot;    .   -&#xA;&#xA;$ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 4 -c 1,2,3,5,6 -o first,min,max,first,first&#xA;I   3746    10230   .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I don't get why when I group based on the 4-th column, for which I have two distinct values, I cannot obtain two lines in the resulting output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand based on the comments on the documentation page that the documentation is not up-to-date. In particular, there is a &lt;code&gt;-full&lt;/code&gt; option that is needed if one wants all fields to be outputted. Re-reading the solution mentioned above, I think I now understand the reason for the multiple columns for the &lt;code&gt;-g option&lt;/code&gt; and for the &lt;code&gt;awk&lt;/code&gt; rearrangement. Hence the following attempt.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 1,4,5,6 -c 2,3 -o min,max -full&#xA;    I   3746    3909    &quot;WBGene00023193&quot;    .   -   3746    10230&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But this still doesn't give me two lines.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there other tools that could do what I want efficiently?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h3&gt;Edit: Solution&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;According to &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/2271/292&quot;&gt;this answer&lt;/a&gt;, the problem with bedtools is that there is a bug in the latest release (2.26.0 as of august 2017). In order to have a functional &lt;code&gt;bedtools groupby&lt;/code&gt;, one needs to get the development version from github.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With the &lt;a href=&quot;https://github.com/arq5x/bedtools2/commit/52db65490b196f95bcd141a20c79ed36d9989496&quot; rel=&quot;nofollow noreferrer&quot;&gt;github version of bedtools&lt;/a&gt;, I can now get the expected result as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ head -6 /tmp/bed_with_gene_ids.bed | bedtools groupby -g 1,4,5,6 -c 2,3 -o min,max | awk -v OFS=&quot;\t&quot; '{print $1,$5,$6,$2,$3,$4}'&#xA;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I include fields 1, 5 and 6 in &lt;code&gt;-g&lt;/code&gt; (besides field 4) in order to have them printed out. In my bed file, they should be the same for a given value of field 4. The &lt;code&gt;awk&lt;/code&gt; part is needed because one has apparently not total control on the output order: the &lt;code&gt;-g&lt;/code&gt; fields come before the &lt;code&gt;-c&lt;/code&gt; fields.&lt;/p&gt;&#xA;" OwnerUserId="292" LastEditorUserId="292" LastEditDate="2017-08-11T15:18:50.827" LastActivityDate="2017-08-13T22:27:07.477" Title="Merging bed records based on name" Tags="&lt;bed&gt;&lt;bedtools&gt;" AnswerCount="5" CommentCount="2" />
  <row Id="2266" PostTypeId="2" ParentId="2265" CreationDate="2017-08-10T13:45:37.637" Score="4" Body="&lt;p&gt;You could do this with the &lt;a href=&quot;https://github.com/CGATOxford/cgat&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;CGAT&lt;/code&gt; toolkit&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;cgat bed2bed --method=merge --merge-by-name -I bed_with_gene_ids.bed&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Installing such a massive package might be overkill for this task though. &lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-08-10T13:45:37.637" CommentCount="5" />
  <row Id="2267" PostTypeId="2" ParentId="2265" CreationDate="2017-08-10T13:59:48.527" Score="0" Body="&lt;p&gt;If you're 100% sure that everything but the start and end positions will be the same for all lines sharing a name, you could just do it yourself. For example, in Perl:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ perl -lane '$start{$F[3]}||=$F[1]; &#xA;              if( $F[1] &amp;lt; $start{$F[3]} ){&#xA;                 $start{$F[3]} = $F[1]&#xA;              } &#xA;              if( $F[2] &amp;gt; $end{$F[3]} ){&#xA;                 $end{$F[3]} = $F[2]&#xA;              } &#xA;              $chr{$F[3]} = $F[0]; &#xA;              $rest{$F[3]} = join &quot;\t&quot;, @F[4,$#F]; &#xA;              END{&#xA;                 foreach $n (keys %chr){&#xA;                  print &quot;$chr{$n}\t$start{$n}\t$end{$n}\t$n\t$rest{$n}&quot;&#xA;                 }&#xA;              }' file.bed &#xA;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="298" LastActivityDate="2017-08-10T13:59:48.527" CommentCount="2" />
  <row Id="2268" PostTypeId="2" ParentId="2241" CreationDate="2017-08-10T15:10:04.033" Score="0" Body="&lt;p&gt;I don't think there is a database with expression data for your particular cell type. I am afraid your &quot;headache&quot; approach doesn't sound so bad...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So get expression data e.g., from GEO for your cell type, and define which genes/proteins you consider as &quot;expressed&quot; (e.g., minimal RPKM value).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, you can use GO annotation to find your genes involved with membrane or secretion (or what ever). You can use biomaRt in R for this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, you want to know all &quot;membrane&quot; genes.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; library(biomaRt)&#xA;&amp;gt; &#xA;&amp;gt; ensembl = useMart(&quot;ensembl&quot;,dataset=&quot;hsapiens_gene_ensembl&quot;)&#xA;&amp;gt; &#xA;&amp;gt; membrane.genes &amp;lt;- getBM(attributes=c('hgnc_symbol',&#xA;&amp;gt; 'ensembl_transcript_id', 'go_id'),&#xA;&amp;gt; filters = 'go', values = 'GO:0016020', mart = ensembl)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And now you want to know it only from genes that are expressed in your ganglion cells (with dummy example of two genes).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; gene.list &amp;lt;- c(&quot;KIR2DL3&quot;, &quot;GPR1&quot;)&#xA;&amp;gt; &#xA;&amp;gt; membrane.genes.list &amp;lt;- membrane.genes[membrane.genes$hgnc_symbol ==&#xA;&amp;gt; gene.list,]&#xA;&amp;gt; &#xA;&amp;gt; membrane.only &amp;lt;- membrane.genes.list[membrane.genes.list$go_id ==&#xA;&amp;gt; 'GO:0016020',]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="939" LastActivityDate="2017-08-10T15:10:04.033" CommentCount="3" />
  <row Id="2269" PostTypeId="1" CreationDate="2017-08-10T16:25:38.047" Score="4" ViewCount="19" Body="&lt;p&gt;I have some data I am working with, and I am curious if I am able to combine p-values from a paired t-test for CpG sites in the genome using Fisher's Method to get one p-value for each unique gene. Linked &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher%27s_method&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; is the Wikipedia page for Fisher's Method. I understand that an assumption of the method used is that each individual p-value being combined must be independent. I'm relatively new to biostatistics, so I'm curious if using CpGs from the same gene would violate this assumption.&lt;/p&gt;&#xA;" OwnerUserId="1309" LastActivityDate="2017-08-10T18:11:16.970" Title="Melt p-values for CpG sites mapping to the same gene" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;gene&gt;&lt;methylation&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2270" PostTypeId="2" ParentId="2216" CreationDate="2017-08-10T16:58:43.040" Score="0" Body="&lt;p&gt;The point I always miss in the discussion about coverage is, that no one tells how it was calculated. Were duplicates removed? How do you count overlapping paired-end reads? As 2 or 1? Just to point out two things that have influence on the coverage.&lt;/p&gt;&#xA;" OwnerUserId="1310" LastEditorUserId="77" LastEditDate="2017-08-10T18:12:40.087" LastActivityDate="2017-08-10T18:12:40.087" CommentCount="6" />
  <row Id="2271" PostTypeId="2" ParentId="2265" CreationDate="2017-08-10T17:40:41.057" Score="3" Body="&lt;p&gt;Although you don't mention it, I'm guessing you're using bedtools v2.26.0. Version 2.26.0 of groupBy has a bug in it, which you've encountered (it was fixed shortly after release, so you'll either have to use a version before the bug was introduced, or compile the current source code yourself from &lt;a href=&quot;https://github.com/arq5x/bedtools2&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/arq5x/bedtools2&lt;/a&gt;) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;v2.26.0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;local10:~/Documents/tmp$ cat asdf.bed &#xA;I	3746	3909	WBGene00023193	.	-&#xA;I	3746	3909	WBGene00023193	.	-&#xA;I	4118	4220	WBGene00022277	.	-&#xA;I	4118	4358	WBGene00022277	.	-&#xA;I	4118	10230	WBGene00022277	.	-&#xA;I	4220	4223	WBGene00022277	.	-&#xA;local10:~/Documents/tmp$ groupBy -i asdf.bed -g 4 -c 2,3 -o min,max &#xA;3746    10230 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;v2.26.0-125-g52db654 (I.E. compiling the source code from github): &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;local10:~/Documents/tmp$ bedtools2/bin/groupBy -i asdf.bed -g 4 -c 2,3 -o min,max&#xA;WBGene00023193  3746    3909&#xA;WBGene00022277  4118    10230&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;To answer your questions: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) You might notice that my output above gives the grouped columns first; you'll have to reorder the output via awk in order to get it back in order. As for why they chose to group on both columns 1 and 4: if you have the same name on multiple chromosomes, you may want to treat them as separate features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Version differences, as stated in the first part of my answer. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;To actually merge the file: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Make sure to run this with a version other than v2.26.0 (As Devon Ryan writes in the comments, you may want to add column 6 to &lt;code&gt;-g&lt;/code&gt; to make it strand-specific):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./bedtools2/bin/groupBy -i asdf.bed -g 1,4 -c 2,3,5,6 -o min,max,first,first \&#xA;     | awk -v OFS='\t' '{print $1, $3, $4, $2, $5, $6}'&#xA;I   3746    3909    WBGene00023193  .   -&#xA;I   4118    10230   WBGene00022277  .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1076" LastEditorUserId="1076" LastEditDate="2017-08-11T17:36:04.583" LastActivityDate="2017-08-11T17:36:04.583" CommentCount="1" />
  <row Id="2272" PostTypeId="2" ParentId="2269" CreationDate="2017-08-10T18:11:16.970" Score="0" Body="&lt;p&gt;Methylation levels have high local correlation, so Fisher's method would be problematic. Having said that, you have no reason to use Fisher's method after a paired t-test. A paired t-test will give you a single p-value per gene, which is what you want. Do be sure to only include CpG with some minimal coverage in both group.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-10T18:11:16.970" CommentCount="0" />
  <row Id="2273" PostTypeId="2" ParentId="2265" CreationDate="2017-08-10T18:48:21.233" Score="1" Body="&lt;pre&gt;&lt;code&gt;$ cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq | awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }' | paste -d &quot;\t&quot; - - | sed 's/_/\t/g' | sort-bed - &amp;gt; answer.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Given your sample input:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ more in.bed&#xA;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   4118    4220    &quot;WBGene00022277&quot;    .   -&#xA;I   4118    4358    &quot;WBGene00022277&quot;    .   -&#xA;I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xA;I   4220    4223    &quot;WBGene00022277&quot;    .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;answer.bed&lt;/code&gt; file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ more answer.bed&#xA;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Sorting with &lt;code&gt;sort-bed&lt;/code&gt; is useful at the end, so that you can pipe it or work with it with other BEDOPS tools, or other tools that now accept sorted BED input.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Streaming is a pretty efficient way to do things, generally.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How this works&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the pipeline again:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq | awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }' | paste -d &quot;\t&quot; - - | sed 's/_/\t/g' | sort-bed - &amp;gt; answer.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We start by cutting columns 4 through 6 (id, score and strand), replacing tabs with underscores, sorting and removing duplicates:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cut -f4-6 in.bed | sed 's/\t/_/g' | sort | uniq&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What we get out of this is a sorted list of &quot;needles&quot; — one for each ID-score-strand combination: an &lt;em&gt;ID-needle&lt;/em&gt; — that we can use to &lt;code&gt;grep&lt;/code&gt; or filter the original BED file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This list is piped to &lt;code&gt;awk&lt;/code&gt; which, for each ID-needle, runs &lt;code&gt;grep&lt;/code&gt; against the original BED file and pipes the subset to &lt;code&gt;bedops --merge -&lt;/code&gt;, which merges overlapping intervals. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that merging only works for overlapping intervals. &lt;em&gt;Merging is not necessarily the same as returning a min-max pair, and this pipeline will break if there are intervals that do not overlap.&lt;/em&gt; But you could modify the &lt;code&gt;awk&lt;/code&gt; statement to process the input intervals and return the minimum and maximum interval coordinates, if that is really what you want, by tracking the min and max values over all intervals that come into &lt;code&gt;awk&lt;/code&gt;, and printing a final interval with an &lt;code&gt;END&lt;/code&gt; block.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;system&lt;/code&gt; command prints the merged interval on one line. The following &lt;code&gt;print $0&lt;/code&gt; statement prints the needle on the next line:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;awk -F'_' '{ system(&quot;grep &quot;$1&quot; in.bed | bedops --merge - &quot;); print $0; }'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We take each pair of alternating lines and re-linearize them with &lt;code&gt;paste&lt;/code&gt;. This result now contains four columns: the three columns of each merged interval, and the ID-needle. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;We then use &lt;code&gt;sed&lt;/code&gt; to replace underscores with tabs, so that we turn the ID-needle back into three, tab-separated ID-score-strand columns:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;paste -d &quot;\t&quot; - - | sed 's/_/\t/g'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The output is now a six-column BED file, but it is ordered by the sort order we applied on ID-needles further up the pipeline, which we don't want. What we really want is BED that is sorted per BEDOPS &lt;code&gt;sort-bed&lt;/code&gt;, so that we can do more set operations and get a correct result. So we pipe this to &lt;code&gt;sort-bed -&lt;/code&gt; to write a sorted file to &lt;code&gt;answer.bed&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sort-bed - &amp;gt; answer.bed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="776" LastEditorUserId="776" LastEditDate="2017-08-11T16:51:49.820" LastActivityDate="2017-08-11T16:51:49.820" CommentCount="1" />
  <row Id="2274" PostTypeId="1" CreationDate="2017-08-10T22:22:01.717" Score="1" ViewCount="34" Body="&lt;p&gt;Let's say I have the genome &lt;code&gt;hg19&lt;/code&gt; loaded into R via BSGenome&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(&quot;BSgenome&quot;)    &#xA;hg19genome = getBSgenome('BSgenome.Hsapiens.UCSC.hg19', masked=FALSE)   &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I then have a list of SNPs loaded as a GRanges object, &lt;code&gt;gr&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(GenomicRanges)&#xA;&#xA; &amp;gt; gr&#xA; GRanges object with 212 ranges and 3 metadata columns:                                                                                                                                                                                                                                                                                                                                                &#xA;        seqnames               ranges strand |     width     REF    ALT                                                                                                                                                                                                                                                                                                                             &#xA;           &amp;lt;Rle&amp;gt;          v &amp;lt;IRanges&amp;gt;  &amp;lt;Rle&amp;gt; | &amp;lt;numeric&amp;gt;                                                                                                                                                                                                                                                                                                                        &#xA;    [1]        1 [86099032, 86099032]      * |         1     C      T                                                                                                                                                                                                                                                                                                                  &#xA;    [2]        1 [86099033, 86099033]      * |         1     C      A                                                                                                                                                                                                                                                                                                                   &#xA;    [3]        1 [86099199, 86099199]      * |         1     G      A  &#xA;  ....&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a straightforward way to inject these SNPs into &lt;code&gt;hg19genome&lt;/code&gt;?&lt;/p&gt;&#xA;" OwnerUserId="146" LastEditorUserId="146" LastEditDate="2017-08-10T22:37:31.110" LastActivityDate="2017-08-11T13:12:29.063" Title="Given a Genomic Ranges of SNPs, how to inject these SNPs in genome via BSGenome?" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;reference-genome&gt;&lt;snp&gt;&lt;biostrings&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="2275" PostTypeId="1" AcceptedAnswerId="2277" CreationDate="2017-08-10T22:44:40.640" Score="6" ViewCount="62" Body="&lt;p&gt;The problem: I have a VCF file, a reference genome, and a bunch of annotations for the reference (genes, repeat regions, etc.) as GFF or BED files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I would like is a tool that takes all of this as input and outputs a tab- or comma-delimited table containing as much information as possible. Potential columns in the output include:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Type of variant (SNV, structural, etc)&lt;/li&gt;&#xA;&lt;li&gt;Details of variant (e.g. reference base, variant base, coverage, position, etc)&lt;/li&gt;&#xA;&lt;li&gt;Annotations overlapping variant&lt;/li&gt;&#xA;&lt;li&gt;Annotations near variant (e.g. is it just upstream of a gene)&lt;/li&gt;&#xA;&lt;li&gt;If it appears in a coding region, does it change the amino acid.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;There are many tools that do &lt;em&gt;something like this&lt;/em&gt;. But to the newbie (like me) it is not clear which tools are worth starting with. Since the majority of tools take a little effort to get working in the first place, my question is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What tool comes closest to doing what I have described, is known to reliable, and is likely to be maintained for the next few years.&lt;/p&gt;&#xA;" OwnerUserId="156" LastActivityDate="2017-08-14T04:20:37.817" Title="Tools to create annotated table of variants from VCF" Tags="&lt;annotation&gt;&lt;vcf&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" />
  <row Id="2277" PostTypeId="2" ParentId="2275" CreationDate="2017-08-11T06:50:40.807" Score="4" Body="&lt;p&gt;I have used FEATnotator and I think it can provide all of the columns you would like to see. There are many output files generated, but the consolidated output has the following columns:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Chromosome     &lt;/li&gt;&#xA;&lt;li&gt;Position       &lt;/li&gt;&#xA;&lt;li&gt;Column_3     &lt;/li&gt;&#xA;&lt;li&gt;Consensus_Allele &lt;/li&gt;&#xA;&lt;li&gt;Annotation_Signature   &lt;/li&gt;&#xA;&lt;li&gt;Reference_Base &lt;/li&gt;&#xA;&lt;li&gt;Alternate_Base &lt;/li&gt;&#xA;&lt;li&gt;Transition/Transversion SNP_Type       &lt;/li&gt;&#xA;&lt;li&gt;Premature_STOP_Gained   STOP_Lost      &lt;/li&gt;&#xA;&lt;li&gt;START_CODON    &lt;/li&gt;&#xA;&lt;li&gt;STOP_CODON     &lt;/li&gt;&#xA;&lt;li&gt;SPLICE_SITE &lt;/li&gt;&#xA;&lt;li&gt;InterGenic     &lt;/li&gt;&#xA;&lt;li&gt;Gene_Body      &lt;/li&gt;&#xA;&lt;li&gt;Intron &lt;/li&gt;&#xA;&lt;li&gt;Exon &lt;/li&gt;&#xA;&lt;li&gt;Coding &lt;/li&gt;&#xA;&lt;li&gt;UTR    &lt;/li&gt;&#xA;&lt;li&gt;Transcription_Start_Site      &lt;/li&gt;&#xA;&lt;li&gt;Nearest_gene   &lt;/li&gt;&#xA;&lt;li&gt;Distance&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;here is some example output records from using a VCF, reference genome and GFF annotation file (sorry about the crappy formatting...there are a lot of fields!):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Chr01   28177   T       G       NA      T     G TRANSVERSION    NA      NO      NO      NO      NO      NO      YES     NO      NO      NO      NO      NO      NO      Eucgr.A00211.v2.0       16054&#xA;Chr01   44876   C       G       gene:Eucgr.A00211.v2.0   mRNA:Eucgr.A00211.1.v2.0[intron]       C       G       TRANSVERSION    NA      NO      NO      NO      NO      NO      NO      YES     YES     NO      NO      NO      NO      NA  NA&#xA;Chr01   46819   A       G       gene:Eucgr.A00211.v2.0   mRNA:Eucgr.A00211.1.v2.0   CDS:Eucgr.A00211.1.v2.0.CDS.4[AAA - K =&amp;gt; GAA - E (MISSENSE)]   exon:Eucgr.A00211.1.v2.0.exon.4      A       G       TRANSITION      MISSENSE        NO  NO       NO      NO      NO      NO      YES   NO        YES     YES     NO      NO      NA      NA&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1281" LastEditorUserId="298" LastEditDate="2017-08-11T13:28:51.170" LastActivityDate="2017-08-11T13:28:51.170" CommentCount="0" />
  <row Id="2278" PostTypeId="2" ParentId="2275" CreationDate="2017-08-11T07:39:25.050" Score="3" Body="&lt;p&gt;snpEff is a great tool for annotating VCF files and you can add custom reference sequences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://snpeff.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://snpeff.sourceforge.net/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Guide to add custom annotation files in snpEff&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://gatkforums.broadinstitute.org/gatk/discussion/50/adding-genomic-annotations-using-snpeff-and-variantannotator&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://gatkforums.broadinstitute.org/gatk/discussion/50/adding-genomic-annotations-using-snpeff-and-variantannotator&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are a bunch of pre curated annotation datasets available in their database.&lt;/p&gt;&#xA;" OwnerUserId="206" LastActivityDate="2017-08-11T07:39:25.050" CommentCount="0" />
  <row Id="2279" PostTypeId="1" AcceptedAnswerId="2280" CreationDate="2017-08-11T07:44:08.053" Score="4" ViewCount="60" Body="&lt;p&gt;I've annotated VCF files using snpEff and looking for a tool or script to parse the VCF file and clean up the file to make it interpretable for a biologist. &lt;/p&gt;&#xA;" OwnerUserId="206" LastActivityDate="2017-08-11T08:04:59.083" Title="Tool or script to parse annotated VCF files" Tags="&lt;variant-calling&gt;&lt;snp&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="2280" PostTypeId="2" ParentId="2279" CreationDate="2017-08-11T08:04:59.083" Score="4" Body="&lt;p&gt;(edit)you can filter the VCF annotations with &lt;strong&gt;snpsift&lt;/strong&gt;, I've also written a &lt;strong&gt;VcfFilterSequenceOntology&lt;/strong&gt; &lt;a href=&quot;http://lindenb.github.io/jvarkit/VcfFilterSequenceOntology.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://lindenb.github.io/jvarkit/VcfFilterSequenceOntology.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've written vcf2table: &lt;a href=&quot;http://lindenb.github.io/jvarkit/VcfToTable.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://lindenb.github.io/jvarkit/VcfToTable.html&lt;/a&gt;&#xA;It decodes VEP and SNPeff annotations:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;chr1/10001/T (n 1)&#xA; Variant&#xA; +--------+--------------------+&#xA; | Key    | Value              |&#xA; +--------+--------------------+&#xA; | CHROM  | chr1               |&#xA; (....)&#xA; VEP&#xA; +--------------------------+------+----------------+------------+-----------------+--------+------------------+-----------------------------------------------+-------------+---------+-----------------+----------------------+&#xA; | PolyPhen                 | EXON | SIFT           | ALLELE_NUM | Gene            | SYMBOL | Protein_position | Consequence                                   | Amino_acids | Codons  | Feature         | BIOTYPE              |&#xA; +--------------------------+------+----------------+------------+-----------------+--------+------------------+-----------------------------------------------+-------------+---------+-----------------+----------------------+&#xA; | probably_damaging(0.956) | 8/9  | deleterious(0) | 1          | ENSG00000102967 | DHODH  | 346/395          | missense_variant                              | R/W         | Cgg/Tgg | ENST00000219240 | protein_coding       |&#xA; |                          | 3/4  |                | 1          | ENSG00000102967 | DHODH  |                  | non_coding_exon_variant&amp;amp;nc_transcript_variant |             |         | ENST00000571392 | retained_intron      |&#xA; |                          |      |                | 1          | ENSG00000102967 | DHODH  |                  | downstream_gene_variant                       |             |         | ENST00000572003 | retained_intron      |&#xA; |                          |      |                | 1          | ENSG00000102967 | DHODH  |                  | downstream_gene_variant                       |             |         | ENST00000573843 | retained_intron      |&#xA; |                          |      |                | 1          | ENSG00000102967 | DHODH  |                  | downstream_gene_variant                       |             |         | ENST00000573922 | processed_transcript |&#xA; |                          |      |                | 1          | ENSG00000102967 | DHODH  | -/193            | intron_variant                                |             |         | ENST00000574309 | protein_coding       |&#xA; | probably_damaging(0.946) | 8/9  | deleterious(0) | 1          | ENSG00000102967 | DHODH  | 344/393          | missense_variant                              | R/W         | Cgg/Tgg | ENST00000572887 | protein_coding       |&#xA; +--------------------------+------+----------------+------------+-----------------+--------+------------------+-----------------------------------------------+-------------+---------+-----------------+----------------------+&#xA; Genotypes&#xA; +---------+------+-------+----+----+-----+---------+&#xA; | Sample  | Type | AD    | DP | GQ | GT  | PL      |&#xA; +---------+------+-------+----+----+-----+---------+&#xA; | M10475  | HET  | 10,2  | 15 | 10 | 0/1 | 25,0,10 |&#xA; | M10478  | HET  | 10,4  | 16 | 5  | 0/1 | 40,0,5  |&#xA; | M10500  | HET  | 10,10 | 21 | 7  | 0/1 | 111,0,7 |&#xA; | M128215 | HET  | 15,5  | 24 | 0  | 0/1 | 49,0,0  |&#xA; +---------+------+-------+----+----+-----+---------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="71" LastActivityDate="2017-08-11T08:04:59.083" CommentCount="0" />
  <row Id="2281" PostTypeId="2" ParentId="2274" CreationDate="2017-08-11T13:12:29.063" Score="1" Body="&lt;p&gt;This seems relatively complicated given the structure of a BSGenome object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The creator of the package answered this question previously on the Bioconductor support forums:&#xA;&lt;a href=&quot;https://support.bioconductor.org/p/86665/#86757&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://support.bioconductor.org/p/86665/#86757&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;We don't provide an easy way to inject arbitrary SNPs in an arbitrary&#xA;  BSgenome at the moment. However, it should not be too hard to forge&#xA;  the BSgenome ... package. First you would need to&#xA;  compute the sequences of the mutated chromosomes (you can&#xA;  use replaceLetterAt for this), then write them to a 2bit file (put&#xA;  them in a DNAStringSet object and call rtracklayer::export on it),&#xA;  then use that 2bit file to forge the BSgenome ....&#xA;  package (see the BSgenomeForge vignette in the BSgenome package for&#xA;  how to do this).&#xA;  - Herve Pages&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Here is another similar question with more info about replaceLetterAt: &lt;a href=&quot;https://support.bioconductor.org/p/26199/&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://support.bioconductor.org/p/26199/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="383" LastActivityDate="2017-08-11T13:12:29.063" CommentCount="2" />
  <row Id="2282" PostTypeId="1" CreationDate="2017-08-11T18:47:12.110" Score="3" ViewCount="63" Body="&lt;p&gt;After reading some of the forum posts in Biostar and SeqAnswers I find it very confusing whether to filter out the duplicate reads from aligned files or not. As far I understand it's very difficult to distinguish between highly expressed genes and duplicate reads and we may lose important information during the filtration process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, is it really necessary to remove the duplicates in differential expression analysis using RNA-seq data?&lt;/p&gt;&#xA;" OwnerUserId="206" LastEditorUserId="96" LastEditDate="2017-08-18T04:38:32.127" LastActivityDate="2017-08-18T04:38:32.127" Title="Removing PCR duplicates in RNA-seq Analysis" Tags="&lt;rna-seq&gt;&lt;differential-expression&gt;&lt;quality-control&gt;" AnswerCount="2" CommentCount="4" FavoriteCount="0" />
  <row Id="2283" PostTypeId="2" ParentId="2282" CreationDate="2017-08-11T21:08:06.510" Score="2" Body="&lt;p&gt;Generally you should just leave them as is. One does remove/mark duplicates in DNA seq. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For further read check this &lt;a href=&quot;https://www.nature.com/articles/srep25533&quot; rel=&quot;nofollow noreferrer&quot;&gt;Nature paper &lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="315" LastActivityDate="2017-08-11T21:08:06.510" CommentCount="2" />
  <row Id="2284" PostTypeId="2" ParentId="998" CreationDate="2017-08-12T21:00:41.833" Score="4" Body="&lt;p&gt;I am the developer of Uberon and I would be happy to help you with what you need, either from Uberon, or from the FMA.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You mentioned you need something simpler than FMA. There are a variety of tools for creating custom subsets. Additionally, some ontologies provide ready-made subsets for particular purposes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, we make a subset called 'basic.obo' from Uberon:&#xA;&lt;a href=&quot;http://uberon.github.io/downloads.html#subsets&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://uberon.github.io/downloads.html#subsets&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can examine this directly in either oboedit or Protege5 (note Protege5 parses obo so you don't need to convert).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From your question, you didn't indicate you needed programmatic access, but if you do there are a variety of options, as well as more programmer-friendly JSON exports.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most ontologies have a number of different options for web-browsing, you can see some of the options for Uberon here:&#xA;&lt;a href=&quot;http://obofoundry.org/ontology/uberon.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://obofoundry.org/ontology/uberon.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You mentioned annotating disease sites. There may be a tools (either desktop or web-based) for helping with this - e.g. providing autocomplete over a desired subset of an ontology.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You may also want to ask your question on a mailing list like obo-discuss. Unfortunately many ontology developers don't read stackexchange, but most are always keen to hear of community requirements, to help them provide better and more useful products.&lt;/p&gt;&#xA;" OwnerUserId="1322" LastActivityDate="2017-08-12T21:00:41.833" CommentCount="0" />
  <row Id="2285" PostTypeId="5" CreationDate="2017-08-12T21:23:24.850" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-08-12T21:23:24.850" LastActivityDate="2017-08-12T21:23:24.850" CommentCount="0" />
  <row Id="2286" PostTypeId="4" CreationDate="2017-08-12T21:23:24.850" Score="0" Body="Ontologies are collections of terms organized in a graph that can be used to annotate biological data, for example, tagging genes or variants with terms from disease or phenotype ontologies. The most well known bio ontology is the Gene Ontology (GO), used for functional annotation of genes. Sites such as the OBO Foundry and Bioportal can be used to find bio-ontologies." OwnerUserId="1322" LastEditorUserId="1322" LastEditDate="2017-08-13T07:14:05.840" LastActivityDate="2017-08-13T07:14:05.840" CommentCount="0" />
  <row Id="2287" PostTypeId="5" CreationDate="2017-08-12T21:25:50.737" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2017-08-12T21:25:50.737" LastActivityDate="2017-08-12T21:25:50.737" CommentCount="0" />
  <row Id="2288" PostTypeId="4" CreationDate="2017-08-12T21:25:50.737" Score="0" Body="The Gene Ontology (GO) is a collection of annotations of genes from multiple species to terms describing the functional role of those genes. The terms are organized in a graph structure that allows for propagation of information. See http://geneontology.org" OwnerUserId="1322" LastEditorUserId="1322" LastEditDate="2017-08-13T07:13:57.537" LastActivityDate="2017-08-13T07:13:57.537" CommentCount="0" />
  <row Id="2289" PostTypeId="1" CreationDate="2017-08-12T22:46:42.073" Score="3" ViewCount="50" Body="&lt;p&gt;I'd like to know how common certain mutation &lt;em&gt;types&lt;/em&gt; are in public data sets like the 1000 Genomes, ExAC, and ESP6500. Specifically, I'd like to know the distribution of stop-gains, stop-losses, frameshift, and other mutation types. For example, what is the median number of stop-gains observed across individuals in the ExAC population? I'd like to have the full distribution, but median values would work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be clear, I am &lt;em&gt;not&lt;/em&gt; looking for allele frequencies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I didn't find this kind of summary data when I looked at the respective websites. Is there a good source for these summary data?&lt;/p&gt;&#xA;" OwnerUserId="136" LastActivityDate="2017-08-12T22:46:42.073" Title="Where can I find summary data for how common certain mutation *types* are?" Tags="&lt;public-databases&gt;&lt;1000-genomes&gt;&lt;exac&gt;&lt;esp6500&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="2290" PostTypeId="2" ParentId="2234" CreationDate="2017-08-13T21:44:02.207" Score="-1" Body="&lt;p&gt;This weekend, I  installed the R language packages rapidGSEA and the Broad Institute GSEA , the Nvidia CUDA Toolkit 6.5-- Custom installation option  and MINGW makefiles on my son's Windows 8.1 notebook computer. &#xA;  This step was very tricky to accomplish because I had to maneuver around the &quot;NVIDIA Installer cannot continue. The NVIDIA graphics driver is not compatible with this version of Windows&quot; installation error message which prevents the NVIDIA compilers from being installed correctly.&#xA;  My objective is to run rapidGSEA without the CUDA enhancements since I am not using a computer with a NVIDIA GPU.&#xA;  I did this step because rapidGSEA and Broad Institute GSEA have a significantly richer set of gene set enrichment analytics than fgseaL as shown below:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The GSEA method takes five default arguments and three optional arguments&#xA;GSEA &amp;lt;- function(exprsData, labelList, geneSets, numPermutations, metricString,&#xA;                 dumpFileName=&quot;&quot;, checkInput=TRUE, doublePrecision=FALSE) {...}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;exprsData, labelList and geneSets refer to the data obtained in the previous section. numPermutations denotes the number of permutations in the resampling test, metricString denotes the local ranking measure (one of the following):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;naive_diff_of_classes&#xA;naive_ratio_of_classes&#xA;naive_log2_ratio_of_classes&#xA;stable_diff_of_classes&#xA;stable_ratio_of_classes&#xA;stable_log2_ratio_of_classes&#xA;onepass_signal2noise&#xA;onepass_t_test&#xA;twopass_signal2noise&#xA;twopass_t_test&#xA;stable_signal2noise&#xA;stable_t_test&#xA;overkill_signal2noise&#xA;overkill_t_test&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1283" LastEditorUserId="1283" LastEditDate="2017-08-13T21:52:33.080" LastActivityDate="2017-08-13T21:52:33.080" CommentCount="3" />
  <row Id="2291" PostTypeId="2" ParentId="2265" CreationDate="2017-08-13T22:27:07.477" Score="2" Body="&lt;p&gt;You can do this easily with &lt;a href=&quot;https://hail.is&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hail&lt;/a&gt;.  Hail primarily uses BED files to annotate genetic datasets (see the last &lt;a href=&quot;https://hail.is/docs/stable/hail.VariantDataset.html#hail.VariantDataset.annotate_variants_table&quot; rel=&quot;nofollow noreferrer&quot;&gt;annotate_variants_table&lt;/a&gt; example), but you can manipulate BED files using Hail's general facilities for manipulating delimited text files.  For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ cat genes.bed&#xA;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   3746    3909    &quot;WBGene00023193&quot;    .   -&#xA;I   4118    4220    &quot;WBGene00022277&quot;    .   -&#xA;I   4118    4358    &quot;WBGene00022277&quot;    .   -&#xA;I   4118    10230   &quot;WBGene00022277&quot;    .   -&#xA;I   4220    4223    &quot;WBGene00022277&quot;    .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The Hail script (python code):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from hail import *&#xA;hc = HailContext()&#xA;(hc&#xA; .import_table('genes.bed', impute=True, no_header=True)&#xA; .aggregate_by_key('f0 = f0, f3 = f3',&#xA;    'f1 = f1.min(), f2 = f2.max(), f4 = &quot;.&quot;, f5 = &quot;-&quot;')&#xA; .select(['f0', 'f1', 'f2', 'f3', 'f4', 'f5'])&#xA; .export('genes_merged.bed', header=False))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The result:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ cat genes_merged.bed &#xA;I   3746    3909    WBGene00023193  .   -&#xA;I   4118    10230   WBGene00022277  .   -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I aggregate over chrom and name so this solution won't merge entries on different chromosomes. The &lt;code&gt;select&lt;/code&gt; is necessary to reorder the fields because &lt;code&gt;aggregate_by_key&lt;/code&gt; places the keys being aggregated over first.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Disclosure: I work on Hail.&lt;/p&gt;&#xA;" OwnerUserId="1024" LastActivityDate="2017-08-13T22:27:07.477" CommentCount="0" />
  <row Id="2292" PostTypeId="2" ParentId="2275" CreationDate="2017-08-14T04:20:37.817" Score="5" Body="&lt;p&gt;&lt;a href=&quot;https://hail.is&quot; rel=&quot;noreferrer&quot;&gt;Hail&lt;/a&gt; might be an option for you.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is actively developed by a growing team at the Broad.  It is rigorously tested (continuous integration, continuous deployment, bug reports get regression tests, blah blah blah).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It was designed to solve this problem (among others).  It can import a variety of formats, including VCF, TSV, UCSC BED, JSON and interval files.  (We don't have explicit support for GFF, but we can probably handle them with general facilities.  If not, get in touch and we'll add support.)  It can call out to VEP (and soon Nirvana, Illumina's VEP rewrite).  It has general facilities to transform, filter, clean and query data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What's more, we've curated a large collection of annotation resources (currently ~22 databases of annotations + VEP), hosted in a public bucket in Google cloud, and built an interactive query builder to select which resources you want to use.  Get in touch if you'd like us to add additional resources.  You can read more about it &lt;a href=&quot;https://hail.is/docs/stable/annotationdb.html&quot; rel=&quot;noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Disclaimer: I work on Hail.&lt;/p&gt;&#xA;" OwnerUserId="1024" LastActivityDate="2017-08-14T04:20:37.817" CommentCount="1" />
  <row Id="2293" PostTypeId="1" AcceptedAnswerId="2325" CreationDate="2017-08-14T08:33:43.183" Score="4" ViewCount="87" Body="&lt;p&gt;Is the mappability of the centromeres in the GRCh38 genome reference similar to each other?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I can remember when GRCh38 came out, the sequence of the centromeres was determined by a combination of sequencing data and software prediction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given the way the sequence of the centromeres has been determined, should we expect Illumina 2x150bp (or shorter, 2x75bp) reads to map relatively equally to all centromere sequences?&lt;/p&gt;&#xA;" OwnerUserId="180" LastActivityDate="2017-08-18T04:54:39.033" Title="GRCh38 centromeres mappability" Tags="&lt;illumina&gt;&lt;grch38&gt;&lt;centromere&gt;&lt;mappability&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2294" PostTypeId="2" ParentId="2043" CreationDate="2017-08-14T08:53:07.083" Score="2" Body="&lt;p&gt;You can use &lt;code&gt;finditer&lt;/code&gt; from python's &lt;a href=&quot;https://docs.python.org/2/library/re.html#module-re&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;re&lt;/code&gt;&lt;/a&gt; module. The advantage of this approach is it allows for getting the indices of those matches, which could be handy down the track.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import re&#xA;&amp;gt;&amp;gt;&amp;gt; matches = re.finditer(r'(?=(AA))', 'AAAA')&#xA;&amp;gt;&amp;gt;&amp;gt; indices = [match.span(1) for match in matches]&#xA;&amp;gt;&amp;gt;&amp;gt; indices&#xA;[(0, 2), (1, 3), (2, 4)]&#xA;&amp;gt;&amp;gt;&amp;gt; num_matches = len(indices)&#xA;&amp;gt;&amp;gt;&amp;gt; num_matches&#xA;3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="149" LastEditorUserId="149" LastEditDate="2017-08-14T08:58:49.933" LastActivityDate="2017-08-14T08:58:49.933" CommentCount="0" />
  <row Id="2295" PostTypeId="2" ParentId="2282" CreationDate="2017-08-14T10:33:13.743" Score="5" Body="&lt;p&gt;For normal RNA-seq PCR duplicates are normally kept in, but the duplication rate can be used as a quality control: The higher the duplication rate, the lower the quality. For expression analysis, it is probably best to discard high duplication rate samples, rather than deduplicate them. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, the smaller the amount of RNA input into the library prepartion the worst the duplication. Many protocols for very low input quantities (such as single cell) include random barcodes called UMIs (Unique Molecular Identifiers). These allow PCR duplicates to be distinguished from genuinely independent molecules that just happen to have the sample mapping position. &lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-08-14T10:33:13.743" CommentCount="1" />
  <row Id="2297" PostTypeId="2" ParentId="2293" CreationDate="2017-08-14T10:49:09.487" Score="3" Body="&lt;p&gt;I doubt it, unless you are asking if mapping would be similarly bad for all centromeres. Here are some repetitive structures (probably not centromeric) that I've found in the nanopore reads for &quot;human&quot; sample NA12878, produced by the &lt;a href=&quot;https://github.com/nanopore-wgs-consortium/NA12878&quot; rel=&quot;nofollow noreferrer&quot;&gt;nanopore-WGS consortium&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/2D3cq.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/2D3cq.jpg&quot; alt=&quot;Repetitive human reads #1&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These structures are consistent in that they repeat lots of times, but the internal patterns can be quite different. Here are a few more:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/SU65U.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/SU65U.jpg&quot; alt=&quot;Repetitive human reads #2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given that centromeres need to be uniquely linkable to a single chromosome, it would make sense to me if the centromere internal structure is unique to each chromosome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's possible to have highly-repetitive structures which don't map to each other. While I haven't dug too deeply into the human reads, I've looked at the 5 most-compressible regions in an assembled rodent parasite (&lt;em&gt;Nippostrongylus brasiliensis&lt;/em&gt;) genome, and found no internal similarities between them:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/GtvQz.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/GtvQz.jpg&quot; alt=&quot;Nippo most-compressible regions&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the issues with assembly from Illumina reads is that these highly-repetitive regions get collapsed into a single repeat (or at best a region up to twice the fragment length). With internal repeat units that have identity of over 98%, assembling the true sequence is very difficult, even with an exact knowledge of paired read separation. Even if this were possible, it can be impossible to correctly place a read because multiple internal units could be identical (or similarly different) to the sequenced read.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-08-14T10:49:09.487" CommentCount="0" />
  <row Id="2298" PostTypeId="1" AcceptedAnswerId="2299" CreationDate="2017-08-14T19:51:21.827" Score="2" ViewCount="51" Body="&lt;p&gt;What the difference between TPM and CPM when dealing with RNA seq data?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What metrics would you use if you have to perform some down stream analysis other than Differential expression for eg. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Clustering analysis using Hclust function and then plotting heat map to find differences in terms of expression levels, correlation and pca&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it wrong to use TPM for such analysis, if yes then when does one use TPM versus CPM. &lt;/p&gt;&#xA;" OwnerUserId="926" LastActivityDate="2017-08-15T09:45:02.523" Title="Difference between CPM and TPM and which one for downstream analysis?" Tags="&lt;rna-seq&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="2299" PostTypeId="2" ParentId="2298" CreationDate="2017-08-14T21:15:27.507" Score="5" Body="&lt;p&gt;You can find the various equations in &lt;a href=&quot;https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/&quot; rel=&quot;noreferrer&quot;&gt;this oft-cited blog post from Harold Pimental&lt;/a&gt;. Anyway, CPM is basically depth-normalized counts whereas TPM is length normalized (and then normalized by the length-normalized values of the other genes).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If one has to choose between those two choices one typically chooses TPM for most things, since generally the length normalization is handy. Realistically, you probably want &lt;code&gt;log(TPM)&lt;/code&gt; since otherwise noise in your most highly expressed genes end up driving everything.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-14T21:15:27.507" CommentCount="3" />
  <row Id="2300" PostTypeId="1" AcceptedAnswerId="2314" CreationDate="2017-08-15T07:11:42.977" Score="1" ViewCount="50" Body="&lt;p&gt;I am getting peptides using biomaRt library in R for the gene 'BRCA1'. 27 different aminoacid sequences are returned, and 12 of those have an asterisk (*) in the end, while 15 do not. What does that mean? I know that asterisk is used to indicate stop codon. So, does that mean the peptides without an asterisk do not have a stop codon? Is that possible at all? Are all of those valid peptides or not?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is the R code I used: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(biomaRt);&#xA;mart = useMart('ensembl', dataset='hsapiens_gene_ensembl');&#xA;seq = getSequence(id='BRCA1', type='hgnc_symbol', seqType='peptide', mart = mart);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;MDLSALRVEEVQNVINAMQKILECPICLELIKEPVSTKCDHIFCKVLLCCPSWSTVVRS*&lt;/code&gt; is an example sequence with an asterisk in the end. &lt;code&gt;MDLSALRVEEVQNVINAMQKILECPICLELIKEPVSTKCDHIFCKSLQESTRFSQLVEEL‌​LKIICAFQLDTGLEYANSYN‌​FAKKENNSPEHLKDEVSI&lt;/code&gt; is an example without an asterisk.&lt;/p&gt;&#xA;" OwnerUserId="1331" LastActivityDate="2017-08-16T08:49:48.813" Title="Why some of the gene peptides returned by biomaRt do not have an asterisk in the end?" Tags="&lt;gene&gt;&lt;proteins&gt;&lt;biomart&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="2301" PostTypeId="2" ParentId="2300" CreationDate="2017-08-15T07:25:59.873" Score="3" Body="&lt;p&gt;If you &lt;a href=&quot;http://www.ensembl.org/Homo_sapiens/Transcript/Summary?g=ENSG00000012048;r=17:43093586-43125364;t=ENST00000477152&quot; rel=&quot;nofollow noreferrer&quot;&gt;look at the original data at Ensembl&lt;/a&gt; you'll notice that most of these are labeled &quot;CDS 3' incomplete&quot; &lt;s&gt;and have a TSL (transcript support level) of 1, which is as low as it goes&lt;/s&gt;. It seems likely that that is simply an incomplete annotation. I'm not surprised that there are a bunch of these for BRCA1, since there was a long time when there were a LOT of legal issues surrounding working on it.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-08-15T09:11:38.693" LastActivityDate="2017-08-15T09:11:38.693" CommentCount="5" />
  <row Id="2302" PostTypeId="1" CreationDate="2017-08-15T08:15:32.233" Score="3" ViewCount="22" Body="&lt;p&gt;Say I have reads that overlap some genes that produce small RNAs, but I want only those reads that start at exactly the TSS of the loci. In other words, reads whose 5' end match the 5' end of a genomic feature. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&#xA;   5'....3'&#xA;...+++++++... Gene&#xA;...0000000... R1&#xA;...00000000.. R2&#xA;..000000000.. R3&#xA;..00000000... R4&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output should be R1 and R2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe this is a fairly common bioinformatics operation but somehow it doesn't seem to be an option in the general use tools I looked at (py-bedtools, samtools, bedops). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A solution I thought of would be:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;reduce the gene coordinates to its 5' end, and &lt;/li&gt;&#xA;&lt;li&gt;using bedtools &lt;code&gt;bedClosest&lt;/code&gt; to annotate the distance between reads and genes, and finally &lt;/li&gt;&#xA;&lt;li&gt;select reads that overlap (d=0) and write them to a (bam) file.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;This involves quite a bit of wrangling data with &lt;code&gt;pybedtools&lt;/code&gt; or &lt;code&gt;awk/bash&lt;/code&gt;, and I wonder is there is a more elegant solution to this. Solutions in &lt;code&gt;R/Bioconductor&lt;/code&gt; are also welcome.&lt;/p&gt;&#xA;" OwnerUserId="108" LastActivityDate="2017-08-15T08:33:27.510" Title="How to filter intervals (reads or genomic coordinates) that have the exact same 5' or 3' ends?" Tags="&lt;bedtools&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="0" />
  <row Id="2303" PostTypeId="2" ParentId="2302" CreationDate="2017-08-15T08:33:27.510" Score="1" Body="&lt;p&gt;Presuming you have deepTools installed:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;#/!usr/bin/env python&#xA;from deeptoolsintervals.parse import GTF&#xA;import pysam&#xA;&#xA;anno = GTF([&quot;some/gtf/or/bed/file&quot;, &quot;or/more/than/one/if you like/&quot;])&#xA;bam = pysam.AlignmentFile(&quot;something.bam&quot;)&#xA;for read in bam:&#xA;    s = read.reference_start&#xA;    if read.is_reverse:&#xA;        s = read.reference_end&#xA;    e = s + 1&#xA;    overlaps = anno.findOverlaps(read.reference_name s, e, matchType=4)&#xA;    # For a GTF, parse the results to see if this is the correct type.&#xA;    # For a BED3/BED6 file there will be no exons, so anything other than&#xA;    # None indicates an appropriate overlap&#xA;&#xA;    do something&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;deeptoolsintervals&lt;/code&gt; in the GTF/BED parsing and custom interval tree library used by deepTools. &lt;code&gt;anno&lt;/code&gt; holds the interval tree and the associated information (gene/transcript name or ID, exons, strand, etc.). the &lt;code&gt;matchType=4&lt;/code&gt; is the pivotal part since that specifies that only overlaps having the exact same start position should be included. Assuming you have a stranded library, you might instead do something like switch between &lt;code&gt;matchType=4&lt;/code&gt; (the same start position) and &lt;code&gt;matchType=5&lt;/code&gt; (the same end position) depending on the orientation of the alignment and the the &lt;code&gt;strand=&lt;/code&gt; of the gene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that you'll need to make a &lt;code&gt;set()&lt;/code&gt; to hold read names to find the mates (that or jump around the file to get them, which is probably slower).&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-15T08:33:27.510" CommentCount="0" />
  <row Id="2305" PostTypeId="2" ParentId="2298" CreationDate="2017-08-15T09:45:02.523" Score="1" Body="&lt;p&gt;Neither CPM nor TPM are well suited here, because neither performs robust cross-sample normalisation (see the blog post Devon linked to).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DESeq2 provides two robust log-space normalisation methods for downstream analysis, the &lt;em&gt;regularised log&lt;/em&gt; (&lt;code&gt;rlog&lt;/code&gt;), and the &lt;em&gt;variance stabilising transformation&lt;/em&gt; (&lt;code&gt;vst&lt;/code&gt;). The &lt;a href=&quot;http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#data-transformations-and-visualization&quot; rel=&quot;nofollow noreferrer&quot;&gt;DESeq2 vignette explains how to use these&lt;/a&gt; for things like hclust.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;On a more general note, CPM does not account for transcript length differences, while TPM does. If the choice is between TPM and CPM I would therefore use TPM. However, if you are only comparing the same transcripts across experiments, the transcript length is actually invariant so it doesn’t matter (but CPM is still not a good cross-experiment normalisation).&lt;/p&gt;&#xA;" OwnerUserId="29" LastActivityDate="2017-08-15T09:45:02.523" CommentCount="6" />
  <row Id="2306" PostTypeId="1" AcceptedAnswerId="2311" CreationDate="2017-08-15T10:46:20.913" Score="6" ViewCount="76" Body="&lt;p&gt;&lt;code&gt;fast5&lt;/code&gt; is a variant of &lt;a href=&quot;https://support.hdfgroup.org/HDF5/whatishdf5.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;HDF5&lt;/code&gt;&lt;/a&gt; the native format in which raw data from Oxford Nanopore MinION are provided. You can easily extract the reads in fast5 format into a standard fastq format, using for example &lt;a href=&quot;https://poretools.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;poretools&lt;/code&gt;&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say I have aligned these reads in &lt;code&gt;fastq&lt;/code&gt; format to an external reference genome, resulting in a &lt;code&gt;SAM&lt;/code&gt; file. Say I have then taken a subset of the  &lt;code&gt;SAM&lt;/code&gt; file, according to the &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMv1.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;bitwise flag&lt;/a&gt;, to include only the reads that map to the reference. With the read ID, I can then grep them out from the file containing the reads in &lt;code&gt;fastq&lt;/code&gt; format, generating a subset file in &lt;code&gt;fastq&lt;/code&gt; format containing only the IDs that have mapped to the reference.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now my question is, can we subset reads from the &lt;code&gt;fast5&lt;/code&gt; archive according to the list of mapping reads as taken from the file with reads in &lt;code&gt;fastq&lt;/code&gt; format? This is for educational purposes, so that we have a smaller starting archive, and the &lt;code&gt;fast5&lt;/code&gt; -&gt; &lt;code&gt;fastq&lt;/code&gt; extraction takes less cpu time. &lt;/p&gt;&#xA;" OwnerUserId="294" LastEditorUserId="149" LastEditDate="2017-08-16T08:52:58.910" LastActivityDate="2017-08-16T09:40:20.417" Title="How to convert fastq to fast5" Tags="&lt;nanopore&gt;&lt;software-recommendation&gt;&lt;fastq&gt;&lt;format-conversion&gt;&lt;fast5&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="0" />
  <row Id="2308" PostTypeId="1" CreationDate="2017-08-15T16:34:21.093" Score="6" ViewCount="30" Body="&lt;p&gt;I'm comparing the results that I obtain when doing a DE analysis with the Wald test and the likelihood-ratio test. One the thing that I've noticed is that there are many genes with 'beta' close to zero that are considered differentially expressed between the conditions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that the likelihood-ratio test does't use the beta values to calculate the p-values, but I find it strange that transcripts with similar expression values between the conditions are being considered differentially expressed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Volcano plot (Wald test q-values):&lt;/strong&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/ATPtp.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ATPtp.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Volcano plot (likelihood-ratio test q-values):&lt;/strong&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/hovqx.png&quot; rel=&quot;noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/hovqx.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1334" LastEditorUserId="96" LastEditDate="2017-08-18T04:41:53.457" LastActivityDate="2017-08-18T04:41:53.457" Title="Sleuth: transcripts with beta close to 0 are considered differentially expressed in a likelihood-ratio test" Tags="&lt;rna-seq&gt;&lt;differential-expression&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="2309" PostTypeId="2" ParentId="2306" CreationDate="2017-08-15T18:44:00.690" Score="2" Body="&lt;p&gt;I do this very frequently, using the read file name to identify FAST5 files associated with particular reads. If a FASTQ record includes the channel number and read number (and preferably the runID as well), then I use that information to find the associated FAST5 read.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If reads are called with Albacore, then the &lt;code&gt;sequencing_summary.txt&lt;/code&gt; file has additional information. The first column is the exact name of the file that was called, the second column is the read ID of the FASTQ sequence, and the third column is the run ID associated with that read ID. This is much easier to work with, but does require calling reads with Albacore (which, admittedly, is what seems to be producing the best results at the moment).&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-08-16T07:38:17.043" LastActivityDate="2017-08-16T07:38:17.043" CommentCount="3" />
  <row Id="2310" PostTypeId="1" AcceptedAnswerId="2312" CreationDate="2017-08-16T01:27:43.727" Score="2" ViewCount="29" Body="&lt;p&gt;In my RNAseq dataset of differentiated stem-cell lines, some samples have far fewer significantly differentially expressed genes than others. QC shows that this is because there are way fewer reads for one sample than the others.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can gene co-expression networks be used to infer differential expression? For example, if certain genes just miss the cutoff q-value cutoff  If so, what papers have done so? &lt;/p&gt;&#xA;" OwnerUserId="823" LastActivityDate="2017-08-16T08:27:56.623" Title="Can gene co-expression networks be used to help identify differentially expressed genes?" Tags="&lt;rna-seq&gt;&lt;differential-expression&gt;&lt;networks&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="2311" PostTypeId="2" ParentId="2306" CreationDate="2017-08-16T04:30:09.763" Score="4" Body="&lt;p&gt;This is something I have been meaning to get around to for a while, so thanks for the prompt.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have &lt;a href=&quot;https://github.com/mbhall88/fast5_in_ref&quot; rel=&quot;nofollow noreferrer&quot;&gt;created a python script&lt;/a&gt; to do what (I think) you're after. Yes, another Nanopore script. Because the world doesn't have enough of them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you have mentioned this is for educational purposes I have added a tonne of comments to the code too so I think you shouldn't have any issues following it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The docs on the &lt;a href=&quot;https://github.com/mbhall88/fast5_in_ref&quot; rel=&quot;nofollow noreferrer&quot;&gt;GitHub repo&lt;/a&gt; have all the info, but for those reading along at home&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/mbhall88/fast5_in_ref.git&#xA;cd fast5_in_ref &amp;amp;&amp;amp; chmod +x fast5_in_ref&#xA;./fast5_in_ref -i &amp;lt;fast5_dir&amp;gt; -r &amp;lt;in.fastq|in.bam|in.sam&amp;gt; -o &amp;lt;out.txt&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What it does is read in &lt;code&gt;&amp;lt;in.fastq|in.bam|in.sam&amp;gt;&lt;/code&gt; and extract the read id from each header. It then goes through all the fast5 files under &lt;code&gt;&amp;lt;fast_dir&amp;gt;&lt;/code&gt; and checks whether their read id is in the set of read ids from &lt;code&gt;&amp;lt;in.fastq|in.bam|in.sam&amp;gt;&lt;/code&gt;. If it is, the path to the file is written to it's own line in &lt;code&gt;&amp;lt;out.txt&amp;gt;&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If no output (&lt;code&gt;-o&lt;/code&gt;) is given, it will write the output to stdout.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So if you wanted to pipe these paths into another program, you could do something like&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mkdir subset_dir&#xA;./fast5_in_ref -i &amp;lt;fast5_dir&amp;gt; -r &amp;lt;in.fastq|in.bam|in.sam&amp;gt; | xargs cp -t subset_dir/&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The above example would copy the fast5 files that are found in your fastq/BAM/SAM to &lt;code&gt;subset_dir/&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:&#xA;I have now added compatibility for BAM and SAM files (was only fastq originally). These are potentially even more useful for this application than fastq. As a result I have updated all examples, plus the code and links on GitHub&lt;/p&gt;&#xA;" OwnerUserId="149" LastEditorUserId="149" LastEditDate="2017-08-16T09:40:20.417" LastActivityDate="2017-08-16T09:40:20.417" CommentCount="2" />
  <row Id="2312" PostTypeId="2" ParentId="2310" CreationDate="2017-08-16T06:41:26.923" Score="3" Body="&lt;p&gt;While you can use networks to find differentially expressed genes (see the &lt;a href=&quot;https://labs.genetics.ucla.edu/horvath/CoexpressionNetwork/Rpackages/WGCNA/&quot; rel=&quot;nofollow noreferrer&quot;&gt;WGCNA&lt;/a&gt; package, which does this) in my experience this ends up largely matching what you'd get using a traditional package with a looser threshold for significance. Given the time savings of traditional packages, there's rarely any gain to using networks (it won't hurt to try, just note that it'll take some time). If some very interesting genes are just above your significance threshold then change your threshold. You need to validate your findings in some way anyway, so your p-value threshold is partly just a way to protect you from wasting your time (but if background knowledge suggests that your time wouldn't be wasted...).&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-16T06:41:26.923" CommentCount="0" />
  <row Id="2313" PostTypeId="2" ParentId="2310" CreationDate="2017-08-16T08:27:56.623" Score="2" Body="&lt;p&gt;Co-expression network will give you an idea about the genes having similar expression patterns and the nodes will be decided on the basis of correlation scores and nothing much you will get from differential gene expression perspective.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would suggest you to try some other models for differential expression analysis like &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/baySeq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;baySeq&lt;/a&gt; , &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/DESeq2.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DESeq&lt;/a&gt;, &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/edgeR.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;edgeR&lt;/a&gt;, &lt;a href=&quot;https://www.bioconductor.org/packages/release/bioc/html/NOISeq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;NOIseq&lt;/a&gt;. Depending upon the model you use for analysis you might get some extra significant DEG. Also, try different significance cutoff and see how it's changing the analysis result to get the threshold that suits the data best.&lt;/p&gt;&#xA;" OwnerUserId="206" LastActivityDate="2017-08-16T08:27:56.623" CommentCount="0" />
  <row Id="2314" PostTypeId="2" ParentId="2300" CreationDate="2017-08-16T08:49:48.813" Score="6" Body="&lt;p&gt;If it's 3' incomplete that means the evidence used to create it was a fragment. Here's the evidence used to construct BRCA1-214 ENST00000477152.5, a 3' incomplete:&#xA;&lt;a href=&quot;http://www.ensembl.org/Homo_sapiens/Transcript/SupportingEvidence?db=core;g=ENSG00000012048;r=17:43093586-43125364;t=ENST00000477152&quot; rel=&quot;noreferrer&quot;&gt;http://www.ensembl.org/Homo_sapiens/Transcript/SupportingEvidence?db=core;g=ENSG00000012048;r=17:43093586-43125364;t=ENST00000477152&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can see that there's a full length cDNA from EMBL, AK307553.1, which was used to create this model:&#xA;&lt;a href=&quot;http://www.ebi.ac.uk/ena/data/view/AK307553.1&quot; rel=&quot;noreferrer&quot;&gt;http://www.ebi.ac.uk/ena/data/view/AK307553.1&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The sequence was mapped against the genome sequence to create the transcript model. When searching for the translation, it was found that the open reading frame was started, but there was no stop codon. This suggests that the cDNA AK307553.1 is actually a fragment, that the mRNA was broken or cleaved before it was reverse-transcribed sequenced. We display it in Ensembl in the hope that this will lead someone to identify the full length transcript that it represents.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many people choose not to work with these incomplete transcripts. You can do this by filtering by Gencode Basic in biomaRt.&lt;/p&gt;&#xA;" OwnerUserId="1332" LastActivityDate="2017-08-16T08:49:48.813" CommentCount="0" />
  <row Id="2316" PostTypeId="1" AcceptedAnswerId="2317" CreationDate="2017-08-17T02:33:01.107" Score="5" ViewCount="35" Body="&lt;p&gt;I am running a slow downstream analysis on a large set of nanopore reads (approx 3 million), and would like to split them into smaller chunks, run the analysis in massively parallel, and then recombine. Originally I just split the FASTQ into chunks, re-aligned each chunk, and then merged the output, but here I would like to use an existing alignment so I can compare results with existing analyses (i.e. the alignments must be the same).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I efficiently split a FASTQ file and a BAM file giving the alignment of the FASTA file into chunks, ensuring that all of the reads in FASTQ chunk 1 are in BAM chunk 1, vice versa and so on?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My FASTQ is approximately 45GB and my BAM is 33GB, so I would prefer to avoid storing one of the two files in memory if possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Here's some pseudocode of exactly what I'm trying to do:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# input: in.bam, in.fastq, chunk_size&#xA;i &amp;lt;- 0&#xA;for fastq_read in in.fastq:&#xA;    bam_read &amp;lt;- extract fastq_read.read_name from in.bam&#xA;    n &amp;lt;- i modulo chunk_size&#xA;    write fastq_read to out.n.fastq&#xA;    write bam_read to out.n.bam&#xA;    i &amp;lt;- i + 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I could swap the above to iterate through the bam file and fetch from the fastq if that's easier.&lt;/p&gt;&#xA;" OwnerUserId="163" LastEditorUserId="163" LastEditDate="2017-08-17T03:41:07.107" LastActivityDate="2017-08-17T13:00:35.147" Title="Split FASTQ and matching BAM into matching chunks" Tags="&lt;bam&gt;&lt;fasta&gt;&lt;nanopore&gt;&lt;fastq&gt;&lt;subset&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2317" PostTypeId="2" ParentId="2316" CreationDate="2017-08-17T07:33:10.510" Score="3" Body="&lt;p&gt;Hmm, it's hard to think of a super efficient way of doing this (assuming the files aren't ordered the same - if they are then this whole answer is basically redundant). And also assuming the read ids for both files aren't a perfect intersection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Off the top of my head you probably want to build a set of read ids for the fastq file and another for the bam. Some python code to get started on that:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pysam&#xA;import itertools&#xA;&#xA;def get_read_id_fastq(ref_path):&#xA;    &quot;&quot;&quot;Extracts the read ids from a fastq file.&quot;&quot;&quot;&#xA;    read_ids = set()&#xA;    with open(ref_path, 'r') as ref:&#xA;        for line in ref:&#xA;            if line.startswith('@'):  # i.e if line is header&#xA;                # split the line on spaces, take the first element, remove @&#xA;                read_id = line.split(' ')[0].replace('@', '')&#xA;                read_ids.add(read_id)&#xA;&#xA;    return read_ids&#xA;&#xA;def get_read_id_bam(ref_path):&#xA;    &quot;&quot;&quot;Extract the read ids from a BAM file.&quot;&quot;&quot;&#xA;&#xA;    read_ids = set()&#xA;    with pysam.AlignmentFile(ref_path, 'r', ignore_truncation=True) as ref:&#xA;        for read in ref:&#xA;            # query_name is the query template name&#xA;            read_ids.add(read.query_name)&#xA;&#xA;    return read_ids&#xA;&#xA;fastq_ids = get_read_id_fastq(fastq_path)&#xA;bam_ids = get_read_id_bam(bam_path)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then take the intersection of these two sets and that's your common read ids. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;common_ids = fastq_ids &amp;amp; bam_ids&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The next part will be a bit more involved. You will have to iterate through each file one at a time. I would suggest that for the first one you iterate through, create a running dictionary with the read id that was written as the key and the 'chuck number' it was written to as the value. You could create a cycle for your chunk size to manage this effectively&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;chunk_cycle = itertools.chain(*zip(range(chunk_size)*len(common_ids)))&#xA;write_idx = {}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The next part will probably require you to put in some hard-fought times getting it to work. I'll put in some rough pseudo-code to give you an idea.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for line in file:&#xA;    read_id = # get line read_id&#xA;    if read_id in common_ids:&#xA;        chunk_num = chunk_cycle.next()&#xA;        write_idx[read_id] = chunk_num&#xA;        file_to_write_to = 'out.{}.bam'.format(chunk_num)&#xA;        # open this file or write to it if already open&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When you go to do the next file when you find a read id is in the common set, you look up it's value in &lt;code&gt;write_idx&lt;/code&gt; and this will give you the chunk number to write to. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reads wont be in the exact same order between the two files, but you could sort later if you needed it (not sure it would matter?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hope this helps. Sorry I couldn't give you more, but this should give you a head start hopefully.&lt;/p&gt;&#xA;" OwnerUserId="149" LastEditorUserId="149" LastEditDate="2017-08-17T13:00:35.147" LastActivityDate="2017-08-17T13:00:35.147" CommentCount="2" />
  <row Id="2318" PostTypeId="1" AcceptedAnswerId="2319" CreationDate="2017-08-17T13:10:12.177" Score="3" ViewCount="16" Body="&lt;p&gt;I recently installed Ubuntu 16.04 (because I was still using 12.04). But it seems my bedtools scripts don't work properly anymore. I can't figure out how to use the new bedtools for my old ways. What I want to do is get the number of reads from a bam file, per interval from a bed file. It was very simple with my old version of Ubuntu and bedtools:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bedtools coverage -abam file.bam -b All_peaks.bed &amp;gt; file.cov.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But this doesn't seem to work anymore. I used to get small files as results (~3MB), but now huge files are created (~4GB) including read names??&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using bedtools v2.25 now.&lt;/p&gt;&#xA;" OwnerUserId="939" LastActivityDate="2017-08-17T13:21:05.450" Title="How to count reads in bam per bed interval with bedtools" Tags="&lt;bam&gt;&lt;bedtools&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2319" PostTypeId="2" ParentId="2318" CreationDate="2017-08-17T13:21:05.450" Score="3" Body="&lt;p&gt;The order of &lt;code&gt;-a&lt;/code&gt; and &lt;code&gt;-b&lt;/code&gt; switched at some point. You want:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bedtools coverage -a All_peaks.bed -b file.bam &amp;gt; file.cov.txt&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For reference, this is the end of the help output in version 2.25:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Default Output:  &#xA;     After each entry in A, reports: &#xA;       1) The number of features in B that overlapped the A interval.&#xA;       2) The number of bases in A that had non-zero coverage.&#xA;       3) The length of the entry in A.&#xA;       4) The fraction of bases in A that had non-zero coverage.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And this is the equivalent output from version 2.19:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Default Output:  &#xA;     After each entry in B, reports: &#xA;       1) The number of features in A that overlapped the B interval.&#xA;       2) The number of bases in B that had non-zero coverage.&#xA;       3) The length of the entry in B.&#xA;       4) The fraction of bases in B that had non-zero coverage.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: The change in behavior happened in version 2.24:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;We have changed the behavior of the coverage tool such that it is consistent with the other tools. Specifically, coverage is now&#xA;  computed for the intervals in the A file based on the overlaps with the B file, rather than vice versa.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-17T13:21:05.450" CommentCount="1" />
  <row Id="2321" PostTypeId="1" CreationDate="2017-08-17T16:42:31.383" Score="0" ViewCount="7" Body="&lt;p&gt;I have not found any work which investigates assessment of differences in levels of secreted proteins by taking advantage of differential expression of the genes which mediate the secretory pathway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, suppose the gene whose product binds specifically to a certain signal peptide is down-regulated. I would assume that this implies a down-regulation in all the levels of secreted proteins to which it binds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is such an analysis possible to date? If so, where? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;" OwnerUserId="823" LastActivityDate="2017-08-17T16:42:31.383" Title="Using signal peptide and the expression levels of signal recognition particle in secretome analysis" Tags="&lt;proteins&gt;&lt;differential-expression&gt;&lt;networks&gt;&lt;interactions&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2322" PostTypeId="2" ParentId="2201" CreationDate="2017-08-17T19:47:04.523" Score="1" Body="&lt;p&gt;Here is my solution to the problem. I am posting it here in case anyone else come up with the same idea but did not know how to formulate it in mathematical notation!&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;$$ \psi_i = \sum_{i=1}^9 S_i $$&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;where $S_i$ is the score function and defined as:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;$$&#xA;S_i = \cases{&#xA;  0 &amp;amp; \text{if $\theta(x_i) &amp;lt; Q_1$ or $\theta(x_i) = $ benign/neutral} \\&#xA;  0.5 &amp;amp; \text{if $Q_1 \leq \theta(x_i) &amp;lt; Q_3$ or $\theta(x_i) = $ possibly damaging/uncertain} \\&#xA;  1 &amp;amp; \text{if $\theta(x_i) \geq Q_3$ or $\theta(x_i) = $ damaging}&#xA;}&#xA;$$&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The $\theta(x_i)$ is the pathogenicity or conservation score for variant $x$ as defined by model $i$ and $Q$ denotes the quartile range for scores from M-CAP, CADD, GERP and Phylop models.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="1250" LastEditorUserId="29" LastEditDate="2017-08-18T17:39:08.600" LastActivityDate="2017-08-18T17:39:08.600" CommentCount="1" />
  <row Id="2323" PostTypeId="1" CreationDate="2017-08-18T00:23:15.570" Score="4" ViewCount="34" Body="&lt;p&gt;I'm trying to confirm that the sequence of a novel gene is derived by exon shuffling between several different genes. I have the promoter sequence, gene sequence, and mRNA (with defined exon/intro boundaries). I've tried performing several database searches with the sequences, but each search produces a set of different unrelated hits. How would one go about using this info to confirm the exon shuffling hypothesis? Any tips are appreciated. Thanks!&lt;/p&gt;&#xA;" OwnerUserId="1343" LastEditorUserId="96" LastEditDate="2017-08-18T04:32:55.503" LastActivityDate="2017-08-18T04:32:55.503" Title="How to confirm exon shuffling in a gene?" Tags="&lt;sequencing&gt;&lt;sequence-analysis&gt;&lt;molecular-genetics&gt;" AnswerCount="0" CommentCount="6" FavoriteCount="0" />
  <row Id="2324" PostTypeId="1" CreationDate="2017-08-18T03:12:20.163" Score="1" ViewCount="48" Body="&lt;p&gt;I'm currently working with data from a Luminex multiplex assay. In this assay, the concentrations of 17 different analyte proteins were identified in 12 groups in triplicate. One of these 17 groups was used as the control, and the log2 fold changes were calculated for the analyte concentration of each sample in each group using the average control concentration for that analyte.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, now I would like to calculate a p-value for the identified fold changes if possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My current preliminary idea is to perform the t test for each group compared with the control group for each analyte, but I don't think that my group sizes are large enough for this. My statistical knowledge is lacking, so if there is a better way of calculating these p-values I don't know about it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a better way of obtaining these p-values?&lt;/p&gt;&#xA;" OwnerUserId="506" LastActivityDate="2017-08-19T17:55:22.827" Title="How to calculate p-values for fold changes?" Tags="&lt;statistics&gt;" AnswerCount="4" CommentCount="1" />
  <row Id="2325" PostTypeId="2" ParentId="2293" CreationDate="2017-08-18T04:54:39.033" Score="3" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;should we expect Illumina 2x150bp (or shorter, 2x75bp) reads to map relatively equally to all centromere sequences?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;No. It has long been established that different chromosomes are associated with different centromeric sequences. It is sometimes possible to tell which chr a read is originated from based on its sequence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The GRCh38 centromeres are trickier. As I remember, the centromeric sequences were computationally generated with a Markov chain (or something alike) modeled after Venter's genome. GRC can distinguish most chromosomes, but not all. Some alpha arrays are placed onto 2 or 4 chromosomes. The original GRCh38 keeps all 4 copies. When you download GRCh38 for mapping purposes, only one copy is retained; additional copies are hard masked along with PARs on chrY.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to know more, see &lt;a href=&quot;http://genome.cshlp.org/content/early/2017/04/07/gr.213611.116.abstract&quot; rel=&quot;nofollow noreferrer&quot;&gt;this paper&lt;/a&gt; and &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/24501022&quot; rel=&quot;nofollow noreferrer&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-08-18T04:54:39.033" CommentCount="0" />
  <row Id="2326" PostTypeId="2" ParentId="2324" CreationDate="2017-08-18T05:08:01.100" Score="0" Body="&lt;p&gt;So to get a p value you must have one null hypothesis like the fold changes are significant or not. Then you test the hypothesis against the data and calculate the p value (likeliness or unlikeliness) for a given threshold.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your case, t test is good enough to calculate the p-value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://willett.ece.wisc.edu/wp-uploads/2016/01/05b-TandP.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://willett.ece.wisc.edu/wp-uploads/2016/01/05b-TandP.pdf&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="206" LastActivityDate="2017-08-18T05:08:01.100" CommentCount="0" />
  <row Id="2327" PostTypeId="2" ParentId="2324" CreationDate="2017-08-18T06:51:59.027" Score="2" Body="&lt;p&gt;Your null hypothesis would be that the fold-changes are 0, so you can either do the T-test accordingly or simply do away with the fold changes and perform the T-test between the raw values in the group (this is preferable to performing a T-test of one group vs. 0, since it allows you to assess the expected variability around 0). Note, however, that 3 samples is pretty much the bare minimum needed for any statistics, so your power will be terrible and your estimation of variance will likely be pretty inaccurate. Consequently, take any p-values with an appropriate grain of salt.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To my knowledge there aren't any good alternatives for your situation without having a good expected background distribution.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-18T06:51:59.027" CommentCount="0" />
  <row Id="2328" PostTypeId="2" ParentId="2324" CreationDate="2017-08-18T08:58:29.290" Score="1" Body="&lt;p&gt;You can't calculate a p-value on the fold-change values, you need to use the  concentrations in triplicate thus giving a measure of the variance for the t-test to use. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;t-test assumes your data are normally distributed, if they aren't you're going to get spurious p-values. If you aren't sure a non-parametric test like Wilcoxon is better. It will be less sensitive although with only 3 replicates your experiment is low powered anyway.&lt;/p&gt;&#xA;" OwnerUserId="1092" LastActivityDate="2017-08-18T08:58:29.290" CommentCount="2" />
  <row Id="2329" PostTypeId="1" AcceptedAnswerId="2330" CreationDate="2017-08-18T10:13:12.797" Score="4" ViewCount="145" Body="&lt;p&gt;I'm trying to understand the magnitude of batch effects in my RNA-seq samples, and I was wondering which expression units are more suitable to draw a PCA. I'm thinking of either &lt;code&gt;counts&lt;/code&gt; or &lt;code&gt;TPM&lt;/code&gt;, but things like &lt;a href=&quot;http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#data-transformations-and-visualization&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;rlog&lt;/code&gt; or &lt;code&gt;vst&lt;/code&gt;&lt;/a&gt; could work too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally, I'm wondering whether either of this units should be log-transformed first, to avoid high-abundance transcripts driving the PCA.&lt;/p&gt;&#xA;" OwnerUserId="123" LastActivityDate="2017-08-18T21:13:03.153" Title="Which measure should be used in a PCA or RNA-seq data? TPM or counts?" Tags="&lt;rna-seq&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="2330" PostTypeId="2" ParentId="2329" CreationDate="2017-08-18T10:17:53.487" Score="4" Body="&lt;p&gt;tldr: log transform counts and TPMs, but rlog/vst are preferred&lt;/p&gt;&#xA;&#xA;&lt;p&gt;TPM should be log transformed to get more useful results. If you're using DESeq2 already (given the reference to &lt;code&gt;rlog&lt;/code&gt; and &lt;code&gt;vst&lt;/code&gt;, this seems likely), then please go ahead and use &lt;code&gt;rlog&lt;/code&gt; or &lt;code&gt;vst&lt;/code&gt;. That will give you more reasonable results than raw counts. If you're stuck with counts for some reason, then first use normalized counts so they're at least a bit more comparable and then log transform them so your highly expressed genes aren't driving everything.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: As an aside, if you know what the batch effect is (e.g., library prep. date), it's sometimes convenient to include that in your model. You can then assess the genes that are actually changed due to that, which is sometimes handy to know (e.g., which genes might be more/less prone to degradation).&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-08-18T10:57:03.527" LastActivityDate="2017-08-18T10:57:03.527" CommentCount="2" />
  <row Id="2331" PostTypeId="2" ParentId="2324" CreationDate="2017-08-18T10:56:05.363" Score="1" Body="&lt;p&gt;The key point here is whether or not the values are approximately normally distributed and whether any transformations can be applied to make them so. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a t-test, the most important thing is that there is no relationship between the mean and the variance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Its also correct that you don't want to divide by the mean of the controls. You want to retain all the information. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So here is how i'd start, first calculate the mean and standard deviation of each of your 18 groups (17 treatments + control), and plot this on a scatter plot (i.e. mean on X, SD on Y).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a relationship between the two?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Repeat the exercise with log transformed values. Is the relationship more or less.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Pick the set of values that has the weakest mean-SD relationship. I'm guessing the log transformed might well be better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If $X = (x_1, x_2, x_3)$ is your controls and $Y_i = (y_{i,1}, y_{i,2}, y_{i,3})$ are the treated samples from treatment i, to test the difference between treatment and control, do a t-test on X and Y (not &lt;code&gt;T/mean(c)&lt;/code&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;for example in R&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X = c(x_1, x_2, x_3)&#xA;Y_1 = c(y_11, y_12, y_13)&#xA;t.test (X, Y_1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or in excel is if the three values for X are in column A and the three values for $Y_1$ in column B&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;=T.TEST(A1:A3,B1:B3,2,3)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you are going to do 17 of these, remember to do a multiple testing correction by multiplying the resulting p-values by 17. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to compare treatment 1 ($Y_1$) vs treatment 2 ($Y_2$) do the comparison directly, not taking the control into account:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;t.test(Y_1, Y_2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Remember that if you use log transformed values, you calculate log fold changes using &lt;code&gt;mean(Y)-mean(X)&lt;/code&gt; rather than &lt;code&gt;mean(Y)/mean(X)&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="235" LastEditorUserId="48" LastEditDate="2017-08-19T17:55:22.827" LastActivityDate="2017-08-19T17:55:22.827" CommentCount="0" />
  <row Id="2332" PostTypeId="2" ParentId="2329" CreationDate="2017-08-18T11:13:10.100" Score="4" Body="&lt;p&gt;PCA works best when the input data is approximately normally distributed on each dimension. It would be a good idea to do some initial data quality checks to verify that this is the case (and transform the data appropriately if not), or at least verify that the data is approximately normally distributed in the aggregate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For looking at Illumina RNASeq data, what worked best for me (i.e. produced the most normal-looking data) was the following steps:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Removing genes that had low raw counts in all samples&lt;/li&gt;&#xA;&lt;li&gt;Using DESeq's variance-stabilized transform (which transforms counts into a log-like distribution)&lt;/li&gt;&#xA;&lt;li&gt;Further normalising the VST values by dividing by the longest transcript length within each gene (which I call VSTPk)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;These steps are stated in a bit more detail in our Th2 paper that was published at the end of last year:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://jem.rupress.org/content/early/2016/12/01/jem.20160470#materials-methods&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://jem.rupress.org/content/early/2016/12/01/jem.20160470#materials-methods&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-08-18T21:13:03.153" LastActivityDate="2017-08-18T21:13:03.153" CommentCount="3" />
  <row Id="2333" PostTypeId="1" AcceptedAnswerId="2334" CreationDate="2017-08-18T12:00:33.640" Score="2" ViewCount="28" Body="&lt;p&gt;I'm pretty new to all of this--forgive me if this is a simple question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I download illumina counts from GEO (like the supplementary file in &#xA;GSE89225). Can I do comparisons directly on that file? Is there some normalization procedure I should go through?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="1226" LastEditorUserId="1226" LastEditDate="2017-08-18T12:05:03.827" LastActivityDate="2017-08-18T12:09:02.667" Title="Analyzing Illumina Counts" Tags="&lt;rna-seq&gt;&lt;normalization&gt;&lt;illumina&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2334" PostTypeId="2" ParentId="2333" CreationDate="2017-08-18T12:09:02.667" Score="4" Body="&lt;p&gt;The counts files for GSE89225 is the output of HTSeq-count as a large matrix. Unless you are developing a differential expression package yourself you should not attempt to directly use this. Rather, you should load it into R and use packages such as DESeq2, edgeR, or limma (those are the most popular ones).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For convenience, in DESeq2 you would want the &lt;code&gt;DESeqDatasetFromMatrix()&lt;/code&gt; function after loading this.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-18T12:09:02.667" CommentCount="0" />
  <row Id="2335" PostTypeId="1" CreationDate="2017-08-19T01:05:10.987" Score="2" ViewCount="44" Body="&lt;p&gt;Is there a tool that can scan fastq files without assembling them for a custom list of user defined snps?&lt;/p&gt;&#xA;" OwnerUserId="167" LastEditorUserId="123" LastEditDate="2017-08-21T11:23:44.720" LastActivityDate="2017-08-21T11:23:44.720" Title="Is there a way to quickly verify the presence of some SNPs in Fastq files?" Tags="&lt;fastq&gt;&lt;vcf&gt;&lt;snp&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="0" />
  <row Id="2336" PostTypeId="2" ParentId="2335" CreationDate="2017-08-19T06:34:42.700" Score="2" Body="&lt;p&gt;Try this pipeline: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Clean raw fastq files (trimmomatic)&lt;/li&gt;&#xA;&lt;li&gt;Align clean fastq files to the reference genome (dna-to-dna aligner of your choice)&lt;/li&gt;&#xA;&lt;li&gt;Convert the alignment sam file to bam (samtools)&lt;/li&gt;&#xA;&lt;li&gt;Call SNPs from bam file (samtools)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Here's a start tutorial on this: &lt;a href=&quot;http://ged.msu.edu/angus/tutorials-2013/snp_tutorial.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://ged.msu.edu/angus/tutorials-2013/snp_tutorial.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="294" LastActivityDate="2017-08-19T06:34:42.700" CommentCount="1" />
  <row Id="2338" PostTypeId="1" CreationDate="2017-08-19T19:58:06.747" Score="1" ViewCount="18" Body="&lt;p&gt;We all have used freemix score at some stage to check contamination or swaps in our sequencing experiment, but can anyone once for all explain how the score is calculated? &lt;/p&gt;&#xA;" OwnerUserId="1250" LastActivityDate="2017-08-25T21:30:44.297" Title="VerifyBamID freemix score" Tags="&lt;bam&gt;&lt;ngs&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2339" PostTypeId="1" CreationDate="2017-08-20T11:15:18.257" Score="1" ViewCount="8" Body="&lt;p&gt;What are the relative advantages / disadvantages of the TM (template modelling) and GDT (global distance test, total score) scores, used in protein structure prediction competitions, such as &lt;a href=&quot;https://www.google.ru/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0ahUKEwir2PqO1-XVAhVHPhQKHcFGBF0QFggmMAA&amp;amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FCASP&amp;amp;usg=AFQjCNENea3DBFw-bFeW2BRImlsjhtnxDA&quot; rel=&quot;nofollow noreferrer&quot;&gt;CASP&lt;/a&gt;? When should I choose one over the other? &lt;/p&gt;&#xA;" OwnerUserId="1249" LastActivityDate="2017-08-20T11:15:18.257" Title="Differences between TM and GDT-TS scores for structure comparison" Tags="&lt;protein-structure&gt;&lt;statistics&gt;&lt;modelling&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2340" PostTypeId="2" ParentId="2335" CreationDate="2017-08-20T22:16:29.723" Score="1" Body="&lt;p&gt;You could use &lt;a href=&quot;http://www.biorxiv.org/content/early/2017/04/07/118000&quot; rel=&quot;nofollow noreferrer&quot;&gt;ARIBA&lt;/a&gt;, which will map you reads to the regions of your interest (more on that in a second) and then assemble/call variants.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/ZHRVk.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/ZHRVk.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The tool is mostly used to find known variants related to AMR (antimicrobial resistance) from a set of databases, but &lt;a href=&quot;https://github.com/sanger-pathogens/ariba/wiki/Task%3A-prepareref#user-provided-data---fasta-and-metadata&quot; rel=&quot;nofollow noreferrer&quot;&gt;according to the project's wiki&lt;/a&gt; you can build your own database with your region of interest.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Additionally, reference sequences can be either of:&#xA;&#xA;    Presence/absence sequences. ARIBA will look for these sequences in the input reads and report any that it finds, and also any variants between these sequences and the input reads.&#xA;&#xA;    Variants only sequences. These should have known variant details specified in the metadata file (see below). ARIBA reports only when it finds at least one of the given variants in each of these these sequences. If a sample has one of these sequences, but does not have one of the supplied variants, then it is not reported. If you supply a variants only sequence, but no variant, then the sequence will be removed during sanity checks.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="123" LastActivityDate="2017-08-20T22:16:29.723" CommentCount="2" />
  <row Id="2341" PostTypeId="1" CreationDate="2017-08-21T09:45:33.583" Score="3" ViewCount="60" Body="&lt;p&gt;I would like to extract all the CDS from a batch of genomes. I have found a perl script from &lt;a href=&quot;https://www.biostars.org/p/46281/&quot; rel=&quot;nofollow noreferrer&quot;&gt;BioStars&lt;/a&gt; but this does not seem to work for me. I would preferably like to have a script/ method which will use the locus tag as a header.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My gff files are from prokka output&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ATCC0000    Prodigal:2.6    CDS 243 434 .   +   0   ID=FKKLIMLP_00001;Parent=FKKLIMLP_00001_gene;inference=ab initio prediction:Prodigal:2.6;locus_tag=FKKLIMLP_00001;product=hypothetical protein&#xA;ATCC0000    prokka  gene    243 434 .   +   .   ID=FKKLIMLP_00001_gene;locus_tag=FKKLIMLP_00001&#xA;ATCC0000    Prodigal:2.6    CDS 1727    2131    .   -   0   ID=FKKLIMLP_00002;Parent=FKKLIMLP_00002_gene;inference=ab initio prediction:Prodigal:2.6;locus_tag=FKKLIMLP_00002;product=hypothetical protein&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="982" LastEditorUserId="982" LastEditDate="2017-08-21T09:51:14.530" LastActivityDate="2017-08-23T04:12:35.133" Title="How to extract / convert gff3 CDS sequences to multifasta" Tags="&lt;fasta&gt;&lt;software-recommendation&gt;&lt;format-conversion&gt;&lt;gff3&gt;" AnswerCount="4" CommentCount="1" />
  <row Id="2342" PostTypeId="2" ParentId="2341" CreationDate="2017-08-21T09:57:25.883" Score="2" Body="&lt;p&gt;Using python and &lt;a href=&quot;https://github.com/chapmanb/bcbb/tree/master/gff&quot; rel=&quot;nofollow noreferrer&quot;&gt;this GFF parser&lt;/a&gt; that mimics &lt;a href=&quot;http://biopython.org/wiki/SeqIO&quot; rel=&quot;nofollow noreferrer&quot;&gt;Biopython's SeqIO parsers&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from BCBio import GFF&#xA;&#xA;# Read the gff&#xA;for seq in GFF.parse('my_file.gff'):&#xA;    # only focus on the CDSs&#xA;    for feat in filter(lambda x: x.type == 'CDS',&#xA;                       seq.features):&#xA;        # extract the locus tag&#xA;        locus_tag = feat.qualifiers.get('locus_tag',&#xA;                                        ['unspecified'])[0]&#xA;        # extract the sequence&#xA;        dna_seq = seq = str(feat.extract(seq).seq)&#xA;        # simply print the sequence in fasta format&#xA;        print('&amp;gt;%s\n%s' % (locus_tag, dna_seq))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="123" LastActivityDate="2017-08-21T09:57:25.883" CommentCount="2" />
  <row Id="2343" PostTypeId="1" AcceptedAnswerId="2368" CreationDate="2017-08-21T10:36:56.920" Score="1" ViewCount="67" Body="&lt;p&gt;I have a lot of protein clusters. I want to perform an enrichment analysis of their functional annotations, against reference datasets or list of genes I select.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More precisely: a method yields cluster of proteins. I want to decide, for each cluster (which is a set of proteins identifiers), if it holds meaning regarding the proteins it contains, by comparing the annotations found in the cluster and the annotations found in common or specific datasets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Initially, I used &lt;a href=&quot;https://david.ncifcrf.gov&quot; rel=&quot;nofollow noreferrer&quot;&gt;DAVID&lt;/a&gt;, which compute the GO annotations from the protein list, then perform the enrichment analysis against common datasets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, DAVID suffer of the following drawbacks:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;since I didn't find any other way to use it, I'm limited by the web interface and its limitations (number of genes, url size, number of request/day).&lt;/li&gt;&#xA;&lt;li&gt;automation over the web interface is a hacky script.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The team behind seems to offer a way to run DAVID in-house (allowing, hopefully, to relax the limitations), but I can't find any way to download the database from their website.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the alternative ways to get enrichment analysis over proteins in a reliable and automated way ?&lt;/p&gt;&#xA;" OwnerUserId="1355" LastEditorUserId="1355" LastEditDate="2017-08-22T10:30:08.333" LastActivityDate="2017-08-23T10:49:18.397" Title="How to run enrichment analysis of protein functional annotation?" Tags="&lt;go&gt;&lt;go-enrichment&gt;" AnswerCount="2" CommentCount="7" />
  <row Id="2344" PostTypeId="2" ParentId="2341" CreationDate="2017-08-21T11:23:34.903" Score="2" Body="&lt;p&gt;The &lt;a href=&quot;http://ccb.jhu.edu/software/stringtie/gff.shtml#gffread&quot; rel=&quot;nofollow noreferrer&quot;&gt;gffread utility&lt;/a&gt; in Cufflinks package might be interesting for you. To generate a multi-fasta file with nucleotide sequences from your GFF file, then you can try: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gffread -w output_transcripts.fasta -g reference_genome.fa input_transcripts.gff&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="294" LastActivityDate="2017-08-21T11:23:34.903" CommentCount="0" />
  <row Id="2345" PostTypeId="2" ParentId="2343" CreationDate="2017-08-21T11:26:13.097" Score="3" Body="&lt;p&gt;I would recommend R instead of sticking to websites.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many tools for enrichment analysis. I like to use &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/goseq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;GOseq&lt;/a&gt;, which is initially made for RNAseq data, but can also be used for proteins (I have used it for proteomics). You can use the length of the protein instead of transcript length (to correct for length bias), or you can exclude the length information by just using the hypergeometric method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/vignettes/goseq/inst/doc/goseq.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;manual&lt;/a&gt; for code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Subsequently you can use &lt;a href=&quot;https://sourceforge.net/projects/gogadget/&quot; rel=&quot;nofollow noreferrer&quot;&gt;gogadget&lt;/a&gt;, a tool I made for visualization of GOseq results with a heatmap or cytoscape network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally, you might wanna take a look at &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/clusterProfiler.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;clusterProfiler&lt;/a&gt;. With this package you can do enrichment analysis for many clusters in parallel.&lt;/p&gt;&#xA;" OwnerUserId="939" LastEditorUserId="939" LastEditDate="2017-08-22T10:43:13.063" LastActivityDate="2017-08-22T10:43:13.063" CommentCount="3" />
  <row Id="2346" PostTypeId="1" CreationDate="2017-08-21T12:05:47.623" Score="5" ViewCount="43" Body="&lt;p&gt;It is a well reported fact that GO analysis of RNAseq results is affected by a number of biases, including length bias and expression level bias. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;bioconductor&lt;/code&gt; package &lt;a href=&quot;https://bioconductor.org/packages/release/bioc/html/goseq.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;code&gt;goseq&lt;/code&gt;&lt;/a&gt; allows you to correct for these biases. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;By default it corrects for length bias, but you can also get it to do read count bias. Using read counts to do the correction is attractive because in theory it should account for both sources of bias ($read counts\approx expression \times length$).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm doing an enrichment analysis were I have tried both options (length and read counts) and get very different answers. If I run a binomial regression on expression and length vs probability of being differential, I can see that both are &lt;em&gt;independently&lt;/em&gt; important.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; model &amp;lt;- glm(sig ~  expression + log(length), data=retained_genes,  family=binomial(link=&quot;logit&quot;))&#xA;&amp;gt; print(anova(model, test=&quot;Chisq&quot;))&#xA;&#xA;Analysis of Deviance Table&#xA;&#xA;Model: binomial, link: logit&#xA;&#xA;Response: sig&#xA;&#xA;Terms added sequentially (first to last)&#xA;&#xA;                       Df Deviance Resid. Df Resid. Dev  Pr(&amp;gt;Chi)    &#xA;NULL                                    6676     4507.1              &#xA;expression              1  114.998      6675     4392.1 &amp;lt; 2.2e-16 ***&#xA;log(length)             1  102.553      6674     4289.5 &amp;lt; 2.2e-16 ***&#xA;expression:log(length)  1   34.094      6673     4255.4 5.252e-09 ***&#xA;---&#xA;Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So I'm know unsure what to do, should I use the analysis corrected for length or read count. Or perhaps take only terms significant in both? Or only in one?&lt;/p&gt;&#xA;" OwnerUserId="235" LastEditorUserId="235" LastEditDate="2017-08-21T15:51:45.610" LastActivityDate="2017-08-21T15:51:45.610" Title="Correct for gene length or read counts in GO enrichment analysis" Tags="&lt;rna-seq&gt;&lt;differential-expression&gt;&lt;go&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="0" />
  <row Id="2348" PostTypeId="2" ParentId="2346" CreationDate="2017-08-21T13:21:22.433" Score="1" Body="&lt;p&gt;I am not sure if it makes sense to use read counts as bias instead of gene length (and I certainly wouldn't expect the same results). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you use total read counts of all your samples (library size)? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The correction for gene length is a pure technical one, the longer a gene the more reads will align (and higher read count genes are easier significant, since they are way above the noise threshold). If you use read counts, you also have a biological factor (expression) in there, which (I think) is the stuff you test with statistics (e.g., with edgeR) and thus not the bias you want to correct for. &lt;/p&gt;&#xA;" OwnerUserId="939" LastActivityDate="2017-08-21T13:21:22.433" CommentCount="2" />
  <row Id="2349" PostTypeId="1" CreationDate="2017-08-21T14:55:53.037" Score="1" ViewCount="13" Body="&lt;p&gt;I am helping a colleague to interpret protein multiple sequence alignment results. My colleague used a third-party proprietary solution (I know...) to calculate the alignment, and one of the resulting tables is a pairwise distance matrix with identity % in the top triangle and &lt;strong&gt;identity plus similarity&lt;/strong&gt; in the bottom triangle. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am struggling to interpret identity plus similarity, despite the fact I found the metric in several papers. &lt;/p&gt;&#xA;" OwnerUserId="1236" LastActivityDate="2017-08-21T23:06:08.790" Title="Identity plus Similarity Interpretation" Tags="&lt;proteins&gt;&lt;sequence-alignment&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2350" PostTypeId="2" ParentId="2346" CreationDate="2017-08-21T15:43:53.230" Score="1" Body="&lt;p&gt;My understanding of the subject is that the bias of the gene length (and other bias) should be taken care when analyzing the expression and &lt;strong&gt;before&lt;/strong&gt; enrichment analysis. The enrichment analysis should be done once the corrections has been performed. Because as the abstract of the &lt;a href=&quot;https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-2-r14&quot; rel=&quot;nofollow noreferrer&quot;&gt;GOseq paper states&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;GO analysis is widely used ... but standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly expressed transcripts.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So first, take care of the differential expression bias by length and then use the GO to reduce complexity. How you take care of the bias in the RNA-seq data is another question. But the &lt;a href=&quot;http://bioconductor.org/packages/cqn&quot; rel=&quot;nofollow noreferrer&quot;&gt;cqn&lt;/a&gt; package of Bioconductor can correct the expression by gene length and GC content. However, this correction might hurt the differential tool used (see &lt;a href=&quot;https://support.bioconductor.org/p/95683/#95713&quot; rel=&quot;nofollow noreferrer&quot;&gt;this discussion&lt;/a&gt; in Bioconductor), so it might be better in some cases to use GOSeq. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, to the question itself:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;So I'm know unsure what to do, should I use the analysis corrected for length or read count. Or perhaps take only terms significant in both? Or only in one?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Use whatever correction method that yields better differentially expressed genes (DEG. If you find that the correction for length improves the accuracy of the predictions of DEG better than correcting by length and GC, then use that one. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another option to obtain accurate GO terms, then you could use other testing procedures which don't rely uniquely in the Fisher test, such as those that take into account the structure of the GO graph. &lt;a href=&quot;http://bioconductor.org/packages/topGO&quot; rel=&quot;nofollow noreferrer&quot;&gt;TopGO&lt;/a&gt; use this approach (note that it is a bit difficult to work with this package),this will reduce the role of the gene length bias (and probably other bias) in the resulting significant GO.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-08-21T15:43:53.230" CommentCount="4" />
  <row Id="2351" PostTypeId="2" ParentId="2349" CreationDate="2017-08-21T23:06:08.790" Score="1" Body="&lt;p&gt;I guess you already know what identity means. But for completeness, it is the % of pairwise alignment position which have an identical amino acid residue. For the identity+similarity it is important to know that amino acids can be grouped according to their physicochemical properties e.g. charge, size etc. While there are standard groupings for amino acids it may be that the proprietary algorithm uses different groupings. Either way, % identity+similarity would imply the number of pairwise alignment positions which contain either the same amino acid or amino acids from the same group (however they define groups).&lt;/p&gt;&#xA;" OwnerUserId="400" LastActivityDate="2017-08-21T23:06:08.790" CommentCount="0" />
  <row Id="2352" PostTypeId="1" AcceptedAnswerId="2354" CreationDate="2017-08-22T06:29:21.313" Score="2" ViewCount="166" Body="&lt;p&gt;I am using plink to check the association between a phenotype and the SNPs of a gene. plink says the phenotype is significantly associated with a SNP on that gene, and when I check the SNP alleles, for 6 out of 49 samples, the nucleotides are 'GA', and for the rest, they are 'AG', so only the order is different. I am not a biologist, but my basic genetics knowledge says that those two are semantically the same. However, obviously, plink does not agree with me and it associates this SNP with a small p-value since 'GA' samples have a higher value for the phenotype than 'AG' samples. I am very new to SNP analysis, any help is appreciated.&lt;/p&gt;&#xA;" OwnerUserId="1331" LastEditorUserId="123" LastEditDate="2017-08-22T09:55:58.653" LastActivityDate="2017-08-22T09:55:58.653" Title="Is there any difference between SNPs 'AG' and 'GA' in association analyses?" Tags="&lt;snp&gt;&lt;gwas&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="2353" PostTypeId="2" ParentId="2352" CreationDate="2017-08-22T06:40:57.690" Score="3" Body="&lt;p&gt;Is &quot;user5054&quot; the same as &quot;user5504&quot;? No? Exactly. Not only does order matter, it's incredibly vitally important. &lt;code&gt;AG&lt;/code&gt; and &lt;code&gt;GA&lt;/code&gt; are completely and totally different from each other. If this is in a coding region, then the resulting amino acid is undoubtedly changed (fun fact, the &lt;a href=&quot;https://en.wikipedia.org/wiki/DNA_codon_table&quot; rel=&quot;nofollow noreferrer&quot;&gt;only exception&lt;/a&gt; is &lt;code&gt;TAG&lt;/code&gt; and &lt;code&gt;TGA&lt;/code&gt;). If it's at a &lt;a href=&quot;https://en.wikipedia.org/wiki/RNA_splicing&quot; rel=&quot;nofollow noreferrer&quot;&gt;splice site&lt;/a&gt; then splicing is likely altered (AG is part of a splice acceptor site). If it's at the binding site of something then it wouldn't be surprised if it didn't bind any more.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-22T06:40:57.690" CommentCount="3" />
  <row Id="2354" PostTypeId="2" ParentId="2352" CreationDate="2017-08-22T07:53:00.347" Score="6" Body="&lt;p&gt;Could you please show us the context in which this appears, as you seem to be interpreting this differently to Devon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If it's appearing as you say, GA and AG, then Devon is right, this usually means that the sequence goes ###AG### or ###GA###, which are two very different sequences.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If, however, as you're implying, the sequences are actually ###G### or ###A###, then semantically, the two ways of writing it have the same outcome. Normally, however, this is written as A/G or G/A.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively, you're looking at genotypes, which would usually appear as A|G and G|A. In this case, both individuals are heterozygotes, and in terms of the individual SNP they are the same. However the side of the pipe is relevant as this tells you which homologous chromosome it is on. This allows you to calculate LD with respect to neighbouring SNPs, and to determine the actual sequences of the proteins in that individual.&lt;/p&gt;&#xA;" OwnerUserId="1332" LastActivityDate="2017-08-22T07:53:00.347" CommentCount="5" />
  <row Id="2355" PostTypeId="2" ParentId="2352" CreationDate="2017-08-22T09:19:27.233" Score="2" Body="&lt;p&gt;Most association analyses are carried out at a single SNP level, so AG and GA are likely to indicate a heterozygous genotype at a particular location. However, the precise notation matters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As @Emily_Ensembl has alluded to, for VCF files, A/G indicates unphased SNPs (order unknown, and shouldn't be considered in analyses), whereas A|G indicates phased SNPs (order matters).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is possible to carry out association analyses by combining multiple variants into a single haplotype, in which case the order would also matter. It's unlikely that this is the case, because such haplotype variant analyses are substantially more complicated to carry out, and will typically be paired with information about SNP linkage (statistic D´ or r²).&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-08-22T09:19:27.233" CommentCount="0" />
  <row Id="2356" PostTypeId="1" CreationDate="2017-08-22T10:11:37.243" Score="0" ViewCount="17" Body="&lt;p&gt;I have multiple libraries of &lt;strong&gt;10x Chromium single-cell RNA-seq&lt;/strong&gt; data, which I'd like to combine. One option is using &lt;code&gt;cellranger aggr&lt;/code&gt; which by default does a depth normalization:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;mapped&lt;/code&gt;: (default) Subsample reads from higher-depth libraries until they all have an equal number of confidently mapped reads per cell. &lt;code&gt;raw&lt;/code&gt;: Subsample reads from higher-depth libraries until they all have an equal number of total (i.e. raw, mapping-independent) reads per cell.&#xA;&lt;code&gt;none&lt;/code&gt;: Do not normalize at all.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The new version of &lt;code&gt;Seurat&lt;/code&gt; (2.0) provides another way via the &lt;code&gt;MergeSeurat()&lt;/code&gt; (or &lt;code&gt;AddSamples()&lt;/code&gt;) functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://satijalab.org/seurat/merge_vignette.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://satijalab.org/seurat/merge_vignette.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These have the arguments:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;do.logNormalize whether to normalize the expression data per cell and transform to log space.&#xA;total.expr      scale factor in the log normalization &#xA;do.scale        In object@scale.data, perform row-scaling (gene-based z-score) &#xA;do.center       In object@scale.data, perform row-centering (gene-based centering)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm wondering how do these methods compare? What are the pros/cons etc? What do we need to consider when we combine samples?&lt;/p&gt;&#xA;" OwnerUserId="208" LastActivityDate="2017-08-24T00:15:42.657" Title="Cellranger aggr and Seurat merge difference?" Tags="&lt;rna-seq&gt;&lt;seurat&gt;&lt;10x&gt;&lt;single-cell&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2358" PostTypeId="2" ParentId="2234" CreationDate="2017-08-22T14:49:00.440" Score="1" Body="&lt;p&gt;I finally found another answer to my question. Please read this great article in May 12 2017 BioMed Central (BMC) Bioinformatics article titled &lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1674-0&quot; rel=&quot;nofollow noreferrer&quot;&gt;Ranking metrics in gene set enrichment analysis: do they matter?&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, please read this blog , &lt;a href=&quot;http://crazyhottommy.blogspot.com/2016/08/gene-set-enrichment-analysis-gsea.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Diving into Genetics and Genomics: Gene Set Enrichment Analysis (GSEA) explained&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After reading these two articles, my choice for the best rapidGSEA local ranking measure is Minimum Significant Difference (i.e., MSD), because it has the best overall false positive rate (i.e., FPR) for larger sample sizes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, it is important to realize that fgseaL's phenotype labeling can hypothetically be emulated by GSEA with either one of it's sixteen possible ranking metrics or a custom ranking metric.&lt;/p&gt;&#xA;" OwnerUserId="1283" LastEditorUserId="1283" LastEditDate="2017-08-23T16:16:10.837" LastActivityDate="2017-08-23T16:16:10.837" CommentCount="0" />
  <row Id="2359" PostTypeId="1" CreationDate="2017-08-22T16:39:42.897" Score="3" ViewCount="35" Body="&lt;p&gt;I would like to modify some reference transcripts from Ensembl (&lt;em&gt;D. melanogaster&lt;/em&gt;) to introduce a controlled rate of random errors in the sequences. The idea would be to introduce random base substitutions in these sequences, no indels for now, because I would like to keep the transcript sequence length as it is in the reference. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The rate of error per transcript will be determined according to an error profile computed from an external set of RNA-seq reads (e.g., generated with ONT MinION)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The aim of this modification would be to establish a rough benchmark of the performances of aligners to use over transcripts from spliced reads (rna-to-genome), aka with more than one exon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any idea of which software would be best for this purpose?&lt;/p&gt;&#xA;" OwnerUserId="294" LastActivityDate="2017-08-25T12:23:33.937" Title="Introduce errors in reference transcripts according to external dataset error model" Tags="&lt;rna-seq&gt;&lt;software-recommendation&gt;&lt;nanopore&gt;&lt;simulated-data&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="0" />
  <row Id="2360" PostTypeId="2" ParentId="2359" CreationDate="2017-08-22T17:39:19.703" Score="0" Body="&lt;p&gt;It sounds like what you're really looking for is a read simulator. A cursory search turns up &lt;a href=&quot;http://www.biorxiv.org/content/early/2016/03/18/044545&quot; rel=&quot;nofollow noreferrer&quot;&gt;NanoSim&lt;/a&gt;, which is designed to simulate reads from a MinION. This has the benefit of at least having been &lt;a href=&quot;https://gigascience.biomedcentral.com/articles/10.1186/s13742-016-0140-7&quot; rel=&quot;nofollow noreferrer&quot;&gt;used in some of the published literature&lt;/a&gt;, which is always a nice sign.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You may also find &lt;a href=&quot;http://www.nature.com/nrg/journal/v17/n8/full/nrg.2016.57.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;this review article&lt;/a&gt; on read simulators useful. It doesn't specifically mention NanoSim, but it should prove to be a useful review of the general concepts anyway if you need to read up on them.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-22T17:39:19.703" CommentCount="2" />
  <row Id="2361" PostTypeId="1" CreationDate="2017-08-22T19:20:21.920" Score="0" ViewCount="20" Body="&lt;p&gt;I have data, obtained from a single metagenomic DNA sample, that consists of two MiSeq FASTQ files (R1 and R2) that I merged using &lt;a href=&quot;https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btt593&quot; rel=&quot;nofollow noreferrer&quot;&gt;PEAR&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to estimate the abundances of the bacteria taxa to generate a figure like this one:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/FlJxD.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/FlJxD.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Figure from: &lt;em&gt;Panosyan, Hovik, and Nils‐Kåre Birkeland. &quot;Microbial diversity in an Armenian geothermal spring assessed by molecular and culture‐based methods.&quot; Journal of basic microbiology 54.11 (2014): 1240-1250.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that there wasn't a step of amplification of the 16S region as the goal of the sequencing was to discover new genes. I've already isolated 16S reads from my sample using &lt;a href=&quot;http://bioinfo.lifl.fr/RNA/sortmerna/&quot; rel=&quot;nofollow noreferrer&quot;&gt;SortMeRNA&lt;/a&gt;, but it seems like softwares that do OTU picking, taxonomic assignment and diversity analyses (such as mothur and QIIME) require that all the reads come from the same region of the 16S gene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way of using these 16S reads that I've filtered using SortMeRNA in a diversity analysis using mothur/QIIME?&lt;/p&gt;&#xA;" OwnerUserId="1334" LastEditorUserId="1334" LastEditDate="2017-08-22T20:23:18.003" LastActivityDate="2017-08-22T20:23:18.003" Title="Microbial diversity analysis using whole-genome metagenomic data" Tags="&lt;metagenome&gt;&lt;taxonomy&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2362" PostTypeId="2" ParentId="2359" CreationDate="2017-08-22T19:43:37.373" Score="2" Body="&lt;p&gt;Do any of the answers for &lt;a href=&quot;https://bioinformatics.stackexchange.com/questions/202/tools-for-simulating-oxford-nanopore-reads/222&quot;&gt;this question&lt;/a&gt; help? Karel Brinda has mentioned a few read simulators in &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/273/73&quot;&gt;the answer to that question&lt;/a&gt;, and has &lt;a href=&quot;http://brinda.cz/publications/brinda_phd.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;a thesis&lt;/a&gt; with more information.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Excluding INDEL errors doesn't sound like a good idea; length can still be preserved even if doing that, it just needs an adjustment at the end of the sequence. Note that if you're trying to model nanopore reads, what you're really modelling is the base-caller, rather than the sequencer. I mention this in more detail in &lt;a href=&quot;https://bioinformatics.stackexchange.com/a/226/73&quot;&gt;my answer&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In most cases where errors are modelled, I find it better to use publicly-available data instead. Especially for nanopore data, there are unmodelled systematic errors in the base-callers and sequencer that can't be simulated using any programs (because they are unmodelled). The following paper would be a good place to start for cDNA sequences, which looks at single-cell data from mouse (C57Bl/6) B1a cells:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.biorxiv.org/content/early/2017/04/13/126847&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.biorxiv.org/content/early/2017/04/13/126847&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Illumina and ONT reads for that study can be found in SRA under accession number &lt;a href=&quot;https://trace.ncbi.nlm.nih.gov/Traces/sra/?study=SRP082530&quot; rel=&quot;nofollow noreferrer&quot;&gt;SRP082530&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know of any recent &lt;em&gt;D. melanogaster&lt;/em&gt; studies that have been done using nanopore. There's always the option of spending $1000 on a &lt;a href=&quot;https://store.nanoporetech.com/minion/rna/sets&quot; rel=&quot;nofollow noreferrer&quot;&gt;purchase of a MinION&lt;/a&gt; with an RNA starter kit to do the study yourself. Here's an older targeted gene study, but bear in mind that it was using an R7.3 flow cell, so errors rates will be much higher than what is currently available:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0777-z&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0777-z&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="73" LastEditorUserId="73" LastEditDate="2017-08-22T19:52:49.440" LastActivityDate="2017-08-22T19:52:49.440" CommentCount="0" />
  <row Id="2363" PostTypeId="1" CreationDate="2017-08-22T20:51:08.273" Score="3" ViewCount="77" Body="&lt;p&gt;I am constructing a bit of software which pipes the outputs of the bam file via &lt;code&gt;samtools view&lt;/code&gt; into a script for parsing. My goal is to (somehow) make this process more efficient, and faster than &lt;code&gt;samtools view&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am only using 3-4 fields in the bam. Naturally, if I only wanted those fields, I could do something like &lt;code&gt;samtools view file1.bam | cut -f ##&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I'm trying to be faster than this approach. More specifically: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;(1) Given the bam file format, is it theoretically possible to output several requested SAM fields faster than &lt;code&gt;samtools view|cut -f&lt;/code&gt;? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;(2) Is there software available to achieve (1), or would one need to hack samtools? &lt;/p&gt;&#xA;" OwnerUserId="146" LastEditorUserId="37" LastEditDate="2017-08-23T12:43:23.493" LastActivityDate="2017-08-25T14:27:47.847" Title="Is there a way to retrieve several SAM fields faster than `samtools view | cut -f`?" Tags="&lt;bam&gt;&lt;samtools&gt;&lt;sam&gt;" AnswerCount="3" CommentCount="5" FavoriteCount="0" />
  <row Id="2364" PostTypeId="2" ParentId="2341" CreationDate="2017-08-23T02:13:50.693" Score="1" Body="&lt;p&gt;I like &lt;code&gt;bedtools getfasta&lt;/code&gt;.  My typical option set is &lt;code&gt;bedtools getfasta -fi &amp;lt;reference&amp;gt; -bed &amp;lt;gff_file&amp;gt; -name -s&lt;/code&gt;.  Be aware of the &lt;code&gt;-s&lt;/code&gt; to make sure you are pulling the correct strand.  I like bedtools because it is a versatile tool overall for handling bed, gff and vcf file manipulations. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# bedtools getfasta&#xA;&#xA;Tool:    bedtools getfasta (aka fastaFromBed)&#xA;Version: v2.26.0&#xA;Summary: Extract DNA sequences from a fasta file based on feature coordinates.&#xA;&#xA;Usage:   bedtools getfasta [OPTIONS] -fi &amp;lt;fasta&amp;gt; -bed &amp;lt;bed/gff/vcf&amp;gt;&#xA;&#xA;Options: &#xA;    -fi Input FASTA file&#xA;    -bed    BED/GFF/VCF file of ranges to extract from -fi&#xA;    -name   Use the name field for the FASTA header&#xA;    -split  given BED12 fmt., extract and concatenate the sequencesfrom the BED &quot;blocks&quot; (e.g., exons)&#xA;    -tab    Write output in TAB delimited format.&#xA;        - Default is FASTA format.&#xA;&#xA;    -s  Force strandedness. If the feature occupies the antisense,&#xA;        strand, the sequence will be reverse complemented.&#xA;        - By default, strand information is ignored.&#xA;&#xA;    -fullHeader Use full fasta header.&#xA;        - By default, only the word before the first space or tab is used.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="964" LastActivityDate="2017-08-23T02:13:50.693" CommentCount="0" />
  <row Id="2365" PostTypeId="2" ParentId="2363" CreationDate="2017-08-23T03:14:30.797" Score="4" Body="&lt;p&gt;The BAM file format is not a text-based format. It has a specific binary structure, specified in reasonable detail in the &lt;a href=&quot;https://samtools.github.io/hts-specs/SAMv1.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;SAM file format specification&lt;/a&gt;. Whenever this information is displayed on a screen as text, it needs to be converted from the binary format to a text format, which takes a bit of time and processing power.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As this question suggests, if only a specific field from alignments is needed (or fields), then it will probably be better to extract just those fields and do any necessary conversion only on those fields. While this &lt;em&gt;can&lt;/em&gt; be done by writing a BAM parser from scratch, many developers have already written software libraries to process BAM files in this way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Devon Ryan has suggested &lt;a href=&quot;http://www.htslib.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;htslib&lt;/a&gt;, which is a C library written by a group that includes the people who wrote the SAM/BAM/CRAM file format specifications. There's also &lt;a href=&quot;https://github.com/pysam-developers/pysam&quot; rel=&quot;nofollow noreferrer&quot;&gt;pysam&lt;/a&gt;, which is a python wrapper around htslib.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The particular tool that is used will depend on your familiarity with programming and the specific thing that you want to do. If you just want to &quot;output a BAM file&quot;, then cat is the quickest:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cat file1.bam&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;... but you probably don't want to do that, because it seems like you want to process a text-based representation with a script. Because you haven't specified which fields you are interested in, it's not possible to suggest the best thing to use. In the end, I expect that an &quot;efficient&quot; solution to your problem would involve &lt;code&gt;htslib&lt;/code&gt; in some form. This is not really hacking &lt;em&gt;samtools&lt;/em&gt;, it's using the backend of samtools to process BAM data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, it's useful to also think about the time cost of coding. What application do you have which means that the text-based processing method is not sufficient? It takes time to write code, and a lot more time to debug that code to make sure it's doing the right thing in all situations. If this is for a one-off thing, then &lt;code&gt;samtools view&lt;/code&gt; output fed into your script may be the quickest solution.&lt;/p&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-08-23T03:14:30.797" CommentCount="2" />
  <row Id="2366" PostTypeId="2" ParentId="2341" CreationDate="2017-08-23T04:12:35.133" Score="1" Body="&lt;p&gt;The &lt;code&gt;xtractore&lt;/code&gt; program from the &lt;a href=&quot;http://brendelgroup.github.io/AEGeAn/&quot; rel=&quot;nofollow noreferrer&quot;&gt;AEGeAn Toolkit&lt;/a&gt; was designed for this type of use case. Just set &lt;code&gt;--type=CDS&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ xtractore -h&#xA;&#xA;xtractore: extract sequences corresponding to annotated features from the&#xA;           given sequence file&#xA;&#xA;Usage: xtractore [options] features.gff3 sequences.fasta&#xA;  Options:&#xA;    -d|--debug            print debugging output&#xA;    -h|--help             print this help message and exit&#xA;    -i|--idfile: FILE     file containing a list of feature IDs (1 per line&#xA;                          with no spaces); if provided, only features with&#xA;                          IDs in this file will be extracted&#xA;    -o|--outfile: FILE    file to which output sequences will be written;&#xA;                          default is terminal (stdout)&#xA;    -t|--type: STRING     feature type to extract; can be used multiple&#xA;                          times to extract features of multiple types&#xA;    -v|--version          print version number and exit&#xA;    -V|--verbose          print verbose warning and error messages&#xA;    -w|--width: INT       width of each line of sequence in the Fasta&#xA;                          output; default is 80; set to 0 for no&#xA;                          formatting&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It looks like you're processing prokaryotic genomes, but this program works also on eukaryotic genomes where CDSs often have introns interspersed.&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2017-08-23T04:12:35.133" CommentCount="0" />
  <row Id="2367" PostTypeId="1" CreationDate="2017-08-23T06:18:51.097" Score="3" ViewCount="28" Body="&lt;p&gt;I want to use Canu to correct my nanopore long read (version: MinION R9.5), but I am not quite sure how to set the correctErrorRate. Should I follow the Canu manual (Nanopore R7 2D and Nanopore R9 1D Increase the maximum allowed difference in overlaps from the default of 14.4% to 22.5% with correctedErrorRate=0.225), or you guys have a better option? &lt;/p&gt;&#xA;" OwnerUserId="1364" LastEditorUserId="96" LastEditDate="2017-08-24T15:42:12.300" LastActivityDate="2017-08-24T15:42:12.300" Title="Error rate setting in Canu error correction" Tags="&lt;nanopore&gt;&lt;canu&gt;&lt;quality-control&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2368" PostTypeId="2" ParentId="2343" CreationDate="2017-08-23T10:49:18.397" Score="3" Body="&lt;p&gt;DAVID depend on a couple of databases from the Gene Ontology Consortium, Reactome, KEGG,... most of them are accessible via Bioconductor. To perform an enrichment analysis you can have a look at the tutorial of the several &lt;a href=&quot;http://bioconductor.org/packages/release/BiocViews.html#___GeneSetEnrichment&quot; rel=&quot;nofollow noreferrer&quot;&gt;packages in Bioconductor&lt;/a&gt; that do this. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of the most important for analyzing enrichment in GO terms are &lt;a href=&quot;http://bioconductor.org/packages/topGO&quot; rel=&quot;nofollow noreferrer&quot;&gt;topGO&lt;/a&gt;, &lt;a href=&quot;http://bioconductor.org/packages/goseq&quot; rel=&quot;nofollow noreferrer&quot;&gt;goseq&lt;/a&gt;, &lt;a href=&quot;http://bioconductor.org/packages/GOstats&quot; rel=&quot;nofollow noreferrer&quot;&gt;GOstats&lt;/a&gt;. I would also recommend &lt;a href=&quot;http://bioconductor.org/packages/GOSemSim&quot; rel=&quot;nofollow noreferrer&quot;&gt;GOSemSim&lt;/a&gt; if you want to compare between GO  to focus on a specific GO terms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other important packages are the &lt;a href=&quot;http://bioconductor.org/packages/fgsea&quot; rel=&quot;nofollow noreferrer&quot;&gt;fgsea&lt;/a&gt; to test any kind of gene set (which is similar to the one hosted by the Broad Institute), &lt;a href=&quot;http://bioconductor.org/packages/gsva&quot; rel=&quot;nofollow noreferrer&quot;&gt;gsva&lt;/a&gt; for enrichment analysis by sample, &lt;a href=&quot;http://bioconductor.org/packages/limma&quot; rel=&quot;nofollow noreferrer&quot;&gt;limma&lt;/a&gt; has some functions for functional enrichment too. &lt;a href=&quot;http://bioconductor.org/packages/piano&quot; rel=&quot;nofollow noreferrer&quot;&gt;Piano&lt;/a&gt;, &lt;a href=&quot;http://bioconductor.org/packages/GSCA&quot; rel=&quot;nofollow noreferrer&quot;&gt;GSCA&lt;/a&gt;, &lt;a href=&quot;http://bioconductor.org/packages/SPIA&quot; rel=&quot;nofollow noreferrer&quot;&gt;SPIA&lt;/a&gt; are also worth mentioning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bioconductor has &quot;standard&quot; data sets of the expressions of some cells, like &lt;a href=&quot;http://bioconductor.org/packages/airway&quot; rel=&quot;nofollow noreferrer&quot;&gt;airway&lt;/a&gt; and &lt;a href=&quot;http://bioconductor.org/packages/ALL&quot; rel=&quot;nofollow noreferrer&quot;&gt;ALL&lt;/a&gt; frequently used in vignettes. They are not reference data sets because there isn't a reference expression for a cell of an organisms. It depends on the type of cell, the experiment, the conditions...&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2017-08-23T10:49:18.397" CommentCount="0" />
  <row Id="2370" PostTypeId="2" ParentId="2367" CreationDate="2017-08-24T00:01:39.523" Score="3" Body="&lt;p&gt;It's generally a good idea to trust the &quot;official&quot; suggestions. You can also adjust the error rate based on coverage according to &lt;a href=&quot;http://canu.readthedocs.io/en/latest/parameter-reference.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;parameter reference&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For low coverage datasets (less than 30X), we recommend increasing&#xA;  correctedErrorRate slightly, by 1% or so.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;For high-coverage datasets (more than 60X), we recommend decreasing&#xA;  correctedErrorRate slighly, by 1% or so.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Raising the correctedErrorRate will increase run time. Likewise,&#xA;  decreasing correctedErrorRate will decrease run time, at the risk of&#xA;  missing overlaps and fracturing the assembly.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;That being said, the sequencing quality can vary from between libraries or flow cells. The error rate is not a constant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to the developer (&lt;a href=&quot;https://github.com/marbl/canu/issues/109&quot; rel=&quot;nofollow noreferrer&quot;&gt;full thread&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;if you want to assemble 1D data or have a bad run, it is possible you&#xA;  would need to increase the error rate. You could run with a high error&#xA;  rate (0.1) and look at the distribution of overlap error rates in the&#xA;  unitig step to look for a peak in the distribution. If you have a near&#xA;  neighbor you could also map the corrected reads to it and estimate the&#xA;  residual error that way.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-08-24T00:01:39.523" CommentCount="1" />
  <row Id="2371" PostTypeId="2" ParentId="2356" CreationDate="2017-08-24T00:15:42.657" Score="1" Body="&lt;p&gt;Cell Ranger aggregate subsamples reads (unless you select &lt;code&gt;none&lt;/code&gt;), so you will end up with less total reads in samples that have more initially. The output is still raw counts, but you will have more or less per cell.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Seurat just merges the raw counts matrices and normalizes those.&lt;/p&gt;&#xA;" OwnerUserId="35" LastActivityDate="2017-08-24T00:15:42.657" CommentCount="0" />
  <row Id="2372" PostTypeId="2" ParentId="2359" CreationDate="2017-08-24T13:36:56.130" Score="0" Body="&lt;p&gt;The executable &lt;code&gt;fastq-sim&lt;/code&gt; in &lt;a href=&quot;http://cbrc3.cbrc.jp/~martin/dnemulator/README.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DNemulator&lt;/a&gt; package is able to modify a set of input sequences in &lt;code&gt;fasta&lt;/code&gt; format according to an external set of quality scores reported in a &lt;code&gt;fastq&lt;/code&gt; file. &lt;/p&gt;&#xA;" OwnerUserId="294" LastActivityDate="2017-08-24T13:36:56.130" CommentCount="0" />
  <row Id="2373" PostTypeId="1" AcceptedAnswerId="2374" CreationDate="2017-08-24T13:42:57.060" Score="4" ViewCount="30" Body="&lt;p&gt;I'm looking for a way to identify low complexity regions and other repeats in the genome of &lt;em&gt;Escherichia coli&lt;/em&gt;. I found that RepeatMasker may be used for example when drafting genomes of prokaryotes (&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3587949/&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;E. coli&lt;/em&gt; example&lt;/a&gt;). But RepeatMasker works on a limited dataset of species, neither of them being prokaryotes. By default, when running RepeatMasker, if no species is specified, it will compare with homo sapiens data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This seems rather inadequate, but the most relevent alternative, &lt;a href=&quot;https://sites.google.com/site/prapsoftware/&quot; rel=&quot;nofollow noreferrer&quot;&gt;PRAP&lt;/a&gt;, requires a &quot;dead&quot; tool (VisCoSe, by Michael Spitzer).&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Is it still wise to to use RepeatMasker on &lt;em&gt;Escherichia coli&lt;/em&gt;?&lt;/li&gt;&#xA;&lt;li&gt;If yes, which settings would maximise relevance ?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="522" LastEditorUserId="73" LastEditDate="2017-08-24T23:29:23.597" LastActivityDate="2017-08-24T23:29:23.597" Title="Is it wise to use RepeatMasker on prokaryotes?" Tags="&lt;genome&gt;&lt;repeat-elements&gt;&lt;sequence-analysis&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2374" PostTypeId="2" ParentId="2373" CreationDate="2017-08-24T13:53:40.627" Score="2" Body="&lt;p&gt;If I understood correctly your question, you want to mask those regions in a (FASTA?) genome. I think you could identify those regions using &lt;a href=&quot;http://mummer.sourceforge.net/&quot; rel=&quot;nofollow noreferrer&quot;&gt;mummer&lt;/a&gt; and mask them using &lt;a href=&quot;http://bedtools.readthedocs.io/en/latest/&quot; rel=&quot;nofollow noreferrer&quot;&gt;bedtools&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# align genome against itself&#xA;nucmer --maxmatch --nosimplify genome.fasta genome.fasta&#xA;&#xA;# select repeats and convert the corrdinates to bed format&#xA;show-coords -r -T out.delta -H | awk '{if ($1 != $3 &amp;amp;&amp;amp; $2 != $4) print $0}' | awk '{print $8&quot;\t&quot;$1&quot;\t&quot;$2}' &amp;gt; repeats.bed&#xA;&#xA;# mask those bases with bedtools&#xA;bedtools maskfasta -fi genome.fasta -bed repeats.bed -fo masked.fasta&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Have a look at &lt;a href=&quot;http://mummer.sourceforge.net/manual/#identifyingrepeats&quot; rel=&quot;nofollow noreferrer&quot;&gt;nucmer&lt;/a&gt; and &lt;a href=&quot;http://bedtools.readthedocs.io/en/latest/content/tools/maskfasta.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;bedtools maskfasta&lt;/a&gt; options to fine-tune your analysis.&lt;/p&gt;&#xA;" OwnerUserId="123" LastActivityDate="2017-08-24T13:53:40.627" CommentCount="0" />
  <row Id="2375" PostTypeId="1" AcceptedAnswerId="2376" CreationDate="2017-08-24T16:38:17.097" Score="1" ViewCount="11" Body="&lt;p&gt;I'm trying to run a differential gene expression analysis using DESeq2, with counts coming from kallisto. I have imported them using tximport and I'm creating the &lt;code&gt;DESeqDataSet&lt;/code&gt; (dds) using the &lt;code&gt;DESeqDataSetFromMatrix&lt;/code&gt; function.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; dds &amp;lt;- DESeqDataSetFromMatrix(counts,&#xA;                                samples,&#xA;                                ~ strain + batch)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I get the following error, expected given my experimental design:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error in checkFullRank(modelMatrix) :&#xA;  the model matrix is not full rank, so the model cannot be fit as specified.&#xA;  One or more variables or interaction terms in the design formula are linear&#xA;  combinations of the others and must be removed.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now, I know that I can just remove one column from the design matrix to make it work, but is there a way to supply my own design matrix to DESeq2? The following code raises an error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; design &amp;lt;- model.matrix(~strain+batch, s2c)&#xA;&amp;gt; design = design[, -9] # removing a column to avoid full-rank&#xA;&amp;gt; dds &amp;lt;- DESeqDataSetFromMatrix(counts, s2c, design=design)&#xA;converting counts to integer mode&#xA;Error: $ operator is invalid for atomic vectors&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a way to provide my own &lt;code&gt;model.matrix&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;p.s. the modified model works in sleuth, but I would like to use DESeq2 for this particular analysis.&lt;/p&gt;&#xA;" OwnerUserId="123" LastActivityDate="2017-08-24T17:32:11.120" Title="Is it possible to create a DESeqDataSet with a user-provided design matrix?" Tags="&lt;rna-seq&gt;&lt;deseq2&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2376" PostTypeId="2" ParentId="2375" CreationDate="2017-08-24T17:32:11.120" Score="1" Body="&lt;p&gt;Provide rank sufficient design to &lt;code&gt;DESeqDataSetFromMatrix&lt;/code&gt; and then use your custom model matrix in &lt;code&gt;DESeq&lt;/code&gt;. In essence:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dds = DESeqDataSetFromMatric(counts, s2c, design=~batch)&#xA;design &amp;lt;- model.matrix(~strain+batch, s2c)&#xA;design = design[, -9]&#xA;DESeq(dds, full=design)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&quot;https://support.bioconductor.org/p/64480/&quot; rel=&quot;nofollow noreferrer&quot;&gt;this thread&lt;/a&gt; on the bioconductor site for details.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-24T17:32:11.120" CommentCount="0" />
  <row Id="2377" PostTypeId="1" CreationDate="2017-08-24T21:07:58.983" Score="3" ViewCount="30" Body="&lt;p&gt;I'm having a difficulty in grasping the general purpose and concept of indel calling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What exactly is this process? &lt;/p&gt;&#xA;" OwnerUserId="711" LastEditorUserId="96" LastEditDate="2017-08-24T22:44:55.177" LastActivityDate="2017-08-25T15:58:42.050" Title="What is Indel Calling and what is its purpose?" Tags="&lt;genome&gt;&lt;variant-calling&gt;&lt;indel&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="2378" PostTypeId="1" AcceptedAnswerId="2381" CreationDate="2017-08-24T21:26:20.367" Score="4" ViewCount="20" Body="&lt;p&gt;Given an experiment consisting of an input (baseline RNA) and IP (pulldown to find RNAs bound to certain protein of interest)... Is a DE analysis performed over the RNA-seq data from the samples (lets say with EdgeR or DESEQ2) suitable to reveal the preferentially bound RNAs? What other software tools would you recommend?&lt;/p&gt;&#xA;" OwnerUserId="1375" LastActivityDate="2017-08-25T11:15:54.753" Title="RIP-seq analysis?" Tags="&lt;rna-seq&gt;&lt;software-recommendation&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="2379" PostTypeId="2" ParentId="2377" CreationDate="2017-08-24T22:44:21.977" Score="4" Body="&lt;p&gt;Insertions and deletions (indels) are one type among many different types of &lt;em&gt;genetic variation&lt;/em&gt;, such as single nucleotide variants (SNVs), copy number variants (CNVs), and structural variants (SVs). I'll assume here that you know how indels are defined, but are simple trying to understand the importance of discovering and analyzing them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The goal of indel calling, like the goal of any variant calling, is to identify genetic variants that can subsequently be associated with important phenotypes, esp. disease. For example, if 60% of patients with disease XYZ have an indel in the promoter region of gene 123, then that is information of extreme interest and value in research and in clinical care.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Genome-wide association studies (GWAS) have been trying to correlate SNVs to disease and other phenotypes for years. Much less work has been done with indels, but their discovery and analysis remains an area of intense interest.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as the &lt;em&gt;process&lt;/em&gt; of indel calling, large indels can usually be found  by mapping paired reads and looking for large discrepancies in the expected distance between pairs and the observed distance.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Huh, my average insert size is 400bp, but the aligned read pairs flanking this area are 1200bp apart. Must be an 800bp deletion in there!&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Smaller indels are much more difficult to detect, since they are harder to distinguish from noise (i.e. the variation in length of sequenced fragments).&lt;/p&gt;&#xA;" OwnerUserId="96" LastEditorUserId="96" LastEditDate="2017-08-25T15:58:42.050" LastActivityDate="2017-08-25T15:58:42.050" CommentCount="1" />
  <row Id="2380" PostTypeId="2" ParentId="2378" CreationDate="2017-08-25T06:54:34.863" Score="4" Body="&lt;p&gt;You can try doing standard differential expression, but I worry that the between-sample normalization will work poorly. Personally, I would do peak calling instead, followed by diffBind. You have a few tools to choose from when it comes to this. In the past, I've rolled my own methods for this using MACS2 and genomic alignments (I then converted those to bedGraph files where entries are transcripts, so peak calling is then in transcript-coordinates).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's probably easier, though, to use premade software. There are a number of packages out there for RIP-seq. These include &lt;a href=&quot;https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/bts569&quot; rel=&quot;nofollow noreferrer&quot;&gt;piranha&lt;/a&gt;, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3632129/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RIPSeeker&lt;/a&gt; and &lt;a href=&quot;https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btt428&quot; rel=&quot;nofollow noreferrer&quot;&gt;ASPeak&lt;/a&gt;. Particularly if you're just starting out, you'd be well served with using one of those tools.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2017-08-25T06:54:34.863" CommentCount="0" />
  <row Id="2381" PostTypeId="2" ParentId="2378" CreationDate="2017-08-25T11:15:54.753" Score="3" Body="&lt;p&gt;You need to be careful of terminology. To me, a RIP-seq experiment involves a pull down, followed by a RNAseq library prep. Thus, the whole transcript is captured, not just the binding site of the protein (as in CLIP-Seq, HITS-CLIP, PAR-CLIP, iCLIP or eCLIP). Thus &quot;peak-callers&quot; whether they be designed for calling protein-DNA or protein-RNA binding sites are not suitable as the signal will not be punctuate or peaky. Of the methods mentioned by @Devon Ryan, only &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3632129/&quot; rel=&quot;nofollow noreferrer&quot;&gt;RIPSeeker&lt;/a&gt; seems setup to deal with this sort of data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In some of the papers that talk about calling RNA-protein interactions they lump RIP-seq in with the CLIP techniques (see piranha and ASPeak papers referenced by @Devon Ryan). They appear to be talking about a technique where the RNA is fragmented before it is pulled down. Thus you would capture peaky binding sites (actually, the protocol would be remarkably similar to CLIP). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You should work out which of these you have. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for using DESeq/EdgeR etc, in the RIPSeeker paper they say:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Furthermore, programs for de novo transcript assembly followed by&#xA;  differential expression (DE) analysis, such as the Cufflinks/Cuffdiff&#xA;  suite (15,16), and for DE on a set of known transcripts, such as DESeq&#xA;  (17), may appear applicable to RIP-seq analysis. Unlike peak-calling&#xA;  strategy, however, the transcript-based methods assume the full&#xA;  transcriptome being sequenced at a fairly deep coverage (as usually&#xA;  the case in RNA-seq) and thus may be sensitive to background noise&#xA;  typical to the IP-based protocols, which is due to both the&#xA;  non-specific RNA interactions with a protein of interest and the&#xA;  non-specific RNA input from the pull-down of the (mutant) control&#xA;  (Supplementary Figures S3 and S4).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I don't know to what extent RIPSeeker performs better than the naive DESeq approach as its not included as one of the comparators in the paper. &lt;/p&gt;&#xA;" OwnerUserId="235" LastActivityDate="2017-08-25T11:15:54.753" CommentCount="2" />
  <row Id="2382" PostTypeId="2" ParentId="2377" CreationDate="2017-08-25T11:31:05.573" Score="4" Body="&lt;p&gt;There are two types of INDELs: short indels and long indels. Some put the threshold at 50bp; others choose 1000bp. Short and long indels are called differently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&amp;lt;50bp short indels are called from read alignment directly. Modern indel callers essentially build a multi-alignment of reads and call an indel with enough read supports. Short indels may break open reading frames.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For Illumina reads, long indels are usually called with split alignment or read pair alignment because mainstream aligners are unable to directly align through a long indel. Long indels are a type of structural variation. They may affect gene structures in a more dramatic way (e.g. delete a whole exon; screw up transcription by transposon insertion; create pseudogenes).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ultimately, both short and long indels are types of genetic variants. Calling them helps to understand how genetics shapes phenotypes.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-08-25T11:31:05.573" CommentCount="0" />
  <row Id="2383" PostTypeId="2" ParentId="2359" CreationDate="2017-08-25T11:42:24.700" Score="1" Body="&lt;p&gt;&lt;a href=&quot;http://www.biorxiv.org/content/early/2017/04/11/126656&quot; rel=&quot;nofollow noreferrer&quot;&gt;This preprint&lt;/a&gt; uses pbsim to simulate ONT RNA-seq reads for fruit fly. It is probably worth reading if you want to do the same thing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You should include INDEL errors. Those are what make RNA-seq alignment challenging. For the benchmark purpose, adding INDELs does not increase the complexity at all. You can parse splice junctions on the reference from CIGAR and compare them to the annotation. You don't need to worry about the base-level alignment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition, there are &lt;a href=&quot;https://www.nature.com/articles/ncomms16027&quot; rel=&quot;nofollow noreferrer&quot;&gt;public real ONT data&lt;/a&gt; (AC:SRP082530) for &lt;a href=&quot;https://www.lexogen.com/sirvs/&quot; rel=&quot;nofollow noreferrer&quot;&gt;SIRV spike-in control&lt;/a&gt; and mouse B cells. You don't actually need simulation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: just noticed that you are an author of the first preprint I cited. I would use real data for evaluation.&lt;/p&gt;&#xA;" OwnerUserId="37" LastEditorUserId="37" LastEditDate="2017-08-25T12:23:33.937" LastActivityDate="2017-08-25T12:23:33.937" CommentCount="0" />
  <row Id="2384" PostTypeId="1" AcceptedAnswerId="2385" CreationDate="2017-08-25T11:46:38.820" Score="1" ViewCount="15" Body="&lt;p&gt;I have 3224 Ensembl id's as rownames in a dataframe &quot;G&quot;. To convert Ensembl ids into Genesymbols I used biomart like following.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library('biomaRt')&#xA;mart &amp;lt;- useDataset(&quot;hsapiens_gene_ensembl&quot;, useMart(&quot;ensembl&quot;))&#xA;genes &amp;lt;- rownames(G)&#xA;G &amp;lt;-G[,-6]&#xA;G_list &amp;lt;- getBM(filters= &quot;ensembl_gene_id&quot;, attributes= c(&quot;ensembl_gene_id&quot;,&quot;hgnc_symbol&quot;),values=genes,mart= mart)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now in G_list I can see only 3200 ensembl ids showing Genesymbols / No Gene_symbols. Why the other 24 ensembl ids are not seen in G_list? If there are no gene_symbol for those 24 ensembl ids it should atleast show &quot;-&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Examples of problematic IDs are: ENSG00000257061, ENSG00000255778, ENSG00000267268. These are not at all shown in G_list (biomaRt). So, I gave them in biodbnet, which seems to handle them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;what is the problem here?&lt;/p&gt;&#xA;" OwnerUserId="1377" LastEditorUserId="77" LastEditDate="2017-08-25T12:07:08.273" LastActivityDate="2017-08-25T13:03:35.240" Title="Ensembl id to GeneSymbol with biomart" Tags="&lt;r&gt;&lt;bioconductor&gt;&lt;biomart&gt;&lt;ensembl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2385" PostTypeId="2" ParentId="2384" CreationDate="2017-08-25T12:10:40.503" Score="2" Body="&lt;p&gt;It looks like you were using an old annotation. The problematic IDs you posted existed in the GRCh37 annotations, but don't in the most recent GRCh38 annotation. For that reason they were excluded. The IDs that have &lt;code&gt;-&lt;/code&gt; as symbols don't have associated symbols, but are present in the database.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To use an archived version in biomart:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mart = useDataset(&quot;hsapiens_gene_ensembl&quot;, useEnsembl(biomart=&quot;ensembl&quot;, version=84))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That's an example for release 84.&lt;/p&gt;&#xA;" OwnerUserId="77" LastEditorUserId="77" LastEditDate="2017-08-25T13:03:35.240" LastActivityDate="2017-08-25T13:03:35.240" CommentCount="12" />
  <row Id="2386" PostTypeId="2" ParentId="2363" CreationDate="2017-08-25T12:17:12.407" Score="3" Body="&lt;p&gt;I modified your original question: as you are extracting 4 fields, you are not outputting BAM. The answer to the modified question is: yes, you can write a C program with htslib (or with bamtools, bioD, bioGo or rust-bio). Formatting an entire SAM is fairly expensive. You can see this by comparing &lt;code&gt;samtools view aln.bam &amp;gt; /dev/null&lt;/code&gt; and &lt;code&gt;samtools view -u aln.bam &amp;gt; /dev/null&lt;/code&gt;. With a C program, you can select fields to output. This will give you a noticeable performance boost, depending on the fields you extract.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, if you really care about performance, don't use script. The time you save from optimizing &lt;code&gt;samtools view|cut -f&lt;/code&gt; will be quickly neutralized by the inefficiency of most scripting languages.&lt;/p&gt;&#xA;" OwnerUserId="37" LastActivityDate="2017-08-25T12:17:12.407" CommentCount="0" />
  <row Id="2387" PostTypeId="2" ParentId="2363" CreationDate="2017-08-25T14:27:47.847" Score="0" Body="&lt;p&gt;Another library you can use for this purpose is the &lt;a href=&quot;https://github.com/samtools/htsjdk&quot; rel=&quot;nofollow noreferrer&quot;&gt;htsjdk&lt;/a&gt;, which is written in java. Using htsjdk with java is analogous to using htslib with C; the BAM format is already handled by the library and you can manipulate fields in your own code. The same basic analysis applies to java as C; you do not need to convert the BAM file to text and then parse it. Generally, a program implemented in java will be slower than C but still significantly faster than scripting. You do get language features of java: portability, array bounds checking, garbage collection, etc. &lt;/p&gt;&#xA;" OwnerUserId="272" LastActivityDate="2017-08-25T14:27:47.847" CommentCount="0" />
  <row Id="2388" PostTypeId="2" ParentId="2338" CreationDate="2017-08-25T21:30:44.297" Score="2" Body="&lt;p&gt;Here's my current understanding of the freemix metric output by verifyBamID. The freemix score is a measure of contamination that is calculated without knowing the genotypes of the individuals. In the paper it is equation (2), and the authors refer to it as the &quot;sequence-only&quot; method. However, in addition to the sequencing reads, it requires some knowledge of SNPs and their allele frequency from the population. They demonstrate in the paper that using the SNPs and allele frequency estimates from a different population actually reduces the estimated level of contamination.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for intuition for what equation (2) is measuring, imagine what would happen to the number of heterozygous sites detected if DNA from two different individuals was sequenced together. The total number of heterozygous sites would increase because it would count all sites where the two individuals are homozygous for opposite alleles or at least one of the two individuals is heterozygous. This would lead to an increased fraction of heterozygous sites over that expected from &lt;a href=&quot;https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hardy-Weinberg Equilibrium&lt;/a&gt; (i.e. 2pq).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the &lt;a href=&quot;https://groups.google.com/d/msg/verifybamid/oNMOfEZDwE4/AbVP_D-mbBEJ&quot; rel=&quot;nofollow noreferrer&quot;&gt;explanation of freemix&lt;/a&gt; directly from the author, Hyun Min Kang:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The key idea of FREEMIX estimate is to use excessive heterozygosity to estimate the level of contamination. Especially for common SNPs, you will observe higher fraction of heterozygous alleles than 2*p*(1-p), and it turns out that you can quantify the contamination very well if you know the population allele frequency already. If you do not have accurate population allele frequency information, than it would be harder to estimate FREEMIX parameters using verifyBamID.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Additional sources:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://genome.sph.umich.edu/wiki/Verifying_Sample_Identities_-_Implementation&quot; rel=&quot;nofollow noreferrer&quot;&gt;Verifying Sample Identities - Implementation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://genome.sph.umich.edu/wiki/VerifyBamID#Interpreting_output_files&quot; rel=&quot;nofollow noreferrer&quot;&gt;Interpreting output files&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://groups.google.com/forum/#!forum/verifybamid&quot; rel=&quot;nofollow noreferrer&quot;&gt;verifyBamID Google Group&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="1302" LastActivityDate="2017-08-25T21:30:44.297" CommentCount="0" />
  <row Id="2389" PostTypeId="1" CreationDate="2017-08-26T18:22:30.873" Score="0" ViewCount="4" Body="&lt;p&gt;How are you using altmetrics, have they made any difference to impact of your publications? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://bjsm.bmj.com/content/early/2017/08/22/bjsports-2017-098258&quot; rel=&quot;nofollow noreferrer&quot;&gt;‘Altmetrics’! Can you afford to ignore it?&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1384" LastActivityDate="2017-08-26T18:22:30.873" Title="Are altmetrics any use?" Tags="&lt;file-formats&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2390" PostTypeId="1" CreationDate="2017-08-26T21:14:09.057" Score="1" ViewCount="2" Body="&lt;p&gt;In my greedy search with pseudocounts algorithm in my bioinformatics course, I did not follow the pseudocode since I wanted to solve the problem in my own way. Unfortunately, although my answer is correct, the answer we are supposed to give is the first correct answer, not the last.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem works as such: Given t strings, you make a profile matrix from the first k-mer in the first string, and then you look at the next string and using that profile matrix, you find the best k-mer match, and then you combine that k-mer with the one from the first string to make a new profile matrix, etc. You eventually do this for all k-mer windows in your first string of DNA. The way I did it was by making a probability profile matrix from the best k-mer groups in each case, and then multiplying the greatest probabilities in each column. This gives me a score, and if the next score is higher (thus, more probable) that group of k-mers is used. Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; def highest_probability(string, k, matrix):&#xA;    score = 1&#xA;    temp_string = &quot;&quot;&#xA;    c = 0&#xA;    best_string = &quot;&quot;&#xA;    temp = 0&#xA;    while c+k &amp;lt; len(string):&#xA;        for i in range(c, c+k):&#xA;            if string[i] == 'A':&#xA;                score *= matrix[i-c]['A']&#xA;                temp_string += 'A'&#xA;            elif string[i] == 'C':&#xA;                score *= matrix[i-c]['C']&#xA;                temp_string += 'C'&#xA;            elif string[i] == 'G':&#xA;                score *= matrix[i-c]['G']&#xA;                temp_string += 'G'&#xA;            elif string[i] == 'T':&#xA;                score *= matrix[i-c]['T']&#xA;                temp_string += 'T'&#xA;            if i == c+k-1:&#xA;                if score &amp;gt;= temp:&#xA;                    temp = score&#xA;                    best_string = temp_string&#xA;        temp_string=&quot;&quot;&#xA;        score=1&#xA;        c+=1&#xA;    print(best_string)&#xA;    return best_string&#xA;&#xA;&#xA;def GreedyMotifSearch(DNA, k, t):&#xA;    best_motifs = [i[0:k] for i in DNA]&#xA;    score=0&#xA;    c=0&#xA;    while c+k&amp;lt;len(DNA[0]):&#xA;        k_mer = DNA[0][c:c+k]&#xA;        motif=[k_mer]&#xA;        matrix = make_matrix(k_mer, matrix=[], i=0)&#xA;        for i in range(1, t):&#xA;            compare_string = DNA[i]&#xA;            good_motif = highest_probability(compare_string, k, matrix)&#xA;            motif.append(good_motif)&#xA;            matrix = make_matrix(good_motif, matrix, i)&#xA;        temp_score=1&#xA;        for dictionary in matrix:&#xA;            temp_score *= max(dictionary.values())&#xA;        print(temp_score)&#xA;        if temp_score &amp;gt; score:&#xA;            score = temp_score&#xA;            best_motifs = motif&#xA;        c+=1&#xA;    return best_motifs&#xA;&#xA;&#xA;&#xA;def make_matrix(string, matrix, i):&#xA;    if len(matrix)==0:&#xA;        for ch in string:&#xA;            new_dict = {'A':0, 'C':0, 'G':0, 'T':0}&#xA;            new_dict[ch] = 1&#xA;            matrix += [new_dict]&#xA;        print(matrix)&#xA;        return matrix&#xA;    elif len(matrix)==3:&#xA;        for j in range(len(string)):&#xA;            matrix[j][string[j]] += 1&#xA;        Sum = [sum(matrix[0].values()),sum(matrix[1].values()),sum(matrix[2].values())]&#xA;        for columns in range(len(matrix)):&#xA;            for keys in matrix[columns]:&#xA;                matrix[columns][keys] /= Sum[columns]&#xA;        print(matrix)&#xA;        return matrix &#xA;&#xA;DNA=['GGCGTTCAGGCA',&#xA;'AAGAATCAGTCA',&#xA;'CAAGGAGTTCGC',&#xA;'CACGTCAATCAC',&#xA;'CAATAATATTCG']&#xA;k=3&#xA;t=5&#xA;print(GreedyMotifSearch(DNA, k, t))  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The output gives me ['TTC', 'GTC', 'TTC', 'GTC', 'TTC'] which is equally correct to the answer given in the problem output which is TTC,ATC,TTC,ATC,TTC. Unfortunately, my answer is the last correct occurrence. I tried adjusting my highest_probability function with if score is greater than temp instead of greater or equal, but that gives me the answer ['GGC', '', '', '', ''] which I don't understand at all. My question is 1) Why is this the case? and 2) How can I fix this? And does anyone have a simpler way of doing this?&lt;/p&gt;&#xA;" OwnerUserId="1385" LastActivityDate="2017-08-27T02:19:55.020" Title="Greedy Motif Search Using Probability Matrices" Tags="&lt;python&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2391" PostTypeId="2" ParentId="2390" CreationDate="2017-08-27T02:19:55.020" Score="0" Body="&lt;p&gt;Assuming you're talking about &lt;a href=&quot;http://rosalind.info/problems/ba2d/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Implement GreedyMotifSearch&lt;/a&gt; from Rosalind, while I haven't actually finished this particular one, I've got an idea why &lt;code&gt;TTC,ATC,TTC,ATC,TTC&lt;/code&gt; doesn't work. Here's my explanation from the &quot;Questions&quot; section of that problem:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The profile is created from the first kmers in each string, and the most likely string from those is CAG. The answers will generally be similar to this.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I've been trying to do Rosalind by only using the information provided in the questions; this is one of the many problems where there wasn't enough information in the Question for me to arrive at a correct solution when I tried it. It's possible that the debug datasets were added to this question since I last looked at it, as they seem to describe this situation:&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;TEST DATASET 1:&lt;/h2&gt;&#xA;&#xA;&lt;h2&gt;Input:&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;3 4&#xA;GCCCAA&#xA;GGCCTG&#xA;AACCTA&#xA;TTCCTT&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Output:&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GCC&#xA;GCC&#xA;AAC&#xA;TTC&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;This dataset checks that your code always picks the first-occurring Profile-most Probable k-mer in a given sequence of Dna. In the first sequence (“GCCCAA”), “GCC” and “CCA” are both Profile-most Probable k-mers. However, you must return “GCC” since it occurs earlier than “CCA”. Thus, if the first sequence of your output is “CCA”, this test case fails your code.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="73" LastActivityDate="2017-08-27T02:19:55.020" CommentCount="0" />
</posts>